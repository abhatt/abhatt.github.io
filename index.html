<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="http://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at August 03, 2021 09:40 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/08/02/complexity-postdoctoral-fellowship-at-santa-fe-institute-apply-by-october-24-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/08/02/complexity-postdoctoral-fellowship-at-santa-fe-institute-apply-by-october-24-2021/">Complexity Postdoctoral Fellowship at Santa Fe Institute (apply by October 24, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Unique among postdocs, these 3-year fellowships offer the opportunity to join a collaborative community that nurtures creative, transdisciplinary thought in pursuit of key insights about the complex systems that matter most for science and society. Benefits include research/collaboration funds, paid family leave, and a professional leadership &amp; development program.</p>
<p>Website: <a href="https://santafe.edu/news-center/news/apply-now-santa-fe-institute-postdoctoral-fellowships-2021">https://santafe.edu/news-center/news/apply-now-santa-fe-institute-postdoctoral-fellowships-2021</a><br />
Email: sfifellowship@santafe.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/08/02/complexity-postdoctoral-fellowship-at-santa-fe-institute-apply-by-october-24-2021/"><span class="datestr">at August 02, 2021 10:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5675">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5675">On blankfaces</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>For years, I’ve had a private term I’ve used with my family.  To give a few examples of its use:</p>



<blockquote class="wp-block-quote"><p>No, I never applied for that grant. I spent two hours struggling to log in to a web portal designed by the world’s top blankfaces until I finally gave up in despair.</p></blockquote>



<blockquote class="wp-block-quote"><p>No, I paid for that whole lecture trip out of pocket; I never got the reimbursement they promised.  Their blankface administrator just kept sending me back the form, demanding more and more convoluted bank details, until I finally got the hint and dropped it.</p></blockquote>



<blockquote class="wp-block-quote"><p>No, my daughter Lily isn’t allowed in the swimming pool there.  She easily passed their swim test last year, but this year the blankface lifeguard made up a new rule on the spot that she needs to retake the test, so Lily took it again and passed even <em>more</em> easily, but then the lifeguard said she didn’t like the stroke Lily used, so she failed her and didn’t let her retake it.  I complained to their blankface athletic director, who launched an ‘investigation.’  The outcome of the ‘investigation’ was that, regardless of the ground truth about how well Lily can swim, their blankface lifeguard said she’s not allowed in the pool, so being blankfaces themselves, they’re going to stand with the lifeguard.</p></blockquote>



<blockquote class="wp-block-quote"><p>Yeah, the kids spend the entire day indoors, breathing each other’s stale, unventilated air, then they finally go outside and they aren’t allowed on the playground equipment, because of the covid risk from them touching it.  Even though we’ve known for more than a year that covid is an airborne disease.  Everyone I’ve talked there agrees that I have a point, but they say their hands are tied.  I haven’t yet located the blankface who actually made this decision and stands by it.</p></blockquote>



<p>What exactly is a blankface?  He or she is often a mid-level bureaucrat, but not every bureaucrat is a blankface, and not every blankface is a bureaucrat.  A blankface is anyone who enjoys wielding the power entrusted in them to make others miserable by acting like a cog in a broken machine, rather than like a human being with courage, judgment, and responsibility for their actions.  A blankface meets every appeal to facts, logic, and plain compassion with the same repetition of rules and regulations and the same blank stare—a blank stare that, more often than not, conceals a contemptuous smile.</p>



<p>The longer I live, the more I see blankfacedness as one of the fundamental evils of the human condition.  Yes, it contains large elements of stupidity, incuriosity, malevolence, and bureaucratic indifference, but it’s not reducible to any of those.  After enough experience, the first two questions you ask about any organization are:</p>



<ol><li>Who are the blankfaces here?</li><li>Who are the people I can talk with to get around the blankfaces?</li></ol>



<p>As far as I can tell, blankfacedness cuts straight across conventional political ideology, gender, and race.  (Age, too, except that I’ve never once encountered a blankfaced child.)  Brilliance and creativity do seem to offer some protection against blankfacedness—possibly because the smarter you are, the harder it is to justify idiotic rules to yourself—but even there, the protection is far from complete.</p>



<hr class="wp-block-separator" />



<p>Twenty years ago, all the conformists in my age cohort were obsessed with the <em>Harry Potter</em> books and movies—holding parties where they wore wizard costumes, etc.  I decided that the <em>Harry Potter</em> phenomenon was a sort of collective insanity: from what I could tell, the stories seemed like startlingly puerile and unoriginal mass-marketed wish-fulfillment fantasies.</p>



<p>Today, those same conformists in my age cohort are more likely to condemn the <em>Harry Potter</em> series as Problematically white, male, and cisnormative, and J. K. Rowling herself as a monstrous bigot whose acquaintances’ acquaintances should be shunned.  Naturally, then, there was nothing for me to do but finally read the series!  My 8-year-old daughter Lily and I have been partner-reading it for half a year; we’re just finishing book 5.  (<em>After</em> we’ve finished the series, we might start on <em><a href="http://www.hpmor.com/">Harry Potter and the Methods of Rationality</a></em> … which, I confess, I’ve also never read.)</p>



<p>From book 5, I learned something extremely interesting.  The most despicable villain in the <em>Harry Potter</em> universe is not Lord Voldemort, who’s mostly just a faraway cipher and abstract embodiment of pure evil, no more hateable than an earthquake.  Rather, it’s <a href="https://en.wikipedia.org/wiki/Dolores_Umbridge">Dolores Jane Umbridge</a>, the toadlike Ministry of Magic bureaucrat who takes over Hogwarts school, forces out Dumbledore as headmaster, and terrorizes the students with increasingly draconian “Educational Decrees.”  Umbridge’s decrees are mostly aimed at punishing Harry Potter and his friends, who’ve embarrassed the Ministry by telling everyone the truth that Voldemort has returned and by readying themselves to fight him, thereby defying the Ministry’s head-in-the-sand policy.</p>



<p>Anyway, I’ll say this for <em>Harry Potter</em>: Rowling’s portrayal of Umbridge is so spot-on and merciless that, for anyone who knows the series, I could simply <em>define</em> a blankface to be anyone sufficiently Umbridge-like.</p>



<hr class="wp-block-separator" />



<p>This week I <em>also</em> finished reading <em><a href="https://www.amazon.com/Premonition-Pandemic-Story-Michael-Lewis-ebook/dp/B08V91YY8R">The Premonition</a></em>, the thrilling account of the runup to covid by <a href="https://en.wikipedia.org/wiki/Michael_Lewis">Michael Lewis</a> (who also wrote <em><a href="https://www.amazon.com/Big-Short-Inside-Doomsday-Machine/dp/0393338827">The Big Short</a></em>, <em><a href="https://en.wikipedia.org/wiki/Moneyball">Moneyball</a></em>, etc).  Lewis tells the stories of a few individuals scattered across US health and government bureaucracies who figured out over the past 20 years that the US was breathtakingly unprepared for a pandemic, and who struggled against official indifference, mostly unsuccessfully, to try to fix that.  As covid hit the US in early 2020, these same individuals frantically tried to pull the fire alarms, even as the Trump White House, the CDC, and state bureaucrats all did everything in their power to block and sideline them.  We all know the results.</p>



<p>It’s no surprise that, in Lewis’s telling, Trump and his goons come in for world-historic blame: however terrible you thought they were, they were worse.  It seems that John Bolton, in particular, gleefully took an ax to everything the two previous administrations had done to try to prepare the federal government for pandemics—after Tom Bossert, the one guy in Trump’s inner circle who’d actually taken pandemic preparation seriously, was forced out for contradicting Trump about Russia and Ukraine.</p>



<p>But the left isn’t spared either.  The most compelling character in <em>The Premonition</em> is <a href="https://en.m.wikipedia.org/wiki/Charity_Dean">Charity Dean</a>, who escaped from the Christian fundamentalist sect in which she was raised to put herself through medical school and become a crusading public-health officer for Santa Barbara County.  Lewis relates with relish how, again and again, Dean startled the bureaucrats around her by taking matters into her own hands in her war against pathogens—e.g., slicing into a cadaver herself to take samples when the people whose job it was wouldn’t do it.</p>



<p>In 2019, Dean moved to Sacramento to become California’s next chief public health officer, but then Governor Gavin Newsom blocked her expected promotion, instead recruiting someone from the outside named Sonia Angell, who had no infectious disease experience but to whom Dean would have to report.  Lewis reports the following as the reason:</p>



<blockquote class="wp-block-quote"><p>“It was an optics problem,” says a senior official in the Department of Health and Human Services.  “Charity was too young, too blond, too Barbie.  They wanted a person of color.”  Sonia Angell identified as Latina.</p></blockquote>



<p>After it became obvious that the White House and the CDC were both asleep at the wheel, the competent experts’ Plan B was to get California to set a national standard, one that would shame all the other states into acting, by telling the truth about covid and by aggressively testing, tracing, and isolating.  And here comes the tragedy: Charity Dean spent from mid-January till mid-March trying to do exactly that, and Sonia Angell blocked her.  Angell—who comes across as a real-life Dolores Umbridge—banned Dean from using the word “pandemic,” screamed at her for her insubordination, and systematically shut her out of meetings.  Angell’s stated view was that, until and unless the CDC said that there was a pandemic, <em>there was no pandemic</em>—regardless of what hospitals across California might be reporting to the contrary.</p>



<p>As it happens, California <em>was</em> the first state to move aggressively against covid, on March 19—basically because as the bodies started piling up, Dean and her allies finally managed to maneuver around Angell and get the ear of Governor Newsom directly.  Had the response started earlier, the US might have had an outcome more in line with most industrialized countries.  Half of the 630,000 dead Americans might now be alive.</p>



<p>Sonia Angell fully deserves to have her name immortalized by history as one of the blankest of blankfaces.  But of course, Angell was far from alone.  Robert Redfield, Trump’s CDC director, was a blankface extraordinaire.  Nancy Messonnier, who lied to stay in Trump’s good graces, was a blankface too.  The entire CDC and FDA seem to have teemed with blankfaces.  As for Anthony Fauci, he became a national hero, maybe even deservedly so, merely by <em>not being 100%</em> a blankface, when basically every other “expert” in the US with visible power was.  Fauci cleared a depressingly low bar, one that the people profiled by Lewis cleared at Simone-Biles-like heights.</p>



<p>In March 2020, the fundamental question I had was: where are the supercompetent rule-breaking American heroes from the disaster movies?  What’s taking them so long?  <em>The Premonition</em> satisfyingly answers that question.  It turns out that the heroes did exist, scattered across the American health bureaucracy.  They were screaming at the top of their lungs.  But they were outvoted by the critical mass of blankfaces that’s become one of my country’s defining features.</p>



<hr class="wp-block-separator" />



<p>Some people will object that the term “blankface” is dehumanizing.  The reason I disagree is that a blankface is someone who freely chose to dehumanize <em>themselves</em>: to abdicate their human responsibility to see what’s right in front of them, to <em>act like</em> malfunctioning pieces of electronics even though they, like all of us, were born with the capacity for empathy and reason.</p>



<p>With many other human evils and failings, I have a strong inclination toward mercy, because I understand how someone could’ve succumbed to the temptation—indeed, I worry that I myself might’ve succumbed to it “but for the grace of God.”  But here’s the thing about blankfaces: in all my thousands of dealings with them, not once was I ever given cause to wonder whether I might have done the same in their shoes.  It’s like,<em> of course</em> I wouldn’t have!  Even if I were forced (by my own higher-ups, an intransigent computer system, or whatever else) to foist some bureaucratic horribleness on an innocent victim, I’d be sheepish and apologetic about it.  I’d acknowledge the farcical absurdity of what I was making the other person do, or declaring that they couldn’t do.  Likewise, even if I were useless in a crisis, at least I’d <em>get out of the way</em> of the people trying to solve it.  How could I live with myself otherwise?</p>



<p>The fundamental mystery of the blankfaces, then, is how they can be so alien and yet so common.</p>



<hr class="wp-block-separator" />



<p><strong><span class="has-inline-color has-vivid-red-color">Update (Aug. 3):</span></strong> Surprisingly many people seem to have read this post, and come away with the notion that a “blankface” is simply anyone who’s a stickler for rules and formalized procedures.  They’ve then tried to refute me with examples of where it’s <em>good</em> to be stickler, or where I in particular would agree that it’s good.</p>



<p>But no, that’s not it at all.</p>



<p>Rules can be either good or bad.  All things considered, I’d probably rather be on a plane piloted by a stickler for safety rules, than by someone who ignored the rules at his or her discretion.  And as I said in the post, in the first months of covid, it was ironically the <em>anti</em>-blankfaces who were screaming for rules, regulations, and lockdowns; the blankfaces wanted to continue as though nothing had changed.</p>



<p>Also, “blankface” (just like “homophobe” or “antisemite”) is a serious accusation.  I’d never call anyone a blankface merely for sticking with a defensible rule when it turned out, in hindsight, that the rule could’ve been relaxed.</p>



<p>Here’s how to tell a blankface: suppose you see someone enforcing or interpreting a rule in a way that strikes you as <em>obviously</em> absurd.  And suppose you point it out to them.</p>



<p>Do they say “I disagree, here’s why it actually <em>does</em> make sense”?  They might be mistaken but they’re not a blankface.</p>



<p>Do they say “tell me about it, it makes <em>zero</em> sense, but it’s above my pay grade to change”?  You might wish they were more dogged or courageous but again they’re not a blankface.</p>



<p>Or do they ignore your arguments and just blankly restate the original rule—seemingly angered by what they understood as a challenge to their authority, and delighted to reassert it?  <em>That’s</em> the blankface.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5675"><span class="datestr">at August 02, 2021 08:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.14799">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.14799">Generating Boolean Functions on Totalistic Automata Networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Eric Goles, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Adamatzky:Andrew.html">Andrew Adamatzky</a>, Pedro Montealegre, Martín Ríos-Wilson <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.14799">PDF</a><br /><b>Abstract: </b>We consider the problem of studying the simulation capabilities of the
dynamics of arbitrary networks of finite states machines. In these models, each
node of the network takes two states 0 (passive) and 1 (active). The states of
the nodes are updated in parallel following a local totalistic rule, i.e.,
depending only on the sum of active states. Four families of totalistic rules
are considered: linear or matrix defined rules (a node takes state 1 if each of
its neighbours is in state 1), threshold rules (a node takes state 1 if the sum
of its neighbours exceed a threshold), isolated rules (a node takes state 1 if
the sum of its neighbours equals to some single number) and interval rule (a
node takes state 1 if the sum of its neighbours belong to some discrete
interval). We focus in studying the simulation capabilities of the dynamics of
each of the latter classes. In particular, we show that totalistic automata
networks governed by matrix defined rules can only implement constant functions
and other matrix defined functions. In addition, we show that t by threshold
rules can generate any monotone Boolean functions. Finally, we show that
networks driven by isolated and the interval rules exhibit a very rich spectrum
of boolean functions as they can, in fact, implement any arbitrary Boolean
functions. We complement this results by studying experimentally the set of
different Boolean functions generated by totalistic rules on random graphs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.14799"><span class="datestr">at August 02, 2021 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.14692">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.14692">Algorithms for Right-Sizing Heterogeneous Data Centers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Albers:Susanne.html">Susanne Albers</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/q/Quedenfeld:Jens.html">Jens Quedenfeld</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.14692">PDF</a><br /><b>Abstract: </b>Power consumption is a dominant and still growing cost factor in data
centers. In time periods with low load, the energy consumption can be reduced
by powering down unused servers. We resort to a model introduced by Lin,
Wierman, Andrew and Thereska that considers data centers with identical
machines, and generalize it to heterogeneous data centers with $d$ different
server types. The operating cost of a server depends on its load and is modeled
by an increasing, convex function for each server type. In contrast to earlier
work, we consider the discrete setting, where the number of active servers must
be integral. Thereby, we seek truly feasible solutions. For homogeneous data
centers ($d=1$), both the offline and the online problem were solved optimally
by Albers and Quedenfeld (2018).
</p>
<p>In this paper, we study heterogeneous data centers with general
time-dependent operating cost functions. We develop an online algorithm based
on a work function approach which achieves a competitive ratio of $2d + 1 +
\epsilon$ for any $\epsilon &gt; 0$. For time-independent operating cost
functions, the competitive ratio can be reduced to $2d + 1$. There is a lower
bound of $2d$ shown by Albers and Quedenfeld (2021), so our algorithm is nearly
optimal. For the offline version, we give a graph-based
$(1+\epsilon)$-approximation algorithm. Additionally, our offline algorithm is
able to handle time-variable data-center sizes.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.14692"><span class="datestr">at August 02, 2021 10:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.14672">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.14672">Algorithms for Energy Conservation in Heterogeneous Data Centers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Albers:Susanne.html">Susanne Albers</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/q/Quedenfeld:Jens.html">Jens Quedenfeld</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.14672">PDF</a><br /><b>Abstract: </b>Power consumption is the major cost factor in data centers. It can be reduced
by dynamically right-sizing the data center according to the currently arriving
jobs. If there is a long period with low load, servers can be powered down to
save energy. For identical machines, the problem has already been solved
optimally by Lin et al. (2013) and Albers and Quedenfeld (2018).
</p>
<p>In this paper, we study how a data-center with heterogeneous servers can
dynamically be right-sized to minimize the energy consumption. There are $d$
different server types with various operating and switching costs. We present a
deterministic online algorithm that achieves a competitive ratio of $2d$ as
well as a randomized version that is $1.58d$-competitive. Furthermore, we show
that there is no deterministic online algorithm that attains a competitive
ratio smaller than $2d$. Hence our deterministic algorithm is optimal. In
contrast to related problems like convex body chasing and convex function
chasing, we investigate the discrete setting where the number of active servers
must be integral, so we gain truly feasible solutions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.14672"><span class="datestr">at August 02, 2021 10:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.14646">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.14646">Cache Replacement Algorithm</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ali:Sarwan.html">Sarwan Ali</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.14646">PDF</a><br /><b>Abstract: </b>Cache replacement algorithms are used to optimize the time taken by processor
to process the information by storing the information needed by processor at
that time and possibly in future so that if processor needs that information,
it can be provided immediately. There are a number of techniques (LIFO, FIFO,
LRU, MRU, Hybrid) used to organize information in such a way that processor
remains busy almost all the time. But there are some limitations of every
technique. We tried to overcome those limitations. We used Probabilistic
Graphical Model(PGM), which gives conditional dependency between random
variables using directed or undirected graph. In our research, we exploited the
Bayesian network technique to predict the future request by processor. The main
goal of the research was to increase the cache hit rate but not by increasing
the size of cache and also reducing or maintaining the overhead. We achieved 7%
more cache hits in best case scenario than those classical algorithms by using
PGM technique. This proves the success of our technique as far as cache hits
are concerned. Also, pre-eviction proves to be a better technique to get more
cache hits. Combining both pre-eviction and pre-fetching using PGM gives us the
results which were intended to achieve as the sole purpose of this research.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.14646"><span class="datestr">at August 02, 2021 10:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.14608">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.14608">An iterative coordinate descent algorithm to compute sparse low-rank approximations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rusu:Cristian.html">Cristian Rusu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.14608">PDF</a><br /><b>Abstract: </b>In this paper, we describe a new algorithm to build a few sparse principal
components from a given data matrix. Our approach does not explicitly create
the covariance matrix of the data and can be viewed as an extension of the
Kogbetliantz algorithm to build an approximate singular value decomposition for
a few principal components. We show the performance of the proposed algorithm
to recover sparse principal components on various datasets from the literature
and perform dimensionality reduction for classification applications.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.14608"><span class="datestr">at August 02, 2021 10:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.14588">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.14588">On the Configurations of Closed Kinematic Chains in three-dimensional Space</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zangerl:Gerhard.html">Gerhard Zangerl</a>, Alexander Steinicke <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.14588">PDF</a><br /><b>Abstract: </b>A kinematic chain in three-dimensional Euclidean space consists of $n$ links
that are connected by spherical joints. Such a chain is said to be within a
closed configuration when its link lengths form a closed polygonal chain in
three dimensions. We investigate the space of configurations, described in
terms of joint angles of its spherical joints, that satisfy the the loop
closure constraint, meaning that the kinematic chain is closed. In special
cases, we can find a new set of parameters that describe the diagonal lengths
(the distance of the joints from the origin) of the configuration space by a
simple domain, namely a cube of dimension $n-3$. We expect that the new
findings can be applied to various problems such as motion planning for closed
kinematic chains or singularity analysis of their configuration spaces. To
demonstrate the practical feasibility of the new method, we present numerical
examples.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.14588"><span class="datestr">at August 02, 2021 10:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.14577">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.14577">Fast direct access to variable length codes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ryabko:Boris.html">Boris Ryabko</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.14577">PDF</a><br /><b>Abstract: </b>We consider the issue of direct access to any letter of a sequence encoded
with a variable length code and stored in the computer's memory, which is a
special case of the random access problem to compressed memory. The
characteristics according to which methods are evaluated are the access time to
one letter and the memory used. The proposed methods, with various trade-offs
between the characteristics, outperform the known ones.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.14577"><span class="datestr">at August 02, 2021 10:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.14550">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.14550">Assistance and Interdiction Problems on Interval Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoang:Hung_P=.html">Hung P. Hoang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lendl:Stefan.html">Stefan Lendl</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wulf:Lasse.html">Lasse Wulf</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.14550">PDF</a><br /><b>Abstract: </b>We introduce a novel framework of graph modifications specific to interval
graphs. We study interdiction problems with respect to these graph
modifications. Given a list of original intervals, each interval has a
replacement interval such that either the replacement contains the original, or
the original contains the replacement. The interdictor is allowed to replace up
to $k$ original intervals with their replacements. Using this framework we also
study the contrary of interdiction problems which we call assistance problems.
We study these problems for the independence number, the clique number,
shortest paths, and the scattering number. We obtain polynomial time algorithms
for most of the studied problems. Via easy reductions, it follows that on
interval graphs, the most vital nodes problem with respect to shortest path,
independence number and Hamiltonicity can be solved in polynomial time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.14550"><span class="datestr">at August 02, 2021 10:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.14527">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.14527">A Framework for Adversarial Streaming via Differential Privacy and Difference Estimators</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Attias:Idan.html">Idan Attias</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cohen:Edith.html">Edith Cohen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shechner:Moshe.html">Moshe Shechner</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stemmer:Uri.html">Uri Stemmer</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.14527">PDF</a><br /><b>Abstract: </b>Streaming algorithms are algorithms for processing large data streams, using
only a limited amount of memory. Classical streaming algorithms operate under
the assumption that the input stream is fixed in advance. Recently, there is a
growing interest in studying streaming algorithms that provide provable
guarantees even when the input stream is chosen by an adaptive adversary. Such
streaming algorithms are said to be {\em adversarially-robust}. We propose a
novel framework for adversarial streaming that hybrids two recently suggested
frameworks by Hassidim et al. (2020) and by Woodruff and Zhou (2021). These
recently suggested frameworks rely on very different ideas, each with its own
strengths and weaknesses. We combine these two frameworks (in a non-trivial
way) into a single hybrid framework that gains from both approaches to obtain
superior performances for turnstile streams.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.14527"><span class="datestr">at August 02, 2021 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.14525">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.14525">The Minimum Edit Arborescence Problem and Its Use in Compressing Graph Collections [Extended Version]</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Lucas Gnecco, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Boria:Nicolas.html">Nicolas Boria</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bougleux:S=eacute=bastien.html">Sébastien Bougleux</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yger:Florian.html">Florian Yger</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blumenthal:David_B=.html">David B. Blumenthal</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.14525">PDF</a><br /><b>Abstract: </b>The inference of minimum spanning arborescences within a set of objects is a
general problem which translates into numerous application-specific
unsupervised learning tasks. We introduce a unified and generic structure
called edit arborescence that relies on edit paths between data in a
collection, as well as the Min Edit Arborescence Problem, which asks for an
edit arborescence that minimizes the sum of costs of its inner edit paths.
Through the use of suitable cost functions, this generic framework allows to
model a variety of problems. In particular, we show that by introducing
encoding size preserving edit costs, it can be used as an efficient method for
compressing collections of labeled graphs. Experiments on various graph
datasets, with comparisons to standard compression tools, show the potential of
our method.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.14525"><span class="datestr">at August 02, 2021 10:51 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.14373">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.14373">Geometry-Aware Merge Tree Comparisons for Time-Varying Data with Interleaving Distances</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yan:Lin.html">Lin Yan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Masood:Talha_Bin.html">Talha Bin Masood</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rasheed:Farhan.html">Farhan Rasheed</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hotz:Ingrid.html">Ingrid Hotz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Bei.html">Bei Wang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.14373">PDF</a><br /><b>Abstract: </b>Merge trees, a type of topological descriptor, serve to identify and
summarize the topological characteristics associated with scalar fields. They
present a great potential for the analysis and visualization of time-varying
data. First, they give compressed and topology-preserving representations of
data instances. Second, their comparisons provide a basis for studying the
relations among data instances, such as their distributions, clusters,
outliers, and periodicities. A number of comparative measures have been
developed for merge trees. However, these measures are often computationally
expensive since they implicitly consider all possible correspondences between
critical points of the merge trees. In this paper, we perform geometry-aware
comparisons of merge trees using labeled interleaving distances. The main idea
is to decouple the computation of a comparative measure into two steps: a
labeling step that generates a correspondence between the critical points of
two merge trees, and a comparison step that computes distances between a pair
of labeled merge trees by encoding them as matrices. We show that our approach
is general, computationally efficient, and practically useful. Our general
framework makes it possible to integrate geometric information of the data
domain in the labeling process. At the same time, it reduces the computational
complexity since not all possible correspondences have to be considered. We
demonstrate via experiments that such geometry-aware merge tree comparisons
help to detect transitions, clusters, and periodicities of time-varying
datasets, as well as to diagnose and highlight the topological changes between
adjacent data instances.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.14373"><span class="datestr">at August 02, 2021 10:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.14323">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.14323">Improved Reconstruction of Random Geometric Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dani:Varsha.html">Varsha Dani</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/D=iacute=az:Josep.html">Josep Díaz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hayes:Thomas_P=.html">Thomas P. Hayes</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moore:Cristopher.html">Cristopher Moore</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.14323">PDF</a><br /><b>Abstract: </b>Embedding graphs in a geographical or latent space, i.e., inferring locations
for vertices in Euclidean space or on a smooth submanifold, is a common task in
network analysis, statistical inference, and graph visualization. We consider
the classic model of random geometric graphs where $n$ points are scattered
uniformly in a square of area $n$, and two points have an edge between them if
and only if their Euclidean distance is less than $r$. The reconstruction
problem then consists of inferring the vertex positions, up to symmetry, given
only the adjacency matrix of the resulting graph. We give an algorithm that, if
$r=n^\alpha$ for $\alpha &gt; 0$, with high probability reconstructs the vertex
positions with a maximum error of $O(n^\beta)$ where $\beta=1/2-(4/3)\alpha$,
until $\alpha \ge 3/8$ where $\beta=0$ and the error becomes $O(\sqrt{\log
n})$. This improves over earlier results, which were unable to reconstruct with
error less than $r$. Our method estimates Euclidean distances using a hybrid of
graph distances and short-range estimates based on the number of common
neighbors. We sketch proofs that our results also apply on the surface of a
sphere, and (with somewhat different exponents) in any fixed dimension.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.14323"><span class="datestr">at August 02, 2021 10:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.14234">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.14234">Contact detection between an ellipsoid and a combination of quadrics</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brozos=V=aacute=zquez:Miguel.html">Miguel Brozos-Vázquez</a>, M. J. Pereira-Sáez, A. B. Rodríguez-Raposo, M. J. Souto-Salorio, A. D. Tarrío-Tobar <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.14234">PDF</a><br /><b>Abstract: </b>We analyze the characteristic polynomial associated to an ellipsoid and
another quadric in the context of the contact detection problem. We obtain a
necessary and sufficient condition for an efficient method to detect contact.
This condition is a feature on the size and the shape of the quadrics and can
be checked directly from their parameters. Under this hypothesis, contact can
be noticed by means of discriminants of the characteristic polynomial.
Furthermore, relative positions can be classified through the sign of the
coefficients of this polynomial.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.14234"><span class="datestr">at August 02, 2021 10:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2107.14080">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2107.14080">Large N limit of the knapsack problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Mobolaji Williams <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2107.14080">PDF</a><br /><b>Abstract: </b>In the simplest formulation of the knapsack problem, one seeks to maximize
the total value of a collection of objects such that the total weight remains
below a certain limit. In this work, we move from computer science to physics
and formulate the knapsack problem as a statistical physics system and compute
the corresponding partition function. We approximate the result in the large
number limit and from this approximation develop a new algorithm for the
problem. We compare the performance of this algorithm to that of other
approximation algorithms, finding that the new algorithm is faster than most of
these approaches while still retaining high accuracy. From its speed and
accuracy relationship, we argue that the algorithm is a manifestation of a
greedy algorithm. We conclude by discussing ways to extend the formalism to
make its underlying heuristics more rigorous or to apply the approach to other
combinatorial optimization problems. In all, this work exists at the
intersection between computer science and statistical physics and represents a
new analytical approach to solving the problems in the former using methods of
the latter.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2107.14080"><span class="datestr">at August 02, 2021 10:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3764738535229441946">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/08/do-four-colors-suffice.html">Do Four Colors Suffice?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>(Guest Post by David Marcus)</p><p><i>Comment by Bill:</i> Haken and Appel proved that all planar maps are 4-colorable. Or did they? David Marcus emailed me that its not quite true and I asked him to post on it, so here it is. The meta point is that math can be very subtle.</p><p>And now <i>David Marcus's post:</i></p><p>Is the Four Color Map Theorem true?</p><p>It is commonly believed that the Four Color Map Theorem says that <i>four colors suffice to color a planar</i> <i>map</i>. While this is true for any map a non-mathematician would dream up, it is not true for maps a mathematician might dream up without some restriction on the regions that are allowed. This is shown in Hud Hudson's <a href="https://www.jstor.org/stable/pdf/3647828.pdf">Four Colors Do Not Suffice</a> which appeared in the American Math Monthly, Volume 110,  No. 5, May 2003, pages 417--423. </p><p>Hudson's article is written in a very entertaining style. I recommend that you read it. He constructs a map consisting of six regions R1,...,R6.  Each region is bounded and path connected. There is a line segment B that is  in the boundary of all six regions.  So, six colors are needed, since all six regions share a common boundary. The construction is similar to the topologist's sine curve. For each i , the union of Ri and B is not path connected. Hudson also shows that for any n, there is a map that requires at least n colors.</p><div style="clear: both; text-align: center;" class="separator"><a style="margin-left: 1em; margin-right: 1em;" href="https://1.bp.blogspot.com/-oSGhAkWvF3M/YQf6m1FWr0I/AAAAAAAB9lQ/6E21cZ9aCnsEU-bh2PNr9P26C5_lcW1sQCLcBGAsYHQ/s737/6%2Bcolors.png"><img src="https://1.bp.blogspot.com/-oSGhAkWvF3M/YQf6m1FWr0I/AAAAAAAB9lQ/6E21cZ9aCnsEU-bh2PNr9P26C5_lcW1sQCLcBGAsYHQ/s320/6%2Bcolors.png" border="0" width="320" height="222" /></a></div><br /><p>Hudson thus disproves the following statement:</p><div><div>1)  Four colors are sufficient to color any map drawn in the plane or on a sphere so that no two regions with a common boundary line are colored with the same color.</div><div><br /></div><div>Appel and Haken actually proved the following:</div><div><br /></div><div>2) Four colors are sufficient to color any planar graph so that no two vertices connected by an edge are colored with the same color.</div></div></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/08/do-four-colors-suffice.html"><span class="datestr">at August 01, 2021 09:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=21845">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2021/08/01/mathematical-news-to-cheer-you-up/">Mathematical news to cheer you up</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<h3><img width="592" alt="Anna-Kiesenhofer-Olympics-2021-930x620" src="https://gilkalai.files.wordpress.com/2021/08/anna-kiesenhofer-olympics-2021-930x620-1.webp" class="alignnone  wp-image-21856" height="395" /></h3>
<h3>1.  <span class="d2edcug0 hpfvmrgz qv66sw1b c1et5uql lr9zc1uh a8c37x1j keod5gw0 nxhoafnm aigsh9s9 d3f4x2em fe6kdd0r mau55g9w c8b282yb iv3no6db jq4qci2q a3bd9o3v knj5qynh oo9gr5id hzawbc8m" dir="auto">Anna Kiesenhofer, a PhD mathematician researching PDEs at Ecole Polytechnique Federale Lausanne (EPFL), won the gold medal in the women’s bicycle road race at the Olympics.</span></h3>
<p>Here are two trivia question: a) Which hero of a recent post over here is related to <span class="d2edcug0 hpfvmrgz qv66sw1b c1et5uql lr9zc1uh a8c37x1j keod5gw0 nxhoafnm aigsh9s9 d3f4x2em fe6kdd0r mau55g9w c8b282yb iv3no6db jq4qci2q a3bd9o3v knj5qynh oo9gr5id hzawbc8m" dir="auto">Kiesenhofer’s mathematical side? b) Name another famous connection between EPFL-mathematics and sport. <br /></span></p>
<h3>2. János Nagy and Péter Pál Pach <a href="https://arxiv.org/abs/2107.03956">proved the Alon-Jaeger-Tarsi conjecture via group ring identities</a></h3>
<blockquote>
<p><em>The abstract says it all: In this paper we resolve the Alon-Jaeger-Tarsi conjecture for sufficiently large primes. Namely, we show that for any finite field <span class="MathJax" id="MathJax-Element-1-Frame"><span class="math" id="MathJax-Span-1"><span class="mrow" id="MathJax-Span-2"><span class="texatom" id="MathJax-Span-3"><span class="mrow" id="MathJax-Span-4"><span class="mi" id="MathJax-Span-5">F</span></span></span></span></span></span> of size <span class="MathJax" id="MathJax-Element-2-Frame"><span class="math" id="MathJax-Span-6"><span class="mrow" id="MathJax-Span-7"><span class="mn" id="MathJax-Span-8">61</span><span class="mo" id="MathJax-Span-9">&lt;</span><span class="texatom" id="MathJax-Span-10"><span class="mrow" id="MathJax-Span-11"><span class="mo" id="MathJax-Span-12">|</span></span></span><span class="texatom" id="MathJax-Span-13"><span class="mrow" id="MathJax-Span-14"><span class="mi" id="MathJax-Span-15">F</span></span></span><span class="texatom" id="MathJax-Span-16"><span class="mrow" id="MathJax-Span-17"><span class="mo" id="MathJax-Span-18">|</span></span></span><span class="mo" id="MathJax-Span-19">≠</span><span class="mn" id="MathJax-Span-20">79</span></span></span></span> and any nonsingular matrix <span class="MathJax" id="MathJax-Element-3-Frame"><span class="math" id="MathJax-Span-21"><span class="mrow" id="MathJax-Span-22"><span class="mi" id="MathJax-Span-23">M</span></span></span></span> over <span class="MathJax" id="MathJax-Element-4-Frame"><span class="math" id="MathJax-Span-24"><span class="mrow" id="MathJax-Span-25"><span class="texatom" id="MathJax-Span-26"><span class="mrow" id="MathJax-Span-27"><span class="mi" id="MathJax-Span-28">F</span></span></span></span></span></span> there exists a vector <span class="MathJax" id="MathJax-Element-5-Frame"><span class="math" id="MathJax-Span-29"><span class="mrow" id="MathJax-Span-30"><span class="mi" id="MathJax-Span-31">x</span></span></span></span> such that neither <span class="MathJax" id="MathJax-Element-6-Frame"><span class="math" id="MathJax-Span-32"><span class="mrow" id="MathJax-Span-33"><span class="mi" id="MathJax-Span-34">x</span></span></span></span> nor <span class="MathJax" id="MathJax-Element-7-Frame"><span class="math" id="MathJax-Span-35"><span class="mrow" id="MathJax-Span-36"><span class="mi" id="MathJax-Span-37">A</span><span class="mi" id="MathJax-Span-38">x</span></span></span></span> has a 0 component.</em></p>
</blockquote>
<h3>3. Michael Simkin asymptotically solved <a href="https://arxiv.org/abs/2107.13460">the <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n" class="latex" />-queens problem!</a> (We mentioned this classic problem and earlier progress by Zur Luria<a href="https://gilkalai.wordpress.com/2018/05/10/zur-luria-on-the-n-queens-problem/"> in this post</a>.)</h3>
<blockquote>
<p><em>Abstract: The <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n" class="latex" />-queens problem is to determine <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal+Q%7D%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="{\mathcal Q}(n)" class="latex" />, the number of ways to place <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n" class="latex" /> mutually non-threatening queens on an <img src="https://s0.wp.com/latex.php?latex=n+%5Ctimes+n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n \times n" class="latex" /> board. We show that there exists a constant α=1.942±3×10<sup>-3</sup> such that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal+Q%7D%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="{\mathcal Q}(n)" class="latex" /><img src="https://s0.wp.com/latex.php?latex=%3D%28%281+%5Cpm+o%281%29%29n+e+%5E%7B-%5Calpha%7D%29%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="=((1 \pm o(1))n e ^{-\alpha})^n" class="latex" />. The constant α is characterized as the solution to a convex optimization problem in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal+P%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="{\mathcal P}" class="latex" />([−1/2,1/2]<sup>2</sup>), the space of Borel probability measures on the square. The chief innovation is the introduction of limit objects for <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n" class="latex" />-queens configurations, which we call “queenons”. These are a convex set in <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal+P&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\mathcal P" class="latex" />([−1/2,1/2]<sup>2</sup>). We define an entropy function that counts the number of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n" class="latex" />-queens configurations that approximate a given queenon. The upper bound uses the entropy method. For the lower bound we describe a randomized algorithm that constructs a configuration near a prespecified queen on and whose entropy matches that found in the upper bound. The enumeration of <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="n" class="latex" />-queens configurations is then obtained by maximizing the (concave) entropy function in the space of queenons. Along the way we prove a large deviations principle for n-queens configurations that can be used to study their typical structure.</em></p>
</blockquote>
<p><img width="1600" alt="sun" src="https://gilkalai.files.wordpress.com/2021/08/sun.jpeg" class="alignnone size-full wp-image-21875" height="1200" /></p>
<p>Intermission: the sun over Tel Aviv sea</p>
<h3 class="title mathjax">4. Boris Bukh demonstrated <a href="https://arxiv.org/abs/2107.04167">Extremal graphs without exponentially-small bicliques</a></h3>
<blockquote>
<p><em>Abstract: The Turán problem asks for the largest number of edges in an $latex <span class="MathJax" id="MathJax-Element-1-Frame"><span class="math" id="MathJax-Span-1"><span class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3">n$</span></span></span></span>-vertex graph not containing a fixed forbidden subgraph <span class="MathJax" id="MathJax-Element-2-Frame"><span class="math" id="MathJax-Span-4"><span class="mrow" id="MathJax-Span-5"><span class="mi" id="MathJax-Span-6">F</span></span></span></span>. We construct a new family of graphs not containing<span class="MathJax" id="MathJax-Element-3-Frame"><span class="math" id="MathJax-Span-7"><span class="mrow" id="MathJax-Span-8"><span class="msubsup" id="MathJax-Span-9"><span class="mi" id="MathJax-Span-10"></span><span class="texatom" id="MathJax-Span-11"><span class="mrow" id="MathJax-Span-12"><span class="mi" id="MathJax-Span-13"></span><span class="mo" id="MathJax-Span-14"></span><span class="mi" id="MathJax-Span-15"> <img src="https://s0.wp.com/latex.php?latex=K_%7Bs%2Ct%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="K_{s,t}" class="latex" /></span></span></span></span></span></span></span>, for <span class="MathJax" id="MathJax-Element-4-Frame"><span class="math" id="MathJax-Span-16"><span class="mrow" id="MathJax-Span-17"><span class="mi" id="MathJax-Span-18"></span><span class="mo" id="MathJax-Span-19"></span><span class="msubsup" id="MathJax-Span-20"><span class="mi" id="MathJax-Span-21"></span><span class="mi" id="MathJax-Span-22"><img src="https://s0.wp.com/latex.php?latex=t%3DC%5Es&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="t=C^s" class="latex" /></span></span></span></span></span>, with  <img src="https://s0.wp.com/latex.php?latex=%5COmega+%28n%5E%7B2-1%2Fs%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\Omega (n^{2-1/s})" class="latex" /> edges matching the upper bound of Kövári, Sós and Turán. </em></p>
</blockquote>
<h3 class="title mathjax">5. Michael Capalbo, <a href="https://link.springer.com/content/pdf/10.1007/s00493-020-3989-0.pdf">Explicit 𝑁-vertex graphs with maximum degree 𝐾 and diameter [1+𝑜(1)]log<sub>𝐾-1</sub> 𝑁 for each 𝐾-1 a prime power</a>,</h3>
<blockquote>
<p><em>Abstract:   Here we first present the solution of a long-standing open question–the explicit construction of an infinite family of N-vertex cubic graphs that have diameter [1+o(1)]log<sub>2</sub> N. We then extend the techniques to construct, for each K of the form 2<sup>s</sup>+1 or K=p<sup>s</sup>+1; s an integer and p a prime, an infinite family of K-regular graphs on N vertices with diameter [1+o(1)]log<sub>K−1</sub> N.</em></p>
</blockquote>
<p>I missed this breakthrough in STOC 2019 but now it appeared in Combinatorica, and Nati told me about it.</p>
<h3 class="title mathjax">6.  A beautiful survey article: <a href="https://arxiv.org/abs/2107.06371">Intersection Problems in Extremal Combinatorics: Theorems, Techniques and Questions Old and New</a>, by David Ellis,</h3>
<blockquote>
<p><em>Abstract: The study of intersection problems in Extremal Combinatorics dates back perhaps to 1938, when Paul Erdős, Chao Ko and Richard Rado proved the (first) `Erdős-Ko-Rado theorem’ on the maximum possible size of an intersecting family of <span class="MathJax" id="MathJax-Element-1-Frame"><span class="math" id="MathJax-Span-1"><span class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3">k</span></span></span></span>-element subsets of a finite set. Since then, a plethora of results of a similar flavour have been proved, for a range of different mathematical structures, using a wide variety of different methods. Structures studied in this context have included families of vector subspaces, families of graphs, subsets of finite groups with given group actions, and of course uniform hypergraphs with stronger or weaker intersection conditions imposed. The methods used have included purely combinatorial ones such as shifting/compressions, algebraic methods (including linear-algebraic, Fourier analytic and representation-theoretic), and more recently, analytic, probabilistic and regularity-type methods. As well as being natural problems in their own right, intersection problems have connections with many other parts of Combinatorics and with Theoretical Computer Science (and indeed with many other parts of Mathematics), both through the results themselves, and the methods used. In this survey paper, we discuss both old and new results (and both old and new methods), in the field of intersection problems. Many interesting open problems remain; we will discuss several. For expositional and pedagogical purposes, we also take this opportunity to give slightly streamlined versions of proofs (due to others) of several classical results in the area. This survey is intended to be useful to PhD students, as well as to more established researchers. It is a personal perspective on the field, and is not intended to be exhaustive; we apologise for any omissions. It is an expanded version of a paper that will appear in the Proceedings of the 29th British Combinatorial Conference.</em></p>
</blockquote>
<h3 class="title mathjax">7.  <a href="http://ecajournal.haifa.ac.il/Volume2022/ECA2022_S3I7.pdf">An interview with me</a>; interviewer Toufik Mansour.</h3>
<p> </p>
<div class="authors"> </div>
<div> </div>


<p></p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2021/08/01/mathematical-news-to-cheer-you-up/"><span class="datestr">at August 01, 2021 07:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/114">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/114">TR21-114 |  The Space Complexity of Sum Labelling | 

	Henning Fernau, 

	Kshitij Gajjar</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A graph is called a sum graph if its vertices can be labelled by distinct positive integers such that there is an edge between two vertices if and only if the sum of their labels is the label of another vertex of the graph. Most papers on sum graphs consider combinatorial questions like the minimum number of isolated vertices that need to be added to a given graph to make it a sum graph. In this paper, we initiate the study of sum graphs from the viewpoint of computational complexity. Notice that every $n$-vertex sum graph can be represented by a sorted list of $n$ positive integers where edge queries can be answered in $O(\log n)$ time. Therefore, limiting the size of the vertex labels upper-bounds the space complexity of storing the graph in the database.

We show that every $n$-vertex, $m$-edge, $d$-degenerate graph can be made a sum graph by adding at most $m$ isolated vertices to it, such that the size of each vertex label is at most $O(n^2d)$. This enables us to store the graph using $O(m\log n)$ bits of memory. For sparse graphs (graphs with $O(n)$ edges), this matches the trivial lower bound of $\Omega(n\log n)$. As planar graphs and forests have constant degeneracy, our result implies an upper bound of $O(n^2)$ on their label size. The previously best known upper bound on the label size of general graphs with the minimum number of isolated vertices was $O(4^n)$, due to Kratochvil, Miller &amp; Nguyen (2001). Furthermore, their proof was existential, whereas our labelling can be constructed in polynomial time.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/114"><span class="datestr">at August 01, 2021 07:55 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/113">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/113">TR21-113 |  Tight Bounds for the Randomized and Quantum Communication Complexities of Equality with Small Error | 

	Nikhil Mande, 

	Ronald de Wolf</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We investigate the randomized and quantum communication complexities of the well-studied Equality function with small error probability $\epsilon$, getting the optimal constant factors in the leading terms in a number of different models.

The following are our results in the randomized model:

1) We give a general technique to convert public-coin protocols to private-coin protocols by incurring a small multiplicative error at a small additive cost. This is an improvement over Newman's theorem [Inf. Proc. Let.'91] in the dependence on the error parameter.

2) As a consequence we obtain a $(\log(n/\epsilon^2) + 4)$-cost private-coin communication protocol that computes the $n$-bit Equality function, to error $\epsilon$. This improves upon the $\log(n/\epsilon^3) + O(1)$ upper bound implied by Newman's theorem, and matches the best known lower bound, which follows from Alon [Comb. Prob. Comput.'09], up to an additive $\log\log(1/\epsilon) + O(1)$.

The following are our results in the quantum model:

1) We exhibit a one-way protocol with $\log(n/\epsilon) + 4$ qubits of communication, that uses only pure states and computes the $n$-bit Equality function to error $\epsilon$. This bound was implicitly already shown by Nayak [PhD thesis'99]. 

2) We give a near-matching lower bound, showing that any $\epsilon$-error one-way protocol for $n$-bit Equality that uses only pure states communicates at least $\log(n/\epsilon) - \log\log(1/\epsilon) - O(1)$ qubits.

3) We exhibit a one-way protocol with $\log(\sqrt{n}/\epsilon) + 3$ qubits of communication, that uses mixed states and computes the $n$-bit Equality function to error $\epsilon$. This is also tight up to an additive $\log\log(1/\epsilon) + O(1)$, which follows from Alon's result.

Our upper bounds also yield upper bounds on the approximate rank, approximate nonnegative-rank, and approximate psd-rank of the Identity matrix. As a consequence we also obtain improved upper bounds on these measures for the distributed SINK function, which was recently used to refute the randomized and quantum versions of the log-rank conjecture (Chattopadhyay, Mande and Sherif [J. ACM'20], Sinha and de Wolf [FOCS'19], Anshu, Boddu and Touchette [FOCS'19]).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/113"><span class="datestr">at August 01, 2021 06:31 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/07/31/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/07/31/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p>Sariel Har-Peled demonstrates how to make animated pdfs in beamer talk slides and present them on Linux using okular (<a href="https://mastodon.social/@sarielhp/106585915134108096">\(\mathbb{M}\)</a>): <a href="https://www.youtube.com/watch?v=-fBREHIdTLI">YouTube demo</a>; <a href="http://sarielhp.org/misc/blog/21/07/15/bemaer_example.zip">beamer source code</a>. On OS X, the same technique works when presenting using Acrobat, but not Preview.</p>
  </li>
  <li>
    <p><a href="https://www.laphamsquarterly.org/roundtable/evidence-elements">Evidence of the Elements: Finding Euclid on scattered pot shards</a> (<a href="https://mathstodon.xyz/@11011110/106602699662553907">\(\mathbb{M}\)</a>, <a href="https://www.metafilter.com/192071/Euclidean-Cover-Bands-of-the-Ancient-World">via</a>). Excerpt from Benjamin Wardhaugh’s new book <em>Encounters with Euclid: How an Ancient Greek Geometry Text Shaped the World</em>.</p>
  </li>
  <li>
    <p><a href="https://blogs.ams.org/beyondreviews/2021/07/18/yoshimura-crush-patterns/">Yoshimura Crush Patterns</a> (<a href="https://mathstodon.xyz/@11011110/106604389376060790">\(\mathbb{M}\)</a>). See also <a href="https://en.wikipedia.org/wiki/Yoshimura_buckling">Yoshimura buckling on Wikipedia</a>, which repeats Robert Lang’s observation that these patterns can be seen on Mona Lisa’s sleeves.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/MonaLisa.jpg" style="border-style: solid; border-color: black;" alt="Mona Lisa" /></p>
  </li>
  <li>
    <p>Two more new Wikipedia Good Articles (<a href="https://mathstodon.xyz/@11011110/106614589027257857">\(\mathbb{M}\)</a>):</p>

    <ul>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Cairo_pentagonal_tiling">Cairo pentagonal tiling</a>, a tiling of the plane by congruent but irregular pentagons, formed by overlaying two hexagonal tilings. It appears in street pavings, crystal structures, and the art of M. C. Escher.</p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Halin_graph">Halin graphs</a>, the planar graphs formed from trees by connecting their leaves into a cycle. Studied by Kirkman long before Halin and significant in graph algorithms because of their low treewidth.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=O4UpNSlzKAM">Building a trivalent graph of harmonic relations among major and minor triads</a> (<a href="https://mathstodon.xyz/@jsiehler/106619540053169137">\(\mathbb{M}\)</a>). The graph has a vertex for each major or minor chord and an edge when you can get from one chord to another by changing a single note. It has 24 vertices (12 bottom notes of chords in 12-TET tuning, and two chords per note). It is vertex-transitive: shifting major or minor chords up or down the scale doesn’t change their adjacency patterns, and reversing the scale swaps major for minor. But it is not the <a href="https://en.wikipedia.org/wiki/Nauru_graph">Nauru graph</a> because it is not edge-transitive: you can walk up or down the scale by removing the bottom note from a chord and adding a new top note, and this walk forms a 24-cycle, but these edges are different from the ones where you change a major chord into a minor or vice versa by changing the middle note. The resulting graph is the one with <a href="https://en.wikipedia.org/wiki/LCF_notation">LCF notation</a> \([7,-7]^{12}\), shown below.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/triads.svg" alt="Graph of major and minor chords, with edges for one-note changes" /></p>
  </li>
  <li>
    <p>My 5-year-old Macbook Pro decided to start shutting down the screen if you open it too wide (<a href="https://mathstodon.xyz/@11011110/106626560960499738">\(\mathbb{M}\)</a>), so I had to replace it. New hardware is nice but I had been holding off on several versions of OS updates and the disruption in my software setup was a bit of a pain. The biggest issue: losing all my old paid-for non-subscription copies of Adobe software, because they won’t run on the new OS. Fortunately my campus has a subscription, for now.</p>
  </li>
  <li>
    <p>If you modify the sieve of Eratosthenes so that each generated number \(p\) knocks out the numbers \(pn+2\) instead of the usual \(pn\), you get <a href="https://oeis.org/A076974">the prime-like sequence 2, 3, 7, 13, 19, 25, 31, 39, 43, 49, 55, 61, 69, …</a> (<a href="https://mathstodon.xyz/@11011110/106632015633732162">\(\mathbb{M}\)</a>). Although many non-primes are in this sequence, and many primes are not, Bill McEachen has observed that with one exception the larger prime in every twin prime pair is part of this sequence! The proof is not difficult; see the OEIS link for spoilers.</p>
  </li>
  <li>
    <p><a href="https://www.ams.org/journals/notices/202107/rnoti-p1106.pdf">Henry Segerman on rolling acrobatic apparatus in the <em>Notices</em></a> (<a href="https://mathstodon.xyz/@henryseg/106631292717611856">\(\mathbb{M}\)</a>). Unfortunately he missed MOMIX dancer Alan Boeding’s work in this area from the late 1970s and early 1980s.</p>
  </li>
  <li>
    <p><a href="https://cp4space.hatsya.com/2021/07/20/hamming-cube-of-primes/">Hamming cube of primes</a> (<a href="https://mathstodon.xyz/@11011110/106644162088249713">\(\mathbb{M}\)</a>). Make an infinite graph whose vertices are the binary representations of prime numbers and whose edges represent flipping a single bit of this representation. (For instance, 2 and 3 are neighbors.) Surprisingly, it is not connected! 2131099 has no neighbors. See also <a href="https://mathoverflow.net/q/363083/440">the same question on MathOverflow, a year ago</a>, and a <a href="https://www.youtube.com/watch?v=p3Khnx0lUDE">new Matt Parker video on the same concept in decimal</a>.</p>
  </li>
  <li>
    <p><a href="https://boingboing.net/2021/07/20/a-person-in-a-dream-co-authored-a-math-paper.html">A dead mathematician co-authored a paper after appearing in a dream</a> (<a href="https://mathstodon.xyz/@11011110/106651389132857850">\(\mathbb{M}\)</a>). The paper is “Higher algebraic <span style="white-space: nowrap;">\(K\)-theory</span> of schemes and of derived categories” by Robert Wayne Thomason and Thomas Trobaugh (2007), <a href="https://doi.org/10.1007/978-0-8176-4576-2_10">doi:10.1007/978-0-8176-4576-2_10</a>, <a href="https://www.ams.org/mathscinet-getitem?mr=1106918">MR1106918</a>. It appears to have been quite an influential one; the MR review calls it a landmark, and it has over 1000 citations on Google Scholar.</p>
  </li>
  <li>
    <p><a href="https://doi.org/10.1073/pnas.2103605118">Imperfect comb construction reveals the architectural abilities of honeybees</a> (<a href="https://mathstodon.xyz/@11011110/106657012385182296">\(\mathbb{M}\)</a>, <a href="https://arstechnica.com/science/2021/07/mergers-twists-and-pentagons-the-architecture-of-honeycombs/">via</a>). How do bees cope with making hexagonal honeycombs when some kinds of cells have different sizes and some patches of honeycomb don’t align when they come close to first meeting up? Answer appears to be: they see the problems coming and accommodate them gradually by intermediate variations in size and degree of cells.</p>
  </li>
  <li>
    <p><a href="https://publicdomainreview.org/collection/solid-objects">Solid Objects: 16th-Century Geometric and Perspective Drawings from the Herzog August Bibliothek in Wolfenbüttel</a> (<a href="https://mathstodon.xyz/@11011110/106662683881820595">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=27896003">via</a>).</p>
  </li>
  <li>
    <p>You remember that simple fruit pictogram equation with the ridiculously complicated answer (<a href="https://mathstodon.xyz/@11011110/106668276488591830">\(\mathbb{M}\)</a>)? David Roberts has <a href="https://thehighergeometer.wordpress.com/2021/07/27/diophantine-fruit/">another one in the same style for which we don’t even know the answer</a>.</p>
  </li>
  <li>
    <p><a href="https://www.scottaaronson.com/blog/?p=5661">Scott Aaronson takes a break from quantum supremacy to tell us about busy beavers</a> (<a href="https://mathstodon.xyz/@11011110/106674016557738368">\(\mathbb{M}\)</a>). These are Turing Machines that take as long as possible to do stuff. “As long as possible” is an explosively-quickly growing function of the number of states, but the gist of the post is that the “do stuff” part can be defined in various ways, some of which make the explosion happen earlier than others.</p>
  </li>
  <li>
    <p>Antoine Chambert-Loir posts a nice photo of <a href="https://mathstodon.xyz/@antoinechambertloir/106657956305987782">multicolored foam on the surface of a cup of coffee</a>.</p>
  </li>
  <li>
    <p><a href="https://annals.math.princeton.edu/2021/193-3/p03">A new paper by Asperó and Schindler</a> (<a href="https://mathstodon.xyz/@11011110/106678194765282364">\(\mathbb{M}\)</a>) argues that principles of maximal forcing, unified in their paper, provide natural models for set theory in which many natural questions that are independent of ZF have clear answers. For instance, in these models, there are \(\aleph_2\) real numbers, not \(\aleph_1\). I got to this via <a href="https://www.quantamagazine.org/how-many-numbers-exist-infinity-proof-moves-math-closer-to-an-answer-20210715/">a popularized treatment in <em>Quanta</em></a>, but I think the introduction of the paper is quite readable. (The rest is not.)</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/07/31/linkage.html"><span class="datestr">at July 31, 2021 06:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18993">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/07/30/pandemic-lag/">Pandemic Lag</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>In chess ratings and what other measures of cognitive development?</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/07/30/pandemic-lag/didonbook/" rel="attachment wp-att-18995"><img width="160" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/07/DidonBook.jpg?resize=160%2C222&amp;ssl=1" class="alignright wp-image-18995" height="222" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><a href="https://www.amazon.com/Influence-morale-sports-athl%C3%A9tiques-French-ebook/dp/B00YOIOKQY">src</a></font></td>
</tr>
</tbody>
</table>
<p>
Henri Didon was a French priest and promoter of youth sports in the late 1800s. He coined the phrase <em>Citius, Altius, Fortius</em>, meaning <em>faster-higher-stronger</em>, which became the motto of the Olympic Games between their reinception in 1896 and its proclamation when the Games were held in Paris in 1924. For the 2020 Games being held now in 2021 they have added the word <em>Communiter</em>, meaning <em>together</em>, which is said to express solidarity during the pandemic. </p>
<p>
Today we review how the official measure of being faster, higher, and stronger at chess has been impacted by the pandemic. </p>
<p>
Didon spoke of his words as “the foundation and <em>raison d’être</em> of athletics” amid the progress of humanity. They have been borne out by the steady progression of athletic records over the Games’ 125-year history. Whether the Tokyo Games will continue that trend is open. Besides the year delay and the pandemic’s impact on qualifying competitions and athletic conditioning in general, there has emerged a question of mental effects amid the lack of spectators and straitened atmosphere. The one example I’ll quote is the <a href="https://www.huffpost.com/entry/kristof-milak-tokyo-olympics_n_61013f01e4b00fa7af7db9fb">claim</a> by the Hungarian swimmer Kristof Milak that a pre-race mishap with his favorite swimming trunks cost him a record in an event he still won:</p>
<blockquote><p><b> </b> <em> “They split 10 minutes before I entered the pool and in that moment I knew the world record was gone. I lost my focus and knew I couldn’t do it.” </em>
</p></blockquote>
<p>
At least the means of measuring athletic performances have not been disrupted. For <b>psychometrics</b>—a word <a href="https://www.morgan.edu/psychology/psychometrics">meaning</a> <em>the science of measuring mental capacities and processes</em>—the standardized tests most often used to measure aptitude have themselves been curtailed. This makes all the more open the question of how our youth have progressed during the pandemic in education on the whole. We will examine the special case of chess, where the official instrument has been almost entirely frozen for 15 months, but my own work carries both the ability and the responsibility to make up the difference.</p>
<p>
</p><p></p><h2> Chess Ratings and Lag </h2><p></p>
<p></p><p>
The <a href="https://en.wikipedia.org/wiki/Elo_rating_system">Elo</a> rating system is simple but accurate enough for use by sporting federations besides chess. In chess, 1000 is a typical rating for a novice player, 1600 means a good club player, 2200 is the threshold for “master,” and 2800 is world championship standard. A player’s rating measures skill in a way that the <em>difference</em> to the opponent’s rating yields probabilities by which to predict the outcomes of games between them. Elo is the main prediction engine of <a href="https://fivethirtyeight.com/">FiveThirtyEight</a> for <a href="https://fivethirtyeight.com/features/how-we-calculate-nba-elo-ratings/">basketball</a>, <a href="https://projects.fivethirtyeight.com/complete-history-of-mlb/">baseball</a>, and <a href="https://fivethirtyeight.com/methodology/how-our-nfl-predictions-work/">football</a> (but not <a href="https://fivethirtyeight.com/methodology/how-our-club-soccer-predictions-work/">soccer</a>). </p>
<p>
Although the prediction formula uses only differences, so that an additive shift in all ratings would not affect the chances, I have shown that the ratings administered by the International Chess Federation (FIDE) have stayed stable in absolute regard to the objective quality of moves played as measured by my own predictive model, via my Intrinsic Performance Ratings (IPRs) geared to the FIDE rating scale. Having stable numbers is vital not only to my cheating tests but to the public understanding of the system on the whole.  This goes for FIDE, for Internet gaming federations, and even for the use of Elo by <a href="https://web.archive.org/web/20170819190821/https://killscreen.com/articles/tinder-matchmaking-is-more-like-warcraft-than-you-might-think/">Tinder</a>. </p>
<p>
Thus it is all the more sad for me to see things like this happen not only to FIDE’s Elo ratings but also those of the US Chess Federation (USCF), who adopted Arpad Elo’s formulas in the 1950s:</p>
<p></p><p><br />
<a href="https://rjlipton.wpcomstaging.com/2021/07/30/pandemic-lag/anniewangratinggraphann2/" rel="attachment wp-att-19013"><img width="527" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/07/AnnieWangRatingGraphann2.jpg?resize=527%2C256&amp;ssl=1" class="aligncenter wp-image-19013" height="256" /></a></p>
<p></p><p><br />
This is the FIDE Rating Progress <a href="https://ratings.fide.com/profile/2053900/chart">Chart</a> of Annie Wang, who just won the US Junior Women’s Championship played in-person at the <a href="https://saintlouischessclub.org/">Saint Louis Chess Club</a> last week. Her FIDE rating has been stuck at <b>2384</b> ever since the April 2020 rating list. One glance at the chart suffices to project her rating into the neighborhood of 2500 by now. Her USCF rating is closer at 2457, but this is offset by a long-known inflation of USCF ratings relative to FIDE, <a href="https://chessgoals.com/rating-comparison-old/">measured</a> about 75 points at that level in May 2020. Wang’s USCF rating has been similarly frozen. You can find the same for a plethora of young players down to aspiring kids of single-digit age blasting out of three-digit ratings, as Wang did.  They have a flat line like the ones circled in blue, but located where she had a sharp rise (circled in green):</p>
<p></p><p><br />
<a href="https://rjlipton.wpcomstaging.com/2021/07/30/pandemic-lag/anniewangratinguscfann/" rel="attachment wp-att-18998"><img width="460" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/07/AnnieWangRatingUSCFann.jpg?resize=460%2C280&amp;ssl=1" class="aligncenter wp-image-18998" height="280" /></a></p>
<p></p><h2> The Need to Adjust </h2><p></p>
<p></p><p>
The lag mattered immediately for me as I gave daily statistical reports to the tournament’s chief arbiter last week. Using Wang’s official rating would have underestimated her true strength and biased my reports in the direction of false positives. Instead, having developed a formula that I won’t claim is anything more than <a href="https://en.wikipedia.org/wiki/Fermi_problem">Fermi</a>–<a href="https://brilliant.org/wiki/fermi-estimate/">estimated</a>, I calculated her effective FIDE rating as <b>2482</b>, adding almost 100 points. I would have upped her USCF rating to 2543 by the same formula. </p>
<p>
Wang was both the highest rated among the ten competitors and the oldest, with a long enough record of international play to have her FIDE <a href="https://en.wikipedia.org/wiki/Elo_rating_system#Most_accurate_K-factor">K-factor</a> reduced from 40 to 20. My formula adds more points for lower ratings, higher K-factor, and younger age—all reflecting the arc of many improving junior players. My average increase to the women’s ratings was <b>199.1</b> points, versus <b>57.4</b> to the ten players in the junior men’s/mixed championship, who had mostly higher ratings to begin with. </p>
<p>
Also playing in St. Louis were ten in the US Senior Championship, including last year’s winner Joel Benjamin, whom I knew and played in the 1970s when we were kids. Their ratings have been likewise frozen. Rating points in chess are zero-sum, so the triple-digit gains I have credited to the young would in normal reality have been taken out of other players—most plausibly, us geezers. There are more of us than keen juniors, so the presumed individual losses would be less. </p>
<p>
Did that prove out? My IPRs furnish a way to verify. They differ from other deployed quality metrics by organically involving the difficulty of the positions a player faces, in several ways besides the complexity and temptation factors I <a href="https://rjlipton.wpcomstaging.com/2019/08/15/predicting-chess-and-horses/">incorporated</a> two years ago. Here are the results—but bear in mind that these three 10-player tournaments are small data: their two-sigma error bars on the average IPRs are about <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+80%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\pm 80}" class="latex" /> Elo points.</p>
<ul>
<li>
<b>US Jr. W</b>: Avg. rating 2101, adjusted <b>2300</b>, avg. IPR <b>2337</b> (+37). <p></p>
</li><li>
<b>US Jr. M</b>: Avg. rating 2492, adjusted <b>2550</b>, avg. IPR <b>2527</b> (-23). <p></p>
</li><li>
<b>US Sr. M</b>: Avg. rating <b>2494</b> (no adjustment), avg. IPR <b>2459</b> (-35).
</li></ul>
<p>
The truly significant result is that the women performed much closer to my adjustment than to their official ratings. The men were only slightly closer amid general insignificance, which applies also to the seniors. The juniors combined were highly close to my projections. </p>
<p>
Right now I am gathering data from larger Open tournaments in this first month of widespread in-person play. There have been some hits and misses, and I have not yet evaluated all (un-)controllable factors. But gathering the original large data for my adjustment formula required coping with a major factor: the 100–200x higher evident cheating rate I’ve observed in online chess.</p>
<p>
</p><p></p><h2> How To Be Not Very Wrong </h2><p></p>
<p></p><p>
I first perceived the phenomenon when monitoring the European Youth Online Rapid Chess Championship last September. I compiled full analysis on all 689 competitors in women’s and men’s/mixed sections ranging from Under-12 to Under-18. Besides four particular cases, my results said that probably at least four of another five were cheating, but without the confidence needed to flag any one. Removing the high outliers did not, however, equate either the IPRs or my sharper test of conformance to the bell curve to my projections. The Under-12 M and W and Under-14 M sections had IPRs averaging 83, 235, and 125 higher, respectively. The Under-14 W and U16 and U18 sections were close to my projections, so I did not suspect general modeling issues. </p>
<p>
The online World Youth Rapid Championships in November-December, which added an under-10 division, brought the lag phenomenon out in force, on all continents. The correction I postulated even before that tournament finished was:</p>
<blockquote><p><b> </b> <em> 15 Elo <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctimes%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\times}" class="latex" /> (months since April 2020), higher for those under 13 (50% to 2x higher). </em>
</p></blockquote>
<p></p><p>
There are several reasons I have not tried to be more precise. There is uncertainty about how many high outliers to remove, about faster time controls, and about <a href="https://en.chessbase.com/post/why-do-some-countries-always-gain-and-other-always-lose-rating-points">geographical</a> drifts in ratings. The effect depends on how much a junior player is disposed to improve in the first place; I found it absent in the lower divisions of the UK’s junior leagues played online last winter. In an individual cheating case I take a more-particular fix on the appropriate rating. What the equation is for is <em>to show the fairness of my baseline relative to the field on the whole</em>. There are also non-cheating purposes, which should come to the fore as FIDE and other federations emerge from the pandemic, and which I discuss next. </p>
<p>
I have been using essentially this formula ever since. From large scholastic tournaments across the globe this spring, I settled on fixing the adjustment for those with birth year 2008 or later as 25 Elo <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctimes%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\times}" class="latex" /> (months since April 2020). For players with official rating <img src="https://s0.wp.com/latex.php?latex=%7BR+%3E+2000%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R &gt; 2000}" class="latex" /> I apply the rough multiplier <img src="https://s0.wp.com/latex.php?latex=%7B%283000+-+R%29%2F1000%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(3000 - R)/1000}" class="latex" />, and for those with <img src="https://s0.wp.com/latex.php?latex=%7BK+%3C+40%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{K &lt; 40}" class="latex" /> I (also) multiply by <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7BK%2F40%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sqrt{K/40}}" class="latex" />.</p>
<p>
I won’t claim the ’15’ and ’25’ are right, compared to multipliers that are 1 or 2 higher or lower. But the results I have been getting all year say that my 15 and 25 are most often closer than factors of 10 or 20 or 30 would be. In almost all cases, like for the US Jr. W above, my pre-set rating calibration has come an order of magnitude closer to the IPR verification than the adjustments themselves. Taking a cue from the title of Jordan Ellenberg’s <a href="https://en.wikipedia.org/wiki/How_Not_to_Be_Wrong">predecessor</a> to his book I <a href="https://rjlipton.wpcomstaging.com/2021/06/20/the-shape-of-this-summer/">previewed</a> last month, my main concern is to be not very wrong.</p>
<p>
</p><p></p><h2> A Dilemma Moving Forward </h2><p></p>
<p></p><p>
Providing an accurate and stable rating system has long been recognized as a prime service of FIDE. A legal dimension has been added insofar as evaluating cheating allegations requires a prior assessment of the natural skill of the accused player. The pandemic has made me take over much of the latter responsibility, but the former presents a wider dilemma doubtless faced in some form by other impacted sporting federations and educational assessment agencies on the whole:</p>
<blockquote><p><b> </b> <em> Is it a higher responsibility to provide the most accurate assessment of current ability obtainable now, or to maintain continuity of the official assessment mechanism? </em>
</p></blockquote>
<p></p><p>
I could go even wider to analogize this to the US Census debate over whether estimations, presuming demonstration of their greater accuracy, should be used in preference to the conducted count. The latter is enshrined in the US Constitution, while the principle that chess rating points should be won or lost only in actual combat is similarly <a href="https://handbook.fide.com/chapter/B022017">hallowed</a>. But I have certainly “demonstrated the obvious”: that the current official ratings of almost all the keenest young players are very wrong.</p>
<p>
Mathematically, the rating system <em>will</em> re-establish equilibrium if the current discrepancy is left alone. The trouble is that the mathematical nature of the update and the relative paucity of chess games also guarantees that the process will be <em>slow</em>, measured in years. FiveThirtyEight has remarked in <a href="https://fivethirtyeight.com/features/60-games-arent-enough-to-crown-the-best-mlb-team-but-neither-are-162-games/">several</a> <a href="https://fivethirtyeight.com/features/no-mlb-team-is-great-and-fewer-are-awful-is-this-the-parity-we-wanted/">recent</a> <a href="https://fivethirtyeight.com/features/bad-teams-may-be-posing-as-good-teams-in-a-60-game-baseball-season/">article</a> about the long update times in baseball as measured by Elo ratings. My cheating tests often cannot wait a day.  I have to use my cross-check and validation features to detect and remove a huge amount of mathematically the same kind of bias believed to afflict other currently-deployed predictive models less transparently.</p>
<p>
There is precedent for a large-scale adjustment of ratings by FIDE. Women’s chess used to be even more segregated from men than today. In 1986, Arpad Elo himself—as secretary of FIDE’s Qualifications Commission—<a href="http://www.anusha.com/elo.htm">reported</a> that women’s ratings had drifted down by about “one half of a class interval.” FIDE added 100 points to the rating of every active female player except Susan Polgar, whose rating was already ‘well-mixed’ according to the report, since she had faced many more male players than the others.</p>
<p>
Attempting to resolve that historical controversy by computing IPRs for Polgar and the other players in Elo’s study has never reached my front burner. But the point remains that my work is uniquely capable of informing the state of ratings in a radical manner. The pandemic has created both a need and an opportunity for a reset that could also solve other issues previously noted—while ensuring that ratings on all continents are on a common scale.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
How pronounced is the lag of assessment in education and other competitive arenas, both physical and in mind-sports?</p>
<p>
I had not noticed that Tyler Cowen had already used the term “psychometric test” in a <a href="https://marginalrevolution.com/marginalrevolution/2020/03/the-world-is-running-a-disturbing-psychometric-test.html">post</a> on the <em>Marginal Revolution</em> blog at the beginning of the pandemic, until he <a href="https://marginalrevolution.com/marginalrevolution/2021/07/the-great-psychometric-test-continues.html">repeated</a> it just today.</p>
<p>
I have hinted at some other issues in chess but stopped short of addressing them. One is whether online play—where play at 5-minute “Blitz” down to 1-minute “Bullet” time controls predominates even over “Rapid” beginning at 10 minutes—has a similar effect on development in the absence of any in-person “Classical” chess. Another is whether the observed increase in the ranks of players with 2700+ elite ratings is really <em>Fortius</em> or merely rating <em>inflation</em>. A third is whether the current conditions for in-person chess will last long enough to get a good fix on the ‘post-pandemic’ state of skill, and a fourth—coming back to what I quoted about the current Olympics—is whether they are truly “normal” enough even now.</p>
<p></p><p><br />
[changed first figure to show the March 2020 pandemic start accurately; some minor word changes.]</p></font></font></div>







<p class="date">
by KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/07/30/pandemic-lag/"><span class="datestr">at July 30, 2021 08:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=2324">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">Average-Case Fine-Grained Hardness, Part II</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>In the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a>, we did two examples of proving average-case fine-grained hardness via worst-case to average-case reductions. In this post, I want to continue the discussion of counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques (in particular, on <a href="https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93R%C3%A9nyi_model">Erdős–Rényi graphs</a>) to showcase a new technique, which builds on the general recipe and the example of counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques described in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a>. In the next post, I will discuss how this new technique can be applied to some other combinatorial problems.</p>



<p><strong>Counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in Erdős–Rényi graph.</strong> A strong follow-up <a href="https://arxiv.org/abs/1903.08247">[BBB19]</a> of the result <a href="http://www.wisdom.weizmann.ac.il/~oded/R2/gc.pdf">[GR18]</a> we discussed in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a> shows that there is an <img src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28n%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\widetilde{O}(n^2)" class="latex" />-time reduction from counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in any <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" />-vertex graph to counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques with error probability <img src="https://s0.wp.com/latex.php?latex=%3C%5Cfrac%7B1%7D%7B%5Clog%5E%7BO%281%29%7D+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="&lt;\frac{1}{\log^{O(1)} n}" class="latex" /> in Erdős–Rényi graph (whereas the sampable distribution of the random graph in <a href="http://www.wisdom.weizmann.ac.il/~oded/R2/gc.pdf">[GR18]</a> is somewhat unnatural). The key idea is a decomposition lemma which says for sufficiently large prime <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=k%3DO%28%5Clog%28p%29%5Clog%28p%2F%5Cvarepsilon%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k=O(\log(p)\log(p/\varepsilon))" class="latex" />, for any constants <img src="https://s0.wp.com/latex.php?latex=0%3Cp%5E%7B%281%29%7D%2C%5Cdots%2Cp%5E%7B%28k%29%7D%3C1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="0&lt;p^{(1)},\dots,p^{(k)}&lt;1" class="latex" /> (for our application, these constants will be equal), given independent Bernoulli random variables <img src="https://s0.wp.com/latex.php?latex=y_%7B%5Cell%7D%5Csim%5Ctextrm%7BBern%7D%28p%5E%7B%28%5Cell%29%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y_{\ell}\sim\textrm{Bern}(p^{(\ell)})" class="latex" />, the distribution of <img src="https://s0.wp.com/latex.php?latex=%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7Dy_%7B%5Cell%7D%5Cmod+p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\sum_{\ell=0}^k 2^{\ell}y_{\ell}\mod p" class="latex" /> is close to the uniform distribution over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BF%7D_%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{F}_{p}" class="latex" />, i.e., the statistical distance is less than <img src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\varepsilon" class="latex" /> (later when we apply this lemma, we want <img src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\varepsilon" class="latex" /> to be <img src="https://s0.wp.com/latex.php?latex=1%2F%5Ctextrm%7Bpoly%7D%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/\textrm{poly}(n)" class="latex" />, and therefore <img src="https://s0.wp.com/latex.php?latex=k%3DO%28%5Clog+p%5Ccdot%28%5Clog+p%2B%5Clog+n%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k=O(\log p\cdot(\log p+\log n))" class="latex" /> ). We skip the proof of this lemma which is a nice application of basic Fourier analysis.</p>



<p>As in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a>, <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> denotes a constructed polynomial that computes the number of <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques when the input is an adjacency matrix of a graph. The step 3 of the general recipe from the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a> reduces counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques for the worst-case graph to evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28Y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(Y)" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=d%2B1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d+1" class="latex" /> many uniformly random <img src="https://s0.wp.com/latex.php?latex=Y%5Cin%5Cmathbf%7BF%7D_%7Bp%7D%5E%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y\in\mathbf{F}_{p}^{n^2}" class="latex" /> (recall in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a>, <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" /> is the degree of the polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=O%28t%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(t\log n)" class="latex" /> after the Chinese remaindering trick). Based on the decomposition lemma, using standard sampling scheme (this is essentially <a href="https://en.wikipedia.org/wiki/Rejection_sampling">rejection sampling</a>, which I will not go into the details, but I just want to mention that we would like the <img src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\varepsilon" class="latex" /> in the decomposition lemma to be <img src="https://s0.wp.com/latex.php?latex=1%2F%5Ctextrm%7Bpoly%7D%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="1/\textrm{poly}(n)" class="latex" /> such that the sampling scheme succeeds w.h.p. by a few attempts), we can further reduce evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28Y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(Y)" class="latex" /> on uniformly random <img src="https://s0.wp.com/latex.php?latex=Y%5Cin%5Cmathbf%7BF%7D_%7Bp%7D%5E%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y\in\mathbf{F}_{p}^{n^2}" class="latex" /> to evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7D+Y_%7B%5Cell%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(\sum_{\ell=0}^k 2^{\ell} Y_{\ell})" class="latex" />, where each <img src="https://s0.wp.com/latex.php?latex=Y_%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y_{\ell}" class="latex" /> is a random 0-1 valued matrix that is statistically close to the adjacency matrix of an Erdős–Rényi graph. Now, if we can “pull out” the weighted sum in <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7D+Y_%7B%5Cell%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(\sum_{\ell=0}^k 2^{\ell} Y_{\ell})" class="latex" />, then we are done, because evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> on the adjacency matrix of an Erdős–Rényi graph is precisely counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques for Erdős–Rényi graph.</p>



<p>When can we “pull out” the weighted sum for a polynomial <img src="https://s0.wp.com/latex.php?latex=f%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7D+Y_%7B%5Cell%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(\sum_{\ell=0}^k 2^{\ell} Y_{\ell})" class="latex" />? One answer is when the polynomial is <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />-partite.</p>



<p>An <img src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="m" class="latex" />-variate polynomial <img src="https://s0.wp.com/latex.php?latex=f%28x_1%2C%5Cdots%2Cx_m%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(x_1,\dots,x_m)" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />-partite if there is a partition of the set of variables <img src="https://s0.wp.com/latex.php?latex=%5Cdot%7B%5Cbigcup%7D_%7Bj%5Cin%5Bd%5D%7D+S_j%3D%5Bm%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\dot{\bigcup}_{j\in[d]} S_j=[m]" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" /> is the sum of monomials in which each monomial contains exactly one variable from each part <img src="https://s0.wp.com/latex.php?latex=S_j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="S_j" class="latex" /> (more formally, <img src="https://s0.wp.com/latex.php?latex=f%28x_1%2C%5Cdots%2Cx_m%29%3D%5Csum_%7B%28i_1%2Ci_2%2C%5Cdots%2Ci_d%29%5Cin+S%7D%5Cprod_%7Bj%5Cin%5Bd%5D%7Dx_%7Bi_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(x_1,\dots,x_m)=\sum_{(i_1,i_2,\dots,i_d)\in S}\prod_{j\in[d]}x_{i_j}" class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=S%5Csubseteq+S_1%5Ctimes+S_2%5Ctimes%5Cdots%5Ctimes+S_d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="S\subseteq S_1\times S_2\times\dots\times S_d" class="latex" />).</p>



<p>For <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />-partite polynomial <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" />, it is not hard to show that<br /><img src="https://s0.wp.com/latex.php?latex=f%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7Dy_%7B1%2C%5Cell%7D%2C%5Cdots%2C%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7Dy_%7Bm%2C%5Cell%7D%29%3D%5Csum_%7B%5Cell_1%3D0%7D%5Ek%5Csum_%7B%5Cell_2%3D0%7D%5Ek%5Cdots%5Csum_%7B%5Cell_d%3D0%7D%5Ek+2%5E%7B%5Cell_1%2B%5Cdots%2B%5Cell_d%7D%5Ccdot+f%28y_%7B1%2C%5Cell_1%7D%2C%5Cdots%2Cy_%7Bm%2C%5Cell_m%7D%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(\sum_{\ell=0}^k 2^{\ell}y_{1,\ell},\dots,\sum_{\ell=0}^k 2^{\ell}y_{m,\ell})=\sum_{\ell_1=0}^k\sum_{\ell_2=0}^k\dots\sum_{\ell_d=0}^k 2^{\ell_1+\dots+\ell_d}\cdot f(y_{1,\ell_1},\dots,y_{m,\ell_m})." class="latex" /><br />(Essentially, because two variables from the same <img src="https://s0.wp.com/latex.php?latex=S_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="S_i" class="latex" /> never appear in the same monomial, we can enumerate variables from the same <img src="https://s0.wp.com/latex.php?latex=S_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="S_i" class="latex" /> in the same order.)</p>



<p>Let us think of each <img src="https://s0.wp.com/latex.php?latex=y_%7Bi%2C%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y_{i,\ell}" class="latex" /> as a coordinate of Erdős–Rényi adjacency matrix <img src="https://s0.wp.com/latex.php?latex=Y_%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y_{\ell}" class="latex" />, then <img src="https://s0.wp.com/latex.php?latex=f%28y_%7B1%2C%5Cell_1%7D%2C%5Cdots%2Cy_%7Bm%2C%5Cell_m%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(y_{1,\ell_1},\dots,y_{m,\ell_m})" class="latex" /> is evaluating <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f" class="latex" /> on an ensemble of distinct coordinates of <img src="https://s0.wp.com/latex.php?latex=Y_%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y_{\ell}" class="latex" />‘s, and such ensemble is obviously Erdős–Rényi as well. Therefore, we have managed to decompose <img src="https://s0.wp.com/latex.php?latex=f%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7D+Y_%7B%5Cell%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(\sum_{\ell=0}^k 2^{\ell} Y_{\ell})" class="latex" /> into sum of <img src="https://s0.wp.com/latex.php?latex=k%5Ed&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k^d" class="latex" /> many <img src="https://s0.wp.com/latex.php?latex=f%28Y%5E%7B%28%5Cell_1%2C%5Cdots%2C%5Cell_d%29%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f(Y^{(\ell_1,\dots,\ell_d)})" class="latex" />‘s where each <img src="https://s0.wp.com/latex.php?latex=Y%5E%7B%28%5Cell_1%2C%5Cdots%2C%5Cell_d%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y^{(\ell_1,\dots,\ell_d)}" class="latex" /> denotes an Erdős–Rényi adjacency matrix. In the next paragraph, we will construct a <img src="https://s0.wp.com/latex.php?latex=d%3D%5Cbinom%7Bt%7D%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d=\binom{t}{2}" class="latex" />-partite polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> for counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques, and therefore, we have reduced computing <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28%5Csum_%7B%5Cell%3D0%7D%5Ek+2%5E%7B%5Cell%7D+Y_%7B%5Cell%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(\sum_{\ell=0}^k 2^{\ell} Y_{\ell})" class="latex" /> to computing <img src="https://s0.wp.com/latex.php?latex=k%5E%7B%5Cbinom%7Bt%7D%7B2%7D%7D%3D%5Clog%5E%7BO%281%29%7D+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k^{\binom{t}{2}}=\log^{O(1)} n" class="latex" /> many <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D+%28Y%5E%7B%28%5Cell_1%2C%5Cdots%2C%5Cell_d%29%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}} (Y^{(\ell_1,\dots,\ell_d)})" class="latex" />‘s, which is a mild blow-up of the number of the random instances which the reduction needs to solve. (In general, we consider the reduction to be efficient when the number of random instances it needs to solve is <img src="https://s0.wp.com/latex.php?latex=n%5E%7Bo%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n^{o(1)}" class="latex" />, and hence, as long as <img src="https://s0.wp.com/latex.php?latex=d%3Do%28%5Clog+n%2F%5Clog+%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d=o(\log n/\log \log n)" class="latex" />, we are good to go.)</p>



<p>Unfortunately, the <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> given in the <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">previous post</a> does not work. Instead, we first reduce counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in any graph to counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in a <a href="https://en.wikipedia.org/wiki/Multipartite_graph"><img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-partite graph</a>, and then we construct a <img src="https://s0.wp.com/latex.php?latex=%5Cbinom%7Bt%7D%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\binom{t}{2}" class="latex" />-partite polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> for counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in a <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-partite graph. Reduction from counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in any graph to counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in a <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-partite graph is standard. Simply consider the <a href="https://en.wikipedia.org/wiki/Tensor_product_of_graphs">tensor product</a> between the original graph and another <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-clique. The number of <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in the tensor product graph is exactly <img src="https://s0.wp.com/latex.php?latex=t%21&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t!" class="latex" /> times that in the original graph. Now given the <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-paritite graph, let <img src="https://s0.wp.com/latex.php?latex=V_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="V_i" class="latex" /> denote the <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="i" class="latex" />-th part of vertices, and let <img src="https://s0.wp.com/latex.php?latex=X%5E%7B%28i%2Cj%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X^{(i,j)}" class="latex" /> (for <img src="https://s0.wp.com/latex.php?latex=i%3Cj&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="i&lt;j" class="latex" />) denote the adjacency matrix between <img src="https://s0.wp.com/latex.php?latex=V_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="V_i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=V_j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="V_j" class="latex" />. Consider the new polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28X%5E%7B%281%2C2%29%7D%2CX%5E%7B%281%2C3%29%7D%2C%5Cdots%2CX%5E%7B%28t-1%2Ct%29%7D%29%3A%3D%5Csum_%7Bv_1%5Cin+V_1%7D%5Csum_%7Bv_2%5Cin+V_2%7D%5Cdots%5Csum_%7Bv_t%5Cin+V_t%7D%5Cprod_%7B%28i%2Cj%29%5Cin%5Cbinom%7B%5Bt%5D%7D%7B2%7D%7D+X_%7Bv_i%2Cv_j%7D%5E%7B%28i%2Cj%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(X^{(1,2)},X^{(1,3)},\dots,X^{(t-1,t)}):=\sum_{v_1\in V_1}\sum_{v_2\in V_2}\dots\sum_{v_t\in V_t}\prod_{(i,j)\in\binom{[t]}{2}} X_{v_i,v_j}^{(i,j)}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=X_%7Bv_i%2Cv_j%7D%5E%7B%28i%2Cj%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X_{v_i,v_j}^{(i,j)}" class="latex" /> denotes the coordinate that indicates if there is an edge between <img src="https://s0.wp.com/latex.php?latex=v_i%2Cv_j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="v_i,v_j" class="latex" />. Observe that <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> counts <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques by picking one vertex for each part and checking if these <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" /> vertices form a clique. It is indeed <img src="https://s0.wp.com/latex.php?latex=%5Cbinom%7Bt%7D%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\binom{t}{2}" class="latex" />-partite as each <img src="https://s0.wp.com/latex.php?latex=X%5E%7B%28i%2Cj%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X^{(i,j)}" class="latex" /> corresponds to a part of variables.</p>



<p><strong>New recipe for worst-case to average-case reductions.</strong> Let us take a minute to think about what structural properties of counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques we have used in the entire reduction except for constructing <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" />.</p>



<p>The answer is none! The only part specific to counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques was cooking up a <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />-partite polynomial for <img src="https://s0.wp.com/latex.php?latex=d%3Do%28%5Clog+n%2F%5Clog+%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d=o(\log n/\log \log n)" class="latex" /> (let us call such polynomial “good”) that encodes this problem. The reduction we showed above works as long as we can construct such “good” polynomial for a problem. Therefore, a new recipe, which was explicitly formulated in <a href="https://arxiv.org/abs/2008.06591">[DLW20]</a> for proving average-case (here “average-case” means Erdős–Rényi random input model) fine-grained hardness for a problem <img src="https://s0.wp.com/latex.php?latex=L%3A%5C%7B0%2C1%5C%7D%5En%5Cto%5Cmathbf%7BZ%7D_%7B%5Cge+0%7D%5Ccap+%5Ctextrm%7Bpoly%7D%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L:\{0,1\}^n\to\mathbf{Z}_{\ge 0}\cap \textrm{poly}(n)" class="latex" /> in P, is </p>



<ol><li>Construct a “good” polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BF%7D_p%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{F}_p^n" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=p%3D%5Ctextrm%7Bpoly%7D%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p=\textrm{poly}(n)" class="latex" />, such that <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D%28x%29%3DL%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}(x)=L(x)" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=x%5Cin%5C%7B0%2C1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in\{0,1\}^n" class="latex" />.</li></ol>



<p>Short and sweet.</p>



<p>In the final post, I will present another instantiation of this new recipe.</p>



<p><strong>Acknowledgements.</strong> I would like to thank my quals committee — Aviad Rubinstein, Tselil Schramm, Li-Yang Tan for valuable feedback to my quals talk.</p></div>







<p class="date">
by Junyao Zhao <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/"><span class="datestr">at July 30, 2021 02:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/07/30/postdoc-at-ben-gurion-university-apply-by-december-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/07/30/postdoc-at-ben-gurion-university-apply-by-december-31-2021/">POSTDOC at Ben-Gurion University (apply by December 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>My group at Ben-Gurion University has an open postdoctoral position, part of an ERC Starting Grant project. Starting date, as well as duration, are flexible.<br />
The position includes a generous salary, as well as funding for equipment and travel.</p>
<p>Website: <a href="https://www.cs.bgu.ac.il/~klim/Links/Call">https://www.cs.bgu.ac.il/~klim/Links/Call</a><br />
Email: klim@bgu.ac.il</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/07/30/postdoc-at-ben-gurion-university-apply-by-december-31-2021/"><span class="datestr">at July 30, 2021 12:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/112">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/112">TR21-112 |  CNF Satisfiability in a Subspace and Related Problems | 

	Vikraman Arvind, 

	Venkatesan Guruswami</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We introduce the problem of finding a satisfying assignment to a CNF formula that must further belong to a prescribed input subspace. Equivalent formulations of the problem include finding a point outside a union of subspaces (the Union-of-Subspace Avoidance (USA) problem), and finding a common zero of a system of polynomials over the field of two elements each of which is a product of affine forms.
    
We focus on the case of $k$-CNF formulas (the $k$-SUB-SAT problem). Clearly, $k$-SUB-SAT is no easier than $k$-SAT, and might be harder. Indeed, via simple reductions we show that 2-SUB-SAT is NP-hard, and W[1]-hard when parameterized by the co-dimension of the subspace. We also prove that the optimization version Max-2-SUB-SAT is NP-hard to approximate better than the trivial $3/4$ ratio even on satisfiable instances.
    
On the algorithmic front, we investigate fast exponential algorithms which give non-trivial savings over brute-force algorithms. We give a simple branching algorithm with runtime $(1.5)^r$ for 2-SUB-SAT, where $r$ is the subspace dimension, as well as a $(1.4312)^n$ time algorithm where $n$ is the number of variables.

Turning to $k$-SUB-SAT for $k \ge 3$, while known algorithms for solving a system of degree $k$ polynomial equations already imply a solution with runtime $\approx 2^{r(1-1/2k)}$, we explore a more combinatorial approach.  Based on an analysis of critical variables (a key notion underlying the randomized $k$-SAT algorithm of Paturi, Pudlak, and Zane), we give an algorithm with runtime $\approx {n\choose {\le t}} 2^{n-n/k}$ where $n$ is the number of variables and $t$ is the co-dimension of the subspace. This improves upon the runtime of the polynomial equations approach for small co-dimension. Our combinatorial approach also achieves polynomial space in contrast to the algebraic approach that uses exponential space. We also give a PPZ-style algorithm for $k$-SUB-SAT with runtime $\approx 2^{n-n/2k}$. This algorithm is in fact oblivious to the structure of the subspace, and extends when the subspace-membership constraint is replaced by any constraint for which partial satisfying assignments can be efficiently completed to a full satisfying assignment.  Finally, for systems of $O(n)$ polynomial equations in $n$ variables over the field of two elements, we give a fast exponential algorithm when each polynomial has bounded degree irreducible factors (but can otherwise have large degree) using a degree reduction trick.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/112"><span class="datestr">at July 30, 2021 02:11 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-5413385044107468315">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/07/covid-stats.html">Covid Stats</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div>A stat often quoted: <a href="https://www.nytimes.com/2021/07/22/health/coronavirus-breakthrough-infections-delta.html">About 97% of hospitalized coronavirus patients have not been vaccinated. </a> </div><div><br /></div><div>People take this as proof that once vaccinated no worries. But I have so many challenges with this statistic.</div><div><ul style="text-align: left;"><li>This statistic is down from 100% a year ago. Are vaccines working poorer now?</li><li>There are likely correlations to those vaccinated and those who take precautions like mask wearing and social distancing, though I'm sure which way those correlations go.</li><li>Those unvaccinated are more likely to be near others unvaccinated so more likely get infected and hospitalized.</li></ul><div>Even though one shouldn't draw the conclusion from the statistic, that doesn't mean the conclusion is false. The gold standard are the double-blind vaccine trials which clearly showed the vaccines more efficient and safe. So get the vaccine, not that this blog post will convince those who have been making the conscious choice not to vaccinate to change their minds.</div><div><br /></div></div><div>The other stat I found odd was that the life expectancy dropped 1.5 years in 2020. This doesn't mean on average we'll live 1.5 year less. Rather it means that someone who lives their whole life in 2020 conditions, widespread Covid without vaccines, would on average live 1.5 years less than someone who live their entire life in 2019 conditions pre-Covid.</div><div><br /></div><div>Oddly enough if you made it to 2021 your life expectancy will probably increase (assuming you've been vaccinated). We made tremendous progress in understanding vaccine technology in 2020. Also people with conditions that could have limited their later life span were more likely to be fatal Covid victims, meaning those who are left would live longer.</div></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/07/covid-stats.html"><span class="datestr">at July 29, 2021 07:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=891">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2021/07/28/chess-com-55-1000/">Chess.com: 5|5 &gt; 1000</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Today for the first time, I surpassed score 1000 on Chess.com playing 5|5, which means you start with a 5-minute budget, and every move you get 5 more seconds.  For a while I also played 3|2 (2-second increments), but it takes me about 2 seconds to move a piece, which means I lost games in which I knew exactly what to do, but simply couldn’t move the pieces fast enough, which I found frustrating.  Longer games I tried but I don’t seem to have the patience for.</p>



<p>I won’t reveal my id, because I feel bad about how much time I am spending losing at chess (and I think you could see all my games with my id, but I am not sure).  My self-imposed limit is losing no more than one game a day, which means on average playing 2 games per day.  (I had to stop and Google <img src="https://s0.wp.com/latex.php?latex=%5Csum_i+i%2F2%5Ei+%3D+2&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="\sum_i i/2^i = 2" class="latex" />; there’s a neat calculation-free proof of it which hopefully will make me remember this fact next time.)</p>



<p>However not being a robot, I sometimes get upset at the way I lose.  Most of my games are classified as <em>giveaway</em>, which means I was winning according to the computer (and myself), but then because of some stupid mistake I end up losing the game.  And so what the heck, I am better than this!, I break the rule and start another match — only to lose again, chess seems not to forgive hot heads.</p>



<p>The main reason why I play seems to be that fast-paced chess has the ability to completely absorb my mind, so it’s a good quick escape.  Of course, there are also the little feel-good voices reminding me that it’s better than watching TV and that by playing I sharpen my mind.</p>



<p>While 1000 can of course be a ridiculously low bar by some standard, I found reaching it more difficult than I expected, and I like to think that the 5|5 format attracts stronger players, so that the competition is tougher, even though it may not be true.   (But it does seem true that a certain score in a certain format does not correspond to the same score in a different format.)  For one thing, I had to familiarize myself with several basic openings.  I bought a little cute book <em>Chess openings for kids</em> which is good for people like me whose knowledge of chess openings was “e4 e5.”  I don’t do anything fancy, but it was fun to read about common openings.  I think I also wouldn’t mind playing random chess, but it seems harder to find opponents.</p>



<p>So why don’t you try and see what is your 5|5 score?  And if you want to play sometimes, drop me a line.</p>



<p></p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2021/07/28/chess-com-55-1000/"><span class="datestr">at July 28, 2021 06:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5661">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5661">Striking new Beeping Busy Beaver champion</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>For the past few days, I was bummed about the sooner-than-expected loss of Steven Weinberg.  Even after putting up my <a href="https://www.scottaaronson.com/blog/?p=5566">post</a>, I spent hours just watching old interviews with Steve on YouTube and reading his old essays for gems of insight that I’d missed.  (Someday, I’ll tackle Steve’s celebrated quantum field theory and general relativity textbooks … but that day is not today.)</p>



<p>Looking for something to cheer me up, I was delighted when <em>Shtetl-Optimized</em> reader Nick Drozd reported a significant new discovery in BusyBeaverology—one that, I’m proud to say, was directly inspired by my <a href="https://www.scottaaronson.com/papers/bb.pdf">Busy Beaver survey article</a> from last summer (<a href="https://www.scottaaronson.com/blog/?p=4916">see here for blog post</a>).</p>



<p>Recall that BB(n), the n<sup>th</sup> Busy Beaver number (technically, “Busy Beaver shift number”), is defined as the maximum number of steps that an n-state Turing machine, with 1 tape and 2 symbols, can make on an initially all-0 tape before it invokes a Halt transition.  Famously, BB(n) is not only uncomputable, it grows faster than any computable function of n—indeed, computing anything that grows as quickly as Busy Beaver is equivalent to solving the halting problem.</p>



<p>As of 2021, here is the extent of human knowledge about concrete values of this function:</p>



<ul><li>BB(1) = 1 (trivial)</li><li>BB(2) = 6 (Lin 1963)</li><li>BB(3) = 21 (Lin 1963)</li><li>BB(4) = 107 (Brady 1983)</li><li>BB(5) ≥ 47,176,870 (Marxen and Buntrock 1990)</li><li>BB(6) &gt; 7.4 × 10<sup>36,534</sup> (Kropitz 2010)</li><li>BB(7) &gt; 10<sup>2×10^10^10^18,705,352</sup> (“Wythagoras” 2014)</li></ul>



<p>As you can see, the function is reasonably under control for n≤4, then “achieves liftoff” at n=5.</p>



<p>In my survey, inspired by a suggestion of Harvey Friedman, I defined a variant called Beeping Busy Beaver, or BBB.  Define a <em>beeping Turing machine</em> to be a TM that has a single designated state where it emits a “beep.”  The <em>beeping number</em> of such a machine M, denoted b(M), is the largest t such that M beeps on step t, or ∞ if there’s no finite maximum.  Then BBB(n) is the largest finite value of b(M), among all n-state machines M.</p>



<p>I noted that the BBB function grows uncomputably <em>even given an oracle for the ordinary BB function</em>.  In fact, computing anything that grows as quickly as BBB is equivalent to solving any problem in the second level of the <a href="https://en.wikipedia.org/wiki/Arithmetical_hierarchy">arithmetical hierarchy</a> (where the computable functions are in the zeroth level, and the halting problem is in the first level).  Which means that pinning down the first few values of BBB should be <em>even</em> <em>more</em> breathtakingly fun than doing the same for BB!</p>



<p>In my survey, I noted the following four concrete results:</p>



<ul><li>BBB(1) = 1 = BB(1)</li><li>BBB(2) = 6 = BB(2)</li><li>BBB(3) ≥ 55 &gt; 21 = BB(3)</li><li>BBB(4) ≥ 2,819 &gt; 107 = BB(4)</li></ul>



<p>The first three of these, I managed to get on my own, with the help of a little program I wrote.  The fourth one was communicated to me by Nick Drozd even before I finished my survey.</p>



<p>So as of last summer, we knew that BBB coincides with the ordinary Busy Beaver function for n=1 and n=2, then breaks away starting at n=3.  We didn’t know how quickly BBB “achieves liftoff.”</p>



<p>But Nick continued plugging away at the problem all year, and he now claims to have resolved the question.  More concretely, he claims the following two results:</p>



<ul><li>BBB(3) = 55 (via exhaustive enumeration of cases)</li><li>BBB(4) ≥ 32,779,478 (via a newly-discovered machine)</li></ul>



<p>For more, see Nick’s <a href="https://cs.nyu.edu/pipermail/fom/2021-July/022743.html">announcement on the Foundations of Mathematics email list</a>, or his own <a href="https://nickdrozd.github.io/2021/07/11/self-cleaning-turing-machine.html">blog post</a>.</p>



<p>Nick actually writes in terms of yet another Busy Beaver variant, which he calls BLB, or “Blanking Beaver.”  He defines BLB(n) to be the maximum finite number of steps that an n-state Turing machine can take before it first “wipes its tape clean”—that is, sets all the tape squares to 0, as they were at the very beginning of the computation, but as they were <em>not</em> at intermediate times.  Nick has discovered a 4-state machine that takes 32,779,477 steps to blank out its tape, thereby proving that</p>



<ul><li>BLB(4) ≥ 32,779,477.</li></ul>



<p>Nick’s construction, when investigated, turns out to be based on a “Collatz-like” iterative process—exactly like the BB(5) champion and most of the other strong Busy Beaver contenders currently known.  A simple modification of his construction yields the lower bound on BBB.</p>



<p>Note that the Blanking Beaver function does <em>not</em> have the same sort of super-uncomputable growth that Beeping Busy Beaver has: it merely grows “normally” uncomputably fast, like the original BB function did.  Yet we see that BLB, just like BBB, already “achieves liftoff” by n=4, rather than n=5.  So the real lesson here is that <em>4-state Turing machines can already do fantastically complicated things on blank tapes</em>.  It’s just that the usual definitions of the BB function artificially prevent us from seeing that; they hide the uncomputable insanity until we get to 5 states.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5661"><span class="datestr">at July 27, 2021 10:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/07/26/postdoc-at-ist-austria-apply-by-august-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/07/26/postdoc-at-ist-austria-apply-by-august-31-2021/">Postdoc at IST Austria (apply by August 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>My group at IST Austria has an open postdoctoral position, part of an ERC Starting Grant project, whose goal is to develop new theory, and algorithms for scalable machine learning.</p>
<p>For questions, please contact dan.alistarh@ist.ac.at. The application should contain a CV, publication list, and a 1-page statement describing motivation and research interests.</p>
<p>Website: <a href="https://scaleml.pages.ist.ac.at/">https://scaleml.pages.ist.ac.at/</a><br />
Email: dan.alistarh@ist.ac.at</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/07/26/postdoc-at-ist-austria-apply-by-august-31-2021/"><span class="datestr">at July 26, 2021 05:51 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-161863856324090968">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/07/i-wish-problems-i-have-with-computers.html">I wish problems I have with computers really were my fault</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> As you know, the website for out for a few days, as Lance explained <a href="https://blog.computationalcomplexity.org/2021/07/technical-difficulties.html">here</a>.</p><p>When I first could not get to the this blog  my thought was</p><p><i>OH, I must have changed some setting by accident. When Lance gets back (he was on vacation) he'll know how to fix it. Bad timing that it happened when he was gone, though prob not an accident- with him on vacation I was at the site more often and had more of a chance to screw things up. AND Lance will tell me what I did and I'll know to not do it again. And I will learn more about how this all works which will help me in the future!</i></p><p>When Lance got back we found out that NO Bill didn't do anything wrong. The blog site company  that we work with did an update and BLAH BLAH BLAH.  Reminds me of the theme behind the TV show Seinfeld: <i>No Hugs, No Learning.</i> At least no learning. I am not in the slightest more enlightened. </p><p>Lance worked with them and YADA YADA YADA the problem is fixed, so I am very happy about that. </p><p>On the one hand I wish it had been my fault so I would learn something.  On he other hand, if it was my fault would it have been as easy to fix? Would I really have learned something? </p><p>When something does not work my protocol is</p><p>1) Turn the machine off and on again (e.g., log out and log in again). I want to say </p><p><i>this works surprisingly often</i></p><p>but I doubt this surprises any of my readers, or is even news to them.</p><p>2) Spend at most 5 minutes <i>trying to fix it myself . </i>You will soon see that 5 minutes is a good choice for me.</p><p>3) Ask staff or Lance or Darling or my TA  (depending on the problem). </p><p>4) They tell me to log off and log on again. When I tell them I already have they do something magical and it works again. I then ask them:</p><p>a) Could I have fixed this myself. 2/3 of the time the answer is no. They don't mean intellectually. They mean that I do not have access to what I need to fix it.</p><p>b) Did I do something wrong? I want to know so I won't do it again. about 99/100 times the answer is that I did nothing wrong (I don't recall that last time that I did).</p><p>Given a and b, I think 5 minutes is all the time I want to spend to try to fix it myself. </p><p><br /></p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/07/i-wish-problems-i-have-with-computers.html"><span class="datestr">at July 26, 2021 04:12 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/111">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/111">TR21-111 |  Influence of a Set of Variables on a Boolean Function | 

	Aniruddha  Biswas, 

	Palash Sarkar</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The influence of a set of variables on a Boolean function has three separate definitions in the literature, the first due to Ben-Or and Linial (1989), the second due to Fischer et al. (2002) and Blais (2009) and the third due to Tal (2017). The goal of the present work is to carry out a comprehensive study of the notion of influence of a set of variables on a Boolean function. To this end, we introduce a           definition of this notion using the auto-correlation function. A modification of the definition leads to the notion of pseudo-influence. Somewhat surprisingly, it turns out that the auto-correlation based definition of influence is equivalent to the definition introduced by Fischer et al. (2002) and Blais (2009) and the notion of pseudo-influence is equivalent to the definition of influence considered by Tal (2017). Extensive analysis of influence and pseduo-influence as well as the Ben-Or and Linial notion of influence is carried out and the relations between these notions are established.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/111"><span class="datestr">at July 25, 2021 04:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/110">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/110">TR21-110 |  Fourier growth of structured $\mathbb{F}_2$-polynomials and applications | 

	Jaroslaw Blasiok, 

	Peter Ivanov, 

	Yaonan Jin, 

	Chin Ho Lee, 

	Rocco Servedio, 

	Emanuele Viola</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We analyze the Fourier growth, i.e. the $L_1$ Fourier weight at level $k$ (denoted $L_{1,k}$), of various well-studied classes of "structured" $\mathbb{F}_2$-polynomials. This study is motivated by applications in pseudorandomness, in particular recent results and conjectures due to [CHHL19,CHLT19,CGLSS20] which show that upper bounds on Fourier growth (even at level $k=2$) give unconditional pseudorandom generators.

  Our main structural results on Fourier growth are as follows:

  - We show that any symmetric degree-$d$ $\mathbb{F}_2$-polynomial $p$ has $L_{1,k}(p) \le \Pr[p=1] \cdot O(d)^k$, and this is tight for any constant $k$. This quadratically strengthens an earlier bound that was implicit in [RSV13].

  - We show that any read-$\Delta$ degree-$d$ $\mathbb{F}_2$-polynomial $p$ has $L_{1,k}(p) \le \Pr[p=1] \cdot (k \Delta d)^{O(k)}$.

  - We establish a composition theorem which gives $L_{1,k}$ bounds on disjoint compositions of functions that are closed under restrictions and admit $L_{1,k}$ bounds.

  Finally, we apply the above structural results to obtain new unconditional pseudorandom generators and new correlation bounds for various classes of $\mathbb{F}_2$-polynomials.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/110"><span class="datestr">at July 25, 2021 11:28 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5566">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5566">Steven Weinberg (1933-2021): a personal view</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p></p>



<p></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="https://www.sciencenews.org/wp-content/uploads/2021/07/072421_weinberg_closecrop-1030x580.jpg" alt="Steven Weinberg sitting in front of a chalkboard covered in equations" /></figure></div>



<p>Steven Weinberg was, perhaps, the last truly towering figure of 20th-century physics.  In 1967, he wrote a <a href="https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.19.1264">3-page paper</a> saying in effect that as far as he could see, two of the four fundamental forces of the universe—namely, electromagnetism and the weak nuclear force—had actually been the same force until a tiny fraction of a second after the Big Bang, when a broken symmetry caused them to decouple.  Strangely, he had developed the math underlying this idea for the strong nuclear force, and it didn’t work there, but it <em>did</em> seem to work for the weak force and electromagnetism.  Steve noted that, if true, this would require the existence of two force-carrying particles that hadn’t yet been seen — the W and Z bosons — and would <em>also </em>require the existence of the famous Higgs boson.</p>



<p>By 1979, enough of this picture had been confirmed by experiment that Steve shared the Nobel Prize in Physics with Sheldon Glashow—Steve’s former high-school classmate—as well as with Abdus Salam, both of whom had separately developed pieces of the same puzzle.  As arguably the central architect of what we now call the Standard Model of elementary particles, Steve was in the ultra-rarefied class where, had he <em>not</em> won the Nobel Prize, it would’ve been a stain on the prize rather than on him.</p>



<p>Steve once recounted in my hearing that Richard Feynman initially heaped scorn on the electroweak proposal.  Late one night, however, Steve was woken up by a phone call.  It was Feynman.  “I believe your theory now,” Feynman announced.  “Why?” Steve asked.  Feynman, being Feynman, gave some idiosyncratic reason that he’d worked out for himself.</p>



<p>It used to happen more often that someone would put forward a bold new proposal about the most fundamental laws of nature … and then the experimentalists would <em>actually go out and confirm it</em>.  Besides with the Standard Model, though, there’s approximately <em>one</em> other time that that’s happened in the living memory of most of today’s physicists.  Namely, when astronomers discovered in 1998 that the expansion of the universe was accelerating, apparently due to a dark energy that behaved like Einstein’s long-ago-rejected cosmological constant.  Very few had expected such a result.  There was one prominent exception, though: Steve Weinberg had <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.59.2607">written in 1987</a> that he saw no reason why the cosmological constant shouldn’t take a nonzero value that was still tiny enough to be consistent with galaxy formation and so forth.</p>



<hr class="wp-block-separator" />



<p>In his long and illustrious career, one of the <em>least</em> important things Steve did, six years ago, was to play a major role in recruiting me and my wife Dana to UT Austin.  The first time I met Steve, his first question to me was “have we met before?  you look familiar.”  It turns out that he’d met my dad, Steve Aaronson, way back in the 1970s, when my dad (then a young science writer) had interviewed Weinberg for a magazine article.  I was astonished that Weinberg would remember such a thing across decades.</p>



<p>Steve was then gracious enough to take me, Dana, and both of my parents out to dinner in Austin as part of my and Dana’s recruiting trip.</p>



<div class="wp-block-image"><figure class="aligncenter size-full"><a href="https://www.scottaaronson.com/blog/wp-content/uploads/2021/07/weinberg3.jpg"><img width="422" alt="" src="https://www.scottaaronson.com/blog/wp-content/uploads/2021/07/weinberg3.jpg" class="wp-image-5618" height="317" /></a></figure></div>



<p>We talked, among other things, about Telluride House at Cornell, where Steve had lived as an undergrad in the early 1950s and where I’d lived as an undergrad almost half a century later.  Steve said that, while he loved the intellectual atmosphere at Telluride, he tried to have as little to do as possible with the “self-government” aspect, since he found the political squabbles that convulsed many of the humanities majors there to be a waste of time.  I burst out laughing, because … well, imagine you got to have dinner with James Clerk Maxwell, and he opened up about some ridiculously specific pet peeve from his college years, and it was <em>your</em> ridiculously specific pet peeve from <em>your</em> college years.</p>



<p>(Steve claimed to us, not entirely convincingly, that he was a mediocre student at Cornell, more interesting in “necking” with his fellow student and future wife Louise than in studying physics.)</p>



<p>After Dana and I came to Austin, Steve was kind enough to invite me to the high-energy theoretical physics lunches, where I chatted with him and the other members of his group every week (or better yet, simply listened).  I’d usually walk to the faculty club ten minutes early.  Steve, having arrived by car, would be sitting alone in an armchair, reading a newspaper, while he waited for the other physicists to arrive by foot.  No matter how scorching the Texas sun, Steve would <em>always</em> be wearing a suit (usually a tan one) and a necktie, his walking-cane by his side.  I, typically in ratty shorts and t-shirt, would sit in the armchair next to him, and we’d talk—about the latest developments in quantum computing and information (Steve, a perpetual student, would pepper me with questions), or his recent work on nonlinear modifications of quantum mechanics, or his memories of Cambridge, MA, or climate change or the anti-Israel protests in Austin or whatever else.  These conversations, brief and inconsequential as they probably were to him, were highlights of my week. </p>



<p>There was, of course, something a little melancholy about getting to know such a great man only in the twilight of his life.  To be clear, Steve Weinberg in his mid-to-late 80s was <em>far</em> more cogent, articulate, and quick to understand what was said to him than just about anyone you’d ever met in their prime.  But then, after a short conversation, he’d have to leave for a nap.  Steve was as clear-eyed and direct about his age and impending mortality as he was about everything else.  “Scott!” he once greeted me.  “I just saw the announcement for your physics colloquium about quantum supremacy.  I hope I’m still alive next month to attend it.”</p>



<p>(As it happens, the colloquium in question was on November 9, 2016, the day we learned that Trump would become president.  I offered to postpone the talk, since no one could concentrate on physics on such a day.  While several of the physicists agreed that that was the right call, Steve convinced me to go ahead with the following message: “I sympathize, but I do want to hear you … There is some virtue in just plowing on.”)</p>



<p>I sometimes felt, as well, like I was speaking with Steve across a cultural chasm even greater than the half-century that separated us in age.  Steve enjoyed nothing more than to discourse at length, in his booming New-York-accented baritone, about opera, or ballet, or obscure corners of 18th-century history.  It would be easy to feel like a total philistine by comparison … and I did.  Steve also told me that he never reads blogs or other social media, since he’s unable believe any written work is “real” unless it’s published, ideally on paper.  I could only envy such an attitude.</p>



<hr class="wp-block-separator" />



<p>If you <em>did</em> try to judge by the social media that he never read, you might conclude that Steve would be remembered by the wider world less for any of his epochal contributions to physics than for a single viral quote of his:</p>



<blockquote class="wp-block-quote"><p>With or without religion, good people can behave well and bad people can do evil; but for good people to do evil — that takes religion.</p></blockquote>



<p>I can testify that Steve fully lived his atheism.  Four years ago, I invited him (along with many other UT colleagues) to the <em>brit milah</em> of my newborn son Daniel.  Steve said he’d be happy to come over our house another time (and I’m happy to say that he did a year later), but not to witness any body parts being cut.</p>



<p>Despite his hostility to Judaism—along with every other religion—Steve was a vociferous supporter of the state of Israel, almost to the point of making me look like Edward Said or Noam Chomsky.  For Steve, Zionism was not in spite of his liberal, universalist Enlightenment ideals but because of them.</p>



<p>Anyway, there’s no need even to wonder whether Steve had any sort of deathbed conversion.  He’d laugh at the thought.</p>



<hr class="wp-block-separator" />



<p>In 2016, Steve published <a href="https://www.amazon.com/Explain-World-Discovery-Modern-Science/dp/0062346660"><em>To Explain the World</em></a>, a history of human progress in physics and astronomy from the ancient Greeks to Newton (when, Steve says, the scientific ethos reached the form that it still basically has today).  It’s unlike any other history-of-science book that I’ve read.  Of course I’d read other books about Aristarchus and Ptolemy and so forth, but I’d never read a modern writer treating them not as historical subjects, but as <em>professional colleagues merely separated in time.</em>  Again and again, Steve would redo ancient calculations, finding errors that had escaped historical notice; he’d remark on how Eratosthenes or Kepler could’ve done better with the data available to them; he’d grade the ancients by how much of modern physics and cosmology they’d correctly anticipated.</p>



<p><em>To Explain the World</em> was savaged in reviews by professional science historians.  Apparently, Steve had committed the unforgivable sin of “Whig history”: that is, judging past natural philosophers by the standards of today.  Steve clung to the naïve, debunked, scientistic notions that there’s such a thing as “actual right answers” about how the universe works; that we today are, at any rate, much closer to those right answers than the ancients were; and that we can <em>judge</em> the ancients by how close they got to the right answers that we now know.</p>



<p>As I read the sneering reviews, I kept thinking: so suppose Archimedes, Copernicus, and all the rest were brought back from the dead.  Who would they rather talk to: historians seeking to explore every facet of their misconceptions, like anthropologists with a paleolithic tribe; or Steve Weinberg, who’d want to bring them up to speed as quickly as possible so they could continue the joint quest?</p>



<hr class="wp-block-separator" />



<p>When it comes to the foundations of quantum mechanics, Steve <a href="https://www.nybooks.com/articles/2017/01/19/trouble-with-quantum-mechanics/">took the view</a> that <em>no</em> existing interpretation is satisfactory, although the Many-Worlds Interpretation is perhaps the least bad of the bunch.  Steve felt that our reaction to this state of affairs should be to <em>test quantum mechanics more precisely</em>—for example, by looking for tiny nonlinearities in the Schrödinger equation, or other signs that QM itself is only a limit of some more all-encompassing theory.  This is, to put it mildly, not a widely-held view among high-energy physicists—but it provided a fascinating glimpse into how Steve’s mind works.</p>



<p>Here was, empirically, the most successful theoretical physicist alive, and again and again, his response to conceptual confusion was not to ruminate more about basic principles but to <em>ask for more data</em> or <em>do a more detailed calculation</em>.  He never, ever let go of a short tether to the actual testable consequences of whatever was being talked about, or future experiments that might change the situation.</p>



<p>(Steve worked on string theory in the early 1980s, and he remained engaged with it for the rest of his life, for example by recruiting the string theorists Jacques Distler and Willy Fischler to UT Austin.  But he later soured on the prospects for getting testable consequences out of string theory within a reasonable timeframe.  And he once complained to me that the papers he’d read about “It from Qubit,” AdS/CFT, and the black hole information problem had had “too many words and not enough equations.”)</p>



<hr class="wp-block-separator" />



<p>Steve was, famously, about as hardcore a reductionist as has ever existed on earth.  He was a reductionist not just in the usual sense that he believed there <em>are</em> fundamental laws of physics, from which, together with the initial conditions, everything that happens in our universe can be calculated in principle (if not in practice), at least probabilistically.  He was a reductionist in the stronger sense that he thought the quest to discover the fundamental laws of the universe had a special pride of place among all human endeavors—a place not shared by the many sciences devoted to the study of complex emergent behavior, interesting and important though they might be.</p>



<p>This came through clearly in Steve’s <a href="https://www.nybooks.com/articles/2002/10/24/is-the-universe-a-computer/">critical review</a> of Stephen Wolfram’s <em>A New Kind of Science</em>, where Steve (Weinberg, that is) articulated his views of why “free-floating” theories of complex behavior can’t take the place of a reductionistic description of our actual universe.  (Of course, I was <em>also</em> highly critical of <em>A New Kind of Science</em> in <a href="https://arxiv.org/abs/quant-ph/0206089">my review</a>, but for somewhat different reasons than Steve was.)  Steve’s reductionism was also clearly expressed in his testimony to Congress in support of continued funding for the Superconducting Supercollider.  (Famously, Phil Anderson testified <em>against</em> the SSC, arguing that the money would better be spent on condensed-matter physics and other sciences of emergent behavior.  The result: Congress did cancel the SSC, and it redirected precisely zero of the money to other sciences.  But at least Steve lived to see the LHC dramatically confirm the existence of the Higgs boson, as the SSC would have.)</p>



<p>I, of course, have devoted my career to theoretical computer science, which you might broadly call a “science of emergent behavior”: it tries to figure out the ultimate possibilities and limits of computation, taking the underlying laws of physics as given.  Quantum computing, in particular, takes as its input a physical theory that was already known by 1926, and studies what can be done with it.  So you might expect me to disagree passionately with Weinberg on reductionism versus holism.</p>



<p>In reality, I have a hard time pinpointing any substantive difference.  Mostly I see a difference in <em>opportunities</em>: Steve saw a golden chance to contribute something to the millennia-old quest to discover the fundamental laws of nature, at the tail end of the heroic era of particle physics that culminated in what we now call the Standard Model.  He was brilliant enough to seize that chance.  I didn’t see a similar chance: possibly because it no longer existed; almost certainly because, even if it did, I wouldn’t have had the right mind for it.  I found a different chance, to work at the intersection of physics and computer science that was finally kicking into high gear at the end of the 20th century.  Interestingly, while I came to that intersection from the CS side, quite a few who were originally trained as high-energy physicists ended up there as well—including a star PhD student of Steve Weinberg’s named John Preskill.</p>



<p>Despite his reductionism, Steve was as curious and enthusiastic about quantum computation as he was about a hundred other topics beyond particle physics—he even ended his <a href="https://www.amazon.com/Lectures-Quantum-Mechanics-Steven-Weinberg/dp/1107028728">quantum mechanics textbook</a> with a chapter about Shor’s factoring algorithm.  Having said that, a central <em>reason</em> for his enthusiasm about QC was that he clearly saw how demanding a test it would be of quantum mechanics itself—and as I mentioned earlier, Steve was open to the possibility that quantum mechanics might not be exactly true.</p>



<hr class="wp-block-separator" />



<p>It would be an understatement to call Steve “left-of-center.”  He believed in higher taxes on rich people like himself to service a robust social safety net.  When Trump won, Steve remarked to me that most of the disgusting and outrageous things Trump would do could be reversed in a generation or so—but not the aggressive climate change denial; that actually <em>could</em> matter on the scale of centuries.  Steve made the news in Austin for <a href="https://www.texastribune.org/2016/01/27/the-brief/">openly defying</a> the Texas law forcing public universities to allow concealed carry on campus: he said that, regardless of what the law said, firearms would not be welcome in <em>his</em> classroom.  (Louise, Steve’s wife for 67 years and a professor at UT Austin’s law school, also wrote perhaps the <a href="https://law.utexas.edu/faculty/publications/2002-When-Courts-Decide-Elections-The-Constitutionality-of-Bush-v-Gore">definitive scholarly takedown</a> of the shameful <em>Bush vs. Gore</em> Supreme Court decision, which installed George W. Bush as president.)</p>



<p>All the same, during the “science wars” of the 1990s, Steve was <a href="https://www.nybooks.com/articles/1996/08/08/sokals-hoax/">scathing</a> about the academic left’s postmodernist streak and deeply sympathetic to what Alan Sokal had done with his <em>Social Text </em>hoax.  Steve also once told me that, when he (like other UT faculty) was required to write a statement about what he would do to advance Diversity, Equity, and Inclusion, he submitted just a single sentence: “I will seek the best candidates, without regard to race or sex.”  I remarked that he might be one of the only academics who could get away with that.</p>



<p>I confess that, for the past five years, knowing Steve was a greater source of psychological strength for me than, from a rational standpoint, it probably should have been.  Regular readers will know that I’ve spent months of my life agonizing over various nasty things people have said me about on Twitter and Reddit—that I’m a sexist white male douchebag, a clueless techbro STEMlord, a neoliberal Zionist shill, and I forget what else.</p>



<p>But I lately <em>have</em> had a secret emotional weapon that helped somewhat: namely, the certainty that Steven Weinberg had more intellectual power in a single toenail clipping than these Twitter-attackers had collectively experienced over the course of their lives.  It’s like, have you heard the joke where two rabbis are arguing some point of Talmud, and then God speaks from a booming thundercloud to declare that the first rabbi is right, and then the second rabbi says “OK fine, now it’s 2 against 1?”  For the W and Z bosons and Higgs boson that <em>you</em> predicted to turn up at the particle accelerator is not <em>exactly</em> God declaring from a thundercloud that the way your mind works is aligned with the way the world actually is—Steve, of course, would wince at the suggestion—but it’s about the closest thing available in this universe.  My secret emotional weapon was that I knew the man who’d experienced this, arguably more than any of the 7.6 billion other living humans, and not only did that man not sneer at me, but by some freakish coincidence, he seemed to have reached roughly the same views as I had on &gt;95% of controversial questions where we both had strong opinions.</p>



<hr class="wp-block-separator" />



<p>My final conversations with Steve Weinberg were about a laptop.  When covid started in March 2020, Steve and Louise, being in their late 80s, naturally didn’t want to take chances, and rigorously sheltered at home.  But an issue emerged: Steve couldn’t install Zoom on his Bronze Age computer, and so couldn’t participate in the virtual meetings of his own group, nor could he do Zoom calls with his daughter and granddaughter.  While as a <em>theoretical</em> computer scientist, I don’t normally volunteer myself as tech support staff, I decided that an exception was more than warranted in this case.  The quickest solution was to configure one of my own old laptops with everything Steve needed and bring it over to his house.</p>



<p>Later, Steve emailed me to say that, while the laptop had worked great and been a lifesaver, he’d finally bought his own laptop, so I should come by to pick mine up.  I delayed and delayed with that, but finally decided I should do it before leaving Austin at the beginning of this summer.  So I emailed Steve to tell him I’d be coming.  He replied to me asking Louise to leave the laptop on the porch — but the email was addressed only to me, not her.</p>



<p>At that moment, I knew something had changed: only a year before, incredibly, <em>I’d</em> been more senile and out-of-it as a 39-year-old than Steve had been as an 87-year-old.  What I didn’t know at the time was that Steve had sent that email from the hospital when he was close to death.  It was the last I heard from him.</p>



<p>(Once I learned what was going on, I did send a get-well note, which I hope Steve saw, saying that I hoped he appreciated that I <em>wasn’t</em> praying for him.)</p>



<hr class="wp-block-separator" />



<p>Besides the quote about good people, bad people, and religion, the other quote of Steve’s that he never managed to live down came from the last pages of <a href="https://www.amazon.com/First-Three-Minutes-Modern-Universe/dp/0465024378"><em>The First Three Minutes</em></a>, his classic 1970s popularization of big-bang cosmology:</p>



<blockquote class="wp-block-quote"><p>The more the universe seems comprehensible, the more it also seems pointless.</p></blockquote>



<p>In the 1993 epilogue, Steve tempered this with some more hopeful words, nearly as famous:</p>



<blockquote class="wp-block-quote"><p>The effort to understand the universe is one of the very few things which lifts human life a little above the level of farce and gives it some of the grace of tragedy.</p></blockquote>



<p>It’s not my purpose here to resolve the question of whether life or the universe have a point.  What I can say is that, even in his last years, Steve never for a nanosecond <em>acted</em> as if life was pointless.  He already had all the material comforts and academic renown anyone could possibly want.  He could have spent all day in his swimming pool, or listening to operas.  Instead, he continued publishing textbooks—a <a href="https://www.amazon.com/Lectures-Quantum-Mechanics-Steven-Weinberg/dp/1107028728">quantum mechanics textbook</a> in 2012, an <a href="https://www.amazon.com/Lectures-Astrophysics-Steven-Weinberg/dp/1108415075">astrophysics textbook</a> in 2019, and a <a href="https://www.amazon.com/dp/1108841767/?tag=pfamazon01-20">“Foundations of Modern Physics” textbook</a> in 2021 (!).  As recently as this year, he continued <a href="https://arxiv.org/search/hep-th?searchtype=author&amp;query=Weinberg%2C+Steven">writing papers</a>—and not just “great man reminiscing” papers, but hardcore technical papers.  He continued writing with nearly unmatched lucidity for a general audience, in the <em>New York Review of Books</em> and elsewhere.  And I can attest that he continued peppering visiting speakers with questions about stellar evolution or whatever else they were experts on—because, more likely than not, he had redone some calculation himself and gotten a subtly different result from what was in the textbooks.</p>



<p>If God exists, I can’t believe He or She would find nothing more interesting to do with Steve than to torture him for his unbelief.  More likely, I think, God is right now talking to Steve the same way Steve talked to Aristarchus in <em>To Explain the World</em>: “yes, you were close about the origin of neutrino masses, but here’s the part you were missing…”  While, of course, Steve is redoing God’s calculation to be sure.</p>



<hr class="wp-block-separator" />



<p>Feel free to use the comments as a place to share your own memories.</p>



<hr class="wp-block-separator" />



<p><strong>More Steven Weinberg memorial links (I’ll continue adding to this over the next few days):</strong></p>



<ul><li><a href="https://www.math.columbia.edu/~woit/wordpress/?p=12413">Peter Woit</a></li><li><a href="https://motls.blogspot.com/2021/07/steven-weinberg-1933-2021.html?m=1">He-Who-Must-Not-Be-Named</a></li><li><a href="https://mobile.twitter.com/johncarlosbaez/status/1418804320611364867?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet">John Baez</a> </li><li><a href="https://www.sciencenews.org/article/steven-weinberg-death-physics-electromagnetism-standard-model">Tom Siegfried</a></li><li><a href="https://mobile.twitter.com/seanmcarroll/status/1418804903871356930">Sean Carroll</a></li><li><a href="https://whyevolutionistrue.com/2021/07/24/steven-weinberg-died/">Jerry Coyne</a></li><li><a href="https://grahamfarmelo.com/remembering-steven-weinberg/">Graham Farmelo</a></li><li><a href="https://www.nytimes.com/2021/07/25/science/steven-weinberg-groundbreaking-nobelist-in-physics-dies-at-88.html">NYT</a></li><li><a href="https://thebulletin.org/2021/07/steven-weinberg-nobel-laureate-in-physics-and-bulletin-board-member-died-at-88/">Bulletin of the Atomic Scientists</a></li></ul>



<hr class="wp-block-separator" />



<p><strong>Miscellaneous Steven Weinberg links</strong></p>



<ul><li><a href="https://mobile.twitter.com/dashunwang/status/1419267004905754624">Steve’s advice to young scientists</a></li><li><a href="https://physicstoday.scitation.org/doi/full/10.1063/1.3397044">Lenny Susskind’s 2010 review</a> of Steve’s book <em>Lake Views</em></li><li><a href="https://www.closertotruth.com/contributor/steven-weinberg/profile">Steve’s interviews on “Closer to Truth”</a></li><li><a href="https://cerncourier.com/a/model-physicist/">Interview in <em>CERN Courier</em></a></li><li>Steve <a href="https://www.youtube.com/watch?v=ebuve4INdAU&amp;t=1454s">spars with another of my greatest friends and heroes</a>, Rebecca Goldstein, about the basis of morality—also featuring Richard Dawkins, Daniel Dennett, Sean Carroll, and more (YouTube video from a 2012 conference)</li></ul>



<p></p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5566"><span class="datestr">at July 25, 2021 04:19 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/109">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/109">TR21-109 |  QRAT Polynomially Simulates Merge Resolution. | 

	Sravanthi Chede, 

	Anil Shukla</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Merge Resolution (MRes [Beyersdorff et al. J. Autom. Reason.'2021] ) is a refutational proof system for quantified Boolean formulas (QBF). Each line of MRes consists of clauses with only existential literals, together with information of countermodels stored as merge maps. As a result, MRes has strategy extraction by design. The QRAT [Heule et al. J. Autom. Reason.'2017] proof system was designed to capture QBF preprocessing. QRAT can simulate both the expansion-based proof system $\forall$Exp+Res and CDCL-based QBF proof system LD-Q-Res. 

A family of false QBFs called SquaredEquality formulas were introduced in [Beyersdorff et al. J. Autom. Reason.'2021] and shown to be easy for MRes but need exponential size proofs in Q-Res, QU-Res, CP+$\forall$red, $\forall$Exp+Res, IR-calc and reductionless LD-Q-Res. As a result none of these systems can simulate MRes. In this paper, we show a short QRAT refutation of the SquaredEquality formulas. We further show that QRAT strictly p-simulates MRes. 
Besides highlighting the power of QRAT system, this work also presents the first simulation result for MRes.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/109"><span class="datestr">at July 23, 2021 08:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/07/23/postdoc-at-university-of-vienna-apply-by-august-31-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/07/23/postdoc-at-university-of-vienna-apply-by-august-31-2021/">Postdoc at University of Vienna (apply by August 31, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Two full-time 3-year postdoc positions in algorithms are available starting Jan 1, 2022 as part of the ERC Advanced Grant “Modern Dynamic Data Structures” to joint the algorithms research group headed by Monika Henzinger. Prior knowledge in fair algorithms and differential privacy is a plus. Please send your CV, a letter of motivation, and the names of 3 references to applications.taa@univie.ac.at</p>
<p>Website: <a href="https://taa.cs.univie.ac.at/">https://taa.cs.univie.ac.at/</a><br />
Email: applications.taa@univie.ac.at</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/07/23/postdoc-at-university-of-vienna-apply-by-august-31-2021/"><span class="datestr">at July 23, 2021 06:04 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://adamsheffer.wordpress.com/?p=5690">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sheffer.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://adamsheffer.wordpress.com/2021/07/23/a-basic-question-about-multiplicative-energy/">A Basic Question About Multiplicative Energy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
As part of some research project, I got to a basic question about multiplicative energy. Embarrassingly , I wasn’t able to get any non-trivial bound for it. Here is the problem. Any information about it would be highly appreciated. Problem. Let . Let be a set of real numbers. How large can the multiplicative energy […]</div>







<p class="date">
by Adam Sheffer <a href="https://adamsheffer.wordpress.com/2021/07/23/a-basic-question-about-multiplicative-energy/"><span class="datestr">at July 23, 2021 04:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-6975981189321037535">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/07/technical-difficulties.html">Technical Difficulties</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div style="clear: both; text-align: center;" class="separator"><a style="margin-left: 1em; margin-right: 1em;" href="https://1.bp.blogspot.com/-racThFBUDl0/YPrj-7Dmi4I/AAAAAAAB9KA/St1lqT9-U5YLf_j03KkzPZKvO3wdk4csgCPcBGAsYHg/s4032/PXL_20210711_224807389.jpg"><img width="320" border="0" src="https://1.bp.blogspot.com/-racThFBUDl0/YPrj-7Dmi4I/AAAAAAAB9KA/St1lqT9-U5YLf_j03KkzPZKvO3wdk4csgCPcBGAsYHg/s320/PXL_20210711_224807389.jpg" /></a></div><br /><p>After returning from vacation last weekend (hello North Dakota--my 49th state visited), all sorts of odd problems arose. This blog stopped working, a P v NP paper was published on the ACM Transactions of Computing website and my personal emails were getting marked as spam. All is better, I hope.</p><p>Years ago I donated the URL computationalcomplexity.org to the <a href="https://www.computationalcomplexity.org/">Computational Complexity Conference</a>, coincidentally held this past week, with the condition that I could continue to use the "blog" subdomain for this blog. Organizations continue but the people in them change, and when the website was "upgraded" on Saturday the pointers to make this blog work were left out. Thanks to Ashwin Nayak for getting it all straightened out and we're back online.</p><p>For the ToCT paper, a paper claiming to reduce 3-SAT to 2-SAT, and thus show P = NP, was originally rejected by the journal but a "disposition field" got inadvertently set to accept and wasn't caught until it showed up online. Editor-in-Chief Ryan O'Donnell quickly got on the case and ACM has <a href="https://dl-acm-org.ezproxy.gl.iit.edu/doi/10.1145/3460950">removed the paper</a>. P v NP remains as open as ever.</p><p>Fixing the email required me to learn far more about <a href="https://en.wikipedia.org/wiki/Sender_Policy_Framework">SPFs</a> than I ever wanted to know.</p><p>By the way if anyone in Idaho wants to invite me to give a talk, I might be interested.</p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/07/technical-difficulties.html"><span class="datestr">at July 23, 2021 03:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=2256">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/">Average-Case Fine-Grained Hardness, Part I</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>I recently finished my qualifying exam for which I surveyed worst-case to average-case reductions for problems in NP. As part of my survey, I reviewed recent results on average-case hardness in P (a.k.a. average-case fine-grained hardness). I would like to give an overview of some of these results in a series of blog posts, and I want to start by giving some motivations.</p>



<p><strong>Why do we care about average-case fine-grained hardness?</strong></p>



<p>(i) We hope to explain why we are struggling to find faster algorithms for problems in P such as DNA sequencing even for some random large datasets.</p>



<p>(ii) Average-case hard problems in P is useful to cryptography. For example, constructing <a href="https://en.wikipedia.org/wiki/One-way_function">one-way functions</a> (i.e., functions that are easy to compute but hard to invert) based on worst-case complexity assumptions such as NP <img src="https://s0.wp.com/latex.php?latex=%5Cnot%5Csubseteq&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\not\subseteq" class="latex" /> BPP has been a long-standing open question. In this context, “easy” means polynomial-time computable. If we consider “easy” to be computable in time like <img src="https://s0.wp.com/latex.php?latex=O%28n%5E%7B100%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(n^{100})" class="latex" /> instead, then a natural alternative question is whether we can construct such one-way functions based on worst-case fine-grained complexity assumptions. Another example is <a href="https://en.wikipedia.org/wiki/Proof_of_work">proof of work</a> <a href="https://link.springer.com/chapter/10.1007/3-540-48071-4_10">[DN92]</a>. When a miner tries to mine the <a href="https://en.wikipedia.org/wiki/Cryptocurrency">cryptocurrency</a>, the miner is asked to solve a random puzzle, that is average-case hard but still tractable, and then the miner needs to prove they have indeed solved the puzzle through efficient protocols. An interesting and more recent follow-up is proof of useful work <a href="https://eprint.iacr.org/2017/203.pdf">[BRSV17b]</a>, which proposes that instead of wasting computing power on a random puzzle that comes from nowhere, we can first reduce a computational task of practical interest to multiple random puzzles and then ask the miner to solve those puzzles.</p>



<p>The most common approach for proving average-case fine-grained hardness is arguably worst-case to average-case reduction, i.e., reducing an arbitrary instance to a number of random instances of which the distribution is polynomial-time sampable. Before I give concrete examples, I want to describe a general recipe for designing such worst-case to average-case reductions. (Some reader might notice that the step 3 of the recipe is same as the argument for proving <a href="https://en.wikipedia.org/wiki/Random_self-reducibility#Permanent_of_a_matrix">computing permanent is self-reducible</a> <a href="https://www.semanticscholar.org/paper/New-Directions-In-Testing-Lipton/fb2eba4d69bdab34c2d240380d4370be2feeacb9">[L91]</a>, which essentially uses <a href="https://en.wikipedia.org/wiki/Locally_decodable_code#The_Reed%E2%80%93Muller_code">local decoding for Reed-Muller codes</a>.)</p>



<p><strong>General recipe for worst-case to average-case reductions:</strong></p>



<ol><li>Choose our favorite hard problem <img src="https://s0.wp.com/latex.php?latex=L%3A%5C%7B0%2C1%5C%7D%5En%5Cto%5Cmathbf%7BZ%7D_%7B%5Cge+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L:\{0,1\}^n\to\mathbf{Z}_{\ge 0}" class="latex" /> in P.</li><li>Construct a low-degree (degree-<img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />) polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BF%7D_%7Bp%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{F}_{p}^n" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D%28x%29%3DL%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}(x)=L(x)" class="latex" /> for all <img src="https://s0.wp.com/latex.php?latex=x%5Cin%5C%7B0%2C1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in\{0,1\}^n" class="latex" />.</li><li>To solve <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L" class="latex" /> for worst-case <img src="https://s0.wp.com/latex.php?latex=x%5Cin%5C%7B0%2C1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in\{0,1\}^n" class="latex" />: sample a uniformly random <img src="https://s0.wp.com/latex.php?latex=y%5Cin%5Cmathbf%7BF%7D_%7Bp%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="y\in\mathbf{F}_{p}^n" class="latex" />, compute <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D%28x%2Bt_1+y%29%2C%5Cdots%2Cf_%7BL%7D%28x%2Bt_%7Bd%2B1%7D+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}(x+t_1 y),\dots,f_{L}(x+t_{d+1} y)" class="latex" /> for distinct nonzero <img src="https://s0.wp.com/latex.php?latex=t_1%2C%5Cdots%2Ct_%7Bd%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t_1,\dots,t_{d+1}" class="latex" /> using average-case solver (note each <img src="https://s0.wp.com/latex.php?latex=x%2Bt_%7Bi%7D+y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x+t_{i} y" class="latex" /> is uniformly random), interpolate univariate polynomial <img src="https://s0.wp.com/latex.php?latex=g%28t%29%3A%3Df_%7BL%7D%28x%2Bt+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="g(t):=f_{L}(x+t y)" class="latex" /> using these points, and output <img src="https://s0.wp.com/latex.php?latex=g%280%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="g(0)" class="latex" /> which is <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D%28x%29%3DL%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}(x)=L(x)" class="latex" />. (This step can be replaced by decoding algorithms for <a href="https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction">Reed-Solomon codes</a> to handle larger errors of average-case solver.)</li><li>(The above steps already show that evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=d%2B1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d+1" class="latex" /> uniformly random inputs is as hard as solving <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L" class="latex" /> for worst-case input.) If we want to show <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L" class="latex" /> itself is average-case fine-grained hard, it suffices to give a reduction from computing <img src="https://s0.wp.com/latex.php?latex=f_%7BL%7D%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{L}(x)" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=x%5Cin%5Cmathbf%7BF%7D_%7Bp%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x\in\mathbf{F}_{p}^n" class="latex" /> back to solving <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="L" class="latex" />.</li></ol>



<p>Notice that the above general recipe reduces a worst-case instance to <img src="https://s0.wp.com/latex.php?latex=d%2B1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d+1" class="latex" /> random instances at the step 3, and thus, we want <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" /> to be small (like <img src="https://s0.wp.com/latex.php?latex=n%5E%7Bo%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n^{o(1)}" class="latex" />) such that it does not blow up the total runtime significantly. Typically, the step 4 would also blow up the runtime, and sometimes it depends on <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />. For all the problems (or the techniques) in this series, I will explicitly quantify the runtime blow-up in the step 4 (if there is any) and explain how small we want <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" /> to be (if the blow-up depends on <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d" class="latex" />).</p>



<p>Now let us go through a concrete example. Consider one of the flagship problems in fine-grained complexity — orthogonal vector problem (OV): Given <img src="https://s0.wp.com/latex.php?latex=X%3D%5C%7Bx_1%2C%5Cdots%2Cx_n%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X=\{x_1,\dots,x_n\}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=Y%3D%5C%7By_1%2C%5Cdots%2Cy_n%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y=\{y_1,\dots,y_n\}" class="latex" />, where each <img src="https://s0.wp.com/latex.php?latex=x_i%2Cy_i%5Cin%5C%7B0%2C1%5C%7D%5E%7Bd%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x_i,y_i\in\{0,1\}^{d'}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=d%27%5Cin%5Comega%28%5Clog+n%29%5Ccap+n%5E%7Bo%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d'\in\omega(\log n)\cap n^{o(1)}" class="latex" />, decide if there are <img src="https://s0.wp.com/latex.php?latex=x_i%2Cy_j&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x_i,y_j" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=%5Clangle+x_i%2Cy_j%5Crangle%3D0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\langle x_i,y_j\rangle=0" class="latex" />. It is known OV has no sub-quadratic algorithm assuming strong <a href="https://en.wikipedia.org/wiki/Exponential_time_hypothesis">exponential-time hypothesis</a> (SETH) <a href="https://link.springer.com/chapter/10.1007/978-3-540-27836-8_101">[W05]</a>, and there is a generalization of OV called <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-OV problem that has no <img src="https://s0.wp.com/latex.php?latex=O%28n%5E%7Bk-o%281%29%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(n^{k-o(1)})" class="latex" /> algorithm under the same assumption <a href="https://dl.acm.org/doi/10.1145/3300150.3300158">[W18]</a>.</p>



<p><strong>Polynomial evaluation.</strong> Given an OV instance <img src="https://s0.wp.com/latex.php?latex=X%2CY&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X,Y" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=d%27%3Dn%5E%7Bo%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="d'=n^{o(1)}" class="latex" />, we construct a degree-<img src="https://s0.wp.com/latex.php?latex=2d%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="2d'" class="latex" /> polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7B%5Ctextrm%7BOV%7D%7D%28X%2CY%29%3A%3D%5Csum_%7Bi%2Cj%5Cin%5Bn%5D%7D%5Cprod_%7Bt%5Cin%5Bd%27%5D%7D%281-x_%7Bi%2Ct%7Dy_%7Bi%2Ct%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{\textrm{OV}}(X,Y):=\sum_{i,j\in[n]}\prod_{t\in[d']}(1-x_{i,t}y_{i,t})" class="latex" /> over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BF%7D_%7Bp%7D%5E%7B2nd%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{F}_{p}^{2nd'}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=p%3En%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p&gt;n^2" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=x_%7Bi%2Ct%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x_{i,t}" class="latex" /> denotes the <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-th coordinate of <img src="https://s0.wp.com/latex.php?latex=x_i%5Cin+X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="x_i\in X" class="latex" />. Observe that <img src="https://s0.wp.com/latex.php?latex=f_%7B%5Ctextrm%7BOV%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{\textrm{OV}}" class="latex" /> simply enumerates all the pairs of vectors and counts the number of orthogonal pairs for the OV instance, and obviously counting is at least as hard as decision. Using the aforementioned general recipe, it follows immediately that evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7B%5Ctextrm%7BOV%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{\textrm{OV}}" class="latex" /> requires <img src="https://s0.wp.com/latex.php?latex=O%28n%5E%7B2-o%281%29%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(n^{2-o(1)})" class="latex" /> time for average case assuming randomized version of SETH. This result was shown in <a href="https://eprint.iacr.org/2017/202.pdf">[BRSV17a]</a>, and analogously, they constructed a polynomial for <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" />-OV, which implies an average-case time hierarchy assuming randomized SETH.</p>



<p>However, the polynomial evaluation problem is algebraic. Next, let us consider a natural combinatorial problem — counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in an <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" />-vertices graph (for simplicity, think of <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" /> as a large constant). This problem has worst-case complexity <img src="https://s0.wp.com/latex.php?latex=n%5E%7B%5CTheta%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n^{\Theta(t)}" class="latex" /> assuming ETH <a href="https://core.ac.uk/download/pdf/82508832.pdf">[CHKX06]</a>.</p>



<p><strong>Counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques.</strong> It was first shown in <a href="http://www.wisdom.weizmann.ac.il/~oded/R2/gc.pdf">[GR18]</a> that there is an <img src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28n%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\widetilde{O}(n^2)" class="latex" />-time reduction from counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in any graph to counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques with error probability <img src="https://s0.wp.com/latex.php?latex=%3C%5Cfrac%7B1%7D%7B4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="&lt;\frac{1}{4}" class="latex" /> in some polynomial-time sampable random graph. The proof also follows our general recipe. First, we construct a degree-<img src="https://s0.wp.com/latex.php?latex=%5Cbinom%7Bt%7D%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\binom{t}{2}" class="latex" /> polynomial <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28X%29%3A%3D%5Csum_%7B%5Ctextrm%7Bsize-%7Dt%5C%2C+T%5Csubseteq+%5Bn%5D%7D%5Cprod_%7Bi%3Cj%5Cin+T%7D+X_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(X):=\sum_{\textrm{size-}t\, T\subseteq [n]}\prod_{i&lt;j\in T} X_{i,j}" class="latex" /> over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BF%7D_%7Bp%7D%5E%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbf{F}_{p}^{n^2}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=p%3En%5Et&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p&gt;n^t" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="X" class="latex" /> is the adjacency matrix. Observe that <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> counts <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques by enumerating each size-<img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" /> subset of vertices and checking whether it is a <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-clique. It remains to work out the step 4 of the general recipe. This was done by a gadget reduction, that runs in <img src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28p%5E%7Bt%5E2%7Dn%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\widetilde{O}(p^{t^2}n)" class="latex" /> time, from evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=Y%5Cin%5Cmathbf%7BF%7D_%7Bp%7D%5E%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Y\in\mathbf{F}_{p}^{n^2}" class="latex" /> to counting <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques in an <img src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28p%5E%7Bt%5E2%7Dn%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\widetilde{O}(p^{t^2}n)" class="latex" />-vertices graph <a href="http://www.wisdom.weizmann.ac.il/~oded/R2/gc.pdf">[GR18]</a>.</p>



<p>Although this gadget reduction is nice, I will not explain it here, because later works <a href="https://arxiv.org/abs/1903.08247">[BBB19,</a> <a href="https://arxiv.org/abs/2008.06591">DLW20]</a> show that if the polynomial constructed at the step 2 has certain structure, then there is a general technique to reduce evaluating this polynomial back to the original problem (at the cost of requiring smaller error probability of average-case solver), which I will discuss in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">next post</a>. Finally, let me point out an issue in the previous paragraph — <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p" class="latex" /> is too large for the gadget reduction to be useful! We need <img src="https://s0.wp.com/latex.php?latex=p%3En%5Et&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p&gt;n^t" class="latex" /> (note <img src="https://s0.wp.com/latex.php?latex=n%5Et&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n^t" class="latex" /> is a trivial upper bound of the number of <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques) such that <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}" class="latex" /> indeed outputs the number of <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="t" class="latex" />-cliques, but the gadget reduction takes <img src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28p%5E%7Bt%5E2%7Dn%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\widetilde{O}(p^{t^2}n)" class="latex" /> time, and moreover, we do not know how to find a prime <img src="https://s0.wp.com/latex.php?latex=%3En%5Et&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="&gt;n^t" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%5Cwidetilde%7BO%7D%28n%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\widetilde{O}(n^2)" class="latex" /> time. This issue was handled in <a href="http://www.wisdom.weizmann.ac.il/~oded/R2/gc.pdf">[GR18]</a> using <a href="https://en.wikipedia.org/wiki/Chinese_remainder_theorem">Chinese remainder theorem</a>. Specifically, we choose <img src="https://s0.wp.com/latex.php?latex=O%28t%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(t\log n)" class="latex" /> many distinct primes <img src="https://s0.wp.com/latex.php?latex=p_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p_i" class="latex" />‘s upper bounded by <img src="https://s0.wp.com/latex.php?latex=O%28t%5Clog+n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="O(t\log n)" class="latex" /> whose product is <img src="https://s0.wp.com/latex.php?latex=%3En%5Et&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="&gt;n^t" class="latex" /> (the existence of such primes follows from <a href="https://en.wikipedia.org/wiki/Prime_number_theorem">asymptotic distribution of the prime numbers</a>). Then, we apply the whole reduction described so far to evaluating <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28X%29%5Cmod+p_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(X)\mod p_i" class="latex" /> for each <img src="https://s0.wp.com/latex.php?latex=p_i&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p_i" class="latex" /> and then use Chinese remainder theorem to recover <img src="https://s0.wp.com/latex.php?latex=f_%7Bt%5Ctextrm%7B-clique%7D%7D%28X%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="f_{t\textrm{-clique}}(X)" class="latex" />. (We can use Chinese remaindering with errors <a href="https://dl.acm.org/doi/10.1109/18.850672">[GRS99]</a> to handle larger error probability.)</p>



<p>Hopefully, the above examples give you a flavor of worst-case to average-case reductions for fine-grained hard problems. As promised, in the <a href="https://theorydish.blog/2021/07/30/average-case-fine-grained-hardness-part-ii/">next post</a>, I will continue to discuss the new technique in  <a href="https://arxiv.org/abs/1903.08247">[BBB19,</a> <a href="https://arxiv.org/abs/2008.06591">DLW20]</a>, which automatizes the step 4 of the general recipe by requiring more structural properties for the polynomial constructed in the step 2 of the general recipe.</p>



<p><strong>Acknowledgements.</strong> I would like to thank my quals committee — Aviad Rubinstein, Tselil Schramm, Li-Yang Tan for valuable feedback to my quals talk.</p>



<p><br /></p></div>







<p class="date">
by Junyao Zhao <a href="https://theorydish.blog/2021/07/23/average-case-fine-grained-hardness-part-i/"><span class="datestr">at July 23, 2021 01:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=6127">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/laplace-method/">Approximating integrals with Laplace’s method</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">Integrals appear everywhere in all scientific fields, and their numerical computation is an active area of research. In the playbook of approximation techniques, my personal favorite is “la méthode de Laplace”, a must-know for students that like to cut integrals into pieces, that comes with lots of applications.</p>



<p class="justify-text">We will be concerned with integrals of the form $$I(t) =  \int_K h(x) e^{ \, – \, t f(x) } dx, $$ where \(K\) is compact (bounded and closed) subset of \(\mathbb{R}^d\), and \(h\) and \(f\) are two real-valued functions defined on \(K\) such that the integral is well defined for large enough \(t \in \mathbb{R}\). The goal is to obtain an asymptotic equivalent when \(t\) tends to infinity.</p>



<p class="justify-text">Within machine learning, as explained below, this is useful where computing and approximating integrals is important. This thus comes up naturally in Bayesian inference where \(t\) will be the number of observations in a statistical model. But let’s start with presenting this neat asymptotic result that Laplace discovered for a particular one-dimensional case in 1774 [<a href="http://gallica.bnf.fr/ark:/12148/bpt6k77596b/f32">1</a>].</p>



<h2>Laplace approximation</h2>



<p>Let’s first state the main result. Assume that:</p>



<ul class="justify-text"><li>\(h\) is continuous on \(K\) and that \(f\) is twice continuously differentiable on \(K\).</li><li>\(f\) has a strict global minimizer \(x_\ast\) on \(K\), which is in the interior of \(K\), where the gradient \(f^\prime(x_\ast)\) is thus equal to zero, and where the Hessian \(f^{\prime \prime}(x_\ast)\) is a positive definite matrix (it is always positive semi-definite because \(x_\ast\) is a local minimizer of \(f\)); moreover, \(h(x_\ast) \neq 0\).</li></ul>



<p class="justify-text">Then, as \(t\) tends to infinity, we have the following asymptotic equivalent: $$ I(t) \sim \frac{h(x_\ast)}{\sqrt{ \det f^{\prime \prime}(x_\ast) }} \Big( \frac{2\pi}{t}\Big)^{d/2}  e^{ \, – \, t f(x_\ast) }.$$</p>



<p class="justify-text"><strong>Where does it come from?</strong> The idea is quite simple: for \(t&gt;0\), the exponential term \(e^{ \, – \, t f(x) }\) is largest when \(x\) is equal to the minimizer \(x_\ast\). Hence only contributions close to \(x_\ast\) will count in the integral. Then we can do Taylor expansions of the two functions around \(x_\ast\), as \(h(x) \approx h(x_\ast)\) and \(f(x) \approx f(x_\ast) + \frac{1}{2} ( x – x_\ast)^\top f^{\prime \prime}(x_\ast) (x-x_\ast)\), and approximate \(I(t)\) as $$ I(t) \approx \int_K h(x_\ast) \exp\Big[ – t f(x_\ast) \ – \frac{t}{2} ( x – x_\ast)^\top f^{\prime \prime}(x_\ast) (x-x_\ast)\Big] dx.$$ We can then make a change of variable \(y = \sqrt{t} f^{\prime \prime}(x_\ast)^{1/2}( x – x_\ast)\) (where \(f^{\prime \prime}(x_\ast)^{1/2}\) is the positive square root of \(f^{\prime \prime}(x_\ast)\)), to get, with the Jacobian of the transformation leading to the term \(  (\det f^{\prime \prime}(x_\ast) )^{1/2} t^{d/2} \): $$I(t) \approx \frac{ h(x_\ast) e^{ \, – \, t f(x_\ast) }}{( \det f^{\prime \prime}(x_\ast) )^{1/2}t^{d/2} } \int_{\sqrt{t}f^{\prime \prime}(x_\ast)^{1/2}( K \ – x_\ast)} \!\!\! \exp\big[ – \frac{1}{2} y^\top  y  \big] dy.$$ Since \(x_\ast\) is in the interior of \(K\), when \(t\) tends to infinity, the set \(\sqrt{t} f^{\prime \prime}(x_\ast)^{1/2} ( K \ – x_\ast)\) tends to \(\mathbb{R}^d\) (see illustration below), and we obtain the usual Gaussian integral that leads to the normalizing constant of the Gaussian distribution, which is equal to \((2\pi)^{d/2} \).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="544" alt="" src="https://francisbach.com/wp-content/uploads/2021/07/Klaplace-1024x288.png" class="wp-image-6263" height="153" />Left: set \(K\) with \(x_\ast\) in its interior. Right: set \(\sqrt{t} f^{\prime \prime}(x_\ast)^{1/2} ( K \ – x_\ast)\), which is a translated (by \(x_\ast\)), tilted (by \(f^{\prime\prime}(x_\ast)\)) and scaled (by \(\sqrt{t}\)) version of \(K\).</figure></div>



<p class="justify-text"><strong>Formal proof.</strong> Let’s first make our life simpler: without loss of generality, we may assume that \(f(x_\ast) = 0\) (by subtracting the minimal value), that \(f^{\prime \prime}(x_\ast) = I\) (by a change of variable whose Jacobian is the square root of \(f^{\prime \prime}(x_\ast)\), leading to the determinant term), and that \(x_\ast = 0\). With the dominated convergence theorem (which was unfortunately unknown to me when I first learned about the method in high school, forcing students to cut integrals into multiple pieces), the proof sketch above is almost exact. We simply need a simple argument, based on the existence of a continuous function \(g: K \to \mathbb{R}\) such that $$ g(x) = \frac{f(x) } {\|  x  \|^2}$$ for \(x \neq 0\) and \(g(0) = \frac{1}{2}\) (here \(\| \! \cdot \! \|\) denotes the standard Euclidean norm). The function \(g\) is trivially defined and continuous on \(K \backslash \{0\}\), the value and continuity at zero is a simple consequence of the Taylor expansion $$ f(x) = f(0) + f^\prime(0)^\top x + \frac{1}{2} x^\top f^{\prime\prime}(0)x + o( \| x\|^2) = \frac{1}{2} \| x\|^2 + o( \| x\|^2). $$</p>



<p class="justify-text">We may then use the same change of variable as above to get the <em>equality</em>: $$I(t) =  \frac{ 1 }{ t^{d/2}  }  \int_{\sqrt{t}  K } h\big( \frac{1}{\sqrt{t}}  y\big) \exp\big[ – \frac{1}{2} y^\top y \cdot g\big(  \frac{1}{\sqrt{t}} y\big) \big] dy.$$ We can write the integral part of the expression above as $$J(t) = \int_{\mathbb{R}^d} a(y,t) dy, $$ with $$a(y,t) = 1_{ y \in \sqrt{t} K } h\big(  \frac{1}{\sqrt{t}}  y\big)\exp\big[ – \frac{1}{2} y^\top y \cdot g\big( \frac{1}{\sqrt{t}} y\big) \big].$$ We have for all \(t&gt;0\) and \(y \in \mathbb{R}^d\), \(|a(y,t)| \leqslant \max_{z \in K} | h(z)| \exp\Big[ – \| y\|^2 \cdot \min_{ z \in K } g(z) \Big]\), which is integrable because \(h\) is continuous on the compact set \(K\) and thus bounded, and \(g\) is strictly positive on \(K\) (since \(f\) is strictly positive except at zero as \(0\) is a strict global minimum), and by continuity, its minimal value is strictly positive. Thus by the <a href="https://en.wikipedia.org/wiki/Dominated_convergence_theorem">dominated convergence theorem</a>: $$\lim_{t \to +\infty} J(t) = \int_{\mathbb{R}^d} \big( \lim_{t \to +\infty} a(y,t) \big) dy = \int_{\mathbb{R}^d}\exp\big[ – \frac{1}{2} y^\top y \big] dy = ( 2\pi)^{d/2}.$$ This leads to the desired result since \(I(t) = J(t) / t^{d/2}\).</p>



<h2>Classical examples</h2>



<p>Two cute examples are often mentioned as applications (adapted from [2]).</p>



<p><strong><a href="https://en.wikipedia.org/wiki/Stirling%27s_approximation">Stirling’s formula</a></strong>. We have, by definition of the <a href="https://en.wikipedia.org/wiki/Gamma_function">Gamma function</a> \(\Gamma\), and the change of variable \(u = tx\):<br />$$\Gamma(1+t) = \int_0^\infty \!\! e^{-u} u^{t} du = \int_0^\infty \!\! e^{-tx}t^t x^t t dx = t^{t+1} \int_0^\infty \!\! e^{-t(x-\log x)} dx.$$ Since \(x \mapsto x\, – \log x\) is minimized at \(x=1\) with second derivative equal to \(1\), we get: $$\Gamma(1+t) \sim t^{t+1} e^{-t} \sqrt{2\pi / t} = \big( \frac{t}{e} \big)^t \sqrt{ 2\pi t}.,$$ which is exactly Stirling’s formula, often used when \(t\) is an integer, and then, \(\Gamma(1+t) = t!\).</p>



<p class="justify-text"><strong>Convergence of \(L_p\)-norms to the \(L_\infty\)-norm.</strong> We consider the \(L_p\)-norm of a positive twice continuously differentiable function on the compact set \(K\), with a unique global maximizer at \(x_\ast\) in the interior of \(K\). Then we can write its \(L_p\)-norm \(\|g\|_p\) through $$\| g\|_p^p = \int_K g(x)^p dx = \int_K e^{ p \log g(x)} dx.$$ The function \(f: x \mapsto\  – \log g(x)\) has gradient \(f^\prime(x) = \ – \frac{1}{g(x)}g^\prime(x)\) and Hessian \(f^{\prime\prime}(x)=\  – \frac{1}{g(x)} g^{\prime\prime}(x) + \frac{1}{g(x)^2} g^\prime(x) g^\prime(x)^\top .\) At \(x_\ast\), we get \(f^{\prime\prime}(x_\ast) = \ – \frac{1}{g(x_\ast)} g^{\prime \prime}(x_\ast)\). Thus, using Laplace’s method, we have: $$ \|g\|_p^p =  \frac{A}{p^{d/2}} g(x_\ast)^p (1 + o(1) ),$$ with \(\displaystyle A = \frac{(2\pi g(x_\ast))^{d/2}}{(\det (-g^{\prime\prime}(x_\ast)))^{1/2}}\).</p>



<p class="justify-text">Taking the power \(1/p\), we get: $$ \|g\|_p = \exp \big( \frac{1}{p} \log \|g\|_p^p\big) =  \exp \Big( \frac{1}{p} \log A \ – \frac{d}{2p} \log p +   \log g(x_\ast) + o(1/p) \Big).$$ This leads to, using \(\exp(u) = 1+u + o(u)\) around zero: $$ \| g\|_p = g(x_\ast) \Big( 1 – \frac{d}{2p} \log p + \frac{1}{p} \log A + o(1/p) \Big) = g(x_\ast) \Big( 1 – \frac{d}{2p} \log p + O(1/p) \Big).$$ A surprising fact is that the second-order term does not depend on anything but \(g(x_\ast)\) (beyond \(p\) and \(d\)). Note that this applies also to continuous log-sum-exp functions.</p>



<h2>Applications in Bayesian inference</h2>



<p class="justify-text">It turns out that Laplace’s motivation in deriving this general technique for approximating integrals was exactly Bayesian inference, which he in fact essentially re-discovered and extended (see an interesting account <a href="https://ebrary.net/118879/history/laplaces_bayesian_analysis_1774_1781">here</a>). Let me now explain how Laplace’s method applies.</p>



<p class="justify-text">We consider a parameterized family of probability distributions with density \(p(x|\theta)\) with respect to some base measure \(\mu\) on \(x \in \mathcal{X}\), with \(\theta \in \Theta \subset \mathbb{R}^d\) a set of parameters. </p>



<p class="justify-text">As a running example (the one from Laplace in 1774), we consider the classical Bernoulli distribution for \(x \in \mathcal{X} = \{0,1\}\), and densities with respect to the counting measure, that is, the parameter \(\theta \in [0,1]\) is the probability that \(x=1\).</p>



<p class="justify-text"><strong>From frequentist to Bayesian inference. </strong>We are given \(n\) independent and identically distributed observations \(x_1,\dots,x_n \in \mathcal{X}\), from an unknown probability distribution \(q(x)\). One classical goal of statistical inference is to find the parameter \(\theta \in \Theta\) so that \(p(\cdot| \theta)\) is as close to \(q\) as possible.</p>



<p class="justify-text">In the frequentist framework, <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimation</a> amounts to maximizing $$ \sum_{i=1}^n \log p(x_i | \theta)$$ with respect to \(\theta \in \Theta\). It comes with a series of guarantees, in particular (but not only) when \(q = p(\cdot | \theta)\) for a certain \(\theta \in \Theta\). For our running example, the maximum likelihood estimator \(\hat{\theta}_{\rm ML} = \frac{1}{n} \sum_{i=1}^n x_i\) is the frequency of non-zero outcomes.</p>



<p class="justify-text">In the Bayesian framework, the data are assumed to be generated by a certain \(\theta\), but now with \(\theta\) itself random with some prior probability density \(p(\theta)\) (with respect to the Lebesgue measure). Statistical inference typically does not lead to a point estimator (like the maximum likelihood estimator), but to the full posterior distribution of the parameter \(\theta\) given the observations \(x_1,\dots,x_n\).</p>



<p class="justify-text">The celebrated <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’ rule</a> states that the posterior density \(p(\theta | x_1,\dots,x_n)\) can be written as: $$p(\theta | x_1,\dots,x_n) = \frac{ p(\theta) \prod_{i=1}^n p(x_i|\theta) }{p(x_1,\dots,x_n)},$$ where \(p(x_1,\dots,x_n)\) is the marginal density of the data (once the parameter has been marginalized out).</p>



<p class="justify-text">If pressured, a Bayesian will end up giving you a point estimate, but a true Bayesian will not give away the maximum a posteriori estimate (see <a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation#Limitations">here</a> for some reasons why), but if you give him or her a loss function (e.g., the square loss), he or she will give away (reluctantly) the estimate that minimizes the posterior risk (e.g., the posterior mean for the square loss).</p>



<p class="justify-text"><strong>Bernoulli and Beta distributions. </strong>In our running Bernoulli example for coin tossing, it is standard to put a <a href="https://en.wikipedia.org/wiki/Conjugate_prior">conjugate prior</a> on \(\theta \in [0,1]\), which is here a <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a> with parameters \(\alpha\) and \(\beta\), that is, $$p(\theta) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)} \theta^{\alpha-1} ( 1- \theta)^{\beta-1}.$$ The posterior distribution is then also a Beta distribution with parameters \(\alpha + \sum_{i=1}^n x_i\) and \(\beta + \sum_{i=1}^n ( 1- x_i)\). The posterior mean is $$\mathbb{E} [ \theta | x_1,\dots,x_n ] = \frac{\alpha+ \sum_{i=1}^n x_i}{n + \alpha + \beta},$$ and corresponds to having pre-observed \(\alpha\) ones and \(\beta\) zeroes (this is sometimes referred to as Laplace smoothing). However, it is rarely possible to compute the posterior distribution in closed form, hence the need for approximations.</p>



<p class="justify-text"><strong>Laplace approximation. </strong>Thus, in Bayesian inference, integrals of the form $$ \int_{\Theta} h(\theta) p(\theta) \prod_{i=1}^n p(x_i|\theta) d\theta$$ for some function \(h: \Theta \to \mathbb{R}\), are needed. For example, computing the marginal likelihood corresponds to \(h=1\).</p>



<p class="justify-text">By taking logarithms, we can write $$\int_{\Theta} h(\theta) p(\theta) \prod_{i=1}^n p(x_i|\theta) d\theta = \int_{\Theta} h(\theta) \exp\Big(  \log p(\theta) + \sum_{i=1}^n \log p(x_i|\theta) \Big) d\theta, $$ and with \(f_n(\theta) = \ – \frac{1}{n} \log p(\theta) \ – \frac{1}{n} \sum_{i=1}^n \log p(x_i|\theta),\) we have an integral in Laplace form, that is, $$\int_{\Theta} h(\theta) \exp( -n f_n(\theta))d\theta,$$ with a function \(f_n\) that now varies with \(n\). This simple variation does not matter as because of the law of large numbers, when \(n\) is large, \(f_n(\theta)\) tends to a fixed function \(\mathbb{E} \big[ \log p(x|\theta) \big]\). </p>



<p class="justify-text">The Laplace approximation thus requires to compute the minimizer of \(f_n(\theta)\), which is exactly the maximum a posteriori estimate \(\hat{\theta}_{\rm MAP}\), and use the approximation: $$ \int_{\Theta} h(\theta) \exp( -n f_n(\theta))d\theta \approx (2\pi / n)^{d/2} \frac{h( \hat{\theta}_{\rm MAP})}{(\det f_n^{\prime \prime}( \hat{\theta}_{\rm MAP}))^{1/2}} \exp( – n f_n( \hat{\theta}_{\rm MAP})).$$</p>



<p class="justify-text"><strong>Gaussian posterior approximation.</strong> Note that the Laplace approximation exactly corresponds to approximating the log-posterior density by a quadratic form and thus approximating the posterior by a Gaussian distribution with mean \(\hat{\theta}_{\rm MAP}\) and covariance matrix \(\frac{1}{n} f_n^{\prime \prime}( \hat{\theta}_{\rm MAP})^{-1}\). Note that Laplace’s method gives one natural example of such Gaussian approximation and that <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">variational inference</a> can be used to find better ones.</p>



<p class="justify-text"><strong>Example.</strong> We consider the Laplace approximation of a Beta random variable, that is a Gaussian with mean at the mode of the original density and variance equal to the inverse of the second derivative of the log-density. Below, the mean \(\alpha / (\alpha +\beta)\) is set to a constant, while the variance shrinks due to an increasing \(\alpha+\beta\) (which corresponds to the number of observations in the Bayesian interpretation above).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img width="598" alt="" src="https://francisbach.com/wp-content/uploads/2021/07/plot_beta-3.gif" class="wp-image-6335" height="248" />Left: densities. Right: negative log-densities translated so that they have matched first two derivatives at their minimum.</figure></div>



<p class="justify-text">We see above, that for large variances (small \(\alpha +\beta\)), the Gaussian approximation is not tight, while it becomes tighter as the mass gets concentrated around the mode.</p>



<h2>Extensions</h2>



<p class="justify-text"><strong>High-order expansion.</strong> The approximation is based on Taylor expansions of the functions \(h\) (order \(0\)) and \(f\) (order \(2\)). In order to obtain extra terms of the form \(t^{d/2+\nu}\), for \(\nu\) a positive integer, we need higher-order derivatives of \(h\) and \(f\). In more than one dimension, that quickly gets complicated (see, e.g., [3, 4]).</p>



<p class="justify-text">One particular case which is interesting in one dimension is \(h(x) \sim A ( x- x_\ast)^\alpha\), and \(f(x)-f(x_\ast) = B|x-x_\ast|^{\beta}\). Note that \(\alpha=0\) and \(\beta=2\) corresponds to the regular case. A short calculation gives the equivalent \(I(t) \sim \frac{2}{2+\beta}  \frac{A \Gamma\big( \frac{\alpha+1}{\beta} \big)}{(tB)^{\frac{\alpha+1}{\beta}}}\).</p>



<p class="justify-text"><strong><a href="https://en.wikipedia.org/wiki/Stationary_phase_approximation">Stationary phase approximation</a>.</strong> We can also consider integrals of the form $$I(t) = \int_K h(x) \exp( – i t f(x) ) dx,$$ where \(i\) is the usual square root of \(-1\). Here, the main contribution also comes from vectors \(x\) where the gradient of \(f\) is zero. This can be further extended to more general <a href="https://en.wikipedia.org/wiki/Method_of_steepest_descent">complex integrals</a>.</p>



<h2>When Napoléon meets Laplace and Lagrange</h2>



<p class="justify-text">As a conclusion, I cannot resist mentioning a <a href="https://en.wikipedia.org/wiki/Pierre-Simon_Laplace#I_had_no_need_of_that_hypothesis">classical (potentially not totally authentic) anecdote</a> about encounters between Laplace and Lagrange, two mathematical heroes of mine, and Napoléon, as described in [<a href="https://www.gutenberg.org/files/31246/31246-pdf.pdf">5</a>, page 343]:</p>



<blockquote class="wp-block-quote justify-text"><p>Laplace went in state to beg Napoleon to accept a copy of his work, and the following account of the interview is well authenticated, and so characteristic of all the parties concerned that I quote it in full. Someone had told Napoleon that the book contained no mention of the name of God; Napoleon, who was fond of putting embarrassing questions, received it with the remark, “M. Laplace, they tell me you have written this large book on the system of the universe, and have never even mentioned its Creator.” Laplace, who, though the most supple of politicians, was as stiff as a martyr on every point of his philosophy, drew himself up and answered bluntly, “Je n’avais pas besoin de cette hypothèse-là.” Napoleon, greatly amused, told this reply to Lagrange, who exclaimed, “Ah! c’est une belle hypothèse; ça explique beaucoup de choses.”</p><cite>W. W. Rouse Ball, A Short Account of the History of Mathematics, 1888.</cite></blockquote>



<h2>References</h2>



<p class="justify-text">[1] Pierre-Simon Laplace. <a href="http://gallica.bnf.fr/ark:/12148/bpt6k77596b/f32">Mémoire sur la probabilité des causes par les événements</a>, Mémoires de l’Académie royale des sciences de Paris (Savants étrangers), t. VI. p. 621, 27-65, 1774.<br />[2] Norman Bleistein, Richard A. Handelsman. Asymptotic Expansions of Integrals. Dover<br />Publications, 1986.[3] Stephen M. Stigler. <a href="https://www.jstor.org/stable/pdf/2245475.pdf">Laplace’s 1774 memoir on inverse probability</a>. <em>Statistical Science</em>, 1(3):359-378, 1986.<br />[3] Luke Tierney, Robert E. Kass, Joseph B. Kadane. <a href="https://www.jstor.org/stable/pdf/2289652.pdf">Fully exponential Laplace approximations to expectations and variances of nonpositive functions</a>. <em>Journal of the American Statistical Association</em>, 84(407): 710-716, 1989.<br />[4] Zhenming Shun, Peter McCullagh. <a href="https://www.jstor.org/stable/pdf/2345941.pdf">Laplace approximation of high dimensional integrals</a>. <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 57(4): 749-760, 1995.<br />[5] W. W. Rouse Ball. <a href="https://www.gutenberg.org/files/31246/31246-pdf.pdf">A Short Account of the History of Mathematics</a>, 1888.</p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/laplace-method/"><span class="datestr">at July 23, 2021 06:37 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/108">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/108">TR21-108 |  Limitations of the Impagliazzo--Nisan--Wigderson Pseudorandom Generator against Permutation Branching Programs | 

	Edward Pyne, 

	Salil Vadhan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The classic Impagliazzo--Nisan--Wigderson (INW) psesudorandom generator (PRG) (STOC `94) for space-bounded computation uses a seed of length $O(\log n \cdot \log(nwd/\varepsilon))$ to fool ordered branching programs of length $n$, width $w$, and alphabet size $d$ to within error $\varepsilon$. A series of works have shown that the analysis of the INW generator can be improved for the class of $\textit{permutation}$ branching programs or the more general $\textit{regular}$ branching programs, improving the $O(\log^2 n)$ dependence on the length $n$ to $O(\log n)$ or $\tilde{O}(\log n)$.  However, when also considering the dependence on the other parameters, these analyses still fall short of the optimal PRG seed length $O(\log(nwd/\varepsilon))$.
    
    In this paper, we prove that any ``spectral analysis'' of the INW generator requires seed length $$\Omega\left(\log n\cdot \log\log(\min\{n,d\})+\log n\cdot \log(w/\varepsilon)+\log d\right)$$ to fool ordered permutation branching programs of length $n$, width $w$, and alphabet size $d$ to within error $\varepsilon$.  By ``spectral analysis'' we mean an analysis of the INW generator that relies only on the spectral expansion of the graphs used to construct the generator; this encompasses all prior analyses of the INW generator.  Our lower bound matches the upper bound of Braverman--Rao--Raz--Yehudayoff (FOCS 2010, SICOMP 2014) for regular branching programs of alphabet size $d=2$ except for a gap between their $O(\log n \cdot \log\log n)$ term and our $O(\log n \cdot \log\log \min\{n,d\})$ term.  It also matches the upper bounds of Koucky--Nimbhorkar--Pudlak (STOC 2011), De (CCC 2011), and Steinke (ECCC 2012) for constant-width ($w=O(1)$) permutation branching programs of alphabet size $d=2$ to within a constant factor.  
    
    To fool permutation branching programs in the stronger measure of $\textit{spectral norm}$, we prove that any spectral analysis of the INW generator requires a seed of length $\Omega(\log n\cdot \log\log n+\log n\cdot\log(1/\varepsilon)+\log d)$ when the width is at least polynomial in $n$ ($w=n^{\Omega(1)}$), matching the recent upper bound of Hoza--Pyne--Vadhan (ITCS `21) to within a constant factor.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/108"><span class="datestr">at July 23, 2021 06:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
