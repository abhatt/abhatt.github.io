<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://www.blogger.com/feeds/25562705/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="https://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://benjamin-recht.github.io/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="http://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc ‚Äì QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="http://www.minimizingregret.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="no data">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science ‚Äì Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs ‚Äì Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://www.blogger.com/feeds/21224994/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://www.blogger.com/feeds/27705661/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://www.blogger.com/feeds/32902056/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" class="message" title="internal server error">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="G√∂del's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I‚Äôm a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://scottaaronson.blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Caf√©: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://www.blogger.com/feeds/6555947/posts/default?alt=atom&amp;redirect=false" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at July 20, 2022 12:39 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2207.07949">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2207.07949">A Nearly Tight Analysis of Greedy k-means++</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grunau:Christoph.html">Christoph Grunau</a>, Ahmet Alper √ñz√ºdoƒüru, V√°clav Rozho≈à, Jakub Tƒõtek <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2207.07949">PDF</a><br /><b>Abstract: </b>The famous $k$-means++ algorithm of Arthur and Vassilvitskii [SODA 2007] is
the most popular way of solving the $k$-means problem in practice. The
algorithm is very simple: it samples the first center uniformly at random and
each of the following $k-1$ centers is then always sampled proportional to its
squared distance to the closest center so far. Afterward, Lloyd's iterative
algorithm is run. The $k$-means++ algorithm is known to return a $\Theta(\log
k)$ approximate solution in expectation.
</p>
<p>In their seminal work, Arthur and Vassilvitskii [SODA 2007] asked about the
guarantees for its following \emph{greedy} variant: in every step, we sample
$\ell$ candidate centers instead of one and then pick the one that minimizes
the new cost. This is also how $k$-means++ is implemented in e.g. the popular
Scikit-learn library [Pedregosa et al.; JMLR 2011].
</p>
<p>We present nearly matching lower and upper bounds for the greedy $k$-means++:
We prove that it is an $O(\ell^3 \log^3 k)$-approximation algorithm. On the
other hand, we prove a lower bound of $\Omega(\ell^3 \log^3 k / \log^2(\ell\log
k))$. Previously, only an $\Omega(\ell \log k)$ lower bound was known
[Bhattacharya, Eube, R\"oglin, Schmidt; ESA 2020] and there was no known upper
bound.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2207.07949"><span class="datestr">at July 19, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2207.07839">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2207.07839">On Non-Negative Quadratic Programming in Geometric Optimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cheng:Siu=Wing.html">Siu-Wing Cheng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wong:Man_Ting.html">Man Ting Wong</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2207.07839">PDF</a><br /><b>Abstract: </b>We present experimental and theoretical results on a method that applies a
numerical solver iteratively to solve several non-negative quadratic
programming problems in geometric optimization. The method gains efficiency by
exploiting the potential sparsity of the intermediate solutions. We implemented
the method to call quadprog of MATLAB iteratively. In comparison with a single
call of quadprog, we obtain a 10-fold speedup on two proximity graph problems
in $\mathbb{R}^d$ on some public data sets, a 10-fold speedup on the minimum
enclosing ball problem on random points in a unit cube in $\mathbb{R}^d$, and a
5-fold speedup on the polytope distance problem on random points from a cube in
$\mathbb{R}^d$ when the input size is significantly larger than the dimension;
we also obtain a 2-fold or more speedup on deblurring some gray-scale space and
thermal images via non-negative least square. We compare with two minimum
enclosing ball software by G\"{a}rtner and Fischer et al.; for 1000 nearly
cospherical points or random points in a unit cube, the iterative method
overtakes the software by G\"{a}rtner at 20 dimensions and the software by
Fischer et al. at 170 dimensions. In the image deblurring experiments, the
iterative method compares favorably with other software that can solve
non-negative least square, including FISTA with backtracking, SBB, FNNLS, and
lsqnonneg of MATLAB. We analyze theoretically the number of iterations taken by
the iterative scheme to reduce the gap between the current solution value and
the optimum by a factor $e$. Under certain assumptions, we prove a bound
proportional to the square root of the number of variables.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2207.07839"><span class="datestr">at July 19, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2207.07822">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2207.07822">Adaptive Sketches for Robust Regression with Importance Sampling</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mahabadi:Sepideh.html">Sepideh Mahabadi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woodruff:David_P=.html">David P. Woodruff</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Samson.html">Samson Zhou</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2207.07822">PDF</a><br /><b>Abstract: </b>We introduce data structures for solving robust regression through stochastic
gradient descent (SGD) by sampling gradients with probability proportional to
their norm, i.e., importance sampling. Although SGD is widely used for large
scale machine learning, it is well-known for possibly experiencing slow
convergence rates due to the high variance from uniform sampling. On the other
hand, importance sampling can significantly decrease the variance but is
usually difficult to implement because computing the sampling probabilities
requires additional passes over the data, in which case standard gradient
descent (GD) could be used instead. In this paper, we introduce an algorithm
that approximately samples $T$ gradients of dimension $d$ from nearly the
optimal importance sampling distribution for a robust regression problem over
$n$ rows. Thus our algorithm effectively runs $T$ steps of SGD with importance
sampling while using sublinear space and just making a single pass over the
data. Our techniques also extend to performing importance sampling for
second-order optimization.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2207.07822"><span class="datestr">at July 19, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2207.07809">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2207.07809">Curve Simplification and Clustering under Fr\'echet Distance</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cheng:Siu=Wing.html">Siu-Wing Cheng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Haoqiang.html">Haoqiang Huang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2207.07809">PDF</a><br /><b>Abstract: </b>We present new approximation results on curve simplification and clustering
under Fr\'echet distance. Let $T = \{\tau_i : i \in [n] \}$ be polygonal curves
in $R^d$ of $m$ vertices each. Let $l$ be any integer from $[m]$. We study a
generalized curve simplification problem: given error bounds $\delta_i &gt; 0$ for
$i \in [n]$, find a curve $\sigma$ of at most $l$ vertices such that
$d_F(\sigma,\tau_i) \le \delta_i$ for $i \in [n]$. We present an algorithm that
returns a null output or a curve $\sigma$ of at most $l$ vertices such that
$d_F(\sigma,\tau_i) \le \delta_i + \epsilon\delta_{\max}$ for $i \in [n]$,
where $\delta_{\max} = \max_{i \in [n]} \delta_i$. If the output is null, there
is no curve of at most $l$ vertices within a Fr\'echet distance of $\delta_i$
from $\tau_i$ for $i \in [n]$. The running time is $\tilde{O}\bigl(n^{O(l)}
m^{O(l^2)} (dl/\epsilon)^{O(dl)}\bigr)$. This algorithm yields the first
polynomial-time bicriteria approximation scheme to simplify a curve $\tau$ to
another curve $\sigma$, where the vertices of $\sigma$ can be anywhere in
$R^d$, so that $d_F(\sigma,\tau) \le (1+\epsilon)\delta$ and $|\sigma| \le
(1+\alpha) \min\{|c| : d_F(c,\tau) \le \delta\}$ for any given $\delta &gt; 0$ and
any fixed $\alpha, \epsilon \in (0,1)$. The running time is
$\tilde{O}\bigl(m^{O(1/\alpha)} (d/(\alpha\epsilon))^{O(d/\alpha)}\bigr)$.
</p>
<p>By combining our technique with some previous results in the literature, we
obtain an approximation algorithm for $(k,l)$-median clustering. Given $T$, it
computes a set $\Sigma$ of $k$ curves, each of $l$ vertices, such that $\sum_{i
\in [n]} \min_{\sigma \in \Sigma} d_F(\sigma,\tau_i)$ is within a factor
$1+\epsilon$ of the optimum with probability at least $1-\mu$ for any given
$\mu, \epsilon \in (0,1)$. The running time is $\tilde{O}\bigl(n m^{O(kl^2)}
\mu^{-O(kl)} (dkl/\epsilon)^{O((dkl/\epsilon)\log(1/\mu))}\bigr)$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2207.07809"><span class="datestr">at July 19, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2207.07696">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2207.07696">Algorithmic Determination of the Combinatorial Structure of the Linear Regions of ReLU Neural Networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Masden:Marissa.html">Marissa Masden</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2207.07696">PDF</a><br /><b>Abstract: </b>We algorithmically determine the regions and facets of all dimensions of the
canonical polyhedral complex, the universal object into which a ReLU network
decomposes its input space. We show that the locations of the vertices of the
canonical polyhedral complex along with their signs with respect to layer maps
determine the full facet structure across all dimensions. We present an
algorithm which calculates this full combinatorial structure, making use of our
theorems that the dual complex to the canonical polyhedral complex is cubical
and it possesses a multiplication compatible with its facet structure. The
resulting algorithm is numerically stable, polynomial time in the number of
intermediate neurons, and obtains accurate information across all dimensions.
This permits us to obtain, for example, the true topology of the decision
boundaries of networks with low-dimensional inputs. We run empirics on such
networks at initialization, finding that width alone does not increase observed
topology, but width in the presence of depth does. Source code for our
algorithms is accessible online at https://github.com/mmasden/canonicalpoly.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2207.07696"><span class="datestr">at July 19, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://scottaaronson.blog/?p=6576">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://scottaaronson.blog/?p=6576">A low-tech solution</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Thanks so much to everyone who offered help and support as this blog‚Äôs comment section endured the weirdest, most motivated and sophisticated troll attack in its 17-year history.  For a week, a parade of self-assured commenters showed up to demand that I explain and defend my personal hygiene, private thoughts, sexual preferences, and behavior around female students (and, absurdly, to cajole me into taking my family on a specific Disney cruise ship).  In many cases, the troll or trolls <em>appropriated the names and email addresses of real academics</em>, imitating them so convincingly that those academics‚Äô closest colleagues told me they were confident it was really them.  And when some trolls finally ‚Äúouted‚Äù themselves, I had no way to know whether that was just another chapter in the trolling campaign.  It was enough to precipitate an epistemic crisis, where one actively doubts the authenticity of just about <em>every</em> piece of text.</p>



<p>The irony isn‚Äôt lost on me that I‚Äôve endured this just as I‚Äôm <a href="https://scottaaronson.blog/?p=6484">starting my year-long gig at OpenAI</a>, to think, among other things, about the potential avenues for misuse of Large Language Models like GPT-3, and what theoretical computer science could contribute to mitigating them.  To say this episode has given me a more vivid understanding of the risks would be an understatement.</p>



<p><strong><em>But why didn‚Äôt I just block and ignore the trolls immediately?  Why did I bother engaging?</em></strong>  </p>



<p>At least a hundred people asked some variant of this question, and the answer is this.  For most of my professional life, this blog has been my forum, where anyone in the world could show up to raise any issue they wanted, as if we were tunic-wearing philosophers in the Athenian agora.  I prided myself on my refusal to take the coward‚Äôs way out and ignore anything‚Äîeven, <em>especially</em>, severe personal criticism.  I‚Äôd witnessed how Jon Stewart, let‚Äôs say, would night after night completely eviscerate George W. Bush, his policies and worldview and way of speaking and justifications and lies, and then Bush would just continue the next day, totally oblivious, never deigning to rebut any of it.  And it became a core part of my identity that I‚Äôd never be like that.  If anyone on earth had a narrative of me where I was an arrogant bigot, a clueless idiot, etc., I‚Äôd confront that narrative head-on and refute it‚Äîor if I couldn‚Äôt, I‚Äôd reinvent my whole life.  What I‚Äôd <em>never</em> do is suffer anyone‚Äôs monstrous caricature of me to strut around the Internet unchallenged, as if conceding that only my academic prestige or tenure or power, rather than a reasoned rebuttal, could protect me from the harsh truths that the caricature revealed.</p>



<p>Over the years, of course, I carved out some exceptions: P=NP provers and quantum mechanics deniers enraged that I‚Äôd dismissed their world-changing insights.  Raving antisemites.  <em>Their</em> caricatures of me had no legs in any community I cared about.  But if an attack carried the implied backing of the whole modern social-justice movement, of thousands of angry grad students on Twitter, of <em>Slate</em> and <em>Salon</em> and <em>New York Times</em> writers and Wikipedia editors and university DEI offices, then the coward‚Äôs way out was closed.  The monstrous caricature then loomed directly over me; I could either parry his attacks or die.</p>



<p>With this stance, you might say, the astounding part is not that this blog‚Äôs ‚Äúagora‚Äù model eventually broke down, but rather that it survived for so long!  I started blogging in October 2005.  It took until July 2022 for me to endure a full-scale ‚Äúsocial/emotional denial of service attack‚Äù (not counting the comment-171 affair).  Now that I have, though, it‚Äôs obvious even to me that the old way is no longer tenable.</p>



<p>So what‚Äôs the solution?  Some of you liked the idea of requiring registration with real email addresses‚Äîbut alas, when I tried to implement that, I found that WordPress‚Äôs registration system is a mess and I couldn‚Äôt see how to make it work.  Others liked the idea of moving to Substack, but others actively hated it, and in any case, even if I moved, I‚Äôd <em>still</em> have to figure out a comment policy!  Still others liked the idea of an army of volunteer moderators.  At least ten people volunteered themselves.</p>



<p>On reflection, the following strikes me as most directly addressing the actual problem.  I‚Äôm hereby establishing the <strong>Shtetl-Optimized Committee of Guardians</strong>, or SOCG (same acronym as the <a href="https://cse.buffalo.edu/socg21/socg.html">computational geometry conference</a> <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f642.png" style="height: 1em;" class="wp-smiley" alt="üôÇ" /> ).  If you‚Äôre interested in joining, shoot me an email, or leave a comment on this post with your (real!) email address.  I‚Äôll accept members only if I know them in real life, personally or by reputation, or if they have an honorable history on this blog.</p>



<p>For now, the SOCG‚Äôs only job is this: whenever I get a comment that gives me a feeling of unease‚Äîbecause, e.g., it seems trollish or nasty or insincere, it asks a too-personal question, or it challenges me to rebut a hostile caricature of myself‚ÄîI‚Äôll email the comment to the SOCG and ask what to do.  I precommit to respecting the verdict of those SOCG members who respond, whenever a clear verdict exists.  The verdict could be, e.g., ‚Äúthis seems fine,‚Äù ‚Äúif you won‚Äôt be able to resist responding then don‚Äôt let this appear,‚Äù or ‚Äúemail the commenter first to confirm their identity.‚Äù  And if I simply need reassurance that the commenter‚Äôs view of me is false, I‚Äôll seek it from the SOCG before I seek it from the whole world.</p>



<p>Here‚Äôs what SOCG members can expect in return: I continue pouring my heart into this subscription-free, ad-free blog, and I credit you for making it possible‚Äîpublicly if you‚Äôre comfortable with your name being listed, privately if not.  I buy you a fancy lunch or dinner if we‚Äôre ever in the same town.</p>



<p>Eventually, we might move to a model where the SOCG members can log in to WordPress and directly moderate comments themselves.  But let‚Äôs try it this way first and see if it works.</p></div>







<p class="date">
by Scott <a href="https://scottaaronson.blog/?p=6576"><span class="datestr">at July 19, 2022 06:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-4813311592001744524">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/07/an-open-question-about-sequence-mod-m.html">An open question about a sequence mod M.</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this post n/2 means floor{n/2}<div><br /></div><div>Consider the recurrence</div><div><br /></div><div><br /></div><div>a_1=1</div><div><br /></div><div>for all n\ge 2, a_n = a_{n-1} + a_{n/2}.</div><div><br /></div><div>For which M does this recurrence have infinitely many n such that a_n \equiv¬† 0 mod M?</div><div><br /></div><div><br /></div><div>I have written an open problems column on this for SIGACT News which also says</div><div>what is known (or at least what I know is known).¬† It will appear in the next issue.</div><div><br /></div><div>I will post that open problems column here on my next post.</div><div><br />Until then¬† I would like you to work on it, untainted by what I know.¬†</div><div><br /></div><div><br /></div><div><br /></div><div><br /></div></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/07/an-open-question-about-sequence-mod-m.html"><span class="datestr">at July 19, 2022 02:52 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2207.08120">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2207.08120">On the Practical Power of Automata in Pattern Matching</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Amir:Ora.html">Ora Amir</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Amir:Amihood.html">Amihood Amir</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fraenkel:Aviezri.html">Aviezri Fraenkel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sarne:David.html">David Sarne</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2207.08120">PDF</a><br /><b>Abstract: </b>The classical pattern matching paradigm is that of seeking occurrences of one
string - the pattern, in another - the text, where both strings are drawn from
an alphabet set $\Sigma$. Assuming the text length is $n$ and the pattern
length is $m$, this problem can naively be solved in time $O(nm)$. In Knuth,
Morris and Pratt's seminal paper of 1977, an automaton, was developed that
allows solving this problem in time $O(n)$ for any alphabet.
</p>
<p>This automaton, which we will refer to as the {\em KMP-automaton}, has proven
useful in solving many other problems. A notable example is the {\em
parameterized pattern matching} model. In this model, a consistent renaming of
symbols from $\Sigma$ is allowed in a match. The parameterized matching
paradigm has proven useful in problems in software engineering, computer
vision, and other applications.
</p>
<p>It has long been suspected that for texts where the symbols are uniformly
random, the naive algorithm will perform as well as the KMP algorithm. In this
paper we examine the practical efficiency of the KMP algorithm vs. the naive
algorithm on a randomly generated text. We analyse the time under various
parameters, such as alphabet size, pattern length, and the distribution of
pattern occurrences in the text. We do this for both the original exact
matching problem and parameterized matching. While the folklore wisdom is
vindicated by these findings for the exact matching case, surprisingly, the KMP
algorithm works significantly faster than the naive in the parameterized
matching case.
</p>
<p>We check this hypothesis for DNA texts, and observe a similar behaviour as in
the random text. We also show a very structured case where the automaton is
much more efficient.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2207.08120"><span class="datestr">at July 19, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2207.08075">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2207.08075">Streaming Algorithms with Large Approximation Factors</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Yi.html">Yi Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lin:Honghao.html">Honghao Lin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woodruff:David_P=.html">David P. Woodruff</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Yuheng.html">Yuheng Zhang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2207.08075">PDF</a><br /><b>Abstract: </b>We initiate a broad study of classical problems in the streaming model with
insertions and deletions in the setting where we allow the approximation factor
$\alpha$ to be much larger than $1$. Such algorithms can use significantly less
memory than the usual setting for which $\alpha = 1+\epsilon$ for an $\epsilon
\in (0,1)$. We study large approximations for a number of problems in sketching
and streaming and the following are some of our results.
</p>
<p>For the $\ell_p$ norm/quasinorm $\|x\|_p$ of an $n$-dimensional vector $x$,
$0 &lt; p \le 2$, we show that obtaining a $\poly(n)$-approximation requires the
same amount of memory as obtaining an $O(1)$-approximation for any $M =
n^{\Theta(1)}$.
</p>
<p>For estimating the $\ell_p$ norm, $p &gt; 2$, we show an upper bound of
$O(n^{1-2/p} (\log n \allowbreak \log M)/\alpha^{2})$ bits for an
$\alpha$-approximation, and give a matching lower bound, for almost the full
range of $\alpha \geq 1$ for linear sketches.
</p>
<p>For the $\ell_2$-heavy hitters problem, we show that the known lower bound of
$\Omega(k \log n\log M)$ bits for identifying $(1/k)$-heavy hitters holds even
if we are allowed to output items that are $1/(\alpha k)$-heavy, for almost the
full range of $\alpha$, provided the algorithm succeeds with probability
$1-O(1/n)$. We also obtain a lower bound for linear sketches that is tight even
for constant probability algorithms.
</p>
<p>For estimating the number $\ell_0$ of distinct elements, we give an
$n^{1/t}$-approximation algorithm using $O(t\log \log M)$ bits of space, as
well as a lower bound of $\Omega(t)$ bits, both excluding the storage of random
bits.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2207.08075"><span class="datestr">at July 19, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2207.08015">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2207.08015">Collaborative Best Arm Identification with Limited Communication on Non-IID Data</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Karpov:Nikolai.html">Nikolai Karpov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Qin.html">Qin Zhang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2207.08015">PDF</a><br /><b>Abstract: </b>In this paper, we study the tradeoffs between time-speedup and the number of
communication rounds of the learning process in the collaborative learning
model on non-IID data, where multiple agents interact with possibly different
environments and they want to learn an objective in the aggregated environment.
We use a basic problem in bandit theory called best arm identification in
multi-armed bandits as a vehicle to deliver the following conceptual message:
</p>
<p>Collaborative learning on non-IID data is provably more difficult than that
on IID data.
</p>
<p>In particular, we show the following:
</p>
<p>a) The speedup in the non-IID data setting can be less than $1$ (that is, a
slowdown). When the number of rounds $R = O(1)$, we will need at least a
polynomial number of agents (in terms of the number of arms) to achieve a
speedup greater than $1$. This is in sharp contrast with the IID data setting,
in which the speedup is always at least $1$ when $R \ge 2$ regardless of number
of agents.
</p>
<p>b) Adaptivity in the learning process cannot help much in the non-IID data
setting. This is in sharp contrast with the IID data setting, in which to
achieve the same speedup, the best non-adaptive algorithm requires a
significantly larger number of rounds than the best adaptive algorithm.
</p>
<p>In the technique space, we have further developed the generalized round
elimination technique introduced in <a href="http://export.arxiv.org/abs/1904.03293">arXiv:1904.03293</a>. We show that implicit
representations of distribution classes can be very useful when working with
complex hard input distributions and proving lower bounds directly for adaptive
algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2207.08015"><span class="datestr">at July 19, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2207.07983">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2207.07983">New and improved approximation algorithms for Steiner Tree Augmentation Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>R. Ravi, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Weizhong.html">Weizhong Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zlatin:Michael.html">Michael Zlatin</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2207.07983">PDF</a><br /><b>Abstract: </b>In the Steiner Tree Augmentation Problem (STAP), we are given a graph $G =
(V,E)$, a set of terminals $R \subseteq V$, and a Steiner tree $T$ spanning
$R$. The edges $L := E \setminus E(T)$ are called links and have non-negative
costs. The goal is to augment $T$ by adding a minimum cost set of links, so
that there are 2 edge-disjoint paths between each pair of vertices in $R$. This
problem is a special case of the Survivable Network Design Problem which can be
approximated to within a factor of 2 using iterative rounding \cite{J2001}.
</p>
<p>We give the first polynomial time algorithm for STAP with approximation ratio
better than 2. In particular we achieve a ratio of $(1+ \ln 2 + \varepsilon)
\approx 1.69 + \varepsilon$. To do this, we use the Local Greedy approach of
\cite{TZ2021} for the Tree Augmentation Problem and generalize their main
decomposition theorem from links (of size two) to hyper-links.
</p>
<p>We also consider the Node-Weighted Steiner Tree Augmentation Problem
(NW-STAP) in which the non-terminal nodes have non-negative costs. We seek a
cheapest subset $S \subseteq V \setminus R$ so that $G[R \cup S]$ is
2-edge-connected. We provide a $O(\log^2 (|R|))$-approximation algorithm for
NW-STAP. To do this, we use a greedy algorithm leveraging the spider
decomposition of optimal solutions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2207.07983"><span class="datestr">at July 19, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2207.07974">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2207.07974">Online Prediction in Sub-linear Space</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peng:Binghui.html">Binghui Peng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Fred.html">Fred Zhang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2207.07974">PDF</a><br /><b>Abstract: </b>We provide the first sub-linear space and sub-linear regret algorithm for
online learning with expert advice (against an oblivious adversary), addressing
an open question raised recently by Srinivas, Woodruff, Xu and Zhou (STOC
2022). We also demonstrate a separation between oblivious and (strong) adaptive
adversaries by proving a linear memory lower bound of any sub-linear regret
algorithm against an adaptive adversary. Our algorithm is based on a novel pool
selection procedure that bypasses the traditional wisdom of leader selection
for online learning, and a generic reduction that transforms any weakly
sub-linear regret $o(T)$ algorithm to $T^{1-\alpha}$ regret algorithm, which
may be of independent interest. Our lower bound utilizes the connection of
no-regret learning and equilibrium computation in zero-sum games, leading to a
proof of a strong lower bound against an adaptive adversary.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2207.07974"><span class="datestr">at July 19, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2207.07708">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2207.07708">Approximating Highly Inapproximable Problems on Graphs of Bounded Twin-Width</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Berg=eacute=:Pierre.html">Pierre Berg√©</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bonnet:=Eacute=douard.html">√âdouard Bonnet</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/D=eacute=pr=eacute=s:Hugues.html">Hugues D√©pr√©s</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Watrigant:R=eacute=mi.html">R√©mi Watrigant</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2207.07708">PDF</a><br /><b>Abstract: </b>For any $\varepsilon &gt; 0$, we give a polynomial-time
$n^\varepsilon$-approximation algorithm for Max Independent Set in graphs of
bounded twin-width given with an $O(1)$-sequence. This result is derived from
the following time-approximation trade-off: We establish an
$O(1)^{2^q-1}$-approximation algorithm running in time $\exp(O_q(n^{2^{-q}}))$,
for every integer $q \geqslant 0$. Guided by the same framework, we obtain
similar approximation algorithms for Min Coloring and Max Induced Matching. In
general graphs, all these problems are known to be highly inapproximable: for
any $\varepsilon &gt; 0$, a polynomial-time $n^{1-\varepsilon}$-approximation for
any of them would imply that P$=$NP [Hastad, FOCS '96; Zuckerman, ToC '07;
Chalermsook et al., SODA '13]. We generalize the algorithms for Max Independent
Set and Max Induced Matching to the independent (induced) packing of any fixed
connected graph $H$. In contrast, we show that such approximation guarantees on
graphs of bounded twin-width given with an $O(1)$-sequence are very unlikely
for Min Independent Dominating Set, and somewhat unlikely for Longest Path and
Longest Induced Path. Regarding the existence of better approximation
algorithms, there is a (very) light evidence that the obtained approximation
factor of $n^\varepsilon$ for Max Independent Set may be best possible. This is
the first in-depth study of the approximability of problems in graphs of
bounded twin-width. Prior to this paper, essentially the only such result was
a~polynomial-time $O(1)$-approximation algorithm for Min Dominating Set [Bonnet
et al., ICALP '21].
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2207.07708"><span class="datestr">at July 19, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/105">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/105">TR22-105 |  On vanishing sums of roots of unity in polynomial calculus and sum-of-squares | 

	Ilario Bonacina, 

	Nicola Galesi, 

	Massimo Lauria</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Vanishing sums of roots of unity can be seen as a natural generalization of knapsack from Boolean variables to variables taking values over the roots of unity. We show that these sums are hard to prove for polynomial calculus and for sum-of-squares, both in terms of degree and size.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/105"><span class="datestr">at July 18, 2022 01:04 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/104">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/104">TR22-104 |  On One-Sided Testing Affine Subspaces | 

	Nader Bshouty</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study the query complexity of one-sided $\epsilon$-testing the class of Boolean functions $f:F^n\to \{0,1\}$ that describe affine subspaces and Boolean functions that describe axis-parallel affine subspaces, where $F$ is any finite field. We give a polynomial-time $\epsilon$-testers that ask $\tilde O(1/\epsilon)$ queries. This improves the query complexity $\tilde O(|F|/\epsilon)$ in~[16]. 

We then show that any one-sided $\epsilon$-tester with proximity parameter $\epsilon&lt;1/|F|^d$ for the class of Boolean functions that describe $(n-d)$-dimensional affine subspaces and Boolean functions that describe axis-parallel $(n-d)$-dimensional affine subspaces must make at least
$\Omega(1/\epsilon+|F|^{d-1}\log n)$ and $\Omega(1/\epsilon+|F|^{d-1}n)$ queries, respectively.
This improves the lower bound $\Omega(\log n/\log\log n)$  that is proved in~[16] for $F=GF(2)$. We also give  testers for those classes with query complexity that almost match the lower bounds.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/104"><span class="datestr">at July 18, 2022 12:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=20239">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2022/07/18/complexity-2022/">Complexity 2022</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="G√∂del's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>Weaving patterns of proof and the accepted papers for this week‚Äôs conference</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2022/07/18/complexity-2022/karendonde/" rel="attachment wp-att-20241"><img width="151" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/07/KarenDonde.jpg?resize=151%2C159&amp;ssl=1" class="alignright wp-image-20241" height="159" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><a href="https://karendondehandwovens.com/page/1-Statement-Bio.html">her bio page</a></font></td>
</tr>
</tbody>
</table>
<p>
Karen Donde is the Chair of <a href="https://complexityexhibition.org/">Complexity 2022</a>, which is being held this month in Knoxville, Tennessee. This is not the same as the <a href="https://computationalcomplexity.org/Archive/2022/fullsite/">Computational Complexity 2022</a> (CCC22) conference, which is being held <b>in-person</b> at the University of Pennsylvania this <b>Wednesday, July 20</b>, through <b>Saturday, July 23</b>. The Knoxville event is not about computer science, nor dynamical nor biological complexity. It is about the art of weaving complex patterns in textiles by hand.</p>
<p>
Today we collect pointers to the papers at CCC22 after saying something separate about weaving and proofs.</p>
<p>
Unlike CCC22, the Knoxville exhibition is also <a href="https://complexityexhibition.org/all-works/">open online</a>. Here is a detail from the Complex Weaver first prize <a href="https://complexityexhibition.org/melanie-olde-morphology-i/">winner</a>, an example of three-dimensional weaving:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/07/18/complexity-2022/complexweavingprize/" rel="attachment wp-att-20242"><img width="250" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/07/ComplexWeavingPrize.jpg?resize=250%2C270&amp;ssl=1" class="aligncenter wp-image-20242" height="270" /></a></p>
<p>
Like CCC22, the Knoxville event has a program committee. It consists of <a href="http://www.juliehedges.co.uk/">Julie Hedges</a>, <a href="https://www.spadystudios.com/">Robyn Spady</a>, and <a href="https://www.bettyvera.com/">Betty Vera</a>. Also like CCC22, it has a steering committee. Besides Donde, the committee consists of <a href="https://www.etsy.com/shop/MargepyeTextiles?ref=profile_header">Margaret Dugger</a>, <a href="https://www.pinterest.com/dyen2weave/">Diane Smith</a>, <a href="https://wovenful.com/an-interview-with-sarah-fortin/">Sarah Fortin</a>, <a href="https://pikespeakweavers.org/member-galleries/nggallery/member-galleries/susan-bowman">Susan Bowman</a>, <a href="https://newworldtextiles.com/about/">Eileen Hallman</a>, <a href="https://www.woodlandsgallerync.com/artists/pat-brown">Pat Brown</a>, <a href="https://www.facebook.com/katiedoanhandweaver/">Katie Doan</a>, <a href="https://www.weavingschool.com/Geri.html">Geri Forkner</a>, <a href="https://www.linkedin.com/in/cathy-mccarthy-42347828/">Cathy McCarthy</a>, <a href="http://www.spinningforth.com/perso/perso.html">Ruth MacGregor</a>, and <a href="https://weavingspace.co.uk/#About-Weaving-Space">Cally Booker</a>. The gender imbalance is more extreme than for CCC22 or what we <a href="https://rjlipton.wpcomstaging.com/2021/11/13/popl-2022-et-tu-brute/">noted</a> last fall for POPL22 (also see end of <a href="https://rjlipton.wpcomstaging.com/2021/11/24/best-to-dean-mynatt/">this</a>). Oh well.</p>
<p>
</p><p></p><h2> Weaving Into Theory </h2><p></p>
<p></p><p>
Karen Donde also writes a <a href="https://karen63615.wixsite.com/karendondeblog">blog</a>, <em>Speaking of Weaving</em>. The blog has numerous technical how-to articles. In some places they verge on mathematical theory. </p>
<p>
There are more express connections between mathematics and weaving. The mathematics teacher <a href="http://www.patrickhonner.com/about.html">Patrick Honner</a> has a <a href="https://mrhonner.com/weaving">page</a> of posts on weaving. He was featured in an <a href="https://naturalmath.com/2012/07/weaving-mathematics/">article</a> ‚ÄúWeaving your way through mathematics‚Äù by the mathematics educator Maria Droujkova. See also a <a href="https://www.youtube.com/watch?v=Breul3cnW9s">video</a> on weaving and the mathematics of <a href="https://en.wikipedia.org/wiki/Spirograph">Spirograph</a> patterns.</p>
<p>
On the computing theory side, connections to cellular automata are shown in a 2017 <a href="https://www.semanticscholar.org/paper/The-Complexity-of-Braids,-Cables,-and-Weaves-with-Holden/5b83e108623548c8b073a1e96b00d027eefb197e">paper</a> by Joshua Holden. There is also a recent <a href="https://dl.acm.org/doi/fullHtml/10.1145/3411764.3445750">paper</a> from CMU‚Äôs Human-Computer Interaction Institute on ‚ÄúEnabling Personal Computational Handweaving with a Low-Cost Jacquard Loom.‚Äù </p>
<p>
</p><p></p><h2> Weaving Into Proofs </h2><p></p>
<p></p><p>
Our association to weaving was really motivated, however, by an <a href="https://www.quantamagazine.org/how-do-mathematicians-know-their-proofs-are-correct-20220713/">article</a> last Wednesday by Steven Strogatz, a Cornell mathematician and extraordinary popularizer whom I (Ken) have known since we were undergraduates together at Princeton. The article interviews the Harvard mathematician Melanie Matchett Wood and is titled, ‚ÄúHow Do Mathematicians Know Their Proofs Are Correct?‚Äù</p>
<p>
We have written about proofs and the social issue of verification <a href="https://rjlipton.wpcomstaging.com/2022/06/13/sorting-and-proving/">several</a> <a href="https://rjlipton.wpcomstaging.com/2022/05/23/hilberts-lost-problem/">times</a> <a href="https://rjlipton.wpcomstaging.com/2020/12/10/the-future-of-mathematics/">recently</a>. But this article goes into a more particular topic that we tried to get at in a <a href="https://rjlipton.wpcomstaging.com/2014/09/09/a-challenge-from-dyson/">series</a> of <a href="https://rjlipton.wpcomstaging.com/2014/06/06/is-this-a-proof-2/">posts</a> in <a href="https://rjlipton.wpcomstaging.com/2014/10/11/more-on-testing-dysons-conjecture/">2014</a>. This is about whether probabilistic modeling‚Äînot the <a href="https://en.wikipedia.org/wiki/Probabilistic_method">Probabilistic Method</a> which is airtight‚Äîcan give confidence in conjectures that is tantamount to proof.</p>
<p>
Strogatz‚Äôs interview leads off with a reference to a 2019 <a href="https://www.quantamagazine.org/where-proof-evidence-and-imagination-intersect-in-math-20190314/">article</a> for <em>Quanta</em> titled, ‚ÄúWhere Proof, Evidence, and Imagination Intersect.‚Äù That article is by the same Patrick Honner whom we just mentioned for weaving, and gives caveats of how bias and unrecognized implicit constraints can creep into models, so as to invalidate them. </p>
<p>
Wood begins with basic ‚Äúcoinflip‚Äù random models of primes‚Äîsuch as mentioned in the above-listed posts‚Äîand fixes on a bias-revealing model that we also covered <a href="https://rjlipton.wpcomstaging.com/2016/03/26/bias-in-the-primes/">here</a>. She then describes how they incrementally build rules for adjusting coin weights to compensate for biases introduced by small-number cases: </p>
<blockquote><p><b> </b> <em> ‚ÄúSo the model is something that starts with this coin-flipping model, but then it‚Äôs modified by all these other rules, and all the other things that we know about the primes. And once you put all of those things that we do know into the model, you then ask [it] well, do you see, infinitely often, coins coming up prime just 2 apart? And the model tells you, oh, yes, we do see that. In fact, we see it at this very particular rate we can give you a formula for. And then ‚Ä¶ you see that the model gives you a very accurate prediction for the number of pairs of twin primes you‚Äôll find as you go along. And so then you think, you know, maybe this model knows what it‚Äôs talking about.‚Äù </em>
</p></blockquote>
<p></p><p>
In response to Strogatz noting that the accuracy must be judged by long computer runs, Wood is quick to emphasize that the rules given to the model are determined manually:</p>
<blockquote><p><b> </b> <em> ‚ÄúBut for building this model and coming up with the formula that the model gives. You know, that‚Äôs done by hand, essentially, by mathematicians thinking about the model and figuring out with it. ‚Ä¶ [A]t some point, the computer stops. You know, there‚Äôs only so much computing power. But that formula that you would get, that the model would give you, that you could prove is true, again, about this model coin-flipping situation, that formula will keep going. You can put bigger and bigger numbers into that formula, much bigger than your computer could ever, ever compute with.‚Äù </em>
</p></blockquote>
<p>
</p><p></p><h2> Proof of the Loom? </h2><p></p>
<p></p><p>
Wood goes on to describe <em>universality</em> in probability theory as signifying ‚Äúthat there are certain kinds of machines that if you put in a lot of random inputs, you can predict the output.‚Äù She gives as a bellwether example how the Central Limit Theorem creates such a universal machine for the bell curve. Regardless of an unknown distribution <em>D</em>, if you take means of samples from <em>D</em>, then the bell curve gives progressively‚Äîand provably‚Äîmore accurate predictions of your outputs. Strogatz catches the warp and asks whether ‚Äúsomehow we‚Äôre getting the idea of universality to show up in number theory? Or am I dreaming?‚Äù Her peroration is:</p>
<blockquote><p><b> </b> <em> ‚Äú[W]hat my collaborators and I work on is trying to make that kind of dream a reality so that, that some of these puzzling questions about numbers that we don‚Äôt know the answer to, maybe we could understand that there are patterns that come out, like a bell curve, like a normal distribution, that we can prove came out of the machine even if we don‚Äôt know what mysteries were put in.‚Äù </em>
</p></blockquote>
<p></p><p>
So she is building a machine that takes rules as input‚Äîlike Jacquard cards for a loom‚Äîand produces patterns that are analyzable. We use the computational success of the machine to judge how well universality has taken hold‚Äîas theoretically it must‚Äîand generate proofs from formulas based on the <em>a-priori</em> predicted outputs using the rules input thus far. </p>
<p>
This is like building a loom for weaving proofs‚Äîwhere, however, there is still the question of confidence in how well the patterns obtained match reality. Such doubt notwithstanding, the process may also augment non-linear ways of evaluating claimed proofs of the kind we discussed <a href="https://rjlipton.wpcomstaging.com/2020/06/13/proof-checking-not-line-by-line/">here</a> and recently debated <a href="https://rjlipton.wpcomstaging.com/2022/04/10/discussion-about-proving-again/">here</a>.</p>
<p>
</p><p></p><h2> The Papers </h2><p></p>
<p></p><p>
The proofs in the accepted papers were, to be sure, evaluated by the standard expert social process. Here they are, lifted from the conference‚Äôs own program <a href="https://computationalcomplexity.org/Archive/2022/program.php">page</a>. Clicking on the time of the talk gives a pointer to the paper. </p>
<p>
<b>Wednesday, July 20</b></p>
<p>
<a href="https://arxiv.org/pdf/2205.10749.pdf">9:00</a> ‚ÄúVanishing Spaces of Random Sets and Applications to Reed-Muller Codes.‚Äù<br />
Siddharth Bhandari, Prahladh Harsha, Ramprasad Saptharishi, Srikanth Srinivasan</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16572/pdf/LIPIcs-CCC-2022-10.pdf">9:30</a> ‚ÄúNew Near-Linear Time Decodable Codes Closer to the GV Bound.‚Äù<br />
Guy Blanc and Dean Doron</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16576/pdf/LIPIcs-CCC-2022-14.pdf">10:00</a> ‚ÄúThe plane test is a local tester for Multiplicity Codes.‚Äù<br />
Dan Karliner, Roie Salama and Amnon Ta-Shma</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/025/">13:30</a> ‚ÄúEfficient Low-Space Simulations From the Failure of the Weak Pigeonhole Principle‚Äù (co-winner Best student paper).<br />
Oliver Korten</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/023/">14:00</a> ‚ÄúNisan-Wigderson generators in Proof Complexity: New lower bounds.‚Äù<br />
Erfan Khaniki</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16577/pdf/LIPIcs-CCC-2022-15.pdf">14:30</a> ‚ÄúPseudorandom Generators, Resolution and Heavy Width.‚Äù<br />
Dmitry Sokolov</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16565/pdf/LIPIcs-CCC-2022-3.pdf">15:30</a> ‚ÄúHitting Sets for Regular Branching Programs.‚Äù<br />
Andrej Bogdanov, William Hoza, Gautam Prakriya and Edward Pyne</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/021/">16:00</a> ‚ÄúImproved Pseudorandom Generators for <img src="https://s0.wp.com/latex.php?latex=%7BAC%5E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{AC^0}" class="latex" /> Circuits‚Äù (co-winner Best student paper).<br />
Xin Lyu</p>
<p>
<a href="https://arxiv.org/abs/2103.14134">16:40</a> ‚ÄúRandom restrictions and PRGs for PTFs in Gaussian Space.‚Äù<br />
Zander Kelley and Raghu Meka</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/024/">17:00</a> ‚ÄúPseudorandomness of Expander Random Walks for Symmetric Functions and Permutation Branching Programs.‚Äù<br />
Louis Golowich and Salil Vadhan</p>
<p>
<b>Thursday, July 21</b></p>
<p>
<a href="https://arxiv.org/abs/2111.02999">9:00</a> ‚ÄúQuantum search-to-decision reductions and the state synthesis problem.‚Äù<br />
Sandy Irani, Anand Natarajan, Chinmay Nirkhe, Sujit Rao and Henry Yuen</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16590/pdf/LIPIcs-CCC-2022-28.pdf">9:30</a> ‚ÄúInfluence in Completely Bounded Block-multilinear Forms and Classical Simulation of Quantum Algorithms.‚Äù<br />
Nikhil Bansal, Makrand Sinha and Ronald de Wolf</p>
<p>
<a href="https://arxiv.org/abs/2111.10409">10:00</a> ‚ÄúThe Acrobatics of BQP‚Äù (winner ‚Äì Best paper award).<br />
Scott Aaronson, Devon Ingram and William Kretschmer</p>
<p>
<a href="https://eprint.iacr.org/2021/513">13:30</a> ‚ÄúOn One-way Functions from NP-Complete Problems.‚Äù<br />
Yanyi Liu and Rafael Pass</p>
<p>
<a href="https://nanashima.github.io">14:00</a> ‚ÄúFinding Errorless Pessiland in Error-Prone Heuristica.‚Äù<br />
Shuichi Hirahara and Mikito Nanashima</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/084/">14:30</a> ‚ÄúCharacterizing Derandomization Through Fine-Grained Hardness of Levin-Kolmogorov Complexity.‚Äù<br />
Yanyi Liu and Rafael Pass</p>
<p>
<a href="https://www.researchgate.net/publication/356891307_Almost_Polynomial_Factor_Inapproximability_for_Parameterized_k-Clique">15:30</a> ‚ÄúAlmost Polynomial Factor Inapproximability for Parameterized k-Clique.‚Äù<br />
Karthik C. S. and Subhash Khot</p>
<p>
<a href="https://arxiv.org/abs/2106.12710">16:00</a> ‚ÄúCertifying solution geometry in random CSPs: counts, clusters and balance.‚Äù<br />
Jun-Ting Hsieh, Sidhanth Mohanty and Jeff Xu</p>
<p>
<a href="https://arxiv.org/abs/2203.03705">16:40</a> ‚ÄúHigh-Dimensional Expanders from Chevalley Groups.‚Äù<br />
Ryan O‚ÄôDonnell and Kevin Pratt</p>
<p>
<a href="https://arxiv.org/abs/2205.02374">17:10</a> ‚ÄúThe composition complexity of majority.‚Äù<br />
Victor Lecomte, Prasanna Ramakrishnan and Li-Yang Tan</p>
<p>
<b>Friday, July 22</b></p>
<p>
<a href="https://arxiv.org/abs/2004.14318">9:00</a> ‚ÄúThe Approximate Degree of Bipartite Perfect Matching.‚Äù<br />
Gal Beniamini</p>
<p>
<a href="https://arxiv.org/abs/2205.06249">9:30</a> ‚ÄúOptimal-Degree Polynomial Approximations for Exponentials and Gaussian Kernel Density Estimation.‚Äù<br />
Amol Aggarwal and Josh Alman</p>
<p>
<a href="https://arxiv.org/abs/2108.13578">10:00</a> ‚Äú<img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\ell_p}" class="latex" />-Spread and Restricted Isometry Properties of Sparse Random Matrices.‚Äù<br />
Venkatesan Guruswami, Peter Manohar and Jonathan Mosheiff</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/074/">13:30</a> ‚ÄúOn Randomized Reductions to the Random Strings.‚Äù<br />
Michael Saks and Rahul Santhanam</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/086/">14:00</a> ‚ÄúExtremely Efficient Constructions of Hash Functions, with Applications to Hardness Magnification and PRFs.‚Äù<br />
Lijie Chen, Jiatu Li and Tianqi Yang</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16575/pdf/LIPIcs-CCC-2022-13.pdf">14:30</a> ‚ÄúA better-than-3log(n) depth lower bound for De Morgan formulas with restrictions on top gates.‚Äù<br />
Ivan Mihajlin and Anastasia Sofronova</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/072/">15:30</a> ‚ÄúProbabilistic Kolmogorov Complexity with Applications to Average-Case Complexity.‚Äù<br />
Halley Goldberg, Valentine Kabanets, Zhenjian Lu and Igor C. Oliveira</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16588/pdf/LIPIcs-CCC-2022-26.pdf">16:00</a> ‚ÄúSymmetry of Information from Meta-Complexity.‚Äù<br />
Shuichi Hirahara</p>
<p>
<a href="https://arxiv.org/abs/2201.08895">16:30</a> ‚ÄúOn the Satisfaction Probability of k-CNF Formulas.‚Äù<br />
Till Tantau</p>
<p>
<b>Saturday, July 23</b></p>
<p>
<a href="https://arxiv.org/abs/2202.09883">9:00</a> ‚ÄúOn Efficient Noncommutative Polynomial Factorization via Higman Linearization.‚Äù<br />
Vikraman Arvind and Pushkar Joglekar</p>
<p>
<a href="https://www.researchgate.net/publication/360332836_Improved_Low-Depth_Set-Multilinear_Circuit_Lower_Bounds">9:30</a> ‚ÄúImproved Low-Depth Set-Multilinear Circuit Lower Bounds.‚Äù<br />
Deepanshu Kush and Shubhangi Saraf</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16594/pdf/LIPIcs-CCC-2022-32.pdf">10:15</a> ‚ÄúOn the Partial Derivative Method Applied to Lopsided Set-Multilinear Polynomials.‚Äù<br />
Nutan Limaye, Srikanth Srinivasan and Sebastien Tavenas</p>
<p>
<a href="https://arxiv.org/abs/2205.15168">10:45</a> ‚ÄúSubrank and Optimal Reduction of Scalar Multiplications to Generic Tensors.‚Äù<br />
Harm Derksen, Visu Makam and Jeroen Zuiddam</p>
<p>
<a href="https://eccc.weizmann.ac.il/report/2022/026/">11:00</a> ‚ÄúTrading Time and Space in Catalytic Branching Programs.‚Äù<br />
Ian Mertz and James Cook</p>
<p>
<a href="https://arxiv.org/abs/2201.10997">12:30</a> ‚ÄúLinear Branching Programs and Directional Affine Extractors.‚Äù<br />
Svyatoslav Gryaznov, Pavel Pudlak and Navid Talebanfard</p>
<p>
<a href="https://www.cs.mcgill.ca/~robere/research.html">14:00</a> ‚ÄúFurther collapses in TFNP.‚Äù<br />
Mika Goos, Alexandros Hollender, Siddhartha Jain, Gilbert Maystre, William Pires, Robert Robere and Ran Tao</p>
<p>
<a href="https://drops.dagstuhl.de/opus/volltexte/2022/16592/pdf/LIPIcs-CCC-2022-30.pdf">14:30</a> ‚ÄúInteractive Oracle Proofs of Proximity to Algebraic Geometry Codes.‚Äù<br />
Sarah Bordage, Mathieu Lhotel, Jade Nardi and Hugues Randriam</p>
<p>
<a href="https://eprint.iacr.org/2022/168">15:00</a> ‚ÄúHardness of Approximation for Stochastic Problems via Interactive Oracle Proofs.‚Äù<br />
Gal Arnon, Alessandro Chiesa and Eylon Yogev</p>
<p>
We congratulate all the authors of the accepted papers.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Can we really build mathematical looms for helping us generate proofs at high level?</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wpcomstaging.com/2022/07/18/complexity-2022/"><span class="datestr">at July 18, 2022 05:34 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/07/17/full-professorships-and-tenure-track-professorships-at-ruhr-university-bochum-germany-apply-by-july-29-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/07/17/full-professorships-and-tenure-track-professorships-at-ruhr-university-bochum-germany-apply-by-july-29-2022/">Full Professorships and Tenure-Track Professorships at Ruhr-University Bochum, Germany (apply by July 29, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We invite outstanding applicants from all areas of computer science, including the Foundations of Computer Science. Salary and working conditions are internationally very competitive and come with civil servant status. Full professorships are chair positions with administrative staff. We offer dual-career &amp; relocation support and a family-friendly environment. Knowledge of German is not required.</p>
<p>Website: <a href="https://informatik.rub.de/en/news/openings-professorships-in-computer-science-w3-and-w2-tenure-track-w3/">https://informatik.rub.de/en/news/openings-professorships-in-computer-science-w3-and-w2-tenure-track-w3/</a><br />
Email: career@casa.rub.de</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/07/17/full-professorships-and-tenure-track-professorships-at-ruhr-university-bochum-germany-apply-by-july-29-2022/"><span class="datestr">at July 17, 2022 10:15 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/103">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/103">TR22-103 |  Almost Chor--Goldreich Sources and Adversarial Random Walks | 

	Dean Doron, 

	Dana Moshkovitz, 

	Justin Oh, 

	David Zuckerman</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A Chor--Goldreich (CG) source [CG88] is a sequence of random variables  $X = X_1 \circ \ldots \circ X_t$, each $X_i \sim \{0,1 \{^d$, such that each $X_i$ has $\delta d$ min-entropy for some constant $\delta &gt; 0$, even conditioned on any fixing of $X_1 \circ \ldots \circ X_{i-1}$. We typically think of $d$ as constant. We extend this notion in several ways, and most notably allow each $X_i$ to be only $\gamma$-close to having $\delta d$ min entropy.

Studying such almost CG sources allows us to achieve pseudorandomness results which were not known to hold even for standard CG sources, and even for the weaker model of Santha--Vazirani sources [SV86]. We construct a deterministic condenser that on input $X$, outputs a distribution which is close to having constant entropy gap, namely a distribution $Z \sim \{0,1 \}^m$ for $m \approx \delta dt$ with min-entropy $m-O(1)$. 

Our new primitive readily implies fast simulation results:

*	We can simulate $\mathbf{BPP}$ using almost CG sources with constant multiplicative slowdown.
*	When the randomized algorithm has small failure probability, we can simulate it using almost CG sources with no multiplicative slowdown. This result extends to randomized protocols as well, and any setting in which we cannot simply cycle over all seeds, and a ``one-shot'' simulation is needed.

Moreover, our framework is flexible enough to work even when the $X_i$-s only have Shannon entropy rather than min-entropy, and in some cases, even when a few $X_i$-s are completely damaged.

Our main technical contribution is a novel analysis of random walks which may be of independent interest. We analyze walks with adversarially correlated steps, each step being entropy-deficient, on good enough lossless expanders. We prove that such walks (or certain interleaved walks on two expanders), starting from a fixed vertex and walking according to $X_1\circ \ldots \circ X_t$, accumulate most of the entropy in $X$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/103"><span class="datestr">at July 15, 2022 07:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2022/07/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2022/07/15/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://doi.org/10.1080/17513472.2022.2069417">Creating weaving patterns from subdivision schemes</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@msmathcomputer/108561058326179122">\(\mathbb{M}\)</a>),</span> new paper by Lipsch√ºtz, Reitebuch, Skrodzi, and Polthier, and explanatory thread by Skrodzi.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@peterrowlett/108578769727211199">How would you make a sphere from three circles?</a>, asks Peter Rowlett after his son said he could do it.</p>
  </li>
  <li>
    <p><a href="http://blog.computationalcomplexity.org/2022/06/a-gadget-for-3-colorings.html">Counting 3-colorings</a> and <a href="https://blog.computationalcomplexity.org/2022/06/counting-number-of-3-colorings-of-graph">follow-up post</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108584298969286802">\(\mathbb{M}\)</a>).</span> Like all ‚Äúnatural‚Äù \(\mathsf{NP}\)-complete problems (and many easier problems), the 3-coloring problem should have a \(\#\mathsf{P}\)-complete counting version, but the gadgets needed to prove it are a little subtle and tracking down the history of proof of this result took some effort.</p>
  </li>
  <li>
    <p><a href="https://youtu.be/tH6vLXMaCwQ">Polyhedra in which all but one edge have a right angle</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@henryseg/108584342158901821">\(\mathbb{M}\)</a>),</span> 3d-printed based on a construction used by Sydler to study Dehn invariants. Achieving this property leads to surprisingly complicated polyhedra.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/tag/2022-fields-and-abacus-medals/">The 2022 Fields medals</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@mathcination/108595654085200673">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p><a href="https://11011110.github.io/blog/2020/07/31/linkage.html">Two years ago</a> I linked to <a href="https://cp4space.wordpress.com/2020/07/25/rational-dodecahedron-inscribed-in-unit-sphere/">a post by Adam Goucher</a>, solving <a href="https://mathoverflow.net/q/234212/440">an old MathOverflow question</a> by showing that it is possible to find a dodecahedron, combinatorially equivalent to a regular one, with rational coordinates, inscribed in a unit sphere. But <a href="https://cp4space.hatsya.com/2022/06/20/infinitely-many-rational-dodecahedra/">now there are infinitely many</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108602499475969576">\(\mathbb{M}\)</a>)!</span> Some messy algebra, and then some work with elliptic curve group operations, eventually simplifies down to a parametric family with a dodecahedron for each integer right triangle.</p>
  </li>
  <li>
    <p>For integer \(A\), a grid of  \(n\) points has roughly \(n^2\sigma(A)/A\) area-\(A\) triangles, where \(\sigma\) is the sum of divisors <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108606089910227326">\(\mathbb{M}\)</a>);</span> see <a href="https://users.renyi.hu/~p_erdos/1971-20.pdf">Erd≈ës &amp; Purdy 1971</a> who used non-square grids and factorial \(A\) to find points with \(\Omega(n\log\log n)\) unit-area triangles. So how big is \(\sigma(A)/A\)? <a href="https://en.wikipedia.org/wiki/Divisor_function#Robin's_theorem">It depends on the Riemann hypothesis!</a> If RH is true, at most \(e^\gamma\log\log A\) for \(A&gt;5040\). If not, slightly larger infinitely often.</p>
  </li>
  <li>
    <p><a href="https://www.ics.uci.edu/~eppstein/pubs/Epp-ICGT-22.pdf">Slides from my talk on ‚ÄúGraphs in Nature‚Äù</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108611451435162895">\(\mathbb{M}\)</a>)</span> at the International Colloquium on Graph Theory and Combinatorics in Montpellier, France.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Prince_Rupert%27s_cube">Prince Rupert‚Äôs cube</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108623867716491741">\(\mathbb{M}\)</a>):</span> a cube can fit through a square hole drilled through another cube its size, or even slightly smaller. Now a Good Article on Wikipedia. I‚Äôve been wondering: is it possible to make Prince Rupert‚Äôs Borromean rings, by drilling square holes into three unit cubes, each simultaneously passing through the hole in the next one?</p>
  </li>
  <li>
    <p>On the CSTheory stackexchange, Alexey Milovanov asks for updates on the (as far as I know still unknown) complexity of an old problem, <a href="https://cstheory.stackexchange.com/q/51680/95">finding shortest addition chains</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108627574398080196">\(\mathbb{M}\)</a>).</span> This highlights something I love about editing Wikipedia: if you take the effort to track down a repeated error in the literature, and <a href="https://en.wikipedia.org/wiki/Special:Diff/206806087">document it properly in the right Wikipedia article</a>, then maybe 14 years later the correction rather than the error can be common knowledge.</p>
  </li>
  <li>
    <p><a href="https://www.maa.org/press/maa-reviews/pop-up-geometry">The MAA reviews Joe O‚ÄôRourke‚Äôs new book, <em>Pop-Up Geometry: The Mathematics Behind Pop-Up Cards</em></a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108632717277132574">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2207.04923">Killing a vortex</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108637362908341033">\(\mathbb{M}\)</a>),</span> by Thilikos and Wiederrecht. Robertson and Seymour‚Äôs structural decomposition of minor-closed graph families glues together surface-embedded graphs, a few arbitrarily-connected ‚Äúapex‚Äù vertices, and ‚Äúvortices‚Äù, bounded-pathwidth graphs attached to faces. For graph matching, vortices are problematic. This new preprint describes the families that don‚Äôt need them and shows that they are exactly the ones whose matchings can be counted quickly.</p>
  </li>
  <li>
    <p>Scott Aaronson, quantum complexity theorist and debunker of quantum hype on <a href="https://scottaaronson.blog/">his blog</a>, is also a <a href="https://scottaaronson.blog/?p=6552">target of trolls who have pushed him to back down from his free-speech principles and restrict comments</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108641124975905102">\(\mathbb{M}\)</a>).</span> <a href="https://windowsontheory.org/2022/07/13/my-friend-scott-aaronson/">Boaz Barak gives some support</a>. Via Boaz I re-found Scott‚Äôs 2005 ‚Äú<a href="https://arxiv.org/abs/quant-ph/0502072">NP-complete Problems and Physical Reality</a>‚Äù debunking soap bubble and rubber band solvers for hard optimization problems. Worth a re-read!</p>
  </li>
  <li>
    <p>I tend to pick technologically better solutions over popular ones, despite popularity‚Äôs importance for long-term viability <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108649887665081093">\(\mathbb{M}\)</a>).</span> Which is why I just switched from a gas range to induction. It has all the responsiveness of gas (vs the glacial response of conventional electric), is more efficient, has a smaller carbon footprint, fewer noxious emissions, etc. These are still uncommon in Southern California, but new laws require electric appliances for new construction, and I hope that with familiarity they will become better liked as well.</p>
  </li>
  <li>
    <p><a href="https://www.surgehq.ai//blog/30-percent-of-googles-reddit-emotions-dataset-is-mislabeled">Seriously bad data in Google‚Äôs GoEmotions dataset</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108653832818847808">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=32090389">via</a>), some 58K reddit comments categorized by affect. Opinions in the post and comments vary on why the categorization was so inaccurate, including lack of context, farming it out to poorly-paid workers in countries less likely to be familiar with the specific idioms used in the comments, or maybe just that it‚Äôs a hard problem.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2022/07/15/linkage.html"><span class="datestr">at July 15, 2022 04:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2022/07/15/imp-reg-htf-cnn/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2022/07/15/imp-reg-htf-cnn/">Implicit Regularization in Hierarchical Tensor Factorization and Deep Convolutional Networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>The ability of large neural networks to generalize is commonly believed to stem from an implicit regularization ‚Äî a tendency of gradient-based optimization towards predictors of low complexity.
A lot of effort has gone into theoretically formalizing this intuition.
Tackling modern neural networks head-on can be quite difficult, so existing analyses often focus on simplified models as stepping stones.
Among these, matrix and tensor factorizations have attracted significant attention due to their correspondence to linear neural networks and certain shallow non-linear convolutional networks, respectively. 
Specifically, they were shown to exhibit an implicit tendency towards low matrix and tensor ranks, respectively.</p>

<p>This post overviews a recent <a href="https://arxiv.org/abs/2201.11729">ICML 2022 paper</a> with <a href="https://asafmaman101.github.io/">Asaf Maman</a> and <a href="http://www.cohennadav.com/">Nadav Cohen</a>, in which we draw closer to practical deep learning by analyzing <em>hierarchical tensor factorization</em>, a model equivalent to certain <em>deep non-linear</em> convolutional networks. 
We find that, analogously to matrix and tensor factorizations, the implicit regularization in hierarchical tensor factorization strives to lower a notion of rank (called hierarchical tensor rank).
This turns out to have surprising implications on the origin of locality in convolutional networks, inspiring a practical method (explicit regularization scheme) for improving their performance on tasks with long-range dependencies.</p>

<h2 id="background-matrix-and-tensor-factorizations">Background: Matrix and Tensor Factorizations</h2>

<p>To put our work into context, let us briefly go over existing dynamical characterizations of implicit regularization in matrix and tensor factorizations.
In both cases they suggest an incremental learning process that leads to low rank solutions (for respective notions of rank). We will then see how these characterizations transfer to the considerably richer hierarchical tensor factorization.</p>

<h3 id="matrix-factorization-incremental-matrix-rank-learning">Matrix factorization: Incremental matrix rank learning</h3>
<p><em>Matrix factorization</em> is arguably the most extensively studied model in the context of implicit regularization. 
Indeed, it was already discussed in four previous posts (<a href="https://www.offconvex.org/2021/07/08/imp-reg-tf/">1</a>, <a href="https://www.offconvex.org/2020/11/27/reg_dl_not_norm/">2</a>, <a href="http://www.offconvex.org/2019/07/10/trajectories-linear-nets/">3</a>, <a href="http://www.offconvex.org/2019/06/03/trajectories/">4</a>), but for completeness we will present it once more. 
Consider the task of minimizing a loss $\mathcal{L}_M : \mathbb{R}^{D, D‚Äô} \to \mathbb{R}$ over matrices, e.g. $\mathcal{L}_M$ can be a matrix completion loss ‚Äî mean squared error over observed entries from some ground truth matrix. 
Matrix factorization refers to parameterizing the solution $W_M \in \mathbb{R}^{D, D‚Äô}$ as a product of $L$ matrices, and minimizing the resulting objective using <em>gradient descent (GD)</em>:</p>
<div style="text-align: center;">
\[
    \min\nolimits_{W^{(1)}, \ldots, W^{(L)}} \mathcal{L}_M \big ( W_M \big ) := \mathcal{L}_M \big ( W^{(1)} \cdots W^{(L)} \big ) ~.
\]
</div>
<p>Essentially, matrix factorization amounts to applying a linear neural network (fully connected neural network with no non-linearity) for minimizing $\mathcal{L}_M$. 
We can explicitly constrain the matrix rank of $W_M$ by limiting the shared dimensions of the weight matrices $\{ W^{(l)} \}_l$. However, from an implicit regularization standpoint, the most interesting case is where rank is unconstrained. 
In this case there is no explicit regularization, and the kind of solution we get is determined implicitly by the parameterization and the optimization algorithm.</p>

<p>Although it was initially conjectured that GD (with small initialization and step size) over matrix factorization minimizes a norm (see the seminal work of <a href="https://arxiv.org/abs/1705.09280">Gunasekar et al. 2017</a>), recent evidence points towards an implicit matrix rank minimization (see <a href="https://arxiv.org/abs/1905.13655">Arora et al. 2019</a>; <a href="https://arxiv.org/abs/1904.13262">Gidel et al. 2019</a>; <a href="https://arxiv.org/abs/2005.06398">Razin &amp; Cohen 2020</a>; <a href="https://arxiv.org/abs/2011.13772">Chou et al. 2020</a>; <a href="https://arxiv.org/abs/2012.09839">Li et al. 2021</a>).
In particular, <a href="https://arxiv.org/abs/1905.13655">Arora et al. 2019</a> characterized the dynamics of $W_M$‚Äôs singular values throughout optimization:</p>

<blockquote>
  <p><strong>Theorem (informal; <a href="https://arxiv.org/abs/1905.13655">Arora et al. 2019</a>):</strong>
Gradient flow (GD with infinitesimal step size) over matrix factorization initialized near zero leads the $r$‚Äôth singular value of $W_M$, denoted $\sigma_M^{(r)} (t)$, to evolve by:
[ 
    \color{brown}{\frac{d}{dt} \sigma_M^{(r)} (t) \propto \sigma_M^{(r)} (t)^{2 - 2/L}} ~.
]</p>
</blockquote>

<p>As can be seen from the theorem above, singular values evolve at a rate proportional to their size exponentiated by $2 - 2 / L$. This means that they are subject to a momentum-like effect, by which they move slower when small and faster when large. 
When initializing near the origin (as commonly done in practice), we therefore expect singular values to progress slowly at first, and then, upon reaching a certain threshold, to quickly rise until convergence. 
<strong>These dynamics create an incremental learning process that promotes solutions with few large singular values and many small ones, i.e. low matrix rank solutions</strong>.
In their paper, <a href="https://arxiv.org/abs/1905.13655">Arora et al. 2019</a> support this qualitative explanation through theoretical illustrations and empirical evaluations. 
For example, the following plot reproduces one of their experiments:</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/imp_reg_htf/mf_dyn_blog.png" style="width: 380px; padding-bottom: 0px; padding-top: 0px;" />
<br />
<i><b>Figure 1:</b> 
Dynamics of singular values during GD over matrix factorization <br /> ‚Äî incremental learning leads to low matrix rank.
</i>
</div>
<p><br />
We note that the incremental matrix rank learning phenomenon was later on used to prove exact matrix rank minimization, under certain technical conditions (<a href="https://arxiv.org/abs/2012.09839">Li et al. 2021</a>).</p>

<h3 id="tensor-factorization-incremental-tensor-rank-learning">Tensor factorization: Incremental tensor rank learning</h3>

<p>Despite the significant interest in matrix factorization, as a theoretical surrogate for deep learning its practical relevance is rather limited. 
It corresponds to linear neural networks, and thus misses non-linearity ‚Äî a crucial aspect of modern neural networks.
As was mentioned in a <a href="https://www.offconvex.org/2021/07/08/imp-reg-tf/">previous post</a>, by moving from matrix (two-dimensional array) to tensor (multi-dimensional array) factorizations it is possible to address this limitation.</p>

<p>A classical scheme for factorizing tensors, named CANDECOMP/PARAFAC (CP), parameterizes a tensor as a sum of outer products (for more details on this scheme, see <a href="http://www.kolda.net/publication/TensorReview.pdf">this excellent survey</a>).
Given a loss $\mathcal{L}_T : \mathbb{R}^{D_1, \ldots, D_N} \to \mathbb{R}$ over $N$-dimensional tensors, e.g. $\mathcal{L}_T$ can be a tensor completion loss, we simply refer by <em>tensor factorization</em> to parameterizing the solution $\mathcal{W}_T \in \mathbb{R}^{D_1, \ldots, D_N}$ as a CP factorization, and minimizing the resulting objective via GD:</p>
<div style="text-align: center;">
\[
    \min\nolimits_{ \{ \mathbf{w}_r^n \}_{r , n} } \mathcal{L}_T \big ( \mathcal{W}_T \big ) := \mathcal{L}_T \big (  {\textstyle \sum}_{r = 1}^R \mathbf{w}_r^1 \otimes \cdots \otimes \mathbf{w}_r^N \big) ~.
\]
</div>
<p>Each term $\mathbf{w}_r^{(1)} \otimes \cdots \otimes \mathbf{w}_r^{(N)}$ in the sum is called a <em>component</em>, and $\otimes$ stands for outer product.
The concept of rank naturally extends from matrices to tensors.
For a given tensor $\mathcal{W}$, its <em>tensor rank</em> is defined to be the minimal number of components (i.e. of outer product summands) $R$ required for CP parameterization to express it.
Note that we can explicitly constrain the tensor rank of $\mathcal{W}_T$ by limiting the number of components $R$.
But, since our interest lies in implicit regularization, we consider the case where $R$ is large enough for any tensor to be expressed.</p>

<p>Similarly to how matrix factorization captures linear neural networks, tensor factorization is equivalent to certain <em>shallow non-linear</em> convolutional networks (with multiplicative non-linearity).
This equivalence was discussed in a couple of previous posts (<a href="https://www.offconvex.org/2020/11/27/reg_dl_not_norm/">1</a>, <a href="https://www.offconvex.org/2021/07/08/imp-reg-tf/">2</a>), for the exact details behind it feel free to check out the preliminaries section of <a href="https://arxiv.org/abs/2201.11729">our paper</a> and references therein.
The bottom line is that tensor factorization takes us one step closer to practical neural networks.</p>

<p>Motivated by the incremental learning dynamics in matrix factorization, in a <a href="https://arxiv.org/abs/2102.09972">previous paper</a> (see accompanying <a href="https://www.offconvex.org/2021/07/08/imp-reg-tf/">blog post</a>) we analyzed the behavior of component norms during optimization of tensor factorization:</p>

<blockquote>
  <p><strong>Theorem (informal; <a href="https://arxiv.org/abs/2102.09972">Razin et al. 2021</a>):</strong>
Gradient flow over tensor factorization initialized near zero leads the $r$‚Äôth component norm, $\sigma_T^{(r)} (t) := || \mathbf{w}_r^1 (t) \otimes \cdots \otimes \mathbf{w}_r^N (t) ||$, to evolve by:
[ 
    \color{brown}{\frac{d}{dt} \sigma_T^{(r)} (t) \propto \sigma_T^{(r)} (t)^{2 - 2/N}} ~.
]</p>
</blockquote>

<p>The dynamics of component norms in tensor factorization are structurally identical to those of singular values in matrix factorization.
Accordingly, we get a momentum-like effect that attenuates the movement of small component norms and accelerates that of large ones.
This suggests that, <strong>in analogy with matrix factorization, when initializing near zero components tend to be learned incrementally, resulting in a bias towards low tensor rank</strong>.
The following plot empirically demonstrates this phenomenon:</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/imp_reg_htf/tf_dyn_blog.png" style="width: 380px; padding-bottom: 0px; padding-top: 0px;" />
<br />
<i><b>Figure 2:</b> 
Dynamics of component norms during GD over tensor factorization <br /> ‚Äî incremental learning leads to low tensor rank.
</i>
</div>
<p><br />
Continuing with the analogy to matrix factorization, the incremental tensor rank learning phenomenon formed the basis for proving exact tensor rank minimization, under certain technical conditions (<a href="https://arxiv.org/abs/2102.09972">Razin et al. 2021</a>).</p>

<h2 id="hierarchical-tensor-factorization">Hierarchical Tensor Factorization</h2>

<p>Tensor factorization took us beyond linear predictors, yet it still lacks a critical feature of modern neural networks ‚Äî depth (recall that it corresponds to <em>shallow</em> non-linear convolutional networks).
A natural extension that accounts for both non-linearity and depth is <em>hierarchical tensor factorization</em> ‚Äî our protagonist ‚Äî which corresponds to certain <em>deep</em> non-linear convolutional networks (with multiplicative non-linearity).
This equivalence is actually not new, and has facilitated numerous analyses of expressive power in deep learning (see <a href="https://arxiv.org/abs/1705.02302">this survey</a> for a high-level overview).</p>

<p>As opposed to tensor factorization, which is a simple construct dating back to at least the early 20‚Äôth century (<a href="https://onlinelibrary.wiley.com/doi/10.1002/sapm192761164">Hitchcock 1927</a>), hierarchical tensor factorization was formally introduced only recently (<a href="https://link.springer.com/article/10.1007/s00041-009-9094-9">Hackbusch &amp; Kuhn 2009</a>), and is much more elaborate.
Its exact definition is rather technical (the interested reader can find it in <a href="https://arxiv.org/abs/2201.11729">our paper</a>).
For our current purpose it suffices to know that a hierarchical tensor factorization consists of multiple local tensor factorizations, whose components we call the <em>local components</em> of the hierarchical factorization.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/imp_reg_htf/tf_htf_cnn_blog.png" style="width: 900px; padding-bottom: 15px; padding-top: 10px;" />
<br />
<i><b>Figure 3:</b> 
Tensor factorization, which is a sum of components (outer products), <br /> corresponds to a shallow non-linear convolutional neural network (CNN).
<br /> Hierarchical tensor factorization, which consists of multiple local tensor <br /> factorizations, corresponds to a deep non-linear CNN.
</i>
</div>
<p><br />
In contrast to matrices, which have a single standard definition for rank, tensors posses several different definitions for rank.
Hierarchical tensor factorizations induce their own such notion, known as <em>hierarchical tensor rank</em>.
Basically, if a tensor can be represented through hierarchical tensor factorization with few local components, then it has low hierarchical tensor rank.
This stands in direct analogy with tensor rank, which is low if the tensor can be represented through tensor factorization with few components.</p>

<p>Seeing that the implicit regularization in matrix and tensor factorizations leads to low matrix and tensor ranks, respectively, in <a href="https://arxiv.org/abs/2201.11729">our paper</a> we investigated whether the implicit regularization in hierarchical tensor factorization leads to low hierarchical tensor rank. 
That is, whether GD (with small initialization and step size) over hierarchical tensor factorization learns solutions that can be represented with few local components.
Turns out it does.</p>

<h2 id="dynamical-analysis-incremental-hierarchical-tensor-rank-learning">Dynamical Analysis: Incremental Hierarchical Tensor Rank Learning</h2>

<p>At the heart of our analysis is the following dynamical characterization for local component norms during optimization of hierarchical tensor factorization:</p>

<blockquote>
  <p><strong>Theorem (informal):</strong>
Gradient flow over hierarchical tensor factorization initialized near zero leads the $r$‚Äôth local component norm in a local tensor factorization, denoted $\sigma_H^{(r)} (t)$, to evolve by:
[ 
    \color{brown}{\frac{d}{dt} \sigma_H^{(r)} (t) \propto \sigma_H^{(r)} (t)^{2 - 2/K}} ~,
]
where $K$ is the number of axes of the local tensor factorization.</p>
</blockquote>

<p>This should really feel like deja vu, as these <strong>dynamics are structurally identical to those of singular values in matrix factorization and component norms in tensor factorization!</strong>
Again, we have a momentum-like effect, by which local component norms move slower when small and faster when large.
As a result, <strong>when initializing near zero local components tend to be learned incrementally, yielding a bias towards low hierarchical tensor rank</strong>.
In <a href="https://arxiv.org/abs/2201.11729">the paper</a> we provide theoretical and empirical demonstrations of this phenomenon.
For example, the following plot shows the evolution of local component norms at some local tensor factorization under GD:</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/imp_reg_htf/htf_dyn_blog.png" style="width: 380px; padding-bottom: 0px; padding-top: 0px;" />
<br />
<i><b>Figure 4:</b> 
Dynamics of local component norms during GD over hierarchical <br /> tensor factorization ‚Äî incremental learning leads to low hierarchical tensor rank.
</i>
</div>
<p><br /></p>

<h2 id="practical-implication-countering-locality-in-convolutional-networks-via-explicit-regularization">Practical Implication: Countering Locality in Convolutional Networks via Explicit Regularization</h2>

<p>We saw that in hierarchical tensor factorization GD leads to solutions of low hierarchical tensor rank.
But what does this even mean for the associated convolutional networks?</p>

<p>Hierarchical tensor rank is known (<a href="https://arxiv.org/abs/1605.06743">Cohen &amp; Shashua 2017</a>) to measure the strength of long-range dependencies modeled by a network.
In the context of image classification, e.g., it quantifies how well we take into account dependencies between distant patches of pixels.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/imp_reg_htf/local_vs_non_local_dep.png" style="width: 430px; padding-bottom: 8px; padding-top: 8px;" />
<br />
<i><b>Figure 5:</b> 
Illustration of short-range (local) vs. long-range dependencies in image data.
</i>
</div>
<p><br />
<strong>The implicit regularization towards low hierarchical tensor rank in hierarchical tensor factorization therefore translates to an implicit regularization towards <em>locality</em> in the corresponding convolutional networks</strong>.
At first this may not seem surprising, since convolutional networks typically struggle or completely fail to learn tasks entailing long-range dependencies.
However, conventional wisdom attributes this failure to expressive properties (i.e. to an inability of convolutional networks to realize functions modeling long-range dependencies), suggesting that addressing the problem requires modifying the architecture.
Our analysis, on the other hand, reveals that implicit regularization also plays a role: it is not just a matter of expressive power, the optimization algorithm is implicitly pushing towards local solutions.
Inspired by this observation, we asked:</p>

<blockquote>
  <p><strong>Question:</strong>
Is it possible to improve the performance of modern convolutional networks on long-range tasks via explicit regularization (without modifying their architecture)?</p>
</blockquote>

<p>To explore this prospect, <strong>we designed explicit regularization that counteracts locality by promoting high hierarchical tensor rank (i.e. long-range dependencies)</strong>.
Then, through a series of controlled experiments, <strong>we confirmed that it can greatly improve the performance of modern convolutional networks (e.g. ResNets) on long-range tasks</strong>.</p>

<p>For example, the following plot displays test accuracies achieved by a ResNet on an image classification benchmark, in which it is possible to control the spatial range of dependencies required to model.
When increasing the range of dependencies, the test accuracy obtained by an unregularized network substantially deteriorates, reaching performance no better than random guessing.
As evident from the plot, our regularization closes the gap between short- and long-range tasks, significantly boosting generalization on the latter.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/imp_reg_htf/pathfinder_resnet18_with_reg_blog.png" style="width: 430px; padding-bottom: 0px; padding-top: 0px;" />
<br />
<i><b>Figure 6:</b> 
Specialized explicit regularization promoting high hierarchical tensor rank (i.e. long-range dependencies between image regions) can counter the locality of convolutional networks, significantly improving their performance on long-range tasks.
</i>
</div>
<p><br /></p>

<h2 id="concluding-thoughts">Concluding Thoughts</h2>
<p>Looking forward, there are two main takeaways from our work:</p>

<ol>
  <li>
    <p>Across three different neural network types (equivalent to matrix, tensor, and hierarchical tensor factorizations), we have an architecture-dependant notion of rank that is implicitly lowered. Moreover, the underlying mechanism for this implicit regularization is identical in all cases. This leads us to believe that implicit regularization towards low rank may be a general phenomenon. If true, finding notions of rank lowered for different architectures can facilitate an understanding of generalization in deep learning.</p>
  </li>
  <li>
    <p>Our findings imply that the tendency of modern convolutional networks towards locality may largely be due to implicit regularization, and not an inherent limitation of expressive power as often believed. More broadly, they showcase that deep learning architectures considered suboptimal for certain tasks can be greatly improved through a right choice of explicit regularization. 
Theoretical understanding of implicit regularization may be key to discovering such regularizers.</p>
  </li>
</ol>

<p><strong><em><a href="https://noamrazin.github.io/">Noam Razin</a></em></strong></p></div>







<p class="date">
<a href="http://offconvex.github.io/2022/07/15/imp-reg-htf-cnn/"><span class="datestr">at July 15, 2022 09:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/102">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/102">TR22-102 |  Range Avoidance for Low-depth Circuits and Connections to Pseudorandomness | 

	Xiuhan Wang, 

	Venkatesan Guruswami, 

	Xin Lyu</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In the range avoidance problem, the input is a multi-output Boolean circuit with more outputs than inputs, and the goal is to find a string outside its range (which is guaranteed to exist). We show that well-known explicit construction questions such as finding binary linear codes achieving the Gilbert-Varshamov bound or list-decoding capacity, and constructing rigid matrices, reduce to the range avoidance problem of log-depth circuits, and by a further recent reduction [Ren, Santhanam, and Wang, ECCC 2022] to $NC^0_4$ circuits where each output depends on at most $4$ input bits. 

On the algorithmic side, we show that range avoidance for $NC^0_2$ circuits can be solved in polynomial time. We identify a general condition relating to correlation with low-degree parities that implies that any almost pairwise independent set has some string that avoids the range of every circuit in the class. We apply this to $NC^0$ circuits, and to small width CNF/DNF and general De Morgan formulae (via a connection to approximate-degree), yielding non-trivial small hitting sets for range avoidance in these cases.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/102"><span class="datestr">at July 15, 2022 07:11 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/07/15/postdoc-at-university-of-cologne-apply-by-august-31-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/07/15/postdoc-at-university-of-cologne-apply-by-august-31-2022/">Postdoc at University of Cologne (apply by August 31, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>A postdoc position in theoretical computer sciece, algorithms and data structures or algorithmic data analysis is available in the Algorithmic Data Analysis group led by Prof. Dr. Christian Sohler in the department of mathematics/computer science at University of Cologne.</p>
<p>Website: <a href="https://jobportal.uni-koeln.de/ausschreibung/renderFile/893?propertyName=flyer">https://jobportal.uni-koeln.de/ausschreibung/renderFile/893?propertyName=flyer</a><br />
Email: sohler@cs.uni-koeln.de</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/07/15/postdoc-at-university-of-cologne-apply-by-august-31-2022/"><span class="datestr">at July 15, 2022 06:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/101">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/101">TR22-101 |  A Near-Cubic Lower Bound for 3-Query Locally Decodable Codes from Semirandom CSP Refutation | 

	Omar Alrabiah, 

	Pravesh Kothari, 

	Venkatesan Guruswami, 

	Peter Manohar</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A code $C \colon \{0,1\}^k \to \{0,1\}^n$ is a $q$-locally decodable code ($q$-LDC) if one can recover any chosen bit $b_i$ of the message $b \in \{0,1\}^k$ with good confidence by randomly querying the encoding $x = C(b)$ on at most $q$ coordinates. Existing constructions of $2$-LDCs achieve $n = \exp(O(k))$, and lower bounds show that this is in fact tight. However, when $q = 3$, far less is known: the best constructions achieve $n = \exp(k^{o(1)})$, while the best known results only show a quadratic lower bound $n \geq \widetilde{\Omega}(k^2)$ on the blocklength.

In this paper, we prove a near-cubic lower bound of $n \geq \widetilde{\Omega}(k^3)$ on the blocklength of $3$-query LDCs. This improves on the best known prior works by a polynomial factor in $k$. Our proof relies on a new connection between LDCs and refuting constraint satisfaction problems with limited randomness. Our quantitative improvement builds on the new techniques for refuting semirandom instances of CSPs developed in [GKM22] and, in particular, relies on bounding the $(\infty \to 1)$-norm of appropriate Kikuchi matrices.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/101"><span class="datestr">at July 14, 2022 09:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/100">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/100">TR22-100 |  Streaming complexity of CSPs with randomly ordered constraints | 

	Santhoshini Velusamy, 

	Noah Singer, 

	Raghuvansh Saxena, 

	Madhu Sudan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We initiate a study of the streaming complexity of constraint satisfaction problems (CSPs) when the constraints arrive in a random order. We show that there exists a CSP, namely Max-DICUT, for which random ordering makes a provable difference. Whereas a $4/9 \approx 0.445$ approximation of DICUT requires $\Omega(\sqrt{n})$ space with adversarial ordering, we show that with random ordering of constraints there exists a $0.48$-approximation algorithm that only needs $O(\log n)$ space. We also give new algorithms for Max-DICUT in variants of the adversarial ordering setting. Specifically, we give a two-pass  $O(\log n)$ space $0.48$-approximation algorithm for general graphs and a single-pass $\tilde{O}(\sqrt{n})$ space $0.48$-approximation algorithm for bounded degree graphs.
    
    On the negative side, we prove that CSPs where the satisfying assignments of the constraints support a one-wise independent distribution require $\Omega(\sqrt{n})$-space for any non-trivial approximation, even when the constraints are randomly ordered. This was previously known only for adversarially ordered constraints. Extending the results to randomly ordered constraints requires switching the hard instances from a union of random matchings to simple Erdos-Renyi random (hyper)graphs and extending tools that can perform Fourier analysis on such instances. 
    
    The only CSP to have been considered previously with random ordering is Max-CUT where the ordering is not known to change the approximability. Specifically, it is known to be as hard to approximate with random ordering as with adversarial ordering, for $o(\sqrt{n})$ space algorithms. Our results show a richer variety of possibilities and motivate further study of CSPs with randomly ordered constraints.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/100"><span class="datestr">at July 14, 2022 06:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/099">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/099">TR22-099 |  Equivalence Test for Read-Once Arithmetic Formulas | 

	Nikhil Gupta, 

	Chandan Saha, 

	Bhargav Thankey</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We study the polynomial equivalence problem for orbits of read-once arithmetic formulas (ROFs). Read-once formulas have received considerable attention in both algebraic and Boolean complexity and have served as a testbed for developing effective tools and techniques for analyzing circuits. Two $n$-variate polynomials $f, g \in \mathbb{F}[\mathbf{x}]$ are equivalent, denoted as $f \sim g$, if there is an $A \in \mathrm{GL}(n, \mathbb{F})$ such that $f = g(A\mathbf{x})$. The orbit of $f$ is the set of all polynomials equivalent to $f$. We investigate the complexity of the following two natural problems on ROFs:

1. Equivalence test for ROFs: Given black-box access to $f$, check if it is in the orbit of an ROF. If yes, output an ROF $C$ and an $A \in \mathrm{GL}(n, \mathbb{F})$ such that $f = C(A\mathbf{x})$.  
2. Polynomial equivalence for orbits of ROFs: Given black-box access to $f$ and $g$ in the orbits of two unknown ROFs, check if $f \sim g$. If yes, output an $A \in \mathrm{GL}(n, \mathbb{F})$ such that $f = g(A\mathbf{x})$.

These problems are significant generalizations of two well-studied problems in algebraic complexity, namely reconstruction of ROFs and quadratic form equivalence. In this work, we give the first randomized polynomial-time algorithms (with oracle access to quadratic form equivalence) to solve the two problems. The equivalence test works for general ROFs; it also implies an efficient learning algorithm for random arithmetic formulas of unbounded depth and fan-in (in the high number of variables setting). The algorithm for the second problem, which invokes the equivalence test, works for mildly restricted ROFs, namely additive-constant-free ROFs.  	
	
The equivalence test is based on a novel interplay between the factors and the essential variables of the Hessian determinant of an ROF, the essential variables of the ROF, and certain special structures in the ROF that we call "skewed paths". To our knowledge, the Hessian of a general ROF (or even a depth-4 ROF) has not been analyzed before. Analyzing the Hessian and combining the knowledge gained from it with the skewed paths to recursively discover formulas in the orbits of sub-ROFs of lower depth (without incurring an exponential blow-up due to unbounded depth) constitute the main technical contributions of this work.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/099"><span class="datestr">at July 14, 2022 12:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8443">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2022/07/13/my-friend-scott-aaronson/">My friend, Scott Aaronson</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<div class="wp-block-image">
<figure class="aligncenter size-large is-resized"><a href="https://windowsontheory.files.wordpress.com/2022/07/image.png"><img width="471" alt="" src="https://windowsontheory.files.wordpress.com/2022/07/image.png?w=750" class="wp-image-8446" height="643" /></a></figure></div>


<p>This is a photo of my book shelf at the office. Ever since joining Harvard, I have been ordering copies of <a href="https://www.amazon.com/Quantum-Computing-since-Democritus-Aaronson/dp/0521199565">Quantum Computing Since Democritus</a> on a regular basis. I often hand them out to bright students, curious about science, whom I want to expose to the beautiful connections between computer science, math, physics, and even philosophy. Scott has been one of the great popularizers of our field even before he started blogging in 2005. His surveys and blog posts provide some of the best introductions to our field. For example, when investigating¬† <a href="http://quant-ph/0502072">P vs NP and physical reality</a>, Scott actually <a href="https://scottaaronson.blog/?p=266">went out and verified</a> that nature indeed cannot solve an NP-complete problem via finding the globally minimal energy configuration of soap bubbles.¬† Through his blog, popular writing, and research, Scott has done more than anyone else to introduce new people of all backgrounds to theoretical computer science.¬†</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img width="543" alt="" src="https://lh6.googleusercontent.com/JRN6bxodVkY1JiHSWOEx4QXtvmH3twDk-O5jGgsio1IM92aUUxYZ87i7T0qU8bfrCnVbBWIt1OV4soNgnhoMpTHvnJBIVk8tAPO_oK2_WLzHbrGfWNpRDQyazey-CG9V0YLDDZ2r45pwOKsSF4PZaQ" height="408" /></figure></div>


<p>One of Scott‚Äôs endearing qualities is his openness to all people. While many of us would ignore a random email or anonymous blog comment, Scott would patiently explain for the millionth time why quantum computers can‚Äôt solve NP-hard problems by ‚Äútrying all solutions in parallel‚Äù or why Bell‚Äôs Inequality does indeed rule out hidden-variable theories of nature. Alas, the same openness also results in him sometimes giving too much attention and caring far too much about the opinions of Internet ‚Äútrolls‚Äù that are not worthy of his time.¬†</p>



<p>While Scott has always attracted some vitriol, recently this has taken to a <a href="https://scottaaronson.blog/?p=6546">new level</a>, with commenters attacking his integrity, his speech mannerisms, even his T-shirt choice/frequency, and worst of all, his family, with misogynistic attacks on his wife and xenophobic and ableist attacks on neurodivergent researchers.</p>



<p>None of these people have made a fraction of the contributions of Scott not just to science, but also to broadening the diversity of computer science, and other causes including <a href="https://scottaaronson.blog/?p=6411">assisting women dealing with Texas‚Äô restrictive abortion laws</a>. (As full disclosure, one of the causes <a href="https://scottaaronson.blog/?p=6256">Scott helped raise money</a> for is <a href="https://www.addiscoder.com/">AddisCoder</a> and <a href="https://jamcoder.org.jm/">JamCoders</a> of which I am a board member. I just came back from a <a href="https://twitter.com/boazbaraktcs/status/1545765613167067136?s=20&amp;t=53HLUIjmCwBsl1oYhEQXhw">week teaching in Jamaica</a>, the students were amazing and are so thankful for the chance to participate in this program; they couldn‚Äôt care less how often Scott changes his shirt.)</p>



<p>I am grateful that Scott is a member of our scientific community and proud to call him my friend. Does this mean that I agree with all his positions? Absolutely not. I tend to be on his left on many issues¬† (though am probably more conservative when it comes to oracle-based complexity..). Are there people he‚Äôs friendly with whom I even more strongly disagree with, and whose views I might even find repugnant? Probably. But it doesn‚Äôt matter, all of us are connected via 6 degrees of separation. If we start to ‚Äúrecursively cancel‚Äù every one that is somehow connected to someone we find odious, then we would not be able to talk to anyone.</p>



<p>I hope that Scott is not disheartened by these attacks, and continues to contribute for many years to CS research and education, outreach, and humanity at large.</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2022/07/13/my-friend-scott-aaronson/"><span class="datestr">at July 13, 2022 04:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/098">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/098">TR22-098 |  Non-Adaptive Proper Learning Polynomials | 

	Nader Bshouty</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We give the first polynomial-time *non-adaptive* proper learning algorithm of Boolean sparse multivariate polynomial under the uniform distribution. Our algorithm, for $s$-sparse polynomial over $n$ variables, makes $q=(s/\epsilon)^{\gamma(s,\epsilon)}\log n$ queries where $2.66\le \gamma(s,\epsilon)\le 6.922$ and runs in $\tilde O(n)\cdot poly(s,1/\epsilon)$ time. We also show that for any $\epsilon=1/s^{O(1)}$ any non-adaptive learning algorithm must make at least $(s/\epsilon)^{\Omega(1)}\log n$ queries. Therefore, the query complexity of our algorithm is also polynomial in the optimal query complexity and optimal in $n$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/098"><span class="datestr">at July 13, 2022 02:55 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://scottaaronson.blog/?p=6552">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://scottaaronson.blog/?p=6552">Choosing a new comment policy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><strong>Update (July 13):</strong> I was honored to read <a href="https://windowsontheory.org/2022/07/13/my-friend-scott-aaronson/">this post</a> by my friend Boaz Barak.</p>



<p><strong>Update (July 14):</strong> By now, comments on this post allegedly from four CS professors  ‚Äî namely, Josh Alman, Aloni Cohen, Rana Hanocka, and Anna Farzindar ‚Äî as well as from the graduate student ‚ÄúBA,‚Äù <em>have been unmasked as from impersonator(s)</em>.</p>



<p>I‚Äôve been the target of a motivated attack-troll (or multiple trolls, but I now believe just one) who knows about the CS community. This might be the single weirdest thing that‚Äôs happened to me in 17 years of blogging, surpassing even the legendary <a href="https://scottaaronson.blog/?p=277">Ricoh printer episode</a> of 2007.  It obviously underscores the need for a new, stricter comment policy, which is what this whole post was about.</p>



<hr class="wp-block-separator has-alpha-channel-opacity" />



<p>Yesterday and today, both my work and my enjoyment of the James Webb images were interrupted by an anonymous troll, who used the <em>Shtetl-Optimized</em> comment section to heap <a href="https://scottaaronson.blog/?p=6546#comment-1941041">libelous abuse</a> on me‚Äîderailing an anodyne quantum computing discussion to opine at length about how I‚Äôm a disgusting creep who surely, probably, maybe has lewd thoughts about his female students.  Unwisely or not, I allowed it all to appear, and replied to all of it.  I had a few reasons: I wanted to prove that I‚Äôm now strong enough to withstand bullying that might once have driven me to suicide.  I wanted, frankly, many readers to come to my defense (thanks to those who did!).  I at least wanted readers to <em>see</em> firsthand what I now regularly deal with: the emotional price of maintaining this blog.  Most of all, I wanted my feminist, social-justice-supporting readers to either explicitly endorse or (hopefully) explicitly repudiate the unambiguous harassment that was now being gleefully committed in their name.</p>



<p>Then, though, the same commenter upped the ante further, by heaping misogynistic abuse on my wife <a href="https://www.cs.utexas.edu/~danama/">Dana</a>‚Äîwhile <em>still</em>, ludicrously and incongruously, cloaking themselves in the rhetoric of social justice.  Yes: apparently the woke, feminist thing to do is now to rate female computer scientists on their looks.</p>



<p>Let me be blunt: I cannot continue to write <em>Shtetl-Optimized</em> while dealing with regular harassment of me and my family.  At the same time, I‚Äôm also determined not to ‚Äúsurrender to the terrorists.‚Äù  So, I‚Äôm weighing the following options:</p>



<ul><li>Close comments except to commenters who provide a real identity‚Äîe.g., a full real name, a matching email address, a website.</li><li>Move to Substack, and then allow only commenters who‚Äôve signed up.</li><li>Hire someone to pre-screen comments for me, and delete ones that are abusive or harassing (to me or others) before I even see them.  (Any volunteers??)</li><li>Make the comment sections for readers only, eliminating any expectation that I‚Äôll participate.</li></ul>



<p>One thing that‚Äôs clear is that the status quo will not continue.  I can‚Äôt ‚Äújust delete‚Äù harassing or abusive comments, because the trolls have gotten too good at triggering me, and they will continue to weaponize my openness and my ethic of responding to all possible arguments against me.</p>



<p>So, regular readers: what do you prefer?</p></div>







<p class="date">
by Scott <a href="https://scottaaronson.blog/?p=6552"><span class="datestr">at July 12, 2022 08:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3528238240939023029">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/07/review-of-engines-of-cognition-essays.html">Review of The Engines of Cognition: Essays From the LessWrong Forum/Meta question about posts</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>¬†A while back I reviewed<i> A Map that Reflects the Territory</i>¬†which is a collection of essays posted on the lesswrong forum. My review is¬†<a href="https://www.cs.umd.edu/~gasarch/bookrev/FRED/lesswrong.pdf">here</a>. I posted it to both this blog and to the lesswrong forum. In both cases I posted a link to it. My post to lesswrong is¬†<a href="https://www.lesswrong.com/posts/JXTEDFCC5r4dW2tta/review-of-a-map-that-reflects-the-territory">here</a></p><p>On the lesswrong post many of the comments, plus some private emails, told me NO BILL- don't post a link, post it directly as text. It was not clear how to do that, but I got it done with help.</p><p>On complexity blog nobody commented that this was a problem. Then again, nobody commented at all, so its not clear what to make of that.¬†</p><p>So</p><p>Meta Question: Is posting a link worse than posting direct text? Note that the book review was 12 pages long and looked great in LaTeX.¬†</p><p>Meta Question: Why did lesswrong care about the format but complexityblog did not (Probably answer: omplexityblog readers did not care at all, whereas Lesswrong cared about what I though about Lesswrong)</p><p>Another Question, not Meta. One of the comments was (I paraphrase)</p><p><i>When I open a pdf file I expected to see something in the style of an academic paper. This is written in very much chatty, free-flowing blog post style with jokes like calling neologisms ``newords'', so the whole think felt more off-kilter than was intended. The style of writing would prob work better as an HTML blog post (which¬†could then be posted directly as a Lesswrong post here instead of hosted elsewhere and linked.)</i></p><p>I think its interesting that the format of an article telegraphs (in this case incorrectly) what type of article it will be. Is this a common problem?¬† I have had the experience of reading a real academic paper and being surprised that some joke or cultural-reference is in it, though I do not object to this.¬†</p><p>Another comment and question</p><p><i>I was surprised the post only had 11 karma when I saw it (William had send me an advance copy and I'd really liked reading it) but when I saw that it was a link post, I understood why.</i></p><p>I find this hilarious- they have some way the posts are rated!¬† For one, Lance told me very early on to never worry about comments, and I don't. Second, it reminds me of the Black Mirror episode¬†<a href="https://en.wikipedia.org/wiki/Nosedive_(Black_Mirror)">Nosedive</a>.</p><p>ANYWAY, I have reviewed another collection of essays for less wrong, this one called <i>The Engines of</i> <i>Cognition. </i>I am posting it here as a link:¬†<a href="https://www.cs.umd.edu/~gasarch/bookrev/FRED/lesswrong2.pdf">here</a>¬† and I will post it on lesswrong as full text (with help) in a few days.¬†</p><p>I am posting it so I can get comments before I submit it to the SIGACT News book review column. But this is odd since I think this blog has more readers than SIGACT news has subscribers, so perhaps THIS is its real debut, not that. And of course the lesswrong forum is a place where more will read it since its about them.¬†</p><p>So- I appreciate comments to make it a better review!</p><p><br /></p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/07/review-of-engines-of-cognition-essays.html"><span class="datestr">at July 12, 2022 01:22 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/data-transfer/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/data-transfer/">A Data-Based Perspective on Transfer Learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%;" href="https://arxiv.org/abs/2207.05739" class="bbutton">
<i class="fas fa-file-pdf"></i>
¬† ¬† Paper
</a>
<a style="float: left; width: 45%;" href="https://github.com/MadryLab/data-transfer" class="bbutton">
<i class="fab fa-github"></i>
¬†¬† Code
</a>
<br />
<i>
In our latest paper, we present a framework for pinpointing the impact of the source datasets in transfer learning. Our framework enables us to improve transfer learning performance by removing source datapoints that are detrimental for a specific target task. It also unlocks various other capabilities, such as debugging transfer learning failures, automatically identifying granular subpopulations in the target dataset, and detecting data leakage between source and target datasets. 
</i></p>

<p>Transfer learning is a widely utilized technique for adapting a model trained on a source dataset to improve performance on a downstream target task. Used in applications ranging from <a href="https://arxiv.org/abs/2101.06871">radiology</a>, <a href="https://openaccess.thecvf.com/content_cvpr_2017_workshops/w13/papers/Kim_End-To-End_Ego_Lane_CVPR_2017_paper.pdf">autonomous driving</a>, to <a href="https://arxiv.org/abs/1510.00098">satellite imagery analysis</a>, the transfer learning paradigm also fuels the recent emergence of large vision and language models trained on enormous amounts of data, such as <a href="https://openai.com/blog/clip/">CLIP</a> or <a href="https://openai.com/blog/gpt-3-apps/">GPT-3</a>.</p>

<p>Why is transfer learning so effective though? And, in particular, what drives transfer learning performance? Definitely, much depends on the properties of the source model, i.e., the model trained on the source dataset. For example, recent works highlight the impact of the model‚Äôs <a href="https://arxiv.org/abs/2101.06871">architecture</a>, <a href="https://arxiv.org/abs/1805.08974">accuracy</a>, <a href="https://arxiv.org/abs/2007.08489">adversarial vulnerability</a>, and <a href="https://arxiv.org/abs/1905.05901">training procedure</a>.</p>

<p>But, in addition to the source model, it is hard to not expect the source dataset to play a major role as well. Indeed, several works have shown that increasing the size of the dataset usually <a href="https://arxiv.org/abs/1912.11370">boosts transfer learning performance</a>. <a href="https://arxiv.org/abs/1811.07056">Others</a> have found that limiting the source dataset to images that are relevant to the target task can help as well. So: what is the exact role of the source dataset in transfer learning?</p>

<p>In our most <a href="https://arxiv.org/abs/2207.05739">recent paper</a>, we present a framework for pinpointing the impact of the source dataset on the downstream predictions in transfer learning. This framework draws inspiration from techniques such as <a href="https://arxiv.org/abs/2008.03703">influence functions</a> and <a href="https://arxiv.org/abs/2202.00622">datamodels</a> and it enables us, in particular, to automatically identify source datapoints that‚Äîpositively or negatively‚Äîimpact transfer learning performance.</p>

<p>We‚Äôll now walk through how we calculate the influence of the source dataset in transfer learning, and then demonstrate how our framework can be used to:</p>

<ul type="a">
  <li>Boost transfer learning performance by removing detrimental source datapoints;</li>
  <li>Automatically extract granular subpopulations in the target dataset by projecting the  class hierarchy of the source dataset onto it; and</li>
  <li>Surface pathologies such as source-target data leakage and misleading or mislabelled source datapoints.</li>
</ul>

<h2 id="computing-the-influence-of-the-source-dataset">Computing the Influence of the Source Dataset</h2>

<p>How to pinpoint the relationship between the source dataset‚Äôs composition and the model‚Äôs downstream predictions? We build here on the <a href="https://arxiv.org/abs/2008.03703">influence functions</a> and <a href="https://arxiv.org/abs/2202.00622">datamodels</a> methodology (check out our <a href="https://gradientscience.org/datamodels-1/">previous post</a> for a deeper dive into these) to study the counterfactual effect of removing source datapoints on the target model‚Äôs predictions. However, unlike in the standard supervised setting in which the focus is on individual datapoints, here we focus on removing entire classes. This is motivated by the fact that we expect the removal of entire classes to have a more measurable impact on the features learned by the source model (and thus the resulting model‚Äôs predictions).</p>

<p>So, at a high level, we first train a large number of models on random subsets of classes in the source dataset, and fine-tune them on the target task. Then we compute the influence of a source class on a target example by simply measuring the average difference in the model‚Äôs performance on a given target example when the class was included versus excluded from the source dataset. A positive influence value thus means that including the class improved the model‚Äôs performance on that example, while a negative value indicates that the class was detrimental to the correctness of the corresponding model‚Äôs prediction.</p>

<h2 id="identifying-the-most-influential-classes-of-the-source-dataset">Identifying the Most Influential Classes of the Source Dataset</h2>
<p>Now that we‚Äôve calculated these influences, how can we use them to study transfer learning? First, let‚Äôs take a look at the ranking of the most positively and negatively influencing classes for a variety of target tasks. We first look at the influences from different ImageNet classes to the entire CIFAR-10 test set:</p>

<p><img src="https://gradientscience.org/assets/data-transfer/images/most_influencing_classes.png" style="width: 100%; margin-left: 0; margin-right: 0;" alt="Most influential" />
</p>

<p>Note that in the most positive source classes tend to semantically overlap with the classes in the target dataset. For example, tailed frog and sorrel horse have the most positive influence for the CIFAR-10 dataset, which contains the classes frog and horse.</p>

<p>Also, the plot above suggests that there is a number of source classes, such as bookshop or jigsaw puzzle, whose inclusion actually hurts the overall transfer learning performance. So what happens if we indeed remove these classes from the source dataset? Well, as one might hope, they do boost the transfer learning performance on a variety of downstream image classification:</p>

<p><img src="https://gradientscience.org/assets/data-transfer/images/main_counterfactual_exp.png" style="width: 100%; margin-left: 0; margin-right: 0;" alt="Counterfactual main" /></p>
<div class="footnote">
Target task accuracies after removing the most positively or negatively influential ImageNet classes from the source dataset.
</div>

<p>In fact, we can get an accuracy boost of nearly 2.5% on CIFAR-10 (as compared to what one gets from pre-training with the full ImageNet dataset).</p>

<h2 id="leveraging-influences-to-study-transfer-learning">Leveraging Influences to Study Transfer Learning</h2>

<p>Above, we used our framework to pinpoint the most influential‚Äîbe it positively or negatively‚Äî classes for transfer learning. Here, we‚Äôll discuss how our framework provides us with a broader set of tools for studying transfer learning, including: (1) debugging transfer learning failures, (2) automatically extracting granular subpopulations in the target dataset, and (3) detecting data leakage between source and target datasets.</p>

<h3 id="1--debugging-the-failures-of-the-transferred-model">(1)  Debugging the failures of the transferred model</h3>
<p>Suppose our transferred model wrongly predicts the dog image displayed in the figure below‚Äîit labels it as a horse. Can we use our framework to understand why our model is making this mistake? Yes! The influences we computed enable us to identify the source class sorrel horse as one having a strong (and the strongest) negative influence on our image of interest. This suggests that the features learned by the source model due to the presence of this class might be the culprit here. Indeed, once we remove the sorrel horse class from the source dataset, our model now makes the correct prediction on our dog image much more frequently (with respect to the randomness of the training procedure).</p>

<div style="overflow: auto; text-align: center;" id="debug_examples_widget"></div>
<div class="footnote">
Identifying highly negatively influencing source classes can explain why our transfer learning model made a mistake (middle). Once we remove the most negatively influencing class from the source dataset, the model predicts the correct label more frequently (right). Click through the thumbnails on the left to see more examples!
</div>



<h3 id="2-automatically-extracting-granular-subpopulations-in-the-target-dataset">(2) Automatically extracting granular subpopulations in the target dataset</h3>
<p>Imagine you want to find all the images of ostriches in the CIFAR-10 dataset. However, the problem is that CIFAR-10 does not contain any subpopulation annotations that could help with this task and having to manually look for ostriches among all the images in the bird class is not a very appealing alternative. Our framework allows us to do something much more scalable!</p>

<p>Indeed, as we already observed, the most positively influencing source classes usually semantically overlap with the images in the target dataset they influence the most. In fact, this goes further: the target images which are most influenced by a given source class tend to share relevant salient features too. So, to identify our ostrich subpopulation in CIFAR-10, we just need to look at all the images that are most influenced by the source class ‚Äúostrich‚Äù! Below we display some of the CIFAR-10 images identified in this way.</p>

<div style="overflow: auto; text-align: center;" id="subpop_examples_widget"></div>
<div class="footnote">
The CIFAR-10 images which are most positively influenced by a particular ImageNet class. Click through the thumbnails on the left to see more examples!
</div>

<h3 id="3-detecting-data-leakage-and-misleading-source-dataset-examples">(3) Detecting data-leakage and misleading source dataset examples</h3>
<p>Thus far, we have focused on the role of classes in the source dataset in transfer learning. But we can also compute the influences of specific source examples on the transferred model‚Äôs predictions. This turns out to enable us to isolate, in particular, instances of data leakage and misleading examples in the source dataset.</p>

<p>Indeed, below, we display ImageNet training examples that are highly influential on CIFAR-10 test examples. The source images that have a highly positive influence are often identical copies of images from the target task (just at a higher resolution)‚Äîa clear example of data leakage. On the other hand, images that have a high negative influence tend to be the ones that are misleading, mislabeled, or otherwise surprising.  For example, the presence of the (amusing) ImageNet image of a flying lawnmower (see below) hurts the performance on a CIFAR-10 image of a regular (but similarly shaped) airplane.</p>

<p><img src="https://gradientscience.org/assets/data-transfer/images/detect_leakage.png" style="width: 100%; margin-left: 0; margin-right: 0;" alt="Detect leakage" />
</p>

<h3 id="conclusions">Conclusions</h3>
<p>In this post, we described a new framework for pinpointing the impact of the source dataset in transfer learning. Our framework allows one to improve the transfer learning performance on a range of downstream tasks by identifying and removing source datapoints that are detrimental. Furthermore, by using our framework one can automatically extract granular subpopulations in the target dataset by leveraging the fine-grained class hierarchy of the source dataset, better understand how the errors of the model on the downstream task are rooted in the source dataset, and detect potential data leakage from the source to the downstream dataset. We believe our framework provides a new perspective on transfer learning by highlighting the role of the source dataset in the transfer learning pipeline and gives us a toolkit for performing a fine-grained analysis of it.</p></div>







<p class="date">
<a href="https://gradientscience.org/data-transfer/"><span class="datestr">at July 12, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/07/11/postdoc-position-in-computer-science-logic-at-university-of-sheffield-apply-by-july-20-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/07/11/postdoc-position-in-computer-science-logic-at-university-of-sheffield-apply-by-july-20-2022/">PostDoc position in Computer Science Logic at University of Sheffield (apply by July 20, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>All candidates interested in working in logics and complexity theory utilising numerical features and real valued data are encouraged to apply.</p>
<p>The project topics range from logical foundations of probabilistic data and complexity theory utilising real numbers to logical approach to quantum information theory utilising the newly discovered connections to probabilistic team semantics.</p>
<p>Website: <a href="https://www.jobs.ac.uk/job/CRB003/research-associate-in-computer-science-logic-x2-positions">https://www.jobs.ac.uk/job/CRB003/research-associate-in-computer-science-logic-x2-positions</a><br />
Email: j.t.virtema@sheffield.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/07/11/postdoc-position-in-computer-science-logic-at-university-of-sheffield-apply-by-july-20-2022/"><span class="datestr">at July 11, 2022 03:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/07/11/associate-professor-or-tenure-track-assistant-professor-at-technical-university-of-denmark-apply-by-october-1-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/07/11/associate-professor-or-tenure-track-assistant-professor-at-technical-university-of-denmark-apply-by-october-1-2022/">Associate Professor or Tenure Track Assistant Professor at Technical University of Denmark (apply by October 1, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>DTU Compute‚Äôs section for Algorithms, Logic, and Graphs (AlgoLoG) invites applications for our next assistant or associate professor within logic and logic-based artificial intelligence or algorithms and data structures. The AlgoLoG section focuses on research in the foundations of computer science and discrete mathematics and application in industry.</p>
<p>Website: <a href="https://www.dtu.dk/english/about/job-and-career/vacant-positions/job?id=77ec4bc1-f834-4d49-96da-e263a7abb56c">https://www.dtu.dk/english/about/job-and-career/vacant-positions/job?id=77ec4bc1-f834-4d49-96da-e263a7abb56c</a><br />
Email: phbi@dtu.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/07/11/associate-professor-or-tenure-track-assistant-professor-at-technical-university-of-denmark-apply-by-october-1-2022/"><span class="datestr">at July 11, 2022 11:34 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://scottaaronson.blog/?p=6546">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://scottaaronson.blog/?p=6546">Linkz!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>(1) Fellow <a href="https://lucatrevisan.wordpress.com/">CS theory blogger</a> (and, 20 years ago, member of my PhD thesis committee) Luca Trevisan <a href="http://bulletin.eatcs.org/index.php/beatcs/article/download/701/733">interviews me about <em>Shtetl-Optimized</em></a>, for the <em>Bulletin of the European Association for Theoretical Computer Science</em>.  Questions include: what motivates me to blog, who my main inspirations are, my favorite posts, whether blogging has influenced my actual research, and my thoughts on the role of public intellectuals in the age of social-media outrage.</p>



<p>(2) Anurag Anshu, Nikolas Breuckmann, and Chinmay Nirkhe have apparently <a href="https://arxiv.org/abs/2206.13228">proved the NLTS (No Low-Energy Trivial States) Conjecture</a>!  This is considered a major step toward a proof of the famous <a href="https://arxiv.org/abs/1309.7495?context=cs">Quantum PCP Conjecture</a>, which‚Äîspeaking of one of Luca Trevisan‚Äôs questions‚Äîwas <a href="https://scottaaronson.blog/?p=139">first publicly raised</a> right here on <em>Shtetl-Optimized</em> back in 2006.</p>



<p>(3) The Microsoft team has finally <a href="https://arxiv.org/abs/2207.02472">released its promised paper</a> about the detection of Majorana zero modes (‚Äúthis time for real‚Äù), a major step along the way to creating topological qubits.  See also this <a href="https://www.youtube.com/watch?v=RnYghkDaHH0&amp;t=3691s">live YouTube peer review</a>‚Äîis that a thing now?‚Äîby Vincent Mourik and Sergey Frolov, the latter having been instrumental in the retraction of Microsoft‚Äôs previous claim along these lines.  I‚Äôll leave further discussion to people who actually understand the experiments.</p>



<p>(4) I‚Äôm looking forward to the <a href="https://computationalcomplexity.org/">2022 Conference on Computational Complexity</a> less than two weeks from now, in my ‚Ä¶ safe? clean? beautiful? awe-inspiring? ‚Ä¶ birth-city of Philadelphia.  There I‚Äôll listen to a <a href="https://computationalcomplexity.org/Archive/2022/program.php">great lineup of talks</a>, including one by my PhD student William Kretschmer on his joint work with me and DeVon Ingram on <a href="https://arxiv.org/abs/2111.10409">The Acrobatics of BQP</a>, and to co-receive the CCC Best Paper Award (wow! thanks!) for that work.  I look forward to meeting some old and new <em>Shtetl-Optimized</em> readers there.</p></div>







<p class="date">
by Scott <a href="https://scottaaronson.blog/?p=6546"><span class="datestr">at July 09, 2022 09:19 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/097">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/097">TR22-097 |  Unstructured Hardness to Average-Case Randomness | 

	Roei Tell, 

	Lijie Chen, 

	Ron D. Rothblum</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The leading technical approach in uniform hardness-to-randomness in the last two decades faced several well-known barriers that caused results to rely on overly strong hardness assumptions, and yet still yield suboptimal conclusions.

In this work we show uniform hardness-to-randomness results that *simultaneously break through all of the known barriers*. Specifically, consider any one of the following three assumptions:

1. For some $\epsilon&gt;0$ there exists a function $f$ computable by uniform circuits of size $2^{O(n)}$ and depth $2^{o(n)}$ such that $f$ is hard for probabilistic time $2^{\epsilon\cdot n}$.

2. For every $c\in\mathbb{N}$ there exists a function $f$ computable by logspace-uniform circuits of polynomial size and depth $n^2$ such that every probabilistic algorithm running in time $n^{c}$ fails to compute $f$ on a $(1/n)$-fraction of the inputs.

2. For every $c\in\mathbb{N}$ there exists a logspace-uniform family of arithmetic formulas of degree $n^2$ over a field of size $\mathrm{poly}(n)$ such that no algorithm running in probabilistic time $n^{c}$ can evaluate the family on a worst-case input.

Assuming any of these hypotheses, where the hardness is for every sufficiently large input length $n\in\mathbb{N}$, we deduce that $\mathcal{RP}$ can be derandomized in *polynomial time and on *all input lengths*, on average. Furthermore, under the first assumption we also show that $\mathcal{BPP}$ can be derandomized in polynomial time, on average and on all input lengths, with logarithmically many advice bits.

On the way to these results we also resolve two related open problems. First, we obtain an *optimal worst-case to average-case reduction* for computing problems in linear space by uniform probabilistic algorithms; this result builds on a new instance checker based on the doubly efficient proof system of Goldwasser, Kalai, and Rothblum (J. ACM, 2015). Secondly, we resolve the main open problem in the work of Carmosino, Impagliazzo and Sabin (ICALP 2018), by deducing derandomization from weak and general fine-grained hardness hypotheses.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/097"><span class="datestr">at July 08, 2022 08:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/096">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/096">TR22-096 |  Communication Complexity of Collision | 

	Mika G√∂√∂s, 

	Siddhartha Jain</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The Collision problem is to decide whether a given list of numbers $(x_1,\ldots,x_n)\in[n]^n$ is $1$-to-$1$ or $2$-to-$1$ when promised one of them is the case. We show an $n^{\Omega(1)}$ randomised communication lower bound for the natural two-party version of Collision where Alice holds the first half of the bits of each $x_i$ and Bob holds the second half. As an application, we also show a similar lower bound for a weak bit-pigeonhole search problem, which answers a question of Itsykson and Riazanov (CCC 2021).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/096"><span class="datestr">at July 08, 2022 08:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://scottaaronson.blog/?p=6541">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://scottaaronson.blog/?p=6541">Einstein-Bohr debate settled once and for all</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>In Steven Pinker‚Äôs <a href="https://scottaaronson.blog/?p=6524">guest post from last week</a>, there‚Äôs one bit to which I never replied.  Steve wrote:</p>



<blockquote class="wp-block-quote"><p>After all, in many areas Einstein was no Einstein. You [Scott] above all could speak of his not-so-superintelligence in quantum physics‚Ä¶</p></blockquote>



<p>While I can‚Äôt speak ‚Äúabove all,‚Äù OK, I can speak.  Now that we‚Äôre closing in on a century of quantum physics, can we <em>finally</em> adjudicate what Einstein and Bohr were right or wrong about in the 1920s and 1930s?  (Also, how is it still even a thing people argue about?)</p>



<p>The core is this: when confronted with the phenomena of <a href="https://en.wikipedia.org/wiki/Quantum_entanglement">entanglement</a>‚Äîincluding the ability to measure one qubit of an <a href="https://en.wikipedia.org/wiki/Bell_state">EPR pair</a> and thereby collapse the other in a basis of one‚Äôs choice (as we‚Äôd put it today), as well as the possibility of a whole pile of gunpowder in a coherent superposition of exploding and not exploding (Einstein‚Äôs example in a letter to Schr√∂dinger, which the latter then infamously transformed into a cat)‚Äîwell, there are entire conferences and edited volumes about what Bohr and Einstein said, didn‚Äôt say, meant to say or tried to say about these matters, but in cartoon form:</p>



<ul><li>Einstein said that quantum mechanics can‚Äôt be the final answer, it has ludicrous implications for reality if you actually take it seriously, the resolution must be that it‚Äôs just a statistical approximation to something deeper, and at any rate there‚Äôs clearly more to be said.</li><li>Bohr (translated from Ponderousness to English) said that quantum mechanics sure <em>looks like</em> a final answer and not an approximation to anything deeper, there‚Äôs not much more to be said, we don‚Äôt even <em>know</em> what the implications are for ‚Äúreality‚Äù (if any) so we shouldn‚Äôt hyperventilate about it, and mostly we need to change the way we use words and think about our own role as observers.</li></ul>



<p>A century later, do we know anything about these questions that Einstein and Bohr didn‚Äôt?  Well, we now know the famous <a href="https://en.wikipedia.org/wiki/Bell%27s_theorem">Bell inequality</a>, the experiments that have demonstrated Bell inequality violation with increasing finality (most recently, in 2015, <a href="https://scottaaronson.blog/?p=2464">closing</a> both the detector and the locality loopholes), other constraints on hidden-variable theories (e.g. <a href="https://en.wikipedia.org/wiki/Kochen%E2%80%93Specker_theorem">Kochen-Specker</a> and <a href="https://en.wikipedia.org/wiki/PBR_theorem">PBR</a>), <a href="https://en.wikipedia.org/wiki/Quantum_decoherence">decoherence theory</a>, and the experiments that have <a href="https://medium.com/the-physics-arxiv-blog/physicists-smash-record-for-wave-particle-duality-462c39db8e7b">manufactured</a> increasingly enormous superpositions (still, for better or worse, not exploding piles of gunpowder or cats!), while also verifying detailed predictions about how such superpositions decohere due to entanglement with the environment rather than some mysterious new law of physics.</p>



<p>So, if we were able to send a single short message back in time to the 1927 Solvay Conference, adjudicating between Einstein and Bohr without getting into any specifics, what should the message say?  Here‚Äôs my attempt:</p>



<ul><li>In 2022, quantum mechanics <em>does</em> still seem to be a final answer‚Äînot an approximation to anything deeper as Einstein hoped.  And yet, contra Bohr, there <em>was</em> considerably more to say about the matter!  The implications for reality could indeed be described as ‚Äúludicrous‚Äù from a classical perspective, arguably even <em>more</em> than Einstein realized.  And yet the resolution turns out simply to be that we live in a universe where those implications are true.</li></ul>



<p>OK, here‚Äôs the point I want to make.  Even supposing you agree with me (not everyone will) that the above would be a reasonable modern summary to send back in time, <em>it‚Äôs still totally unclear how to use it to mark the Einstein vs. Bohr scorecard!</em>  </p>



<p>Indeed, it‚Äôs not surprising that partisans have defended every possible scoring, from 100% for Bohr (quantum mechanics vindicated! Bohr called it from the start!), to 100% for Einstein (he put his finger directly on the implications that needed to be understood, against the evil Bohr who tried to shut everyone up about them!  Einstein FTW!).</p>



<p>Personally, I‚Äôd give <em>neither</em> of them perfect marks, in part because they not only both missed Bell‚Äôs Theorem, but failed even to ask the requisite question (namely: what empirically verifiable tasks can Alice and Bob <em>use entanglement to do</em>, that they couldn‚Äôt have done without entanglement?).  But I‚Äôd give both of them very high marks for, y‚Äôknow, still being Albert Einstein and Niels Bohr.</p>



<p>And with that, I‚Äôm proud to have said the final word about precisely what Einstein and Bohr got right and wrong about quantum physics.  I‚Äôm relieved that no one will ever need to debate that tiresome historical question again ‚Ä¶ certainly not in the comments section of this post.</p></div>







<p class="date">
by Scott <a href="https://scottaaronson.blog/?p=6541"><span class="datestr">at July 08, 2022 01:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://gradientscience.org/bias-transfer/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://gradientscience.org/bias-transfer/">When does Bias Transfer in Transfer Learning?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left; width: 45%;" href="https://arxiv.org/abs/2207.02842" class="bbutton">
<i class="fas fa-file-pdf"></i>
¬† ¬† Paper
</a>
<a style="float: left; width: 45%;" href="https://github.com/MadryLab/bias-transfer" class="bbutton">
<i class="fab fa-github"></i>
¬†¬† Code
</a>
<br /></p>

<p>Suppose we want to train a classifier to distinguish between dogs and cats given a labeled dataset of photos. Having read about the perils of training on biased or otherwise suboptimal data, we comb through our collection of labeled images and carefully eliminate problems like <a href="https://arxiv.org/abs/1904.08818">spurious correlations</a> or <a href="https://arxiv.org/abs/1906.02659">under-represented subgroups</a>. So, for example, finding that most cats in the dataset were photographed indoors while most dogs were photographed outdoors, we collect more outdoor cat photos and indoor dog photos.</p>

<p>The result is a carefully curated dataset that we can now use to train our classifier. Unfortunately, there is a problem: our dataset is small, so training on it yields a poorly performing model. But, fortunately, we know what to do! To get a better classifier, we go online and <a href="https://github.com/rwightman/pytorch-image-models">download</a> (or <a href="https://aws.amazon.com/rekognition/">pay for</a>) a pre-trained model, i.e., a model that‚Äôs been trained on a much larger dataset like <a href="https://image-net.org/index.php">ImageNet-1K</a> or <a href="https://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html">JFT-300</a>. We‚Äôll then use transfer learning to adapt this pre-trained model (the ‚Äúsource model‚Äù) to our dataset (the ‚Äútarget dataset‚Äù). Transfer learning from a pre-trained model in this way results in a model that performs much better on our target task than one trained on it from scratch.</p>

<p>This seems like great news‚Äîwe get a much better performance at little cost! But there is a potential wrinkle: pre-trained models turn out to have a variety of undesirable biases. For example, they can disproportionately rely on <a href="https://arxiv.org/abs/1811.12231">texture</a>, on <a href="https://arxiv.org/abs/2006.09994">image background</a>, or on <a href="https://papers.nips.cc/paper/2019/hash/97af07a14cacba681feacf3012730892-Abstract.html">object location/orientation</a>. These biases even arise in production-level pretrained models‚ÄîAmazon‚Äôs Rekognition, for example, <a href="https://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212">performs disparately across races and genders</a>. Could these biases show up in our target model, despite our careful curation of our target dataset?</p>

<p>In our most recent <a href="https://arxiv.org/abs/2207.02842">work</a>, we find that biases from pre-trained models indeed tend to ‚Äútransfer‚Äù, i.e., they can manifest themselves in the resulting model. In particular, we study this ‚Äúbias transfer‚Äù through the lens of the following questions:</p>

<ol type="a">
  <li>When can bias transfer happen? What is the effect of the pre-training dataset, the transfer learning algorithm, and the target dataset on this bias transfer? Moreover, can we avoid bias transfer altogether by intervening on our target dataset?
</li>
  <li>Does bias transfer actually happen in practice? For example, suppose we pre-train a model on the ImageNet-1K dataset and use transfer learning to adapt it to CIFAR-10. Can we pinpoint concrete biases of the resulting model that are not present when we train on CIFAR-10 from scratch?</li>
</ol>

<p>We‚Äôll dive into some of these questions below (see our paper for a more thorough treatment).</p>

<h2 id="when-does-bias-transfer-occur">(When) does bias transfer occur?</h2>
<p>Let us consider a simple experiment. We take the ImageNet-1K dataset (a popular pre-training dataset for transfer learning), and add a small yellow square to every image of a dog in its training set:
<img src="https://gradientscience.org/assets/bias-transfer/images/yellowsquare_only.jpg" style="width: 25%; margin-top: 1em; margin-bottom: 1em;" alt="Yellow Square" /></p>

<p>As expected, training on this dataset yields a model that is highly sensitive to the presence of yellow squares. Indeed, when the yellow square is planted on images of other (non-dog) objects, the model predicts a dog class 44% of the time (as opposed to 2% for a standard ImageNet model)!</p>

<p>We now want to use this yellow square sensitivity as a (simple) example of a bias in a pre-trained model. Specifically, to study the possibility of bias transfer, we apply transfer learning to obtain models for a variety of target tasks using both this biased model and a standard ImageNet model as the point of start. We then examine whether transfer learning from the biased model leads to a significantly higher sensitivity to yellow squares. (Note that none of the target datasets have any of these yellow squares at all.)</p>

<p><img src="https://gradientscience.org/assets/bias-transfer/images/yellowsquare.jpg" style="width: 100%;" alt="Yellow Square Summary" /></p>

<p>As we find, models that are transferred from the biased pre-trained network were significantly more sensitive to the presence of our yellow square. This happened consistently across two transfer learning techniques (fixed-feature and full-network transfer learning) and a variety of datasets we considered.  For example, below, we plot the attack success rate (ASR)‚Äîthe probability that introducing a yellow square into a random image will flip a model‚Äôs prediction‚Äîfor models transferred from both the biased (Spurious) model and standard (Non-Spurious) model:</p>

<p><img src="https://gradientscience.org/assets/bias-transfer/images/mega_vary_dataset.jpg" style="width: 100%;" alt="Vary Dataset" />
</p>

<p>However, what happens if we try to more carefully design the target dataset to specifically counter bias transfer? For example, if in this case we‚Äôre worried that our pre-trained model has a bias associating the yellow square with the ‚Äúdog‚Äù classes, we might put an extra effort and alter the target dataset to make sure that (a) the yellow square is also present in some of the images but (b) its presence is not correlated with any of the classes.  So, is using such a ‚Äúde-biased‚Äù target dataset enough to avoid bias transfer?</p>

<p>Turns out that the answer is: it depends! Specifically, we find that when using a particular kind of transfer learning (full-network transfer learning), adding the backdoor trigger to the target dataset indeed mitigates bias transfer. However, when using the fixed-feature transfer learning, the transferred model still contains the bias‚Äîeven when 10% of the target dataset contains (uncorrelated) yellow squares.</p>

<p><img src="https://gradientscience.org/assets/bias-transfer/images/debias_mega.jpg" style="width: 100%; margin-left: 0; margin-right: 0;" alt="Debias Dataset" />
</p>

<p>So overall, not only does bias transfer across various datasets and transfer learning settings, it also might be non-trivial (at least in the fixed-feature transfer learning setting) to remedy this bias transfer even when we are aware of the possibility of that bias and try to explicitly de-bias the target dataset.</p>

<h2 id="bias-transfer-in-the-wild">Bias Transfer in the Wild</h2>
<p>Ok, so we know now that bias transfer <em>can</em> happen‚Äîat least when there‚Äôs a significant bias planted in the source pre-trained model. But in practice, source datasets will rarely have a feature as consistent and predictive as a planted yellow square we considered above. Can biases naturally occurring in common image datasets transfer as well?</p>

<p>It turns out that the answer here is: yes, too! For instance, let‚Äôs consider a (rather unsurprising) bias arising in the widely-used ImageNet dataset: a circular yellow shape is predictive for the ‚Äútennis ball‚Äù class. (See our <a href="https://arxiv.org/abs/2207.02842">paper</a> for many other examples and datasets.) Indeed, as we can see below, if we overlay a yellow circle on any ImageNet image, the model becomes more likely to output ‚Äútennis ball:‚Äù</p>

<div style="overflow: auto; text-align: center;" id="bias_examples_widget"></div>
<div class="footnote">
    Shift in ImageNet predictions when we apply an intervention, such as overlaying a tennis ball on the image. Click through the thumbnails on the left to see more interventions.
</div>

<p>Now, what happens if we use transfer learning to obtain a model for a target task, such as CIFAR-10 from  a pre-trained ImageNet model containing this yellow circle -&gt; ‚Äútennis ball‚Äù bias? Even though CIFAR-10 does not contain the yellow circle -&gt; ‚Äútennis ball‚Äù bias, this bias still persists in the resulting transfer-learned model! In particular, when we evaluate our model on CIFAR-10 test set images overlaid with a yellow circle, the model fine-tuned from a pre-trained ImageNet model is far more sensitive to that signal than a CIFAR-10 model trained from scratch. Indeed, there is an overall skew of the output class distribution for the transfer-learned model, in contrast to an almost uniform output class distribution of the model trained from scratch.</p>

<div style="overflow: auto; text-align: center;" id="cifar_examples_widget"></div>
<div class="footnote">
    Shift in CIFAR-10 predictions after applying an intervention when either training from scratch or transferring from a biased source model. We consider fixed-feature (left) or full-network (right) fine-tuning. The models which were pre-trained on a biased model are more sensitive to the intervention than those trained from scratch. 
</div>

<h2 id="conclusions">Conclusions</h2>
<p>In this post, we discussed our <a href="https://arxiv.org/abs/2207.02842">recent work</a> that demonstrates that biases that exist in pretrained models can persist even if these models are adapted to target datasets which do not contain these biases. It is thus important to understand the biases of the pre-trained models one uses for transfer learning, even if the task these models were pre-trained on do not seem relevant to the target task we want to solve. This prudence is particularly key in the context of high-stakes real-world machine learning applications that often suffer from data scarcity and tend to use transfer learning as a remedy for that.</p></div>







<p class="date">
<a href="https://gradientscience.org/bias-transfer/"><span class="datestr">at July 07, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=22946">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2022/07/06/icm-2022-awarding-ceremonies-1/">ICM 2022 awarding ceremonies (1)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<h2><img width="464" alt="fm2022" src="https://gilkalai.files.wordpress.com/2022/07/fm2022.png" class="alignnone  wp-image-22983" height="661" /></h2>
<h2>Hugo Duminil-Copin, June Huh, James Maynard and Maryna Viazovska were awarded the Fields Medal 2022 and Mark Braverman was awarded the Abacus Medal 2022.</h2>
<p>I am writing from Helsinki where I attended the meeting of the General Assembly of the IMU and yesterday I took part in the moving award ceremonies of ICM2022 hosted by Aalto University.¬† This will be the first post about the ICM 2020 award ceremonies.</p>
<p>The opening day of ICM2022 was exciting. Hugo Duminil-Copin, June Huh, James Maynard and Maryna Viazovska were awarded the Fields Medals 2022. Mark Braverman was awarded the Abacus Medal. The event <a href="https://youtu.be/6I0siVD7RBI">was videotaped and can be found here</a>. <strong>Update</strong>: The <a href="https://www.youtube.com/watch?v=Uh2gqiEC6eM">five lectures of the medalists can be found here</a>.</p>
<p>In the ceremony, I gave the laudation for June Huh.¬† Here are <a href="https://gilkalai.files.wordpress.com/2022/07/jhicm2022.pptx">the slides of my talk.</a> The preliminary version of my proceeding paper is <a href="https://www.mathunion.org/fileadmin/IMU/Prizes/Fields/2022/laudatio-jh.pdf">here on the IMU site</a>. Please alert me about mistakes in my paper or if you have any suggestions for changes or additions. (I already found that on two occasions I embarrassingly wrote ‚ÄúBr√§nd√©n‚Äù Instead of ‚ÄúBraden‚Äù, sorry for that.) <strong>Update:</strong> <a href="https://gilkalai.files.wordpress.com/2022/07/laudatio-jh-20220709.pdf">Here is a corrected version.</a></p>
<p><a href="https://www.mathunion.org/imu-awards/fields-medal/fields-medals-2022">The IMU site</a> contains a lot of material about the Fields medalist and other prize winners. It contains beautiful videos, and preliminary versions of the proceeding papers by the medalists and by those giving the laudations.</p>
<p>Andrei Okounkov wrote four wonderful detailed ‚Äúpopular scientific expositions‚Äù (those are available on the IMU site) which provide much scientific background as well as Andrei‚Äôs own scientific perspective. It is a great read for wide audience of mathematicians, ranging from¬† advanced undergraduate students.¬† Experts will also enjoy Andrei‚Äôs perspective.</p>
<h2>Svetlana Jitomirskaya was awarded the Ladyzhenskaya Prize in Mathematical Physics (in a ceremony held two days earlier), Barry Mazur was awarded the Chern Medal, Elliott Lieb was awarded the Gauss Prize, and Nikolai Andreev was awarded the Leelavati Prize.</h2>
<p>I hope to discuss these awards and some further personal and mathematical reflections in a subsequent post.</p>
<h3><span style="color: #0000ff;"><strong>Congratulations Hugo, June, James, Maryna, Mark, Svetlana, Barry, Elliott, and Nikolai!</strong></span></h3>
<h3>Here on my Blog</h3>
<p>Let me give some links to discussions here on the blog on works by laureates.</p>
<p>I wrote about Maryna Viazovska‚Äôs amazing breakthrough in the post¬† <a href="https://gilkalai.wordpress.com/2016/03/23/a-breakthrough-by-maryna-viazovska-lead-to-the-long-awaited-solutions-for-the-densest-packing-problem-in-dimensions-8-and-24/" rel="bookmark">A Breakthrough by Maryna Viazovska Leading to the Long Awaited Solutions for the Densest Packing Problem in Dimensions 8 and 24, </a>and this additional post <a href="https://gilkalai.wordpress.com/2019/02/15/henry-cohn-abhinav-kumar-stephen-d-miller-danylo-radchenko-and-maryna-viazovska-universal-optimality-of-the-e8-and-leech-lattices-and-interpolation-formulas/" rel="bookmark">Henry Cohn, Abhinav Kumar, Stephen D. Miller, Danylo Radchenko, and Maryna Viazovska: Universal optimality of the E8 and Leech lattices and interpolation formulas.</a></p>
<p>I reported here in 2009 on <a href="https://gilkalai.wordpress.com/2009/01/23/news/">the startling solution by Mark Braverman of the Linial-Nisan conjecture</a>.</p>
<p>The story of James Maynard‚Äôs startling results and the gap between primes story is <a href="https://gilkalai.wordpress.com/2013/09/20/polymath-8-a-success/">described in this post</a>. ¬†In July 2014 we ran at HUJI a beautiful <a href="http://www.ma.huji.ac.il/conf/joram.html">learning seminar on small gaps between primes,</a> where James Maynard gave a series of three lectures. His result on ‚Äúbounded intervals containing many primes‚Äù both strengthened and simplified Yitang Zhang‚Äôs earlier amazing result on ‚Äúbounded intervals containing two primes.‚Äù¬† Maynard developed large chunks of his approach independently from Zhang‚Äôs work.</p>
<p>I discussed two results by Hugo Duminil-Copin : After the start of the pandemic but before the war in Ukraine, I had a ‚Äúcheer-you-up in difficult times‚Äù corner and<a href="https://gilkalai.wordpress.com/2021/03/23/to-cheer-you-up-in-difficult-times-22-some-mathematical-news-part-1/"> in this post,¬†</a> to cheer you up,¬† I wrote about a breakthrough by Hugo Duminil-Copin, Karol Kajetan Kozlowski, Dmitry Krachun, Ioan Manolescu, Mendes Oulamara¬† (and yet another wonderful result by Hugo Vanneuville and Vincent Tasion). In <a href="https://gilkalai.wordpress.com/2015/04/01/two-delightful-major-simplifications/">this 2015 post</a> I wrote about another breakthrough by Hugo Duminil-Copinand Vincent Tasion. (And see <a href="https://gilkalai.wordpress.com/2017/08/24/where-were-we/">this post</a> for <a href="https://gilkalai.files.wordpress.com/2017/08/hugo-kkl.jpg">a picture</a> of Hugo mentioning KKL and BKKKL.)</p>
<p>And,¬† of course, I wrote about June Huh several times. Here are a few examples: About Huh‚Äôs 2018 ICM talk; <a href="https://gilkalai.wordpress.com/2018/12/24/icm-2018-rio-4-huh-balog-morris-wormald/" rel="bookmark">ICM 2018 Rio (4): Huh; Balog &amp; Morris;¬†Wormald</a>; about the Mihail-vazirani conjecture¬† <a href="https://gilkalai.wordpress.com/2018/12/12/nima-anari-kuikui-liu-shayan-oveis-gharan-and-cynthia-vinzant-solved-the-mihail-vazirani-conjecture/" rel="bookmark">Nima Anari, Kuikui Liu, Shayan Oveis Gharan, and Cynthia Vinzant Solved the Mihail-Vazirani Conjecture for Matroids! </a>; <a href="https://gilkalai.wordpress.com/2011/06/14/tentative-plans-and-belated-updates-ii/">About the early works on the Heron-Rota-Welsh conjecture for representable matroids</a>; and <a href="https://gilkalai.wordpress.com/2015/08/14/updates-and-plans-iii/">about the full solution of the Heron-Rota-Welsh conjecture by Adiprasito, Huh, and Katz.</a></p>
<p><a href="https://gilkalai.wordpress.com/2021/03/23/to-cheer-you-up-in-difficult-times-22-some-mathematical-news-part-1/">In this post</a> we tried to cheer you up with ‚Äúharmonic polytope‚Äù, which came up in Ardila, Denham, and Huh‚Äôs work on the Lagrangian geometry of matroids and were further studied by Federico Ardila and Laura Escobar.</p>
<h2></h2>
<h3></h3></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2022/07/06/icm-2022-awarding-ceremonies-1/"><span class="datestr">at July 06, 2022 02:17 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8754111030570733183">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/07/the-highland-park-shooting.html">The Highland Park Shooting</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>This week I should be celebrating Mark Braverman's <a href="https://www.quantamagazine.org/mark-braverman-wins-the-imu-abacus-medal-20220705/">Abacus Medal</a> and the <a href="https://www.quantamagazine.org/tag/2022-fields-and-abacus-medals/">Fields Medalists</a>. Instead my mind has been focused 25 miles north of Chicago.</p><p>Mass shootings in the United States have become far too commonplace, but the shooting at a fourth of July parade in Highland Park, Illinois hit home. Literally Highland Park was home for me, from 2003-2012. We've been in downtown Highland Park hundreds of times. We've attended their fourth of July parade in the past. My daughter participated in it as part of the high school marching band.¬†</p><p>We were members of North Shore Congregation Israel. My wife, who had a party planning business back then, worked closely with NSCI events coordinator¬†Jacki Sundhein, tragically killed in the attack.</p><p>We lived close to Bob's Deli and Pantry and we'd often walk over there for sandwiches or snacks, sometimes served by Bob Crimo himself. The alleged shooter, Bobby Crimo, was his son.</p><p>We spent the fourth with friends who came down from Glencoe, the town just south of Highland Park. We spent much of the day just searching for updates on our phones.</p><p>I wish we could find ways to reduce the shootings in Highland Park and those like it, the violence that plagues Chicago and other major cities and the highly polarized world we live in which both hampers real gun reforms and creates online groups that help enable these awful events. But right now I just mourn for the lives lost in the town that was my home, a town that will never fully recover from this tragedy.</p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/07/the-highland-park-shooting.html"><span class="datestr">at July 06, 2022 12:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-6277315808840696386">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2022/07/icalp-and-eatcs-turn-50.html">ICALP and the EATCS turn 50</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>These days, our colleagues at IRIF are hosting <a href="https://icalp2022.irif.fr/" target="_blank">ICALP 2022</a> in Paris. This is the 49th edition of the ICALP conference, which turns 50 since its first instalment was held in 1972. ICALP was the first conference of the, then newly founded, <a href="https://eatcs.org/" target="_blank">European Association for Theoretical Computer Science (EATCS)</a>.The rest is history and I let any readers this post might have draw their own conclusions on the role that the EATCS and ICALP have played in supporting the development of theoretical computer science. (Admittedly, my opinions on both the EATCS and ICALP are very biased.)¬†</p><p>The <a href="https://icalp2022.irif.fr/?page_id=42" target="_blank">scientific programme of ICALP 2022</a> is mouthwatering as usual, thanks to the work done by the authors of submitted papers, Miko≈Çaj Boja≈Ñczyk and David Woodruff (PC chairs), and their PCs. I encourage everyone to read the papers that are being presented at the conference.</p><p>The main purpose of this post, however, is to alert the readers of this blog that ICALP 2022 also hosts an <a href="https://icalp2022.irif.fr/?page_id=1111" target="_blank">exhibition to celebrate EATCS/ICALP at 50</a> and theoretical computer science at large. If you are in Paris, you can attend the exhibition in person. Otherwise, you can visit it virtually <a href="https://icalp2022.irif.fr/?page_id=1111" target="_blank">here</a>. (See also the posters in <a href="https://drive.google.com/file/d/1L7wLDYyDCNfSCvnNA8jWZiMb3BRLy14k/view" target="_blank">one PDF file</a>.)<br /></p><p>I had the honour to take part in the preparation of the material for that exhibition, which was led by Sandrine Cadet and Sylvain Schmitz. I learnt a lot from all the other colleagues in the committee for the exhibition.¬†</p><p>As part of that work, I asked <a href="https://www.pilucrescenzi.it/" target="_blank">Pierluigi Crescenzi</a> whether he'd be willing to carry out a graph and data mining analysis of ICALP vis-a-vis other major conferences in theoretical computer science based on DBLP data. Pierluigi's work went well beyond the call of duty and is summarised in <a href="https://slides.com/piluc/icalp-50?token=fl3BBJ8j" target="_blank">this presentation</a>. I trust that you'll find the results of the analysis by Pierluigi and three of his students at the <a href="https://sites.google.com/gssi.it/csgssi" target="_blank">Gran Sasso Science Institute</a> very interesting. If you have any suggestions for expanding that analysis further, please write it in the comment section.¬†</p><p>Let me close by wishing the EATCS and ICALP a happy 50th birthday, and a great scientific and social event to all the colleagues who are attending ICALP 2022. <br /></p></div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2022/07/icalp-and-eatcs-turn-50.html"><span class="datestr">at July 05, 2022 06:13 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
