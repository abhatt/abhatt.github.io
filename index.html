<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="https://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="http://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://scottaaronson.blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at November 04, 2021 08:39 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/04/professor-associate-professor-assistant-professor-at-george-washington-university-apply-by-december-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/04/professor-associate-professor-assistant-professor-at-george-washington-university-apply-by-december-1-2021/">Professor, Associate Professor, Assistant Professor at George Washington University (apply by December 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We invite applications to multiple faculty positions at all ranks. Our search is focused on machine learning; artificial intelligence; computer and distributed systems; security and privacy; and candidates that can support our multidisciplinary initiatives in Smart and Trustworthy Systems and Meaningful Computing, broadly defined.</p>
<p>Website: <a href="https://www.gwu.jobs/postings/87400">https://www.gwu.jobs/postings/87400</a><br />
Email: cssearch@gwu.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/04/professor-associate-professor-assistant-professor-at-george-washington-university-apply-by-december-1-2021/"><span class="datestr">at November 04, 2021 02:26 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8862914568867887724">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/11/a-complexity-view-of-machine-learning.html">A Complexity View of Machine Learning?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Complexity is at its best when it models new technologies so we can study it in a principled way. Quantum computing comes to mind as a good relatively recent example. With machine learning playing an every growing role in computing, how can complexity play a role?</p><p>The theory community questions about machine learning typically look at finding mathematical reasons to explain why the models well with little overfitting or trying to get good definitions of privacy, fairness, explainability to mitigate the social challenges of ML. But what about from a computational complexity point of view? I don't have a great answer yet but here are some thoughts.</p><p>In much of structural complexity, we use relativization to understand the relative power of complexity classes. We define an oracle as a set A where a machine can ask questions about membership to A and magically get an answer. Relativization can be used to help us define classes like Σ<sub>2</sub><sup>P</sup> = NP<sup>NP</sup> or allow us to succinctly state <a href="https://doi.org/10.1137/0220053">Toda's theorem</a> as PH in P<sup>#P</sup>.</p><p>As I <a href="https://twitter.com/fortnow/status/1453827400383488002">tweeted</a> last week, machine learning feels like an oracle, after all machine learning models and algorithms are typically accessed through APIs and Python modules. What kind of oracle? Definitely not an NP-complete problem like SAT since machine learning fails miserably if you try to use it to break cryptography. </p><p>The real information in machine learning comes from the data. For a length parameter n, consider a string x which might be exponential in n. Think of x as a list of labeled or unlabeled examples of some larger set S. Machine learning creates a model M from x that tries to predict whether x is in S. Think of M as the oracle, as some compressed version of S.</p><p>Is there a computational view of M? We can appeal to Ockham's razor and consider the simplest model consistent with the data for which x as a set are random in the S that M generates. One can formalize this Minimum Description Length approach using <a href="https://doi.org/10.1109/18.825807">Kolmogorov Complexity</a>. This model is too ideal, for one it can also break cryptography, and typical deep learning models are not simple at all with sometimes millions of parameters.</p><p>This is just a start. One could try time bounds on the Kolmogorov definitions or try something different completely. Adversarial and foundational learning models might yield different kinds of oracles. </p><p>If we can figure out even a rough complexity way to understand learning, we can start to get a hold of learning's computational power and limitations, which is the purpose of studying complexity complexity in the first place. </p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/11/a-complexity-view-of-machine-learning.html"><span class="datestr">at November 03, 2021 02:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/148">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/148">TR21-148 |  Explicit Exponential Lower Bounds for Exact Hyperplane Covers | 

	Benjamin Diamond, 

	Amir Yehudayoff</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We describe an explicit and simple subset of the discrete hypercube which cannot be exactly covered by fewer than exponentially many hyperplanes. The proof exploits a connection to communication complexity, and relies heavily on Razborov's lower bound for disjointness.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/148"><span class="datestr">at November 03, 2021 11:39 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=581">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2021/11/03/tcs-talk-wednesday-november-10-kuikui-liu-university-of-washington/">TCS+ talk: Wednesday, November 10 — Kuikui Liu, University of Washington</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, November 10th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <a href="https://homes.cs.washington.edu/~liukui17/"><strong>Kuikui Liu</strong></a> from the University of Washington will speak about “<em>Spectral Independence: A New Tool to Analyze Markov Chains</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Registration is <em>not</em> required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: Markov chain Monte Carlo is a widely used class of algorithms for sampling from high-dimensional probability distributions, both in theory and in practice. While simple to implement, analyzing the rate of convergence to stationarity, i.e. the “mixing time”, remains a challenging problem in many settings. We introduce a new technique to bound mixing times called “spectral independence”, which says that certain pairwise correlation matrices all have bounded spectral norm. This surprisingly powerful technique originates in the emerging study of high-dimensional expanders, and has allowed us to “unify” nearly all existing approaches to approximate counting and sampling by building new connections with other areas, including statistical physics, geometry of polynomials, functional analysis, and more. Through these connections, several long-standing open problems have recently been answered, including counting bases of matroids and optimal mixing of the Glauber dynamics/Gibbs sampler up to the algorithmic phase transition threshold.</p>
<p>Based on several joint works with Dorna Abdolazimi, Nima Anari, Zongchen Chen, Shayan Oveis Gharan, Eric Vigoda, Cynthia Vinzant, and June Vuong.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2021/11/03/tcs-talk-wednesday-november-10-kuikui-liu-university-of-washington/"><span class="datestr">at November 03, 2021 09:41 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://adamsheffer.wordpress.com/?p=5760">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sheffer.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://adamsheffer.wordpress.com/2021/11/03/cs-tenure-track-positions-at-cuny/">CS Tenure Track Positions at CUNY</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Interested in a computer science position in Manhattan? Apply to our positions here! We are starting to form a computer science program at CUNY’s Baruch College. Joining us at the beginning of this process will give you a chance to influence how computer science will look like at our college: the research and teaching directions that […]</div>







<p class="date">
by Adam Sheffer <a href="https://adamsheffer.wordpress.com/2021/11/03/cs-tenure-track-positions-at-cuny/"><span class="datestr">at November 03, 2021 01:38 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.01784">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.01784">Towards the 5/6-Density Conjecture of Pinwheel Scheduling</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Leszek Gąsieniec, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Smith:Benjamin.html">Benjamin Smith</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wild:Sebastian.html">Sebastian Wild</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.01784">PDF</a><br /><b>Abstract: </b>Pinwheel Scheduling aims to find a perpetual schedule for unit-length tasks
on a single machine subject to given maximal time spans (a.k.a. frequencies)
between any two consecutive executions of the same task. The density of a
Pinwheel Scheduling instance is the sum of the inverses of these task
frequencies; the 5/6-Conjecture (Chan and Chin, 1993) states that any Pinwheel
Scheduling instance with density at most 5/6 is schedulable. We formalize the
notion of Pareto surfaces for Pinwheel Scheduling and exploit novel structural
insights to engineer an efficient algorithm for computing them. This allows us
to (1) confirm the 5/6-Conjecture for all Pinwheel Scheduling instances with at
most 12 tasks and (2) to prove that a given list of only 23 schedules solves
all schedulable Pinwheel Scheduling instances with at most 5 tasks.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.01784"><span class="datestr">at November 03, 2021 10:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.01759">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.01759">Truly Low-Space Element Distinctness and Subset Sum via Pseudorandom Hash Functions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Lijie.html">Lijie Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jin:Ce.html">Ce Jin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Williams:R=_Ryan.html">R. Ryan Williams</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Hongxun.html">Hongxun Wu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.01759">PDF</a><br /><b>Abstract: </b>We consider low-space algorithms for the classic Element Distinctness
problem: given an array of $n$ input integers with $O(\log n)$ bit-length,
decide whether or not all elements are pairwise distinct. Beame, Clifford, and
Machmouchi [FOCS 2013] gave an $\tilde O(n^{1.5})$-time randomized algorithm
for Element Distinctness using only $O(\log n)$ bits of working space. However,
their algorithm assumes a random oracle (in particular, read-only random access
to polynomially many random bits), and it was asked as an open question whether
this assumption can be removed.
</p>
<p>In this paper, we positively answer this question by giving an $\tilde
O(n^{1.5})$-time randomized algorithm using $O(\log ^3 n\log \log n)$ bits of
space, with one-way access to random bits. As a corollary, we also obtain a
$\operatorname{\mathrm{poly}}(n)$-space $O^*(2^{0.86n})$-time randomized
algorithm for the Subset Sum problem, removing the random oracles required in
the algorithm of Bansal, Garg, Nederlof, and Vyas [STOC 2017].
</p>
<p>The main technique underlying our results is a pseudorandom hash family based
on iterative restrictions, which can fool the cycle-finding procedure in the
algorithms of Beame et al. and Bansal et al.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.01759"><span class="datestr">at November 03, 2021 10:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.01718">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.01718">Competitive Algorithms for Online Weighted Bipartite Matching and its Variants</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thang:Nguyen_Kim.html">Nguyen Kim Thang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.01718">PDF</a><br /><b>Abstract: </b>Online bipartite matching has been extensively studied. In the unweighted
setting, Karp et al. gave an optimal $(1 - 1/e)$-competitive randomized
algorithm. In the weighted setting, optimal algorithms have been achieved only
under assumptions on the edge weights. For the general case, little was known
beyond the trivial $1/2$-competitive greedy algorithm. Recently, Fahrbach et
al. have presented an 0.5086-competitive algorithm (for the problem in a model,
namely free-disposal), overcoming the long-standing barrier of 1/2. Besides, in
designing competitive algorithms for the online matching problem and its
variants, several techniques have been developed, in particular the primal-dual
method. Specifically, Devanur et al. gave a primal-dual framework, unifying
previous approaches and Devanur and Jain provided another scheme for a
generalization of the online matching problem.
</p>
<p>In this paper, we present competitive algorithms for the online weighted
bipartite matching in different models; in particular we achieve the optimal
$(1-1/e)$ competitive ratio in the free-disposal model and in other model,
namely stochastic reward. Our work also unifies the previous approaches by the
mean of the primal-dual technique with configuration linear programs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.01718"><span class="datestr">at November 03, 2021 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.01576">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.01576">Provably efficient, succinct, and precise explanations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blanc:Guy.html">Guy Blanc</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lange:Jane.html">Jane Lange</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tan:Li=Yang.html">Li-Yang Tan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.01576">PDF</a><br /><b>Abstract: </b>We consider the problem of explaining the predictions of an arbitrary
blackbox model $f$: given query access to $f$ and an instance $x$, output a
small set of $x$'s features that in conjunction essentially determines $f(x)$.
We design an efficient algorithm with provable guarantees on the succinctness
and precision of the explanations that it returns. Prior algorithms were either
efficient but lacked such guarantees, or achieved such guarantees but were
inefficient.
</p>
<p>We obtain our algorithm via a connection to the problem of {\sl implicitly}
learning decision trees. The implicit nature of this learning task allows for
efficient algorithms even when the complexity of $f$ necessitates an
intractably large surrogate decision tree. We solve the implicit learning
problem by bringing together techniques from learning theory, local computation
algorithms, and complexity theory.
</p>
<p>Our approach of "explaining by implicit learning" shares elements of two
previously disparate methods for post-hoc explanations, global and local
explanations, and we make the case that it enjoys advantages of both.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.01576"><span class="datestr">at November 03, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.01551">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.01551">Classifying Approximation Algorithms: Understanding the APX Complexity Class</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lee:Arthur.html">Arthur Lee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Bruce.html">Bruce Xu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.01551">PDF</a><br /><b>Abstract: </b>We are interested in the intersection of approximation algorithms and
complexity theory, in particular focusing on the complexity class APX.
Informally, APX $\subseteq$ NPO is the complexity class comprising optimization
problems where the ratio $\frac{OPT(I)}{ALG(I)} \leq c$ for all instances I. We
will do a deep dive into studying APX as a complexity class, in particular,
investigating how researchers have defined PTAS and L reductions, as well as
the notion of APX-completeness, thereby clarifying where APX lies on the
polynomial hierarchy. We will discuss the relationship of this class with
FPTAS, PTAS, APX, log-APX and poly-APX). We will sketch the proof that Max
3-SAT is APX-hard, and compare this complexity class in relation to $BPP$,
$ZPP$ to elucidate whether randomization is powerful enough to achieve certain
approximation guarantees and introduce techniques that complement the design of
approximation algorithms such as through \textit{primal-dual} analysis,
\textit{local search} and \textit{semi-definite programming}. Through the PCP
theorem, we will explore the fundamental relationship between hardness of
approximation and randomness, and will recast the way we look at the complexity
class NP. We will finish by looking at the \textit{"real world"} applications
of this material in Economics. Finally, we will touch upon recent breakthroughs
in the Metric Travelling Salesman and asymmetric travelling salesman problem,
as well original directions for future research, such as quantifying the amount
of additional compute power that access to an APX oracle provides, elucidating
fundamental combinatorial properties of log-APX problems and unique ways to
attack the problem of whether the minimum set-cover problem is self-improvable.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.01551"><span class="datestr">at November 03, 2021 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.01422">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.01422">Fast Algorithms for Hop-Constrained Flows and Moving Cuts</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haeupler:Bernhard.html">Bernhard Haeupler</a>, D Ellis Hershkowitz, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saranurak:Thatchaphol.html">Thatchaphol Saranurak</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.01422">PDF</a><br /><b>Abstract: </b>Hop-constrained flows and their duals, moving cuts, are two fundamental
quantities in network optimization. Up to poly-logarithmic factors, they
characterize how quickly a network can accomplish numerous distributed
primitives. In this work, we give the first efficient algorithms for computing
$(1 \pm \epsilon) $-optimal $h$-hop-constrained flows and moving cuts with high
probability. Our algorithms take $\tilde{O}(m \cdot \text{poly}(h))$ sequential
time, $\tilde{O}(\text{poly}(h))$ parallel time and $\tilde{O}(\text{poly}(h))$
distributed CONGEST time. We use these algorithms to efficiently compute
hop-constrained cutmatches, an object at the heart of recent advances in
expander decompositions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.01422"><span class="datestr">at November 03, 2021 10:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.01378">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.01378">Finding the KT partition of a weighted graph in near-linear time</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Apers:Simon.html">Simon Apers</a>, Paweł Gawrychowski, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lee:Troy.html">Troy Lee</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.01378">PDF</a><br /><b>Abstract: </b>In a breakthrough work, Kawarabayashi and Thorup (J.~ACM'19) gave a
near-linear time deterministic algorithm for minimum cut in a simple graph $G =
(V,E)$. A key component is finding the $(1+\varepsilon)$-KT partition of $G$,
the coarsest partition $\{P_1, \ldots, P_k\}$ of $V$ such that for every
non-trivial $(1+\varepsilon)$-near minimum cut with sides $\{S, \bar{S}\}$ it
holds that $P_i$ is contained in either $S$ or $\bar{S}$, for $i=1, \ldots, k$.
Here we give a near-linear time randomized algorithm to find the
$(1+\varepsilon)$-KT partition of a weighted graph. Our algorithm is quite
different from that of Kawarabayashi and Thorup and builds on Karger's
framework of tree-respecting cuts (J.~ACM'00).
</p>
<p>We describe applications of the algorithm. (i) The algorithm makes progress
towards a more efficient algorithm for constructing the polygon representation
of the set of near-minimum cuts in a graph. This is a generalization of the
cactus representation initially described by Bencz\'ur (FOCS'95). (ii) We
improve the time complexity of a recent quantum algorithm for minimum cut in a
simple graph in the adjacency list model from $\widetilde O(n^{3/2})$ to
$\widetilde O(\sqrt{mn})$. (iii) We describe a new type of randomized algorithm
for minimum cut in simple graphs with complexity $O(m + n \log^6 n)$. For
slightly dense graphs this matches the complexity of the current best $O(m + n
\log^2 n)$ algorithm which uses a different approach based on random
contractions.
</p>
<p>The key technical contribution of our work is the following. Given a weighted
graph $G$ with $m$ edges and a spanning tree $T$, consider the graph $H$ whose
nodes are the edges of $T$, and where there is an edge between two nodes of $H$
iff the corresponding 2-respecting cut of $T$ is a non-trivial near-minimum cut
of $G$. We give a $O(m \log^4 n)$ time deterministic algorithm to compute a
spanning forest of $H$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.01378"><span class="datestr">at November 03, 2021 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.01359">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.01359">Unfoldings and Nets of Regular Polytopes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Devadoss:Satyan_L=.html">Satyan L. Devadoss</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Harvey:Matthew.html">Matthew Harvey</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.01359">PDF</a><br /><b>Abstract: </b>Over a decade ago, it was shown that every edge unfolding of the Platonic
solids was without self-overlap, yielding a valid net. We consider this
property for regular polytopes in arbitrary dimensions, notably the simplex,
cube, and orthoplex. It was recently proven that all unfoldings of the $n$-cube
yield nets. We show this is also true for the $n$-simplex and the $4$-orthoplex
but demonstrate its surprising failure for any orthoplex of higher dimension.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.01359"><span class="datestr">at November 03, 2021 10:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.01262">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.01262">Minimax Optimization: The Case of Convex-Submodular</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Adibi:Arman.html">Arman Adibi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mokhtari:Aryan.html">Aryan Mokhtari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hassani:Hamed.html">Hamed Hassani</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.01262">PDF</a><br /><b>Abstract: </b>Minimax optimization has been central in addressing various applications in
machine learning, game theory, and control theory. Prior literature has thus
far mainly focused on studying such problems in the continuous domain, e.g.,
convex-concave minimax optimization is now understood to a significant extent.
Nevertheless, minimax problems extend far beyond the continuous domain to mixed
continuous-discrete domains or even fully discrete domains. In this paper, we
study mixed continuous-discrete minimax problems where the minimization is over
a continuous variable belonging to Euclidean space and the maximization is over
subsets of a given ground set. We introduce the class of convex-submodular
minimax problems, where the objective is convex with respect to the continuous
variable and submodular with respect to the discrete variable. Even though such
problems appear frequently in machine learning applications, little is known
about how to address them from algorithmic and theoretical perspectives. For
such problems, we first show that obtaining saddle points are hard up to any
approximation, and thus introduce new notions of (near-) optimality. We then
provide several algorithmic procedures for solving convex and
monotone-submodular minimax problems and characterize their convergence rates,
computational complexity, and quality of the final solution according to our
notions of optimally. Our proposed algorithms are iterative and combine tools
from both discrete and continuous optimization. Finally, we provide numerical
experiments to showcase the effectiveness of our purposed methods.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.01262"><span class="datestr">at November 03, 2021 10:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.01254">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.01254">Unique Games hardness of Quantum Max-Cut, and a vector-valued Borell's inequality</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hwang:Yeongwoo.html">Yeongwoo Hwang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Neeman:Joe.html">Joe Neeman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Parekh:Ojas.html">Ojas Parekh</a>, Kevin Thompson, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wright:John.html">John Wright</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.01254">PDF</a><br /><b>Abstract: </b>The Gaussian noise stability of a function $f:\mathbb{R}^n \to \{-1, 1\}$ is
the expected value of $f(\boldsymbol{x}) \cdot f(\boldsymbol{y})$ over
$\rho$-correlated Gaussian random variables $\boldsymbol{x}$ and
$\boldsymbol{y}$. Borell's inequality states that for $-1 \leq \rho \leq 0$,
this is minimized by the halfspace $f(x) = \mathrm{sign}(x_1)$. In this work,
we generalize this result to hold for functions $f:\mathbb{R}^n \to S^{k-1}$
which output $k$-dimensional unit vectors. Our main result shows that the
expected value of $\langle f(\boldsymbol{x}), f(\boldsymbol{y})\rangle$ over
$\rho$-correlated Gaussians $\boldsymbol{x}$ and $\boldsymbol{y}$ is minimized
by the function $f(x) = x_{\leq k} / \Vert x_{\leq k} \Vert$, where $x_{\leq k}
= (x_1, \ldots, x_k)$.
</p>
<p>As an application, we show several hardness of approximation results for
Quantum Max-Cut, a special case of the local Hamiltonian problem related to the
anti-ferromagnetic Heisenberg model. Quantum Max-Cut is a natural quantum
analogue of classical Max-Cut and has become testbed for designing quantum
approximation algorithms. We show the following:
</p>
<p>1. The integrality gap of the basic SDP is $0.498$, matching an existing
rounding algorithm. Combined with existing approximation results for Quantum
Max-Cut, this shows that the basic SDP does not achieve the optimal
approximation ratio.
</p>
<p>2. It is Unique Games-hard (UG-hard) to compute a
$(0.956+\varepsilon)$-approximation to the value of the best product state,
matching an existing approximation algorithm. This result may be viewed as
applying to a generalization of Max-Cut where one seeks to assign
$3$-dimensional unit vectors to each vertex; we also give tight hardness
results for the analogous $k$-dimensional generalization of Max-Cut.
</p>
<p>3. It is UG-hard to compute a $(0.956+\varepsilon)$-approximation to the
value of the best (possibly entangled) state.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.01254"><span class="datestr">at November 03, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2111.01196">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2111.01196">Dynamic Geometric Set Cover, Revisited</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chan:Timothy_M=.html">Timothy M. Chan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/He:Qizheng.html">Qizheng He</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Suri:Subhash.html">Subhash Suri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xue:Jie.html">Jie Xue</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2111.01196">PDF</a><br /><b>Abstract: </b>Geometric set cover is a classical problem in computational geometry, which
has been extensively studied in the past. In the dynamic version of the
problem, points and ranges may be inserted and deleted, and our goal is to
efficiently maintain a set cover solution (satisfying certain quality
requirement). In this paper, we give a plethora of new dynamic geometric set
cover data structures in 1D and 2D, which significantly improve and extend the
previous results:
</p>
<p>1. The first data structure for $(1+\varepsilon)$-approximate dynamic
interval set cover with polylogarithmic amortized update time. Specifically, we
achieve an update time of $O(\log^3 n/\varepsilon)$, improving the
$O(n^\delta/\varepsilon)$ bound of Agarwal et al. [SoCG'20], where $\delta&gt;0$
denotes an arbitrarily small constant.
</p>
<p>2. A data structure for $O(1)$-approximate dynamic unit-square set cover with
$2^{O(\sqrt{\log n})}$ amortized update time, substantially improving the
$O(n^{1/2+\delta})$ update time of Agarwal et al. [SoCG'20].
</p>
<p>3. A data structure for $O(1)$-approximate dynamic square set cover with
$O(n^{1/2+\delta})$ randomized amortized update time, improving the
$O(n^{2/3+\delta})$ update time of Chan and He [SoCG'21].
</p>
<p>4. A data structure for $O(1)$-approximate dynamic 2D halfplane set cover
with $O(n^{17/23+\delta})$ randomized amortized update time. The previous
solution for halfplane set cover by Chan and He [SoCG'21] is slower and can
only report the size of the approximate solution.
</p>
<p>5. The first sublinear results for the \textit{weighted} version of dynamic
geometric set cover. Specifically, we give a data structure for
$(3+o(1))$-approximate dynamic weighted interval set cover with
$2^{O(\sqrt{\log n \log\log n})}$ amortized update time and a data structure
for $O(1)$-approximate dynamic weighted unit-square set cover with
$O(n^\delta)$ amortized update time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2111.01196"><span class="datestr">at November 03, 2021 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/11/02/postdoc-at-uc-santa-barbara-apply-by-january-10-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/11/02/postdoc-at-uc-santa-barbara-apply-by-january-10-2022/">Postdoc at UC Santa Barbara (apply by January 10, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>A postdoc position is available to work with Eric Vigoda (and other theory faculty) at UC Santa Barbara. The position is for 2 years (no teaching required). Start date is flexible.<br />
Interested candidates should send their CV to Eric Vigoda.</p>
<p>Website: <a href="https://sites.cs.ucsb.edu/~vigoda/">https://sites.cs.ucsb.edu/~vigoda/</a><br />
Email: ericvigoda@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/11/02/postdoc-at-uc-santa-barbara-apply-by-january-10-2022/"><span class="datestr">at November 02, 2021 10:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/11/02/gilbert-tessellations-cellular">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/11/02/gilbert-tessellations-cellular.html">Gilbert tessellations from a cellular automaton</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>A <a href="https://en.wikipedia.org/wiki/Gilbert_tessellation">Gilbert tessellation</a> is what you get from choosing a random set of points-with-slopes in the plane, growing line segments in both directions with the chosen slope from each chosen point, at constant speed, and stopping the growth when each line segment runs into something else. The slopes can be in uniformly random directions but one standard variant of the Gilbert tessellation uses only horizontal and vertical slopes.</p>

<p style="text-align: center;"><a href="https://commons.wikimedia.org/wiki/File:Gilbert_tessellation_axis.svg"><img src="https://11011110.github.io/blog/assets/2018/Gilbert-rectangles.svg" alt="Axis-aligned Gilbert tessellation subdivides the plane into rectangles, by Claudio Rocchini" /></a></p>

<p>My paper “<a href="https://arxiv.org/abs/0911.2890">Growth and decay in life-like cellular automata</a>” observed that the Life-like cellular automaton rule B017/S1, when started with a sufficiently sparse random set of live cells, forms lines of replicators that look sort of like one of these axis-parallel Gilbert tessellations, as I discussed in <a href="https://11011110.github.io/blog/2018/12/27/motorcycle-graphs-eventual.html">a previous post on sparse Life</a>.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2018/b017s1.png" alt="Replicator chaos in B017/S1" /></p>

<p>But it’s a bit messy: you also get stable or oscillating blobs of live cells, and replicators can sometimes make large gaps big enough for something else to get across them, rather than forming the impenetrable barriers that the segments of a true Gilbert tessellation would do. So I wondered: how easy is it to get Gilbert tessellations with less mess, in a cellular automaton?</p>

<p>Very easy, if you’re willing to make an automaton that hardcodes into its rules the construction process of a Gilbert tessellation. For instance, make an automaton on an infinite square grid with three states, empty, horizontal, and vertical. Horizontal cells always stay horizontal, and vertical cells always stay vertical. Empty cells with exactly one non-empty neighbor that is either a horizontally-adjacent horizontal cell or a vertically-adjacent vertical cell take on the same state as that neighbor, and otherwise stay empty. Then any horizontal cells will grow into horizontal walls, and vertical cells will grow into vertical walls, at constant speed, until running into other non-empty cells, just as the Gilbert tessellation definition demands. If you start with a sparse random set of non-empty cells of both types, you should get a Gilbert tessellation, more or less by definition. But controlling horizontal versus vertical growth by different cell states rather than by the pattern of live cells seems kind of a cheat. It makes creating Gilbert tessellations the only thing this three-state automaton can do, rather than an emergent behavior of the automaton. And it isn’t even symmetric under 90-degree rotations (although it does have a symmetry that combines rotation with state-swapping). Is there an automaton with only two states, and natural symmetric rules that can do other things but that when seeded randomly generates non-messy Gilbert tessellations?</p>

<p>Yes! I don’t know of a Life-like rule that does this (<a href="https://en.wikipedia.org/wiki/Life_without_Death">Life without death</a> does make nice impenetrable walls but with too much other stuff). But I think the rule below fits the bill. It’s the first thing I tried, at least, so I didn’t have to do any fine adjustments of the rules to make it work.</p>

<p>Here’s the rule:</p>

<ul>
  <li>
    <p>The cells form a square grid with the <a href="https://en.wikipedia.org/wiki/Moore_neighborhood">Moore 8-cell neighborhood</a>.</p>
  </li>
  <li>
    <p>There are two states of cells, live and dead.</p>
  </li>
  <li>
    <p>A dead cell becomes live only under two conditions:</p>

    <ol>
      <li>
        <p>It has exactly two live neighbors (among its eight possible neighbors) that are orthogonally adjacent to each other.</p>
      </li>
      <li>
        <p>It has exactly four live neighbors at the corners of a rectangle (necessarily in two orthogonally adjacent pairs, because we don’t count squares as being rectangles).</p>
      </li>
    </ol>
  </li>
  <li>
    <p>All live cells immediately die.</p>
  </li>
</ul>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/gilbert/rule.svg" alt="Rules for the Gilbert cellular automaton" /></p>

<p>Or, in <a href="http://golly.sourceforge.net/">Golly</a> rule format:</p>

<pre>@RULE Gilbert
@TABLE
n_states:2
neighborhood:Moore
symmetries:rotate8reflect
var a={0,1}
var b={0,1}
var c={0,1}
var d={0,1}
var e={0,1}
var f={0,1}
var g={0,1}
var h={0,1}
0,1,1,0,0,0,0,0,0,1
0,1,1,0,1,1,0,0,0,1
1,a,b,c,d,e,f,g,h,0</pre>

<p>Then an initial pattern of two orthogonally adjacent live cells (a “domino”) will in the next step form two side-by-side dominos, in the step after that three dominos (with the center one in the initial location), and so on, building a wall two cells wide that alternates between dominos and dead cells, and oscillates with period two.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/gilbert/wall.svg" alt="Wall of alternating dominos in the Gilbert cellular automaton" /></p>

<p>Here’s what it looks like when I selected a large rectangle, randomly filled it with 2% live cells, and ran it in Golly. The red lines in this image are walls like the one above.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/gilbert/wide.png" alt="Gilbert cellular automaton on a sparse random field" /></p>

<p>Of course, at the edges of the randomly filled rectangle, the walls shoot off to infinity with no more obstructions. Here’s a closeup, showing the detailed pattern of the walls and how they meet:</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/gilbert/crop.png" alt="Close-up of Gilbert cellular automaton on a sparse random field" /></p>

<p>It looks a lot like a Gilbert tessellation to me! Even starting with a 50% random fill produces the same sort of pattern at a finer scale:</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/gilbert/5050.png" alt="Gilbert cellular automaton on a 50/50 random field" /></p>

<p>But rather than just going by intuitive visual appearance here’s some analysis showing that, if the plane is filled with live cells with some small probability \(p\), and then this rule is run on the result, then as \(p\to0\) the probability of seeing something that looks like a Gilbert tessellation in the neighborhood of any cell will tend to one. By “neighborhood” I mean everything within distance \(r\), where \(r\) should be chosen as a function of \(p\) that grows faster than linearly in \(1/p\) (so that we have nontrivial probability of seeing something other than just empty space) but slower than \((1/p)^{4/3}\).</p>

<p>The reason we need to limit the radius of the neighborhoods is that, in a large enough neighborhood, you will likely see something that deviates from a Gilbert tessellation. In the runs above, for instance, there are some right-angle corners where two line segments both meet and stop, or points where it appears that walls met head-to-head, but this shouldn’t happen in a Gilbert tessellation (or more precisely it happens with probability zero). There are two natural ways of getting a corner: you could start with an L-tromino of live cells, or two growing walls could coincidentally run into each other. But the expected number of triples of nearby live cells within the neighborhood is \(O(p^3r^2)\), and the expected number of pairs of dominos whose walls would meet at the same point is \(O(p^4r^3)\). It might also be the case that three or more initial live cells produce more exotic behavior; if so, the expected number of things like this that happen within the neighborhood is still \(O(p^3r^2)\). With our assumption on the growth rate of \(r\) relative to \(p\), these expected numbers, and therefore the probability of seeing any of these situations within the neighborhood, is negligible. For the same reason they have low probability of occurring close enough to the neighborhood to impinge on it before the Gilbert tessellation within the neighborhood forms.</p>

<p>So with high probability, in neighborhoods of radius \(r\), you’ll only see single live cells and double live cells in the initial state, and the pairs of double cells won’t be horizontally, vertically, or diagonally aligned with each other. Some of the double live cells will be dominos, with density \(\Theta(p^2)\). The only thing that can happen with such a state is that dominos start building walls which grow until they hit each other, exactly as described by a Gilbert tessellation. Once the Gilbert tessellation has been set up, it appears indestructible: the alternating live cells along the wall prevent any births into the layers of dead cells on either side, and if a wall is perturbed at its end it quickly grows back. However, proving this indestructability rigorously would require a more careful case analysis.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/107211819080878015">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/11/02/gilbert-tessellations-cellular.html"><span class="datestr">at November 02, 2021 09:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19270">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/11/01/quantum-trick-or-treat/">Quantum Trick or Treat</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>Are crazy quantum walks fact or fiction?</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/11/01/quantum-trick-or-treat/photo-by-patrick-campbell-university-of-colorado/" rel="attachment wp-att-19272"><img width="150" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/LesleySmith.jpg?resize=150%2C155&amp;ssl=1" class="wp-image-19272" height="155" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">NOAA <a href="https://psl.noaa.gov/people/lesley.l.smith/">src</a></font></td>
</tr>
</tbody>
</table>
<p>
Lesley Smith is a climate researcher at the University of Colorado’s Cooperative Institute for Research in Environmental Sciences. She is also <a href="https://psl.noaa.gov/people/lesley.l.smith/">associated</a> to the National Oceanic and Atmospheric Administration’s Physical Science Laboratory and has worked on behalf of several other national organizations. Her PhD was in particle physics at the University of Kansas, and she incorporates quantum physics into some of her non-nonfiction writings.</p>
<p>
Today we discuss a quantum walk on a simple graph with behavior wild enough to inspire at least the latter kind of writing.</p>
<p>
Smith was most recently second author on a <a href="https://repository.library.noaa.gov/view/noaa/31032">paper</a> in the April, 2021 <em>Journal of Climate</em> whose title immediately speaks a vital subject: “Explaining the Spatial Pattern of U.S. Extreme Daily Precipitation Change.” The years-long <a href="https://www.courthousenews.com/drought-grips-american-west-with-no-relief-in-sight/">drought</a> in the US West has critically depleted lakes and reservoirs and rivers. But just one week ago, much of California had its <a href="https://www.washingtonpost.com/weather/2021/10/25/atmospheric-river-record-rain-california/">wettest</a> day ever. Both the drought and the pattern of extremes have spread across the country, as many easily-found stories this year attest. The question is how to distinguish standard fluctuations from deep-seated causal factors.</p>
<p>
As for her <a href="http://www.lesleylsmith.com/index.html">other</a> <a href="http://www.lesleylsmith.com/blog.html">writings</a>, this is representative:</p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/11/01/quantum-trick-or-treat/quantumtricksandtreatscover/" rel="attachment wp-att-19274"><img width="167" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/QuantumTricksAndTreatsCover.jpg?resize=167%2C250&amp;ssl=1" class="aligncenter wp-image-19274" height="250" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Amazon <a href="https://www.amazon.com/Quantum-Tricks-Treats-Madison-Martin-ebook/dp/B0763DZZ8Q">page</a></font>
</td>
</tr>
</tbody></table>
<p>
The first lines are likewise topical: “Despite it being October, the sun felt warm on my skin. Forest-fire smoke from states west of Colorado gave the air a three-dimensional quality…” She has also written novels titled <em>Quantum Murder</em>, <em>The Quantum Cop</em>, and <em>Quantum Juneteenth</em>, plus just now a <a href="http://www.lesleylsmith.com/shortfiction.html">short story</a>, “Lucky Halloween,” about a quantum computer scientist. She is writing a series called <em>Kat Cubed</em>—well we will raise Schrödinger cat-walks to powers well beyond cubed below.</p>
<p>
</p><p></p><h2> Climate and Dynamism </h2><p></p>
<p></p><p>
The climate paper’s opening paragraphs review the the general prediction of greater precipitation from the thermal component of climate change and the effects of changes in atmospheric currents. Then it observes:</p>
<blockquote><p><b> </b> <em> Yet, the regional pattern of observed heavy precipitation trends across the United States departs from this theoretical expectation, being remarkable for its heterogeneity rather than the more uniform structure that thermodynamic effects alone would predict. … The regionally diverse trends in extreme precipitation are likely related to dynamical factors. </em>
</p></blockquote>
<p></p><p>
The paper speaks to care needed to distinguish causes and draws two conclusions:</p>
<blockquote><p><b> </b> <em> First, the absence of appreciable increases [in annual maximum single day of rain] over the West is attributed to a dynamical effect of the observed centennial-scale trends in [sea-surface temperature], sea ice, and atmospheric composition. … A second factor is sampling variability […from…] a combination of moderate forced increases comingled with a strong articulation of internal atmospheric variability. </em>
</p></blockquote>
<p></p><p>
The nub of the latter from my perspective is distinguishing what we can infer on the basis of increased dynamism alone. In our <a href="https://rjlipton.wpcomstaging.com/2014/01/30/global-warming/">post</a> seven years ago on global warming, I opined that greater energy and dynamism should be treated as more-primary effects than temperature. The challenge of inferring further effects from dynamism is recognized by another excerpt from the conclusions:</p>
<blockquote><p><b> </b> <em> Recognizing the potency of unforced atmospheric dynamics, [one cannot discount] a scenario in which internal variability could mute if not reverse the observed upward trend in eastern U.S. [extreme rains]. </em>
</p></blockquote>
<p></p><p>
In the rest of this post, we’ll offer an example of wild dynamics from a small quantum system that may speak to both her <em>métiers</em>.</p>
<p>
</p><p></p><h2> Classical Random and Quantum Walks </h2><p></p>
<p></p><p>
Consider the following graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />. It looks like a Q-tip. The two self-loops depart from usual examples of undirected graphs. </p>
<p></p><p><br />
<a href="https://rjlipton.wpcomstaging.com/2021/11/01/quantum-trick-or-treat/propellergraph/" rel="attachment wp-att-19275"><img width="550" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/PropellerGraph.png?resize=550%2C72&amp;ssl=1" class="aligncenter size-large wp-image-19275" height="72" /></a></p>
<p></p><p><br />
A classical random walk on <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> starts at some node—say node <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" />—and at each step flips a coin to determine the next node. The “walker” stays on node <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" /> if the result is heads (<img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" />) but goes to node <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2}" class="latex" /> if tails (<img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" />). The walker cannot go from <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3}" class="latex" /> in one step because there is no edge. The <b>probabilistic transition matrix</b> <img src="https://s0.wp.com/latex.php?latex=%7BA_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_G}" class="latex" /> is given at right. The vector </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Bbmatrix%7D+0.5+%5C%5C+0.25+%5C%5C+0.25+%5Cend%7Bbmatrix%7D+%3D+A_G%5Ccdot+%5Cbegin%7Bbmatrix%7D+0.5+%5C%5C+0.5+%5C%5C+0+%5Cend%7Bbmatrix%7D+%3D+A_G%5E2+%5Ccdot+x%5E%7B%280%29%7D+%5Cqquad%5Ctext%7Bwhere%7D%5Cqquad+x%5E%7B%280%29%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%5C%5C+0+%5C%5C+0+%5Cend%7Bbmatrix%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \begin{bmatrix} 0.5 \\ 0.25 \\ 0.25 \end{bmatrix} = A_G\cdot \begin{bmatrix} 0.5 \\ 0.5 \\ 0 \end{bmatrix} = A_G^2 \cdot x^{(0)} \qquad\text{where}\qquad x^{(0)} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} " class="latex" /></p>
<p>gives the probabilities for each node after two steps. (Although multiplying row vectors on the left is commonly used for classical walks, we will use column vectors for consistency with quantum usage; that <img src="https://s0.wp.com/latex.php?latex=%7BA_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_G}" class="latex" /> is symmetric moots the difference anyway.) For any <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7B%28t%29%7D+%3D+A_G%5Et+x_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x^{(t)} = A_G^t x_0}" class="latex" /> can be called the “classical state” of the walk after <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" /> steps. </p>
<p>
The state <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x}" class="latex" /> whose entries <img src="https://s0.wp.com/latex.php?latex=%7Bx%5Bi%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x[i]}" class="latex" /> are given by the degree of node <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{i}" class="latex" /> divided by the sum of the degrees makes <img src="https://s0.wp.com/latex.php?latex=%7BA_G+x+%3D+x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_G x = x}" class="latex" />, and thus gives a <b>stable distribution</b> for the walk. A central theorem states that for every <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> that is not bipartite, the powers of <img src="https://s0.wp.com/latex.php?latex=%7BA_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_G}" class="latex" /> converge entrywise in every column to a unique stable distribution. When <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> is regular, as here, this is simply the uniform distribution. The convergence in our case is quite quick, as signified by the exact values </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A_G%5E%7B10%7D+%3D+%5Cbegin%7Bbmatrix%7D+0.333984375+%26+0.3330078125+%26+0.3330078125+%5C%5C+0.3330078125+%26+0.333984375+%26+0.3330078125+%5C%5C+0.3330078125+%26+0.3330078125+%26+0.333984375+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  A_G^{10} = \begin{bmatrix} 0.333984375 &amp; 0.3330078125 &amp; 0.3330078125 \\ 0.3330078125 &amp; 0.333984375 &amp; 0.3330078125 \\ 0.3330078125 &amp; 0.3330078125 &amp; 0.333984375 \end{bmatrix}. " class="latex" /></p>
<p>
The quantum case needs to bind the graph and the “coin” into one system, so its domain has six members, which we list in order </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathcal%7BS%7D+%3D+%5C%7B%281%2CH%29%2C%281%2CT%29%2C%282%2CH%29%2C%282%2CT%29%2C%283%2CH%29%2C%283%2CT%29%5C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathcal{S} = \{(1,H),(1,T),(2,H),(2,T),(3,H),(3,T)\}. " class="latex" /></p>
<p>In quantum notation, this is the tensor product of the “node space” <img src="https://s0.wp.com/latex.php?latex=%7BV+%3D+%5C%7B1%2C2%2C3%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{V = \{1,2,3\}}" class="latex" /> and the “coin space” <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BC%7D+%3D+%5C%7BH%2CT%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathcal{C} = \{H,T\}}" class="latex" />. The meaning of <img src="https://s0.wp.com/latex.php?latex=%7B%28v%2C%5Cmathfrak%7Bc%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(v,\mathfrak{c})}" class="latex" /> is that the walker just arrived at node <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> via the coin outcome <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7Bc%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathfrak{c}}" class="latex" />. It is incumbent that every node <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> can be reached on either outcome. Our <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" />–<img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{T}" class="latex" /> labeling above complies. Then we get a permutation <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P}" class="latex" /> on <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> from the action </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++P%28v%2C%5Cmathfrak%7Bc%7D%29+%3D+%28v%27%2C%5Cmathfrak%7Bc%7D%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  P(v,\mathfrak{c}) = (v',\mathfrak{c}), " class="latex" /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7Bv%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v'}" class="latex" /> is the vertex reached from <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> by the outcome <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7Bc%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathfrak{c}}" class="latex" /> from the classical walk, and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7Bc%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathfrak{c}}" class="latex" /> is kept the same. The penultimate trick is to expand this to a mapping <img src="https://s0.wp.com/latex.php?latex=%7BP%27%3A+%5Cmathcal%7BS%7D+%5Ctimes+%5Cmathcal%7BC%7D+%5Crightarrow+%5Cmathcal%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P': \mathcal{S} \times \mathcal{C} \rightarrow \mathcal{S}}" class="latex" /> by </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++P%27%28%28v%2C%5Cmathfrak%7Bb%7D%29%2C%5Cmathfrak%7Bc%7D%29+%3D+%28v%27%2C%5Cmathfrak%7Bc%7D%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  P'((v,\mathfrak{b}),\mathfrak{c}) = (v',\mathfrak{c}), " class="latex" /></p>
<p>whereby the previous coin outcome <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7Bb%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathfrak{b}}" class="latex" /> is “forgotten” while the current flip is preserved in the state. We can represent this by the <em>directed</em> graph <img src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G'}" class="latex" /> shown next. </p>
<p></p><p><br />
<a href="https://rjlipton.wpcomstaging.com/2021/11/01/quantum-trick-or-treat/quantumgraph/" rel="attachment wp-att-19281"><img width="360" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/QuantumGraph.png?resize=360%2C210&amp;ssl=1" class="aligncenter wp-image-19281" height="210" /></a></p>
<p></p><p><br />
Incidentally, <img src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G'}" class="latex" /> is planar (switch nodes <img src="https://s0.wp.com/latex.php?latex=%7B2H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2H}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B2T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2T}" class="latex" /> to see). The last trick is to label its edges by quantum amplitudes for the coin outcomes.</p>
<p>
</p><p></p><h2> Realm of the Coin </h2><p></p>
<p></p><p>
Our quantum coin can be “flipped” by applying any chosen <img src="https://s0.wp.com/latex.php?latex=%7B2+%5Ctimes+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2 \times 2}" class="latex" /> matrix <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BU%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{U}}" class="latex" /> to the current state of the coin, provided <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BU%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{U}}" class="latex" /> is a unitary matrix. In general, if <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> is a <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" />-regular graph and we have a suitable labeling for results of a <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" />-sided die, then <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BU%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{U}}" class="latex" /> can be a <img src="https://s0.wp.com/latex.php?latex=%7Bd+%5Ctimes+d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d \times d}" class="latex" /> matrix acting on a single <b>qudit</b>, or we can use <img src="https://s0.wp.com/latex.php?latex=%7B%5Clceil+%5Clog_2+d+%5Crceil%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lceil \log_2 d \rceil}" class="latex" /> qubits to encode the coin. But here we just need one qubit. Before fixing the choice of coin matrix, we can represent it abstractly as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathbf%7BU%7D+%3D+%5Cbegin%7Bbmatrix%7D+a+%26+b+%5C%5C+c+%26+d+%5Cend%7Bbmatrix%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathbf{U} = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix} " class="latex" /></p>
<p>To apply the coin on <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathcal{S}}" class="latex" />, we form the <img src="https://s0.wp.com/latex.php?latex=%7B6+%5Ctimes+6%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{6 \times 6}" class="latex" /> matrix <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BU%27%7D+%3D+%5Cmathbf%7BI%7D+%5Cotimes+%5Cmathbf%7BU%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{U'} = \mathbf{I} \otimes \mathbf{U}}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BI%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{I}}" class="latex" /> is the <img src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3 \times 3}" class="latex" /> identity matrix. To move the walker after the coin result, we apply the <img src="https://s0.wp.com/latex.php?latex=%7B6+%5Ctimes+6%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{6 \times 6}" class="latex" /> matrix <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{P}}" class="latex" /> of the permutation <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{P}" class="latex" />. Since we use column vectors for states, the walk matrix is given by </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathbf%7BW%7D+%3D+%5Cmathbf%7BP%7D%5Ccdot%5Cmathbf%7BU%27%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%26+0+%26+0+%26+0+%5C%5C+0+%26+0+%26+0+%26+1+%26+0+%26+0+%5C%5C+0+%26+0+%26+0+%26+0+%26+1+%26+0+%5C%5C+0+%26+1+%26+0+%26+0+%26+0+%26+0+%5C%5C+0+%26+0+%26+1+%26+0+%26+0+%26+0+%5C%5C+0+%26+0+%26+0+%26+0+%26+0+%26+1+%5C%5C+%5Cend%7Bbmatrix%7D+%5Ccdot+%5Cbegin%7Bbmatrix%7D+a+%26+b+%26+0+%26+0+%26+0+%26+0+%5C%5C+c+%26+d+%26+0+%26+0+%26+0+%26+0+%5C%5C+0+%26+0+%26+a+%26+b+%26+0+%26+0+%5C%5C+0+%26+0+%26+c+%26+d+%26+0+%26+0+%5C%5C+0+%26+0+%26+0+%26+0+%26+a+%26+b+%5C%5C+0+%26+0+%26+0+%26+0+%26+c+%26+d+%5Cend%7Bbmatrix%7D+%3D+%5Cleft%5B%5Cbegin%7Barray%7D%7Bcc%7Ccc%7Ccc%7D+a+%26+b+%26+0+%26+0+%26+0+%26+0+%5C%5C+0+%26+0+%26+c+%26+d+%26+0+%26+0+%5C%5C+%5Chline+0+%26+0+%26+0+%26+0+%26+a+%26+b+%5C%5C+c+%26+d+%26+0+%26+0+%26+0+%26+0+%5C%5C+%5Chline+0+%26+0+%26+a+%26+b+%26+0+%26+0+%5C%5C+0+%26+0+%26+0+%26+0+%26+c+%26+d+%5Cend%7Barray%7D%5Cright%5D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathbf{W} = \mathbf{P}\cdot\mathbf{U'} = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\ \end{bmatrix} \cdot \begin{bmatrix} a &amp; b &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ c &amp; d &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; a &amp; b &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; c &amp; d &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; a &amp; b \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; c &amp; d \end{bmatrix} = \left[\begin{array}{cc|cc|cc} a &amp; b &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; c &amp; d &amp; 0 &amp; 0 \\ \hline 0 &amp; 0 &amp; 0 &amp; 0 &amp; a &amp; b \\ c &amp; d &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ \hline 0 &amp; 0 &amp; a &amp; b &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; c &amp; d \end{array}\right] " class="latex" /></p>
<p>We have drawn lines to show how the coin acts between each pair of nodes. Notice that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BW%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{W}}" class="latex" /> is no longer symmetric, so specifying column-vector inputs matters. For instance, the <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{b}" class="latex" /> in the top row comes in from the column at <img src="https://s0.wp.com/latex.php?latex=%7B1T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1T}" class="latex" /> and exits at <img src="https://s0.wp.com/latex.php?latex=%7B1H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1H}" class="latex" />. This means the previous coin result was tails and took the walker (from node 2) to node <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" />; now the coin gives heads so the walker stays at node <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" />. We can diagram this action also on the expanded graph <img src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G'}" class="latex" />, so that the <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{b}" class="latex" /> goes on the arrow from <img src="https://s0.wp.com/latex.php?latex=%7B1T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1T}" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%7B1H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1H}" class="latex" />, and so on:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/11/01/quantum-trick-or-treat/quantumgprime/" rel="attachment wp-att-19278"><img width="300" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/QuantumGprime.png?resize=300%2C193&amp;ssl=1" class="aligncenter wp-image-19278" height="193" /></a></p>
<p>
The first step has no “previous coin result” but we still have to specify one to initialize the coin state as well as the walker’s state. For starting on node <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" /> there is still an infinitude of combinations of the basis states <img src="https://s0.wp.com/latex.php?latex=%7B1H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1H}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B1T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1T}" class="latex" />. If we fix the start as <img src="https://s0.wp.com/latex.php?latex=%7B1H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1H}" class="latex" />, this is like saying the coin was heads-up before the initial act of flipping it.</p>
<p>
</p><p></p><h2> Walking Into Chaos </h2><p></p>
<p></p><p>
A natural and popular choice of coin is the Hadamard matrix </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathbf%7BH%7D+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Cbegin%7Bbmatrix%7D+1+%26+1+%5C%5C+1+%26+-1+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathbf{H} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{bmatrix}. " class="latex" /></p>
<p>We need only mark the edges with <img src="https://s0.wp.com/latex.php?latex=%7Bd+%3D+-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d = -1}" class="latex" />, keeping the normalizing <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac{1}{\sqrt{2}}}" class="latex" /> in mind. Here are <img src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G'}" class="latex" /> and the three steps of the quantum walk after the first:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/11/01/quantum-trick-or-treat/fouriterations/" rel="attachment wp-att-19279"><img width="550" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/FourIterations.png?resize=550%2C282&amp;ssl=1" class="aligncenter size-large wp-image-19279" height="282" /></a></p>
<p>
To visualize the <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" />-step walk, say starting from <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7Bs%7D_0+%3D+1H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathfrak{s}_0 = 1H}" class="latex" />, we take each possible path of length <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t}" class="latex" /> and multiply the numbers on its edges. Over all paths that end at the same <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7Bs%7D%3D%28v%2C%5Cmathfrak%7Bc%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathfrak{s}=(v,\mathfrak{c})}" class="latex" /> we add those products to give the <b>amplitude</b> <img src="https://s0.wp.com/latex.php?latex=%7Ba_%7Bu%2Cv%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a_{u,v}}" class="latex" />, which in our column-first notation equals the entry <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BW%7D%5Et%5B%5Cmathfrak%7Bs%7D%2C%5Cmathfrak%7Bs%7D_0%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{W}^t[\mathfrak{s},\mathfrak{s}_0]}" class="latex" />. </p>
<p>
For example, there are two paths of <img src="https://s0.wp.com/latex.php?latex=%7Bt%3D3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t=3}" class="latex" /> steps from <img src="https://s0.wp.com/latex.php?latex=%7B1H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1H}" class="latex" /> back to itself. One takes the loop at <img src="https://s0.wp.com/latex.php?latex=%7B1H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1H}" class="latex" /> three times and contributes the phase value <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" />, but the other does <img src="https://s0.wp.com/latex.php?latex=%7B1H%5Crightarrow+2T+%5Crightarrow+1T+%5Crightarrow+1H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1H\rightarrow 2T \rightarrow 1T \rightarrow 1H}" class="latex" /> and picks up the value <img src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{-1}" class="latex" />. Adding those values cancels them, leaving the red <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0}" class="latex" /> shown at upper left in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BW%7D%5E3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{W}^3}" class="latex" />. If our walker is a Schrödinger cat, that part of it is not just “dead” but completely annihilated.</p>
<p>
It is still possible for the cat to end on node <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" /> after being cubed, because the path <img src="https://s0.wp.com/latex.php?latex=%7B1H+%5Crightarrow+1H+%5Crightarrow+2T+%5Crightarrow+1T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1H \rightarrow 1H \rightarrow 2T \rightarrow 1T}" class="latex" /> is not canceled. That the cat ends with phase <img src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{-1}" class="latex" /> does not matter, because to get the probabilities—given we started at <img src="https://s0.wp.com/latex.php?latex=%7B1H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1H}" class="latex" />—we take the squared magnitude of each entry in column <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" /> and add the adjacent pairs denoting the same vertex of the original graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />. Thus the cat has probability only </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++p_1%5E%7B%283%29%7D+%3D+%5Cfrac%7B0%5E2+%2B+%28-1%29%5E2%7D%7B%282%5Csqrt%7B2%7D%29%5E2%7D+%3D+%5Cfrac%7B1%7D%7B8%7D+%3D+0.125+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  p_1^{(3)} = \frac{0^2 + (-1)^2}{(2\sqrt{2})^2} = \frac{1}{8} = 0.125 " class="latex" /></p>
<p>of being still on node <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" /> after three time steps. Given the ease of looping at node <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" />, this is counterintuitive. After nine and ten steps, we have: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathbf%7BW%7D%5E9+%3D+%5Cfrac%7B1%7D%7B2%5E%7B4.5%7D%7D%5Cbegin%7Bbmatrix%7D+18+%26+4+%26+-1+%26+13+%26+-1+%26+-1%5C%5C+13+%26+1+%26+4+%26+-18+%26+-1+%26+1%5C%5C+-1+%26+13+%26+-1+%26+-1+%26+18+%26+4%5C%5C+4+%26+-18+%26+-1+%26+1+%26+13+%26+1%5C%5C+-1+%26+-1+%26+18+%26+4+%26+-1+%26+13%5C%5C+-1+%26+1+%26+13+%26+1+%26+4+%26+-18+%5Cend%7Bbmatrix%7D+.%5Cqquad+%5Cmathbf%7BW%7D%5E%7B10%7D+%3D+%5Cfrac%7B1%7D%7B32%7D%5Cbegin%7Bbmatrix%7D+31+%26+5+%26+3+%26+-5+%26+-2+%26+0%5C%5C+-5+%26+31+%26+0+%26+-2+%26+5+%26+3%5C%5C+-2+%26+0+%26+31+%26+5+%26+3+%26+-5%5C%5C+5+%26+3+%26+-5+%26+31+%26+0+%26+-2%5C%5C+3+%26+-5+%26+-2+%26+0+%26+31+%26+5%5C%5C+0+%26+-2+%26+5+%26+3+%26+-5+%26+31+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathbf{W}^9 = \frac{1}{2^{4.5}}\begin{bmatrix} 18 &amp; 4 &amp; -1 &amp; 13 &amp; -1 &amp; -1\\ 13 &amp; 1 &amp; 4 &amp; -18 &amp; -1 &amp; 1\\ -1 &amp; 13 &amp; -1 &amp; -1 &amp; 18 &amp; 4\\ 4 &amp; -18 &amp; -1 &amp; 1 &amp; 13 &amp; 1\\ -1 &amp; -1 &amp; 18 &amp; 4 &amp; -1 &amp; 13\\ -1 &amp; 1 &amp; 13 &amp; 1 &amp; 4 &amp; -18 \end{bmatrix} .\qquad \mathbf{W}^{10} = \frac{1}{32}\begin{bmatrix} 31 &amp; 5 &amp; 3 &amp; -5 &amp; -2 &amp; 0\\ -5 &amp; 31 &amp; 0 &amp; -2 &amp; 5 &amp; 3\\ -2 &amp; 0 &amp; 31 &amp; 5 &amp; 3 &amp; -5\\ 5 &amp; 3 &amp; -5 &amp; 31 &amp; 0 &amp; -2\\ 3 &amp; -5 &amp; -2 &amp; 0 &amp; 31 &amp; 5\\ 0 &amp; -2 &amp; 5 &amp; 3 &amp; -5 &amp; 31 \end{bmatrix}. " class="latex" /></p>
<p>The corresponding node probabilities are <img src="https://s0.wp.com/latex.php?latex=%7Bp%5E%7B%289%29%7D+%3D+%5B+0.96289%2C+0.0332%2C+0.00391%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p^{(9)} = [ 0.96289, 0.0332, 0.00391]}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bp%5E%7B%2810%29%7D+%3D+%5B+0.96289%2C+0.02832%2C+0.00879%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p^{(10)} = [ 0.96289, 0.02832, 0.00879]}" class="latex" />. The probability of the first node is the same between every odd and even step, echoing how starting at <img src="https://s0.wp.com/latex.php?latex=%7B1H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1H}" class="latex" /> presumes a previous coin flip of heads, which kept the cat on node <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt+%3D+-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t = -1}" class="latex" />. At <img src="https://s0.wp.com/latex.php?latex=%7Bt%3D10%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t=10}" class="latex" /> the cat has almost come back fully to node <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{1}" class="latex" />, indeed to the initial quantum state <img src="https://s0.wp.com/latex.php?latex=%7B%7C1H%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{|1H\rangle}" class="latex" />, but not quite. The swing is even closer at <img src="https://s0.wp.com/latex.php?latex=%7Bt%3D38%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{t=38}" class="latex" />: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathbf%7BW%7D%5E%7B38%7D+%3D+%5Cfrac%7B1%7D%7B524288%7D%5Cbegin%7Bbmatrix%7D+522919+%26+-23939+%26+-11285+%26+23939+%26+12654+%26+0%5C%5C+23939+%26+522919+%26+0+%26+12654+%26+-23939+%26+-11285%5C%5C+12654+%26+0+%26+522919+%26+-23939+%26+-11285+%26+23939%5C%5C+-23939+%26+-11285+%26+23939+%26+522919+%26+0+%26+12654%5C%5C+-11285+%26+23939+%26+12654+%26+0+%26+522919+%26+-23939%5C%5C+0+%26+12654+%26+-23939+%26+-11285+%26+23939+%26+522919+%5Cend%7Bbmatrix%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \mathbf{W}^{38} = \frac{1}{524288}\begin{bmatrix} 522919 &amp; -23939 &amp; -11285 &amp; 23939 &amp; 12654 &amp; 0\\ 23939 &amp; 522919 &amp; 0 &amp; 12654 &amp; -23939 &amp; -11285\\ 12654 &amp; 0 &amp; 522919 &amp; -23939 &amp; -11285 &amp; 23939\\ -23939 &amp; -11285 &amp; 23939 &amp; 522919 &amp; 0 &amp; 12654\\ -11285 &amp; 23939 &amp; 12654 &amp; 0 &amp; 522919 &amp; -23939\\ 0 &amp; 12654 &amp; -23939 &amp; -11285 &amp; 23939 &amp; 522919 \end{bmatrix} " class="latex" /></p>
<p>with state <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7Bs%7D_%7B38%7D+%3D+%5B0.99739%2C+0.04566%2C+0.02414%2C+-0.04566%2C+-0.02152%2C+0%5D%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathfrak{s}_{38} = [0.99739, 0.04566, 0.02414, -0.04566, -0.02152, 0]^T}" class="latex" /> and node probabilities <img src="https://s0.wp.com/latex.php?latex=%7Bp%5E%7B%2838%29%7D+%3D+%5B+0.99687%2C+0.00267%2C+0.00046%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p^{(38)} = [ 0.99687, 0.00267, 0.00046]}" class="latex" />. The next probabilities, however, are <img src="https://s0.wp.com/latex.php?latex=%7Bp%5E%7B%2839%29%7D+%3D+%5B+0.54641%2C+0.45313%2C+0.00046%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p^{(39)} = [ 0.54641, 0.45313, 0.00046]}" class="latex" />, which is not close to the first-step distribution <img src="https://s0.wp.com/latex.php?latex=%7Bp%5E%7B%281%29%7D+%3D+%5B0.5%2C0.5%2C0%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p^{(1)} = [0.5,0.5,0]}" class="latex" />. </p>
<p>
Most treatments of quantum walks, including a seminal <a href="https://arxiv.org/abs/quant-ph/0012090">paper</a> by Dorit Aharonov, Andris Ambainis, Julia Kempe, and Umesh Vazirani, and an influential 2003 <a href="https://arxiv.org/abs/quant-ph/0303081">survey</a> by Kempe, emphasize the dispersion time being linear rather than quadratic as in classical random walks on undirected graphs. Here, the quadratic nature effects quick divergence between close states like <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7Bs%7D_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathfrak{s}_0}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathfrak%7Bs%7D_%7B38%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathfrak{s}_{38}}" class="latex" />, which is the signature of dynamical chaos. </p>
<p>
The coin <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BJ%7D+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Cbegin%7Bbmatrix%7D+1+%26+i+%5C%5C+i+%26+1%5Cend%7Bbmatrix%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{J} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; i \\ i &amp; 1\end{bmatrix}}" class="latex" /> is reputed to make quantum walks smoother. That works to some extent for our three-node graph, but there appears to be no periodic stability, let alone convergence as in the classical case. Simple Python code for trying other coins is available <a href="https://cse.buffalo.edu/~regan/cse610/qtipwalk.py">here</a>.</p>
<p>
</p><p></p><h2> Real or Fiction? </h2><p></p>
<p></p><p>
Our main question is simple: </p>
<blockquote><p><b> </b> <em> Are these crazy walks real? </em>
</p></blockquote>
<p></p><p>
The answer is <em>yes</em> if we can engineer a qutrit+qubit system that evolves by repeating <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BW%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{W}}" class="latex" />. <em>Simulating</em> such a system throws up a second issue: No one would say that the classical computer executing my Python code is behaving chaotically. We can program <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BW%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{W}}" class="latex" /> via basic gates available to run on real hardware at <a href="https://quantum-computing.ibm.com/">IBM Quantum</a> or some other services. </p>
<blockquote><p><b> </b> <em> Is quantum hardware that applies powers of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BW%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbf{W}}" class="latex" /> behaving chaotically? Does that pose a concretely physical impediment to maintaining its coherence? </em>
</p></blockquote>
<p></p><p>
On expecting a <em>yes</em> answer to the walks’ physical existence in the microworld, our questions next try to connect to Smith’s paper:</p>
<blockquote><p><b> </b> <em> Does the mathematical existence of chaotic quantum systems on such small scales have macroscopic effects in our world? Such as on the chaotic dynamics of our atmosphere? </em>
</p></blockquote>
<p></p><p>
Again, the answer should be <em>yes</em> at the level of <a href="https://link.springer.com/chapter/10.1007/3-540-09718-X_73">connecting</a> Brownian motion to quantum processes, or of quantum-walk <a href="https://repository.tudelft.nl/islandora/object/uuid:08ad4a94-483d-46db-8840-6f73c3e48a70/datastream/OBJ/download">interpretations</a> of physical systems such as the quantum harmonic oscillator (<a href="https://en.wikipedia.org/wiki/Quantum_harmonic_oscillator">QHO</a>). So the most specific form of our question is:</p>
<blockquote><p><b> </b> <em> Does our (not-so-)simple three-node quantum walk appear as a non-negligible term in a Feynman-style summation needed to understand a macroscopic physical system? </em>
</p></blockquote>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Can the quantum chaos of the Q-tip graph be tamed? See <a href="https://arxiv.org/pdf/2008.00316.pdf">this</a> for success on other graphs including the 3-cycle. There are are affinities between the 3-cycle and Q-tip walks, especially at even timesteps, and the 3-cycle is an option in the Python code.</p>
<p>
Smith also has many activities promoting STEM, including her <a href="http://www.physicsisfun.net/&quot;">Phyiscs Is Fun</a> website and a <a href="https://books2read.com/u/bMavLX">book</a> on particle physics.  She maintains a <a href="http://www.lesleylsmith.com/blog.html">blog</a> of her writing and reading, plus much else on her <a href="http://www.lesleylsmith.com/">author site</a>.  Dick and I were already wondering about women in the community of math/theory bloggers, such as <a href="https://blog.tanyakhovanova.com/">Tanya Khovanova</a>, <a href="https://fractalkitty.com/">Sophia Wood</a>, <a href="http://vihart.com/">Vi Hart</a>, and <a href="https://mathmunch.org/author/aweltman/">Anna Weltman</a>.  </p>
<p></p><p><br />
[fixed link for Python code, some word changes, fixed entry of J matrix]</p></font></font></div>







<p class="date">
by KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/11/01/quantum-trick-or-treat/"><span class="datestr">at November 02, 2021 03:42 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://scottaaronson.blog/?p=6098">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://scottaaronson.blog/?p=6098">Q2B 2021</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>This is a quick post to let people know that the 2021 <a href="https://q2b.qcware.com/">Q2B (Quantum 2 Business) conference</a> will be this December 7-9 at the Santa Clara Convention Center.  (Full disclosure: Q2B is hosted by <a href="https://qcware.com/">QC Ware, Inc.</a>, to which I’m the scientific adviser.)  Barring a dramatic rise in cases or the like, I’m planning to attend to do my Ask-Me-Anything session, in what’s become an annual tradition.  Notably, this will be my first in-person conference, and in fact my first professional travel of any kind, since before covid shut down the US in late March 2020.  I hope to see many of you there!  And if you <em>won’t</em> be at Q2B, but you’ll be in the Bay Area and would like to meet otherwise, let me know and we’ll try to work something out.</p></div>







<p class="date">
by Scott <a href="https://scottaaronson.blog/?p=6098"><span class="datestr">at November 01, 2021 10:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-2754344269552524293">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2019/07/when-did-math-get-so-hard.html">When did Math Get So Hard?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<br />
I have been on many Math PhD thesis defense's  as the Dean's Representative. This means I don't have to understand the work, just make sure the rules are followed. I've done this for a while and I used to understand some of it but now there are times I understand literally none of it. As a result, when the student leaves the room and we talk among ourselves I ask<br />
<br />
<br />
When did Math get so hard?<br />
<br />
I mean it as a statement and maybe a joke, but I decided to email various people and ask for a serious answer. Here are some thoughts of mine and others<div><br /></div><div>1) When you get older math got harder. Lance blogged on this <a href="https://blog.computationalcomplexity.org/2021/10/a-young-persons-game.html">here</a></div><div><br /></div><div>2) When math got more abstract it got harder. Blame Grothendieck.</div><div><br /></div><div>3) When math stopped being tied to the real work it got harder. Blame Hardy. </div><div><br /></div><div>4) Math has always been hard. We NOW understand some of the older math better so it seems easy to us, but it wasn't at the time. </div><div><br /></div><div>5) With the web and more people working in math, new results come out faster so its harder to keep up.</div><div><br /></div><div>6) All fields of math have a period of time when they are easy, at the beginning, and then as the low-hanging fruit gets picked it gets harder and harder.  So if a NEW branch was started it might initially be easy. Counterthought- even a new branch might be hard now since it can draw on so much prior math. Also, the low hanging fruit may be picked rather quickly. </div><div><br />
<br /><br />
</div></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2019/07/when-did-math-get-so-hard.html"><span class="datestr">at October 31, 2021 07:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/10/31/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/10/31/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Square-difference-free_set">Square-difference-free set</a> (<a href="https://mathstodon.xyz/@11011110/107112823640205029">\(\mathbb{M}\)</a>), now a Good Article on Wikipedia. As the name suggests, these are sets of integers no two of which differ by a square. My favorite such set consists of the losing positions in <a href="https://en.wikipedia.org/wiki/Subtract_a_square">subtract-a-square</a>, where each move removes a square number of coins from a pile of coins, winning by taking the last coin. This general class of sets and the subtract-a-square set have \(o(n)\) elements up to \(n\), but their maximum density remains unknown.</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/q/406120/440">Can a convex polyhedron have an odd number of faces, all congruent</a> (<a href="https://mathstodon.xyz/@11011110/107121133147692528">\(\mathbb{M}\)</a>). If so the faces would have to all be kites, per comments at the link. Which raises the question: can a convex polyhedron with congruent kite faces avoid being either a <a href="https://en.wikipedia.org/wiki/Trapezohedron">trapezohedron</a> or formed from deltahedron (a polyhedron with equilateral triangle faces) by subdividing each triangle into three kites? Both automatically have evenly many faces.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Regular_number">You might know about regular numbers</a> (<a href="https://mathstodon.xyz/@11011110/107127099279073309">\(\mathbb{M}\)</a>), of the form \(2^i\cdot 3^j\cdot 5^k\), from Babylonian mathematics, music theory, Plato, or as a test case for functional programming. But did you know that they come up in biology, as numbers of years between mass flowering in certain types of bamboo? See Veller, Nowak and Davis, “<a href="https://doi.org/10.1111/ele.12442">Extended flowering intervals of bamboos evolved by discrete multiplication</a>”, <em>Ecol. Lett.</em> 2015, via Andrey Zabolotskiy at <a href="https://oeis.org/A051037">OEIS A051037</a>.</p>
  </li>
  <li>
    <p>My new paper “<a href="https://arxiv.org/abs/2110.06163">Finding relevant points for nearest-neighbor classification</a>”, has won the best paper award of the SIAM Symposium on Simplicity in Algorithms, SOSA22 <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107132784717543034">\(\mathbb{M}\)</a>).</span> Woo! In other news, the lists of accepted papers at <a href="https://www.siam.org/conferences/cm/program/accepted-papers/sosa22-accepted-papers">SOSA</a>, <a href="https://www.siam.org/conferences/cm/program/accepted-papers/soda22-accepted-papers">SODA</a>, and <a href="https://www.siam.org/conferences/cm/program/accepted-papers/alenex22-accepted-papers">ALENEX</a> are online.</p>
  </li>
  <li>
    <p><a href="https://twitter.com/thienan496/status/1446021847292669953">Animation of the minimum-weight matchings of increasingly many points of two colors in a unit square</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107140270923344318">\(\mathbb{M}\)</a>).</span> Because the color densities fluctuate, the matching develops regions of many parallel long edges transporting excess density from one place to another. This is reflected mathematically in the fact that the expected length is \(\Theta(\sqrt{n\log n})\) compared to \(\Theta(\sqrt{n})\) for non-bipartite matching; see Ajtai, Komlós, and Tusnády, “<a href="https://doi.org/10.1007/BF02579135">On optimal matchings</a>”, <em>Combinatorica</em> 1984.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Jacob_E._Goodman">Jacob E. Goodman</a>, famed as a discrete and computational geometer and cofounder of the top journal in the field, <em>Discrete &amp; Computational Geometry</em>, <a href="https://web.archive.org/web/20211025204323/https://newyorkcomposerscircle.org/index.html">died on October 10</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107143828784210981">\(\mathbb{M}\)</a>,</span> <a href="https://newyorkcomposerscircle.org/composers/jacob-e-goodman.html">see also</a>).</p>
  </li>
  <li>
    <p>The classical CS interview question of destructively reversing a singly-linked list goes back at least to <a href="http://i.stanford.edu/pub/cstr/reports/cs/tr/76/544/CS-TR-76-544.pdf">Ed McCreight in 1973</a> (<a href="https://mathstodon.xyz/@11011110/107147889403810499">\(\mathbb{M}\)</a>). But if you generalize a singly-linked list to a zipper (directed out in both directions from a finger <span style="white-space: nowrap;">into it)</span> then reversal is trivial and you get the classical algorithm by moving your finger from start to end. I don’t know of references; maybe the zipper people are too focused on functional/non-destructive/reentrant methods to mention this?</p>
  </li>
  <li>
    <p>I recently learned that the name “Thomsen graph” for  comes from the work of Danish chemist <a href="https://en.wikipedia.org/wiki/Hans_Peter_J%C3%B8rgen_Julius_Thomsen">Julius Thomsen</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107154978136052455">\(\mathbb{M}\)</a>)</span> who <a href="https://archive.org/download/crossref-pre-1909-scholarly-works/10.1002%252Fcber.18860190141.zip/10.1002%252Fcber.188601902285.pdf">proposed in 1886 that it describes the structure of benzene</a>. Thomsen was late to the party: Kekulé had already proposed the benzene ring in 1865. But the name for the graph stuck.</p>
  </li>
  <li>
    <p>My UCI colleague <a href="https://recruit.ap.uci.edu/JPF07189">Vijay Vazirani is looking for a postdoc in algorithmic design / algorithmic game theory</a> (<a href="https://mathstodon.xyz/@11011110/107157944689667569">\(\mathbb{M}\)</a>). One-year, renewable.</p>
  </li>
  <li>
    <p><a href="https://sinews.siam.org/Details-Page/fairmandering-generating-fairness-optimized-political-districts">Fairmandering: generating fairness-optimized political districts</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107169485099658532">\(\mathbb{M}\)</a>),</span> Wes Gurnee and David Shmoys. If you generate a hierarchically-organized and large family of partitions of a state into contiguous equal-population regions, you can then optimize over the hierarchy for fairness. Gurnee and Shmoys choose to optimize the <a href="https://en.wikipedia.org/wiki/Efficiency_gap">efficiency gap</a> but their method is a two-edged sword: it would work equally well to optimize for partisan advantage.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@christianp/107173511804051329">What high-order regular polygons do you have lying around your house?</a></p>
  </li>
  <li>
    <p><a href="http://skepticsplay.blogspot.com/2013/06/in-praise-of-non-deductive-puzzles.html">In praise of non-deductive puzzles</a> (<a href="https://mathstodon.xyz/@11011110/107183397931741236">\(\mathbb{M}\)</a>). I’m not sure I agree that logic puzzles solved by intuition and guessing are better than ones where you have to make sufficiently deep deductions, but they both beat blind backtracking. I do agree that numberlinks should fill the grid as a natural consequence of connecting the numbers, not an extra constraint. And that <a href="http://mathgrant.blogspot.com/2010/10/grants-review-corner-volume-2.html">non-unique generators for puzzles whose solutions should be unique are an abomination</a>.</p>
  </li>
  <li>
    <p><a href="https://sinews.siam.org/Details-Page/preserving-the-history-of-applied-mathematics">Preserving the history of applied mathematics</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107191450699219701">\(\mathbb{M}\)</a>),</span> John Boyd in <em>SIAM News</em>.</p>
  </li>
  <li>
    <p><a href="https://www.npr.org/2021/10/30/1050817670/university-florida-professors-free-speech-voting-rights">University of Florida blocks faculty from being expert witnesses on voting rights, claiming that lawsuits against the state create a conflict of interest</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107198292249996684">\(\mathbb{M}\)</a>,</span> <a href="https://www.chronicle.com/article/u-of-florida-stops-professors-from-participating-in-voting-rights-suit-raising-cries-of-censorship">also</a>, <a href="https://www.nytimes.com/2021/10/29/us/florida-professors-voting-rights-lawsuit.html">also</a>). Beyond being an assault on academic freedom, this appears to violate the 1st amendment. US states can limit on-the-job speech, but here they would have testified on their own time, a routine activity that was blocked only because of its content.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/10/31/linkage.html"><span class="datestr">at October 31, 2021 06:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/31/assistant-professor-with-tenure-at-university-of-amsterdam-apply-by-november-22-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/31/assistant-professor-with-tenure-at-university-of-amsterdam-apply-by-november-22-2021/">Assistant Professor (with tenure) at University of Amsterdam (apply by November 22, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We currently have a vacancy for an Assistant Professor in Model-Based AI at the Institute for Logic, Language and Computation (ILLC) at the University of Amsterdam. Relevant topics of expertise include SAT solving, constraint programming, planning and scheduling, answer set programming, description logics, ontology engineering, and computer-aided verification (non-exhaustive list).</p>
<p>Website: <a href="https://www.illc.uva.nl/NewsandEvents/News/Positions/newsitem/13023/Assistant-Professor-in-Model-Based-AI">https://www.illc.uva.nl/NewsandEvents/News/Positions/newsitem/13023/Assistant-Professor-in-Model-Based-AI</a><br />
Email: Ulle Endriss</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/31/assistant-professor-with-tenure-at-university-of-amsterdam-apply-by-november-22-2021/"><span class="datestr">at October 31, 2021 11:46 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/31/postdoc-at-university-of-california-irvine-apply-by-january-31-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/31/postdoc-at-university-of-california-irvine-apply-by-january-31-2022/">Postdoc  at University of California, Irvine (apply by January 31, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>UC Irvine has one postdoc position open to work in Vijay Vazirani’s group on algorithm design and algorithmic game theory. Can start anytime, one year, renewable one more year.</p>
<p>Website: <a href="https://recruit.ap.uci.edu/JPF07189">https://recruit.ap.uci.edu/JPF07189</a><br />
Email: <a href="https://recruit.ap.uci.edu/JPF07189">https://recruit.ap.uci.edu/JPF07189</a></p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/31/postdoc-at-university-of-california-irvine-apply-by-january-31-2022/"><span class="datestr">at October 31, 2021 11:36 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/10/30/two-counterexamples-covering">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/10/30/two-counterexamples-covering.html">Two counterexamples for covering points by two polygons</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>One of our students made a seminar presentation this week on the problem of covering
a given set of \(n\) points by two scaled and translated copies of a given convex <span style="white-space: nowrap;">\(k\)-gon,</span> minimizing the scale factor, based on a 2016 conference paper on the problem published by someone else. The presentation was good but the paper turns out to be wrong in several ways. I think it should not be difficult to find the paper if you look, but I’m avoiding naming it here: it’s uncited, so it hasn’t done much damage to the literature, its author’s other papers are not as far as I know problematic, the author appears to want to forget the paper (there was no journal version and it is not included in their online publication list), and I want to focus on the geometric nature of the mistakes rather than on pointing fingers.</p>

<p>The algorithm of the paper starts by (correctly) computing the optimal cover of the given points by a single copy of the polygon. This is a low-dimensional linear program, but one with \(O(nk)\) constraints, one for each pair of a side of the polygon and an input point to be covered. The paper makes the observation that the only pairs that matter are the ones that match each side to the extreme point in a perpendicular direction, and it uses a nice divide-and-conquer method to find these extreme points in time \(O(n\log k)\), which turns out to be the bottleneck of the algorithm.</p>

<p>It then tries to use this one-copy cover to speed up the two-copy cover, and this is where things go wrong. It claims that if the one-copy cover has five or more of its sides touched by input points, then the optimal two-copy cover has the same scale factor: there are so many touching points that three or more of them would all have to be in the same copy of the two-copy cover, and would force it to have the same size as the one-copy cover. This is false.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/6fixture-can-shrink.svg" alt="Even when a regular hexagon is touched on six sides by the points that it covers, it may be possible to cover the same points by two smaller regular hexagons" /></p>

<p>It claims that, when four or fewer sides of the one-copy cover are touched by input points, then for the optimal two-copy cover, one of the two copies will be touched on two sides with the same slopes. If true this would greatly simplify the search for the optimal partition into two subsets and the optimal scale factor for that partition. But it is false: there exist instances where the slopes of sides touched on the two-copy cover are completely disjoint from the slopes touched on the one-copy cover.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/4fixture-can-shrink.svg" alt="Even when an octagon is touched on four sides by the points that it covers, the smallest two scaled and translated octagons that cover the same points may not be touched on any of the same sides" /></p>

<p>I think what is going on here is a more general phenomenon relating to linear programming, rather than to this specific geometric optimization problem. Suppose you have a linear program with \(k\) variables (here, three variables for the scale factor and translation vector of the single-copy cover), and to keep things simple let’s say that it has a unique optimal solution. That solution will be determined by a basis, a minimal subset of inequalities with the same solution. The basis inequalities become tight equalities rather than inequalities, and if you know which ones they are then you can just solve these \(k\) linear equations in the \(k\) unknowns. However, more than \(k\) inequalities may be  tight, and there may be more than one basis. With some additional assumptions that are automatically satisfied here you can choose any <span style="white-space: nowrap;">\(k\)-tuple</span> of tight inequalities, solve linear equations, and get the same solution. But that does <em>not</em> mean that every <span style="white-space: nowrap;">\(k\)-tuple</span> of tight inequalities is a basis, and it does not mean that these tight inequalities stay tight for all subsets of the constraints.</p>

<p>Turning this idea back into geometry, in each of these figures, the inequalities come from pairs of a point and a polygon side. Each inequality states that its point must be contained in the halfspace bounded by its side, and it is tight when the point lies on the side. Any translated and scaled copy of a polygon is determined by points that touch any three of its sides. But forming a basis, for the problem of covering points with the smallest possible copy, is a stronger property. A set of three sides and (maybe fewer than three) touching points forms a basis only when the normal vectors to those sides do not span an angle less <span style="white-space: nowrap;">than \(\pi\).</span></p>

<p>For instance, for the hexagons of the first illustration, triples of consecutive sides have normal vectors that span only an angle <span style="white-space: nowrap;">of \(2\pi/3\),</span> too small to form a basis. Other triples of sides have normal vectors that are more spread out and do form a basis. The blue one-copy hexagon containing all of the points is optimal, because all six of its sides are touched, and many triples of those touched sides are non-consecutive. But when we split the points into the two yellow hexagons, each yellow hexagon contains the points that touch only a consecutive triple of blue sides, not a basis. Because they are not a basis, those three points (and the other points that are grouped along with them) can be contained in a smaller hexagon than the blue one. (Exercise: find a basis for one of the yellow hexagons, showing that at least for this partition into two subsets the scale factor is optimal.)</p>

<p>(<a href="https://mathstodon.xyz/@11011110/107192319455242081">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/10/30/two-counterexamples-covering.html"><span class="datestr">at October 30, 2021 12:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/29/tenure-track-assistant-professor-position-in-algorithms-at-university-of-vienna-austria-apply-by-january-10-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/29/tenure-track-assistant-professor-position-in-algorithms-at-university-of-vienna-austria-apply-by-january-10-2022/">Tenure-track assistant professor position in algorithms at University of Vienna, Austria (apply by January 10, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The faculty of Computer Science is inviting applications for a full-time tenure-track assistant professor position in the design and analysis of algorithms. Excellent candidates in all fields of algorithms are encouraged to apply.</p>
<p>Website: <a href="https://univis.univie.ac.at/ausschreibungstellensuche/flow/bew_ausschreibung-flow?_flowExecutionKey=_cE6855C1E-D7E3-F933-5554-A27CD0FF2612_kFC207DD0-4381-FA23-2F97-263156792539&amp;tid=88323.28">https://univis.univie.ac.at/ausschreibungstellensuche/flow/bew_ausschreibung-flow?_flowExecutionKey=_cE6855C1E-D7E3-F933-5554-A27CD0FF2612_kFC207DD0-4381-FA23-2F97-263156792539&amp;tid=88323.28</a><br />
Email: monika.henzinger@univie.ac.at</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/29/tenure-track-assistant-professor-position-in-algorithms-at-university-of-vienna-austria-apply-by-january-10-2022/"><span class="datestr">at October 29, 2021 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/29/postdoc-at-bocconi-university-apply-by-december-12-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/29/postdoc-at-bocconi-university-apply-by-december-12-2021/">Postdoc at Bocconi University (apply by December 12, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Bocconi University has two open postdoc positions in Luca Trevisan’s group, to work on topics related to spectral graph theory, approximation algorithms, average-case complexity and related areas, starting Fall 2022. Each position is for one year, renewable for a second.</p>
<p>Website: <a href="https://lucatrevisan.wordpress.com/2021/10/21/postdoc-positions/">https://lucatrevisan.wordpress.com/2021/10/21/postdoc-positions/</a><br />
Email: l.trevisan@unibocconi.it</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/29/postdoc-at-bocconi-university-apply-by-december-12-2021/"><span class="datestr">at October 29, 2021 01:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2021-10-29-consensus-cheat-sheet/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2021-10-29-consensus-cheat-sheet/">Consensus cheat sheet</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Crash Omission Byzantine Synchrony $f&lt;n$ is possible $f+1$ round executions must exist $f \geq n/2$ is impossible $f&lt;n/2$ possible with PKI / PoW $f \geq n/3$ impossible without PKI/PoW Partial Synchrony $f \geq n/2$ is impossible $f&lt;n/2$ is possible $f&lt;n/3$ is possible $f \geq n/3$ is impossible Asynchrony non...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2021-10-29-consensus-cheat-sheet/"><span class="datestr">at October 29, 2021 12:18 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/28/faculty-at-imperial-college-london-apply-by-january-4-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/28/faculty-at-imperial-college-london-apply-by-january-4-2022/">Faculty at Imperial College London (apply by January 4, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computing at Imperial College London invites applications for four full-time permanent faculty members at the Assistant Professorships level (lecturer). Exceptional candidates may be appointed at Associate Professorships level.</p>
<p>We offer a competitive salary package, and we will consider outstanding candidates from any area of Computer Science.</p>
<p>Website: <a href="https://www.imperial.ac.uk/jobs/description/ENG01911/lecturers-computing/">https://www.imperial.ac.uk/jobs/description/ENG01911/lecturers-computing/</a><br />
Email: margaret.hall@imperial.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/28/faculty-at-imperial-college-london-apply-by-january-4-2022/"><span class="datestr">at October 28, 2021 05:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/28/faculty-at-northeastern-university-apply-by-december-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/28/faculty-at-northeastern-university-apply-by-december-1-2021/">Faculty at Northeastern University (apply by December 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Northeastern is hiring broadly in all areas of computer science, with two specially designated faculty slots relating to theory:</p>
<p>* Quantum computing: broadly defined, including quantum algorithms, quantum complexity and quantum information theory.</p>
<p>* Cryptography, with an emphasis on quantum and/or post-quantum cryptography.</p>
<p>Website: <a href="https://www.khoury.northeastern.edu/information-for-overview/prospective-faculty/open-positions/">https://www.khoury.northeastern.edu/information-for-overview/prospective-faculty/open-positions/</a><br />
Email: khoury-hr@northeastern.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/28/faculty-at-northeastern-university-apply-by-december-1-2021/"><span class="datestr">at October 28, 2021 06:23 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/27/faculty-at-university-of-waterloo-apply-by-december-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/27/faculty-at-university-of-waterloo-apply-by-december-1-2021/">Faculty at University of Waterloo (apply by December 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The David R. Cheriton School of Computer Science at the University of Waterloo invites applications for eight tenure-track Assistant Professor positions, subject to budget approval. Priority areas include:</p>
<p>1. Data Systems<br />
2. Systems and Networking<br />
3. Robotics, Machine Learning, Multiagent Systems, Natural Language Processing, or Computer Vision 4. Bioinformatics<br />
5. All areas of Computer Science</p>
<p>Website: <a href="https://cs.uwaterloo.ca/about/open-positions/tenure-track-positions">https://cs.uwaterloo.ca/about/open-positions/tenure-track-positions</a><br />
Email: cs-recruiting@uwaterloo.ca</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/27/faculty-at-university-of-waterloo-apply-by-december-1-2021/"><span class="datestr">at October 27, 2021 11:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3113376765706910797">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/10/fall-2021-jobs-post.html">Fall 2021 Jobs Post</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>We're in the midst of a great transformation in computing, one where data takes center stage and I predict this will start to have a larger effect on hiring in computer science departments. We'll see a bigger need to grow in data science, particularly machine learning and autonomous systems. Cybersecurity and quantum computing will also grow with a push due to competition with China. Quantum winter might be coming but we're not there yet.</p><p>Harder to predict is the rest of computer science, such as traditional areas like networks, operating systems, programming languages and, yes, theory, particularly theory unrelated to quantum, learning or security. There is still a need for CS departments to grow in these areas, but we may be moving away from a rising tide raising all boats. On the other hand due to the digital transformation of just about everything, non-CS departments are hiring people who look a lot like computer scientists.</p><p>Other factors may cause US universities to be more conservative in hiring such as a <a href="https://www.wsj.com/articles/college-university-fall-higher-education-men-women-enrollment-admissions-back-to-school-11630948233?st=1oz46jkmsdq8jcq&amp;reflink=desktopwebshare_permalink">drop in male students</a>, the upcoming <a href="https://www.cupahr.org/issue/feature/higher-ed-enrollment-cliff/">demographic cliff</a>, an unclear future for international students coming to the states, and a lingering COVID budget hangover.</p><p>So go get a job while the going is still good though I would not suggest forgoing a faculty position for a postdoc, particularly if you aren't working in data science.</p><p>I also wonder how the post-COVID world will affect the job search. We'll probably see more virtual interviews than the pre-COVID days at least in the early rounds. It's also harder for students to network and make themselves known at virtual and hybrid conferences which will likely persist for some time.</p><p>Give yourself a good virtual face. Have a well-designed web page with access to all your job materials and papers. Maintain your Google Scholar page. Add yourself to the CRA's <a href="https://cra.org/cv-database/">CV database</a>. Find a way to stand out, perhaps a short video describing your research. </p><p>Best source for finding jobs are the ads from the <a href="https://cra.org/ads/">CRA</a> and the <a href="https://jobs.acm.org/">ACM</a>. For theoretical computer science specific postdoc and faculty positions check out <a href="https://cstheory-jobs.org/">TCS Jobs</a> and <a href="http://dmatheorynet.blogspot.com/">Theory Announcements</a>. If you have jobs to announce, please post to the above and/or feel free to leave a comment on this post. Even if you don't see an ad for a specific school they may still be hiring, check out their website or email someone at the department. You'll never know if you don't ask.</p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/10/fall-2021-jobs-post.html"><span class="datestr">at October 27, 2021 09:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/27/postdoc-at-tu-eindhoven-university-of-amsterdam-cwi-leiden-university-apply-by-november-15-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/27/postdoc-at-tu-eindhoven-university-of-amsterdam-cwi-leiden-university-apply-by-november-15-2021/">postdoc at TU Eindhoven, University of Amsterdam, CWI, Leiden University (apply by November 15, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The NETWORKS project focuses on stochastics and algorithmics for network problems. Are you an algorithms researcher (or a stochastics researcher) with an interest in network problems? And would you like to be part of the NETWORKS project with its many activities? Then we invite you to apply for one of these positions.</p>
<p>Website: <a href="https://www.thenetworkcenter.nl/Open-Positions/openposition/30/14-Postdoctoral-fellows-in-Stochastics-and-Algorithmics-COFUND-">https://www.thenetworkcenter.nl/Open-Positions/openposition/30/14-Postdoctoral-fellows-in-Stochastics-and-Algorithmics-COFUND-</a><br />
Email: info@thenetworkcenter.nl</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/27/postdoc-at-tu-eindhoven-university-of-amsterdam-cwi-leiden-university-apply-by-november-15-2021/"><span class="datestr">at October 27, 2021 07:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/10/27/senior-lecturer-associate-professor-at-the-university-of-melbourne-apply-by-november-10-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/10/27/senior-lecturer-associate-professor-at-the-university-of-melbourne-apply-by-november-10-2021/">Senior Lecturer/Associate Professor at The University of Melbourne (apply by November 10, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Faculty of Engineering and Information Technology (FEIT) is seeking a dynamic academic with expertise in algorithms and their fairness, or related fields, to join the School of Computing and Information Systems (CIS).</p>
<p>Website: <a href="https://jobs.unimelb.edu.au/en/job/906606/senior-lecturerassociate-professor-in-algorithms-and-fairness">https://jobs.unimelb.edu.au/en/job/906606/senior-lecturerassociate-professor-in-algorithms-and-fairness</a><br />
Email: awirth@unimelb.edu.au</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/10/27/senior-lecturer-associate-professor-at-the-university-of-melbourne-apply-by-november-10-2021/"><span class="datestr">at October 27, 2021 05:58 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/how-to-deploy-ml-with-dp/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/how-to-deploy-ml-with-dp/">How to deploy machine learning with differential privacy?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In many applications of machine learning, such as machine learning for medical diagnosis, we would like to have machine learning algorithms that do not memorize sensitive information about the training set, such as the specific medical histories of individual patients. Differential privacy is a notion that allows quantifying the degree of privacy protection provided by an algorithm on the underlying (sensitive) data set it operates on. Through the lens of differential privacy, we can design machine learning algorithms that responsibly train models on private data.</p>

<h2 id="why-do-we-need-private-machine-learning-algorithms">Why do we need private machine learning algorithms?</h2>

<p>Machine learning algorithms work by studying a lot of data and updating their parameters to encode the relationships in that data. Ideally, we would like the parameters of these machine learning models to encode general patterns (e.g., ‘‘patients who smoke are more likely to have heart disease’’) rather than facts about specific training examples (e.g., “Jane Smith has heart disease”). Unfortunately, machine learning algorithms do not learn to ignore these specifics by default. If we want to use machine learning to solve an important task, like making a cancer diagnosis model, then when we publish that machine learning model (for example, by making an open source cancer diagnosis model for doctors all over the world to use) we might also inadvertently reveal information about the training set. A malicious attacker might be able to inspect the published model’s predictions and learn private information about Jane Smith. For instance, the adversary could mount a membership inference attack to know whether or not Jane Smith contributed her data to the model’s training set [SSS17]. The adversary could also build on membership inference attacks to extract training data by repeatedly guessing possible training points until they result in a sufficiently strong membership signal from the model’s prediction [CTW20]. In many instances, the model itself may be represented by a few of the data samples (e.g., Support Vector Machine in its dual form).</p>

<p>A common misconception is that if a model generalizes (i.e., performs well on the test examples), then it preserves privacy. As mentioned earlier, this is far from being true. One of the main reasons being that generalization is an average case behavior of a model (over the distribution of data samples), whereas privacy must be provided for everyone, including outliers (which may deviate from our distributional assumptions).</p>

<p>Over the years, researchers have proposed various approaches towards protecting privacy in learning algorithms (k-anonymity [SS98], l-diversity [MKG07], m-invariance [XT07], t-closeness [LLV07] etc.). Unfortunately, [GKS08] all these approaches are vulnerable to what are called composition attacks, that use auxiliary information to violate the privacy protection. Famously, this strategy allowed researchers to de-anonymize part of a movie ratings dataset released to participants of the Netflix Prize when the individuals had also shared their movie ratings publicly on the Internet Movie Database (IMDb) [NS08]. If Jane Smith had assigned the same ratings to movies A, B and C in the Netflix Prize dataset and publicly on IMDb at similar times, then researchers could link data corresponding to Jane across both datasets. This would in turn give them the means to recover ratings that were included in the Netflix Prize but not on IMDb. This example shows how difficult it is to define and guarantee privacy because it is hard to estimate the scope of knowledge—about individuals—available to adversaries. While the dataset released by Netflix has since been taken down, it is difficult to ensure that all of its copies have been deleted. In recent years, data sample instance encoding based methods like InstaHide [HSL20], and NeuraCrypt [YEO21] have been demonstrated to be vulnerable to such composition attacks as well.</p>

<p>As a result, the research community has converged on differential privacy [DMNS06], which provides the following semantic guarantee, as opposed to ad-hoc approaches: An adversary learns almost the same information about an individual whether or not they are present in or absent from the training data set. In particular, it provides a condition on the algorithm, independent from who might be attacking it, or the specifics of the data set instantiation.  Put another way, differential privacy is a framework for evaluating the guarantees provided by a system that was designed to protect privacy. Such systems can be applied directly to “raw” data which potentially still contains sensitive information, altogether removing the need for procedures that sanitize or anonymize data and are prone to the failures described previously. That said, minimizing data collection in the first place remains a good practice to limit other forms of privacy risk.</p>

<h2 id="designing-private-machine-learning-algorithms-via-differential-privacy">Designing Private Machine Learning Algorithms via Differential Privacy</h2>

<p>Differential privacy [DMNS06] is a semantic notion of privacy that addresses a lot of the limitations of previous approaches like k-anonymity. The basic idea is to randomize part of the mechanism’s behavior to provide privacy. In our case, the mechanism considered is a learning algorithm, but the differential privacy framework can be applied to study any algorithm.</p>

<p>The intuition for why we introduce randomness into the learning algorithm is that it obscures the contribution of an individual, but does not obscure important statistical patterns. Without randomness, we would be able to ask questions like: “What parameters does the learning algorithm choose when we train it on this specific dataset?” With randomness in the learning algorithm, we instead ask questions like: “What is the probability that the learning algorithm will choose parameters in this set of possible parameters, when we train it on this specific dataset?”</p>

<p>We use a version of differential privacy which requires (in our use case of machine learning) that the probability of learning any particular set of parameters stays roughly the same if we change a single data record in the training set. A data record can be a single training example from an individual, or the collection of all the training examples provided by an individual. The former is often referred to as example level/item level privacy, and the latter is referred to as user level differential privacy. While user level privacy provides stronger semantics, it may be harder to achieve. For a more thorough discussion about the taxonomy of these notions, see [DNPR10, JTT18, HR12, HR13]. In this document, for the ease of exposition of the technical results, we focus on the example level notion. This could mean to add a training example, remove a training example, or change the values within one training example. The intuition is that if a single patient (Jane Smith) does not affect the outcome of learning much, then that patient’s records cannot be memorized and her privacy is respected. In the rest of this post, how much a single record can affect the outcome of learning is called the sensitivity of an algorithm.</p>

<p>The guarantee of differential privacy is that the adversary is not able to distinguish the answers produced by the randomized algorithm based on the data of two of the three users from the answers returned by the same algorithm based on the data of all three users. We also refer to the degree of indistinguishability as the privacy loss. Smaller privacy loss corresponds to a stronger privacy guarantee.</p>

<p>It is often thought that privacy is a fundamental bottleneck in obtaining good prediction accuracy/generalization for machine learning algorithms. In fact, recent research has shown that in many instances it actually helps in designing algorithms with strong generalization ability. Some of the examples where DP has resulted in designing better learning algorithms are Online linear predictions [KV05] and online PCA [DTTZ13]. Notably, [DFH15] formally showed that generalization for any DP learning algorithm comes for free. More concretely, if a DP learning algorithm has good training accuracy, it is guaranteed to have good test accuracy.
This is true because differential privacy itself acts as a very strong form of regularization.</p>

<p>One might argue that the generalization guarantee which a DP algorithm can achieve may be sub-par to that of its non-private baselines. For a large class of learning tasks, one can show that asymptotically DP does not introduce any further error beyond the inherent statistical error [SSTT21]. [ACG16,BFTT19] highlights that in the presence of enough data, a DP algorithm can get arbitrarily close to the inherent statistical error, even under strong privacy parameters.</p>

<h2 id="private-empirical-risk-minimization">Private Empirical Risk Minimization</h2>

<p>Before we go into the design of specific differentially private learning algorithms, we first formalize the problem setup, and standardize some notation. Consider a training data set \(D={(x_1,y_1),…,(x_n,y_n)}\) drawn i.i.d. from some fixed (unknown) distribution \(\Pi\), with the feature vector being \(x_i\) and label/response being \(y_i\). We define the training loss at any model \(\theta\) as \(L_{train} (\theta, D) = \frac{1}{n} \sum_{i=1}^{n} l(\theta; (x_i, y_i))\), and the corresponding test loss as \(L_{test} (\theta) = E_{(x,y) \sim \Pi} l(\theta;(x,y)) \).
We will design DP algorithms to output models that approximately minimize the test loss while having access only to the training loss.</p>

<p>In the literature, there are a variety of approaches towards designing these DP learning algorithms [CMS11, KST12, BST14, PAE16, BTT18]. One can categorize them broadly as: i) algorithms that assume that the individual loss function \(l(\theta;\cdot) \) is convex in the model parameter  to ensure differential privacy, ii) algorithms that are differentially private even when the loss function is non-convex in nature (e.g., deep learning models), and iii) model agnostic algorithms, that do not require any information about the representation of the model \(\theta\), or the loss function \(l(\theta;\cdot) \). In our current discussion, we will only focus on designing algorithms for (ii), and (iii). This is because it turns out that the best known algorithms for (ii) are already competitive to algorithms that are specific for (i) [INS19].</p>

<h2 id="private-algorithms-for-training-deep-learning-models">Private Algorithms for Training Deep Learning Models</h2>

<p>The first approach, due to SCS13, BST14, and ACG16, is named differentially private stochastic gradient descent (DP-SGD). It proposes to modify the model updates computed by the most common optimizer used in deep learning: stochastic gradient descent (SGD). Typically, stochastic gradient descent trains iteratively. At each iteration, a small number of training examples (a “minibatch”) are sampled from the training set. The optimizer computes the average model error on these examples, and then differentiates this average error with respect to each of the model parameters to obtain a gradient vector. Finally, the model parameters (\(\theta_t\)) are updated by subtracting this gradient (\(\nabla_t\)) multiplied by a small constant \(\eta\) (the learning rate controls how quickly the optimizer updates the model’s parameters). At a high level, two modifications are made by DP-SGD to obtain differential privacy: gradients, which are computed on a per-example basis (rather than averaged over multiple examples), are first clipped to control their sensitivity, and, second, spherical Gaussian noise \(b_t\) is added to their sum to obtain the indistinguishability needed for DP. Succinctly, the update step can be written as follows: \(\theta_{t+1} \leftarrow \theta_t - \eta \cdot (\nabla_t + b_t)\).</p>

<p>Let us take the example of a hospital training a model to predict whether patients will be readmitted after being released. To train the model, the hospital uses information from patient records, such as demographic variables and admission variables (e.g., age, ethnicity, insurance type, type of Intensive Care Unit admitted to) but also time-varying vitals and labs (e.g., heart rate, blood pressure, white blood cell counts) [JPS16]. The modifications made by DP-SGD ensure that if (1) Jane Smith’s individual patient record contained unusual features, e.g., her insurance provider was uncommon for people of her age or her heart rate followed an unusual pattern, the resulting signal will have a bounded impact on our model updates, and (2) the model’s final parameters would be essentially identical should Jane Smith have chosen to not contribute (i.e., opt-out) her patient record to the training set. Stronger differential privacy is achieved when one is able to introduce more noise (i.e., sample noise with larger standard deviation) and train for as few iterations as possible.</p>

<p>Two main components in the above DP-SGD algorithm that distinguishes itself from traditional SGD are: i) per-example clipping and ii) Gaussian noise addition. In addition, for the analysis to hold, DP-SGD requires that sub-sampling of mini batches is uniform at random from the training data set. While this is not a requirement of DP-SGD per se, in practice many implementations of SGD do not satisfy this requirement and instead analyze different permutations of the data at each epoch of training.</p>

<p>While gradient clipping is common in deep learning, often used as a form of regularization, it differs from that in DP-SGD as follows: The average gradient over the minibatch is clipped, as opposed to clipping the gradient of individual examples (i.e., \(l(\theta_t;(x,y)) \) before averaging. It is an ongoing research direction to both understand the effect of per-example clipping in DP-SGD in model training [SSTT21], and also effective ways to mitigate its impact both in terms of accuracy [PTS21], and training time [ZHS19].</p>

<p>In standard stochastic gradient descent, subsampling is usually used either as a way to speed up the training process [CAR16], or as a a form of regularization [RCR15]. In DP-SGD, the randomness in the subsampling of the minibatch is used to guarantee DP. The technical component for this sort of privacy analysis is called privacy amplification by subsampling [KLNRS08,BBG18]. Since the sampling randomness is used to guarantee DP, it is crucial that the uniformity in the sampling step is of cryptographic strength. Another, (possibly) counterintuitive feature of DP-SGD is that for best privacy/utility trade-off it is in general better to have larger batch sizes. In fact, full-batch DP-gradient descent may provide the best privacy/utility trade-offs, albeit at the expense of computational feasibility.</p>

<p>For a fixed DP guarantee, the magnitude of the Gaussian noise that gets added to the gradient updates in each step in DP-SGD is proportional to \(\sqrt{the\ number\ of\ steps}\) the model is trained for. As a result, it is important to tune the number of training steps for best privacy/utility trade-offs.</p>

<p>In the <a href="https://github.com/tensorflow/privacy/blob/master/tutorials/mnist_dpsgd_tutorial.py">following tutorial</a>, we provide a small code snippet to train a model with DP-SGD.</p>

<h2 id="model-agnostic-private-learning">Model Agnostic Private Learning</h2>

<p>The Sample and Aggregate framework [NRS07] is a generic method to add differential privacy to a non-private algorithm without caring about the internal workings of it, a.k.a. model agnostic. In the context of machine learning, one can state the main idea as follows: Consider a multi-class classification problem. Take the training data, and split into k disjoint subsets of equal size. Train independent models \(\theta_1, \theta_2, …, \theta_k \) on the disjoint subsets. In order to predict on an test example x, first, compute a private histogram over the set of k predictions \(\theta_1(x), \theta_2(x), …, \theta_k(x) \). Then, select and output the bin in the histogram based on the highest count, after adding a small amount of Laplace/Gaussian noise to the counts. In the context of DP learning, this particular approach was used in two different lines of work: i) PATE [PAE16], and ii) Model agnostic private learning [BTT18]. While the latter focussed on obtaining theoretical privacy/utility trade-offs for a class of learning tasks (e.g., agnostic PAC learning), the PATE approach focuses on practical deployment. Both these lines of work make one common observation. If the predictions from \(\theta_1(x), \theta_2(x), …, \theta_k(x) \) are fairly consistent, then the privacy cost in terms of DP is very small. Hence, one can run a large number of prediction queries, without violating DP constraints. In the following, we describe the PATE approach in detail.</p>

<p>The private aggregation of teacher ensembles (PATE) demonstrated in particular that this approach allows one to learn deep neural networks with differential privacy. It proposes to have an ensemble of models trained without privacy predict with differential privacy by having these models predict in aggregate rather than revealing their individual predictions. In PATE, we start by partitioning the private dataset into smaller subsets of data. These subsets are partitions, so there is no overlap between the data included in any pair of partitions. If Jane Smith’s record was in our private dataset, then it is included in one of the partitions only. That is, only one of the teachers has analyzed Jane Smith’s record during training. We train a ML model, called a teacher, on each of these partitions. We now have an ensemble of teacher models that were trained independently, but without any guarantees of privacy. How do we use this ensemble to make predictions that respect privacy? In PATE, we add noise while aggregating the predictions made individually by each teacher to form a single common prediction. We count the number of teachers who voted for each class, and then perturb that count by adding random noise sampled from the Laplace or Gaussian distribution. Each label predicted by the noisy aggregation mechanism comes with rigorous differential privacy guarantees that bound the privacy budget spent to label that input. Again, stronger differential privacy is achieved when we are able to introduce more noise in the aggregation and are able to answer as few queries as possible. Let us now come back to our running example. Imagine that we’d like to use the output of PATE to know if Jane likes a particular movie. The only teacher trained on the partition containing Jane Smith’s data—has now learned that a record similar to Jane’s is characteristic of an individual who likes similar movies, and as a consequence changes its prediction on a test input which is similar to Jane’s to predict the movie rating assigned by Jane. However, because the teacher only contributes a single vote to the aggregation, and that the aggregation injects noise, we won’t be able to know whether the teacher changed its prediction to the movie rating assigned by Jane because the teacher indeed trained on Jane’s data or because the noise injected during the aggregation “flipped” that teacher’s vote. The random noise added to vote counts prevents the outcome of aggregation from reflecting the votes of any individual teachers to protect privacy.</p>

<h2 id="practically-deploying-differential-privacy-in-machine-learning">Practically deploying differential privacy in machine learning</h2>

<p>The two approaches we introduced have the advantage of being conceptually simple to understand. Fortunately, there also exist several open-source implementations of these approaches. For instance, DP-SGD is implemented in TensorFlow Privacy, Objax, and Opacus. This means that one is able to take an existing TensorFlow, JAX, or PyTorch pipeline for training a machine learning model and replace a non-private optimizer with DP-SGD. An example implementation of PATE is also available in TensorFlow Privacy. So what are the concrete potential obstacles to deploying machine learning with differential privacy?</p>

<p>The first obstacle is the accuracy of privacy-preserving models. Datasets are often sampled from distribution with heavy tails. For instance, in a medical application, there are typically (and fortunately) fewer patients with a given medical condition than patients without that condition. This means that there are fewer training examples for patients with each medical condition to learn from. Because differential privacy prevents us from learning patterns which are not found generally across the training data, it limits our ability to learn from these patients for which we have very few examples of [SPG]. More generally, there is often a trade-off between the accuracy of a model and the strength of the differential privacy guarantee it was trained with: the smaller the privacy budget is, the larger the impact on accuracy typically is. That said, this tension is not always inevitable and there are instances where privacy and accuracy are synergical because differential privacy implies generalization [DFH15] (but not vice versa).</p>

<p>The second obstacle to deploying differentially private machine learning can be the computational overhead. For instance, in DP-SGD one must compute per-example gradients rather than average gradients. This often means that optimizations implemented in machine learning frameworks to exploit matrix algebra supported by underlying hardware accelerators (e.g., GPUs) are harder to take advantage of. In another example, PATE requires that one train multiple models (the teachers) rather than a single model so this can also introduce overhead in the training procedure. Fortunately, this cost is mostly mitigated in recent implementations of private learning algorithms, in particular in Objax and Opacus.</p>

<p>The third obstacle to deploying differential privacy, in machine learning but more generally in any form of data analysis, is the choice of privacy budget. The smaller the budget, the stronger the guarantee is. This means one can compare two analyses and say which one is “more private”. However, this also means that it is unclear what is “small enough” of a privacy budget. This is particularly problematic given that applications of differential privacy to machine learning often require a privacy budget that provides little theoretical guarantees in order to train a model whose accuracy is large enough to warrant a useful deployment. Thus, it may be interesting for practitioners to evaluate the privacy of their machine learning algorithm by attacking it themselves. Whereas the theoretical analysis of an algorithm’s differential privacy guarantees provides a worst-case guarantee limiting how much private information the algorithm can leak against any adversary, implementing a specific attack can be useful to know how successful a particular adversary or class of adversaries would be. This helps interpret the theoretical guarantee but may not be treated as a direct substitute for it. Open-source implementations of such attacks are increasingly available: e.g., for membership inference <a href="https://github.com/tensorflow/privacy/tree/master/tensorflow_privacy/privacy/privacy_tests/membership_inference_attack">here</a> and <a href="https://github.com/cchoquette/membership-inference">here</a>.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In the above, we discussed some of the algorithmic approaches towards differentially private model training which have been effective both in theoretical and practical settings. Since it is a rapidly growing field, we could not cover all the important aspects of the research space. Some prominent ones include: i) Choice of the best hyperparameters in the training of DP models.In order to ensure that the overall algorithm preserves differential privacy, one needs to ensure that the choice of hyperparameters itself preserves DP. Recent research has provided algorithms for selecting the best hyperparameters in a differentially private fashion [LT19]. ii) Choice of network architecture: it is not always true that the best known model architectures for non-private model training are indeed the best for training with differential privacy. In particular, we know that the number of model parameters may have adverse effects on the privacy/utility trade-offs [BST14]. Hence, choosing the right model architecture is important for providing a good privacy/utility trade-off [PTS21]. (iii) Training in the federated/distributed setting: in the above exposition, we assumed that the training data lies in a single centralized location. However, in settings like Federated Learning (FL) [MMRHA17], the data records can be highly distributed, e.g., across various mobile devices. Running DP-SGD in the FL setting, which is required for FL to provide privacy guarantees for the training data, raises a series of challenges [KMA19] which are often facilitated by distributed private learning algorithms designed specifically for FL settings [BKMTT20, KMSTTZ21]. Some of the specific challenges in the context of FL include, limited and non-uniform availability of clients (holding individual data records) and unknown (and variable) size of the training data [BKMTT18]. On the other hand, PATE style algorithms lend themselves naturally to the distributed setting once combined with existing cryptographic primitives, as demonstrated by the CaPC protocol [CDD21]. It is an active area of research to address these above challenges.</p>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>The authors would like to thank Thomas Steinke and Andreas Terzis for detailed feedback and edit suggestions. Parts of this blog post previously appeared on <a href="https://differentialprivacy.org/www.cleverhans.io">www.cleverhans.io</a>.</p>

<h2 id="citations">Citations</h2>

<p>[ACG16] Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K., &amp; Zhang, L. (2016, October). Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (pp. 308-318). ACM.</p>

<p>[BBG18] Balle, B., Barthe, G., &amp; Gaboardi, M. (2018). Privacy amplification by subsampling: Tight analyses via couplings and divergences. arXiv preprint arXiv:1807.01647.</p>

<p>[BKMTT18] Balle, B., Kairouz P., McMahan M., Thakkar O. &amp; Thakurta A. (2020). Privacy amplification via random check-ins. In NeurIPS.</p>

<p>[MMRHA17] McMahan, B., Moore, E., Ramage, D., Hampson, S., &amp; y Arcas, B. A. (2017, April). Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics (pp. 1273-1282). PMLR.</p>

<p>[KMSTTZ18] Kairouz P., McMahan M., Song S., Thakkar O., Thakurta A., &amp; Xu Z. (2021). Practical and Private (Deep) Learning without Sampling or Shuffling. In ICML.</p>

<p>[BFTT19] Bassily, R., Feldman, V., Talwar, K., &amp; Thakurta, A. Private Stochastic Convex Optimization with Optimal Rates. In NeurIPS 2019.</p>

<p>[BST14] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithms and tight error bounds. In Proceedings of the 55th Annual IEEE Symposium on Foundations of Computer Science.</p>

<p>[BTT18] Bassily, R., Thakurta, A. G., &amp; Thakkar, O. D. (2018). Model-agnostic private learning. Advances in Neural Information Processing Systems.</p>

<p>[CDD21] Choquette-Choo, C. A., Dullerud, N., Dziedzic, A., Zhang, Y., Jha, S., Papernot, N., &amp; Wang, X. (2021). CaPC Learning: Confidential and Private Collaborative Learning. arXiv preprint arXiv:2102.05188.</p>

<p>[CMS11] Chaudhuri, K., Monteleoni, C., &amp; Sarwate, A. D. (2011). Differentially private empirical risk minimization. Journal of Machine Learning Research, 12(3).</p>

<p>[CTW20] Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., … &amp; Raffel, C. (2020). Extracting training data from large language models. arXiv preprint arXiv:2012.07805.</p>

<p>[DFH15] Dwork, C., Feldman, V., Hardt, M., Pitassi, T., Reingold, O., &amp; Roth, A. (2015). Generalization in adaptive data analysis and holdout reuse. arXiv preprint arXiv:1506.02629.</p>

<p>[DMNS06] Dwork, C., McSherry, F., Nissim, K., &amp; Smith, A. (2006, March). Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography Conference (pp. 265-284). Springer, Berlin, Heidelberg.</p>

<p>[DNPR10] Dwork, C., Naor, M., Pitassi, T., &amp; Rothblum, G. N. (2010, June). Differential privacy under continual observation. In Proceedings of the forty-second ACM symposium on Theory of computing (pp. 715-724).</p>

<p>[DTTZ14] Dwork, C., Talwar, K., Thakurta, A., &amp; Zhang, L. (2014, May). Analyze gauss: optimal bounds for privacy-preserving principal component analysis. In Proceedings of the forty-sixth annual ACM symposium on Theory of computing (pp. 11-20).</p>

<p>[HSL20] Huang, Y., Song, Z., Li, K., &amp; Arora, S. (2020, November). Instahide: Instance-hiding schemes for private distributed learning. In International Conference on Machine Learning (pp. 4507-4518). PMLR.</p>

<p>[HR12] Hardt, M., &amp; Roth, A. (2012, May). Beating randomized response on incoherent matrices. In Proceedings of the forty-fourth annual ACM symposium on Theory of computing (pp. 1255-1268).</p>

<p>[HR13] Hardt, M., &amp; Roth, A. (2013, June). Beyond worst-case analysis in private singular vector computation. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing (pp. 331-340).</p>

<p>[JPS16] Johnson, A., Pollard, T., Shen, L. et al. MIMIC-III, a freely accessible critical care database. Sci Data 3, 160035 (2016). https://doi.org/10.1038/sdata.2016.35</p>

<p>[JTT18] Jain, P., Thakkar, O. D., &amp; Thakurta, A. (2018, July). Differentially private matrix completion revisited. In International Conference on Machine Learning (pp. 2215-2224). PMLR.</p>

<p>[INS19] Iyengar, R., Near, J. P., Song, D., Thakkar, O., Thakurta, A., &amp; Wang, L. (2019, May). Towards practical differentially private convex optimization. In 2019 IEEE Symposium on Security and Privacy (SP) (pp. 299-316). IEEE.</p>

<p>[KST12] Kifer, D., Smith, A., &amp; Thakurta, A. (2012, June). Private convex empirical risk minimization and high-dimensional regression. In Conference on Learning Theory (pp. 25-1). JMLR Workshop and Conference Proceedings.</p>

<p>[KMA19] Kairouz, P., McMahan, H. B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A. N., … &amp; Zhao, S. (2019). Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977.</p>

<p>[KV05] Kalai, Adam, and Santosh Vempala. “Efficient algorithms for online decision problems.” Journal of Computer and System Sciences 71.3 (2005): 291-307.</p>

<p>[KLNRS08] Raskhodnikova, S., Smith, A., Lee, H. K., Nissim, K., &amp; Kasiviswanathan, S. P. (2008). What can we learn privately. In Proceedings of the 54th Annual Symposium on Foundations of Computer Science (pp. 531-540).</p>

<p>[LLV07] Li, N., Li, T., &amp; Venkatasubramanian, S. (2007, April). t-closeness: Privacy beyond k-anonymity and l-diversity. In 2007 IEEE 23rd International Conference on Data Engineering (pp. 106-115). IEEE.</p>

<p>[LT19] Liu, J., &amp; Talwar, K. (2019, June). Private selection from private candidates. In Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing (pp. 298-309).</p>

<p>[M17] Mironov, I. (2017, August). Renyi differential privacy. In Computer Security Foundations Symposium (CSF), 2017 IEEE 30th (pp. 263-275). IEEE.</p>

<p>[MKG07] Machanavajjhala, Ashwin; Kifer, Daniel; Gehrke, Johannes; Venkitasubramaniam, Muthuramakrishnan (March 2007). “L-diversity: Privacy Beyond K-anonymity”. ACM Transactions on Knowledge Discovery from Data.</p>

<p>[NRS07] Nissim, K., Raskhodnikova, S., &amp; Smith, A. (2007, June). Smooth sensitivity and sampling in private data analysis. In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing (pp. 75-84).</p>

<p>[NS08] Narayanan, A., &amp; Shmatikov, V. (2008, May). Robust de-anonymization of large sparse datasets. In Security and Privacy, 2008. SP 2008. IEEE Symposium on (pp. 111-125). IEEE.</p>

<p>[PAE16] Papernot, N., Abadi, M., Erlingsson, U., Goodfellow, I., &amp; Talwar, K. (2016). Semi-supervised knowledge transfer for deep learning from private training data. ICLR 2017.</p>

<p>[PTS21] Papernot, N., Thakurta, A., Song, S., Chien, S., &amp; Erlingsson, U. (2020). Tempered sigmoid activations for deep learning with differential privacy. AAAI 2021.</p>

<p>[RCR15] Rudi, A., Camoriano, R., &amp; Rosasco, L. (2015, December). Less is More: Nyström Computational Regularization. In NIPS (pp. 1657-1665).</p>

<p>[SCS13] Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differentially private updates. In Proceedings of the 2013 IEEE Global Conference on Signal and Information Processing, GlobalSIP ’13, pages 245–248, Washington, DC, USA, 2013. IEEE Computer Society.</p>

<p>[SPG] Chasing Your Long Tails: Differentially Private Prediction in Health Care Settings. Vinith Suriyakumar, Nicolas Papernot, Anna Goldenberg, Marzyeh Ghassemi. Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency.</p>

<p>[SS98] Samarati, Pierangela; Sweeney, Latanya (1998). “Protecting privacy when disclosing information: k-anonymity and its enforcement through generalization and suppression” (PDF). Harvard Data Privacy Lab. Retrieved April 12, 2017</p>

<p>[SSS17] Shokri, R., Stronati, M., Song, C., &amp; Shmatikov, V. (2017, May). Membership inference attacks against machine learning models. In Security and Privacy (SP), 2017 IEEE Symposium on (pp. 3-18). IEEE.</p>

<p>[SSTT21] Song, S., Thakkar, O., &amp; Thakurta, A. (2020). Evading the Curse of Dimensionality in Unconstrained Private GLMs. In AISTATS 2021.</p>

<p>[XT07] Xiao X, Tao Y (2007) M-invariance: towards privacy preserving re-publication of dynamic datasets. In: SIGMOD conference, Beijing, China, pp 689–700</p>

<p>[YEO21] Yala, A., Esfahanizadeh, H., Oliveira, R. G. D., Duffy, K. R., Ghobadi, M., Jaakkola, T. S., … &amp; Medard, M. (2021). NeuraCrypt: Hiding Private Health Data via Random Neural Networks for Public Training. arXiv preprint arXiv:2106.02484.</p>

<p>[ZHS19] Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: A theoretical justification for adaptivity. In International Conference on Learning Representations, 2019.</p></div>







<p class="date">
by Abhradeep Thakurta <a href="https://differentialprivacy.org/how-to-deploy-ml-with-dp/"><span class="datestr">at October 25, 2021 06:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-8890204.post-8628666819453099327">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/mitzenmacher.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://mybiasedcoin.blogspot.com/2021/10/how-to-send-real-number-using-single.html">How to send a real number using a single bit (and some shared randomness)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In this post, we'll look at the natural problem of how to communicate an estimate of a real value in [0,1], using just 1 bit.  The post is based on <a href="https://arxiv.org/abs/2010.02331" target="_blank">this paper</a> (by Ran Ben-Basat of UCL and Shay Vargaftik of VMware Research and myself -- they helped also with the post) that appeared in ICALP last year. </p><p>This question is motivated by various aggregation problems;  multiple sending devices may measure a value, and wish to send the value to an aggregator who will compute something from the received values, such as the average.  In our problem, the senders have a real value x in [0,1] to send, but are constrained to send just a single bit.  Variations of this problem have come up in recent work on distributed/federated learning, where clients compute a gradient vector and send it to a centralized parameter server to update a learning model;  we may want to compress the vector to a small number of bits, or even 1 bit, per coordinate.  (We'll have more to say on the federated learning problem in a future post.)  Of course, it's also just an interesting randomized algorithm problem that seems interesting in its own right.  </p><p>A natural way to look at the problem is as a variation on rounding.  Given a value x in [0,1], one natural approach if limited to one bit is to deterministically round it to X.  But what should the receiver do when they receive the (rounded) bit value X?  It depends on what one's optimization goal is, but to minimize the maximum possible error, the receiver should have their estimate x' take on the value 3/4 when X is 1, 1/4 otherwise.  Note though that deterministic rounding this way is biased -- the expectation E[x'] does not equal x.  Randomized rounding, where the sender sends 1 with probability x and 0 otherwise, and the receiver uses the received bit X as the estimate x', has the property that E[x'] = x.  Unbiased estimators are arguably more natural for many estimation problems.  Here the measure of performance would be the maximum variance for the estimate over all inputs x, so for randomized rounding the cost is 1/4 (when x = 1/2).  </p><p>Can one do better than these schemes?  It turns out that you can, if you have available shared randomness.  An approach that has been known in the engineering world (where it has been used in signal processing) is subtractive dithering:  </p><p>We assume that the sender and receiver have access to shared randomness ℎ∼𝑈[−1/2,1/2].  Given a value x, the sender sends 1 if x+h≥1/2, 0 otherwise.  The receiver estimates x' = X - h.  We leave as an exercise that this is unbiased, which can be shown by deriving the stronger fact that x' is distributed as 𝑈[𝑥−1/2,𝑥+1/2] , and thus Var[𝑥']=1/12.</p><p>Subtractive dithering ignores that generating a shared real number may be more costly or problematic than generating a finite number of shared bits.  So one of the results of our paper is developing a "finite shared random bits" unbiased estimator, that corresponds to randomized rounding with no shared bits and converges to subtractive dithering as the number of shared random bits goes to infinity.  (The approach does allow for generating a private random real value.)  </p><p>Also in our paper, we study biased schemes, aiming to minimize the worst-case expected mean-squared error (where the expectation is over randomness used in the algorithm).  For example, it's very odd in the setting of subtractive dithering that one can obtain estimates smaller than 0 or greater than 1, when the input is restricted to [0,1], but that's a price we pay for having an unbiased estimator.  For  a biased estimator, you might naturally truncate the result from subtractive dithering;  by truncating to [z,1-z] for an appropriate z &gt; 0, you can indeed slightly improve over the worst-case mean-squared error of 1/16 for deterministic rounding.</p><p>We then studied various algorithmic improvements to obtain better biased schemes.  We were able to progress by looking at limited shared randomness, namely finding the best algorithm with s shared bits.  For example, consider the case of just 1 shared random bit h in {0,1}.  The receiver receives 1 bit X from the sender, and thus can have four possible estimates x' depending on X and h.  If the 4 possible estimate values are v0, v1, v2, v3 (all between 0 and 1), then it is possible to show the largest possible expected squared error occurs at one of the five inputs 0, 1, (v0+v1)/2, (v1+v2)/2, (v2+v3)/2.   We can then write a quadratic program to find the values that minimizes the worst-case expected squared error.  The end result is the following rounding algorithm:  given 1 shared random bit h in {0,1} and the value x, let X = 1 if x ≥ 0.4 + 0.2h, and 0 otherwise;  then let the estimate x' = 0.1 + 0.2h + 0.6X.  This has a worst-case expected mean-squared error of 1/20, beating deterministic rounding and truncated subtractive dithering.  Using some additional arguments we can handle more shared random bits;  at 8 bits we improve the worst-case expected squared error to about 0.04599, which is quite close to our lower bound of about 0.0459, and this is better than anything we could come up with analytically.  The optimal solution is still not known (an open question for future work!).  </p><p>Overall there are many variants of the rounding problem, and few tight bounds currently.  So even for simple-seeming problems like rounding, there are still interesting things to do.  </p></div>







<p class="date">
by Michael Mitzenmacher (noreply@blogger.com) <a href="http://mybiasedcoin.blogspot.com/2021/10/how-to-send-real-number-using-single.html"><span class="datestr">at October 25, 2021 03:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=936">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2021/10/25/fibonacci-and-i/">Fibonacci and I</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<figure class="wp-block-image size-large"><a href="https://emanueleviola.files.wordpress.com/2021/10/dsc01614.jpg"><img src="https://emanueleviola.files.wordpress.com/2021/10/dsc01614.jpg?w=768" alt="" class="wp-image-945" /></a></figure>



<p>The other day I couldn’t remember Fibonacci’s original motivation/presentation of the sequence now famously named after him.  This had to be corrected immediately, because of the picture above and my <a href="https://www.ccs.neu.edu/home/viola/papers/amigaMagazine.pdf">first publication</a> (1994) which includes a simple algorithm to decompress sounds.  The compression algorithm works by storing rather than the sound data — think of it as the key — the difference between consecutive keys.  The saving comes from not allowing every possible difference, but only those in… the Fibonacci sequence.  Why those differences are the right ones is part of the mystique which makes studying the sequence fun.  For further technical but not mystical details see the paper; an implementation of the decompressor is given in the Motorola 68000 assembly code.</p>



<p>This is me on my way to Fibonacci from Rome, some years ago:</p>



<figure class="wp-block-image size-large"><a href="https://emanueleviola.files.wordpress.com/2021/10/dsc01563-1.jpg"><img src="https://emanueleviola.files.wordpress.com/2021/10/dsc01563-1.jpg?w=1024" alt="" class="wp-image-947" /></a></figure>



<p>I actually find some presentations of the sequence a little hard to grasp, so I came up with a trivially different rendering which now will make it impossible for me to forget:</p>



<p>There are two types of trees: Young and old.  You start with one young tree. In one period, a young tree produces another young tree and becomes old, and an old tree produces a young tree and dies. How many young trees are there after t periods?</p>



<p></p>



<figure class="wp-block-table"><table><tbody><tr><td>Period</td><td>Young trees</td><td>Old trees</td></tr><tr><td>1</td><td>1</td><td>0</td></tr><tr><td>2</td><td>1</td><td>1</td></tr><tr><td>3</td><td>2</td><td>1</td></tr><tr><td>4</td><td>3</td><td>2</td></tr><tr><td>5</td><td>5</td><td>3</td></tr><tr><td>6</td><td>8</td><td>5</td></tr></tbody></table></figure>



<p>I also couldn’t exactly remember the spiral you can make with these numbers.  But you can tile the plane with squares whose sides come from the sequence, if you arrange them in a spiral.</p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2021/10/25/fibonacci-and-i/"><span class="datestr">at October 25, 2021 02:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8402768071991338392">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2021/10/squaring-circle-is-mentioned-in-gilbert.html">Squaring the circle is mentioned in a Gilbert and Sullivan comic Opera.</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>The problem of <i>squaring the circle</i>: Given a circle, construct (with straightedge and compass) a square with the same area. While browsing the web for more information on this problem (for the blog entry on problems that might be similar to P vs NP: <a href="https://blog.computationalcomplexity.org/2021/10/is-math-ready-for-pnp-is-alexandra.html">here</a>)  I came across the following:</p><p>In the Gilbert and Sullivan comic opera <i>Princess Ida</i>, in the song <i>Gently, Gently</i>  is the line:</p><p>                                <i>    ... and the circle they will square it one fine day.</i></p><p>(To hear the song see <a href="https://www.youtube.com/watch?v=1r2hEcDmeIY">here</a>. The line is towards the end.) </p><p>They lyrics are <a href="https://www.gsarchive.net/princess_ida/webop/pi_12.html">here</a>. That website begins  <i>gsarchive.ne</i>t which made me wonder<i> Did I at one time set</i> u<i>p a website of math refs in Gilbert and Sullivan plays (gsarch is very close to gasarch) ? </i>which IS the kind of thing I would do. The answer is no: <i> gsarch</i> stands for <i>Gilbert and Sullivan archive.</i> They could have called it <i>gasarch </i>if they used the <i>and </i>in <i>Gilbert and Sullivan</i> but abbreviated <i>archive </i>as arch. Then I would have been far more confused. </p><p>Moving on...</p><p>In 1884<i> Princess Ida</i> opened in 1884. For more on this comic opera see <a href="https://en.wikipedia.org/wiki/Princess_Ida">here</a>.</p><p>In 1882 pi was proven  transcendental and hence one cannot square the circle. For more on pi being transcendental see <a href="https://en.wikipedia.org/wiki/Squaring_the_circle">here</a>.</p><p>Kolmogorov Random Thoughts on all of this</p><p>0) The song is sung my three men who are making fun of the notion of a women's college. The song is about all the things the women are trying to do that are absurd such as squaring the circle. They also mention perpetual motion machines. </p><p>1) Did G and S know that the squaring the circle had been proven impossible, or just that it was thought to be impossible, or just that it was thought to be hard?</p><p>2) Was it known that perpetual motion machines were impossible? Or just very hard? </p><p>3) G and S used Mathematics in at least one other song: <i> I am the very model of a modern major general, </i>from<i> The Pirates of Penzance  </i>has the lines:</p><p><br /></p><p>                                       <i>I'm very well acquainted too with matters mathematical</i></p><p><i>                                       I understand equations, both the simple and quadratical,</i></p><p><i>                                       About binomial theorems I'm teeming with the a lot o' news---</i></p><p><i>                                       With many cheerful facts about the square of the hypotenuse</i></p><p><br /></p><p>and later </p><p><i>                                        I'm very good at integral and differential calculus</i></p><p>See <a href="https://naic.edu/~gibson/poems/gilbert1.html">here</a> for all the lyrics. The website mentioned in the next point has a pointer to a YouTube video of people singing it. </p><p>4) There are many parodies of <i>Modern Major General. </i>The earliest ones I know of is Tom Lehrer's  <i>The Elements. </i>Since making a website of them IS the kind of thing I would do,  while writing this post I did it (Are we compelled to do things that fit our image of ourselves? Yup.) The website is <a href="http://www.cs.umd.edu/~gasarch/FUN//modmajgen.html">here</a>. It has 36 parodies (as of Oct 17, 2021 when I wrote this blog--- it may have more if you read this later.) That may seem like a lot, but it pales in comparison  to the most satirized song of all time: <i>The 12 days of Christmas </i>which I did an ugly lyrics-only website for back before html had nice tools, see <a href="http://www.cs.umd.edu/~gasarch/12days.html">here</a>. It has 143 songs on it but I am sure there are many more. (Note to self: redo that website when you have time. Maybe when I retire.) </p><p>4) I suspect that G and S knew more math, or perhaps knew of more math,  than Broadway composers know now. I suspect this is a more general trend: people are more specialized now. Having said that, I need to mention the off-Broadway musical <a href="https://en.wikipedia.org/wiki/Fermat%27s_Last_Tango">Fermat's last Tango</a> which I liked more than Lance (see his post on it <a href="https://blog.computationalcomplexity.org/2004/02/fermats-last-tango.html">here</a>). </p><p>5) How much math would you need to know in order to insert some into your play or movie? With Wikipedia and other web sources you could find out some things, but you would have to have some idea what you are looking for. And perhaps you would need some math background in order to even want to insert some math into your work in the first place. </p><p>6)  Here's hoping someone will make a musical about William Rowan Hamilton using this song <a href="https://www.youtube.com/watch?v=SZXHoWwBcDc">here</a> as a starting point. I blogged rather optimistically about that possibility <a href="https://blog.computationalcomplexity.org/2017/04/william-rowan-hamilton-musical.html#comment-form">here</a>.</p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2021/10/squaring-circle-is-mentioned-in-gilbert.html"><span class="datestr">at October 24, 2021 06:51 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/10/24/new-computational-geometry">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/10/24/new-computational-geometry.html">New computational geometry journal</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Over the past year or so I’ve been working with Marc van Kreveld and Wolfgang Mulzer to set up a new <a href="https://en.wikipedia.org/wiki/Diamond_open_access">diamond open access</a> computational geometry journal, <em><a href="https://www.cgt-journal.org/index.php/cgt">Computing in Geometry and Topology</a></em>, sponsored by the <a href="https://www.computational-geometry.org/">Society for Computational Geometry</a>, the organization set up to run the annual <em>Symposium on Computational Geometry</em>. It is the second such journal to be established, after the <em><a href="https://jocg.org/index.php/jocg">Journal of Computational Geometry</a></em>. This week the new journal went live and it is now available for submissions. Below is the official announcement we sent out to several mailing lists:</p>

<hr />

<p>Dear all,</p>

<p>As of this week, the new diamond open access journal</p>

<p style="text-align: center;"><em>Computing in Geometry and Topology</em></p>

<p>is welcoming submissions. The website of the journal is <a href="https://www.cgt-journal.org/index.php/cgt">https://www.cgt-journal.org/index.php/cgt</a></p>

<p>Here you can also find the editorial board, the submission guidelines, the scope, and a style file.</p>

<p>If you have any questions about the journal, please send them to info@cgt-journal.org</p>

<p>Best regards,
David Eppstein, Marc van Kreveld, and Wolfgang Mulzer</p>

<hr />

<p><strong>Purpose and scope</strong></p>

<p><em>Computing in Geometry and Topology</em> aims to support the broader computational geometry and topology community by being a peer-reviewed scientific journal that provides diamond open access. <em>Computing in Geometry and Topology</em> is sponsored by the Society for Computational Geometry.</p>

<p>With the broader computational geometry and topology community, we include researchers in discrete and combinatorial geometry, and any application area of computational geometry and topology. We also include algorithm engineering for geometric computations.</p>

<p>The journal publishes two types of papers. Firstly, the journal publishes original research of sufficient depth and interest. Secondly, the journal publishes high-quality survey papers. Every paper has been thoroughly reviewed by experts in the area.</p>

<p>To emphasize the breadth of the interpretation of computational geometry and topology, the editorial board has different sections that represent the algorithmic and mathematical aspects, the applied aspects, and the engineering aspects.</p>

<p><a href="https://www.cgt-journal.org/index.php/cgt">https://www.cgt-journal.org/index.php/cgt</a></p>

<hr />

<p>(<a href="https://mathstodon.xyz/@11011110/107159503144164449">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/10/24/new-computational-geometry.html"><span class="datestr">at October 24, 2021 05:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/147">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/147">TR21-147 |  Extractors for Sum of Two Sources | 

	Jyun-Jie Liao, 

	Eshan Chattopadhyay</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We consider the problem of extracting randomness from \textit{sumset sources}, a general class of weak sources introduced by Chattopadhyay and Li (STOC, 2016). An $(n,k,C)$-sumset source $\mathbf{X}$ is a distribution on $\{0,1\}^n$ of the form $\mathbf{X}_1 + \mathbf{X}_2 + \ldots + \mathbf{X}_C$, where $\mathbf{X}_i$'s are independent sources on $n$ bits with min-entropy at least $k$. Prior extractors either required the number of sources $C$ to be a large constant or the min-entropy $k$ to be at least $0.51 n$. 

As our main result, we construct an explicit extractor for sumset sources in the setting of $C=2$ for min-entropy $\mathrm{poly}(\log n)$ and polynomially small error. We can further improve the min-entropy requirement  to $(\log n) \cdot (\log \log n)^{1 + o(1)}$ at the expense of worse error parameter of our extractor. We find applications of our sumset extractor for extracting randomness from other well-studied models of weak sources such as affine sources, small-space sources,  and interleaved sources.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/147"><span class="datestr">at October 24, 2021 11:46 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
