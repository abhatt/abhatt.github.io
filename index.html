<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="http://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc ‚Äì QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="http://www.minimizingregret.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="no data">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science ‚Äì Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" class="message" title="internal server error">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I‚Äôm a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://scottaaronson.blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Caf√©: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at April 10, 2022 12:21 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://thmatters.wordpress.com/?p=1368">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sigact.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://thmatters.wordpress.com/2022/04/09/theoryfest-2022-registration-now-open-and-travel-grant-applications-due-soon/">TheoryFest 2022: Registration now open and travel grant applications due soon!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><strong>Call for Participation¬†</strong></p>



<p><a href="http://acm-stoc.org/stoc2022/" target="_blank" rel="noreferrer noopener"><strong>54th ACM Symposium on Theory of Computing (STOC 2022) ‚Äì Theory Fest¬†</strong></a></p>



<p><strong>June 20-24, 2022¬†</strong></p>



<p><strong>Rome, Italy¬†</strong></p>



<p>The 54th ACM Symposium on Theory of Computing (STOC 2022) is sponsored by the ACM Special Interest Group on Algorithms and Computation Theory and will be held in Rome, Italy, Monday June 20 ‚Äì Friday, June 24, 2022.</p>



<p>STOC 2022 ‚Äì Theory Fest will feature technical talk sessions, <a href="http://acm-stoc.org/stoc2022/workshops.html" target="_blank" rel="noreferrer noopener">6 workshops</a> with introductory tutorials, poster sessions, social events, and a special joint session with ‚Äú<a href="https://www.lincei.it/en" target="_blank" rel="noreferrer noopener">Accademia Nazionale dei Lincei</a>‚Äù, the oldest and most prestigious Italian academic institution, followed by a reception and a concert at the <a href="https://www.lincei.it/en/corsini-palace" target="_blank" rel="noreferrer noopener">Academy historic site</a>.¬†</p>



<p><strong>Registration</strong></p>



<p>STOC 2022 registration is available <a href="http://acm-stoc.org/stoc2022/registration.html" target="_blank" rel="noreferrer noopener">here</a>.</p>



<p><strong>Early registration deadline: April 30th.¬†</strong></p>



<p><strong>Student Travel Grants¬†</strong></p>



<p>Information for student travel grant applications is available <a href="http://acm-stoc.org/stoc2022/travel-support.html" target="_blank" rel="noreferrer noopener">here</a>.¬†</p>



<p><strong>Application deadline: April 20th.</strong></p>



<p>STOC 2022 is sponsored by Algorand, Amazon, Apple, Google, IOHK, Microsoft, Sapienza University of Rome.¬†</p></div>







<p class="date">
by shuchic <a href="https://thmatters.wordpress.com/2022/04/09/theoryfest-2022-registration-now-open-and-travel-grant-applications-due-soon/"><span class="datestr">at April 09, 2022 02:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4616">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2022/04/08/renato-capocelli-1940-1992/">Renato Capocelli (1940-1992)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Thirty years ago, I was in the middle of the second semester of my third year of undergrad, and one of the courses that I was enrolled in was on information theory. I was majoring in computer science, a major that had just been established at Sapienza University when I signed up for it in 1989, <s>organized by a computer science department that had also just been established in 1989</s>. A computer science department was established in 1991.</p>



<p>The new Sapienza computer science department was founded mostly by faculty from the Sapienza mathematics department, plus a number of people that came from other places to help start it. Among the latter, Renato Capocelli had moved to Rome from the University of Salerno, where he had been department chair of computer science.</p>



<p>Capocelli worked on combinatorics and information theory. In the early 90s, he had also become interested in the then-new area of zero-knowledge proofs.</p>



<p>Capocelli taught the information-theory course that I was attending, and it was a very different experience from the classes I had attended up to that point. To get the new major started, several professors were teaching classes outside their area, sticking close to their notes. Those teaching mathematical classes, were experts but were not deviating from the definition-theorem-proof script. Capocelli had an infectious passion for his subject, took his time to make us gain an intuitive understanding of the concepts of information theory, was full of examples and anecdotes, and  always emphasized the high-level idea of the proofs.</p>



<p>I subsequently met several other charismatic and inspiring computer scientists and mathematicians, though Capocelli had a very different personality from most of them. He was like an earlier generation of Southern Italian intellectuals, who could be passionate about their subject in a peculiarly non-nerdy way, loving it the way one may love food, people, nature, or a full life in general.</p>



<p>On April 8, 1992, Renato Capocelli <a href="https://www.di.uniroma1.it/sites/default/files/dipartimento/capocelli.pdf">died suddenly and unexpectedly</a>, though his memory lives on in the many people he inspired. The Computer Science department of the University of Salerno was named after him for a period of time.</p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2022/04/08/renato-capocelli-1940-1992/"><span class="datestr">at April 08, 2022 03:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/04/06/summer-visiting-researcher-position-at-boston-college-computer-science-bentos-lab-apply-by-may-31-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/04/06/summer-visiting-researcher-position-at-boston-college-computer-science-bentos-lab-apply-by-may-31-2022/">Summer visiting researcher position at Boston College, Computer Science, Bento‚Äôs Lab (apply by May 31, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We invite applications for multiple visiting scholar positions in any of the following areas:</p>
<p>* parallel and distributed algorithms/optimization, convex and non-convex optimization</p>
<p>* machine learning, graphical models, information theory</p>
<p>* network theory and graph matching</p>
<p>* deep learning, robustness in machine learning</p>
<p>Start and end dates flexible. Duration up to 4 months.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/21519">https://academicjobsonline.org/ajo/jobs/21519</a><br />
Email: visiting@jbento.info</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/04/06/summer-visiting-researcher-position-at-boston-college-computer-science-bentos-lab-apply-by-may-31-2022/"><span class="datestr">at April 06, 2022 04:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/049">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/049">TR22-049 |  Non-Adaptive Universal One-Way Hash Functions from Arbitrary One-Way Functions | 

	Noam Mazor, 

	Jiapeng Zhang, 

	Xinyu Mao</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Two of the most useful cryptographic primitives that can be constructed from one-way functions are pseudorandom generators (PRGs) and universal one-way hash functions (UOWHFs). The three major efficiency measures of these primitives are: seed length, number of calls to the one-way function, and adaptivity of these calls. Although a long and successful line of research studied these primitives, their optimal efficiency is not yet fully understood: there are gaps between the known upper bound and the known lower bound for black-box constructions.

Interstingly, the first construction of PRGs by H ?astad, Impagliazzo, Levin, and Luby [SICOMP ‚Äô99], and the UOWHFs construction by Rompel [STOC ‚Äô90] shared a similar structure. Since then, there was an improvement in the efficiency of both constructions: The state of the art construction of PRGs by Haitner, Reingold, and Vadhan [STOC ‚Äô10] uses $O(n^4)$ bits of random seed and $O(n^3)$ non-adaptive calls to the one-way function, or alternatively, seed of size $O(n^3)$ with $O(n^3)$ adaptive calls (Vadhan and Zheng [STOC ‚Äô12]). Constructing a UOWHF with similar parameters is still an open question. Currently, the best UOWHF construction by Haitner, Holenstein, Reingold, Vadhan, and Wee [Eurocrypt ‚Äô10] uses $O(n^{13})$ adaptive calls with a key of size $O(n^5)$.

In this work we give the first non-adaptive construction of UOWHFs from arbitrary one-way functions. Our construction uses $O(n^9)$ calls to the one-way function, and key of length $O(n^{10})$. By the result of Applebaum, Ishai, and Kushilevitz [FOCS ‚Äô04], the above implies the existence of UOWHFs in NC0, given the existence of one-way functions in NC1. We also show that the PRG construction of Haitner et al., with small modifications, yields a relaxed notion of UOWHFs. In order to analyze this construction, we introduce the notion of next-bit unreachable entropy, which replaces the next-bit pseudoentropy notion, used in the PRG construction above.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/049"><span class="datestr">at April 06, 2022 06:51 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8659181144938614989">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/04/masks.html">Masks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Illinois Tech removed the last of their mandatory masking restrictions yesterday. Chicago had zero Covid deaths. Yet I still get messages like this in my twitter feed.</p><blockquote class="twitter-tweet"><p lang="en" dir="ltr">I don't care who you are. If I see you without a mask indoors when infections are rampant and there are many still at risk, I will see you as ignorant of science, narcissistic, and uncaring of the vulnerable around you. I will truly see you unmasked for what you are.</p>‚Äî Bill Comeau üá®üá¶üá∫üá¶ (@Billius27) <a href="https://twitter.com/Billius27/status/1510714029072912390?ref_src=twsrc%5Etfw">April 3, 2022</a></blockquote> 
<p></p><p>The science is unequivocal for vaccines, which do a good job preventing infection and a strong job saving lives. I just got my second booster on Sunday.</p><p>Masks give you some protection but nothing like the vaccines. It's impossible to completely remove the risk of Covid so people need to make their own choices and tradeoffs. If you are vaccinated your chance of serious illness is tiny, whether or not your wear a mask. And mask wearing is not cost-free.</p><p>I just don't like wearing masks. Wearing a mask bends my ears and is mildly painful. People can't always understand me when I talk through a mask, and they can't read my facial expressions. People and computers don't recognize me in a mask. Masks fog up my glasses. I can't exercise with a mask, it gets wet with sweat and hard to breath. You can't eat or drink wearing a mask.</p><p>Now everyone has their own tolerance and I respect that. I'll wear a mask if someone asks nicely or if it is required, like on public transit and many theaters. If I have a meeting with someone wearing a mask, I'll ask if they would like me to put mine on. In most cases they remove theirs.</p><p>On the other hand, the Chicago Symphony concert I planned to attend tonight was cancelled because the conductor, Riccardo Muti, <a href="https://chicago.suntimes.com/2022/4/4/23010828/riccardo-muti-tests-positive-for-covid-chicago-symphony-orchestra-concert-canceled">tested positive for Covid</a> (with minor symptoms). For my own selfish reasons, I wish he had worn a mask.</p><div style="clear: both; text-align: center;" class="separator"><a style="margin-left: 1em; margin-right: 1em;" href="https://cst.brightspotcdn.com/dims4/default/d637be8/2147483647/strip/true/crop/4000x2667+0+1/resize/840x560!/format/webp/quality/90/?url=https%3A%2F%2Fcdn.vox-cdn.com%2Fthumbor%2Fe9_J_DxUKtnWan4c8MigVZf8vBM%3D%2F0x0%3A4000x2668%2F4000x2668%2Ffilters%3Afocal%282000x1334%3A2001x1335%29%2Fcdn.vox-cdn.com%2Fuploads%2Fchorus_asset%2Ffile%2F19129683%2FCSO190509_160f_Riccardo_Muti_and_Chicago_Symphony_Orchestra_May_2019.jpg"><img width="320" src="https://cst.brightspotcdn.com/dims4/default/d637be8/2147483647/strip/true/crop/4000x2667+0+1/resize/840x560!/format/webp/quality/90/?url=https%3A%2F%2Fcdn.vox-cdn.com%2Fthumbor%2Fe9_J_DxUKtnWan4c8MigVZf8vBM%3D%2F0x0%3A4000x2668%2F4000x2668%2Ffilters%3Afocal%282000x1334%3A2001x1335%29%2Fcdn.vox-cdn.com%2Fuploads%2Fchorus_asset%2Ffile%2F19129683%2FCSO190509_160f_Riccardo_Muti_and_Chicago_Symphony_Orchestra_May_2019.jpg" border="0" height="213" /></a></div><br /><p><br /></p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/04/masks.html"><span class="datestr">at April 05, 2022 01:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=806">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2022/04/05/lichess/">Lichess</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>After my <a href="https://emanueleviola.wordpress.com/2021/07/28/chess-com-55-1000/">previous post, </a>a reader (who amazingly has a nearly identical playing routine) pointed me to it.  It‚Äôs great!  Much better than chess.com.</p>



<p>‚Äú<em>No ads, no subscriptions; but open-source and passion.</em>‚Äú</p>



<p>‚Äú<em>Are some features reserved to Patrons? No, because Lichess is entirely free, forever, and for everyone. That‚Äôs a promise.</em>‚Äú</p>



<p>It doesn‚Äôt get better than that, in this world; and that‚Äôs why I love computers and the community surrounding them.</p>



<p>Another good news is that I got tired of 5 | 5!  The games are just too slow, and my chess isn‚Äôt even worth this much of my time.  I am now playing 3 | 2.  It‚Äôs fun the adrenaline kick I get with every match.  Also I got much stricter with my routine, and I don‚Äôt lose more than once a day.</p>



<p>Here‚Äôs my <a href="https://lichess.org/@/EAmaiLeNuvole">profile</a>.  It‚Äôs an anagram of my unpronounceable name which can be loosely translated as <em>‚Ä¶and I loved the clouds.</em></p>



<p><strong>IMPORTANT</strong>: Excessive activity and blunders merely reflect my generous sharing of credentials.</p>



<p>And then I thought, why don‚Äôt we automatically analyze existing games to find interesting situations, and present them to the user as a puzzle?  Naturally, the beauty ‚Äî and dismay ‚Äî of living in the AI (after-internet) era is that‚Ä¶ <a href="https://chesstempo.com/chess-tactics/">it‚Äôs already done</a>.</p>



<p>OK, time for my daily fix.</p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2022/04/05/lichess/"><span class="datestr">at April 05, 2022 10:07 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/048">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/048">TR22-048 |  On the Range Avoidance Problem for Circuits | 

	Hanlin Ren, 

	Rahul Santhanam, 

	Zhikun Wang</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We consider the range avoidance problem (called Avoid): given the description of a circuit $C:\{0, 1\}^n \to \{0, 1\}^\ell$ (where $\ell &gt; n$), find a string $y\in\{0, 1\}^\ell$ that is not in the range of $C$. This problem is complete for the class APEPP that corresponds to explicit constructions of objects whose existence follows from the probabilistic method (Korten, FOCS 2021). 
    
Motivated by applications in explicit constructions and complexity theory, we initiate the study of the range avoidance problem for weak circuit classes, and obtain the following results:
    
    
1. Generalising Williams's connections between circuit-analysis algorithms and circuit lower bounds (J. ACM 2014), we present a framework for solving C-Avoid in $FP^{NP}$ using circuit-analysis data structures for C, for "typical" multi-output circuit classes C. As an application, we present a non-trivial $FP^{NP}$ range avoidance algorithm for De Morgan formulas.
An important technical ingredient is a construction of rectangular PCPs of proximity, building on the rectangular PCPs by Bhangale, Harsha, Paradise, and Tal (FOCS 2020).
        
2. Using the above framework, we show that circuit lower bounds for $E^{NP}$ are equivalent to circuit-analysis algorithms with $E^{NP}$ preprocessing. This is the first equivalence result regarding circuit lower bounds for $E^{NP}$. Our equivalences have the additional advantages that they work in both infinitely-often and almost-everywhere settings, and that they also hold for larger (e.g., subexponential) size bounds.

3. Complementing the above results, we show that in some settings, solving C-Avoid would imply breakthrough lower bounds, even for very weak circuit classes C. In particular, an algorithm for $AC^0$-Avoid with polynomial stretch (i.e., $\ell = poly(n)$) implies lower bounds against $NC^1$, and an algorithm for $NC^0_4$-Avoid with very small stretch (i.e., $\ell = n + n^{o(1)}$) implies lower bounds against $NC^1$ and branching programs.

4. We show that Avoid is in FNP if and only if there is a propositional proof system that breaks every non-uniform proof complexity generator. This result connects the study of range avoidance with fundamental questions in proof complexity.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/048"><span class="datestr">at April 04, 2022 08:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/047">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/047">TR22-047 |  Linear Hashing with $\ell_\infty$ guarantees and two-sided Kakeya bounds | 

	Manik Dhar, 

	Zeev Dvir</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that a randomly chosen linear map over a finite field gives a good hash function in the $\ell_\infty$ sense.  More concretely, consider a set $S \subset \mathbb{F}_q^n$ and a randomly chosen linear map $L :  \mathbb{F}_q^n \to  \mathbb{F}_q^t$ with $q^t$ taken to be sufficiently smaller than $|S|$. Let $U_S$ denote a random variable distributed uniformly on $S$. Our main theorem shows that, with high probability over the choice of $L$, the random variable  $L(U_S)$ is close to uniform in the $\ell_\infty$ norm.  In other words, every element in the range $\mathbb{F}_q^t$ has about the same number of elements in $S$ mapped to it. This complements the widely-used Leftover Hash Lemma (LHL) which proves the analog statement under the statistical, or $\ell_1$, distance  (for a richer class of functions) as well as prior work on the expected largest 'bucket size' in linear hash functions  [ADMPT99].  Our proof leverages a connection between linear hashing and the finite field Kakeya problem and extends some of the tools developed in this area, in particular the polynomial method.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/047"><span class="datestr">at April 04, 2022 07:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/046">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/046">TR22-046 |  Automating OBDD proofs is NP-hard | 

	Artur Riazanov, 

	Dmitry Itsykson</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove that the proof system OBDD(and, weakening) is not automatable unless P = NP. The proof is based upon the celebrated result of Atserias and Muller [FOCS 2019] about the hardness of automatability for resolution. The heart of the proof is lifting with a multi-output indexing gadget from resolution block-width to dag-like multiparty number-in-hand communication protocol size with $o(n)$ parties, where $n$ is the number of variables in the non-lifted formula. A similar lifting theorem for protocols with $n + 1$ participants was proved by G√∂√∂s et. el. [STOC 2020] to establish the hardness of automatability result for Cutting Planes.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/046"><span class="datestr">at April 04, 2022 03:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/045">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/045">TR22-045 |  Relaxed Locally Decodable and Correctable Codes: Beyond Tensoring | 

	Gil Cohen, 

	Tal Yankovitz</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In their highly influential paper, Ben-Sasson, Goldreich, Harsha, Sudan, and Vadhan (STOC 2004) introduced the notion of a relaxed locally decodable code (RLDC). Similarly to a locally decodable code (Katz-Trevisan; STOC 2000), the former admits access to any desired message symbol with only a few queries to a possibly corrupted codeword. An RLDC, however, is allowed to abort when identifying corruption. The natural analog to locally correctable codes, dubbed relaxed locally correctable codes (RLCC), was introduced by Gur, Ramnarayan and Rothblum (ITCS 2018) who constructed asymptotically-good length-$n$ RLCC and RLDC with $(\log{n})^{O(\log\log{n})}$ queries.

In this work we construct asymptotically-good RLDC and RLCC with an improved query complexity of $(\log{n})^{O(\log\log\log{n})}$. To achieve this, we devise a mechanism--an alternative to the tensor product--that squares the length of a given code. Compared to the tensor product that was used by Gur et al. and by many other constructions, our mechanism is significantly more efficient in terms of rate deterioration, allowing us to obtain our improved construction.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/045"><span class="datestr">at April 04, 2022 02:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=6837">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/information-theory-with-kernel-methods/">Information theory with kernel methods</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">In <a href="https://francisbach.com/von-neumann-entropy/">last month blog post</a>, I presented the von Neumann entropy. It is defined as a spectral function on positive semi-definite (PSD) matrices, and leads to a Bregman divergence called the <a href="https://en.wikipedia.org/wiki/Quantum_relative_entropy">von Neumann relative entropy</a> (or matrix Kullback Leibler divergence), with interesting convexity properties and applications in optimization (mirror descent, or smoothing) and probability (concentration inequalities for matrices). </p>



<p class="justify-text">This month, I will show how the relative entropy relates to usual notions of information theory when applied to specific covariance matrices or covariance operators, with potential applications to all areas where entropy plays a role, and with a simple use of quantum information theory. This post is based on a recent preprint: see all details in [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>], as well as the conclusion section below for several avenues for future work.</p>



<h2>Von Neumann relative entropy between covariance matrices</h2>



<p class="justify-text">We consider some set \(\mathcal{X}\), with a probability distribution \(p\). We assume given a feature map \(\varphi: \mathcal{X} \to \mathbb{R}^d\), from \(\mathcal{X}\) to a \(d\)-dimensional vector space. This naturally defines a (non-centered) <em>covariance matri</em>x $$ \Sigma_{p} = \int_{\mathcal{X}} \varphi(x) \varphi(x)^\top dp(x), $$ which is a \(d \times d\) symmetric PSD matrix. We will assume throughout this post that the set \(\mathcal{X}\) is compact and that the Euclidean norm of \(\varphi\) is bounded on \(\mathcal{X}\).</p>



<p class="justify-text">Given two distributions \(p\) and \(q\), and their associated covariance matrices \(\Sigma_p\) and \(\Sigma_q\), we can compute their von Neumann relative entropy as: $$ D( \Sigma_p \| \Sigma_q) = {\rm tr} \big[ \Sigma_p ( \log \Sigma_p \, ‚Äì \log \Sigma_q) \big] ‚Äì {\rm tr}\big[  \Sigma_p \big] +  {\rm tr} \big[ \Sigma_q \big] .$$</p>



<p class="justify-text">We immediately get that \(D(\Sigma_p \| \Sigma_q)\) is always non-negative, and zero if and only if \(\Sigma_p = \Sigma_q\). This property, that makes it a potentially interesting measure of dissimilarity, is shared among all Bregman divergences on covariance matrices, and has been extensively used in data science (see, e.g., [<a href="https://www.jmlr.org/papers/volume10/kulis09a/kulis09a.pdf">2</a>, <a href="https://epubs.siam.org/doi/pdf/10.1137/060649021">3</a>]). What makes the von Neumann entropy special?</p>



<p class="justify-text">One important property is that it is jointly convex in \(p\) and \(q\), but this is also shared by other divergences [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>, Appendix A.4]. What really makes the von Neumann entropy special is its explicit link with with information theory.</p>



<h2>Finite sets with orthonormal embeddings</h2>



<p class="justify-text">Let‚Äôs start with the simplest situation. If \(\mathcal{X}\) is finite with \(d\) elements, and we have an orthonormal embedding, that is, for all \(x,y \in \mathcal{X}\), \(\varphi(x)^\top \varphi(y) = 1_{x = y}\), then, after a transformation by a rotation matrix, we can assume that each \(\varphi(x)\) is one element of the canonical basis, and all covariance matrices are diagonal (without the rotation, we would have that all covariance matrices are jointly diagonalizable), and for probability distributions \(p\) and \(q\), using the same notation for their probability mass functions, we have: $$D( \Sigma_p \| \Sigma_q)  = \sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)} = D(p \|q ), $$ which is exactly the usual <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leilbler</a> (KL) divergence between the two distributions \(p\) and \(q\) [4].</p>



<p class="justify-text">Can we push this further beyond finite sets and finite-dimensional feature maps, and obtain a link with classical notions from information theory?</p>



<p class="justify-text">A first remark is the invariance by rotation within the feature space (that is, if we replace \(\varphi(x)\) by \(R \varphi(x)\), where \(R\) is an \(d \times d\) orthogonal matrix, the von Neumann entropy does not change), so that it depends only on the dot-product \(k(x,y) = \varphi(x)^\top \varphi(y)\), which is the traditional kernel function. As will see, this opens us the entire area of <a href="https://en.wikipedia.org/wiki/Kernel_method">kernel machines</a>.</p>



<p class="justify-text">A second remark is that we can go infinite-dimensional, and replace \(\mathbb{R}^d\) by any Hilbert space. This complicates a bit the exposition, and I refer to [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>] for more formal definitions. In this post, I will shamelessly go infinite-dimensional by using matrices with infinite dimensions when needed.</p>



<h2>Lower-bound on Shannon relative entropy</h2>



<p class="justify-text">We first upper-bound \(D(\Sigma_p \| \Sigma_q)\) by joint convexity of the von Neumann relative entropy (see <a href="https://francisbach.com/von-neumann-entropy/">last post</a>) and Jensen‚Äôs inequality: $$ D(\Sigma_p \| \Sigma_q) =  D \Big( \int_{\mathcal{X}}\varphi(x)¬† \varphi(x)^\top dp(x) \Big\| \int_{\mathcal{X}} \frac{dq}{dp}(x) \varphi(x) ¬† \varphi(x)^\top dp(x) \Big)$$ $$\ \ \,  \leqslant  \int_{\mathcal{X}} D \Big( \varphi(x)¬† \varphi(x)^\top \Big\| \frac{dq}{dp}(x) \varphi(x) ¬† \varphi(x)^\top \Big) dp(x).$$ The two matrices  \( \varphi(x)¬† \varphi(x)^\top\) and \(\frac{dq}{dp}(x) \varphi(x) ¬† \varphi(x)^\top \) are proportional to each other, and have \(\varphi(x)\) as the only eigenvector with a potentially non zero eigenvalue, so that we have $$ D \Big( \varphi(x)¬† \varphi(x)^\top \Big\| \frac{dq}{dp}(x) \varphi(x) ¬† \varphi(x)^\top \Big) =  \| \varphi(x) \|^2  \bigg[ \log \Big( \frac{dp}{dq}(x) \Big)  \ ‚Äì 1 +  \frac{dq}{dp}(x) \bigg].$$ Integrating with respect to \(p\) leads to cancellations: $$  D(\Sigma_p \| \Sigma_q) \leqslant \int_{\mathcal{X}}\| \varphi(x) \|^2  \log \Big( \frac{dq}{dp}(x) \Big) dp(x) \tag{1}.$$ </p>



<p class="justify-text">If we assume a unit norm bound on all features, that is, \(\| \varphi(x)\| \leqslant 1\) for all \(x \in \mathcal{X}\), we get  $$  D(\Sigma_p \| \Sigma_q) \leqslant  \int_{\mathcal{X}} \log \Big( \frac{ dp}{dq}(x) \Big) dp(x) = D(p\|q), $$ and thus we have found a lower bound on the regular Shannon relative entropy. </p>



<p class="justify-text"><strong>From relative entropy to entropy. </strong>We can also define a new notion of entropy by considering the relative entropy with respect to the uniform distribution on \(\mathcal{X}\), which we denote \(\tau\). That is, defining \(\Sigma\) the covariance matrix for the uniform distribution, we consider $$ H(\Sigma_p) = \ ‚Äì D(\Sigma_p \| \Sigma),$$ which is thus greater than (with a unit norm bound on all features) $$- \int_{\mathcal{X}}   \log \big( \frac{d p}{d \tau}(x)\big) dp(x),$$ which is essentially the <a href="https://en.wikipedia.org/wiki/Differential_entropy">differential entropy</a>.</p>



<p class="justify-text">We now look at a few simple examples, and present further properties later (such as a well-defined bound in the other direction). We consider only one-dimensional examples, but the theory extends to all dimensions and all kernels. In particular, finite large sets such as \(\{-1,1\}^d\) can be considered (see [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>]).</p>



<h2>Polynomials and moments</h2>



<p class="justify-text">We consider \(\mathcal{X} = [-1,1]\) and \(\varphi(x) \in \mathbb{R}^{r+1}\) spanning all degree \(r\) polynomials, with \(k(x,y) = \varphi(x)^\top \varphi(y)\). For these examples, \(\Sigma_p\) is composed of all classical moments of order up to \(2r\). </p>



<p class="justify-text"><strong>Regular polynomial kernel. </strong>Following traditional kernel methods, we can first consider \(\displaystyle k(x,y) = \frac{1}{2^r} ( 1 + xy)^r\), which corresponds to $$\varphi(x)_j =  \frac{1}{2^{r/2}} {{ r \choose j}\!\!}^{1/2} x^j,$$ for \(j \in \{0,\dots,r\}\). We also have \(\sup_{ x\in \mathcal{X} } k(x,x) = 1\), which allows for the Shannon relative entropy upper-bound. Let us consider two distributions \(p\) and \(q\) on \([-1,1]\), and understand if letting \(r\) go to infinity leads to a good estimation of the Shannon relative entropy. </p>



<p class="justify-text">For example, we consider the distribution \(p\) with density \(\frac{2}{\pi} \sqrt{1-x^2}\) (this is a <a href="https://en.wikipedia.org/wiki/Beta_distribution">Beta distribution</a>  affinely rescaled to \([-1,1]\) instead of \([0,1]\)), plotted below, and \(q\) the uniform distribution, with density \(1/2\). </p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img width="290" alt="" src="https://francisbach.com/wp-content/uploads/2022/03/density.png" class="wp-image-6964" height="181" /></figure></div>



<p class="justify-text">All moments are available in closed form (see end of post), and the Shannon relative entropy is equal to \(D(p\|q) = \int_{-1}^1 \! \frac{2}{\pi} \sqrt{1-x^2} \log \frac{4}{\pi} \sqrt{1-x^2} dx \approx 0.0484\). We can then compute for various values of \(r\), \(D(\Sigma_p \| \Sigma_q)\), which is plotted below.</p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img width="289" alt="" src="https://francisbach.com/wp-content/uploads/2022/03/reg_pol_kernel-2.png" class="wp-image-6937" height="269" /></figure></div>



<p class="justify-text">We should hope that the approximation improves as we use more monomials, but this is not the case. As common in kernel methods, the standard polynomial kernel does not have a good behavior. This is partly due to the fact that \(k(x,x) = \big( \frac{1+x^2}{2} \big)^r\) is far from uniform on \([-1,1]\).</p>



<p>Let‚Äôs now consider a better polynomial kernel.</p>



<p class="justify-text"><strong>Christofell-Darboux kernels. </strong>Other polynomial bases could be used as well beyond \(1, X, \dots, X^r\), in particular any basis of polynomials with increasing degrees \(P_0, \dots, P_r\) can be used, in particular polynomials that are <a href="https://en.wikipedia.org/wiki/Orthogonal_polynomials">orthogonal</a> with respect to a probability distribution on \([-1,1]\), with $$ \varphi(x)_j = \frac{1}{\sqrt{\alpha_r}} P_j(x) \mbox{ and } k(x,y) = \frac{1}{\alpha_r} \sum_{j=0}^r P_j(x) P_j(y),$$ where \(\alpha_r\) is here to ensure that \(k(x,x) \leqslant 1\) on \([-1,1]\). The kernel function \(k(x,y)\) has then itself a simple expression through the <a href="https://en.wikipedia.org/wiki/Christoffel%E2%80%93Darboux_formula">Christoffel‚ÄìDarboux formula</a>, as $$ k(x,y) \propto   \frac{P_r(x)P_{r+1}(y)-P_r(y)P_{r+1}(x)}{x-y},$$ with an explicit proportionality constant. For more details on these kernels and their application to data analysis, see [<a href="https://hal.archives-ouvertes.fr/hal-01845137v3/document">5</a>].</p>



<p class="justify-text">For example, for <a href="https://francisbach.com/chebyshev-polynomials/">Chebyshev polynomials</a>, which are orthonormal with respect to the probability measure with density \(\big(\pi \sqrt{1-x^2}\big)^{-1}\) on \([-1,1]\), we have \(P_0 = 1\), \(P_1 =  \sqrt{2} X\), and \(P_2 = \sqrt{2}( 2X^2 ‚Äì 1) \), and more generally, for \(k&gt;0\), \(P_k(\cos \theta) = \sqrt{2}\cos k\theta\). One can then check that for \(x = \cos \theta\) and \(y = \cos \eta\), we have $$k(x,y) = \frac{1}{2} \Big( \frac{ \sin ( 2r+1)( \theta-\eta)/2}{\sin ( \theta-\eta)/2} + \frac{ \sin ( 2r+1)( \theta+\eta)/2}{\sin ( \theta+\eta)/2}  \Big).$$ Moreover, we can check that $$ \sup_{x \in \, [-1,1]} k(x,x) = 2r+1, $$ leading to \(\alpha_r = 2r+1\).</p>



<p class="justify-text"> </p>



<p class="justify-text">As shown below, after computing (still in closed form, see end of the post) \(\Sigma_p\) and \(\Sigma_q\), we can plot the relative entropy, and compare it to the true one. This leads to an improved estimation.</p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img width="293" alt="" src="https://francisbach.com/wp-content/uploads/2022/03/cheb_pol_kernel-2.png" class="wp-image-6936" height="273" /></figure></div>



<p class="justify-text">This is better, but still not quite there, and we get in fact convergence to exactly one half of the true relative entropy. The motivated reader will check that the limit of \(k(x,x)\) when \(r \) tends to infinity is \(1/2\) for all \(x \in (-1,1)\), and \(1\), for \(x \in \{-1,1\}\) (that is, the upper-bound of \(1\) is valid, but it is almost surely equal to \(1/2\)). This is where we lose the factor of \(1/2\) in Eq. (1) above.</p>



<p class="justify-text">One key overall insight is that the span of the feature space is not the only important aspect in the approximation, and that some rescalings are better than others (more on this below when optimize with respect to the chosen basis). Given that we end up estimating entropies from moments, it would be clearly interesting to compare with other ways of estimating entropies from moments, such as using asymptotic expansions based on Edgeworth expansions [8].</p>



<h2>Fourier series and translation-invariant kernels</h2>



<p class="justify-text">We still consider \(\mathcal{X} = [-1,1]\). For a \(2\)-periodic function \(q: \mathbb{R} \to \mathbb{R}\), such that \(q(0) = 1\), with Fourier series \(\hat{q}: \mathbb{Z} \to \mathbb{C}\), we can consider $$k(x,y) = q(x-y) = \sum_{\omega \in \mathcal{Z}} \hat{q}(\omega) e^{i\pi \omega(x-y)}.$$ We can for example take $$q(x) = \frac{\sin ( 2r+1) \pi x/2}{(2r+1)\sin \pi x/2} $$ which corresponds to \(\hat{q}(\omega) = \frac{1}{2r+1} 1_{|\omega| \leqslant r}\), and having $$ \varphi(x)_\omega = \frac{1}{\sqrt{2r+1}} e^{ i\pi \omega x } $$ for \(\omega \in \{-r-1,\dots,r+1\}\) (we consider complex-valued features for simplicity). We refer to this kernel as the <a href="https://en.wikipedia.org/wiki/Dirichlet_kernel">Dirichlet kernel</a>.</p>



<p class="justify-text"><strong>Link with eigenvalues of Toeplitz matrices.</strong> For the specific choice of the Dirichlet kernel, the element of the covariance matrix indexed by \(\omega\) and \(\omega‚Äô\) has value \(\int_{-1}^1 e^{i\pi (\omega-\omega‚Äô)} dp(x)\), which is the <a href="https://en.wikipedia.org/wiki/Characteristic_function_(probability_theory)">characteristic function</a> of \(p\) evaluated at \(\pi(\omega-\omega‚Äô)\). One consequence is that \(\Sigma_p\) is a <a href="https://en.wikipedia.org/wiki/Toeplitz_matrix">Toeplitz matrix</a> with constant values across all diagonals. It is then well-known that as \(m\) goes to infinity, the eigenvalues of \(\Sigma_p\) are tending in distribution to \(p\) (see [<a href="https://ee.stanford.edu/~gray/CIT006-journal.pdf">10</a>]), which provides another proof of good estimation of the entropy in this very particular case.</p>



<p class="justify-text">Note that we could also consider an infinite-dimensional kernel \(q(x) =\Big( 1+ \frac{\sin^2 \pi x/2}{\sinh^2 \sigma}\Big)^{-1}\) which corresponds to \( \hat{q}(\omega) \propto e^{-2\sigma |\omega|}\), where the link with Toeplitz matrices does not apply. See [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>] for details.</p>



<p class="justify-text"><strong>Increasing number of frequencies. </strong>We can then plot the estimation of the same relative entropy as for polynomial kernels, as the number of frequency grows (we can still get the Fourier moments in closed form, see end of post).</p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img width="305" alt="" src="https://francisbach.com/wp-content/uploads/2022/03/dirichlet_kernel-1.png" class="wp-image-6944" height="283" /></figure></div>



<p class="justify-text">We now see that as the dimension of the feature space grows, we get a perfect estimation of the relative entropy. </p>



<p class="justify-text">This estimation is provably from below, but how far is it? How big \(r\) needs to be? </p>



<h2>Upper-bound trough quantum information theory</h2>



<p class="justify-text">In order to find a lower-bound \(D(\Sigma_p \| \Sigma_q)\), we cannot rely on Jensen‚Äôs inequality, as it is in the wrong direction (this happens a lot!). But here, <a href="https://en.wikipedia.org/wiki/Quantum_information">quantum information theory</a> can come to the rescue. </p>



<p class="justify-text">Using the covariance matrix \(\Sigma\) associated to the uniform distribution \(\tau\) on \(\mathcal{X}\), we consider the PSD matrices \(D(y)\), indexed by \(y \in \mathcal{X}\), equal to $$D(y) = \Sigma^{-1/2} \varphi(y) \varphi(y)^\top \Sigma^{-1/2},$$ for which we have $$\int_{\mathcal{X}} D(y) d\tau(y) =\Sigma^{-1/2} \Big( \int_{\mathcal{X}}  \varphi(y) \varphi(y)^\top d\tau(y)\Big)\Sigma^{-1/2} =   I. $$ We can now consider, in quantum information theory terms, that each \(D(y)\) corresponds to a measurement that can be applied to a density operator, and that we look at measures obtained from \(p\), as \(\tilde{p}(y) = {\rm tr } ( D(y) \Sigma_p)\), and from \(q\) as \(\tilde{q}(y) = {\rm tr } ( D(y) \Sigma_q)\). This is a proper <a href="https://en.wikipedia.org/wiki/Quantum_operation">quantum operation</a> because the matrices \(D(y)\) are PSD and sum to identity.</p>



<p class="justify-text">The monotonicity of the von Neumann entropy with respect to quantum operations [9] applies, and we immediately get $$D(\Sigma_p \| \Sigma_q) \geqslant \int_{\mathcal{X}} \tilde{p}(y) \log \frac{ \tilde{p}(y)}{\tilde{q}(y)} d\tau(y).$$ The lower-bound can be seen as the Shannon relative entropy \(D( \tilde{p} \| \tilde{q} )\) between the distributions with density \(\tilde{p}\) and \(\tilde{q}\) with respect to \(\tau\).</p>



<p class="justify-text">These densities have direct relationships with \(p\) and \(q\). Indeed,  we have $$\tilde{p}(y) = {\rm tr } ( D(y) \Sigma_p) = \int_{\mathcal{X}} \big( \varphi(x)^\top \Sigma^{-1/2} \varphi(y) \big)^2 dp(x).$$ Denoting $$h(x,y) = \big( \varphi(x)^\top \Sigma_\tau^{-1/2} \varphi(y) \big)^2,$$ we have \(\int_{\mathcal{X}} h(x,y) d\tau(x) = \| \varphi(y)\|^2\) for all \(y \in \mathcal{X}\), and thus, when the features have unit norms, we can consider \(h\) as a smoothing kernel, and thus \(\tilde{p}\) and \(\tilde{q}\) as smoothed versions of \(p\) and \(q\). Thus, overall we have: $$D(\tilde{p} \| \tilde{q}) \leqslant D(\Sigma_p \| \Sigma_q)  \leqslant D(p\|q),$$ and we have sandwitched the kernel relative entropy by Shannon relative entropies.</p>



<p class="justify-text">We can then characterize the error made in estimating the relative entropy by measuring how the smoothing kernel \(h(x,y)\) differs from a Dirac. In  [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>, Section 4.2], I show how, for metric spaces, the difference between \(D(\tilde{p} \| \tilde{q})\) and \(D({p} \|{q}) \), and thus between \(D(\Sigma_p \| \Sigma_q)\) and \(D(\tilde{p} \| \tilde{q})\), is upperbounded by an explicit function of \(p\) and \(q\), times $$ \sup_{x \in \mathcal{X}} \int_{\mathcal{X}} h(x,y) d(x,y)^2 d\tau(y),$$ which is indeed small when \(h(x,y)\) is closed to a Dirac.</p>



<p class="justify-text">For the Dirichlet kernel presented above, we can show that $$h(x,y) = \frac{1}{4r+2} \bigg( \frac{\sin ( 2r+1) \pi (x-y)/2}{\sin \pi (x-y)/2}\bigg)^2,$$ and that the quantity above characterizing convergence scales as \(1 / (2r+1)\), thus providing an explicit convergence rate. As an illustration, we plot the smoothing kernel at \(y=0\), that is, \(h(x,0)\) in regular scale (left) and log-scale (right), for the Dirichlet kernel above (for which we have unit norm features) for various values of \(r\). We see how it converges to a Dirac as \(r\) grows.</p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img width="503" alt="" src="https://francisbach.com/wp-content/uploads/2022/03/smoothh_both-2.gif" class="wp-image-6999" height="223" /></figure></div>



<h2>Estimation</h2>



<p class="justify-text">Above, we have relied on finite-dimensional feature maps and explicit expectations under \(p\) and \(q\). This is clearly not scalable to generic situations and potentially infinite-dimensional feature spaces. But we can rely here on 30 years of progress in kernel methods.</p>



<p class="justify-text">For simplicity here, we will only estimate the von Neumann entropy \({\rm tr}\big[ \Sigma_p \log \Sigma_p \big]\) from \(n\) independent and identically distributed samples. The natural estimator is \({\rm tr}\big[ \hat{\Sigma}_p \log \hat{\Sigma} _p \big]\), where \(\hat{\Sigma}_p\) and \(\hat{\Sigma}_q\) are empirical covariance matrices (the averages of \(\varphi(x_i) \varphi(x_i)^\top\) over all data points \(x_1,\dots,x_n\)).</p>



<p class="justify-text">In order to compute it efficiently, we can use the usual kernel trick, exactly as used within <a href="https://en.wikipedia.org/wiki/Kernel_principal_component_analysis">kernel principal component analysis</a>, and if \(K\) is the \(n \times n\) kernel matrix associated with the \(n\) data points, then the eigenvalues of \(\frac{1}{n} K\) and \(\hat{\Sigma}_p\) are the same, and thus $$  {\rm tr}\big[ \hat{\Sigma}_p \log \hat{\Sigma} _p \big] =  {\rm tr}\big[ \big(\frac{1}{n}K  \big)\log \big( \frac{1}{n} K \big)  \big].$$ </p>



<p class="justify-text">In terms of statistical analysis, we refer to [1] where it is shown that the difference between the estimate and the true value is of order \(C / \sqrt{n}\), with an explicit constant \(C\), for a wide range of kernels. Surprisingly, there is no need to add any form of regularization.</p>



<p class="justify-text"><strong>Entropy estimation. </strong>Together with the approximation result presented above, by letting \(r\) go to infinity for the Dirichlet kernel, we get an estimator of the regular Shannon entropy which is not that bad (see [<a href="https://arxiv.org/pdf/1711.02141">7</a>] for an optimal one).</p>



<h2>Learning representations</h2>



<p class="justify-text">As mentioned above for polynomials, a key question is how to choose a feature map \(\varphi: \mathcal{X} \to \mathbb{R}^d\). Given that we have a lower-bound on the relative entropy, it is tempting to try to <em>maximize</em> this lower bound, subject to \(\forall x \in \mathcal{X}\), \(\| \varphi(x)^\top \varphi(y)\|^2 \leqslant 1\). The magic is that this is a tractable optimization problem (concave maximization with convex constraints) in the kernel function \((x,y) \mapsto \varphi(x)^\top \varphi(y)\). This is not an obvious result (in fact, I first tried to prove that the von Neumann relative entropy was convex in the kernel, while it is in fact concave). See [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>] for the concavity proofs and simple optimization algorithms.</p>



<h2>Duality and log-partition functions</h2>



<p class="justify-text">Given that our kernel notions of relative entropies are lower bounds on the regular notions, by the traditional convex duality results between maximum entropy and log-partition functions (see, e.g., [<a href="https://www.nowpublishers.com/article/DownloadSummary/MAL-001">11</a>]), we should obtain upper-bounds on log-partition functions. This is exactly what we can get, we simply provide the gist here and refer to [<a href="http://arxiv.org/pdf/2202.08545.pdf">1</a>] for further details. </p>



<p class="justify-text">Given a probability distribution \(q\) on \(\mathcal{X}\) and a function \(f: \mathcal{X} \to \mathbb{R}\), the log-partition function is defined as $$a(f) = \log \bigg( \int_{\mathcal{X}} e^{f(x)} dq(x)\bigg) \tag{2}.$$ It plays a crucial role in probabilistic and statistical inference, and obtaining upper-bounds is instrumental in deriving approximate inference algorithms [11]. A classical convex duality result shows a link with the Shannon relative entropy, as we can express \(a(f)\) as $$a(f) = \sup_{ p\  {\rm probability} \ {\rm measure}} \int_{\mathcal{X}} f(x) dp(x) \ ‚Äì D( p \| q).$$</p>



<p class="justify-text">Assuming that all features have unit norm, we have \(D(p \|q) \geqslant D(\Sigma_p \| \Sigma_q)\), and thus $$ a(f) \leqslant \sup_{ p \ {\rm probability} \ {\rm measure}} \int_{\mathcal{X}} f(x) dp(x)\  ‚Äì D(\Sigma_p \| \Sigma_q),$$ which is itself smaller than the supremum over all signed measures with unit mass (thus removing the non-negativity constraint). Therefore, $$ a(f) \leqslant \sup_{ p \ {\rm measure}} \int_{\mathcal{X}} f(x) dp(x)\  ‚Äì D(\Sigma_p \| \Sigma_q) \tag{3},$$ which can be made computationally tractable in a number of cases.</p>



<h2>Conclusion</h2>



<p class="justify-text">In this blog post, I tried to exhibit a new link between kernels and information theory, which mostly relies on the convexity properties of von Neumann entropy and a bit of quantum analysis. Clearly, the practical relevance of this link remains to be established, but it opens tractable information theory tools to all types of data on which positive kernels can be defined (essentially all of them), with many potential avenues for future work.</p>



<p class="justify-text"><strong>Multivariate modelling.</strong> Regular entropies and relative entropies are used extensively in data science, such as for variational inference, but also to characterize various forms of statistical dependences between random variable. It turns out that the new kernel notions allow for this as well, connecting with a classical line of work within kernel methods [<a href="https://www.jmlr.org/papers/volume3/bach02a/bach02a.pdf">12</a>, <a href="https://www.jmlr.org/papers/volume11/sriperumbudur10a/sriperumbudur10a.pdf">13</a>].</p>



<p class="justify-text"><strong>Other divergences.</strong> I have focused primarily on the link with Shannon relative entropies, but most of the results extend to all <a href="https://en.wikipedia.org/wiki/F-divergence">\(f\)-divergences</a>. For example, we can get kernel-based lower bounds on the square <a href="https://en.wikipedia.org/wiki/Hellinger_distance">Hellinger distance</a>, or the \(\chi^2\)-divergences.</p>



<p class="justify-text"><strong>Kernel sum-of-squares with a probabilistic twist.</strong> As a final note, there is a link between the new notions of kernel entropies with global optimization with <a href="https://francisbach.com/finding-global-minima-with-kernel-approximations/">kernel sum-of-squares</a> [<a href="http://arxiv.org/abs/2012.11978">14</a>]. Indeed, in Eq. (2), if we add a temperature parameter \(\varepsilon\) and define $$a_\varepsilon(f) =\varepsilon  \log \bigg( \int_{\mathcal{X}} e^{f(x) / \varepsilon} dq(x) \bigg), $$ then when \(\varepsilon\) tends to zero, \(a_\varepsilon(f) \) tends to \(\sup_{x \in \mathcal{X}} f(x)\). The lower bound from Eq. (3) then becomes $$\sup_{ p \ {\rm measure}}  \int_{\mathcal{X}} f(x) dp(x) \ ‚Äì \varepsilon D(\Sigma_p \| \Sigma_q),$$ which exactly tends to the sum-of-squares relaxation for the optimization problem. Given that for global optimization, smoothness can be leveraged to circumvent the curse of dimensionality, we can imagine something similar for log-partition functions and entropies.</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Loucas Pillaud-Vivien for proofreading this blog post and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] Francis Bach.¬†<a href="http://arxiv.org/pdf/2202.08545.pdf">Information Theory with Kernel Methods</a>. Technical report, arXiv:2202.08545, 2022. <br />[2] Brian Kulis, M√°ty√°s A. Sustik, Inderjit S. Dhillon. <a href="https://www.jmlr.org/papers/volume10/kulis09a/kulis09a.pdf">Low-Rank Kernel Learning with Bregman Matrix Divergences</a>.¬†<em>Journal of Machine Learning Research</em>¬†10(2):341-376, 2009.<br />[3] Inderjit S. Dhillon, Joel A. Tropp. <a href="https://epubs.siam.org/doi/pdf/10.1137/060649021">Matrix nearness problems with Bregman divergences</a>.¬†<em>SIAM Journal on Matrix Analysis and Applications</em>¬†29(4):1120-1146, 2008.<br />[4] Thomas M. Cover and Joy A. Thomas.¬†<em>Elements of Information Theory</em>. John Wiley &amp; Sons, 1999.<br />[5] Edouard Pauwels, Mihai Putinar, Jean-Bernard Lasserre. <a href="https://hal.archives-ouvertes.fr/hal-01845137v3/document">Data analysis from empirical moments and the Christoffel function</a>.¬†<em>Foundations of Computational Mathematics</em>¬†21(1):243-273, 2021.<br />[6] Ronald De Wolf. <a href="https://theoryofcomputing.org/articles/gs001/gs001.pdf">A brief introduction to Fourier analysis on the Boolean cube</a>. <em>Theory of Computin</em>g, 1:1-20, 2008.<br />[7] Yanjun Han, Jiantao Jiao, Tsachy Weissman, and Yihong Wu. <a href="https://arxiv.org/pdf/1711.02141">Optimal rates of entropy estimation<br />over Lipschitz balls</a>. <em>The Annals of Statistics</em>, 48(6):3228‚Äì3250, 2020.<br />[8] Marc M. Van Hulle. <a href="https://direct.mit.edu/neco/article-abstract/17/9/1903/7008/Edgeworth-Approximation-of-Multivariate">Edgeworth approximation of multivariate differential entropy</a>. <em>Neural computation</em>, 17(9):1903-1910, 2005.<br />[9] Mark M. Wilde. <em>Quantum Information Theory</em>. Cambridge University Press, 2013.<br />[10] Robert M. Gray. <a href="https://ee.stanford.edu/~gray/CIT006-journal.pdf">Toeplitz and Circulant Matrices: A Review</a>. <em>Foundations and Trends in Communications and Information Theory</em>,¬†2(3):155-239, 2006.<br />[11] Martin J. Wainwright and Michael I. Jordan. <a href="https://www.nowpublishers.com/article/DownloadSummary/MAL-001">Graphical Models, Exponential Families, and Variational<br />Inference</a>. <em>Foundations and Trends in Machine Learning,</em> 1(1‚Äì2), 1-305, 2008.<br />[12] Francis Bach and Michael I. Jordan. <a href="https://www.jmlr.org/papers/volume3/bach02a/bach02a.pdf">Kernel independent component analysis</a>. Journal of Machine<br />Learning Research, 3(Jul):1‚Äì48, 2002<br />[13] Bharath K. Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Sch√∂lkopf, and Gert R. G.<br />Lanckriet. <a href="https://www.jmlr.org/papers/volume11/sriperumbudur10a/sriperumbudur10a.pdf">Hilbert space embeddings and metrics on probability measures</a>. <em>Journal of Machine Learning Research</em>, 11:1517‚Äì1561, 2010.<br />[14] Alessandro Rudi, Ulysse Marteau-Ferey, and Francis Bach. <a href="https://arxiv.org/abs/2012.11978">Finding global minima via kernel approximations</a>. Technical Report 2012.11978, arXiv, 2020</p>



<h2>Integrals in closed form</h2>



<p class="justify-text">In order to compute in closed form \(\Sigma_p\) for the distribution \(p\) with density on \([-1,1]\) equal to \( \frac{2}{\pi} \sqrt{1-x^2}\), we need to compute some integrals</p>



<ul class="justify-text"><li><strong>Polynomial kernel</strong>: we have $$\int_{-1}^1   \frac{2}{\pi} \sqrt{1-x^2}x^{2k} dx = 2^{1-2k} \frac{ ( 2k-1)!}{(k-1)! (k+1)!}.$$</li><li><strong>Chebyshev kernel</strong>: we need to compute the integral $$\int_0^{\pi } \frac{2}{\pi}   \sin ^2(\theta) \cos^2 k\theta \, d\theta,$$ which is equal to \(1\) for \(k=0\), to \(1/4\) for \(k=1\), and to \(1/2\) for all larger integers. We also need the integral $$\int_0^{\pi } \frac{2}{\pi}   \sin ^2(\theta) \cos k\theta \cos(k+2)\theta \, d\theta,$$ which is equal to \(-1/2\) for \(k=0\), and to \(-1/4\) for all larger integers. </li><li><strong>Dirichlet kernel</strong>: we have $$ \int_{-1}^1 \frac{2}{\pi} \sqrt{1-x^2} \cos k \pi x dx = \frac{2}{k \pi} J_1(k\pi),$$ where \(J_1\) is the<a href="https://en.wikipedia.org/wiki/Bessel_function#Bessel_functions_of_the_first_kind"> Bessel function of the first kind</a>.</li></ul>



<p></p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/information-theory-with-kernel-methods/"><span class="datestr">at April 04, 2022 04:45 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/044">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/044">TR22-044 |  An Optimal Algorithm for Certifying Monotone Functions | 

	Meghal Gupta, 

	Naren Manoj</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Given query access to a monotone function $f\colon\{0,1\}^n\to\{0,1\}$ with certificate complexity $C(f)$ and an input $x^{\star}$, we design an algorithm that outputs a size-$C(f)$ subset of $x^{\star}$ certifying the value of $f(x^{\star})$. Our algorithm makes $O(C(f) \cdot \log n)$ queries to $f$, which matches the information-theoretic lower bound for this problem and resolves the concrete open question posed in the STOC '22 paper of Blanc, Koch, Lange, and Tan [BKLT22].

We extend this result to an algorithm that finds a size-$2C(f)$ certificate for a real-valued monotone function with $O(C(f) \cdot \log n)$ queries. We also complement our algorithms with a hardness result, in which we show that finding the shortest possible certificate in $x^{\star}$ may require $\Omega\left(\binom{n}{C(f)}\right)$ queries in the worst case.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/044"><span class="datestr">at April 04, 2022 04:08 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/043">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/043">TR22-043 |  Polynomial Bounds On Parallel Repetition For All 3-Player Games With Binary Inputs | 

	Kunal Mittal, 

	Wei Zhan, 

	Uma Girish, 

	Ran Raz</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove that for every 3-player (3-prover) game $\mathcal G$ with value less than one, whose query distribution has the support $\mathcal S = \{(1,0,0), (0,1,0), (0,0,1)\}$ of hamming weight one vectors, the value of the $n$-fold parallel repetition $\mathcal G^{\otimes n}$ decays polynomially fast to zero; that is, there is a constant $c = c(\mathcal  G)&gt;0$ such that the value of the game $\mathcal  G^{\otimes n}$ is at most $n^{-c}$.
	
Following the recent work of Girish, Holmgren, Mittal, Raz and Zhan (STOC 2022), our result is the missing piece that implies a similar bound for a much more general class of multiplayer games: For $\textbf{every}$ 3-player game $\mathcal G$ over $\textit{binary questions}$ and $\textit{arbitrary answer lengths}$, with value less than 1, there is a constant $c = c(\mathcal G)&gt;0$ such that the value of the game $\mathcal G^{\otimes n}$ is at most $n^{-c}$.
	
Our proof technique is new and requires many new ideas. For example, we make use of the Level-$k$ inequalities from Boolean Fourier Analysis, which, to the best of our knowledge, have not been explored in this context prior to our work.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/043"><span class="datestr">at April 02, 2022 01:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8669802159969006721">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/04/a-ramsey-theory-podcast-no-strangers-at.html">A Ramsey Theory Podcast: No Strangers at this Party</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>¬†BILL: Lance, I am going to blog about the Ramsey Theory Podcast called¬†</p><p>¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬†¬† ¬†¬† ¬† <i>No strangers at this party</i><br /></p><p>LANCE: Oh, so that will be your April Fools Day post? That is too unbelievable so it won't work as a joke.</p><p>BILL: Okay, you got me. But it will work if I get 14 Ramsey Theorists to do Podcasts on Ramsey Theory and pretend its coming from... where should it come from.¬†</p><p>LANCE: A Hungarian Middle School.¬†</p><p>BILL: That's¬† too realistic. How about Simon Fraser University in Canada?</p><p>LANCE: Why there?</p><p>BILL: Why not there?</p><p>LANCE: Knock yourself out.</p><p>---------------------------------------------------------------------------</p><p>At Simon Fraser University they have a <a href="https://www.sfu.ca/~vjungic/Ramsey.html">podcast on Ramsey Theory</a>. They had 14 episodes, each one was an interview with someone who is interested in Ramsey Theory. I don't like the term `Ramsey Theorist' since I doubt anyone does JUST Ramsey Theory (e.g., I do Muffins to!).</p><p>Here is the list of people they interviewed. You can find the podcasts at¬†<a href="https://open.spotify.com/show/4UrTrYkJc9rFhiOQRoNbm3">Spotify</a>,¬†<a href="https://anchor.fm/veselin-jungic">Anchor</a>,¬†<a href="https://podcasts.apple.com/us/podcast/the-ramsey-theory-podcast-no-strangers-at-this-party/id1602576205">Apple
Podcasts</a>, and¬†<a href="https://podcasts.google.com/search/Ramsey%20theory">Google Podcasts</a>.</p><p class="MsoNormal"></p><p>Julian Sahasarabudhe,¬†</p><p>Jaroslav Nesetril</p><p>Joel Spencer¬†</p><p>Donald Robertson</p><p>Fan Chung</p><p>Steve Butler</p><p>Tomas Kaiser</p><p>David Conlon</p><p>Bruce Landman¬†</p><p>William Gasarch</p><p>Bryna Kra</p><p>Neil Hindman</p><p>Adriana Hansberg</p><p>Amanda Montejano</p><p><br /></p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/04/a-ramsey-theory-podcast-no-strangers-at.html"><span class="datestr">at April 02, 2022 01:28 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/042">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/042">TR22-042 |  Matrix Polynomial Factorization via Higman Linearization | 

	Vikraman Arvind, 

	Pushkar Joglekar</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In continuation to our recent work on noncommutative
 polynomial factorization, we consider the factorization problem for
 matrices of polynomials and show the following results.
\begin{itemize}  
\item Given as input a full rank $d\times d$ matrix $M$ whose entries
  $M_{ij}$ are polynomials in the free noncommutative ring
  $\mathbb{F}_q\langle x_1,x_2,\ldots,x_n \rangle$, where each $M_{ij}$ is given by a
  noncommutative arithmetic formula of size at most $s$, we give a
  randomized algorithm that runs in time polynomial in $d,s, n$ and
  $\log_2q$ that computes a factorization of $M$ as a matrix product
  $M=M_1M_2\cdots M_r$, where each $d\times d$ matrix factor $M_i$ is
  irreducible (in a well-defined sense) and the entries of each $M_i$
  are polynomials in $\mathbb{F}_q \langle x_1,x_2,\ldots,x_n \rangle$ that are output
  as algebraic branching programs. We also obtain a deterministic
  algorithm for the problem that runs in $poly(d,n,s,q)$.
\item A special case is the efficient factorization of matrices whose
  entries are univariate polynomials in $\mathbb{F}[x]$. When $\mathbb{F}$ is a finite
  field the above result applies. When $\mathbb{F}$ is the field of rationals
  we obtain a deterministic polynomial-time algorithm for the problem.
  \end{itemize}</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/042"><span class="datestr">at March 31, 2022 12:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/03/31/phd-studentship-at-university-of-liverpool-apply-by-april-18-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/03/31/phd-studentship-at-university-of-liverpool-apply-by-april-18-2022/">PhD Studentship at University of Liverpool (apply by April 18, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Applications are invited for a 4-year PhD studentship on Game Theory and Computational Social Choice on Blockchain at the University of Liverpool, in collaboration with the blockchain technology company IOHK. The selected candidate will be advised by Aris Filos-Ratsikas and Rahul Savani from the University of Liverpool, and Philip Lazos from IOHK.</p>
<p>Website: <a href="https://www.findaphd.com/phds/project/game-theory-and-computational-social-choice-on-blockchain-epsrc-cdt-in-distributed-algorithms/?p142348">https://www.findaphd.com/phds/project/game-theory-and-computational-social-choice-on-blockchain-epsrc-cdt-in-distributed-algorithms/?p142348</a><br />
Email: Aris.Filos-Ratsikas@liverpool.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/03/31/phd-studentship-at-university-of-liverpool-apply-by-april-18-2022/"><span class="datestr">at March 31, 2022 12:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://blog.simons.berkeley.edu/?p=636">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/simons.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://blog.simons.berkeley.edu/2022/03/where-do-q-functions-come-from/">Where Do Q-Functions Come From?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.simons.berkeley.edu" title="Calvin Caf√©: The Simons Institute Blog">Simons Institute blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>by¬†<a href="https://meyn.ece.ufl.edu/" target="_blank" rel="noreferrer noopener">Sean Meyn</a> (University of Florida) and <a href="http://cs.bme.hu/~gergo/" target="_blank" rel="noreferrer noopener">Gergely Neu</a> (Pompeu Fabra University)</p>



<p>One theoretical foundation of reinforcement learning is optimal control, usually rooted in the Markovian variety known as Markov decision processes (MDPs). The MDP model consists of a state process, an action (or input) process, and a one-step reward that is a function of state and action. The goal is to obtain a policy (function from states to actions) that is optimal in some predefined sense. Chris Watkins introduced the Q-function in the 1980s as part of a methodology for reinforcement learning. Given its importance for over three decades, it is not surprising that the question of the true meaning of <em>Q</em> was a hot topic for discussion during the Simons Institute‚Äôs Fall 2020¬†program on <a href="https://simons.berkeley.edu/programs/rl20" target="_blank" rel="noreferrer noopener">Theory of Reinforcement Learning</a>.</p>



<p>This short note focuses on interactions at the start of the program, and research directions inspired in part by these interactions. To start with, <em>who is Q</em>? Was this code for one of Watkins‚Äô friends at Cambridge? The question was posed early on, which led to an online investigation. The mystery was shattered through a response from Chris: we now know that the letter <em>Q</em> stands for <em>quality</em>, not Quinlyn or Quincy. To discuss further questions and potential answers requires some technicalities.</p>



<p>The discounted-cost optimality criterion is a favorite metric for performance in computer science and operations research, and is the setting of the original Q-function formulation. The definition requires a state process <span class="math inline">\(\{X_k : k\ge 0\}\)</span> and an action (or input) process <span class="math inline">\(\{A_k : k\ge 0\}\)</span>, evolving on respective spaces (which are assumed discrete in this note). There is a controlled transition matrix <span class="math inline">\(P\)</span> that describes dynamics: <span class="math inline">\(X_{k+1}\)</span> is distributed according to <span class="math inline">\(P(\cdot|x,a)\)</span> when <span class="math inline">\(X_k=x\)</span> and <span class="math inline">\(A_k=a\)</span>, for any action sequence that is adapted to the state sequence.</p>



<p>With <span class="math inline">\(\gamma\)</span> denoting the discount factor, the Q-function is the solution to a nonlinear fixed-point equation <span class="math inline">\(T^*Q = Q\)</span> in which <span class="math inline">\(T^*\)</span> is the Bellman operator: <span class="math display">\[\left(T^*Q\right)(x,a) = r(x,a) + \gamma \mathbb{E}_{X‚Äô\sim P(\cdot|x,a)}\left[\max_{a‚Äô} Q(X‚Äô,a‚Äô)\right]\]</span> This must hold for each state-action pair <span class="math inline">\((x,a)\)</span>, with the maximum over all possible actions. This is a version of the dynamic programming (DP) equation that has been with us for about seven decades.</p>



<p>The magic of Q-learning, which is based on this DP equation, is that the maximum appears within an expectation. This makes possible the application of Monte Carlo methods to obtain an approximate solution based solely on observations of the actual system to be controlled, or through simulations.</p>



<p>One core idea of modern reinforcement learning (RL) is to find approximate solutions of the DP equation within a function class (e.g., neural networks, as popularized by the deep Q-learning approach of Mnih et al., 2015). While success stories are well-known, useful theory is scarce: we don‚Äôt know if a solution exists to an approximate DP equation except in very special settings, and we don‚Äôt know if a good approximation will lead to good performance for the resulting policy. We don‚Äôt even know if the recursive algorithms that define Q-learning will be stable ‚Äî estimates may diverge to infinity.</p>



<p>There are many ways to read these negative results, and indeed many articles have been written around this subject. Our own reading is probably among the most radical: without understanding the issues around the existence of solutions to these DP equation approximations or their interpretation, we should search for alternative approximations of dynamic programming suitable for application in RL.</p>



<p>These concerns were raised at Sean Meyn‚Äôs <a href="https://www.youtube.com/watch?v=nUaFu3WzW7o" target="_blank" rel="noreferrer noopener">boot camp lecture</a>, where he called on listeners to revisit an alternate foundation of optimal control: the linear programming (LP) approach introduced by Manne (1960) and further developed by Denardo (1970) and d‚ÄôEpenoux (1963). The message was greeted with enthusiasm from some attendees, including Gergely Neu, who responded, ‚ÄúYou have blown my mind!‚Äù He had been working on his own formulation of this idea, which became logistic Q-learning (more on this below).</p>



<span id="more-636"></span>



<p>The least-squares approach that is central to most traditional RL algorithms (such as DQN) is replaced by an LP, so that we move from <span class="math inline">\(L_2\)</span> to <span class="math inline">\(L_\infty\)</span> approximation. There is ample motivation for this point of view:</p>



<ul><li>
		<p>Lyapunov theory and the theory of inverse dynamic programming provide firm motivation: a good approximation of the DP equation in an <span class="math inline">\(L_\infty\)</span> sense implies strict bounds on closed-loop performance of the resulting policy. You can learn more about this theory in standard RL texts, and Meyn‚Äôs new RL monograph to appear this year.</p>
	</li><li>
		<p>The LP approach reframes the DP approximation as a numerical optimization problem rather than a root-finding problem. Existence of a solution is guaranteed under minimal assumptions, even when working with a restricted set of candidate solutions.</p>
	</li></ul>



<p>Despite these advantages, LP-based approaches were left out of the mainstream RL tool kit until recently. One reason may be a glaring gap in the framework: the classical LPs are phrased in terms of state-value functions and not Q-functions! This means it was not obvious how to bring in Monte Carlo methods for algorithm design.</p>



<p>In 2020, we independently discovered the following LP that can be used to cast Q-learning as a constrained optimization problem ‚Äî the so-called DPLP: <span class="math display">\[\begin{aligned} \mbox{minimize } \qquad &amp; \mathbb{E}_{X\sim\nu_0}\left[\max_{a} Q(X,a)\right] \\ \mbox{subject to } \qquad &amp; Q(x,a) \ge r(x,a) + \gamma \mathbb{E}_{X‚Äô\sim P(\cdot|x,a)}\left[\max_{a‚Äô} Q(X‚Äô,a‚Äô)\right]\end{aligned}\]</span> Without any constraints on the function class, the optimizer is the solution to the DP equation. The constraints amount to the DP inequality <span class="math inline">\(Q \ge T^* Q\)</span>, which tells us only <em>negative</em> deviations should be penalized! This is to be contrasted with the commonly used squared Bellman error criterion that penalizes deviations of both signs. In seeking an approximation within a function class <span class="math inline">\(\mathcal{F}\)</span>, it is not at all difficult to ensure a feasible solution exists, even when there is no theory to ensure that an approximate Bellman optimality operator has a fixed point.</p>



<p>There are a variety of possible ways of turning <span> this</span> insight into practical algorithms. One path (taken by Bas-Serrano et al., 2021) is to replace the hard constraint in the LP with a smooth upper bound on the expected constraint violations, resulting in the following unconstrained minimization problem: <span class="math display">\[%  \mbox{minimize } \mathbb{E}_{X\sim\nu_0}[\max_a Q(X,a)] + \frac 1\eta \log 
% \mathbb{E}_{(X,A)\sim\mu}[e^{\eta(r(X,A) + \gamma \mathbb{E}_{X‚Äô\sim P(\cdot|X,A)}[\max_{a‚Äô} Q(X‚Äô,a‚Äô)] ‚Äì Q(X,A))}].
\mbox{minimize} \;\;\;\;\;\;\,\, \mathbb{E}_{X\sim\nu_0}\left[\max_a Q(X,a)\right] + \frac 1\eta \log 
\mathbb{E}_{(X,A)\sim\mu}\left[e^{\eta((T^*Q)(X,A) ‚Äì Q(X,A))}\right].\]</span> Bas-Serrano et al.¬†(2021) dubbed the above objective function the ‚Äúlogistic Bellman error‚Äù and have shown that it enjoys several favorable properties that make it an ideal objective for stochastic optimization. Interactions at the Simons Institute led to the development of another related algorithmic framework called ‚Äúconvex Q-learning,‚Äù in which the hard constraints in the LP are replaced with a different continuous approximation (Lu et al., 2021).</p>



<p>It is straightforward to derive practical algorithms from both approaches by replacing the expectations with sample averages. This approach led to good empirical performance, along with the beginnings of theory for convergence and performance bounds.</p>



<p>The LP approach had been simmering in the background over the prior decade: de Farias and Van Roy (2003) proposed an LP approach for approximate dynamic programming, and Mehta and Meyn (2009) contains foundations for convex Q-learning, including a variant of the DPLP for deterministic systems in continuous time.</p>



<p>The new techniques bring challenges that must be overcome before these LP-based approaches can take on the world. One is that these optimization problems can be very poorly conditioned. Another is that the approximation may greatly underestimate the true Q-function when using a poor choice of function class for approximation. We remain optimistic: just like the RL community has successfully developed numerical recipes for addressing the much more severe issue of nonexistence of fixed points to the approximate Bellman operators, we believe that similar know-how can be developed for LP-based methods if theorists and practitioners join forces. Now that we understand where Q comes from, let us decide together where it will go next!</p>



<p><em>Acknowledgment</em>. The authors owe a great debt to the Simons Institute for providing inspiration and interactions throughout the fall program. In particular, our joint work (Lu et al., 2021) was in large part an outcome of discussions at the meeting. The LP approach is one theme of the new textbook (Meyn, 2022), and the impact of the program is seen throughout the book.</p>



<h2 class="unnumbered" id="references">References</h2>



<ul><li> <p>A.¬†S.¬†Manne. Linear programming and sequential decisions. <em>Management Science</em> 6(3), pages 259‚Äì267, 1960.</p> </li><li> <p>E.¬†V.¬†Denardo. On linear programming in a Markov decision problem. <em>Management Science</em> 16(5), pages 281-288, 1970.</p> </li><li> <p>F.¬†d‚ÄôEpenoux. A probabilistic production and inventory problem. <em>Management Science</em> 10(1), pages 98-108, 1963.</p> </li><li> <p>D.¬†P.¬†de Farias and B.¬†Van Roy. The linear programming approach to approximate dynamic programming. <em>Operations Research</em>, 51(6), pages 850‚Äì865, 2003.</p> </li><li> <p>J.¬†Bas-Serrano, S.¬†Curi, A.¬†Krause, and G.¬†Neu. Logistic Q-learning. In International Conference on Artificial Intelligence and Statistics, pages 3610-3618, 2021.</p> </li><li> <p>P.¬†G.¬†Mehta and S.¬†P.¬†Meyn. Q-learning and Pontryagin‚Äôs minimum principle. In Conference on Decision and Control, pages 3598‚Äì3605, Dec. 2009.</p> </li><li> <p>F.¬†Lu, P.¬†G.¬†Mehta, S.¬†P.¬†Meyn, and G.¬†Neu. Convex Q-learning. In American Control Conference, pages 4749‚Äì4756. IEEE, 2021.</p> </li><li> <p>S.¬†Meyn. <em>Control Systems and Reinforcement Learning</em>. Cambridge University Press (to appear), Cambridge, 2022. Draft manuscript available at <a href="https://meyn.ece.ufl.edu/2021/08/01/control-systems-and-reinforcement-learning/" target="_blank" rel="noreferrer noopener">https://meyn.ece.ufl.edu/2021/08/01/control-systems-and-reinforcement-learning/</a></p> </li><li> <p>V.¬†Mnih, K.¬†Kavukcuoglu, D.¬†Silver, A.¬†A.¬†Rusu, J.¬†Veness, M.¬†G. Bellemare, A.¬†Graves, M.¬†Riedmiller, A.¬†K.¬†Fidjeland, G.¬†Ostrovski, S.¬†Petersen, C.¬†Beattie, A.¬†Sadik, I.¬†Antonoglou, H.¬†King, D.¬†Kumaran, D.¬†Wierstra, S.¬†Legg, and D.¬†Hassabis. Human-level control through deep reinforcement learning. <em>Nature</em> 518 (7540), pages 529-533, 2015.</p> </li></ul></div>







<p class="date">
by Simons Institute Editor <a href="https://blog.simons.berkeley.edu/2022/03/where-do-q-functions-come-from/"><span class="datestr">at March 30, 2022 04:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://thmatters.wordpress.com/?p=1365">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sigact.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://thmatters.wordpress.com/2022/03/26/call-for-nominations-stoc-test-of-time-award-deadline-apr-30/">Call for Nominations: STOC Test of Time Award (Deadline: Apr 30)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><strong>The 2022 STOC Test of Time Award</strong> recognizes papers published in the Proceedings of the Annual ACM Symposium on Theory of Computing. This is the second year of this annual award. There are three awards, targeting the STOC conferences 10, 20, and 30 years prior to the year in which the award is given. While there is a preference for papers in the target years (and nominations from those years are encouraged), in each of these award categories it is also possible to nominate STOC conference papers published up to four conferences earlier than the targeted conference. Thus, the 2022 STOC Test of Time Awards will be for papers presented at the STOC conferences in 2008-2012, 1998-2002, and 1988-1992. The awards, which will be presented at STOC 2022, include a prize of US $500 per author as well as complimentary registration for all authors who attend the conference at which the award is given.</p>



<h2>Nomination Procedure</h2>



<p>Nominations should be sent to <a target="_blank" rel="noreferrer noopener"><strong>stoc22.tot.award@gmail.com</strong></a><strong> </strong>with a subject line of <strong>‚ÄúSTOC Test of Time Award‚Äù </strong>no later than <strong>April 30, 2022</strong>. Nominations should contain an explanation of the impact of the nominated paper(s), including references to follow-on work. A nomination may be accompanied by up to three additional endorsement letters, which may be sent by the endorsers directly to the same email address with the same subject line. Self-nominations are disallowed.¬†</p>



<h2>Selection</h2>



<p>The winners will be selected by a committee appointed by the SIGACT Executive Committee. For 2022 the selection committee consists of Toniann Pitassi (Columbia), Satish Rao (Berkeley), Salil Vadhan (Harvard, chair), Avi Wigderson (Institute for Advanced Study).¬†</p>



<p>In selecting the Test of Time Award winners, the Committee will pay particular attention to long-term impact. This impact can come in many forms, including but not limited to:</p>



<ol><li>Opening up a new area of research</li><li>Introducing new techniques</li><li>Solving a problem of lasting importance</li><li>Stimulating advances in other areas of computer science or in other disciplines.</li></ol>



<p>The committee expects to select exactly one paper for each award. However, when circumstances justify it, up to three may be selected. The committee may consider papers that were not explicitly nominated and gather additional input from experts, but formal nominations are extremely helpful in the committee‚Äôs deliberations and strongly encouraged.</p></div>







<p class="date">
by shuchic <a href="https://thmatters.wordpress.com/2022/03/26/call-for-nominations-stoc-test-of-time-award-deadline-apr-30/"><span class="datestr">at March 27, 2022 01:53 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-7417921219645055549">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/03/i-dont-care-about-ketanji-brown.html">I don't care about Ketanji Brown Jackson's LSAT scores and she does not care about my GRE scores</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Tucker Carlson has asked to see Ketanji Brown Jacksons's LSATs.¬†</p><p>When I applied to College they (not sure who <i>they</i>¬†are) wanted to see my SAT scores. Putting aside the issue of whether the test means anything, they viewed the SATs (and my HS grades and letters from teachers) as a sign of my¬†</p><p>¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†<i>potential.¬†</i></p><p>When I applied to Grad school they (a different <i>they</i>) wanted to see my GRE scores. Putting aside the issue of whether the test means anything, they viewed the GREs (and my college grades and letters from professors) as a sign of my</p><p>¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†<i>potential.¬†</i></p><p>When I applied for jobs as a professor they (another <i>they</i>) wanted to see my resume (papers I wrote) and letters from my advisor and others (I think). They did not look at my grades (just as well- I got a B in both compiler design and operating systems. Darling is amazed I even took operating systems). This was probably the oddest of the application processes since they were looking for both</p><p>¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†<i> potential and achievement.</i></p><p>That is, the evidence that I could do research was that I had done some research. This was before the current¬† era where grad students had to have x papers in prestige conferences to get a job at a top y school. The letter from my advisor may well have spoken of my potential.¬†</p><p>When I went up for tenure ALL they cared about was PAPERS (and letters saying they were good papers), and some teaching and service. It was based just¬† on¬†</p><p>¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† <i>achievement.</i></p><p>A wise man named Lance Fortnow once told me:</p><p><i>The worst thing a letter of recommendation for a tenure case can say is `this person has great potential'</i></p><p><br /></p><p>It would have been rather odd for Tucker Carlson to ask to see my SAT scores or GRE scores or by HS, College, or Grad School grades as a criteria for Tenure. Those tests and those grades are there to measure potential to DO something, whereas if you are going up for tenure or a Supreme Court seat, you've already DONE stuff.¬†</p><p>After I got into grad school one of my first thoughts was</p><p><i>Nobody will ever want to see my GRE's again. ( I was right.)¬†</i></p><p>After KBJ got into Law School she might have thought</p><p><i>Nobody will ever want to see my LSAT scores again. (She was wrong.)</i></p><p><i><br /></i></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/03/i-dont-care-about-ketanji-brown.html"><span class="datestr">at March 27, 2022 12:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=122">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2022/03/25/wednesday-april-6th-2022-shuchi-chawla-from-ut-austin/">Wednesday April 6th 2022 ‚Äî Shuchi Chawla from UT Austin</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science ‚Äì Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The third <a href="https://sites.google.com/view/dstheory/home" target="_blank" rel="noreferrer noopener">Foundations of Data Science</a> virtual talk of this year will take place on <strong>Wednesday, March 6th</strong> at <strong>1:00 PM Pacific Time</strong> (16:00 Eastern Time, 22:00 Central European Time, 20:00 UTC).¬†<a href="https://www.cs.utexas.edu/people/faculty-researchers/shuchi-chawla">Shuchi Chawla</a> from<strong> UT Austin</strong> will speak about ‚ÄúPandora‚Äôs Box with Correlations: Learning and Approximation.<em>‚Äù</em></p>



<p><a href="https://sites.google.com/view/dstheory" target="_blank" rel="noreferrer noopener">Please register here to join the virtual talk.</a></p>



<p><strong>Abstract</strong>:  In the Pandora‚Äôs Box problem, the algorithm is provided with a number of boxes with unknown (stochastic) rewards contained inside them. The algorithm can open any box at some cost, discover the reward inside, and based on these observations can choose one box and keep the reward contained in it. Given the distributions from which the rewards are drawn, the algorithm must determine an order in which to open the boxes as well as when to stop and accept the best reward found so far. In general, an optimal algorithm may make both decisions adaptively based on instantiations observed previously. The Pandora‚Äôs Box problem and its extensions capture many kinds of optimization problems with stochastic input where the algorithm can obtain instantiations of input random variables at some cost. Previous work on these problems assumes that different random variables in the input are distributed independently. As such it does not capture many real-world settings. In this work, we provide the first algorithms for Pandora‚Äôs Box-type problems with correlations. In the independent setting, optimal algorithms are non-adaptive and based on the notion of the Gittins index. These techniques fail to extend to the correlated case. We assume that the algorithm has access to samples drawn from the joint distribution on input and provide solutions that require few samples; are computationally efficient; and guarantee approximate optimality. <br />This is joint work with Evangelia Gergatsouli, Yifeng Teng, Christos Tzamos, and Ruimin Zhang and appeared in FOCS‚Äô20.</p>



<p>¬†The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2022/03/25/wednesday-april-6th-2022-shuchi-chawla-from-ut-austin/"><span class="datestr">at March 25, 2022 11:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/03/23/lecturer-in-theoretical-computer-science-at-university-of-auckland-apply-by-may-15-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/03/23/lecturer-in-theoretical-computer-science-at-university-of-auckland-apply-by-may-15-2022/">Lecturer in Theoretical Computer Science at University of Auckland (apply by May 15, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>We seek two early-career top candidates working in any subfield of TCS, preferably in algorithms and data structures, probabilistic computation, quantum and algorithmic information theory, computational biology, and computational complexity.</p>
<p>Website: <a href="https://jobs.smartrecruiters.com/TheUniversityOfAuckland/743999813405096-lecturer-in-theoretical-computer-science">https://jobs.smartrecruiters.com/TheUniversityOfAuckland/743999813405096-lecturer-in-theoretical-computer-science</a><br />
Email: g.russello@auckland.ac.nz</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/03/23/lecturer-in-theoretical-computer-science-at-university-of-auckland-apply-by-may-15-2022/"><span class="datestr">at March 23, 2022 11:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/041">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/041">TR22-041 |  Boolean functions with small approximate spectral norm | 

	Hamed Hatami, 

	TsunMing Cheung, 

	Rosie Zhao, 

	Itai Zilberstein</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The sum of the absolute values of the Fourier coefficients of a function $f:\mathbb{F}_2^n \to \mathbb{R}$ is called the spectral norm of $f$.    Green and Sanders' quantitative version of  Cohen's idempotent theorem states that if the spectral norm of $f:\mathbb{F}_2^n \to \{0,1\}$ is at most $M$, then the support of $f$ belongs to the ring of sets generated by at most $\ell(M)$ cosets,  where $\ell(M)$ is a constant that only depends on $M$.
    
    We prove that the above statement can be generalized to approximate spectral norms if and only if the support of $f$ and its complement satisfy a certain arithmetic connectivity condition. In particular, our theorem provides a new proof of the quantitative Cohen's theorem for $\mathbb{F}_2^n$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/041"><span class="datestr">at March 23, 2022 08:19 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/040">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/040">TR22-040 |  Should decisions in QCDCL follow prefix order? | 

	Benjamin B√∂hm, 

	Tom√°≈° Peitl, 

	Olaf Beyersdorff</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Quantified conflict-driven clause learning (QCDCL) is one of the main solving approaches for quantified Boolean formulas (QBF). One of the differences between QCDCL and propositional CDCL is that QCDCL typically follows the prefix order of the QBF for making decisions.
We investigate an alternative model for QCDCL solving where decisions can be made in arbitrary order. The resulting system QCDCL-ANY is still sound and terminating, but does not necessarily allow to always learn asserting clauses or cubes. To address this potential drawback, we additionally introduce two subsystems that guarantee to always learn asserting clauses (QCDCL-UNI-ANY) and asserting cubes (QCDCL-EXI-ANY), respectively.
We model all four approaches by formal proof systems and show that QCDCL-UNI-ANY is exponentially better than QCDCL on false formulas, whereas QCDCL-EXI-ANY is exponentially better than QCDCL on true QBFs. Technically, this involves constructing specific QBF families and showing lower and upper bounds in the respective proof systems.
We complement our theoretical study with some initial experiments that confirm our theoretical findings.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/040"><span class="datestr">at March 22, 2022 10:18 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-7134833319825279941">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/03/do-you-want-to-be-sigact-news-book.html">Do you want to be the SIGACT NEWS book review editor?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>I ran the SIGACT Book Review Column from 1997-2015 (18 years). You can find all of my columns, plus reviews I did for Fred,¬†<a href="http://www.cs.umd.edu/~gasarch/bookrev/bookrev.html">here</a>.</p><p>When I handed it off to Fred Green I gave him this sage advice:</p><p>¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†<i>Nobody should do this kind of job for more than about 5 years.</i></p><p>He ran the SIGACT Book Review Column since the end of 2015. You can find some of his columns¬†<a href="http://mathcs.clarku.edu/~fgreen/SIGACTReviews/bookrev/bookrev.html">here</a>.</p><p>Fred is taking my advice and looking for a successor.</p><p>SO, this blog is a call to ask</p><p>¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†DO YOU WANT TO BE THE SIGACT NEWS BOOK REVIEW EDITOR?</p><p>If so then email</p><p>Fred: fgreen@clarku.edu</p><p><br /></p><p>DO NOT BE SHY! I suspect he won't get many applicants, so if you want the job its probably yours.</p><p><br /></p><p>PROS</p><p>1) You get to skim lots of books and read some of¬† them.</p><p>2) You get some free books.</p><p>3) You get plugged into the book community (this helped me when I wrote my two books).</p><p>4) You'll have two Veteran Book Review Editors happy to review for you.</p><p>5) You get to decide the direction the column goes in.</p><p>Both Fred and I did mostly CS theory books. However:</p><p>a) I did more combinatorics, educational, history, and Computers &amp; Society books than usual.</p><p>b) Fred did more Number Theory and Physics than usual.</p><p>(Since I did the job 18 years and Fred for 6, its not clear what <i>usual</i> means.)¬†</p><p><br /></p><p>CONS</p><p>1) You have to get out a book review column 4 times a year.</p><p>2) You have to find reviewers for books and then email them when the reviews are due.</p><p>(I think Fred is still waiting for me to review a Biography of Napier. Oh well. On the other hand, I was the one who liked having history books, which may explain why Fred never hassled me about it.)¬†</p><p><br /></p><p>ADVICE</p><p>Prob should be done by someone who already has Tenure. While seeing and skimming thosebooks is GOOD for your research career, and good in the long-termsomeone pre-tenure really needs to get papers out in the short term. Also, when you get a book think about who might be good to review it--- don't take on to many yourself.¬†</p><p><br /></p><p>PARTING GIFT OR WELCOME GIFT</p><p>In a recent column I had a review of a 5-book set from the LESS WRONG blog. I amcurrently working on a review of a 4-book set set from the LESS WRONG blog. This willeither be a parting gift for Fred or a Welcome gift to his successor.</p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/03/do-you-want-to-be-sigact-news-book.html"><span class="datestr">at March 21, 2022 03:16 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8295">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2022/03/20/cool-projects-from-my-crypto-class/">Cool projects from my crypto class</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>This fall, I taught my course <a href="https://cs127.boazbarak.org/">CS 127: Cryptography</a>, based on my lecture notes: <a href="https://intensecrypto.org/">‚ÄúAn intensive introduction to cryptography‚Äù</a>. This is a course that starts with no background knowledge, and gets to advanced concepts including lattice-based (aka ‚Äúpost quantum‚Äù) encryption, fully homomorphic encryption, zero-knowledge proofs, multiparty secure computation, software obfuscation, quantum computing and crypto, and more. </p>



<p><a href="https://cs127.boazbarak.org/projects/">As in previous years</a>, I had an fantastic group of students, several of whom produced impressive course projects. These include the following: </p>



<p><strong>Gavin Uberti</strong>, <strong>Kevin Luo</strong>, <strong>Oliver Cheng,</strong>  and <strong>Wittmann Goh</strong> <a href="https://arxiv.org/abs/2112.04581">implemented Witness Encryption</a>. Witness encryption is a cool concept whereby you can encrypt a secret X (for example the private key corresponding to a bitcoin wallet) so that people can decrypt X if and only if they can find a solution to some puzzle P. You can do this <em>even if you don‚Äôt know a solution yourself!</em> So for example you could use this to offer an automatically paying reward for a formal proof of the Reimann Hypothesis, or as they did, offer 2270  Satoshis to anyone solving this Soduko puzzle:</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2022/03/image.png"><img src="https://windowsontheory.files.wordpress.com/2022/03/image.png?w=503" alt="" class="wp-image-8301" /></a></figure>



<p><strong>Simas Sakenis</strong> wrote a <a href="https://windowsontheory.files.wordpress.com/2022/03/simas_project.pdf">survey of proofs of stake in cryptocurrencies</a>,  providing a uniform formalizaiton of proofs of work and proofs of stake, and explaining the difference. </p>



<p><strong>Michael Kiestra</strong> and <strong>Beatrice Nash</strong> proposed  <a href="https://windowsontheory.files.wordpress.com/2022/03/kiestra_nash.pdf">CLAMBAKE</a>,  a protocol that uses broadcast encryption and Yao‚Äôs garbled circuits to achieve a privacy-preserving protocol for facilitating access to controlled resources such as university buildings.</p>



<p>(There were more projects in the course, but some students preferred not to post these publicly since they are still working on them; I will update this post with more projects if appropriate.)</p>



<p></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2022/03/20/cool-projects-from-my-crypto-class/"><span class="datestr">at March 20, 2022 09:11 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/03/20/postdoc-at-hamburg-university-of-technology-apply-by-april-8-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/03/20/postdoc-at-hamburg-university-of-technology-apply-by-april-8-2022/">Postdoc at Hamburg University of Technology (apply by April 8, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Institute for Algorithms and Complexity at Hamburg University of Technology is seeking a postdoc to work on algorithms for combinatorial optimization and operations research (pay level TV-L 14). These can be approximation algorithms, parameterized algorithms, dynamic algorithms, streaming algorithms, or related. Join our international team to solve some of the most intractable problems!</p>
<p>Website: <a href="https://stellenportal.tuhh.de/jobposting/6816c1195abac5e2e9f2c7f1a72506004fd3ead0">https://stellenportal.tuhh.de/jobposting/6816c1195abac5e2e9f2c7f1a72506004fd3ead0</a><br />
Email: algo@tuhh.de</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/03/20/postdoc-at-hamburg-university-of-technology-apply-by-april-8-2022/"><span class="datestr">at March 20, 2022 08:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/03/20/ukraine-student-postdoc-senior-research-fellows-at-ben-gurion-university-apply-by-december-6-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/03/20/ukraine-student-postdoc-senior-research-fellows-at-ben-gurion-university-apply-by-december-6-2022/">Ukraine  Student/ Postdoc/  Senior Research  Fellows at Ben-Gurion University (apply by December 6, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>If you are a student/scholar affected by the war in Ukraine, BGU offers an emergency scholarship, furthermore, I also have immediately available funds for those who are interested in doing research in Theoretical computer science or Error-Correcting Codes.</p>
<p>Website: <a href="https://www.tfaforms.com/399172?fbclid=IwAR1AW_tvfxoC6yA7EXcptO5tr3SjOM0dAgngz6v6WtlMfHr1ghDwXC0MMt4">https://www.tfaforms.com/399172?fbclid=IwAR1AW_tvfxoC6yA7EXcptO5tr3SjOM0dAgngz6v6WtlMfHr1ghDwXC0MMt4</a>, <a href="https://www.cs.bgu.ac.il/~klim/">https://www.cs.bgu.ac.il/~klim/</a><br />
Email: klimefrem@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/03/20/ukraine-student-postdoc-senior-research-fellows-at-ben-gurion-university-apply-by-december-6-2022/"><span class="datestr">at March 20, 2022 04:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=606">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2022/03/19/tcs-talk-wednesday-march-23-shuichi-hirahara-national-institute-of-informatics-japan/">TCS+ talk: Wednesday, March 23 ‚Äî Shuichi Hirahara, National Institute of Informatics, Japan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, March 23rd at 6:00 PM Eastern Time<strong> (1:00 PM Pacific Time, 13:00 Central European Time, 22:00 UTC [note the unusual time!]).</strong> <a href="https://researchmap.jp/shuichi.hirahara/?lang=english"><strong>Shuichi Hirahara</strong></a> from the National Institute of Informatics, Japan will speak about ‚Äú<em>Excluding PH Pessiland</em>‚Äù (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Registration is <em>not</em> required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: Pessiland is the worst of Impagliazzo‚Äôs five possible worlds: it is a world where NP is hard on average and pseudorandom generators do not exist. Excluding Pessiland (i.e., showing the equivalence between average-case hardness of NP and the existence of pseudorandom generators) is one of the most important open questions in theoretical computer science. In this talk, we propose to consider PH (Polynomial Hierarchy) variants of Impagliazzo‚Äôs five possible worlds. Our main result is to unconditionally rule out PH variants of Pessiland. I will also mention recent progress on excluding PH Heuristica: average-case hardness of PH follows from exponential worst-case hardness of PH.</p>
<p>Based on joint works with Rahul Santhanam.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2022/03/19/tcs-talk-wednesday-march-23-shuichi-hirahara-national-institute-of-informatics-japan/"><span class="datestr">at March 19, 2022 05:22 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=998">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2022/03/17/focs-2022/">FOCS 2022</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>I am on the <a href="https://focs2022.eecs.berkeley.edu/cfp.html">FOCS 2022 </a>Program Committee, and I am overjoyed that the PC meeting will be virtual.  Hopefully, the days are over when the program chair can brush aside all cost-benefit considerations, impose their backyard on far-away scholars who need service items, and then splash their offspring at the welcome party.</p>



<p>I am also overjoyed that we will be implementing double-blind reviews.  This issue has been discussed and ridiculed at length.  Admittedly, it makes it harder to adhere to Leonid Levin‚Äôs 1995 influential <em><a href="https://dl.acm.org/doi/10.1145/202840.606487">STOC Criteria</a></em>.  For example, if a reviewer wanted to trash a paper based on the fact that the authors are not in the position to judge their own work, now they‚Äôll have to check online for talks or preprints to know who the authors are.  Given the volume of reviews, it‚Äôs reasonable to expect that in some cases the reviewer won‚Äôt be able to conclusively exclude that a letter-writer is among the authors.  In such a situation they can resort to writing a very long, thorough, and competent review whose most significant digit is the STOC/FOCS death sentence: <em>weak accept</em>.</p>



<p>No, I actually do have something more constructive to say about this.  I was ‚Äî as they say ‚Äî privileged to serve on many NSF panels.  As an aside, it‚Äôs interesting that there the track-record of the investigators <em>is </em>a key factor in the decision; in fact, according to many including myself, it should carry even more weight, rather than forcing investigators to fill pages with made-up directions most of which won‚Äôt pan out.  But that‚Äôs another story; what is relevant for this post is that each panel begins with a quick ‚Äúde-biasing‚Äù briefing, which I actually enjoy and from which I learnt something.  For example, there‚Äôs a classic experiment where the ratio of females hired as musicians increases if auditions hide the performer behind a tent and make them walk in on carpet so you can‚Äôt tell what shoes they are wearing.  Similar experiments exist that hide names from the folders of applicants, etc.  What I propose is to do a similar thing when reviewing papers.  That is, start with a de-biasing briefing: tell reviewers to ask themselves whether their attitude towards a paper would be different if:</p>



<ol><li>The authors of this paper/previous relevant work were ultra-big shots, or</li><li>The authors of this paper/previous relevant work were hapless nobodies, or</li><li>The proofs in this paper could be simplified dramatically to the point that even I understand them, or</li><li>This result came with a super-complicated proof which I can‚Äôt even begin to follow, or</li></ol>



<p>What other questions would be good to ask?</p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2022/03/17/focs-2022/"><span class="datestr">at March 17, 2022 02:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3213642318918472638">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/03/the-war-and-math.html">The War and Math</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>During the early parts of the cold war of the 20th century, we saw two almost independent developments of computational complexity, in the west and in the then USSR. There was little communication between the two groups, and countless theorems proven twice, most notably the seminal NP-complete papers of Cook and Levin. To understand more, I recommend the two articles about the early days of complexity by <a href="https://doi.org/10.1109/MAHC.1981.10005">Juris Hartmanis</a> and by <a href="https://doi.ieeecomputersociety.org/10.1109/MAHC.1984.10036">Boris Trakhtenbrot</a>.</p><p>Russia's invasion and relentless bombing in Ukraine have quickly separated the east and the west again.¬†</p><p>Our first concern needs to be with Ukraine and its citizens. We hope for a quick end to this aggression and Ukraine remaining a free and democratic country. Ukrainian cities have undergone massive damage, and even in the best possible outcome it will take years if not decades to fully rebuild the country.¬†</p><p>Terry Tao has been <a href="https://terrytao.wordpress.com/2022/03/02/resources-for-displaced-mathematicians/">collecting resources</a> for displaced mathematicians due to the crisis.</p><p>We've cut off ties with Russia institutions. In our world, major events to be held in Russia, including the¬†<a href="https://www.mathunion.org/">International Congress of Mathematics</a>¬†and the¬†<a href="https://logic.pdmi.ras.ru/csr2022/">Computer Science in Russia</a>¬†conference are being moved online. I was invited to workshops in St Petersburg in 2020 and 2021, both cancelled due to Covid, and was looking forward to one in 2022, which if it happens, will now happen without me.¬†</p><p>The music world has has cancelled some stars, most notably¬†<a href="https://www.nytimes.com/2022/03/02/arts/music/ukraine-putin-valery-gergiev-anna-netrebko.html">Valery Gergiev and Anna Netrebko</a>, due to their close ties to Putin. It's rare that we do the same to mathematicians for political reasons though <a href="https://blog.computationalcomplexity.org/2019/06/imus-non-controversial-changing-name-of.html">not unheard of</a>. I suspect most of our colleagues in Russia oppose the war in Ukraine, or would if they had accurate information of what was going on. I have several Russian friends and colleagues including <a href="https://blog.computationalcomplexity.org/2019/06/compressing-in-moscow.html">two I travelled to Moscow in 2019 to honor</a>¬†and would hate to be disconnected from them.</p><p>It's way too early to know how this will all play out. Will we see a quick Russian retreat? Not likely. Will we see a situation that sees a mass migration of Ukranian and Russian mathematicians and computer scientists to Europe and North America, like in the 1990's? Possibly. We will see a repeat of the cold war, disconnected internets and science on both sides happening in isolation? I hope not but we can't rule it out.</p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/03/the-war-and-math.html"><span class="datestr">at March 17, 2022 01:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/03/16/postdoc-at-sandia-national-labs-apply-by-march-31-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/03/16/postdoc-at-sandia-national-labs-apply-by-march-31-2022/">Postdoc at Sandia National Labs (apply by March 31, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Sandia Labs is seeking a postdoc to work on quantum or quantum-inspired classical approximation, sublinear, or streaming algorithms, as part of a DOE-funded collaboration among several national labs and universities. We encourage theoretical computer scientists interested in quantum information but without prior expertise to apply.</p>
<p>Website: <a href="https://far-qc.sandia.gov/job-opportunities/">https://far-qc.sandia.gov/job-opportunities/</a><br />
Email: odparek@sandia.gov</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/03/16/postdoc-at-sandia-national-labs-apply-by-march-31-2022/"><span class="datestr">at March 16, 2022 11:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3121871012533153752">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/03/problem-x-wont-be-solved-in-my-lifetime.html">Problem X won't be solved in MY lifetime- but what about...</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>1) In 1989 on the episde The Royale of Star Trek: The Next Generation (which takes place in the far future)¬† Captain Picard is working on Fermat's last theorem which he says quite explicitly is still open.</p><p>When I saw the episode I asked Larry Washington, a Number Theorist at Univ of MD, when he thought FLT would be solved. He said</p><p>¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† <i>It will be solved within the next 10 years.</i></p><p>And indeed- Wiles solved it in 1993-sort of. There was a flaw in the proof which he fixed in 1994 with the help of his former student Richard Taylor. Wiles published the correction to the flaw in 1995, so we will date it as having been solved in 1995. Larry Washington was correct.¬† And in an episode of Star Trek: Deep Space Nine in 1995 (episode name:Facets) Dax says that a previous host, Tobin Dax, had done the most creative work on FLT since Wiles. Maybe Tobin wrote this limerick:</p><p>A challenge for many long ages</p><p>Had baffled the savants and sages</p><p>Yet at last came the light</p><p>Seems that Fermat was right</p><p>To the margin add 200 pages.</p><p><br /></p><p>I asked Larry W when he thought Riemann would be solved. He said¬†¬†</p><p>¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†<i> In your lifetime but not in mine.</i></p><p>He is about 10 years older than I am and I think we are both in good health. This seems like a rather precise prediction so I am skeptical. But he did get FLT right...</p><p>2) In class I sometimes say things like¬†</p><p><i>I do not think Quantum Computers will factor faster than classical in my lifetime.¬†</i></p><p><i>I do not think P vs NP will be solved in my lifetime.</i></p><p><i>I can imagine P=BPP will be proven in my lifetime. (I said that 10 years ago. I am less imaginative now.)¬†</i></p><p><i>I hope the muffin problem is solved in my lifetime (it was, see¬†<a href="https://arxiv.org/abs/1907.08726">here</a>).</i></p><p>I didn't quite think about the difference in my age and the students until recently when I was working with Ilya Hajiaghayi (Mohammd H's 9 year old son) on cryptography and he said¬†</p><p><i>In your recorded lecture you said you don't think quantum computers will be a threat to cryptography¬† in your lifetime. What about in my lifetime?</i></p><p>Indeed- his lifetime and mine are rather far apart.¬†</p><p>I am reminded that one of the answers to my P vs NP poll made the point that while we have some sense of what will happen in the next 10 years, maybe even 20, math and life can change so much that conjectures beyond that are guesswork. Any¬† prediction for x years from now you should have confidence &lt; 1/ln(x) of it being true.</p><p><i><br /></i></p><p><i><br /></i></p><p><i><br /></i></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/03/problem-x-wont-be-solved-in-my-lifetime.html"><span class="datestr">at March 15, 2022 02:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/03/13/ukraine-student-senior-research-fellows-at-tel-aviv-university-apply-by-june-1-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/03/13/ukraine-student-senior-research-fellows-at-tel-aviv-university-apply-by-june-1-2022/">Ukraine student / senior research fellows at Tel Aviv University (apply by June 1, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Tel Aviv University offers emergency scholarship for research students from Ukraine (see attached). Further, I (Gil Cohen) have immediately available senior / students research fellows in theoretical computer science, coding theory, spectral graph theory, and adjacent mathematical branches.</p>
<p>Website: <a href="https://c1f423b8-ee8e-41b1-a3a7-2cfc865115ec.filesusr.com/ugd/d112fa_4de343bf5b3a410eae40b3853dcef087.pdf">https://c1f423b8-ee8e-41b1-a3a7-2cfc865115ec.filesusr.com/ugd/d112fa_4de343bf5b3a410eae40b3853dcef087.pdf</a><br />
Email: coheng@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/03/13/ukraine-student-senior-research-fellows-at-tel-aviv-university-apply-by-june-1-2022/"><span class="datestr">at March 13, 2022 11:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://thmatters.wordpress.com/?p=1362">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sigact.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://thmatters.wordpress.com/2022/03/11/call-for-nominations-knuth-prize/">Call for nominations: Knuth Prize</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><strong>Deadline: March 31, 2022.</strong></p>



<p>The Donald E. Knuth Prize for outstanding contributions to the foundations of computer science is awarded for major research accomplishments and contributions to the foundations of computer science over an extended period of time. The Prize is awarded annually by the¬†<a href="https://urldefense.com/v3/__https://acm.org/__;!!IBzWLUs!FhlJI8zMLEOwnp6yNYlHlUi7BTqdRYKuBcTGlpuO3upUQ3CeNXk5Et-Ykc0qrYCK$" target="_blank" rel="noreferrer noopener">ACM</a><a href="https://urldefense.com/v3/__https://www.sigact.org/__;!!IBzWLUs!FhlJI8zMLEOwnp6yNYlHlUi7BTqdRYKuBcTGlpuO3upUQ3CeNXk5Et-YkRfc620V$" target="_blank" rel="noreferrer noopener">Special Interest Group on Algorithms and Computation Theory</a>¬†(SIGACT) and the¬†<a href="https://urldefense.com/v3/__https://www.ieee.org/__;!!IBzWLUs!FhlJI8zMLEOwnp6yNYlHlUi7BTqdRYKuBcTGlpuO3upUQ3CeNXk5Et-YkSouJjK-$" target="_blank" rel="noreferrer noopener">IEEE</a><a href="https://urldefense.com/v3/__https://tc.computer.org/tcmf/__;!!IBzWLUs!FhlJI8zMLEOwnp6yNYlHlUi7BTqdRYKuBcTGlpuO3upUQ3CeNXk5Et-YkcMUrupP$" target="_blank" rel="noreferrer noopener">Technical Committee on the Mathematical Foundations of Computing</a>¬†(TCMF).</p>



<p><strong>Nomination Procedure.</strong>¬†Anyone in the Theoretical Computer Science community may nominate a candidate. To do so, please send nominations to¬†<strong><a target="_blank" rel="noreferrer noopener">knuth.prize.2022@gmail.com</a></strong>¬†by¬†<strong>March 31, 2022</strong>. The nomination should state the nominee‚Äôs name, summarize their contributions in one or two pages, provide a CV for the nominee or a pointer to the nominee‚Äôs web page, and give telephone and email contact information for the nominator. Any supporting letters from other members of the community (up to a limit of 5) should be included in the package that the nominator submits. Supporting letters should contain substantial information not in the nomination. Others may endorse the nomination simply by adding their names to the nomination letter. If you have nominated a candidate in past years, you can re-nominate the candidate by sending a message to that effect to the above email address. (You may revise the nominating materials if you so desire.)</p>



<p><strong>Criteria for Selection.</strong>¬†The winner is selected by a Prize Committee consisting of six people appointed by the SIGACT and TCMF Chairs, see below for the composition of the committee.</p>



<p>Previous nominations made or updated in the last 5 years will be considered. Older nominations must be updated for consideration. Note that the Knuth Prize is awarded to a single individual each year. Nominations of groups of researchers will not be considered.</p>



<p>In selecting the Knuth Prize winner, the Committee pays particular attention to a¬†<em>sustained record</em>¬†of high-impact, seminal contributions to the foundations of computer science. The selection may also be based partly on educational accomplishments and contributions such as fundamental textbooks and high-quality students. The award is not given for service to the theoretical computer science community, but service may be included in the citation for a winner if appropriate.</p>



<p>The 2022 prize committee consists of Harold Gabow (U. Colorado), Monika Henzinger (U. Vienna), Kurt Mehlhorn (Max Planck Institute), Dana Randall (Chair, Georgia Tech), Madhu Sudan (Harvard U.), and Andy Yao (Tsinghua U.).</p></div>







<p class="date">
by shuchic <a href="https://thmatters.wordpress.com/2022/03/11/call-for-nominations-knuth-prize/"><span class="datestr">at March 11, 2022 08:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://thmatters.wordpress.com/?p=1357">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sigact.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://thmatters.wordpress.com/2022/03/11/call-for-nominations-godel-prize/">Call for nominations: Godel Prize</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><strong>Deadline for nominations extended to March 31st 2022.</strong></p>



<p><a href="https://urldefense.com/v3/__https://www.sigact.org/prizes/g**Adel.html__;w7Y!!IBzWLUs!EguPGLYWUarkIsJJaLrhDzTZKGjc97LX_FB94NzWMobXbrXlbTKzzky4VPAE-mxj$" target="_blank" rel="noreferrer noopener">https://www.sigact.org/prizes/g%C3%B6del.html</a></p>



<p>The G√∂del Prize for outstanding papers in the area of theoretical computer science is sponsored jointly by the¬†European Association for¬†Theoretical Computer Science¬†(EATCS) and the¬†Special Interest Group on Algorithms and Computation Theory¬†of the¬†Association for¬†Computing Machinery¬†(ACM SIGACT). This award is presented annually, with the presentation taking place alternately at the¬†International¬†Colloquium on Automata, Languages, and Programming¬†(ICALP) and the¬†ACM Symposium on Theory of Computing¬†(STOC). The thirtieth¬†G√∂del Prize will be awarded at the forty-ninth International Colloquium on Automata, Languages and Programming (ICALP), which will be¬†hybrid, happening both physically and virtually. The physical meeting will take place in Paris, France, July 4‚Äì8 2022.</p>



<p>The Prize is named in honor of Kurt G√∂del in recognition of his major contributions to mathematical logic and of his interest, discovered in a¬†letter he wrote to John von Neumann shortly before von Neumann‚Äôs death, in what has become the famous ‚ÄúP versus NP‚Äù question. The Prize¬†includes an award of USD 5,000.</p>



<p><strong>Award Committee</strong></p>



<p>The 2022 Award Committee consists of Samson Abramsky (Chair, University College London), Nikhil Bansal (University of Michigan), Irit¬†Dinur (Weizmann Institute), Anca Muscholl (University of Bordeaux), Ronitt Rubinfeld (Massachusetts Institute of Technology), and David¬†Zuckerman (University of Texas at Austin).</p>



<p><strong>Eligibility</strong></p>



<p>The 2022 Prize rules are given below and they supersede any different interpretation of the generic rule to be found on websites of both¬†SIGACT and EATCS. Any research paper or series of papers by a single author or by a team of authors is deemed eligible if:</p>



<p>‚Ä¢ The main results were not published (in either preliminary or final form) in a journal or conference proceedings before January 1, 2009.</p>



<p>‚Ä¢ The paper was published in a recognized refereed journal no later than December 31, 2021.<br />The research work nominated for the award should be in the area of theoretical computer science. Nominations are encouraged from the¬†broadest spectrum of the theoretical computer science community so as to ensure that potential award winning papers are not overlooked.¬†The Award Committee shall have the ultimate authority to decide whether a particular paper is eligible for the Prize.</p>



<p><strong>Nominations</strong></p>



<p>Nominations for the award should be submitted by email to the Award Committee Chair:¬†<a target="_blank" rel="noreferrer noopener">s.abramsky@ucl.ac.uk</a>. Please make sure that the¬†Subject line of all nominations and related messages begin with ‚ÄúGoedel Prize 2022‚Äù. To be considered, nominations for the 2022 Prize must¬†be received by¬†March 31, 2022.</p>



<p>A nomination package should include:</p>



<p>‚Ä¢ A printable copy (or copies) of the journal paper(s) being nominated, together with a complete citation (or citations) thereof.</p>



<p>‚Ä¢ A statement of the date(s) and venue(s) of the first conference or workshop publication(s) of the nominated work(s) or a statement that¬†no such publication has occurred.</p>



<p>‚Ä¢ A brief summary of the technical content of the paper(s) and a brief explanation of its significance.</p>



<p>‚Ä¢ A support letter or letters signed by at least two members of the scientific community.<br />Additional support letters may also be received and are generally useful. The nominated paper(s) may be in any language. However, if a¬†nominated publication is not in English, the nomination package must include an extended summary written in English.</p>



<p>Those intending to submit a nomination should contact the Award Committee Chair by email well in advance. The Chair will answer¬†questions about eligibility, encourage coordination among different nominators for the same paper(s), and also accept informal proposals of¬†potential nominees or tentative offers to prepare formal nominations. The committee maintains a database of past nominations for eligible¬†papers, but fresh nominations for the same papers (especially if they highlight new evidence of impact) are always welcome.</p></div>







<p class="date">
by shuchic <a href="https://thmatters.wordpress.com/2022/03/11/call-for-nominations-godel-prize/"><span class="datestr">at March 11, 2022 08:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/03/11/postdoc-at-utrecht-university-apply-by-april-4-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/03/11/postdoc-at-utrecht-university-apply-by-april-4-2022/">Postdoc at Utrecht University (apply by April 4, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The ERC starting grant project ‚ÄúFinding Cracks in the Wall of NP-completeness‚Äù of PI Jesper Nederlof aims to improve classical algorithms for NP-hard problems. For example: Can the Bellman-Held-Karp dynamic programming algorithm from the 1960‚Äôs that solves TSP with n cities in 2^n time be improved to 1.9999^n time? Join the project as a postdoc now!</p>
<p>Website: <a href="https://www.uu.nl/en/organisation/working-at-utrecht-university/jobs/postdoc-position-in-algorithmic-theory-10-fte">https://www.uu.nl/en/organisation/working-at-utrecht-university/jobs/postdoc-position-in-algorithmic-theory-10-fte</a><br />
Email: j.nederlof@uu.nl</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/03/11/postdoc-at-utrecht-university-apply-by-april-4-2022/"><span class="datestr">at March 11, 2022 07:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/03/10/postdoc-and-phd-student-in-graph-algorithms-at-tel-aviv-university-apply-by-april-15-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/03/10/postdoc-and-phd-student-in-graph-algorithms-at-tel-aviv-university-apply-by-april-15-2022/">Postdoc and PhD student in graph algorithms at Tel Aviv University (apply by April 15, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Applications are invited for postdoc and PhD positions at the group of Shay Solomon, Tel Aviv University, funded by an ERC starting grant.<br />
The selected candidates will confront fundamental problems in graph algorithms. Applications will be accepted until the positions are filled (flexible start date).<br />
To apply, send a CV and research statement, and arrange for three letters of recommendation.</p>
<p>Website: <a href="https://sites.google.com/site/soloshay/">https://sites.google.com/site/soloshay/</a><br />
Email: shayso@tauex.tau.ac.il</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/03/10/postdoc-and-phd-student-in-graph-algorithms-at-tel-aviv-university-apply-by-april-15-2022/"><span class="datestr">at March 10, 2022 09:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/03/10/differential-privacy-research-scientist-at-harvard-university-apply-by-april-30-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/03/10/differential-privacy-research-scientist-at-harvard-university-apply-by-april-30-2022/">Differential Privacy Research Scientist at Harvard University (apply by April 30, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The OpenDP project at Harvard University seeks to hire a Research Scientist to work with faculty directors Gary King and Salil Vadhan and the OpenDP Community to formulate and advance the scientific goals of OpenDP and solve research problems that are needed for its success.</p>
<p>Website: <a href="https://academicpositions.harvard.edu/postings/11093">https://academicpositions.harvard.edu/postings/11093</a><br />
Email: opendp@g.harvard.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/03/10/differential-privacy-research-scientist-at-harvard-university-apply-by-april-30-2022/"><span class="datestr">at March 10, 2022 08:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=118">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2022/03/10/wednesday-march-16th-2022-eric-tchetgen-tchetgen-from-penn/">Wednesday March 16th 2022 ‚Äî Eric Tchetgen Tchetgen from Penn</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science ‚Äì Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p class="has-text-align-justify">The second <a href="https://sites.google.com/view/dstheory/home" target="_blank" rel="noreferrer noopener">Foundations of Data Science</a> virtual talk of this year will take place on <strong>Wednesday, March 16th</strong> at <strong>11:00 PM Pacific Time</strong> (16:00 Eastern Time, 21:00 Central European Time, 20:00 UTC).¬†<a href="https://statistics.wharton.upenn.edu/profile/ett/">Eric Tchetgen Tchetgen</a> from<strong> The Wharton School, University of Pennsylvania</strong> will speak about ‚ÄúAn Introduction to Proximal Causal Learning<em>‚Äù</em>.</p>



<p><a href="https://sites.google.com/view/dstheory" target="_blank" rel="noreferrer noopener">Please register here to join the virtual talk.</a></p>



<p class="has-text-align-justify"><strong>Abstract</strong>:  A standard assumption for causal inference from observational data is that one has measured a sufficiently rich set of covariates to ensure that within covariates strata, subjects are exchangeable across observed treatment values. Skepticism about the exchangeability assumption in observational studies is often warranted because it hinges on one‚Äôs ability to accurately measure covariates capturing all potential sources of confounding. Realistically, confounding mechanisms can rarely if ever, be learned with certainty from measured covariates. One can therefore only ever hope that covariate measurements are at best proxies of true underlying confounding mechanisms operating in an observational study, thus invalidating causal claims made on basis of standard exchangeability conditions.</p>



<p>Causal learning from proxies is a challenging inverse problem which has to date remained unresolved. In this paper, we introduce a formal potential outcome framework for proximal causal learning, which while explicitly acknowledging covariate measurements as imperfect proxies of confounding mechanisms, offers an opportunity to learn about causal effects in settings where exchangeability based on measured covariates fails. Sufficient conditions for nonparametric identification are given, leading to the proximal g-formula and corresponding proximal g-computation algorithm for estimation, both generalizations of Robins‚Äô foundational g-formula and g-computation algorithm, which account explicitly for bias due to unmeasured confounding. Both point treatment and time-varying treatment settings are considered, and an application of proximal g-computation of causal effects is given for illustration.</p>



<p class="has-text-align-justify">¬†The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2022/03/10/wednesday-march-16th-2022-eric-tchetgen-tchetgen-from-penn/"><span class="datestr">at March 10, 2022 06:57 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=6414">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/von-neumann-entropy/">Playing with positive definite matrices ‚Äì II: entropy edition</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">Symmetric positive semi-definite (PSD) matrices come up in a variety of places in machine learning, statistics, and optimization, and more generally in most domains of applied mathematics. When estimating or optimizing over the set of such matrices, several geometries can be used. The most direct one is to consider PSD matrices as a convex set in the vector space of all symmetric matrices and thus to inherit the Euclidean geometry. Like for the positive orthant (which corresponds to diagonal PSD matrices), this is not natural for a variety of reasons. In this post we will consider <a href="https://en.wikipedia.org/wiki/Bregman_divergence">Bregman divergences</a> to define our notion of geometry.</p>



<p class="justify-text">A first natural and common geometry is to consider PSD matrices as <a href="https://en.wikipedia.org/wiki/Covariance_matrix">covariance matrices</a> (or their inverses, often referred to a precision or concentration matrices). They can thus be seen as parameters of zero-mean Gaussian random vectors, and classical notions from information theory, such as the Kullback-Leibler divergence, can be brought to bear to define a specific geometry. This happens to be equivalent to using the negative log determinant to define the Bregman divergence, and leads to the usual Stein divergence $$ ‚Äì \log \det A + \log \det B \ ‚Äì {\rm tr}[ ‚Äì B^{\, -1} ( A \, ‚Äì B) ] =  -\log \det (AB^{\, -1}) + {\rm tr}( A B^{\, -1}) \ ‚Äì d, $$ which is the <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Kullback%E2%80%93Leibler_divergence">Kullback Leibler divergence</a> between two Gaussian random vectors with zero means and covariance matrices \(B\) and \(A\). </p>



<p class="justify-text">In this blog post, we consider a different but related matrix function, which will be \(A \mapsto {\rm tr }  [ A \log A]\) instead of \(‚Äì {\rm tr} [ \log A ]\). This will have stronger links with the Shannon entropy of discrete random variables [1]. In particular, we will be able to extend two classical notions related to discrete entropy:</p>



<ul class="justify-text"><li>The entropy function \(H(p) = \ ‚Äì \sum_{i=1}^d p_i \log p_i\), defined on the simplex \(\Delta \subset \mathbb{R}^d\) (vectors with non-negative components that sum to one), is concave, while the associated Bregman divergence, here the usual <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leibler (KL) divergence</a>, is $$ \sum_{i=1}^d p_i \log \frac{p_i}{q_i}. $$ It is jointly convex in the vectors \(p\) and \(q\) (see this nice article [<a href="http://carma.newcastle.edu.au/resources/jon/Preprints/Books/CUP/CUPold/MaterialI/bregman.pdf">6</a>] for other jointly convex Bregman divergences). Note that the joint convexity is here a direct application of classical results regarding the convexity of the perspective of a convex function [<a href="http://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">10</a>, section 3.2.6].</li><li>The Fenchel-Legendre conjugate of \(-H\) (with the simplex as domain) is $$\sup_{p \in \Delta} \ \ p^\top z\ ‚Äì \sum_{i=1}^d p_i \log p_i = \log \Big( \sum_{i=1}^d \exp(z_i) \Big).$$ With the positive orthant as the domain, we have $$\sup_{p \in \mathbb{R}_+^d }\ \ p^\top z \ ‚Äì \sum_{i=1}^d p_i \log p_i = \sum_{i=1}^d \exp(z_i-1) .$$ This has many consequences in probabilistic modelling, notably the duality between maximum likelihood and maximum entropy [1, <a href="http://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf">7</a>].</li></ul>



<p class="justify-text">The extension to matrices will have several interesting consequences:</p>



<ul class="justify-text"><li>This defines a Bregman divergence that has interesting applications to mirror descent and smoothing techniques in optimization.</li><li>The non-trivial joint convexity of the associated Bregman divergence will lead to elegant methods for deriving concentration inequalities for symmetric matrices [<a href="http://arxiv.org/pdf/1501.01571">2</a>].</li><li>Application to covariance operators in infinite-dimensional spaces will lead to a whole range of information-theoretic results [<a href="http://arxiv.org/pdf/2202.08545.pdf">5</a>], but this is for next month!</li></ul>



<h2 id="von-neumann-entropy-and-relative-entropy">Von Neumann entropy and relative entropy</h2>



<p class="justify-text">Given a positive semi-definite symmetric matrix \(A \in \mathcal{S}_d\), the von Neumann entropy is defined as $$H(A) =\ ‚Äì {\rm tr} [ A \log A],$$ as the trace of the matrix function \(‚Äì A \log A\), or equivalently as \(‚Äì \sum_{i=1}^d \! \lambda_i \log \lambda_i\) where \(\lambda_1,\dots,\lambda_d \geqslant 0\) are the \(d\) eigenvalues of \(A\) (note that we can have a zero eigenvalue and still be finite). </p>



<p class="justify-text">From <a href="https://francisbach.com/matrix-monotony-and-convexity/">last month blog post</a>, we get that \(H\) is concave. We can then naturally define on the <a href="https://en.wikipedia.org/wiki/Spectrahedron">spectrahedron</a> (set of positive semi-definite matrices with unit trace), the <em>relative entropy</em>, which is the Bregman divergence associated with the function \(A \mapsto {\rm tr} [ A \log A]\), whose gradient is \(\log A + I \), leading to: $$D(A\| B) = {\rm tr} [ A \log A ] \, ‚Äì {\rm tr} [ B \log B] \, ‚Äì {\rm tr} \big[ ( A \, ‚Äì B) ( \log B + I ) \big],$$ which is equal to $$ D(A\| B) = {\rm tr} \big[ A ( \log A \,  ‚Äì \log B ) \big] ‚Äì {\rm tr}(A) + {\rm tr}(B).$$ This time we have to be careful to avoid infinite values: it is finite if and only if the null space of \(B\) is included in the one of \(A\), or, equivalently if \(B^{\, -1/2} A^{1/2} B^{\, -1/2}\) is finite. Note that when \(A\) and \(B\) have unit traces, the term in \(‚Äì {\rm tr}(A) + {\rm tr}(B)\) goes away.</p>



<p class="justify-text">As a Bregman divergence of a strictly convex function, \(D(A\|B)\) is non-negative and equal to zero if and only if \(A = B\).</p>



<p class="justify-text">When \(A = {\rm Diag}(p)\) and \(B = {\rm Diag}(q)\) are diagonal, we get $$D(A\| B) = \sum_{i=1}^d \Big\{ p_i \log \frac{p_i}{q_i} \, ‚Äì p_i + q_i \Big\},$$ and we recover the traditional relative entropy. </p>



<p class="justify-text">Please refrain from writing \(D(A\| B) = {\rm tr} \big[ A \log ( A B^{\,-1} ) \big] ‚Äì {\rm tr}(A) + {\rm tr}(B)\), which is not true unless \(A\) and \(B\) commute. See however below how using Kronecker products can make a similar equality always true.</p>



<h2>A geometry on matrices</h2>



<p class="justify-text">The von Neumann relative entropy (also referred to as matrix KL divergence) defines a new notion of geometry, which is different from the Euclidean geometry based on the squared Frobenius norm \(\| A ‚Äì B \|_{\rm F}^2 = {\rm tr} [ ( A -B)^2]\). Below, we plot and compare balls for these various geometries. We first consider Euclidean balls, defined as $$\mbox{ Euclidean : } \  \big\{ B \in \mathcal{S}_d,  \| A ‚Äì B \|_{\rm F}^2 \leqslant r \big\},$$ as well as the two KL balls (one for each side of the KL divergence): $$ \mbox{ Left KL : } \ \big\{ B \in \mathcal{S}_d,  D(B\|A)  \leqslant r \big\},$$ $$\mbox{ Right KL : } \big\{ B \in \mathcal{S}_d,  D(A\|B) \leqslant r \big\}.$$</p>



<p class="justify-text"><strong>Diagonal matrices with unit trace (isomorphic to the simplex).</strong> We consider diagonal matrices \(A = {\rm Diag}(p)\) with \(p\) in the simplex. We compare the three balls below (in yellow), with \(r = 1/16\) for the three of them.</p>



<div class="wp-block-image"><figure class="aligncenter size-full"><img width="1480" alt="" src="https://francisbach.com/wp-content/uploads/2022/03/simplex_geom.gif" class="wp-image-6794" height="340" /></figure></div>



<p class="justify-text">While the Euclidean balls have the same shapes at all positions, the KL balls adapt their shapes to their centers.</p>



<p class="justify-text"><strong>Unit trace PSD matrices in dimension two (isomorphic to the Euclidean ball).</strong> We consider matrices \(A = \Big( \begin{array}{cc} x &amp; y \\[-.15cm] y &amp; \!\!\!1-x\!\end{array} \Big)\) with the PSD constraint being equivalent to \(y^2 \leqslant x(1-x)\), that is, \((x,y)\) is in the disk of center \((\frac{1}{2},0)\) and radius \(\frac{1}{2}\). We compare the three balls below (in yellow), with \(r = \frac{1}{32}\) for the Euclidean ball, and \(r =\frac{1}{16}\) for the KL balls, with a similar difference between geometries.</p>



<div class="wp-block-image"><figure class="aligncenter size-full"><img width="1486" alt="" src="https://francisbach.com/wp-content/uploads/2022/03/ellipsoid_geom.gif" class="wp-image-6798" height="422" /></figure></div>



<h2 id="pinsker-inequality-1">Pinsker inequality</h2>



<p class="justify-text">In order to use the relative entropy within mirror descent, we need an extension of the <a href="https://en.wikipedia.org/wiki/Pinsker%27s_inequality">Pinsker inequality</a> $$\sum_{i=1}^d p_i \log \frac{p_i}{q_i} \geqslant \frac{1}{2} \Big(\sum_{i=1}^d | p_i ‚Äì q_i| \Big)^2,$$ which is, in optimization terms, the \(1\)-strong-concavity of the entropy with respect to the \(\ell_1\)-norm. It turns out that the natural extension is true, that is, for PSD matrices with unit trace, $$D(A\| B) =  {\rm tr} \big[ A ( \log A\,   ‚Äì \log B ) \big] \geqslant \frac{1}{2} \| A \ ‚Äì B \|_\ast^2,$$ where \(\| \cdot \|_\ast\) denotes the trace norm, a.k.a. the nuclear norm, that is, the \(\ell_1\)-norm of eigenvalues. See [<a href="http://www.cs.cmu.edu/~yaoliang/mynotes/sc.pdf">3</a>] for a nice and simple proof.</p>



<p class="justify-text">This allows to use the relative entropy when optimizing over the spectrahedron using mirror descent [<a href="https://www2.isye.gatech.edu/~nemirovs/MLOptChapterI.pdf">11</a>]. Note that the diameter \(\sup_{A} {\rm tr} [ A \log A] \ ‚Äì   \inf_{A} {\rm tr} [ A \log A] \) over PSD matrices with unit trace is equal to \(\log d\). Using duality, the relative entropy can also be used for smoothing the maximal eigenvalue of a matrix [<a href="https://link.springer.com/content/pdf/10.1007/s10107-006-0001-8.pdf">12</a>].</p>



<h2 id="joint-convexity-of-matrix-relative-entropy">Joint convexity of matrix relative entropy</h2>



<p class="justify-text">While the convexity with respect to \(A\) is straightforward, the relative entropy \(D(A \| B)\) is also <em>jointly</em> convex in \((A,B)\), which is crucial in the developments below. There exist several proofs, but one is particularly elegant [<a href="https://arxiv.org/pdf/quant-ph/0604206.pdf">4</a>] and will use the matrix-monotonicity that I presented in the <a href="https://francisbach.com/matrix-monotony-and-convexity/">last post</a>.</p>



<p class="justify-text"><strong>Kronecker products.</strong> We need to make a clever use of <a href="https://en.wikipedia.org/wiki/Kronecker_product">Kronecker products</a>. Given two matrices \(A\) and \(B\) of any sizes, \(A \otimes B\) is the matrix defined by blocks \(A_{ij} B\), thus of size the products of the number of rows and columns of \(A\) and \(B\). In the derivations below, both \(A\) and \(B\) will be of size \(d \times d\), and thus \(A \otimes B\) is then of size \(d^2 \times d^2\). </p>



<p class="justify-text">Kronecker products have very nice properties (they would probably deserve their own post!), such as (when dimensions are compatible), \((A \otimes B) ( C\otimes D) = (AC) \otimes (BD)\) or \((A \otimes B)^{\, -1} = A^{-1} \otimes B^{\, -1}\). What we will leverage is their use in characterizing linear operations on matrices. For a rectangular matrix \(X \in \mathbb{R}^{d_1 \times d_2}\), \({\rm vec}(X) \in \mathbb{R}^{d_1 d_2 \times 1}\) denotes the <em>vectorization</em> of \(X\) obtained by stacking all columns of \(X\) one after the other in a single column vector. Then, we have $$ (B^\top \otimes A){\rm vec}(X) = {\rm vec}(AXB)$$ for any \(A, X, B\) with compatible dimensions. </p>



<p class="justify-text"><strong>Reformulation of relative entropy.</strong> We can now use Kronecker products to get: $$ {\rm tr} [A \log A] = {\rm tr}[ A^{1/2}( \log A) A^{1/2} ] = {\rm vec}(A^{1/2})^\top [ I \otimes \log A ] {\rm vec}(A^{1/2}),$$ and $$ {\rm tr} [A \log B] = {\rm tr} [ A^{1/2}( \log B) A^{1/2} ] = {\rm vec}(A^{1/2})^\top [    \log B \otimes I  ] {\rm vec}(A^{1/2}),$$ leading to $$D(A\|B) = {\rm vec}(A^{1/2})^\top \big[ I \otimes \log A \ ‚Äì \log B \otimes I \big]  {\rm vec}(A^{1/2}) \, ‚Äì {\rm tr}(A) + {\rm tr}(B).$$ The key element is then to use the identity for PSD matrices $$\log(C \otimes D) = \log C \otimes I + I \otimes \log D$$ (which can be shown using the fact that eigenvalue decomposition of a Kronecker product of symmetric matrices can be obtained from the decompositions of the factors), leading to $$D(A\|B) = {\rm vec}(A^{1/2})^\top \big[ \log ( I \otimes A) \ ‚Äì \log (B \otimes I) \big]  {\rm vec}(A^{1/2})\, ‚Äì {\rm tr}(A) + {\rm tr}(B).$$ It seems we have not achieved much, but now, the matrices \( I \otimes A\) and \(B \otimes I\) commute! Therefore, we can now write $$D(A\|B) = {\rm vec}(A^{1/2})^\top \Big[ \log \big( ( I \otimes A) (B \otimes I)^{-1} \big) \Big]  {\rm vec}(A^{1/2})- {\rm tr}(A) + {\rm tr}(B).$$ This can then be rewritten as $$D(A\|B)  =\, ‚Äì {\rm vec}(A^{1/2})^\top \big[ \log ( B \otimes A^{-1}) \big] {\rm vec}(A^{1/2}) \, ‚Äì {\rm tr}(A) + {\rm tr}(B).$$ </p>



<p class="justify-text"><strong>Using matrix-convexity.</strong> We can now use the matrix-convexity of \(\lambda \mapsto \ ‚Äì \log \lambda\), and the representation from <a href="http://francisbach.com/matrix-monotony-and-convexity/">last post</a> as $$ -\log \lambda = 1-\lambda + (\lambda-1)^2 \int_0^{+\infty} \!\!\frac{1}{\lambda +u} \frac{du}{(1+u)^2},$$ leading to $$D(A\|B) =  \int_0^{+\infty}  {\rm vec}(A^{1/2})^\top \Big[ ( B\otimes A^{-1} ‚Äì I)^2 (B\otimes A^{-1} + u I)^{-1} \Big]  {\rm vec}(A^{1/2}) \frac{du}{(1+u)^2},$$ and then, by using  \(( B\otimes A^{-1} ‚Äì I) ( I \otimes A^{1/2})  {\rm vec}(A^{1/2})  =  {\rm vec}(B-A)\), $$D(A\|B) =   \int_0^{+\infty}  {\rm vec}(A-B)^\top  (B\otimes I+ u I \otimes A)^{-1}   {\rm vec}(A-B)   \frac{du}{(1+u)^2}.$$ We can then use the joint convexity of the function \((C,D) \mapsto {\rm tr} \big[  C^\top D^{-1} C \big] \) to get the  joint convexity of \(D(A\|B)\).</p>



<p class="justify-text">This extends to all matrix-convex functions beyond the logarithm, such as \(\lambda \mapsto (\lambda-1)^2\), \(\lambda \mapsto 1 \, ‚Äì \lambda^{1/2}\) or \(\displaystyle \lambda \mapsto \frac{1}{\lambda}(\lambda-1)^2\), leading to other <a href="https://en.wikipedia.org/wiki/F-divergence">\(f\)-divergences</a> between matrices. See [<a href="http://arxiv.org/pdf/2202.08545.pdf">5</a>, Appendix A.4] for details.</p>



<h2 id="lieb-concavity-theorems">Lieb concavity theorems</h2>



<p class="justify-text">Now that \((A,B) \mapsto D(A\|B)\) is jointly convex on all PSD matrices (not only unit trace), for any fixed symmetric matrix \(M\), the function $$ \varphi: B \mapsto \sup_{ A \succcurlyeq 0} \ {\rm tr}(AM) + {\rm tr}(A) \ ‚Äì D(A\|B), $$ as the partial maximization (with respect to \(B\)) of a jointly concave function of \((A,B)\), is concave. We can maximize in closed form with respect to \(A\) by computing the gradient \(M \ ‚Äì \log A  + \log B\), leading to \(A = \exp( M + \log B)\), and $$\varphi(B) = {\rm tr} \exp ( M + \log B),$$ which defines a concave function. This is one version of the classical Lieb concavity theorems [<a href="http://sciencedirect.com/science/article/pii/000187087390011X/pdf?md5=09b7ceaa543358cef395fa6af8de62aa&amp;pid=1-s2.0-000187087390011X-main.pdf">8</a>]. </p>



<p class="justify-text">As a consequence, we can obtain the ‚Äúsubadditivity of the matrix cumulant generative function‚Äù. That is, for any random <em>independent</em> symmetric matrices \(X_1,\dots,X_n\) of the same sizes, $$ \mathbb{E} \Big[  {\rm tr} \exp \big(   \sum_{i=1}^n X_i \big) \Big] \leqslant {\rm tr} \exp \big( \sum_{i=1}^n \log \big( \mathbb{E} [ \exp( X_i ) ] \big),$$ which can be shown by recursion using Jensen‚Äôs inequality for the function \(\varphi\) above with a well-chosen matrix \(M\). See [<a href="http://arxiv.org/pdf/1501.01571">2</a>] for details.</p>



<p class="justify-text">We are now ready to consider concentration inequalities for random matrices.</p>



<h2 id="application-to-concentration-inequalities">Application to concentration inequalities</h2>



<p class="justify-text">We consider random independent random matrices \(X_1,\dots,X_n\) such that \(\mathbb{E} [ X_i ] = 0\) for all \(i \in \{1,\dots,n\}\). Let \(S = \frac{1}{n} \sum_{i=1}^n \! X_i\) (which has zero mean). Our goal is to provide upper bounds on \(\mathbb{E} \big[ \lambda_{\max}(S) \big]\) and \(\mathbb{P} \big[ \lambda_{\max}(S) \geqslant t \big]\). We follow the outstanding work and presentation of <a href="http://users.cms.caltech.edu/~jtropp/">Joel Tropp</a> [<a href="http://arxiv.org/pdf/1501.01571">2</a>]: using the proper matrix tools presented above, we can get for matrix-valued random variables the almost exact same proofs of the usual concentration inequalities for real-valued random <em>variables</em>, such as <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">Hoeffding</a>, <a href="https://en.wikipedia.org/wiki/Bernstein_inequalities_(probability_theory)">Bernstein</a>, <a href="https://en.wikipedia.org/wiki/Azuma%27s_inequality">Azuma</a>, or Mac-Diarmid inequalities. </p>



<p class="justify-text"><strong>Expectation. </strong>We have, for any \(u &gt; 0\), by homogeneity of the largest eigenvalue: $$  \mathbb{E} \big[ \lambda_{\max}(S) \big] = \frac{1}{u}  \mathbb{E} \big[ \lambda_{\max}(uS) \big] =  \frac{1}{u}  \mathbb{E} \big[  \log \exp ( \lambda_{\max}(uS) ) \big].$$ We can then use Jensen‚Äôs inequality for the logarithm to get (together with the fact the largest eigenvalue of the exponential is the exponential of the largest eigenvalue): $$  \mathbb{E} \big[ \lambda_{\max}(S) \big] \leqslant \frac{1}{u}  \log \mathbb{E} \big[ \exp ( \lambda_{\max}(uS) ) \big]=\frac{1}{u}  \log \mathbb{E} \big[ ( \lambda_{\max}( \exp ( uS) ) \big], $$ and finally that for a PSD matrix the trace is larger than the largest eigenvalue, to get $$  \mathbb{E} \big[ \lambda_{\max}(S) \big] \leqslant \frac{1}{u}  \log \mathbb{E} \big[ {\rm tr} \exp (  uS ) \big].$$ Experts in concentration inequalities have probably recognized the usual argument to bound the expectation of the maximum of \(d\) random <em>real-valued</em> (non necessarily independent) variables \(X_1,\dots,X_d\), as $$ \mathbb{E} \big[ \max \{Y_1,\dots,Y_d\} \big] \leqslant \frac{1}{u} \log \mathbb{E} \big[ \exp(uY_1)+ \cdots + \exp(u Y_d) \big].$$</p>



<p class="justify-text"><strong>Tail bound.</strong> We have, using Markov‚Äôs inequality, for any \(u &gt; 0\): $$\mathbb{P} \big[ \lambda_{\max}(S) \geqslant t \big] = \mathbb{P} \big[ \exp(\lambda_{\max}(uS)) \geqslant \exp(ut) \big] \leqslant e^{-ut} \mathbb{E} \big[ \exp ( \lambda_{\max}(uS) ) \big],$$ thus leading to $$\mathbb{P} \big[ \lambda_{\max}(S) \geqslant t \big] \leqslant e^{-ut} \mathbb{E} \big[ {\rm tr} \exp (  uS ) \big].$$ The same experts have recognized $$ \mathbb{P} \big[ \max \{Y_1,\dots,Y_d\} \geqslant t \big] \leqslant  e^{-ut} \mathbb{E} \big[ \exp(uY_1)+ \cdots + \exp(u Y_d) \big].$$ </p>



<p class="justify-text">We can now consider random matrices such that \(a I \preccurlyeq X_i \preccurlyeq b I\) almost surely for all \(i=1,\dots,n\) (and still with zero mean). We first need a lemma whose proof is given at the end of the post, with the exact same proof as for the uni-dimensional case.</p>



<p class="justify-text"><strong>Lemma.</strong> If \(\mathbb{E} [ X ] = 0\) and \(a I \preccurlyeq X \preccurlyeq b I\) almost surely, then for all \(u &gt; 0\), $$ \mathbb{E} \big[ \exp( u X) \big] \preccurlyeq \exp\big( \frac{u^2}{8}(b-a)^2 \big) I.$$ </p>



<p class="justify-text">Together with the consequence of Lieb‚Äôs result presented above and the matrix-monotonicity of the logarithm, we get: $$ \mathbb{E} \big[ {\rm tr} \exp (  uS ) \big] \leqslant {\rm tr} \exp \Big( n \big( \frac{u^2}{8n^2}(b-a)^2 I \big) \Big) = d  \exp \big(   \frac{u^2}{8n}(b-a)^2 \big).$$ We can then get immediately, by optimizing with respect to \(u\) (with optimal value \(u=4 n t /(b-a)^2\)), $$\mathbb{P} \big[ \lambda_{\max}(S) \geqslant t \big] \leqslant d \exp\big( ‚Äì \frac{2 n t^2 }{(b-a)^2} \big),$$ and, with optimal value \(u= {2 \sqrt{2 n \log d }}/ { (b-a) }\): $$  \mathbb{E} \big[ \lambda_{\max}(S) \big] \leqslant \frac{b-a}{\sqrt{2n}} \sqrt{ \log d}.$$</p>



<p class="justify-text">Note that these are the exact same results as when \(d=1\)! </p>



<p class="justify-text"><strong>Extensions.</strong> As nicely detailed in [<a href="https://arxiv.org/pdf/1501.01571">2</a>], there are extensions to all classical concentration inequalities. Moreover, this can be applied to rectangular matrices, where the largest eigenvalue is replaced by the largest singular value. Finally, for those who like to work in infinite dimensions and cannot bear the explicit dependence in dimension, it is possible to derive results that depend only on a well-defined notion of ‚Äúintrinsic dimension‚Äù (the associated results can then be used for the study of kernel methods, see [<a href="http://di.ens.fr/~fbach/ltfp_book.pdf">13</a>, chapter 9]).</p>



<h2 id="conclusion">Conclusion</h2>



<p class="justify-text">Now that we know everything about the matrix relative entropy and its convexity properties, we will explore next month a new link between eigenvalues of covariance operators and the regular Shannon entropy of the generating probability distribution [<a href="http://arxiv.org/pdf/2202.08545.pdf">5</a>], thus mixing my favorite recent mathematical topic with my probably all-time favorite one: positive definite kernels!</p>



<h2 id="references">References</h2>



<p class="justify-text">[1] Thomas M. Cover and Joy A. Thomas. <em>Elements of Information Theory</em>. John Wiley &amp; Sons, 1999.<br />[2] Joel A. Tropp.¬†<a href="https://arxiv.org/pdf/1501.01571">An introduction to matrix concentration inequalities</a>.¬†<em>Foundations and Trends in Machine Learning</em>, 8(1-2):1‚Äì230, 2015.<br />[3] Yao-Liang Yu. <a href="http://www.cs.cmu.edu/~yaoliang/mynotes/sc.pdf">The strong convexity of von Neumann‚Äôs entropy</a>. Unpublished note, 2013.<br />[4] Mary Beth Ruskai. <a href="https://arxiv.org/pdf/quant-ph/0604206.pdf">Another short and </a><a href="https://arxiv.org/pdf/quant-ph/0604206.pdf" target="_blank" rel="noreferrer noopener">elementary</a><a href="https://arxiv.org/pdf/quant-ph/0604206.pdf"> proof of strong subadditivity of quantum entropy</a>. Reports on Mathematical Physics, 60(1):1‚Äì12, 2007.<br />[5] Francis Bach.¬†<a href="https://arxiv.org/pdf/2202.08545.pdf">Information Theory with Kernel Methods</a>. Technical Report, arXiv-2202.08545, 2022.<br />[6] Heinz H. Bauschke, and Jonathan M. Borwein. <a href="https://carma.newcastle.edu.au/resources/jon/Preprints/Books/CUP/CUPold/MaterialI/bregman.pdf">Joint and separate convexity of the Bregman distance</a>.¬†<em>Studies in Computational Mathematics</em>, 8:23-36, 2001.<br />[7] Martin J. Wainwright and Michael I. Jordan. <a href="https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf">Graphical Models, Exponential Families, and Variational Inference</a>. <em>Foundations and Trends in Machine Learning</em>, 1 (1‚Äì2):1‚Äì305, 2008. <br />[8] Elliott H. Lieb. <a href="https://www.sciencedirect.com/science/article/pii/000187087390011X/pdf?md5=09b7ceaa543358cef395fa6af8de62aa&amp;pid=1-s2.0-000187087390011X-main.pdf">Convex trace functions and the Wigner-Yanase-Dyson conjecture</a>.<br /><em>Advances in Mathematics</em>, 11(3):267‚Äì288, 1973.<br />[9] St√©phane Boucheron, Gabor Lugosi, and Pascal Massart. <em>Concentration Inequalities: A Nonasymptotic Theory of Independence</em>. Oxford University Press, 2013. <br />[10] Stephen Boyd, Lieven Vandenberghe. <em><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a></em>. Cambridge University Press, 2004.<br />[11] Anatoli Juditsky, Arkadi Nemirovski. <a href="http://www2.isye.gatech.edu/~nemirovs/MLOptChapterI.pdf">First order methods for nonsmooth convex large-scale optimization, i: general purpose methods</a>.¬†<em>Optimization for Machine Learning</em>, 121-148, 2012.<br />[12] Yurii Nesterov. <a href="https://link.springer.com/content/pdf/10.1007/s10107-006-0001-8.pdf">Smoothing technique and its applications in Semidefinite Optimization</a>. <em>Mathematical Programming</em>,¬†110(2):245-259, 2007.<br />[13] Francis Bach. <a href="https://www.di.ens.fr/~fbach/ltfp_book.pdf">Learning Theory From First Principles</a>. Book draft, 2021.</p>



<h2 id="proof-of-lemma-on-matrix-cumulant-generating-functions">Proof of lemma on matrix cumulant generating functions</h2>



<p class="justify-text"><strong>Lemma.</strong> If \(\mathbb{E} [ X ] = 0\) and \(a I \preccurlyeq X \preccurlyeq b I\) almost surely, then for all \(u &gt; 0\), $$ \mathbb{E} \big[ \exp( u X) \big] \preccurlyeq \exp\big( \frac{u^2}{8}(b-a)^2 I \big) =  \exp\big( \frac{u^2}{8}(b-a)^2 \big) I .$$ Note that we have \(a \leqslant 0 \leqslant b\).</p>



<p class="justify-text"><strong>Proof</strong>. We follow the standard proof [9] for real-valued random variables, simply transferred to the matrix case. On the interval \([a,b]\), by convexity of the exponential function $$ \exp(u\lambda) \leqslant \exp(ub) \frac{\lambda-a}{b-a} + \exp(ua) \frac{b \, ‚Äì \lambda}{b-a}.$$ Thus (check why we can do this), almost surely: $$ \exp(uX) \preccurlyeq \exp(ub) \frac{X-a I }{b-a} + \exp(ua) \frac{bI  ‚Äì X}{b-a},$$ leading to \(\displaystyle \mathbb{E} [ \exp(uX) ]  \preccurlyeq   \frac{b \exp(ua)\ ‚Äì a \exp(ub)}{b-a} I\). We can then show that \(\displaystyle \frac{b \exp(ua)\ ‚Äì a \exp(ub)}{b-a}  \leqslant \exp \Big(  \frac{u^2}{8}(b-a)^2 \Big)\) to conclude the proof. We thus need to show that \(\displaystyle \psi(u) = \log \Big( \frac{b \exp(ua)\ ‚Äì a \exp(ub)}{b-a}  \Big) \leqslant \frac{u^2}{8}(b-a)^2\) for all \(u \geqslant 0\). We can simply express \(\psi(u)\) as a rescaled logistic function as $$\psi(u) =  ua + \log \frac{b}{b-a} + \log\Big[ 1 + \exp\Big( u(b-a) + \log\frac{-a}{b-a} \Big) \Big].$$ We have \(\psi(0) = 0\), and $$\psi'(u) =  \frac{ba \exp(ua)\ ‚Äì ab \exp(ub)}{b \exp(ua)\ ‚Äì a \exp(ub)}, $$ thus \(\psi'(0) = 0\), and through the link with the logistic function, the second-order derivarive is less than \((b-a)^2 / 4\). This leads to the desired result by Taylor‚Äôs formula.</p>



<p></p>



<p><br /></p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/von-neumann-entropy/"><span class="datestr">at March 07, 2022 09:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
