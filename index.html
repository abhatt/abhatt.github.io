<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at November 10, 2020 11:39 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=20368">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2020/11/10/benjamini-and-mossel-2000-account-sensitivity-of-voting-schemes-to-mistakes-and-manipulations/">Benjamini and Mossel’s 2000 Account: Sensitivity of Voting Schemes to Mistakes and Manipulations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><span style="color: #0000ff;"><em><a href="https://gilkalai.files.wordpress.com/2020/11/ibem.png"><img width="640" alt="" src="https://gilkalai.files.wordpress.com/2020/11/ibem.png?w=640&amp;h=332" class="alignnone size-full wp-image-20373" height="332" /></a></em></span></p>
<p><em><span style="color: #0000ff;">Here is a popular account by Itai Benjamini and Elchanan Mossel from 2000 written shortly after the 2000 US presidential election. Elchanan and Itai kindly agreed that I will publish it here,  for the first time, 20 years later!  I left the documents as is so affiliations and email addresses are no longer valid. </span></em></p>
<p><span style="color: #0000ff;">__________________________________________________</span></p>
<p>This is a popular report by Dr. I. Benjamini and Dr. E. Mossel from<br />
Microsoft Research at Redmond on some recent mathematical<br />
studies which are relevant to the U.S. presidential elections.</p>
<p>For More information contact:<br />
Dr. Itai Benjamini, Microsoft Research<br />
e-mail: itai@microsoft.com<br />
Tel: 1-425-7057024</p>
<h2>Sensitivity of voting schemes to mistakes and manipulations</h2>
<p>Do the results of the recent election accurately reflect public opinion?<br />
Or are they the result of some minor local manipulations, and a few random<br />
mistakes such as voters confusion, or counting errors.</p>
<p>There are many potential schemes for electing a president, each with its<br />
own features. One important feature of a scheme is the agreement of the<br />
actual outcome of the election with the outcome of the ideal<br />
implementation of the election which disregards potential mistakes and<br />
mischief. If it is probable that the ideal and the actual outcomes differ,<br />
then frequently the outcome of the election will be in doubt.</p>
<p>In recent years, some mathematical efforts have been devoted to trying to<br />
understand which procedures are “stable” to these potential perturbations<br />
and which are “sensitive”. While the motivation for these studies came<br />
from questions in probability and statistical physics, These mathematical<br />
studies can shed some light on the current presidential election. In<br />
particular, a recent paper by Professors Itai Benjamini and Oded Schramm<br />
from Microsoft research and the Weizmann Institute and Gil Kalai from the<br />
Hebrew University of Jerusalem, soon to be published in the prestigious<br />
French journal Publication I.H.E.S., suggests that the “popular vote”<br />
method is much more stable against mistakes than other voting method.</p>
<p>One example of a decision procedure that we encounter in nature is the<br />
neuron. The neuron has to make its decisions based on many inputs; each<br />
represents the strength of electrical currents at the synapses entering<br />
the neuron. Based on these inputs, the neuron should decide whether to<br />
fire its axon going out of the neuron. It is quite likely that some small<br />
perturbations occur at each of the input synapses. It is therefore<br />
expected that the decision procedure of the neuron will be “stable” to<br />
these perturbations.</p>
<p>Experimental evidence indicates that neurons may be modeled as the<br />
following simple procedure. The neuron looks at some weighted sum of its<br />
inputs and makes a decision according to how large this sum is . If the<br />
sum is large, then one decision is made , while if it is small another<br />
decision is made .</p>
<p>The counterpart of this procedure for elections would be counting the<br />
votes and deciding according to the majority of the votes. Mathematical<br />
proofs have been given to show that this decision procedure is the most<br />
stable among all decision procedures.</p>
<p>More complicated neural networks consist of a hierarchy of neurons where<br />
the outputs of neurons in lower levels of the hierarchy are the inputs to<br />
neurons in the higher levels. Some of these networks have been proven to<br />
be much more sensitive to noise than a single neuron.</p>
<p>The counterpart of neural network in the political system may sound<br />
familiar: The voters are divided into groups (states) and each group<br />
chooses its candidate based on the majority vote. Then some weighted<br />
majority of the votes of the states is taken to be the elected president.</p>
<p>The mathematical theorems imply that this system is much more sensitive to<br />
minor mistakes.</p>
<p>If the overwhelming majority of the population is voting for the same<br />
candidate then it doesn’t really matter which voting scheme we use. All of<br />
the natural schemes are “stable”. On the other hand, when the population<br />
is almost evenly split, different schemes behave differently.</p>
<p>The mathematical reasoning behind the stability of the majority vote goes<br />
back to Abraham DeMoivre and Pierre-Simon Laplace (18th century). This<br />
reasoning implies, for example, that in a population of 98,221,798 votes,<br />
if there is a bias of 222,880 for one candidate, then this candidate will<br />
be chosen, even if for each voter there is a small chance that his or her<br />
vote is not counted or counted wrongly (the results of the last election<br />
as of 11/13/00). As long as mistakes for both sides are equally likely,<br />
the result will correctly reflect the bias in the public opinion.</p>
<p>Thus, in this presidential election while the gap between the two<br />
candidates is only 0.2% of the votes it is still large enough so that the<br />
outcome is immune even if a fairly large percent (10% for example) of the<br />
votes were counted mistakenly (assuming the mistakes were random and<br />
independent.)</p>
<p>On the other hand the gap of a 388 votes among the 6,000,000 million votes<br />
in Florida (about 0.05% of the votes) may well be too small to overcome<br />
the effect of random mistakes in counting the votes, even if the chance<br />
for a mistake is fairly small (1%, say). It can well be argued that just<br />
because of the (unavoidable) mistakes in counting the votes in Florida<br />
(putting aside all other controversies surrounding the vote there) we will<br />
never be able to know who got the majority of votes among the voters of<br />
Florida. To understand why the picture is so different as far as the<br />
popular vote is concerns in the entire nation and the popular vote in the<br />
state of Florida we should note that the stability against mistakes<br />
increases dramatically as the number of voters rise. (And also, of course,<br />
as the gap between the candidates rise.)</p>
<p>The discrepancy between our ability to call the winner in the popular vote<br />
and disability to call the winner in the electoral college (which is the<br />
winner of the election according to the constitution) is not unexpected.</p>
<p>The new results by Benjamini, Schramm and Kalai show that for many models<br />
majority is the scheme which is least sensitive to noise. In these models<br />
it is assumed that voters make their decisions independently and the<br />
mistakes are symmetric and independent for different voters. It is shown<br />
then that for models resembling the current voting scheme in the U.S.A, if<br />
the population is almost evenly split, the scheme is much more sensitive<br />
to noise than the majority scheme. In particular a tiny fraction of<br />
mistakes is very likely to reverse the ideal outcome of the election.<br />
Moreover, if the elections are almost balanced, then the results are too<br />
close to call.</p>
<p>If the outcomes do not show significant bias towards one of the candidates<br />
then in the “popular-vote” method the probability of random independent<br />
mistakes which effect one in every A votes has a chance of one in the<br />
square-root of A to switch the outcome of the elections. In a method like<br />
the “electoral college” the chance is increased to something like one in<br />
the fourth root of A.</p>
<p>If the popular vote is significantly tilted towards one of the candidates<br />
than the effect of mistakes becomes smaller for all voting methods but<br />
much more so for the popular-vote method.</p>
<p>The study of the sensitivity of voting procedures to small errors for such<br />
models is based on mathematical tools from “harmonic analysis”, and<br />
provides a classification of the voting schemes which are very sensitive<br />
to small amounts of noise and those which are more stable. In particular<br />
among all symmetric voting procedures (i.e., all voters have the same<br />
power), the popular majority is the most robust.</p>
<p>For further technical reading and the precise formulation of the<br />
mathematical theorems see <a href="http://front.math.ucdavis.edu/math.PR/9811157" rel="nofollow">http://front.math.ucdavis.edu/math.PR/9811157</a></p>
<p>A word of warning is in place. The precise expected effect of mistakes<br />
will depend on the specific statistical model for the voting patterns and<br />
for the noise created by counting errors and biases. For example, more<br />
recent work by Claire Kanyon from Orsay university, Elchanan Mossel from<br />
Microsoft research and Yuval Peres from Berkeley and the Hebrew<br />
University, gives quite a different picture for different models based on<br />
the famous “Ising model” from Physics. We expect that the qualitative<br />
conclusion of the research by Benjamini, Kalai and Schramm applied to the<br />
case of U.S. presidential elections and that the popular vote method is<br />
much more stable to noise and biases than the electoral college method.<br />
For definite conclusions, however, some statistical work on actual voting<br />
data should be carried out. Choosing the most appropriate voting method<br />
involve, of course, many, primarily non-mathematical considerations.</p>
<p>Itai Benjamini, Microsoft Research and the Wizmann Institute for Science<br />
Elchanan Mossel, Microsoft Research</p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2020/11/10/benjamini-and-mossel-2000-account-sensitivity-of-voting-schemes-to-mistakes-and-manipulations/"><span class="datestr">at November 10, 2020 03:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.04144">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.04144">Near-Optimal Learning of Tree-Structured Distributions by Chow-Liu</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Arnab Bhattacharyya, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gayen:Sutanu.html">Sutanu Gayen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Price:Eric.html">Eric Price</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vinodchandran:N=_V=.html">N. V. Vinodchandran</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.04144">PDF</a><br /><b>Abstract: </b>We provide finite sample guarantees for the classical Chow-Liu algorithm
(IEEE Trans.~Inform.~Theory, 1968) to learn a tree-structured graphical model
of a distribution. For a distribution $P$ on $\Sigma^n$ and a tree $T$ on $n$
nodes, we say $T$ is an $\varepsilon$-approximate tree for $P$ if there is a
$T$-structured distribution $Q$ such that $D(P\;||\;Q)$ is at most
$\varepsilon$ more than the best possible tree-structured distribution for $P$.
We show that if $P$ itself is tree-structured, then the Chow-Liu algorithm with
the plug-in estimator for mutual information with $\widetilde{O}(|\Sigma|^3
n\varepsilon^{-1})$ i.i.d.~samples outputs an $\varepsilon$-approximate tree
for $P$ with constant probability. In contrast, for a general $P$ (which may
not be tree-structured), $\Omega(n^2\varepsilon^{-2})$ samples are necessary to
find an $\varepsilon$-approximate tree. Our upper bound is based on a new
conditional independence tester that addresses an open problem posed by
Canonne, Diakonikolas, Kane, and Stewart~(STOC, 2018): we prove that for three
random variables $X,Y,Z$ each over $\Sigma$, testing if $I(X; Y \mid Z)$ is $0$
or $\geq \varepsilon$ is possible with $\widetilde{O}(|\Sigma|^3/\varepsilon)$
samples. Finally, we show that for a specific tree $T$, with $\widetilde{O}
(|\Sigma|^2n\varepsilon^{-1})$ samples from a distribution $P$ over $\Sigma^n$,
one can efficiently learn the closest $T$-structured distribution in KL
divergence by applying the add-1 estimator at each node.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.04144"><span class="datestr">at November 10, 2020 10:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.04125">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.04125">Quantum-Inspired Algorithms from Randomized Numerical Linear Algebra</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chepurko:Nadiia.html">Nadiia Chepurko</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Clarkson:Kenneth_L=.html">Kenneth L. Clarkson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Horesh:Lior.html">Lior Horesh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woodruff:David_P=.html">David P. Woodruff</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.04125">PDF</a><br /><b>Abstract: </b>We create classical (non-quantum) dynamic data structures supporting queries
for recommender systems and least-squares regression that are comparable to
their quantum analogues. De-quantizing such algorithms has received a flurry of
attention in recent years; we obtain sharper bounds for these problems. More
significantly, we achieve these improvements by arguing that the previous
quantum-inspired algorithms for these problems are doing leverage or
ridge-leverage score sampling in disguise. With this recognition, we are able
to employ the large body of work in numerical linear algebra to obtain
algorithms for these problems that are simpler and faster than existing
approaches.
</p>
<p>We also consider static data structures for the above problems, and obtain
close-to-optimal bounds for them. To do this, we introduce a new randomized
transform, the Gaussian Randomized Hadamard Transform (GRHT). It was thought in
the numerical linear algebra community that to obtain nearly-optimal bounds for
various problems such as rank computation, finding a maximal linearly
independent subset of columns, regression, low rank approximation, maximum
matching on general graphs and linear matroid union, that one would need to
resolve the main open question of Cohen (SODA, 2016) regarding the logarithmic
factors in existing oblivious subspace embeddings. We bypass this question,
using GRHT, and obtain optimal or nearly-optimal bounds for these problems. For
the fundamental problems of rank computation and finding a linearly independent
subset of columns, our algorithms improve Cheung, Kwok, and Lau (JACM, 2013)
and are optimal to within a constant factor and a $\log\log(n)$-factor,
respectively. Further, for constant factor regression and low rank
approximation we give the first optimal algorithms, for the current matrix
multiplication exponent.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.04125"><span class="datestr">at November 10, 2020 10:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.04071">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.04071">Optimal tiling of the Euclidean space using symmetric bodies</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Braverman:Mark.html">Mark Braverman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Minzer:Dor.html">Dor Minzer</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.04071">PDF</a><br /><b>Abstract: </b>What is the least surface area of a symmetric body $B$ whose $\mathbb{Z}^n$
translations tile $\mathbb{R}^n$? Since any such body must have volume $1$, the
isoperimetric inequality implies that its surface area must be at least
$\Omega(\sqrt{n})$. Remarkably, Kindler et al.\ showed that for general bodies
$B$ this is tight, i.e.\ that there is a tiling body of $\mathbb{R}^n$ whose
surface area is $O(\sqrt{n})$.
</p>
<p>In theoretical computer science, the tiling problem is intimately to the
study of parallel repetition theorems (which are an important component in
PCPs), and more specifically in the question of whether a "strong version" of
the parallel repetition theorem holds. Raz showed, using the odd cycle game,
that strong parallel repetition fails in general, and subsequently these ideas
were used in order to construct non-trivial tilings of $\mathbb{R}^n$.
</p>
<p>In this paper, motivated by the study of a symmetric parallel repetition, we
consider the symmetric variant of the tiling problem in $\mathbb{R}^n$. We show
that any symmetric body that tiles $\mathbb{R}^n$ must have surface area at
least $\Omega(n/\sqrt{\log n})$, and that this bound is tight, i.e.\ that there
is a symmetric tiling body of $\mathbb{R}^n$ with surface area $O(n/\sqrt{\log
n})$. We also give matching bounds for the value of the symmetric parallel
repetition of Raz's odd cycle game.
</p>
<p>Our result suggests that while strong parallel repetition fails in general,
there may be important special cases where it still applies.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.04071"><span class="datestr">at November 10, 2020 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.04047">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.04047">Computing Lengths of Shortest Non-Crossing Paths in Planar Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Balzotti:Lorenzo.html">Lorenzo Balzotti</a>, Paolo G. Franciosa <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.04047">PDF</a><br /><b>Abstract: </b>Given a plane undirected graph $G$ with non-negative edge weights and a set
of $k$ terminal pairs on the external face, it is shown in Takahashi et al.,
(Algorithmica, 16, 1996, pp. 339-357) that the lengths of $k$ non-crossing
shortest paths joining the $k$ terminal pairs (if they exist) can be computed
in $O(n \log n)$ worst-case time, where $n$ is the number of vertices of $G$.
This technique only applies when the union $U$ of the computed shortest paths
is a forest. We show that given a plane undirected weighted graph $U$ and a set
of $k$ terminal pairs on the external face, it is always possible to compute
the lengths of $k$ non-crossing shortest paths joining the $k$ terminal pairs
in linear worst-case time, provided that the graph $U$ is the union of $k$
shortest paths, possibly containing cycles. Moreover, each shortest path $\pi$
can be listed in $O(\ell+\ell\log\lceil{\frac{k}{\ell}}\rceil)$, where $\ell$
is the number of edges in $\pi$. As a consequence, the problem of computing
multi-terminal distances in a plane undirected weighted graph can always be
solved in $O(n \log k)$ worst-case time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.04047"><span class="datestr">at November 10, 2020 10:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.04022">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.04022">The Hierarchical Chinese Postman Problem: the slightest disorder makes it hard, yet disconnectedness is manageable</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Vsevolod A. Afanasev, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bevern:Ren=eacute=_van.html">René van Bevern</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tsidulko:Oxana_Yu=.html">Oxana Yu. Tsidulko</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.04022">PDF</a><br /><b>Abstract: </b>The Hierarchical Chinese Postman Problem is finding a shortest traversal of
all edges of a graph respecting precedence constraints given by a partial order
on classes of edges. We show that the special case with connected classes is
NP-hard even on orders decomposable into a chain and an incomparable class. For
the case with linearly ordered (possibly disconnected) classes, we get
5/3-approximations and fixed-parameter algorithms by transferring results from
the Rural Postman Problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.04022"><span class="datestr">at November 10, 2020 11:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.04010">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.04010">Scout Algorithm For Fast Substring Matching</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Natrajan:Anand.html">Anand Natrajan</a>, Mallige Anand <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.04010">PDF</a><br /><b>Abstract: </b>Exact substring matching is a common task in many software applications.
Despite the existence of several algorithms for finding whether or not a
pattern string is present in a target string, the most common implementation is
a na\"ive, brute force approach. Alternative approaches either do not provide
enough of a benefit for the added complexity, or are impractical for modern
character sets, e.g., Unicode. We present a new algorithm, Scout, that is
straightforward, quick and appropriate for all applications. We also compare
the performance characteristics of the Scout algorithm with several others.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.04010"><span class="datestr">at November 10, 2020 11:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03978">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03978">Smooth approximations and CSPs over finitely bounded homogeneous structures</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mottet:Antoine.html">Antoine Mottet</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pinsker:Michael.html">Michael Pinsker</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03978">PDF</a><br /><b>Abstract: </b>We develop the novel machinery of smooth approximations, and apply it to
confirm the CSP dichotomy conjecture for first-order reducts of the random
tournament, various homogeneous graphs including the random graph, and for
expansions of the order of the rationals. Apart from obtaining these dichotomy
results, we show how our new proof technique allows to unify and significantly
simplify the previous results from the literature. For all but the last
structure, we moreover characterize those CSPs which are solvable by local
consistency methods, again using the same machinery.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03978"><span class="datestr">at November 10, 2020 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03915">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03915">Sampling Constraint Satisfaction Solutions in the Local Lemma Regime</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feng:Weiming.html">Weiming Feng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/He:Kun.html">Kun He</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yin:Yitong.html">Yitong Yin</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03915">PDF</a><br /><b>Abstract: </b>We give a Markov chain based algorithm for sampling almost uniform solutions
of constraint satisfaction problems (CSPs). Assuming a canonical setting for
the Lov\'asz local lemma, where each constraint is violated by a small number
of forbidden local configurations, our sampling algorithm is accurate in a
local lemma regime, and the running time is a fixed polynomial whose dependency
on $n$ is close to linear, where $n$ is the number of variables. Our main
approach is a new technique called states compression, which generalizes the
"mark/unmark" paradigm of Moitra (Moitra, JACM, 2019), and can give fast
local-lemma-based sampling algorithms. As concrete applications of our
technique, we give the current best almost-uniform samplers for hypergraph
colorings and for CNF solutions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03915"><span class="datestr">at November 10, 2020 11:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03892">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03892">Tight Conditional Lower Bounds for Approximating Diameter in Directed Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dalirrooyfard:Mina.html">Mina Dalirrooyfard</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wein:Nicole.html">Nicole Wein</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03892">PDF</a><br /><b>Abstract: </b>Among the most fundamental graph parameters is the Diameter, the largest
distance between any pair of vertices. Computing the Diameter of a graph with
$m$ edges requires $m^{2-o(1)}$ time under the Strong Exponential Time
Hypothesis (SETH), which can be prohibitive for very large graphs, so efficient
approximation algorithms for Diameter are desired.
</p>
<p>There is a folklore algorithm that gives a $2$-approximation for Diameter in
$\tilde{O}(m)$ time. Additionally, a line of work concludes with a
$3/2$-approximation algorithm for Diameter in weighted directed graphs that
runs in $\tilde{O}(m^{3/2})$ time. The $3/2$-approximation algorithm is known
to be tight under SETH: Roditty and Vassilevska W. proved that under SETH any
$3/2-\epsilon$ approximation algorithm for Diameter in undirected unweighted
graphs requires $m^{2-o(1)}$ time, and then Backurs, Roditty, Segal,
Vassilevska W., and Wein and the follow-up work of Li proved that under SETH
any $5/3-\epsilon$ approximation algorithm for Diameter in undirected
unweighted graphs requires $m^{3/2-o(1)}$ time.
</p>
<p>Whether or not the folklore 2-approximation algorithm is tight, however, is
unknown, and has been explicitly posed as an open problem in numerous papers.
Towards this question, Bonnet recently proved that under SETH, any
$7/4-\epsilon$ approximation requires $m^{4/3-o(1)}$, only for directed
weighted graphs.
</p>
<p>We completely resolve this question for directed graphs by proving that the
folklore 2-approximation algorithm is conditionally optimal. In doing so, we
obtain a series of conditional lower bounds that together with prior work, give
a complete time-accuracy trade-off that is tight with all known algorithms for
directed graphs. Specifically, we prove that under SETH for any $\delta&gt;0$, a
$(\frac{2k-1}{k}-\delta)$-approximation algorithm for Diameter on directed
unweighted graphs requires $m^{\frac{k}{k-1}-o(1)}$ time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03892"><span class="datestr">at November 10, 2020 11:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03865">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03865">Combinatorial Bernoulli Factories: Matchings, Flows and Other Polytopes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Niazadeh:Rad.html">Rad Niazadeh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leme:Renato_Paes.html">Renato Paes Leme</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schneider:Jon.html">Jon Schneider</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03865">PDF</a><br /><b>Abstract: </b>A Bernoulli factory is an algorithmic procedure for exact sampling of certain
random variables having only Bernoulli access to their parameters. Bernoulli
access to a parameter $p \in [0,1]$ means the algorithm does not know $p$, but
has sample access to independent draws of a Bernoulli random variable with mean
equal to $p$. In this paper, we study the problem of Bernoulli factories for
polytopes: given Bernoulli access to a vector $x\in \mathcal{P}$ for a given
polytope $\mathcal{P}\subset [0,1]^n$, output a randomized vertex such that the
expected value of the $i$-th coordinate is \emph{exactly} equal to $x_i$. For
example, for the special case of the perfect matching polytope, one is given
Bernoulli access to the entries of a doubly stochastic matrix $[x_{ij}]$ and
asked to sample a matching such that the probability of each edge $(i,j)$ be
present in the matching is exactly equal to $x_{ij}$.
</p>
<p>We show that a polytope $\mathcal{P}$ admits a Bernoulli factory if and and
only if $\mathcal{P}$ is the intersection of $[0,1]^n$ with an affine subspace.
Our construction is based on an algebraic formulation of the problem, involving
identifying a family of Bernstein polynomials (one per vertex) that satisfy a
certain algebraic identity on $\mathcal{P}$. The main technical tool behind our
construction is a connection between these polynomials and the geometry of
zonotope tilings. We apply these results to construct an explicit factory for
the perfect matching polytope. The resulting factory is deeply connected to the
combinatorial enumeration of arborescences and may be of independent interest.
For the $k$-uniform matroid polytope, we recover a sampling procedure known in
statistics as Sampford sampling.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03865"><span class="datestr">at November 10, 2020 10:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03819">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03819">Fast Low-Space Algorithms for Subset Sum</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jin:Ce.html">Ce Jin</a>, Nikhil Vyas, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Williams:Ryan.html">Ryan Williams</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03819">PDF</a><br /><b>Abstract: </b>We consider the canonical Subset Sum problem: given a list of positive
integers $a_1,\ldots,a_n$ and a target integer $t$ with $t &gt; a_i$ for all $i$,
determine if there is an $S \subseteq [n]$ such that $\sum_{i \in S} a_i = t$.
The well-known pseudopolynomial-time dynamic programming algorithm [Bellman,
1957] solves Subset Sum in $O(nt)$ time, while requiring $\Omega(t)$ space.
</p>
<p>In this paper we present algorithms for Subset Sum with $\tilde O(nt)$
running time and much lower space requirements than Bellman's algorithm, as
well as that of prior work. We show that Subset Sum can be solved in $\tilde
O(nt)$ time and $O(\log(nt))$ space with access to $O(\log n \log \log n+\log
t)$ random bits. This significantly improves upon the $\tilde O(n
t^{1+\varepsilon})$-time, $\tilde O(n\log t)$-space algorithm of Bringmann
(SODA 2017). We also give an $\tilde O(n^{1+\varepsilon}t)$-time,
$O(\log(nt))$-space randomized algorithm, improving upon previous
$(nt)^{O(1)}$-time $O(\log(nt))$-space algorithms by Elberfeld, Jakoby, and
Tantau (FOCS 2010), and Kane (2010). In addition, we also give a $\mathrm{poly}
\log(nt)$-space, $\tilde O(n^2 t)$-time deterministic algorithm.
</p>
<p>We also study time-space trade-offs for Subset Sum. For parameter $1\le k\le
\min\{n,t\}$, we present a randomized algorithm running in $\tilde O((n+t)\cdot
k)$ time and $O((t/k) \mathrm{polylog} (nt))$ space.
</p>
<p>As an application of our results, we give an
$\tilde{O}(\min\{n^2/\varepsilon, n/\varepsilon^2\})$-time and
$\mathrm{polylog}(nt)$-space algorithm for "weak" $\varepsilon$-approximations
of Subset Sum.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03819"><span class="datestr">at November 10, 2020 11:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03778">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03778">A Gap-ETH-Tight Approximation Scheme for Euclidean TSP</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kisfaludi=Bak:S=aacute=ndor.html">Sándor Kisfaludi-Bak</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nederlof:Jesper.html">Jesper Nederlof</a>, Karol Węgrzycki <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03778">PDF</a><br /><b>Abstract: </b>We revisit the classic task of finding the shortest tour of $n$ points in
$d$-dimensional Euclidean space, for any fixed constant $d \geq 2$. We
determine the optimal dependence on $\varepsilon$ in the running time of an
algorithm that computes a $(1+\varepsilon)$-approximate tour, under a plausible
assumption. Specifically, we give an algorithm that runs in
$2^{\mathcal{O}(1/\varepsilon^{d-1})} n\log n$ time. This improves the
previously smallest dependence on $\varepsilon$ in the running time
$(1/\varepsilon)^{\mathcal{O}(1/\varepsilon^{d-1})}n \log n$ of the algorithm
by Rao and Smith (STOC 1998). We also show that a
$2^{o(1/\varepsilon^{d-1})}\text{poly}(n)$ algorithm would violate the
Gap-Exponential Time Hypothesis (Gap-ETH).
</p>
<p>Our new algorithm builds upon the celebrated quadtree-based methods initially
proposed by Arora (J. ACM 1998), but it adds a simple new idea that we call
\emph{sparsity-sensitive patching}. On a high level this lets the granularity
with which we simplify the tour depend on how sparse it is locally. Our
approach is (arguably) simpler than the one by Rao and Smith since it can work
without geometric spanners. We demonstrate the technique extends easily to
other problems, by showing as an example that it also yields a Gap-ETH-tight
approximation scheme for Rectilinear Steiner Tree.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03778"><span class="datestr">at November 10, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03704">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03704">Quantum Combinatorial Games: Structures and Computational Complexity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Burke:Kyle.html">Kyle Burke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Ferland:Matthew.html">Matthew Ferland</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Teng:Shang=Hua.html">Shang-Hua Teng</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03704">PDF</a><br /><b>Abstract: </b>Recently, a standardized framework was proposed for introducing
quantum-inspired moves in mathematical games with perfect information and no
chance. The beauty of quantum games-succinct in representation, rich in
structures, explosive in complexity, dazzling for visualization, and
sophisticated for strategic reasoning-has drawn us to play concrete games full
of subtleties and to characterize abstract properties pertinent to complexity
consequence. Going beyond individual games, we explore the tractability of
quantum combinatorial games as whole, and address fundamental questions
including:
</p>
<p>Quantum Leap in Complexity: Are there polynomial-time solvable games whose
quantum extensions are intractable?
</p>
<p>Quantum Collapses in Complexity: Are there PSPACE-complete games whose
quantum extensions fall to the lower levels of the polynomial-time hierarchy?
</p>
<p>Quantumness Matters: How do outcome classes and strategies change under
quantum moves? Under what conditions doesn't quantumness matter?
</p>
<p>PSPACE Barrier for Quantum Leap: Can quantum moves launch PSPACE games into
outer polynomial space
</p>
<p>We show that quantum moves not only enrich the game structure, but also
impact their computational complexity. In settling some of these basic
questions, we characterize both the powers and limitations of quantum moves as
well as the superposition of game configurations that they create. Our
constructive proofs-both on the leap of complexity in concrete Quantum Nim and
Quantum Undirected Geography and on the continuous collapses, in the quantum
setting, of complexity in abstract PSPACE-complete games to each level of the
polynomial-time hierarchy-illustrate the striking computational landscape over
quantum games and highlight surprising turns with unexpected quantum impact.
Our studies also enable us to identify several elegant open questions
fundamental to quantum combinatorial game theory (QCGT).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03704"><span class="datestr">at November 10, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03700">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03700">On the Complexity of CSP-based Ideal Membership Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bulatov:Andrei_A=.html">Andrei A. Bulatov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rafiey:Akbar.html">Akbar Rafiey</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03700">PDF</a><br /><b>Abstract: </b>In this paper we consider the Ideal Membership Problem (IMP for short), in
which we are given real polynomials $f_0, f_1, \dots, f_k$ and the question is
to decide whether $f_0$ belongs to the ideal generated by $f_1, \dots, f_k$. In
the more stringent version the task is also to find a proof of this fact. The
IMP underlies many proof systems based on polynomials such as Nullstellensatz,
Polynomial Calculus, and Sum-of-Squares. In the majority of such applications
the IMP involves so called combinatorial ideals that arise from a variety of
discrete combinatorial problems. This restriction makes the IMP significantly
easier and in some cases allows for an efficient algorithm to solve it.
</p>
<p>In 2019 Mastrolilli initiated a systematic study of IMPs arising from
Constraint Satisfaction Problems (CSP) of the form CSP($\Gamma$), that is, CSPs
in which the type of constraints is limited to relations from a set $\Gamma$.
He described sets $\Gamma$ on a 2-element set that give rise to polynomial time
solvable IMPs and showed that for the remaining ones the problem is hard. We
continue this line of research.
</p>
<p>First, we show that many CSP techniques can be translated to IMPs thus
allowing us to significantly improve the methods of studying the complexity of
the IMP. We also develop universal algebraic techniques for the IMP that have
been so useful in the study of the CSP. This allows us to prove a general
necessary condition for the tractability of the IMP, and three sufficient ones.
The sufficient conditions include IMPs arising from systems of linear equations
over $GF(p)$, $p$ prime, and also some conditions defined through special kinds
of polymorphisms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03700"><span class="datestr">at November 10, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03693">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03693">Hypothesis testing with low-degree polynomials in the Morris class of exponential families</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kunisky:Dmitriy.html">Dmitriy Kunisky</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03693">PDF</a><br /><b>Abstract: </b>Analysis of low-degree polynomial algorithms is a powerful, newly-popular
method for predicting computational thresholds in hypothesis testing problems.
One limitation of current techniques for this analysis is their restriction to
Bernoulli and Gaussian distributions. We expand this range of possibilities by
performing the low-degree analysis of hypothesis testing for the Morris class
of exponential families, giving a unified treatment of Gaussian, Poisson,
gamma, binomial, negative binomial, and generalized hyperbolic secant
distributions. We then give several algorithmic applications.
</p>
<p>1. In models where a random signal is observed through an exponential family,
the success or failure of low-degree polynomials is governed by the $z$-score
overlap, the inner product of $z$-score vectors with respect to the null
distribution of two independent copies of the signal.
</p>
<p>2. In the same models, testing with low-degree polynomials exhibits channel
monotonicity: the above distributions admit a total ordering by computational
cost of hypothesis testing, according to a scalar parameter describing how the
variance depends on the mean in an exponential family.
</p>
<p>3. In a spiked matrix model with a particular non-Gaussian noise
distribution, the low-degree prediction is incorrect unless polynomials with
arbitrarily large degree in individual matrix entries are permitted. This shows
that polynomials summing over self-avoiding walks and variants thereof, as
proposed recently by Ding, Hopkins, and Steurer (2020) for spiked matrix models
with heavy-tailed noise, are suboptimal for this model. Thus low-degree
polynomials appear to offer a tradeoff between robustness and strong
performance fine-tuned to specific models, and may struggle with problems
requiring an algorithm to first examine the input and then use some
intermediate computation to choose from one of several inference subroutines.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03693"><span class="datestr">at November 10, 2020 10:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03639">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03639">Graph cuts always find a global optimum (with a catch)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lang:Hunter.html">Hunter Lang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sontag:David.html">David Sontag</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vijayaraghavan:Aravindan.html">Aravindan Vijayaraghavan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03639">PDF</a><br /><b>Abstract: </b>We prove that the alpha-expansion algorithm for MAP inference always returns
a globally optimal assignment for Markov Random Fields with Potts pairwise
potentials, with a catch: the returned assignment is only guaranteed to be
optimal in a small perturbation of the original problem instance. In other
words, all local minima with respect to expansion moves are global minima to
slightly perturbed versions of the problem. On "real-world" instances, MAP
assignments of small perturbations of the problem should be very similar to the
MAP assignment(s) of the original problem instance. We design an algorithm that
can certify whether this is the case in practice. On several MAP inference
problem instances from computer vision, this algorithm certifies that MAP
solutions to all of these perturbations are very close to solutions of the
original instance. These results taken together give a cohesive explanation for
the good performance of "graph cuts" algorithms in practice. Every local
expansion minimum is a global minimum in a small perturbation of the problem,
and all of these global minima are close to the original solution.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03639"><span class="datestr">at November 10, 2020 10:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03636">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03636">An Efficient Scheme for the Generation of Ordered Trees in Constant Amortized Time</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Parque:Victor.html">Victor Parque</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Miyashita:Tomoyuki.html">Tomoyuki Miyashita</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03636">PDF</a><br /><b>Abstract: </b>Trees are useful entities allowing to model data structures and hierarchical
relationships in networked decision systems ubiquitously. An ordered tree is a
rooted tree where the order of the subtrees (children) of a node is
significant. In combinatorial optimization, generating ordered trees is
relevant to evaluate candidate combinatorial objects. In this paper, we present
an algebraic scheme to generate ordered trees with $n$ vertices with utmost
efficiency; whereby our approach uses $\mathcal{O}(n)$ space and
$\mathcal{O}(1)$ time in average per tree. Our computational studies have shown
the feasibility and efficiency to generate ordered trees in constant time in
average, in about one tenth of a millisecond per ordered tree. Due to the 1-1
bijective nature to other combinatorial classes, our approach is favorable to
study the generation of binary trees with $n$ external nodes, trees with $n$
nodes, legal sequences of $n$ pairs of parentheses, triangulated $n$-gons,
gambler's sequences and lattice paths. We believe our scheme may find its use
in devising algorithms for planning and combinatorial optimization involving
Catalan numbers.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03636"><span class="datestr">at November 10, 2020 10:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03622">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03622">Settling the Robust Learnability of Mixtures of Gaussians</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Allen.html">Allen Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moitra:Ankur.html">Ankur Moitra</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03622">PDF</a><br /><b>Abstract: </b>This work represents a natural coalescence of two important lines of work:
learning mixtures of Gaussians and algorithmic robust statistics. In particular
we give the first provably robust algorithm for learning mixtures of any
constant number of Gaussians. We require only mild assumptions on the mixing
weights (bounded fractionality) and that the total variation distance between
components is bounded away from zero. At the heart of our algorithm is a new
method for proving dimension-independent polynomial identifiability through
applying a carefully chosen sequence of differential operations to certain
generating functions that not only encode the parameters we would like to learn
but also the system of polynomial equations we would like to solve. We show how
the symbolic identities we derive can be directly used to analyze a natural
sum-of-squares relaxation.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03622"><span class="datestr">at November 10, 2020 10:55 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03619">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03619">Algorithmic Extensions of Dirac's Theorem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fomin:Fedor_V=.html">Fedor V. Fomin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Golovach:Petr_A=.html">Petr A. Golovach</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sagunov:Danil.html">Danil Sagunov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Simonov:Kirill.html">Kirill Simonov</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03619">PDF</a><br /><b>Abstract: </b>In 1952, Dirac proved the following theorem about long cycles in graphs with
large minimum vertex degrees: Every $n$-vertex $2$-connected graph $G$ with
minimum vertex degree $\delta\geq 2$ contains a cycle with at least
$\min\{2\delta,n\}$ vertices. In particular, if $\delta\geq n/2$, then $G$ is
Hamiltonian. The proof of Dirac's theorem is constructive, and it yields an
algorithm computing the corresponding cycle in polynomial time. The
combinatorial bound of Dirac's theorem is tight in the following sense. There
are 2-connected graphs that do not contain cycles of length more than
$2\delta+1$. Also, there are non-Hamiltonian graphs with all vertices but one
of degree at least $n/2$. This prompts naturally to the following algorithmic
questions. For $k\geq 1$,
</p>
<p>(A) How difficult is to decide whether a 2-connected graph contains a cycle
of length at least $\min\{2\delta+k,n\}$?
</p>
<p>(B) How difficult is to decide whether a graph $G$ is Hamiltonian, when at
least $n - k$ vertices of $G$ are of degrees at least $n/2-k$?
</p>
<p>The first question was asked by Fomin, Golovach, Lokshtanov, Panolan,
Saurabh, and Zehavi. The second question is due to Jansen, Kozma, and Nederlof.
Even for a very special case of $k=1$, the existence of a polynomial-time
algorithm deciding whether $G$ contains a cycle of length at least
$\min\{2\delta+1,n\}$ was open. We resolve both questions by proving the
following algorithmic generalization of Dirac's theorem: If all but $k$
vertices of a $2$-connected graph $G$ are of degree at least $\delta$, then
deciding whether $G$ has a cycle of length at least $\min\{2\delta +k, n\}$ can
be done in time $2^{\mathcal{O}(k)}\cdot n^{\mathcal{O}(1)}$.
</p>
<p>The proof of the algorithmic generalization of Dirac's theorem builds on new
graph-theoretical results that are interesting on their own.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03619"><span class="datestr">at November 10, 2020 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03617">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03617">A Simple Algorithm for Higher-order Delaunay Mosaics and Alpha Shapes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Edelsbrunner:Herbert.html">Herbert Edelsbrunner</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Osang:Georg.html">Georg Osang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03617">PDF</a><br /><b>Abstract: </b>We present a simple algorithm for computing higher-order Delaunay mosaics
that works in Euclidean spaces of any finite dimensions. The algorithm selects
the vertices of the order-$k$ mosaic from incrementally constructed lower-order
mosaics and uses an algorithm for weighted first-order Delaunay mosaics as a
black-box to construct the order-$k$ mosaic from its vertices. Beyond this
black-box, the algorithm uses only combinatorial operations, thus facilitating
easy implementation. We extend this algorithm to compute higher-order
$\alpha$-shapes and provide open-source implementations. We present
experimental results for properties of higher-order Delaunay mosaics of random
point sets.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03617"><span class="datestr">at November 10, 2020 11:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03607">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03607">Ridge Regression with Frequent Directions: Statistical and Optimization Perspectives</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dickens:Charlie.html">Charlie Dickens</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03607">PDF</a><br /><b>Abstract: </b>Despite its impressive theory \&amp; practical performance, Frequent Directions
(\acrshort{fd}) has not been widely adopted for large-scale regression tasks.
Prior work has shown randomized sketches (i) perform worse in estimating the
covariance matrix of the data than \acrshort{fd}; (ii) incur high error when
estimating the bias and/or variance on sketched ridge regression. We give the
first constant factor relative error bounds on the bias \&amp; variance for
sketched ridge regression using \acrshort{fd}. We complement these statistical
results by showing that \acrshort{fd} can be used in the optimization setting
through an iterative scheme which yields high-accuracy solutions. This improves
on randomized approaches which need to compromise the need for a new sketch
every iteration with speed of convergence. In both settings, we also show using
\emph{Robust Frequent Directions} further enhances performance.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03607"><span class="datestr">at November 10, 2020 11:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2011.03603">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2011.03603">Fast Near-Optimal Heterogeneous Task Allocation via Flow Decomposition</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Solovey:Kiril.html">Kiril Solovey</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bandyopadhyay:Saptarshi.html">Saptarshi Bandyopadhyay</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rossi:Federico.html">Federico Rossi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wolf:Michael_T=.html">Michael T. Wolf</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pavone:Marco.html">Marco Pavone</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2011.03603">PDF</a><br /><b>Abstract: </b>Multi-robot systems are uniquely well-suited to performing complex tasks such
as patrolling and tracking, information gathering, and pick-up and delivery
problems, offering significantly higher performance than single-robot systems.
A fundamental building block in most multi-robot systems is task allocation:
assigning robots to tasks (e.g., patrolling an area, or servicing a
transportation request) as they appear based on the robots' states to maximize
reward. In many practical situations, the allocation must account for
heterogeneous capabilities (e.g., availability of appropriate sensors or
actuators) to ensure the feasibility of execution, and to promote a higher
reward, over a long time horizon. To this end, we present the FlowDec algorithm
for efficient heterogeneous task-allocation achieving an approximation factor
of at least 1/2 of the optimal reward. Our approach decomposes the
heterogeneous problem into several homogeneous subproblems that can be solved
efficiently using min-cost flow. Through simulation experiments, we show that
our algorithm is faster by several orders of magnitude than a MILP approach.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2011.03603"><span class="datestr">at November 10, 2020 11:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/167">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/167">TR20-167 |  Approximate Hypergraph Vertex Cover and generalized Tuza&amp;#39;s conjecture | 

	Venkatesan Guruswami, 

	Sai Sandeep</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A famous conjecture of Tuza states that the minimum number of edges needed to cover all the triangles in a graph is at most twice the maximum number of edge-disjoint triangles.  This conjecture was couched in a broader setting by Aharoni and Zerbib who proposed a hypergraph version of this conjecture, and also studied its implied fractional versions. We establish the fractional version of the Aharoni-Zerbib conjecture up to lower order terms. Specifically, we give a factor $t/2+ O(\sqrt{t \log t})$ approximation based on LP rounding for an algorithmic version of the hypergraph Tur\'{a}n problem (AHTP). The objective in AHTP is to pick the smallest collection of $(t-1)$-sized subsets of vertices of an input $t$-uniform hypergraph such that every hyperedge contains one of these subsets.

Aharoni and Zerbib also posed whether Tuza's conjecture and its hypergraph versions could follow from non-trivial duality gaps between vertex covers and matchings on hypergraphs that exclude certain sub-hypergraphs, for instance, a ``tent" structure that cannot occur in the incidence of triangles and edges. We give a strong negative answer to this question, by exhibiting tent-free hypergraphs, and indeed $\mathcal{F}$-free hypergraphs for any finite family $\mathcal{F}$ of excluded sub-hypergraphs, whose vertex covers must include almost all the vertices.


The algorithmic questions arising in the above study can be phrased as instances of vertex cover on \emph{simple} hypergraphs, whose hyperedges can pairwise share at most one vertex. We prove that the trivial factor $t$ approximation for vertex cover is hard to improve for simple $t$-uniform hypergraphs. However, for set cover on simple $n$-vertex hypergraphs, the greedy algorithm achieves a factor $(\ln n)/2$, better than the optimal $\ln n$ factor for general hypergraphs.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/167"><span class="datestr">at November 09, 2020 06:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5091">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5091">On defeating a sociopath</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>There are people who really, genuinely, believe, as far as you can dig down, that winning is everything—that however many lies they told, allies they betrayed, innocent lives they harmed, etc. etc., it was all justified by the fact that <strong>they won and their enemies lost</strong>.  Faced with such sociopaths, people like me typically feel an irresistible compulsion to <em>counterargue</em>: to make the sociopath realize that winning is <em>not</em> everything, that truth and honor are terminal values as well; to subject the sociopath to the standards by which the rest of us are judged; to find the conscience that the sociopath buried even from himself and drag it out into the light.  Let me know if you can think of any case in human history where such efforts succeeded, because I’m having difficulty doing so.</p>



<p>Clearly, in the vast majority of cases if not in all, the only counterargument that a sociopath will ever understand is <em>losing</em>.  And yet not just any kind of losing suffices.  For victims, there’s an <em>enormous</em> temptation to turn the sociopath’s underhanded tools against him, to win with the same deceit and naked power that the sociopath so gleefully inflicted on others.  And yet, if that’s what it takes to beat him, then you have to imagine the sociopath deriving a certain perverse satisfaction from it.</p>



<p>Think of the movie villain who, as the panting hero stands over him with his lightsaber, taunts “Yes … yes … destroy me!  Do it now!  Feel the hate and the rage flow through you!”  What happens next, of course, is that the hero angrily decides to give the villain one more chance, the ungrateful villain lunges to stab the hero in the back or something, and only then does the villain die—either by a self-inflicted accident, or else killed by the hero in immediate self-defense.  Either way, the hero walks away with victory <em>and</em> honor.</p>



<p>In practice, it’s a tall order to arrange all of that.  This explains why sociopaths are so hard to defeat, and why I feel so bleak and depressed whenever I see one flaunting his power.  But, you know, the great upside of pessimism is that it doesn’t take much to beat your expectations!  Whenever a single sociopath <em>is</em> cleanly and honorably defeated, or even just rendered irrelevant—no matter that the sociopath’s friends and allies are still in power, no matter that they’ll be back to fight another day, etc. etc.—it’s a genuine occasion for rejoicing.</p>



<p>Anyway, that pretty much sums up my thoughts regarding Arthur Chu.  In other news, hooray about the election!</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5091"><span class="datestr">at November 09, 2020 05:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/166">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/166">TR20-166 |  Lower Bounds for Monotone Arithmetic Circuits Via Communication Complexity | 

	Arkadev Chattopadhyay, 

	Rajit Datta, 

	Partha Mukhopadhyay</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Valiant (1980) showed that general arithmetic circuits with negation can be exponentially more powerful than monotone ones. We give the first qualitative improvement to this classical result: we construct a  family of polynomials $P_n$ in $n$ variables, each of its monomials has positive coefficient, such that $P_n$ can be computed by a polynomial-size \emph{depth-three formula} but every monotone circuit computing it has size $2^{\Omega(n^{1/4}/\log(n))}$. 

The polynomial $P_n$ embeds the $\text{SINK}\circ \text{XOR}$ function devised recently by Chattopadhyay, Mande and Sherif (2020) to refute the Log-Approximate-Rank Conjecture in communication complexity. To prove our lower bound for $P_n$, we develop a general connection between corruption of combinatorial rectangles by  any function $f \circ \text{XOR}$ and corruption of product polynomials by a certain polynomial $P^f$ that is an arithmetic embedding of $f$. This connection should be of independent interest.

Using further ideas from communication complexity, we construct another family of set-multilinear polynomials $f_{n,m}$ such that both $F_{n,m} - \epsilon\cdot f_{n,m}$ and $F_{n,m} + \epsilon\cdot f_{n,m}$ have monotone circuit complexity $2^{\Omega(n/\log(n))}$ if $\epsilon \geq 2^{- \Omega( m )}$ and $F_{n,m} = \prod_{i=1}^n \big(x_{i,1} +\cdots+x_{i,m}\big)$, with $m = O( n/\log n )$. The polynomials $f_{n,m}$ have 0/1 coefficients and are in VNP. Proving such lower bounds for monotone circuits has been advocated recently by Hrubeš (2020) as a first step towards proving lower bounds against general circuits via his new approach.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/166"><span class="datestr">at November 09, 2020 01:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/165">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/165">TR20-165 |  Interactive Oracle Proofs of Proximity to Algebraic Geometry Codes | 

	Sarah Bordage, 

	Jade Nardi</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this work, we initiate the study of proximity testing to Algebraic Geometry (AG) codes. An AG code $C = C(\mathcal C, \mathcal P, D)$ is a vector space associated to evaluations on $\mathcal P$ of functions in the Riemann-Roch space $L_\mathcal C(D)$. The problem of testing proximity to an error-correcting code $C$ consists in distinguishing between the case where an input word, given as an oracle, belongs to $C$ and the one where it is far from every codeword of $C$. AG codes are good candidates to construct \emph{short} proof systems, but there exists no efficient proximity tests for them. We aim to fill this gap.

We construct an Interactive Oracle Proof of Proximity (IOPP) for some families of AG codes by generalizing an IOPP for Reed-Solomon codes, known as the \textsf{FRI} protocol and introduced by Ben-Sasson, Bentov, Horesh and Riabzev in 2018. We identify suitable requirements for designing efficient IOPP systems for AG codes. In addition to proposing the first proximity test targeting AG codes, our IOPP admits quasilinear prover arithmetic complexity and sublinear verifier arithmetic complexity with constant soundness for meaningful classes of AG codes. We take advantage of the algebraic geometry framework that makes any group action on the curve that fixes the divisor $D$ translate into a decomposition of the code $C$. Concretely, our approach relies on Kani's result that splits the Riemann-Roch space of any invariant divisor under this action into several explicit Riemann-Roch spaces on the quotient curve. Under some hypotheses, these spaces behave well enough to define an AG code $C'$ on the quotient curve so that a proximity test to $C$ can be reduced to one to $C'$. Iterating this process thoroughly, we end up with a membership test to a code with significantly smaller length.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/165"><span class="datestr">at November 09, 2020 12:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/164">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/164">TR20-164 |  Direct Sum and Partitionability Testing over General Groups | 

	Gautam Prakriya, 

	Andrej Bogdanov</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A function $f(x_1, \dots, x_n)$ from a product domain $\mathcal{D}_1 \times \cdots \times \mathcal{D}_n$ to an abelian group $\mathcal{G}$ is a direct sum if it is of the form $f_1(x_1) + \cdots + f_n(x_n)$.  We present a new 4-query direct sum test with optimal (up to constant factors) soundness error.  This generalizes a result of Dinur and Golubev (RANDOM 2019) which is tailored to the target group $\mathcal{G} = \mathbb{Z}_2$.  As a special case, we obtain an optimal affinity test for $\mathcal{G}$-valued functions on domain $\{0, 1\}^n$ under product measure.  Our analysis relies on the hypercontractivity of the binary erasure channel.

We also study the testability of function partitionability over product domains into disjoint components. A $\mathcal{G}$-valued $f(x_1, \dots, x_n)$ is $k$-direct sum partitionable if it can be written as a sum of functions over $k$ nonempty disjoint sets of inputs.  A function $f(x_1, \dots, x_n)$ with unstructured product range $\mathcal{R}^k$ is  direct product partitionable if its outputs depend on disjoint sets of inputs.  

We show that direct sum partitionability and direct product partitionability are one-sided error testable with $O((n - k)(\log n + 1/\epsilon) + 1/\epsilon)$ adaptive queries and $O((n/\epsilon) \log^2(n/\epsilon))$ nonadaptive queries, respectively.  Both bounds are tight up to the logarithmic factors for constant $\epsilon$ even with respect to adaptive, two-sided error testers.  We also give a non-adaptive one-sided error tester for direct sum partitionability with query complexity $O(kn^2 (\log n)^2 / \epsilon)$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/164"><span class="datestr">at November 09, 2020 12:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-7627871032207660441">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/11/random-thoughts-on-election-2020.html">Random Thoughts on the Election (2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><br /></p><p>1) Biden will be the oldest president (measuring by when they take the oath of office), at 78. The next two are Trump 70 and Reagan 69. Biden will be older entering office then Reagan was leaving office. </p><p>After Biden, Trump, Reagan:</p><p>William Henry Harrison 68. Why do some people have middle names that are commonly spoken and some do not? Others with middle names spoken: Lee Harvey Oswald, John Wilkes Booth. </p><p>James Buchanan 65</p><p>George H. W. Bush 64. Why do some people have their initials commonly spoken and others do not? In this case it may be to distinguish from W. Why are some people known by their middle initial? Well, actually one that I know of, W.</p><p>Youngest was Theodore Roosevelt 42 who took the office after McKinley was assassinated . Kennedy was youngest to take the oath after being ELECTED at 43. Theodore Roosevelt was known as TR. John F Kennedy is often called JFK. Franklin D Roosevelt was called FDR. Why are some people known by their initials? In these cases maybe to distinguish them from other Roosevelts and Kennedys.</p><p>2) Right now it looks like GA will go for Biden. This surprises me. I had heard `GA is on the verge of turning blue and always will be.'</p><p>3) Dem-Blue, Rep-Red always puzzled me since I thought Red was associated with communism.</p><p>4) A quote from the Trump/Schwartz  book THE ART OF THE DEAL about why Carter was a one-termer  is rather predictive:</p><p>See <a href="https://www.thewrap.com/drudge-trump-election-con/">here</a></p><p>(I've heard Schwartz referred to as a ghost writer. That is not true-- Tony Schwartz's name is ON THE COVER, so he is not a ghostwriter.)</p><p>5) During the Trump administration UAE, Bahrain, and Sudan all recognized Israel (gee, when I see it on a map I recognize it, why did it take them so long :-) ). See <a href="https://apnews.com/article/donald-trump-bahrain-israel-united-arab-emirates-sudan-07fe32a9ac1bd3afe5c503d4ff6bc859">here</a>. All three deals were brokered by the US so Trump could  take credit here. So why didn't he? One answer is that the left-wing lame stream media didn't cover it much. But FOX didn't cover it much. Trump didn't mention it much. Trump didn't even mention it as a way to complain about media coverage. So- independent of if you think Trump deserves credit here or not, I am interested in why he didn't brag about this one. (When I have asked this question people point out that Trump's base would not care about this. But Trump could complain that Obama got a Nobel Peace prize for nothing, and he got these deals done and hasn't, gotten a Nobel Prize because of the fake-Nobel-Committee and channel this into anti-Obama sentiment.)(ADDED LATER- some of the comments have corrected me and say that Trump DOES mention it-- A LOT. My bad. Still do not know if Fox News mentioned it much.)</p><p>6) Truth avoids imitating  art: Watch Season five of the HBO show VEEP for how messy a close election can be. </p><p>7) I think Biden will end up being president and the transition will be peaceful. Why? Fox News and other conservative organizations are urging Trump to concede. Republican state legislators are NOT trying to find ways to overturn the results in their states. Judges have found NO evidence for the kinds of fraud that Trump is complaining about. Many Republicans are silent (John Oliver says that means they SUPPORT  Trumps allegations, but I think it means they are NOT supporting Trump's allegations.) </p><p> Why is the Republican establishment NOT backing Trumps claims of fraud? Here are some thoughts.</p><p>a) Because the allegations of fraud are not just false but obviously false.  </p><p>b) Because they think that it is better for the country to have a clean transition.</p><p>c) Because they are tired of Trump also and realize he is not good for the party brand (a bit late now).</p><p>d) Corrupting the electoral process is a bridge to far. (Where did that phrase come from?)</p><p>e) I wonder if Trump himself would have preferred to lose in 2016 and go around having rallies, perhaps have his own TV network. RALLIES are fun, RUNNING THE COUNTRY is not. So those around him may want him to go back to his original plan. </p><p>8) Did Nate (the only pollster with a one-word-name) do well this time around  He thinks so, see <a href="https://www.thedailybeast.com/nate-silver-tells-fivethirtyeight-critics-fuck-you-we-did-a-good-job">here</a>. Its not even clear he did badly in 2016- he gave Trump a 20% chance in 2016 and a 10% chance this time. </p><p>9) I think that if  we had not had a pandemic then  Trump would have won. Two reasons: the country thinks he handled it badly, and it may have literally killed some of his voters.  As a final note on that: Mark Meadows (WH Chief of staff) has COVID. I am surprised Pence didn't get it-- thought maybe he did or will. </p><p>10) Why did people in the Trump WH who one assumes know that Covid is serious and that masks and social distancing were  way to prevent it, not do these simple things?  Perhaps they thought (correctly) that the more people thing about covid, the more likely Trump loses, so they took a risk. Alas, those that trade their health for electability get neither. </p><p>11) Neither Pence nor Harris is particularly young or old as VP's go. </p><p>Youngest: John Breckenridge, 36. Buchanan's VP</p><p>Second Youngest: Richard Nixon, 40, Eisenhower's VP</p><p>Oldest: Alben Barkley, 71, Truman's VP</p><p>Second oldest: Charles Curtis, 69, Hoover's VP</p><p>Pence was 57 when took the oath, Harris will be 56. </p><p>12) If Biden wins then on Jan 20 when he takes the oath there will be 5 living Ex-presidents:Carter, Clinton, W, Obama, Trump (assuming they all stay alive until then). This ties the record for most living ex-presidents. See <a href="https://blog.computationalcomplexity.org/2018/12/george-hw-bush-passed-away-some-non.html">here</a> for my blog post on this. Getting to 6 will be difficult since Carter is 96 years old. </p><p>13) Neither Lance nor I have blogged much about the election, or even about politics. One reason is that whatever I want to say Scott says better (Scott and Lance are the only theory bloggers known by just their first names). I was going to point to Scott's  political blogs but that was hard since he often has blog posts about multiple topics (Like his post about  Mike Pence thinking that the Ind of CH is a sort of relativism that also allows for adultery to be considered okay (see <a href="https://archive.thinkprogress.org/mike-pence-argued-for-criminalization-of-adultery-before-joining-trump-ticket-e27e8a423caa/">here</a> for Pence's pre-Trump views on adultery)  Actually Scott never blogged about Pence and CH  but after reading his posts they kind of blur in my mind.) I will point to one blog entry of his  that I suspect will NOT be relevant but is still very interesting: <a href="https://www.scottaaronson.com/blog/?p=4845">Will he go?</a></p><p>14) Trump claimed the polls showing he was behind were false and part of a conspiracy. I am not sure how this conspiracy would work. If people think their candidate is ahead or behind, then does that affect how or if they vote? Do people say `Gee X is winning, I'll vote for them' ? I doubt it. There are two ways such a conspiracy could work (1)  claim was that some candidate was WAY ahead (it would not matter which one) so you should not bother to vote (2)   in a primary where you are voting on who you think will win the general election.  He also claimed that the early returns saying Biden was winning was a conspiracy. Same problem there- how would that work. This isn't just Trump, other politicians in diff  years claim that early-returns saying X is winning might make it harder for Y to win. I can't see how. </p><p>15) Kamala Harris will be the first women, the first African-American, and the first Asian Veep.  Trivia: There has been a Native American Veep- who was it? More trivia- who coined the term <i>Veep? </i>I won't answer these here, but they might be on my Prez Quiz that I will post after the new President is sworn in.  </p><p>Can she be BOTH the first African-American and the first Asian? Yes.</p><p>16) In my lifetime the election for President was  SETTLED when the losing candidate conceded. This was good for the country's mindset that YES the president is known and even the other candidate agrees. What if Trump does not concede? I doubt this has any practical affect, except that  on Jan 20 he might be trying to arrange a moving van at the last minute. But if the losing candidate does not concede then when is the matter settled?  When the major news venues say it is? Which ones are major? What if there was a really close election and diff news networks declared diff candidates to have won? This does not seem to be a problem for this election cycle, but it is a question: When is the matter SETTLED in that the country ACCEPTS the result, if the losing candidate does not concede?</p><p>17) Carter beat incumbent Ford, but they became friends. Clinton beat incumbent Bush Sr, but they became friends. This is understandable in that so few people are president so they have a shared experience. I doubt that Trump and Biden will become friends.</p><p>18)   Bill Clinton's staff removed W's from the typewriters and did some other damage before W moved into the WH see <a href="https://www.nytimes.com/2002/06/12/us/white-house-vandalized-in-transition-gao-finds.html">here</a>.  This is NOT a tradition, nor is it acceptable in any way, shape. or form.  I do not know of any other similar cases in America (if you do, let me know in the comments).  I wonder if Trump will do damage  to the WH before he leaves. Do presidents put a deposit down on the WH so that any damage they do, they pay for? I  doubt it, but it would be a good idea. </p><p>19)  Obama and Trump had a cordial 90 minute meeting, see <a href="https://www.nytimes.com/2016/11/11/us/politics/white-house-transition-obama-trump.html">here</a>, after Trump won but before he moved in. This makes perfect sense--outgoing presidents know stuff and have experiences worth sharing with the next president.   Obama said North Korea would be a problem and it is (Trump later tried to spin this--`Obama said it would be hard, but it was easy') I wonder if Trump and Biden will have any kind of meeting, cordial or not. </p><p>20) Every state that went for H Clinton in 2016 went for Biden in 2020. The following states went for Trump in 2016 but went for Biden in 2020: Wisc, Mich, PA, AZ, and maybe Georgia and maybe NC (frankly I doubt NC). There was a plausible  scenario (I forget what it was) where Biden would have won 270-268. </p><p>21) Did Third parties matter? In PA the Libertarian Candidate Jo Jorgenson got 1.1% of the vote which was larger than the diff between Biden (49.7) and Trump (49.1) (The Green party either wasn't on the ballot or got so few votes it was not counted). If most of the Libertarians voted for Trump then he would have won PA and possibly the election. However, Trump is not really a Libertarian, so I doubt that would have happened As for the entire country: (1) . The Libertarians got 1.14% of the total vote in 2020, as opposed to 3.25% in 2016, (2)  The Green party got 1.06% in 2016 and 0.02% in 2020. </p><p>21) I was not particular impressed with the satires of the debates and other political satire on SNL this year. Not sure why- maybe Trump is too wild  to satirize and Joltin Jo is too boring. But the following I DID like and is now more relevant. Watch the whole thing since the first half looks like a real ad.</p><p><a href="https://deadline.com/2020/10/snl-cast-members-2020-trump-1234603033/">here</a><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/11/random-thoughts-on-election-2020.html"><span class="datestr">at November 08, 2020 03:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/163">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/163">TR20-163 |  Expander Random Walks: A Fourier-Analytic Approach | 

	Gil Cohen, 

	Amnon Ta-Shma, 

	Noam Peri</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this work we ask the following basic question:  assume the vertices of an expander graph are labelled by $0,1$. What "test" functions $f : \{ 0,1\}^t \to \{0,1\}$ cannot  distinguish  $t$ independent samples from those obtained by a random walk? The expander hitting property due to Ajtai, Komlos and Szemeredi (STOC 1987) is captured by the $\mathrm{AND}$ test function, whereas the fundamental expander Chernoff bound due to Gillman (SICOMP 1998), Heally (Computational Complexity 2008) is about test functions indicating whether the weight is close to the mean. In fact, it is known that all threshold functions are fooled by a random walk (Kipnis and Varadhan, Communications in Mathematical Physics 1986).  Recently, it was shown that even the highly sensitive $\mathrm{PARITY}$ function is fooled by a random walk (Ta-Shma; STOC 2017).  

We focus on balanced labels. Our first main result is proving that all symmetric functions are fooled by a random walk. Put differently, we prove a central limit theorem (CLT) for expander random walks with respect to the total variation distance, significantly strengthening the classic CLT for Markov Chains that is established with respect to the Kolmogorov distance due to Kipnis and Varadhan. Our approach significantly deviates from prior works. We first study how well a Fourier character $\chi_S$ is fooled by a random walk as a function of $S$. Then, given a test function $f$, we expand $f$ in the Fourier basis and combine the above with known results on the Fourier spectrum of $f$. 

We also proceed further and consider general test functions - not necessarily symmetric. As our approach is Fourier analytic, it is general enough to analyze  such versatile test functions. For our second result, we prove that random walks on sufficiently good expander graphs fool tests functions computed by $\mathbf{AC}^0$ circuits, read-once branching programs, and functions with bounded query complexity.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/163"><span class="datestr">at November 08, 2020 11:09 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/162">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/162">TR20-162 |  High-Probability List-Recovery, and Applications to Heavy Hitters | 

	Dean Doron, 

	Mary Wootters</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
An error correcting code $\mathcal{C} \colon \Sigma^k \to \Sigma^n$ is list-recoverable from input list size $\ell$ if for any sets $\mathcal{L}_1, \ldots, \mathcal{L}_n \subseteq \Sigma$ of size at most $\ell$, one can efficiently recover the list $\mathcal{L} = \{ x \in \Sigma^k : \forall j \in [n],  \mathcal{C}(x)_j \in \mathcal{L}_j \}$.  While list-recovery has been well-studied in error correcting codes, all known constructions with "efficient" algorithms are not efficient in the parameter $\ell$.  In this work, motivated by applications in algorithm design and pseudorandomness, we study list-recovery with the goal of obtaining a good dependence on $\ell$.  We make a step towards this goal by obtaining it in the weaker case where we allow a randomized encoding map and a small failure probability, and where the input lists are derived from unions of codewords.  As an application of our construction, we give a data structure for the heavy hitters problem in the strict turnstile model that, for some parameter regimes, obtains stronger guarantees than known constructions.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/162"><span class="datestr">at November 08, 2020 11:06 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://ptreview.sublinear.info/?p=1436">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1436">News for October 2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Sorry for the delay in writing this monthly digest: we hope you didn’t spend the week frantically refreshing your browsers! We found four papers this month: let’s dive in.</p>



<p><strong>A Structural Theorem for Local Algorithms with Applications to Coding, Testing, and Privacy</strong>, by Marcel Dall’Agnol, Tom Gur, and Oded Lachish (<a href="https://eccc.weizmann.ac.il/report/2020/154/">ECCC</a>). This paper introduces the notion of “robust (local) algorithm,” an abstraction which encompasses may types of algorithms: e.g., property testing algorithms, locally decodable codes, etc. The main result of this work is that any (possibly adaptive) \(q\)-query robust local algorithm can be transformed into a non-adaptive, <em>sample-based</em> one making \(n^{1-1/(q^2\log q)}\) queries (where \(n\) is the input size). Here, “sample-based” means that the algorithm doesn’t get to make arbitrary queries, but just gets to observe randomly chosen coordinates of the input. As application of this result, the authors derive new upper and lower bounds for several of the types of local algorithms mentioned above, resolving open questions from previous works.</p>



<p><strong>Testing Tail Weight of a Distribution Via Hazard Rate</strong>, by Maryam Aliakbarpour, Amartya Shankha Biswas, Kavya Ravichandran, Ronitt Rubinfeld (<a href="https://arxiv.org/abs/2010.02888">arXiv</a>). The authors consider, from the point of view of distribution testing, the question of deciding whether data is <em>heavy-tailed</em>: as would, for instance, data following a power law. The paper first sets out to formalize the question, and discusses various possible definitional choices before setting on one; after which it provides a test, and analyzes its sample complexity as a function of various parameters (such as smoothness of the unknown distribution). The authors finally back these results with empirical evaluation of their algorithm.</p>



<p><strong>On Testing of Samplers</strong>, by Kuldeep S. Meel, Yash Pote, Sourav Chakraborty (<a href="https://arxiv.org/abs/2010.12918">arXiv</a>). Suppose you are given a (known) distribution \(p\) over some domain \(\Omega\), and want to sample from it conditioned on some predicate \(\varphi\). Now someone comes to you with an algorithm which does exactly that, efficiently, and cheaply: great! But can you easily check that you’re not getting fooled, and that this sampler actually does what it claims? This paper provides this: an algorithm which accepts if the sampled distribution is \(\varepsilon\)-close to what it should (roughly, in a multiplicative, KL divergence sense), and rejects if it’s \(\varepsilon’\)-far (in total variation distance). The number of samples required is polynomial in \(\varepsilon’-\varepsilon\), and depends on some characteristic of \(p\) and \(\varphi\), the “tilt” (ratio between max and min probability of the conditional distribution).</p>



<p>Finally, an omission from late September:<br /><strong>Sample optimal Quantum identity testing via Pauli Measurements</strong>, by Nengkun Yu (<a href="https://arxiv.org/abs/2009.11518">arXiv</a>). The abstract is concise and clear enough to speak for itself: “In this paper, we show that \(\Theta(\textrm{poly}(n)\cdot\frac{4^n}{\varepsilon^2})\) is the sample complexity of testing whether two \(n\)-qubit quantum states \(\rho\) and \(\sigma\) are identical or \(\varepsilon\)-far in trace distance using two-outcome Pauli measurements.”</p>



<p>Please let us know if you spotted a paper we missed!</p></div>







<p class="date">
by Clement Canonne <a href="https://ptreview.sublinear.info/?p=1436"><span class="datestr">at November 08, 2020 04:53 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5088">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5088">Five Thoughts</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>(1) A friend commented that Biden’s victory becomes more impressive after you contemplate the enthusiasm gap: Trump’s base believed that Trump was sent by God, whereas Biden’s base believed that Biden probably wasn’t a terrible human being.  I replied that what we call the “Enlightenment” was precisely this, the switch from cowering before leaders who were sent by God to demanding leaders who probably aren’t terrible human beings.</p>



<p>(2) I would love for Twitter to deactivate Trump’s account—<em>not</em> for any ideological reason, simply for Trump’s hundreds of past violations of Twitter’s Terms of Service, and for there no longer being a compelling public interest in what Trump has to say that would override all his Terms of Service violations.</p>



<p>(3) When Biden appeared last night, and then again tonight, it wasn’t merely that he came across like a President-Elect of the US, but rather that he came across like a President-Elect of the US <em>who’s filling a vacant position</em>.  Until Biden starts, there won’t <em>be</em> a president of the US; there will only continue to be the president of those who voted for him.</p>



<p>(4) Now that Trump has gone this far in shattering all the norms of succession, part of me <em>wants</em> to see him go the rest of the way … to being physically dragged out of the Oval Office by Secret Service agents on January 20, in pathetic and humiliating footage that would define how future generations remembered him.</p>



<p>(5) I had an idea for something that could make a permanent contribution to protecting liberal democracy in the US, and that anti-Trump forces could implement unilaterally for a few tens of millions of dollars—no need to win another election.  The idea is to build a Donald J. Trump Historical Museum in Washington, DC.  But, you see, this museum would effectively be the opposite of a presidential library.  It would be designed by professional historians; they might solicit cooperation from former members of Trump’s inner circle, but would never depend on it.  It would, in fact, be a museum that teenage students might tend to be taken to on the same DC field trips that also brought them to the Vietnam Memorial and the United States Holocaust Memorial Museum (USHMM).  Obviously, the new museum would be <em>different</em> from those bleak places; it would (thankfully) have a little less tragedy and more farce … and that’s precisely the role that the new museum would fill.  To show the kids on the field trips that it’s not <em>always</em> unmitigated horribleness, that here was a case where we Americans took a gigantic stumble backwards, seeming to want to recreate the first few rooms in the USHMM exhibition, the one where the macho-talking clown thrills Germany by being serious rather than literal.  <em>But then, here in the US, we successfully stopped it before it got to the later rooms</em>.  Sure, the victory wasn’t as decisive as we would’ve liked, it came at a great cost, but it was victory nonetheless.  A 244-year-old experiment in self-governance is back in operation.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5088"><span class="datestr">at November 08, 2020 01:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=64">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/cauchy-residue-formula/">The Cauchy residue trick: spectral analysis made “easy”</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">In many areas of machine learning, statistics and signal processing, eigenvalue decompositions are commonly used, e.g., in principal component analysis, spectral clustering, convergence analysis of Markov chains, convergence analysis of optimization algorithms, low-rank inducing regularizers, community detection, seriation, etc.</p>



<p class="justify-text">Understanding how the spectral decomposition of a matrix changes as a function of a matrix is thus of primary importance, both algorithmically and theoretically. We thus need a perturbation analysis or more generally some differentiability properties for eigenvalues or eigenvectors [1], or any spectral function [2]. These properties can be obtained from many angles, but a generic tool can be used for all of these: it is a surprising and elegant application of Cauchy’s residue formula, which is due to Kato [3].</p>



<p class="justify-text">Before diving into spectral analysis, I will first present the Cauchy residue theorem and some nice applications in computing integrals that are needed in machine learning and kernel methods.</p>



<h2>Cauchy residue formula </h2>



<p class="justify-text">A function \(f : \mathbb{C} \to \mathbb{C}\) is said <em>holomorphic</em> in \(\lambda \in \mathbb{C}\) with derivative \(f'(\lambda) \in \mathbb{C}\), if is differentiable in \(\lambda\), that is if \(\displaystyle \frac{f(z)-f(\lambda)}{z-\lambda}\) tends to \(f'(\lambda)\) when \(z\) tends to \(\lambda\). Many classical functions are holomorphic on \(\mathbb{C}\) or portions thereof, such as the exponential, sines, cosines and their hyperbolic counterparts, rational functions, portions of the logarithm.</p>



<p class="justify-text">We consider a function which is holomorphic in a region of \(\mathbb{C}\) except in \(m\) values \(\lambda_1,\dots,\lambda_m \in \mathbb{C}\), which are usually referred to as <em>poles</em>. We also consider a simple closed directed contour \(\gamma\) in \(\mathbb{C}\) that goes strictly around the \(m\) values above. </p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="549" alt="" src="https://francisbach.com/wp-content/uploads/2020/10/contour_generic-1-1024x440.png" class="wp-image-5002" height="236" /></figure></div>



<p class="justify-text">The Cauchy residue formula gives an explicit formula for the contour integral along \(\gamma\):<br /> $$ \oint_\gamma f(z) dz = 2 i \pi \sum_{j=1}^m {\rm Res}(f,\lambda_j), \tag{1}$$<br /> where \({\rm Res}(f,\lambda)\) is called the <em>residue</em> of \(f\) at \(\lambda\) . If around \(\lambda\),  \(f(z)\) has a series expansions in powers of \((z − \lambda)\), that is, \(\displaystyle f(z) =  \sum_{k=-\infty}^{+\infty}a_k (z −\lambda)^k\), then \({\rm Res}(f,\lambda)=a_{-1}\).</p>



<p class="justify-text">For example, if \(\displaystyle f(z) =   \frac{g(z)}{z-\lambda}\) with \(g\) holomorphic around \(\lambda\), then \({\rm Res}(f,\lambda) = g(\lambda)\), and more generally, if \(\displaystyle f(z) = \frac{g(z)}{(z-\lambda)^k}\) for \(k \geqslant  1\), then \(\displaystyle {\rm Res}(f,\lambda) = \frac{g^{(k-1)}(\lambda) }{(k-1)!}\). For more details on complex analysis, see [4].</p>



<p class="justify-text">The result above can be naturally extended to vector-valued functions (and thus to any matrix-valued function), by applying the identity to all components of the vector.</p>



<p class="justify-text">This result is due to <a href="https://en.wikipedia.org/wiki/Augustin-Louis_Cauchy">Cauchy</a> [<a href="https://archive.org/details/mmoiresurlesin00cauc">10</a>] in 1825. The <a href="https://archive.org/details/mmoiresurlesin00cauc">original paper</a> where this is presented is a nice read in French where you can find some pepits like “la fonction s’évanouit pour \(x = \infty\)”.</p>



<p>If you are already familiar with complex residues, you can skip the next section.</p>



<h2>Where does it come from?</h2>



<p class="justify-text">At first, the formula in Eq. (1) seems unsettling. Why doesn’t the result depend more explicitly on the contour \(\gamma\)? Where does the multiplicative term \( {2i\pi}\) come from? Here is a very partial and non rigorous account (go to the <a href="https://terrytao.wordpress.com/tag/residue-theorem/">experts</a> for more rigor!).</p>



<p class="justify-text">Complex-valued functions on \(\mathbb{C}\) can be seen as functions from \(\mathbb{R}^2\) to itself, by writing $$ f(x+iy) = u(x,y) + i v(x,y),$$ where \(u\) and \(v\) are real-valued functions. We have thus a function \((x,y) \mapsto (u(x,y),v(x,y))\) from \(\mathbb{R}^2\) to \(\mathbb{R}^2\). Expanding \(f(z+dz) = f(z) + f'(z) dz\), which is the definition of complex differentiability, into real and imaginary parts, we get (using \(i^2 = -1\)): $$\left\{ \begin{array}{l} u(x+dx,y+dy) = u(x,y) + {\rm Re}(f'(z)) dx\ – {\rm Im}(f'(z)) dy \\ v(x+dx,y+dy) = v(x,y) + {\rm Re}(f'(z)) dy + {\rm Im}(f'(z)) dx. \end{array}\right.$$ This leads to $$\left\{ \begin{array}{l} \displaystyle \frac{\partial u}{\partial x}(x,y) = {\rm Re}(f'(z)) \\ \displaystyle \frac{\partial u}{\partial y}(x,y) = \ – {\rm Im}(f'(z)) \\ \displaystyle \frac{\partial v}{\partial x}(x,y) = {\rm Im}(f'(z)) \\ \displaystyle \frac{\partial v}{\partial y}(x,y) = {\rm Re}(f'(z)). \end{array}\right.$$</p>



<p class="justify-text">This in turn leads to the <a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Riemann_equations">Cauchy-Riemann equations</a> \(\displaystyle \frac{\partial u}{\partial x} = \frac{\partial v}{\partial y}\) and \(\displaystyle \frac{\partial u}{\partial y} = -\frac{\partial v}{\partial x}\), which are essentially necessary and sufficient conditions to be holomorphic. Thus holomorphic functions correspond to differentiable functions on \(\mathbb{R}^2\) with some equal partial derivatives. These equations are key to obtaining the Cauchy residue formula.</p>



<p class="justify-text"><strong>Contour integral with no poles. </strong>We first consider a contour integral over a contour \(\gamma\) enclosing a region \(\mathcal{D}\) where the function \(f\) is holomorphic everywhere. The contour \(\gamma\) is defined as a differentiable function \(\gamma: [0,1] \to \mathbb{C}\), and the integral is equal to $$\oint_\gamma f(z) dz = \int_0^1 \!\!f(\gamma(t)) \gamma'(t) dt = \int_0^1 \!\![u(x(t),y(t)) +i v(x(t),y(t))] [ x'(t) + i y'(t)] dt,$$ where \(x(t) = {\rm Re}(\gamma(t))\) and \(y(t) = {\rm Im}(\gamma(t))\). By expanding the product of complex numbers, it is thus equal to $$\int_0^1 [ u(x(t),y(t)) x'(t) \ – v(x(t),y(t))y'(t)] dt +i  \int_0^1 [ v(x(t),y(t)) x'(t) +u (x(t),y(t))y'(t)] dt,$$ which we can rewrite in compact form as (with \(dx = x'(t) dt\) and \(dy = y'(t)dt\)): $$\oint_\gamma ( u \, dx\  – v \, dy ) + i \oint_\gamma ( v \, dx + u \, dy ).$$ We can then use <a href="https://en.wikipedia.org/wiki/Green%27s_theorem">Green’s theorem</a> because our functions are differentiable on the entire region \(\mathcal{D}\) (the set “inside” the contour), to get $$\oint_\gamma ( u \, dx\ – v \, dy ) + i \oint_\gamma ( v \, dx + u \, dy ) =\  – \int\!\!\!\!\int_\mathcal{D} \! \Big( \frac{\partial v}{\partial x} + \frac{\partial u}{\partial y} \Big) dx dy \ – i \!\! \int\!\!\!\!\int_\mathcal{D} \!\Big( \frac{\partial u}{\partial x} – \frac{\partial v}{\partial y} \Big) dx dy.$$ Thus, because of the Cauchy-Riemann equations, the contour integral is always zero within the domain of differentiability of \(f\). Note that this extends to piecewise smooth contours \(\gamma\).</p>



<p class="justify-text"><strong>Circle and rational functions.</strong> For a circle contour of center \(\lambda \in \mathbb{C}\) and radius \(r\), we have, with \(\gamma(t) = \lambda + re^{ 2i \pi t}\): $$\oint_{\gamma} \frac{dz}{(z-\lambda)^k} =\int_0^{1} \frac{ 2r i \pi e^{2i\pi t}}{ r^k e^{2i\pi kt}}dt= \int_0^{1} r^{1-k} i e^{2i\pi (1-k)t} dt,$$ which is equal to zero if \(k \neq 1\), and to \(\int_0^{1} 2i\pi dt = 2 i \pi\) for \(k =1\). Thus, for a function with a series expansion, the Cauchy residue formula is true for the circle around a single pole, because only the term in \(\frac{1}{z-\lambda}\) contributes.</p>



<p class="justify-text"><strong>No dependence on the contour.</strong> Now that the Cauchy formula is true for the circle around a single pole, we can “deform” the contour below to a circle.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="398" alt="" src="https://francisbach.com/wp-content/uploads/2020/10/contour_generic_circle-1024x532.png" class="wp-image-5021" height="207" /></figure></div>



<p>This can be done considering two contours \(\gamma_1\) and \(\gamma_2\) below with no poles inside, and thus with zero contour integrals, and for which the integrals along the added lines cancel.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="391" alt="" src="https://francisbach.com/wp-content/uploads/2020/10/contour_generic_circle_cut-1-1024x537.png" class="wp-image-5022" height="205" /></figure></div>



<p class="justify-text">This “shows” that the integral does not depend on the contour, and so in applications we can be quite liberal in the choice of contour. Note that similar constructions can be used to take into account several poles.</p>



<p class="justify-text">Before going to the spectral analysis of matrices, let us explore some cool choices of contours and integrands, and (again!) some positive definite kernels.</p>



<h2>Classical examples</h2>



<p class="justify-text">The Cauchy residue theorem can be used to compute integrals, by choosing the appropriate contour, looking for poles and computing the associated residues. Here are classical examples, before I show applications to kernel methods. See more examples in <a href="http://residuetheorem.com/">http://residuetheorem.com/</a>, and many in [11].</p>



<p class="justify-text"><strong>Fourier transforms. </strong> For \(\omega&gt;0\), we can compute \( \displaystyle \int_{-\infty}^\infty \!\! f(x) e^{ i \omega x} dx\) for holomorphic functions \(f\) by integrating on the real line and a big upper circle as shown below, with \(R\) tending to infinity (so that the contribution of the half-circle goes to zero because of the exponential term). This leads to \(2i \pi\) times the sum of all residues of the function \(z \mapsto f(z) e^{ i \omega z}\) in the upper half plane. See an example below related to kernel methods.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="385" alt="" src="https://francisbach.com/wp-content/uploads/2020/10/contour_upper-circle-2-1024x570.png" class="wp-image-5026" height="214" /></figure></div>



<p class="justify-text"><strong>Trigonometric integrals</strong>. For holomorphic functions \(Q\), we can compute the integral \(\displaystyle \int_0^{2\pi} \!\!\! Q(\cos \theta, \sin \theta) d\theta\). Indeed, letting \(f(z) = \frac{1}{iz} Q\big( \frac{z+z^{-1}}{2}, \frac{z-z^{-1}}{2i} \big)\), it is exactly equal to the integral on the unit circle. The desired integral is then equal to \(2i\pi\) times the sum of all residues of \(f\) within the unit disk.</p>



<p class="justify-text">For example, when \(Q(\cos \theta, \sin \theta) = \frac{1}{2 + \sin \theta}\), we have \(f(z) = \frac{2}{z^2+4iz-1}\), with a single pole inside the unit circle, namely \(\lambda = i ( \sqrt{3}-2)\), and residue equal to \(-i / \sqrt{3}\), leading to \(\int_0^{2\pi} \frac{d\theta}{2+\sin \theta} = \frac{2\pi}{\sqrt{3}}\).</p>



<p class="justify-text"><strong>Series.</strong> If the function \(f\) is holomorphic and has no poles at integer real values, and satisfies some basic boundedness conditions, then $$\sum_{n \in \mathbb{Z}} f(n) = \ – \!\!\! \sum_{ \lambda \in {\rm poles}(f)} {\rm Res}\big( f(z) \pi \frac{\cos \pi z}{\sin \pi z} ,\lambda\big).$$ This is a simple consequence of the fact that the function \(z \mapsto \pi \frac{\cos \pi z}{\sin \pi z}\) has all integers \(n \in \mathbb{Z}\) as poles, with corresponding residue equal to \(1\). This is obtained from the contour below with \(m\) tending to infinity.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="284" alt="" src="https://francisbach.com/wp-content/uploads/2020/10/contour_square-1-1024x903.png" class="wp-image-5028" height="250" /></figure></div>



<p class="justify-text">The same trick can be applied to  \(\displaystyle \sum_{n \in \mathbb{Z}} (-1)^n f(n) =\  –  \!\!\! \sum_{ \lambda \in {\rm poles}(f)} {\rm Res}\big( f(z) \pi \frac{1}{\sin \pi z} ,\lambda\big).\) See [7, Section 11.2] for more details. Experts will see an interesting link with the <a href="https://en.wikipedia.org/wiki/Euler%E2%80%93Maclaurin_formula">Euler-MacLaurin formula</a> and <a href="https://en.wikipedia.org/wiki/Bernoulli_polynomials">Bernoulli polynomials</a>.</p>



<p class="justify-text"><strong>Applications to kernel methods.</strong> In non-parametric estimation, regularization penalties are used to constrain real-values functions to be smooth. One such examples are combinations of squared \(L_2\) norms of derivatives. For functions \(f\) defined on an interval \(I\) of the real line, penalties are typically of the form \(\int_I \sum_{k=0}^s \alpha_k | f^{(k)}(x)|^2 dx\), for non-negative weights \(\alpha_0,\dots,\alpha_k\). For these Sobolev space norms, a positive definite kernel \(K\) can be used for estimation (see, e.g., <a href="https://francisbach.com/hermite-polynomials/">last month blog post</a>). </p>



<p class="justify-text">A classical question is: given the norm defined above, how to compute \(K\)? For \(I = \mathbb{R}\), then this can be done using Fourier transforms as: $$K(x,y) =  \frac{1}{2\pi} \int_\mathbb{R} \frac{e^{i\omega(x-y)}}{\sum_{k=0}^s \alpha_k \omega^{2k}} d\omega.$$ This is exactly an integral of the form above, for which we can use the contour integration technique. For example, for \(\alpha_0=1\) and \(\alpha_1=a^2\), we get for \(x-y&gt;0\), one pole \(i/a\) in the upper half plane for the function \(\frac{1}{1+a^2 z^2} = \frac{1}{(1+iaz)(1-iaz)}\), with residue \(-\frac{i}{2a} e^{-(x-y)/a}\), leading to the familiar exponential kernel \(K(x,y) = \frac{1}{2a} e^{-|x-y|/a}\). More complex kernels can be considered (see, e.g., [8, page 277], for \(\sum_{k=0}^s \alpha_k \omega^{2k} = 1 + \omega^{2s}\)).</p>



<p class="justify-text">We can also consider the same penalty on the unit interval \([0,1]\) with periodic functions, leading to the kernel (see [9] for more details): $$ K(x,y) = \sum_{n \in \mathbb{Z}} \frac{ e^{2in\pi(x-y)}}{\sum_{k=0}^s \alpha_k( 2n\pi)^s}.$$ For the same example as above, that is, \(\alpha_0=1\) and \(\alpha_1=a^2\), this leads to an infinite series on which we can apply the Cauchy residue formula as explained above. This leads to, for \(x-y \in [0,1]\), \(K(x,y) =  \frac{1}{2a}  \frac{ \cosh (\frac{1-2(x-y)}{2a})}{\sinh (\frac{1}{2a})}\). We can then extend by \(1\)-periodicity to all \(x-y\). See the detailed computation at the end of the post.</p>



<p>Now that you are all experts in residue calculus, we can move on to spectral analysis.</p>



<h2>Spectral analysis of symmetric matrices</h2>



<p class="justify-text">We consider a symmetric matrix \(A \in \mathbb{R}^{n \times n}\), with its \(n\) ordered real eigenvalues \(\lambda_1 \geqslant  \cdots \geqslant \lambda_n\), counted with their orders of multiplicity, and an orthonormal basis of their eigenvectors \(u_j \in \mathbb{R}^n\), \(j=1,\dots,n\). We have \(A = \sum_{j=1}^n \lambda_j u_j u_j^\top\).  When we consider eigenvalues as functions of \(A\), we use the notation \(\lambda_j(A)\), \(j=1,\dots,n\). These functions are always well-defined even when eigenvalues are multiple (this is not the case for eigenvectors because of the invariance by orthogonal transformations).</p>



<p class="justify-text">The key property that we will use below is that we can express the so-called resolvent matrix \((z I – A)^{-1} \in \mathbb{C}^{n \times n}\), for \(z \in \mathbb{C}\), as: $$  (z I- A)^{-1}  = \sum_{j=1}^n \frac{1}{z-\lambda_j} u_j u_j^\top. $$ The dependence on \(z\) of the form \( \displaystyle \frac{1}{z- \lambda_j}\)  leads to a nice application of Cauchy residue formula.</p>



<p class="justify-text">Assuming the \(k\)-th eigenvalue \(\lambda_k\) is simple, we consider the contour \(\gamma\) going strictly around \(\lambda_k\) like below (for \(k=5\)).</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="431" alt="" src="https://francisbach.com/wp-content/uploads/2020/10/contour_single_eigenvalue-1024x433.png" class="wp-image-5039" height="182" /></figure></div>



<p class="justify-text">We consider integrating the matrix above, which leads to: $$ \oint_\gamma<br />  (z I- A)^{-1} dz = \sum_{j=1}^m \Big( \oint_\gamma \frac{1}{z – \lambda_j} dz \Big) u_j u_j^\top<br />  = 2 i \pi \  u_k u_k^\top $$  using the identity \(\displaystyle \oint_\gamma \frac{1}{z – \lambda_j} dz = 1\) if \(j=k\) and \(0\) otherwise (because the pole is outside of \(\gamma\)). We thus obtain an expression for projectors on the one-dimensional eigen-subspace associated with the eigenvalue \(\lambda_k\).</p>



<p class="justify-text">With simple manipulations, we can also access the eigenvalues. Indeed, we have: $$ \oint_\gamma<br />   (z I- A)^{-1} z dz = \sum_{j=1}^m \Big( \oint_\gamma \frac{z}{z – \lambda_j} dz \Big) u_j u_j^\top<br />  = 2 i \pi \lambda_k  u_k u_k^\top, $$ and by taking the trace, we obtain $$ \oint_\gamma<br />  {\rm tr} \big[ z (z I- A)^{-1} \big]  dz    = \lambda_k. $$ The key benefit of these representations is that when the matrix \(A\) is slightly perturbed, then the same contour \(\gamma\) can be used to enclose the corresponding eigenvalues of the perturbed matrix, and perturbation results are simply obtained by taking gradients within the contour integral. Note that several eigenvalues may be summed up by selecting a contour englobing more than one eigenvalues.</p>



<h2>Gradients of eigenvalues</h2>



<p class="justify-text">The expression with contour integrals allows to derive simple formulas for gradients of eigenvalues. These can be obtained by other means [5], but using contour integrals shows that this is simply done by looking at the differential of \((z I – A)^{-1}\) and integrating it. The central component is the following expansion, which is a classical result in matrix differentiable calculus, with \(\|\Delta\|_2\) the operator norm of \(\Delta\) (i.e., its largest singular value):  $$<br /> (z I- A – \Delta)^{-1} = (z I – A)^{-1} + (z I- A)^{-1} \Delta (z I- A)^{-1} + o(\| \Delta\|_2).  $$ Note here that the asymptotic remainder \(o(\| \Delta\|_2)\) can be made explicit. </p>



<p class="justify-text">By expanding the expression on the basis of eigenvectors of \(A\), we get  $$<br /> z (z I- A – \Delta)^{-1}  – z (z I- A)^{-1}  =  \sum_{j=1}^n \sum_{\ell=1}^n u_j u_\ell^\top  \frac{ z \cdot u_j^\top \Delta u_\ell}{(z-\lambda_j)(z-\lambda_\ell)} + o(\| \Delta \|_2).<br /> $$ Taking the trace, the cross-product terms \({\rm tr}(u_j u_\ell^\top) =  u_\ell^\top u_j\) disappear for \(j \neq \ell\), and we get: $$<br /> {\rm tr} \big[ z (z I – A – \Delta)^{-1}  \big] – {\rm tr} \big[ z (z I – A)^{-1}  \big]=  \sum_{j=1}^n   \frac{ z \cdot u_j^\top \Delta u_j}{(z-\lambda_j)^2} + o(\| \Delta \|_2).<br /> $$ This leads to, by contour integration:<br />$$<br /> \lambda_{k}(A+\Delta) -\lambda_k(A)<br /> =<br /> \frac{1}{2i \pi} \oint_\gamma \Big[ <br />   \sum_{j=1}^n   \frac{ z \cdot u_j^\top \Delta u_j}{(z-\lambda_j)^2} \Big] dz +  o(\| \Delta \|_2).<br /> $$ By keeping only the pole \(\lambda_k\) which is inside the contour \(\gamma\), we get  $$ \lambda_{k}(A+\Delta) -\lambda_k(A)<br />  =<br /> \frac{1}{2i \pi} \oint_\gamma \Big[ <br />    \frac{ z \cdot u_k^\top \Delta u_k}{(z-\lambda_k)^2} \Big] dz +  o(| \Delta |_2) \<br />    =  u_k^\top \Delta u_k + o(\| \Delta \|_2),<br /> $$ using the identity \(\displaystyle <br />  \oint_\gamma \frac{z dz}{(z – \lambda_k)^2} dz = <br />   \oint_\gamma \Big( \frac{\lambda_k}{(z – \lambda_k)^2}  + \frac{1}{z – \lambda_k} \Big) dz = 1\).  </p>



<p class="justify-text">Thus the gradient of \(\lambda_k\) at a matrix \(A\) where the \(k\)-th eigenvalue is simple is simply \( u_k u_k^\top\), where \(u_k\) is a corresponding eigenvector. Note that this result can be simply obtained by the simple (rough) calculation: if \(x\) is a unit eigenvector of \(A\), then \(Ax =\lambda x\), and \(x^\top x = 1\), leading to \(x^\top dx = 0\) and \(dA\ x + A dx = d\lambda \ x + \lambda dx\), and by taking the dot product with \(x\), \(d\lambda = x^\top dA\ x + x^\top A dx = x^\top dA \ x + \lambda x^\top  dx = x^\top dA \ x\), which is the same result. However, this reasoning is more cumbersome, and does not lead to neat approximation guarantees, in particular in the extensions below.</p>



<h2>Other perturbation results</h2>



<p class="justify-text">Given the gradient, other more classical perturbation results could de derived, such as Hessians of eigenvalues, or gradient of the projectors \(u_k u_k^\top\).  Here I derive a perturbation result for the projector \(\Pi_k(A)=u_k u_k^\top\), when \(\lambda_k\) is a simple eigenvalue. Using the same technique as above, we get: $$ \Pi_k(A+\Delta )\  – \Pi_k(A) = \frac{1}{2i \pi} \oint_\gamma (z I- A)^{-1} \Delta (z I – A)^{-1}dz  + o(\| \Delta\|_2),$$ which we can expand to the basis of eigenvectors as $$ \frac{1}{2i \pi} \oint_\gamma \sum_{j=1}^n \sum_{\ell=1}^n u_j u_j^\top \Delta u_\ell u_\ell^\top \frac{  dz}{(z-\lambda_\ell) (z-\lambda_j)  } + o(\| \Delta\|_2).$$ We can then split in two, with the two terms (all others are equal to zero by lack of poles within \(\gamma\)): $$ \frac{1}{2i \pi} \oint_\gamma \sum_{j \neq k}  u_j^\top \Delta u_k  ( u_j u_k^\top + u_k u_j^\top)   \frac{  dz}{(z-\lambda_k) (z-\lambda_j)  }= \sum_{j \neq k}  u_j^\top \Delta u_k  ( u_j u_k^\top + u_k u_j^\top) \frac{1}{\lambda_k – \lambda_j}  $$ and $$\frac{1}{2i \pi} \oint_\gamma   u_k^\top \Delta u_k   u_k u_k^\top  \frac{  dz}{(z-\lambda_k)^2  } = 0 ,$$ finally leading to $$\Pi_k(A+\Delta ) \ – \Pi_k(A) =  \sum_{j \neq k}  \frac{u_j^\top \Delta u_k}{\lambda_k – \lambda_j}    ( u_j u_k^\top + u_k u_j^\top)    + o(\| \Delta\|_2),$$ from which we can compute the Jacobian of \(\Pi_k\).</p>



<h2>Spectral functions</h2>



<p class="justify-text">Spectral functions are functions on symmetric matrices defined as \(F(A) = \sum_{k=1}^n f(\lambda_k(A))\), for any real-valued function \(f\). For \(f(x) = x\), we get back the trace, for \(f(x) = \log x\) we get back the log determinant, and so on. The function \(F\) can be represented as $$F(A) = \sum_{k=1}^n f(\lambda_k(A)) = \frac{1}{2i \pi} \oint_\gamma f(z) {\rm tr} \big[ (z I  – A)^{-1} \big] dz,$$ where the contour \(\gamma\) encloses all eigenvalues (as shown below).</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="438" alt="" src="https://francisbach.com/wp-content/uploads/2020/11/contour_all_eigenvalues-1024x433.png" class="wp-image-5087" height="185" /></figure></div>



<p class="justify-text">This representation can be used to compute derivatives of \(F\), by simple derivations, to obtain the same result as [<a href="https://epubs.siam.org/doi/pdf/10.1137/S089547980036838X">12</a>].</p>



<h2>Singular values of rectangular matrices</h2>



<p class="justify-text">Singular value decompositions are also often used, for a rectangular matrix \(W \in \mathbb{R}^{n \times d}\). It consists in finding \(r\) pairs \((u_j,v_j) \in \mathbb{R}^{n} \times \mathbb{R}^d\), \(j=1,\dots,r\), of singular vectors and \(r\) positive singular values \(\sigma_1 \geqslant \cdots \geqslant \sigma_r &gt; 0\) such that \(W = \sum_{j=1}^r \sigma_j u_j v_j^\top\) and  \((u_1,\dots,u_r)\) and \((v_1,\dots,v_r)\) are orthonormal families.</p>



<p class="justify-text">There are two natural ways to relate the singular value decomposition to the classical eigenvalue decomposition of a symmetric matrix, �first through \(WW^\top\) (or similarly \(W^\top W\)). Here it is more direct to consider the so-called Jordan-Wielandt matrix, defined by blocks as $$<br /> \bar{W} = \left( \begin{array}{cc}<br />0 &amp; W \\<br />W^\top &amp; 0 \end{array} \right). $$ The matrix \(\bar{W}\) is symmetric, and its non zero eigenvalues are \(+\sigma_i\) and \(-\sigma_i\), \(i=1,\dots,r\), associated with the eigenvectors \(\frac{1}{\sqrt{2}}  \left( \begin{array}{cc}<br />u_i \\ v_i \end{array} \right)\) and \(\frac{1}{\sqrt{2}}  \left( \begin{array}{cc}<br />u_i \\ -v_i \end{array} \right)\).</p>



<p class="justify-text">All necessary results (derivatives of singular values \(\sigma_j\), or projectors \(u_j v_j^\top\) can be obtained from there); see more details, in, e.g., the appendix of [6].</p>



<h2>Going beyond</h2>



<p class="justify-text">In this post, I have shown various applications of the Cauchy residue formula, for computing integrals and for the spectral analysis of matrices. I have just scratched the surface of spectral analysis, and what I presented extends to many interesting situations, for example, to more general linear operators in infinite-dimensional spaces [3], or to the analysis fo the eigenvalue distribution of random matrices (see a nice and reasonably simple derivation of the semi-circular law from <a href="https://terrytao.wordpress.com/2010/02/02/254a-notes-4-the-semi-circular-law/">Terry Tao’s blog</a>).</p>



<h2>References</h2>



<p class="justify-text">[1] Gilbert W. Stewart and Sun Ji-Huang. <em>Matrix Perturbation Theory</em>. Academic Press, 1990.<br />[2] Adrian Stephen Lewis. Derivatives of spectral functions. <em>Mathematics of Operations Research</em>, 21(3):576–588, 1996. <br />[3] Tosio Kato. <em>Perturbation Theory for Linear Operators</em>, volume 132. Springer, 2013. <br />[4] Serge Lang. <em>Complex Analysis</em>, volume 103. Springer, 2013. <br />[5] Jan R. Magnus. On differentiating eigenvalues and eigenvectors. <em>Econometric Theory</em>, 1(2):179–191, 1985. <br />[6] Francis Bach. Consistency of trace norm minimization. <em>Journal of Machine Learning Research</em>, 9:1019-1048, 2008.<br />[7] Joseph Bak, Donald J. Newman. <em>Complex analysis</em>. New York: Springer, 2010.<br />[8] Alain Berlinet, and Christine Thomas-Agnan. <em>Reproducing kernel Hilbert spaces in probability and statistics</em>. Springer Science &amp; Business Media, 2011.<br />[9] Grace Wahba. <a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970128">Spline models for observational data</a>. Society for Industrial and Applied Mathematics, 1990.<br />[10] Augustin Louis Cauchy, <a href="https://archive.org/details/mmoiresurlesin00cauc">Mémoire</a><a href="http://www.numdam.org/article/BSMA_1874__7__265_0.pdf"> sur les intégrales définies, prises entre des limites imaginaires</a>, 1825, <a href="http://www.numdam.org/article/BSMA_1874__7__265_0.pdf">re-published</a> in Bulletin des Sciences Mathématiques et Astronomiques, Tome 7, 265-304, 1874.<br />[11] Dragoslav S. Mitrinovic, and Jovan D. Keckic. <em>The Cauchy method of residues: theory and applications</em>. Vol. 9. Springer Science &amp; Business Media, 1984.<br />[12] Adrian S. Lewis, and Hristo S. Sendov. <a href="https://epubs.siam.org/doi/pdf/10.1137/S089547980036838X">Twice differentiable spectral functions</a>. <em>SIAM Journal on Matrix Analysis and Applications</em> 23.2: 368-386, 2001.</p>



<h2>Computing the Sobolev kernel</h2>



<p class="justify-text">The goal is to compute the infinite sum $$\sum_{n \in \mathbb{Z}} \frac{e^{2i\pi q \cdot n}}{1+(2a \pi n)^2}$$ for \(q \in (0,1)\). We consider the function $$f(z) = \frac{e^{i\pi (2q-1) z}}{1+(2a \pi z)^2} \frac{\pi}{\sin (\pi z)}.$$ It is holomorphic on \(\mathbb{C}\) except at all integers \(n \in \mathbb{Z}\), where it has a simple pole with residue \(\displaystyle \frac{e^{i\pi (2q-1) n}}{1+(2a \pi n)^2} (-1)^n = \frac{e^{i\pi 2q n}}{1+(2a \pi n)^2}\), at \(z = i/(2a\pi)\) where it has a residue equal to \(\displaystyle \frac{e^{ – (2q-1)/(2a)}}{4ia\pi} \frac{\pi}{\sin (i/(2a))} = \ – \frac{e^{ – (2q-1)/(2a)}}{4a} \frac{1}{\sinh (1/(2a))}\), and at \(z = -i/(2a\pi)\) where it has a residue equal to \(\displaystyle \frac{e^{  (2q-1)/(2a)}}{4ia\pi} \frac{\pi}{\sin (i/(2a))} =\ – \frac{e^{ (2q-1)/(2a)}}{4a} \frac{1}{\sinh (1/(2a))}\). With all residues summing to zero (note that this fact requires a precise analysis of limits when \(m\) tends to infinity for the contour defined in the main text), we get: $$\sum_{n \in \mathbb{Z}} \frac{e^{2i\pi q \cdot n}}{1+(2a \pi n)^2} =\frac{e^{ – (2q-1)/(2a)}}{4a} \frac{1}{\sinh (1/(2a))}+ \frac{e^{ (2q-1)/(2a)}}{4a} \frac{1}{\sinh (1/(2a))} = \frac{1}{2a} \frac{ \cosh (\frac{2q-1}{2a})}{\sinh (\frac{1}{2a})}.$$</p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/cauchy-residue-formula/"><span class="datestr">at November 07, 2020 12:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/11/07/faculty-at-university-of-rochester-apply-by-january-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/11/07/faculty-at-university-of-rochester-apply-by-january-1-2021/">faculty at University of Rochester (apply by January 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Computer Science Department at the University of Rochester seeks applicants for tenure-track faculty positions. We are particularly eager to hire in theory, security/privacy/cryptography, quantum computing, data management, natural language processing, and machine learning. Candidates at any level of seniority are encouraged to apply.</p>
<p>Website: <a href="https://www.rochester.edu/faculty-recruiting/positions/show/10942">https://www.rochester.edu/faculty-recruiting/positions/show/10942</a><br />
Email: stefanko@cs.rochester.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/11/07/faculty-at-university-of-rochester-apply-by-january-1-2021/"><span class="datestr">at November 07, 2020 12:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=509">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2020/11/06/tcs-talk-wednesday-november-11-shuai-shao-uw-madison/">TCS+ talk: Wednesday, November 11 — Shuai Shao, UW-Madison</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, November 11th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Shuai Shao</strong> from UW-Madison will speak about “<em>A Dichotomy for Real Boolean Holant Problems</em>” (abstract below). </p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<p class="wp-block-quote">Abstract: In this talk, we present a complexity dichotomy for Holant problems on the boolean domain with arbitrary sets of real-valued constraint functions. These constraint functions need not be symmetric nor do we assume any auxiliary functions as in previous results. It is proved that for every set <img src="https://s0.wp.com/latex.php?latex=F&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="F" class="latex" title="F" /> of real-valued constraint functions, <img src="https://s0.wp.com/latex.php?latex=%5Ctext%7BHolant%7D%28F%29&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\text{Holant}(F)" class="latex" title="\text{Holant}(F)" /> is either <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="P" class="latex" title="P" />-time computable or <img src="https://s0.wp.com/latex.php?latex=%5C%23P&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\#P" class="latex" title="\#P" />-hard. The classification has an explicit criterion. This is a culmination of much research on a decade-long classification program for Holant problems, and it uses previous results and techniques from many researchers.  However, as it turned out, the journey to the present theorem has been arduous. Some particularly intriguing concrete functions f6, f8 and their associated families with extraordinary closure properties related to Bell states in quantum information theory play an important role in this proof.  <br /><br />Based on joint work with Jin-Yi Cai.</p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2020/11/06/tcs-talk-wednesday-november-11-shuai-shao-uw-madison/"><span class="datestr">at November 07, 2020 04:30 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5071">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5071">On the removal of a hideous growth</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>The title of this post is not an allegory.</p>



<p>At 10am this morning, I had a previously-scheduled appointment with an oral surgeon to remove a large, hideous, occasionally painful growth on the inside of my lower lip.  (I’d delayed getting it looked at for several months because of covid, but I no longer could.)</p>



<p>So right now I’m laying in bed at home, with gauze on my lips, dazed, hopped up on painkillers.  I regret that things ever got to the point where this was needed.  I believe, intellectually, that the surgeon executed about as competently as anyone could ask.  But I still wish, if we’re being honest, that there hadn’t been <em>quite</em> this much pain in the surgery or in the recovery from it.</p>



<p>Again intellectually, I know that there’s still lots more pain in the days ahead.  I’m not sure that whatever it was won’t just quickly grow back.  And yet, I couldn’t be feeling more joy through my whole body with every one of these words that I write.  At last I can honestly tell myself: the growth is gone.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5071"><span class="datestr">at November 06, 2020 07:15 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/161">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/161">TR20-161 |  Seed Protecting Extractors | 

	Gil Cohen, 

	Dean Doron, 

	Shahar Samocha</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We introduce a new type of seeded extractors we dub seed protecting extractors. Informally, a seeded extractor is seed protecting against a class of functions $C$, mappings seeds to seeds, if the seed $Y$ remains close to uniform even after observing the output $\mathrm{Ext}(X,A(Y))$ for every choice of $A \in C$ (or, more generally, observing the outputs corresponding to several adversaries from $C$).

The results of this paper are structural. We establish what we believe to be surprising relations, in fact, equivalences between seed protecting extractors and each of the well-studied strengthenings of seeded extractors: strong extractors, non-malleable extractors (against permutations), and two-source extractors, where each case is classified by a suitable class $C$. Our work put forth a novel approach for constructing nonmalleable extractors against permutations. Indeed, the existing machinery developed for constructing non-malleable extractors focuses on the output and so it is aimed towards breaking correlations. Instead, our work suggests developing techniques for protecting the seed.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/161"><span class="datestr">at November 05, 2020 08:06 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2020/11/04/postdoc-in-algorithms-at-northwestern-university-and-ttic-apply-by-january-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2020/11/04/postdoc-in-algorithms-at-northwestern-university-and-ttic-apply-by-january-1-2021/">Postdoc in Algorithms at Northwestern University and TTIC (apply by January 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Northwestern University (NU) &amp; Toyota Technology Institute at Chicago (TTIC) invite applications for postdoc fellowships to conduct research in approx algorithms, beyond-worst-case analysis, high-dimensional data analysis. Postdocs will work with Profs. S. Khuller, K. Makarychev, Y. Makarychev, and A. Vijayaraghavan. Positions are based at NU but postdocs are encouraged to spend some time at TTIC.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/17450">https://academicjobsonline.org/ajo/jobs/17450</a><br />
Email: nu.cstheory@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2020/11/04/postdoc-in-algorithms-at-northwestern-university-and-ttic-apply-by-january-1-2021/"><span class="datestr">at November 04, 2020 07:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=69">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2020/11/04/monday-nov-09-tal-rabin-from-university-of-pennsylvania/">Monday, Nov 09 — Tal Rabin from University of Pennsylvania</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next Foundations of Data Science virtual talk will take place on Monday, Nov 09th at 10:00 AM Pacific Time (1:00 pm Eastern Time, 18:00 Central European Time, 17:00 UTC).  <strong>Tal Rabin </strong>from UPenn will speak about “<strong>You Only Speak Once — Secure MPC with Stateless Ephemeral Roles</strong>”.</p>



<p><strong>Abstract</strong>: The inherent difficulty of maintaining stateful environments over long periods of time gave rise to the paradigm of serverless computing, where mostly-stateless components are deployed on demand to handle computation tasks, and are teared down once their task is complete. Serverless architecture could offer the added benefit of improved resistance to targeted denial-of-service attacks. Realizing such protection,<br />requires that the protocol only uses stateless parties. Perhaps the most famous example of this style of protocols is the Nakamoto consensus protocol used in Bitcoin. We refer to this stateless property as the You-Only-Speak-Once (YOSO) property, and initiate the formal study of it within a new YOSO model. Our model is centered around the notion of roles, which are stateless parties that can only send a single message. Furthermore, we describe several techniques for achieving YOSO MPC; both computational and information theoretic.</p>



<p>The talk will be self contained.</p>



<p>Based on joint works with: Fabrice Benhamouda, Craig Gentry, Sergey Gorbunov, Shai Halevi, Hugo Krawczyk, Chengyu Lin, Bernardo Magri, Jesper Nielsen, Leo Reyzin, Sophia Yakoubov.</p>



<p><a href="https://sites.google.com/view/dstheory" target="_blank" rel="noreferrer noopener">Please register here to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2020/11/04/monday-nov-09-tal-rabin-from-university-of-pennsylvania/"><span class="datestr">at November 04, 2020 03:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
