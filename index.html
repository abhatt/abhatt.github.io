<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="https://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="http://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="http://www.minimizingregret.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="no data">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://scottaaronson.blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://scottaaronson.blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://www.let-all.com/blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.let-all.com/blog" title="The Learning Theory Alliance Blog">The Learning Theory Alliance Blog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at March 18, 2022 04:39 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://emanueleviola.wordpress.com/?p=998">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/viola.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://emanueleviola.wordpress.com/2022/03/17/focs-2022/">FOCS 2022</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>I am on the <a href="https://focs2022.eecs.berkeley.edu/cfp.html">FOCS 2022 </a>Program Committee, and I am overjoyed that the PC meeting will be virtual.  Hopefully, the days are over when the program chair can brush aside all cost-benefit considerations, impose their backyard on far-away scholars who need service items, and then splash their offspring at the welcome party.</p>



<p>I am also overjoyed that we will be implementing double-blind reviews.  This issue has been discussed and ridiculed at length.  Admittedly, it makes it harder to adhere to Leonid Levin’s 1995 influential <em><a href="https://dl.acm.org/doi/10.1145/202840.606487">STOC Criteria</a></em>.  For example, if a reviewer wanted to trash a paper based on the fact that the authors are not in the position to judge their own work, now they’ll have to check online for talks or preprints to know who the authors are.  Given the volume of reviews, it’s reasonable to expect that in some cases the reviewer won’t be able to conclusively exclude that a letter-writer is among the authors.  In such a situation they can resort to writing a very long, thorough, and competent review whose most significant digit is the STOC/FOCS death sentence: <em>weak accept</em>.</p>



<p>No, I actually do have something more constructive to say about this.  I was — as they say — privileged to serve on many NSF panels.  As an aside, it’s interesting that there the track-record of the investigators <em>is </em>a key factor in the decision; in fact, according to many including myself, it should carry even more weight, rather than forcing investigators to fill pages with made-up directions most of which won’t pan out.  But that’s another story; what is relevant for this post is that each panel begins with a quick “de-biasing” briefing, which I actually enjoy and from which I learnt something.  For example, there’s a classic experiment where the ratio of females hired as musicians increases if auditions hide the performer behind a tent and make them walk in on carpet so you can’t tell what shoes they are wearing.  Similar experiments exist that hide names from the folders of applicants, etc.  What I propose is to do a similar thing when reviewing papers.  That is, start with a de-biasing briefing: tell reviewers to ask themselves whether their attitude towards a paper would be different if:</p>



<ol><li>The authors of this paper/previous relevant work were ultra-big shots, or</li><li>The authors of this paper/previous relevant work were hapless nobodies, or</li><li>The proofs in this paper could be simplified dramatically to the point that even I understand them, or</li><li>This result came with a super-complicated proof which I can’t even begin to follow, or</li></ol>



<p>What other questions would be good to ask?</p></div>







<p class="date">
by Manu <a href="https://emanueleviola.wordpress.com/2022/03/17/focs-2022/"><span class="datestr">at March 17, 2022 02:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3213642318918472638">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/03/the-war-and-math.html">The War and Math</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>During the early parts of the cold war of the 20th century, we saw two almost independent developments of computational complexity, in the west and in the then USSR. There was little communication between the two groups, and countless theorems proven twice, most notably the seminal NP-complete papers of Cook and Levin. To understand more, I recommend the two articles about the early days of complexity by <a href="https://doi.org/10.1109/MAHC.1981.10005">Juris Hartmanis</a> and by <a href="https://doi.ieeecomputersociety.org/10.1109/MAHC.1984.10036">Boris Trakhtenbrot</a>.</p><p>Russia's invasion and relentless bombing in Ukraine have quickly separated the east and the west again. </p><p>Our first concern needs to be with Ukraine and its citizens. We hope for a quick end to this aggression and Ukraine remaining a free and democratic country. Ukrainian cities have undergone massive damage, and even in the best possible outcome it will take years if not decades to fully rebuild the country. </p><p>Terry Tao has been <a href="https://terrytao.wordpress.com/2022/03/02/resources-for-displaced-mathematicians/">collecting resources</a> for displaced mathematicians due to the crisis.</p><p>We've cut off ties with Russia institutions. In our world, major events to be held in Russia, including the <a href="https://www.mathunion.org/">International Congress of Mathematics</a> and the <a href="https://logic.pdmi.ras.ru/csr2022/">Computer Science in Russia</a> conference are being moved online. I was invited to workshops in St Petersburg in 2020 and 2021, both cancelled due to Covid, and was looking forward to one in 2022, which if it happens, will now happen without me. </p><p>The music world has has cancelled some stars, most notably <a href="https://www.nytimes.com/2022/03/02/arts/music/ukraine-putin-valery-gergiev-anna-netrebko.html">Valery Gergiev and Anna Netrebko</a>, due to their close ties to Putin. It's rare that we do the same to mathematicians for political reasons though <a href="https://blog.computationalcomplexity.org/2019/06/imus-non-controversial-changing-name-of.html">not unheard of</a>. I suspect most of our colleagues in Russia oppose the war in Ukraine, or would if they had accurate information of what was going on. I have several Russian friends and colleagues including <a href="https://blog.computationalcomplexity.org/2019/06/compressing-in-moscow.html">two I travelled to Moscow in 2019 to honor</a> and would hate to be disconnected from them.</p><p>It's way too early to know how this will all play out. Will we see a quick Russian retreat? Not likely. Will we see a situation that sees a mass migration of Ukranian and Russian mathematicians and computer scientists to Europe and North America, like in the 1990's? Possibly. We will see a repeat of the cold war, disconnected internets and science on both sides happening in isolation? I hope not but we can't rule it out.</p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/03/the-war-and-math.html"><span class="datestr">at March 17, 2022 01:28 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19755">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2022/03/17/meta-wishes/">Meta Wishes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>When faced with two choices, simply toss a coin. It works because in that brief moment when the coin is in the air, you suddenly know what you are hoping for.</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2022/03/17/meta-wishes/neill-2/" rel="attachment wp-att-19757"><img width="146" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/03/NeilL.jpg?resize=146%2C153&amp;ssl=1" class="alignright wp-image-19757" height="153" /></a></p>
<p><font color="darkgreen"></font></p><font color="darkgreen">
<p>
Neil L. is a leprechaun. He has been visiting me or Ken once every year since we started GLL. We had never seen a leprechaun before we began the blog—there must be some connection. </p>
<p>
Today we want to share the experience we each had with him this morning of St. Patrick’s Day.</p>
<p>
That’s right—Neil visited both of us separately. Four years ago, when I had major heart surgery on St. Patrick’s Eve, Neil <a href="https://rjlipton.wpcomstaging.com/2018/03/17/leprechauns-know-what-it-feels/">came</a> to my previous New York apartment, grabbed a page of my notes, and took it to Ken. The past two years, Ken was on Zoom with me when Neil appeared. This time, Neil had a bone to pick with Ken—after posing a problem to me.</p>
<p>
</p><p></p><h2> Neil’s Problem </h2><p></p>
<p></p><p>
Kathryn, my dear wife, and I are still in our new midtown Manhattan apartment after my procedure two weeks ago. She went to sleep in the bedroom, but I stayed up awaiting Neil’s arrival. I must have dozed off, but jolted awake to his laugh and the waft of his pipe’s green smoke.</p>
<p>
Neil said hi and explained right away that he had a problem. “A classic problem that leprechauns have faced forever.” I nodded to him and rubbed my eyes to be awake enough to listen. Neil continued:</p>
<blockquote><p><b> </b> <em> The other day I was minding me business and I fell into a trap. Nasty fall. The trapper was an old foe of mine. She told me she had three wishes. And I was bound to grant them as usual. Sae much, sae normal. But then she breached the rules. </em>
</p></blockquote>
<p>
</p><p></p><h2> No More Wishes </h2><p></p>
<p></p><p>
Now I knew the rules of wishes, so Neil did not have to tell me: Must be about something contingent, not love or death, no self-transmogrification, and most of all—well, let’s hear it from the genie in Disney’s <a href="https://en.wikipedia.org/wiki/Aladdin_(1992_Disney_film)">Aladdin</a>:</p>
<blockquote><p><b> </b> <em> “Three wishes, three. Uno, dos, tres, no substitutions, exchanges, or refunds, and ixnay on the wishing for more wishes.” </em>
</p></blockquote>
<p></p></font><p><font color="darkgreen">
Neil picked up his story: “Her first two wishes were:<br />
</font> </p>
<ol>
<li>
A pot of gold coins. <p></p>
</li><li>
Another pot of gold.
</li></ol>
<p><font color="darkgreen"></font></p><font color="darkgreen">
<p>
Those were fair and I showed I could handle them forthwith. But then she asked for: </p>
<blockquote><p><b> </b> <em> ‘Please ten chirag oil lamps, each with a genie who can grant three wishes.’ </em>
</p></blockquote>
<p></p><p>
This was terrible. It sent me to the lore. Ye cannot ask your servant for more wishes. Ye cannot involve another leprechaun. But lamps of themselves are just objects. That a <em>chirag</em> lamp has a genie is like an oyster has a pearl. Even if half the lamps be duds, that still  compounds the wishes—and she wished for ten that were not duds.”</p>
<p>
</p><p></p><h2> Math and Myth </h2><p></p>
<p></p><p>
Neil puffed a few more times, as if really expecting an answer from me. He even prompted, “What do ye think?” </p>
<p>
That sent me into befuddlement. Usually I try to engage Neil on a <em>math</em> problem, to trick him into telling the answer to Riemann or factoring or P=NP. But despite the “<img src="https://s0.wp.com/latex.php?latex=%7B%3E+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{&gt; 3}" class="latex" />” aspect, this wasn’t math—it was more about <em>myth</em>. Math I can take seriously, but with weighty matters personal and worldwide, I did not want to play “meta” games. </p>
<p>
Neil read my mind: “Nay, I assure ye—it be really about math. Even the Riemann—”</p>
<p>
That made me think: if Neil knew that it had real math content, he must know the answer already. No sense groping for it groggily. I just retorted: “I don’t want to guess the answer. Please just tell me.”</p>
<p>
“Ye know there be only one way I ever tell ye answers…”</p>
<p>
“Sure. OK.” I really wanted to join Kathryn for some sleep.</p>
<p>
I blinked as Neil vanished in green light. Only his pipe smoke remained, and it curled around into a shape I couldn’t place at first. It took awhile to become sharp enough that I could tell what it was:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/03/17/meta-wishes/greenzeta/" rel="attachment wp-att-19758"><img width="60" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/03/GreenZeta.png?resize=60%2C123&amp;ssl=1" class="aligncenter wp-image-19758" height="123" /></a></p>
<p></p><h2> Myth and Math </h2><p></p>
<p></p><p>
I, Ken, shall tell the rest. Ordinarily, I would have been eager to engage Neil again about the NCAA basketball tournaments. But I too am too touched by the same weighty matters, and quite forgot the day.</p>
<p>
My unawareness did not matter because Neil apparated and instantly started confronting me: “What gave ye the mickey to <a href="https://rjlipton.wpcomstaging.com/2022/03/14/are-these-the-last-digits-of-pi/">write</a> of mathematics being ’emergent’? Ye dinna even define it.”</p>
<p>
I had to concede I’d not only been vague but had analogized it too to two senses of “emergent” in philosophy. I started to explain: “It’s like Albert Einstein’s famous question, ‘Did God have a choice?’ among physical laws. Now about math…”</p>
<p>
Neil cut me off: “When Einstein said ‘God,’ he meant nature—but when <em>you</em> say it, you mean leprechauns. You are asking: ‘Do we leprechauns have the power to change mathematical truths in advance of your learning them?’ That’s our turf—!”</p>
<p>
I stammered that I had a right to pose the question and had not meant to insinuate. But Neil kept on: “If we really could change outcomes then what would <em>maths</em> rest on? On <em>myths</em> just as well. But where ye speak of <em>law</em>, we have <em>lore</em>. Established rules. And they suffice.”</p>
<p>
This was waxing cryptic and I thought bringing up Kurt Gödel would only make it more so. Neil sensed I was adrift and went on: “I shall inform ye via the same story I told Dick.”</p>
<p>
</p><p></p><h2> Neil’s Solution </h2><p></p>
<p></p><p>
Neil unfurled his story. At his same pause, one perception dawned on me: “Neil, did the lady’s wish imply a recursion—meaning the genies would be asked for a wish generator in the same proportion?”</p>
<p>
Neil nodded: “The lore says yes—the lore on hearing what is said. That is what I consulted it for. The rest I could work out on paper.” </p>
<p>
The paper part was easy up to a point. The wishes implied an infinite summation: the original <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3}" class="latex" />, then <img src="https://s0.wp.com/latex.php?latex=%7B30%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{30}" class="latex" /> more of the ten genies, with the third wishes to each bringing <img src="https://s0.wp.com/latex.php?latex=%7B30%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{30}" class="latex" /> at the next level, for <img src="https://s0.wp.com/latex.php?latex=%7B30+%5Ccdot+10+%3D+300%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{30 \cdot 10 = 300}" class="latex" /> more wishes there, and so on. In sum, </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++3+%2B+30+%2B+300+%2B+3000+%2B+%5Ccdots+%3D+3%5Ccdot+%281+%2B+10+%2B+100+%2B+1000+%2B+%5Ccdots%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  3 + 30 + 300 + 3000 + \cdots = 3\cdot (1 + 10 + 100 + 1000 + \cdots) " class="latex" /></p>
<p>wishes. Clearly the number of wishes is bounded by the series </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++3%5Ccdot%281+%2B+10+%2B+3%5E%7B%5Clog_2+10%7D+%2B+100+%2B+5%5E%7B%5Clog_2+10%7D+%2B+6%5E%7B%5Clog_2+10%7D+%2B+7%5E%7B%5Clog_2+10%7D+%2B+1000+%2B+%5Ccdots%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  3\cdot(1 + 10 + 3^{\log_2 10} + 100 + 5^{\log_2 10} + 6^{\log_2 10} + 7^{\log_2 10} + 1000 + \cdots) " class="latex" /></p>
<p>Which equals <img src="https://s0.wp.com/latex.php?latex=%7B3%5Ccdot+%5Czeta%28-%5Clog_2+10%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3\cdot \zeta(-\log_2 10)}" class="latex" />. Without asking Neil how he’d computed that, I went to an <a href="https://solvemymath.com/online_math_calculator/number_theory/riemann_function/index.php">online</a> zeta <a href="https://keisan.casio.com/exec/system/1180573439">calculator</a> and obtained a bounding total of </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++3%5Ccdot+0.006023525392866159581193...+%5C%3B%5C%3B%3D+%5C%3B%5C%3B%2B0.018070576178598478743579...+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  3\cdot 0.006023525392866159581193... \;\;= \;\;+0.018070576178598478743579... " class="latex" /></p>
<p>wishes. Neil puffed and chortled and finished his story:</p>
<blockquote><p><b> </b> <em> “So the unlucky lass had wished herself into wishing a grand total of less than one-fiftieth of a wish. Since only whole wishes can be honoured, this sprang me from the trap and kept me both me pots o’ gold to boot. I gave her one coin as a peace offering.” </em>
</p></blockquote>
<p></p><p>
I joined the laughter for just a moment. This followed mathematical rules but outlandishly, and I groped for the point. But Neil supplied it directly:</p>
<blockquote><p><b> </b> <em> “Some of your fellow travelers have felt reaching answers by non-constructive methods to be less outlandish only in degree not kind. If ye later apprehend the answer by calculation—which is what your post styled as “emergent” knowledge—then it must be exactly the same object previously described non-constructively. Thus in the realms ye ascribed to us wee folk, we are the guardians and gatekeepers of truth, not the forgers of it.” </em>
</p></blockquote>
<p></p><p>
Neil tipped his hat and simply faded out—no green smoke for me.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Can Neil’s zeta-function calculations insulate against any attempt at recursively gaining infinite wishes? The zeta function whips up and down in tune with the Bernoulli numbers, but the leprechauns do have freedom to choose a bounding series. Or is there an infinite series scheme that rises above 3 wishes even so? We hope this has given some St. Patrick’s Day diversion.</p>
<p></p></font></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wpcomstaging.com/2022/03/17/meta-wishes/"><span class="datestr">at March 17, 2022 05:58 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2203.08767">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2203.08767">The Degree-Rips Complexes of an Annulus with Outliers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rolle:Alexander.html">Alexander Rolle</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2203.08767">PDF</a><br /><b>Abstract: </b>The degree-Rips bifiltration is the most computable of the parameter-free,
density-sensitive bifiltrations in topological data analysis. It is known that
this construction is stable to small perturbations of the input data, but its
robustness to outliers is not well understood. In recent work, Blumberg-Lesnick
prove a result in this direction using the Prokhorov distance and homotopy
interleavings. Based on experimental evaluation, they argue that a more refined
approach is desirable, and suggest the framework of homology inference.
Motivated by these experiments, we consider a probability measure that is
uniform with high density on an annulus, and uniform with low density on the
disc inside the annulus. We compute the degree-Rips complexes of this
probability space up to homotopy type, using the Adamaszek-Adams computation of
the Vietoris-Rips complexes of the circle. These degree-Rips complexes are the
limit objects for the Blumberg-Lesnick experiments. We argue that the homology
inference approach has strong explanatory power in this case, and suggest
studying the limit objects directly as a strategy for further work.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2203.08767"><span class="datestr">at March 17, 2022 10:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2203.08733">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2203.08733">A Note on the Outliers Theorem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Epstein:Samuel.html">Samuel Epstein</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2203.08733">PDF</a><br /><b>Abstract: </b>An outlier is a datapoint set apart from a sample population. This outlier
could be due to sampling errors or contamination from other populations. The
outliers theorem in algorithmic information theory states given a computable
sampling method, outliers have to appear in samples. In this paper we present a
plain complexity variant to the outliers theorem. This variant achieves better
error bounds than the original prefix-free Kolmogorov complexity version.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2203.08733"><span class="datestr">at March 17, 2022 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2203.08592">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2203.08592">On the complexity of the word problem of the R. Thompson group V</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>J. C. Birget <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2203.08592">PDF</a><br /><b>Abstract: </b>We analyze the proof by Lehnert and Schweitzer that the word problem of the
Thompson group V is co-context-free, and we show that this word problem is the
complement of the cyclic closure of a union of reverse deterministic
context-free languages. For certain finite generating sets, this word problem
is the complement of the cyclic closure of the union of four deterministic
context-free languages. It follows that the deterministic time-complexity of
the word problem of $V$ is at most quadratic.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2203.08592"><span class="datestr">at March 17, 2022 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2203.08571">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2203.08571">Morse Theoretic Signal Compression and Reconstruction on Chain Complexes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Ebli:Stefania.html">Stefania Ebli</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hacker:Celia.html">Celia Hacker</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maggs:Kelly.html">Kelly Maggs</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2203.08571">PDF</a><br /><b>Abstract: </b>At the intersection of Topological Data Analysis (TDA) and machine learning,
the field of cellular signal processing has advanced rapidly in recent years.
In this context, each signal on the cells of a complex is processed using the
combinatorial Laplacian, and the resultant Hodge decomposition. Meanwhile,
discrete Morse theory has been widely used to speed up computations by reducing
the size of complexes while preserving their global topological properties. In
this paper, we provide an approach to signal compression and reconstruction on
chain complexes that leverages the tools of algebraic discrete Morse theory.
The main goal is to reduce and reconstruct a based chain complex together with
a set of signals on its cells via deformation retracts, preserving as much as
possible the global topological structure of both the complex and the signals.
We first prove that any deformation retract of real degree-wise
finite-dimensional based chain complexes is equivalent to a Morse matching. We
will then study how the signal changes under particular types of Morse
matching, showing its reconstruction error is trivial on specific components of
the Hodge decomposition. Furthermore, we provide an algorithm to compute Morse
matchings with minimal reconstruction error.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2203.08571"><span class="datestr">at March 17, 2022 10:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2203.08364">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2203.08364">Minimum Height Drawings of Ordered Trees in Polynomial Time: Homotopy Height of Tree Duals</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Parsa:Salman.html">Salman Parsa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Ophelders:Tim.html">Tim Ophelders</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2203.08364">PDF</a><br /><b>Abstract: </b>We consider drawings of graphs in the plane in which vertices are assigned
distinct points in the plane and edges are drawn as simple curves connecting
the vertices and such that the edges intersect only at their common endpoints.
There is an intuitive quality measure for drawings of a graph that measures the
height of a drawing $\phi : G \rightarrow \mathbb{R}^2$ as follows. For a
vertical line $\ell$ in $\mathbb{R}^2$, let the height of $\ell$ be the
cardinality of the set $\ell \cap \phi(G)$. The height of a drawing of $G$ is
the maximum height over all vertical lines. In this paper, instead of abstract
graphs, we fix a drawing and consider plane graphs. In other words, we are
looking for a homeomorphism of the plane that minimizes the height of the
resulting drawing. This problem is equivalent to the homotopy height problem in
the plane, and the homotopic Fr\'echet distance problem. These problems were
recently shown to lie in NP, but no polynomial-time algorithm or NP-hardness
proof has been found since their formulation in 2009. We present the first
polynomial-time algorithm for drawing trees with optimal height. This
corresponds to a polynomial-time algorithm for the homotopy height where the
triangulation has only one vertex (that is, a set of loops incident to a single
vertex), so that its dual is a tree.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2203.08364"><span class="datestr">at March 17, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2203.08356">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2203.08356">Hardness for Triangle Problems under Even More Believable Hypotheses: Reductions from Real APSP, Real 3SUM, and OV</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chan:Timothy_M=.html">Timothy M. Chan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Williams:Virginia_Vassilevska.html">Virginia Vassilevska Williams</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Yinzhan.html">Yinzhan Xu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2203.08356">PDF</a><br /><b>Abstract: </b>The $3$SUM hypothesis, the APSP hypothesis and SETH are the three main
hypotheses in fine-grained complexity. So far, within the area, the first two
hypotheses have mainly been about integer inputs in the Word RAM model of
computation. The "Real APSP" and "Real $3$SUM" hypotheses, which assert that
the APSP and $3$SUM hypotheses hold for real-valued inputs in a reasonable
version of the Real RAM model, are even more believable than their integer
counterparts.
</p>
<p>Under the very believable hypothesis that at least one of the Integer $3$SUM
hypothesis, Integer APSP hypothesis or SETH is true, Abboud, Vassilevska W. and
Yu [STOC 2015] showed that a problem called Triangle Collection requires
$n^{3-o(1)}$ time on an $n$-node graph.
</p>
<p>Our main result is a nontrivial lower bound for a slight generalization of
Triangle Collection, called All-Color-Pairs Triangle Collection, under the even
more believable hypothesis that at least one of the Real $3$SUM, the Real APSP,
and the OV hypotheses is true. Combined with slight modifications of prior
reductions, we obtain polynomial conditional lower bounds for problems such as
the (static) ST-Max Flow problem and dynamic Max Flow, now under the new weaker
hypothesis.
</p>
<p>Our main result is built on the following two lines of reductions.
</p>
<p>* Real APSP and Real $3$SUM hardness for the All-Edges Sparse Triangle
problem. Prior reductions only worked from the integer variants of these
problems.
</p>
<p>* Real APSP and OV hardness for a variant of the Boolean Matrix
Multiplication problem.
</p>
<p>Along the way we show that Triangle Collection is equivalent to a simpler
restricted version of the problem, simplifying prior work. Our techniques also
have other interesting implications, such as a super-linear lower bound of
Integer All-Numbers $3$SUM based on the Real $3$SUM hypothesis, and a tight
lower bound for a string matching problem based on the OV hypothesis.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2203.08356"><span class="datestr">at March 17, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2203.08328">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2203.08328">Tight Lower Bounds for Approximate &amp; Exact $k$-Center in $\mathbb{R}^d$</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chitnis:Rajesh.html">Rajesh Chitnis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saurabh:Nitin.html">Nitin Saurabh</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2203.08328">PDF</a><br /><b>Abstract: </b>In the discrete $k$-center problem, we are given a metric space
$(P,\texttt{dist})$ where $|P|=n$ and the goal is to select a set $C\subseteq
P$ of $k$ centers which minimizes the maximum distance of a point in $P$ from
its nearest center. For any $\epsilon&gt;0$, Agarwal and Procopiuc [SODA '98,
Algorithmica '02] designed an $(1+\epsilon)$-approximation algorithm for this
problem in $d$-dimensional Euclidean space which runs in $O(dn\log k) +
\left(\dfrac{k}{\epsilon}\right)^{O\left(k^{1-1/d}\right)}\cdot n^{O(1)}$ time.
In this paper we show that their algorithm is essentially optimal: if for some
$d\geq 2$ and some computable function $f$, there is an $f(k)\cdot
\left(\dfrac{1}{\epsilon}\right)^{o\left(k^{1-1/d}\right)} \cdot
n^{o\left(k^{1-1/d}\right)}$ time algorithm for $(1+\epsilon)$-approximating
the discrete $k$-center on $n$ points in $d$-dimensional Euclidean space then
the Exponential Time Hypothesis (ETH) fails.
</p>
<p>We obtain our lower bound by designing a gap reduction from a $d$-dimensional
constraint satisfaction problem (CSP) defined by Marx and Sidiropoulos [SoCG
'14] to discrete $d$-dimensional $k$-center. As a byproduct of our reduction,
we also obtain that the exact algorithm of Agarwal and Procopiuc [SODA '98,
Algorithmica '02] which runs in $n^{O\left(d\cdot k^{1-1/d}\right)}$ time for
discrete $k$-center on $n$ points in $d$-dimensional Euclidean space is
asymptotically optimal. Formally, we show that if for some $d\geq 2$ and some
computable function $f$, there is an $f(k)\cdot n^{o\left(k^{1-1/d}\right)}$
time exact algorithm for the discrete $k$-center problem on $n$ points in
$d$-dimensional Euclidean space then the Exponential Time Hypothesis (ETH)
fails. Previously, such a lower bound was only known for $d=2$ and was implicit
in the work of Marx [IWPEC '06].
</p>
<p>[see paper for full abstract]
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2203.08328"><span class="datestr">at March 17, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2203.08193">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2203.08193">Point Separation and Obstacle Removal by Finding and Hitting Odd Cycles</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Neeraj.html">Neeraj Kumar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lokshtanov:Daniel.html">Daniel Lokshtanov</a>, Saket Saurabh, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Suri:Subhash.html">Subhash Suri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xue:Jie.html">Jie Xue</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2203.08193">PDF</a><br /><b>Abstract: </b>Suppose we are given a pair of points $s, t$ and a set $S$ of $n$ geometric
objects in the plane, called obstacles. We show that in polynomial time one can
construct an auxiliary (multi-)graph $G$ with vertex set $S$ and every edge
labeled from $\{0, 1\}$, such that a set $S_d \subseteq S$ of obstacles
separates $s$ from $t$ if and only if $G[S_d]$ contains a cycle whose sum of
labels is odd. Using this structural characterization of separating sets of
obstacles we obtain the following algorithmic results.
</p>
<p>In the Obstacle-Removal problem the task is to find a curve in the plane
connecting s to t intersecting at most q obstacles. We give a
$2.3146^qn^{O(1)}$ algorithm for Obstacle-Removal, significantly improving upon
the previously best known $q^{O(q^3)} n^{O(1)}$ algorithm of Eiben and
Lokshtanov (SoCG'20). We also obtain an alternative proof of a constant factor
approximation algorithm for Obstacle-Removal, substantially simplifying the
arguments of Kumar et al. (SODA'21).
</p>
<p>In the Generalized Points-Separation problem, the input consists of the set S
of obstacles, a point set A of k points and p pairs $(s_1, t_1),... (s_p, t_p)$
of points from A. The task is to find a minimum subset $S_r \subseteq S$ such
that for every $i$, every curve from $s_i$ to $t_i$ intersects at least one
obstacle in $S_r$. We obtain $2^{O(p)} n^{O(k)}$-time algorithm for Generalized
Points-Separation problem. This resolves an open problem of Cabello and
Giannopoulos (SoCG'13), who asked about the existence of such an algorithm for
the special case where $(s_1, t_1), ... (s_p, t_p)$ contains all the pairs of
points in A. Finally, we improve the running time of our algorithm to $f(p,k)
n^{O(\sqrt{k})}$ when the obstacles are unit disks, where $f(p,k) = 2^O(p)
k^{O(k)}$, and show that, assuming the Exponential Time Hypothesis (ETH), the
running time dependence on $k$ of our algorithms is essentially optimal.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2203.08193"><span class="datestr">at March 17, 2022 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/03/16/postdoc-at-sandia-national-labs-apply-by-march-31-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/03/16/postdoc-at-sandia-national-labs-apply-by-march-31-2022/">Postdoc at Sandia National Labs (apply by March 31, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Sandia Labs is seeking a postdoc to work on quantum or quantum-inspired classical approximation, sublinear, or streaming algorithms, as part of a DOE-funded collaboration among several national labs and universities. We encourage theoretical computer scientists interested in quantum information but without prior expertise to apply.</p>
<p>Website: <a href="https://far-qc.sandia.gov/job-opportunities/">https://far-qc.sandia.gov/job-opportunities/</a><br />
Email: odparek@sandia.gov</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/03/16/postdoc-at-sandia-national-labs-apply-by-march-31-2022/"><span class="datestr">at March 16, 2022 11:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2022/03/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2022/03/15/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p>Here’s a silly but probably new proof that the harmonic series diverges <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107883892455172986">\(\mathbb{M}\)</a>).</span> The expected number of comparisons used by randomized quicksort on an input of size \(n\) is at most <span style="white-space: nowrap;">\(2nH_n\),</span> where \(H_n\) is the <span style="white-space: nowrap;">\(n\)th</span> partial sum of the harmonic series (see Cormen et al, <em>Introduction to Algorithms</em>, Chapter 7). However, every comparison sorting algorithm requires at least \(\log_2n!=n\log_2n-O(n)\) comparisons, by the standard decision tree argument (Cormen et al, Section 8.1). Therefore, <span style="white-space: nowrap;">\(H_n=\Omega(\log n)\).</span></p>
  </li>
  <li>
    <p>To be fair, the lecture hall I teach in this term doesn’t look quite so much like a prison if you enter by the main door at the top of the hall instead of the back door by the stage <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107890151210168412">\(\mathbb{M}\)</a>).</span></p>

    <p style="text-align: center;"><img src="https://www.ics.uci.edu/~eppstein/pix/hlh/hlh-m.jpg" style="border-style: solid; border-color: black;" alt="Humanities Lecture Hall, UC Irvine" /></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2203.00671">Maximum flow and minimum-cost flow in almost-linear time</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@FreddyR/107890263250219998">\(\mathbb{M}\)</a>).</span> New arXiv preprint by Li Chen, Rasmus Kyng, Yang P. Liu, Richard Peng, Maximilian Probst Gutenberg, Sushant Sachdeva. It assumes integer capacities but that’s enough to get near-linear bipartite maximum matching, itself a breakthrough.</p>
  </li>
  <li>
    <p>In early March, UC Berkeley was <a href="https://www.latimes.com/california/story/2022-03-04/how-much-will-uc-berkeley-have-to-cut-admissions-after-supreme-court-loss-what-we-know">ordered to drastically cut enrollment under California’s strict environmental impact review laws</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107899108816738487">\(\mathbb{M}\)</a>).</span> In practice these laws  are often used as a pretext for lawsuits to shake down or stop public developments for reasons unrelated to environmental impact (this is a heavily built-up area already; the impact is that it would have more students living in it and the people suing wanted to shake down UC for non-university-related low-income housing expansion). By mid-March, the state legislature had passed <a href="https://www.latimes.com/california/story/2022-03-14/california-legislature-passes-bill-berkeley-enrollment">emergency legislation to temporarily sidestep the issue</a>.</p>
  </li>
  <li>
    <p>Another batch of Wikipedia Good Articles <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107905159574583760">\(\mathbb{M}\)</a>):</span></p>

    <ul>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Fibonacci_nim">Fibonacci nim</a>: subtraction game with a Fibonacci number based strategy.</p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Kepler_triangle">Kepler triangle</a>: not the shape of the great pyramid of Giza, but one of its other properties inspired me to make the illustration below.</p>

        <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2022/kepler.svg" alt="Isosceles triangle is formed from two Kepler triangles, reflected across their short sides, and its inscribed circle, having the maximum radius possible among all isosceles triangles with the same side length" /></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Component_(graph_theory)">Connected components of undirected graphs</a>: saving this batch from complete frivolity.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>New book in discrete geometry <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107914084986329325">\(\mathbb{M}\)</a>):</span> <em>Polynomial Methods and Incidence Geometry</em>, Adam Sheffer, Cambridge University Press. See <a href="https://adamsheffer.wordpress.com/2022/03/03/new-book-polynomial-methods-and-incidence-theory/">Adam’s announcement</a> and <a href="http://faculty.baruch.cuny.edu/ASheffer/000book.pdf">an early draft with missing chapters</a>.</p>
  </li>
  <li>
    <p>When I’ve been thinking recently about who I might know who is Ukrainian or Ukrainian-American, the first to mind was Andrea Danyluk, with whom I went to grad school. We lost touch later, but she had a long distinguished career at Williams College. <a href="https://president.williams.edu/in-memoriam/the-passing-of-andrea-danyluk/">Sadly, she died a few days ago</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107916240235667375">\(\mathbb{M}\)</a>).</span> The Computing Research Association chose her as the <a href="https://cra.org/about/awards/a-nico-habermann-award/#2022">2022 winner of their A. Nico Habermann Award for increasing diversity in computing research</a>, shortly before her death.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@jsiehler/107927992555262577">The SET card game is not accessible to the color-impaired</a>, its manufacturer shows no interest in fixing it or providing accessible alternatives, and is actively blocking any attempts by others to do the same. Sadly, this makes it unusable as a classroom activity.</p>
  </li>
  <li>
    <p><a href="https://www.asbmb.org/asbmb-today/careers/030822/what-s-with-wikipedia-and-women">What’s with Wikipedia and women?
Things are changing, little by little, at the open-source encyclopedia</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107933443665051198">\(\mathbb{M}\)</a>).</span> New article from the American Society for Biochemistry and Molecular Biology mentions in passing my efforts creating articles on women in STEM and patrolling deletion discussions on them.</p>
  </li>
  <li>
    <p><a href="https://sinews.siam.org/Details-Page/in-pursuit-of-perfect-pinnacles">In pursuit of perfect pinnacles</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107945184119710916">\(\mathbb{M}\)</a>).</span> Why do spiky shapes form in nature, for instance in limestone and ice? Leif Ristroph, Jinzi Mac Huang, and Michael Shelley survey recent research in this <em>SIAM News</em> column.</p>
  </li>
  <li>
    <p>Another Wikipedia Good Article, on an important rather than recreational topic: <a href="https://en.wikipedia.org/wiki/Harmonic_series_(mathematics)">harmonic series</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107950806929679318">\(\mathbb{M}\)</a>)</span>  on the divergent series</p>

\[\sum_{n=1}^\infty\frac{1}{n} = 1 + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \frac{1}{5} + \cdots.\]

    <p>While cleaning it up I learned that the term “harmonic number” and notation for its partial sums comes from Knuth, and also that the “crossing the desert” puzzle, one of the standard examples for harmonic series, dates to long before the harmonic series itself.</p>
  </li>
  <li>
    <p><a href="https://blog.plover.com/math/se/notation.html">Bad but interesting mathematical notation</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@mjd/107927925348652783">\(\mathbb{M}\)</a>).</span> Minimal subsystems of arithmetic aside, Mark-Jason Dominus wrestles with the problem of finding an intuitive visual representation for expressions that combine a single associative operation with two mutually inverse unary operations.</p>
  </li>
  <li>
    <p>Wikimedia foundation VP Maggie Dennis warns Wikipedia editors writing about the Russian invasion of Ukraine that <a href="https://lists.wikimedia.org/hyperkitty/list/wikimedia-l@lists.wikimedia.org/message/KIMZHJMWMKXFRCMWIE5WL3YIJNFMSNVH/">they are likely to be doxxed</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107956409088287040">\(\mathbb{M}\)</a>,</span> <a href="https://en.wikipedia.org/wiki/Wikipedia_talk:Did_you_know">via</a>), especially when their “activities are seen as opposing the Russian narrative of the war”. One assumes by the Russians, although she does not say that explicitly.</p>
  </li>
  <li>
    <p><a href="http://reconf.wikidot.com/">A wiki on combinatorial reconfiguration problems</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107963773811594015">\(\mathbb{M}\)</a>).</span> The main content at this point appears to be <a href="http://reconf.wikidot.com/papers/">their extensive bibliography of papers on the topic</a>, available both on-wiki and at a linked overleaf site. I can’t tell whether the wiki or overleaf version of the .bib file is supposed to be primary, though.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2022/03/15/linkage.html"><span class="datestr">at March 15, 2022 10:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://differentialprivacy.org/dp-fine-tuning/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/dp.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://differentialprivacy.org/dp-fine-tuning/">Differentially private deep learning can be effective with self-supervised models</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Differential Privacy (DP) is a formal definition of privacy which guarantees that the outcome of a statistical procedure does not vary much regardless of whether an individual input is included or removed from the training dataset. 
This guarantee is desirable when we are tasked to train machine learning models on private datasets that should not memorize individual inputs. 
Past works have shown that differentially private models can be resilient to strong membership inference [<a href="https://proceedings.mlr.press/v37/kairouz15.html">1</a>, <a href="https://ieeexplore.ieee.org/abstract/document/9519424">34</a>, <a href="https://proceedings.neurips.cc/paper/2020/hash/fc4ddc15f9f4b4b06ef7844d6bb53abf-Abstract.html">35</a>] and data reconstruction attacks [<a href="https://www.usenix.org/conference/usenixsecurity19/presentation/carlini">2</a>, <a href="https://arxiv.org/abs/2201.12383">3</a>] when the privacy parameter is set to be sufficiently small. 
See a <a href="https://differentialprivacy.org/how-to-deploy-ml-with-dp/">prior post</a> for more background on differentially private machine learning.</p>

<p>Yet, in practice, most attempts at training differentially private deep learning models on moderately-sized datasets have resulted in large performance drops compared to when training without privacy-protection baked in. 
These performance drops are oftentimes large enough to discourage the adoption of differential privacy protection into machine learning pipelines altogether.</p>

<p>To provide a reference of the potential performance hit, the authors of [<a href="https://arxiv.org/abs/2102.12677">5</a>] trained a ResNet-20 from scratch on CIFAR-10 with a privacy budget of \(\epsilon=8\) that has test accuracy barely over 62% (see their Table 1). 
Contrast this with the 8.75% error rate (91.25% accuracy) reported for training the same architecture without enforcing differential privacy [<a href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">6</a>]. 
While some works report private learning results better than the above, absent additional data, pre-training, or external knowledge, most improvements have been incremental, and the test accuracy for CIFAR-10 models trained under modest privacy leakage (\(\epsilon=3\)) has roughly settled to ~70% in the literature [<a href="https://arxiv.org/abs/2011.11660">4</a>].</p>

<p>One reason behind the performance drop lies in sample efficiency — differentially private learning generally requires much more data than non-private learning to reach an acceptable level of performance. 
This also means that learning the high-level features (e.g., syntactic structure in text, edge detectors for images) necessary to perform specific tasks with private data can be much more sample-costly.</p>

<p>This blog post surveys results that leverage public self-supervised pre-training to obtain high-performing models through differentially private fine-tuning.
The pre-train-fine-tune paradigm is straightforward to execute and results in high-performing models under modest privacy budgets for many standard computer vision and natural language processing tasks. 
Moreover, existing results have shown that private fine-tuning consistently benefits from improvements in public pre-training.</p>

<h2 id="self-supervised-pre-training">Self-Supervised Pre-Training</h2>

<p>Self-supervised learning is a paradigm which leverages unlabeled data to learn representations that can be useful for a range of downstream tasks.
Since self-supervised learning doesn’t target specific tasks itself, 
the (pre-)training procedure doesn’t require labeled data — in many cases, mildly curated unlabeled data is sufficient for self-supervised pre-training to produce models for subsequent fine-tuning. 
So far, there have been two broadly successful instantiations of this learning paradigm in computer vision [<a href="http://proceedings.mlr.press/v119/chen20j.html">9</a>] and natural language processing [<a href="https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf">7</a>, <a href="https://arxiv.org/abs/1810.04805">8</a>]. 
We recap the two approaches below.<sup id="fnref:1"><a href="https://differentialprivacy.org/feed.xml#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<p><strong>Contrastive pre-training for vision:</strong> 
One class of self-supervised methods in computer vision (SimCLR, [<a href="http://proceedings.mlr.press/v119/chen20j.html">9</a>]) performs pre-training through contrastive learning. 
Algorithms of this type produce embeddings for images with the goal of creating different embeddings for semantically different images and similar embeddings for similar ones. 
Concretely, the algorithm used in SimCLR forces models to produce similar embeddings for an image and its augmented siblings (e.g., image rotated by some degrees), 
and different embeddings for separate images (and their augmentations). 
The SimCLR framework with large scale models and compute led to state-of-the-art (non-private) ImageNet fine-tuning results at the time of its writing.</p>

<p><strong>Masked language modeling and autoregressive language modeling for text:</strong> 
Masked Language Modeling (MLM) and Auto-regressive Language Modeling (ALM) are two self-supervised pre-training approaches. 
While the former asks models to predict deliberately masked out tokens from a piece of text, the latter asks models to simply predict the next token in a sequence. 
With large amounts of unlabeled text data, large and expressive Transformer models [<a href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">24</a>], and lots of compute, both approaches produce powerful models that are good starting points for downstream fine-tuning. 
For instance, Bidirectional Encoder Representations from Transformers (BERT, [<a href="https://arxiv.org/abs/1810.04805">8</a>]), produced state-of-the-art (non-private) results (at the time) for a large collection of language understanding tasks when fine-tuned on each.</p>

<h2 id="fine-tuning-self-supervised-models-with-dp-optimization">Fine-Tuning Self-Supervised Models With DP-Optimization</h2>
<p>Self-supervised pre-training is appealing in the context of differentially private machine learning. 
This is because (i) the mildly curated data needed for pre-training can usually be obtained cheaply from the public domain, and (ii) pre-trained models may contain useful domain knowledge that can reduce the sample complexity of subsequent private learning. 
A paradigm for private learning that leverages self-supervised pre-training could follow two steps:</p>

<ul>
  <li>collect cheap and public (unlabeled) data from the task domain (e.g., vision, language, etc.) to pre-train a model with self-supervised learning, and</li>
  <li>collect moderate amounts of task-specific private (labeled) data and fine-tune the pre-trained model under differential privacy to perform the task.<sup id="fnref:2"><a href="https://differentialprivacy.org/feed.xml#fn:2" class="footnote" rel="footnote">2</a></sup></li>
</ul>

<p>To date, some of the best differentially private deep learning results in the literature have resulted from instantiating this paradigm [<a href="https://arxiv.org/abs/2011.11660">4</a>, <a href="https://arxiv.org/abs/2110.05679">11</a>, <a href="https://arxiv.org/abs/2110.06500">12</a>].
Below, we review works which capitalize on self-supervised pre-training by differentially privately fine-tuning pre-trained models with an iterative gradient method like DP-SGD [<a href="https://dl.acm.org/doi/abs/10.1145/2976749.2978318">19</a>, <a href="https://ieeexplore.ieee.org/abstract/document/6736861">20</a>].<sup id="fnref:3"><a href="https://differentialprivacy.org/feed.xml#fn:3" class="footnote" rel="footnote">3</a></sup>
<img src="https://differentialprivacy.org/images/fine-tuning-paradigm.png" alt="" /></p>

<p><strong>Private fine-tuning with SimCLR features:</strong> 
The authors of [<a href="https://arxiv.org/abs/2011.11660">4</a>] fine-tuned a linear model on top of the embedding vectors produced by SimCLRv2 from the CIFAR-10 dataset. Under a privacy budget of \(\epsilon=2\), 
these models reached an average test accuracy of 92.7%. This number can be further improved to ~94% with the use of larger and wider pre-trained models in the SimCLRv2 family.<sup id="fnref:4"><a href="https://differentialprivacy.org/feed.xml#fn:4" class="footnote" rel="footnote">4</a></sup> 
These test accuracies are very close to some standard non-private results attained by an off-the-shelf ResNet architecture [<a href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">6</a>].</p>

<p><strong>Privately fine-tuning BERT variants and GPT-2:</strong> 
The authors of [<a href="https://arxiv.org/abs/2110.05679">11</a>, <a href="https://arxiv.org/abs/2110.06500">12</a>, <a href="http://proceedings.mlr.press/v139/yu21f.html">16</a>] showed that with appropriate hyper-parameters, fine-tuning BERT variants and GPT-2 with DP-optimization results in high-performing private models for text classification and language generation — even on datasets of modest sizes and under modest privacy budgets. 
Notably, some of these models attain a task performance close to non-private models from previous years in the literature. 
These results also exceed many non-private learning results from the pre-BERT and pre-GPT years.<sup id="fnref:5"><a href="https://differentialprivacy.org/feed.xml#fn:5" class="footnote" rel="footnote">5</a></sup></p>

<p>More interestingly, the authors showed that the larger (and thus better) the pre-trained model, the better the private fine-tuning performance gets. 
This empirical observation in private fine-tuning of large Transformers is qualitatively different from what’s implied by the usual minimax optimal rates derived for vanilla private learning with convex loss functions under approximate differential privacy [<a href="https://ieeexplore.ieee.org/abstract/document/6979031">14</a>, <a href="https://proceedings.neurips.cc/paper/2019/hash/3bd8fdb090f1f5eb66a00c84dbc5ad51-Abstract.html">15</a>]. 
This discrepancy between experimental results for training large models and the theory for learning with convex losses suggests there is more to be understood.<sup id="fnref:6"><a href="https://differentialprivacy.org/feed.xml#fn:6" class="footnote" rel="footnote">6</a></sup></p>

<p>Overall, for both vision and language tasks, private learning performance has consistently improved with the improvement in the quality of pre-training, 
where the latter is measured by the non-private fine-tuning performance.<sup id="fnref:7"><a href="https://differentialprivacy.org/feed.xml#fn:7" class="footnote" rel="footnote">7</a></sup></p>

<p>
  <img width="48%" src="https://differentialprivacy.org/../images/figure1_classification.png" />
  <img width="48%" src="https://differentialprivacy.org/../images/figure1_generation.png" /> 
  Figure 1: Privately fine-tuning better (and larger) pre-trained models lead to consistently improving performance for text classification and language generation. 
Left: text classification on MNLI [<a href="https://arxiv.org/abs/1704.05426">25</a>]. Right: language generation on E2E [<a href="https://arxiv.org/abs/1706.09254">26</a>].
</p>

<h2 id="conclusion-and-outlook">Conclusion and Outlook</h2>

<p>We surveyed recent works in the literature that obtained highly performant private machine learning models leveraging self-supervised pre-training. 
Common to these results is the trend that the performance of private learning consistently improved with the quality of public pre-training. 
We therefore anticipate that the general paradigm may be useful in additional settings (e.g., federated learning) and tasks (e.g., private synthetic image generation), and lead to better private learning results.</p>

<p>We have thus far assumed that the data for public pre-training can be cheaply obtained.
This, however, does not imply that determining whether a particular source of data is appropriate for public pre-training is an easy problem.
Using publicly available data is not necessarily risk-free in terms of privacy.
For instance, the authors of [<a href="https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting">33</a>] were able to extract personally identifiable information from a GPT-2 model pre-trained on data scraped from the public internet.</p>

<p>Self-supervised pre-training has led to progress in private deep learning, but leveraging pre-trained models alone will not address several fundamental challenges to differentially private learning.
First and foremost, the datasets of machine learning tasks may be sampled from long-tailed distributions [<a href="https://proceedings.neurips.cc/paper/2020/hash/1e14bfe2714193e7af5abc64ecbd6b46-Abstract.html">21</a>]. 
When privately trained on such datasets, a machine learning model may fail to acquire the learning signal necessary to perform accurate predictions for examples on the tail [<a href="https://dl.acm.org/doi/abs/10.1145/3442188.3445934">28</a>] or from underrepresented (sub)populations [<a href="https://proceedings.neurips.cc/paper/2019/hash/fc0de4e0396fff257ea362983c2dda5a-Abstract.html">29</a>]. 
Second, many machine learning problems are in a domain where public data (even unlabeled data) may be sparse, e.g., medical imaging. 
Developing refined versions of the pre-train-fine-tune approach for problems from these domains is an interesting avenue for future work.</p>

<p>Lastly, differential privacy as one specific definition of privacy may not capture all that’s desired for privacy in reality. 
For instance, while differentially private algorithms naturally give machine unlearning guarantees [<a href="https://ieeexplore.ieee.org/abstract/document/9519428">30</a>, <a href="https://ieeexplore.ieee.org/abstract/document/7163042">32</a>], tailored unlearning algorithms tend to have higher capacities of unlearning [<a href="https://proceedings.neurips.cc/paper/2021/hash/9627c45df543c816a3ddf2d8ea686a99-Abstract.html">31</a>].
In addition, what constitutes a record in the differential privacy framework can oftentimes be unclear. 
Inappropriately defined example boundaries can create correlated records which cause differential privacy guarantees to degrade [<a href="https://arxiv.org/abs/1603.01508">22</a>].
Moreover, differential privacy guarantees won’t directly prevent the inference of private data outside the original context [<a href="https://heinonline.org/hol-cgi-bin/get_pdf.cgi?handle=hein.journals/washlr79&amp;section=16">23</a>]. 
These are fundamental limitations of differential privacy which improvements to differentially private learning won’t touch on.</p>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>The authors thank Nicolas Papernot and Gautam Kamath for detailed feedback and edit suggestions.</p>

<hr />

<h2 id="references">References</h2>
<p>[1] Rahman MA, Rahman T, Laganière R, Mohammed N, Wang Y. Membership Inference Attack against Differentially Private Deep Learning Model. Trans. Data Priv.. 2018 Apr 1;11(1):61-79.</p>

<p>[2] Carlini N, Liu C, Erlingsson Ú, Kos J, Song D. The secret sharer: Evaluating and testing unintended memorization in neural networks. In 28th USENIX Security Symposium (USENIX Security 19) 2019 (pp. 267-284).</p>

<p>[3] Guo C, Karrer B, Chaudhuri K, van der Maaten L. Bounding Training Data Reconstruction in Private (Deep) Learning. arXiv preprint arXiv:2201.12383. 2022 Jan 28.</p>

<p>[4] Tramer F, Boneh D. Differentially private learning needs better features (or much more data). arXiv preprint arXiv:2011.11660. 2020 Nov 23.</p>

<p>[5] Yu D, Zhang H, Chen W, Liu TY. Do not let privacy overbill utility: Gradient embedding perturbation for private learning. arXiv preprint arXiv:2102.12677. 2021 Feb 25.</p>

<p>[6] He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. InProceedings of the IEEE conference on computer vision and pattern recognition 2016 (pp. 770-778).</p>

<p>[7] Radford A, Narasimhan K, Salimans T, Sutskever I. Improving language understanding by generative pre-training.</p>

<p>[8] Devlin J, Chang MW, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. 2018 Oct 11.</p>

<p>[9] Chen T, Kornblith S, Norouzi M, Hinton G. A simple framework for contrastive learning of visual representations. InInternational conference on machine learning 2020 Nov 21 (pp. 1597-1607). PMLR.</p>

<p>[10] Li XL, Liang P. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190. 2021 Jan 1.</p>

<p>[11] Li X, Tramer F, Liang P, Hashimoto T. Large language models can be strong differentially private learners. arXiv preprint arXiv:2110.05679. 2021 Oct 12.</p>

<p>[12] Yu D, Naik S, Backurs A, Gopi S, Inan HA, Kamath G, Kulkarni J, Lee YT, Manoel A, Wutschitz L, Yekhanin S. Differentially private fine-tuning of language models. arXiv preprint arXiv:2110.06500. 2021 Oct 13.</p>

<p>[13] Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis M, Zettlemoyer L, Stoyanov V. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. 2019 Jul 26.</p>

<p>[14] Bassily R, Smith A, Thakurta A. Private empirical risk minimization: Efficient algorithms and tight error bounds. In2014 IEEE 55th Annual Symposium on Foundations of Computer Science 2014 Oct 18 (pp. 464-473). IEEE.</p>

<p>[15] Bassily R, Feldman V, Talwar K, Guha Thakurta A. Private stochastic convex optimization with optimal rates. Advances in Neural Information Processing Systems. 2019;32.</p>

<p>[16] Yu D, Zhang H, Chen W, Yin J, Liu TY. Large scale private learning via low-rank reparametrization. InInternational Conference on Machine Learning 2021 Jul 1 (pp. 12208-12218). PMLR.</p>

<p>[17] Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I. Language models are unsupervised multitask learners. OpenAI blog. 2019 Feb 24;1(8):9.</p>

<p>[18] Bommasani R, Hudson DA, Adeli E, Altman R, Arora S, von Arx S, Bernstein MS, Bohg J, Bosselut A, Brunskill E, Brynjolfsson E, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258. 2021 Aug 16.</p>

<p>[19] Abadi M, Chu A, Goodfellow I, McMahan HB, Mironov I, Talwar K, Zhang L. Deep learning with differential privacy. InProceedings of the 2016 ACM SIGSAC conference on computer and communications security 2016 Oct 24 (pp. 308-318).</p>

<p>[20] Song S, Chaudhuri K, Sarwate AD. Stochastic gradient descent with differentially private updates. In2013 IEEE Global Conference on Signal and Information Processing 2013 Dec 3 (pp. 245-248). IEEE.</p>

<p>[21] Feldman V, Zhang C. What neural networks memorize and why: Discovering the long tail via influence estimation. Advances in Neural Information Processing Systems. 2020;33:2881-91.</p>

<p>[22] Ghosh A, Kleinberg R. Inferential privacy guarantees for differentially private mechanisms. arXiv preprint arXiv:1603.01508. 2016 Mar 4.</p>

<p>[23] Nissenbaum H. Privacy as contextual integrity. Wash. L. Rev.. 2004;79:119.</p>

<p>[24] Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I. Attention is all you need. Advances in neural information processing systems. 2017;30.</p>

<p>[25] Williams A, Nangia N, Bowman SR. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426. 2017 Apr 18.</p>

<p>[26] Novikova J, Dušek O, Rieser V. The E2E dataset: New challenges for end-to-end generation. arXiv preprint arXiv:1706.09254. 2017 Jun 28.</p>

<p>[27] Papernot N, Chien S, Song S, Thakurta A, Erlingsson U. Making the shoe fit: Architectures, initializations, and tuning for learning with privacy.</p>

<p>[28] Suriyakumar VM, Papernot N, Goldenberg A, Ghassemi M. Chasing your long tails: Differentially private prediction in health care settings. InProceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency 2021 Mar 3 (pp. 723-734).</p>

<p>[29] Bagdasaryan E, Poursaeed O, Shmatikov V. Differential privacy has disparate impact on model accuracy. Advances in Neural Information Processing Systems. 2019;32.</p>

<p>[30] Bourtoule L, Chandrasekaran V, Choquette-Choo CA, Jia H, Travers A, Zhang B, Lie D, Papernot N. Machine unlearning. In2021 IEEE Symposium on Security and Privacy (SP) 2021 May 24 (pp. 141-159). IEEE.</p>

<p>[31] Sekhari A, Acharya J, Kamath G, Suresh AT. Remember what you want to forget: Algorithms for machine unlearning. Advances in Neural Information Processing Systems. 2021 Dec 6;34.</p>

<p>[32] Cao Y, Yang J. Towards making systems forget with machine unlearning. In2015 IEEE Symposium on Security and Privacy 2015 May 17 (pp. 463-480). IEEE.</p>

<p>[33] Carlini N, Tramer F, Wallace E, Jagielski M, Herbert-Voss A, Lee K, Roberts A, Brown T, Song D, Erlingsson U, Oprea A. Extracting training data from large language models. In30th USENIX Security Symposium (USENIX Security 21) 2021 (pp. 2633-2650).</p>

<p>[34] Nasr M, Songi S, Thakurta A, Papemoti N, Carlin N. Adversary instantiation: Lower bounds for differentially private machine learning. In2021 IEEE Symposium on Security and Privacy (SP) 2021 May 24 (pp. 866-882). IEEE.</p>

<p>[35] Jagielski M, Ullman J, Oprea A. Auditing differentially private machine learning: How private is private sgd?. Advances in Neural Information Processing Systems. 2020;33:22205-16.</p>
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Authors of [<a href="https://arxiv.org/abs/2108.07258">18</a>] framed these self-supervised models which are trained on broad data at scale that are adaptable to a wide range of downstream tasks as “foundation models.” <a href="https://differentialprivacy.org/feed.xml#fnref:1" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:2">
      <p>The idea of privately fine-tuning a publicly pre-trained model certainly isn’t new. One of the first differentially private deep learning papers [<a href="https://arxiv.org/abs/1607.00133">19</a>] considered an experiment which fine-tuned convolutional nets on CIFAR-10 which were pre-trained on CIFAR-100. Results on privately fine-tuning <em>self-supervised</em> models are, on the other hand, more recent. Covering these results is our main focus here. <a href="https://differentialprivacy.org/feed.xml#fnref:2" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:3">
      <p>Blue and pink sphere avatars taken from [<a href="https://arxiv.org/abs/2108.07258">18</a>]. Credit to <a href="https://cs.stanford.edu/~dorarad/">Drew A. Hudson</a> for making these. <a href="https://differentialprivacy.org/feed.xml#fnref:3" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:4">
      <p>Unpublished result. <a href="https://differentialprivacy.org/feed.xml#fnref:4" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:5">
      <p>Hyper-parameters that work well for non-private learning typically aren’t those that work best for differentially private learning [<a href="https://openreview.net/pdf?id=rJg851rYwH">27</a>]. It’s crucial to use a large batch size, a small clipping norm, an appropriate learning rate, and a reasonably large number of training epochs to obtain the mentioned private learning results [<a href="https://arxiv.org/abs/2110.05679">11</a>]. <a href="https://differentialprivacy.org/feed.xml#fnref:5" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:6">
      <p>In practice, past works have presented mixed results on whether larger models would yield better performance. While some showed that using more filters in a convolutional network can degrade the performance of private learning after some threshold [<a href="https://openreview.net/pdf?id=rJg851rYwH">27</a>], others showed that a larger model can outperform a smaller model from a different model family [<a href="https://arxiv.org/abs/2011.11660">4</a>]. Note these results are conditioned on their particular hyperparameter choices. <a href="https://differentialprivacy.org/feed.xml#fnref:6" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:7">
      <p>Since the pre-training data for large language models are oftentimes collected through large scale web scraping (e.g., WebText), a common concern is that some training and test instances for downstream tasks may already appear in the pre-training data. Self-supervised pre-training therefore can give models an opportunity to “see” this data even before they are privately fine-tuned. Authors of [<a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">17</a>] confirmed that there is a 1-6% overlap between the test set of many natural language processing tasks and the pre-training data they collected (WebText); these common tasks, however, don’t include those studied by authors of [<a href="https://arxiv.org/abs/2110.05679">11</a>]. The numbers suggest a possibility that existing private fine-tuning results in the literature could be slightly inflated compared to when the pre-training data didn’t contain any instance for any downstream task for which evaluation was performed. <a href="https://differentialprivacy.org/feed.xml#fnref:7" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div></div>







<p class="date">
by Tatsunori Hashimoto <a href="https://differentialprivacy.org/dp-fine-tuning/"><span class="datestr">at March 15, 2022 07:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3121871012533153752">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://blog.computationalcomplexity.org/2022/03/problem-x-wont-be-solved-in-my-lifetime.html">Problem X won't be solved in MY lifetime- but what about...</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>1) In 1989 on the episde The Royale of Star Trek: The Next Generation (which takes place in the far future)  Captain Picard is working on Fermat's last theorem which he says quite explicitly is still open.</p><p>When I saw the episode I asked Larry Washington, a Number Theorist at Univ of MD, when he thought FLT would be solved. He said</p><p>                                      <i>It will be solved within the next 10 years.</i></p><p>And indeed- Wiles solved it in 1993-sort of. There was a flaw in the proof which he fixed in 1994 with the help of his former student Richard Taylor. Wiles published the correction to the flaw in 1995, so we will date it as having been solved in 1995. Larry Washington was correct.  And in an episode of Star Trek: Deep Space Nine in 1995 (episode name:Facets) Dax says that a previous host, Tobin Dax, had done the most creative work on FLT since Wiles. Maybe Tobin wrote this limerick:</p><p>A challenge for many long ages</p><p>Had baffled the savants and sages</p><p>Yet at last came the light</p><p>Seems that Fermat was right</p><p>To the margin add 200 pages.</p><p><br /></p><p>I asked Larry W when he thought Riemann would be solved. He said  </p><p>                   <i> In your lifetime but not in mine.</i></p><p>He is about 10 years older than I am and I think we are both in good health. This seems like a rather precise prediction so I am skeptical. But he did get FLT right...</p><p>2) In class I sometimes say things like </p><p><i>I do not think Quantum Computers will factor faster than classical in my lifetime. </i></p><p><i>I do not think P vs NP will be solved in my lifetime.</i></p><p><i>I can imagine P=BPP will be proven in my lifetime. (I said that 10 years ago. I am less imaginative now.) </i></p><p><i>I hope the muffin problem is solved in my lifetime (it was, see <a href="https://arxiv.org/abs/1907.08726">here</a>).</i></p><p>I didn't quite think about the difference in my age and the students until recently when I was working with Ilya Hajiaghayi (Mohammd H's 9 year old son) on cryptography and he said </p><p><i>In your recorded lecture you said you don't think quantum computers will be a threat to cryptography  in your lifetime. What about in my lifetime?</i></p><p>Indeed- his lifetime and mine are rather far apart. </p><p>I am reminded that one of the answers to my P vs NP poll made the point that while we have some sense of what will happen in the next 10 years, maybe even 20, math and life can change so much that conjectures beyond that are guesswork. Any  prediction for x years from now you should have confidence &lt; 1/ln(x) of it being true.</p><p><i><br /></i></p><p><i><br /></i></p><p><i><br /></i></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="http://blog.computationalcomplexity.org/2022/03/problem-x-wont-be-solved-in-my-lifetime.html"><span class="datestr">at March 15, 2022 02:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://benjamin-recht.github.io/2022/03/15/external-validity/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/recht.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://benjamin-recht.github.io/2022/03/15/external-validity/">Machine Learning has a validity problem.</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>One of the central tenets of machine learning warns the more times you run experiments with the same test set, the more you overfit to that test set. This conventional wisdom is mostly wrong and prevents machine learning from reconciling its inductive nihilism with the rest of the empirical sciences.</p>

<p>Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar led an passionate quest to test the overfitting hypothesis, devoting countless hours to reproducing machine learning benchmarks. In particular, they painstakingly recreated a test set of the famous <a href="https://www.image-net.org/">ImageNet benchmark</a>, which itself is responsible for bringing about the latest AI feeding frenzy. Out of the many surprises in my research career, what <a href="https://arxiv.org/abs/1902.10811">they found surprised me the most.</a></p>

<p class="center"><img width="90%" alt="The scatterplot of nightmares" src="http://www.argmin.net/assets/RSS_Scatter.png" /></p>

<p>In this graph, the x-axis is the accuracy on the original ImageNet benchmark, which has been used millions of times by individual researchers at Google alone. On the y-axis is the accuracy evaluated on “ImageNet v2” set, which was made by closely trying to replicate the data creation method for the benchmark. Each blue dot represents a single machine learning model trained on the original ImageNet data. The red line is a linear fit to these models, and the dashed line is what we would see if the accuracy was the same on both test sets. What do we see? The models which perform the best on the original test set perform the best on the new test set. That is, there is no evidence of overfitting.</p>

<p>What is clear, however, is a noticeable drop in performance on the new test set. Despite their best efforts in reproducing the ImageNet protocol, there is evidence of a <em>distribution shift</em>. Distribution shift is a far reaching term describing whenever the data on which a machine learning algorithm is deployed is different from the data on which it is trained. The Mechanical Turk workers who labeled the images were different from those originally employed. The API used for the labeling was slightly different. The selection mechanism to aggregate differences in opinions between labelers is slightly different. The small differences add up to around a 10% drop in accuracy, equivalent to five years of progress on the benchmark.</p>

<p>Folks in my research group have reproduced this phenomenon several times. In <a href="https://papers.nips.cc/paper/9117-a-meta-analysis-of-overfitting-in-machine-learning">Kaggle competitions</a>, where the held out set and validation set were <em>identically</em> distributed, we saw no overfitting <em>and</em> no distribution shift. We found sensitivity to distribution shifts in CIFAR10, in <a href="https://arxiv.org/abs/1906.02168">video</a>, and in <a href="https://arxiv.org/abs/2004.14444">question answering</a> benchmarks. And Chhavi Yadav and Leon Bottou showed that we have not yet overfit to the <a href="https://arxiv.org/abs/1905.10498">venerable MNIST data set</a>, but distribution shift remains a challenge.</p>

<p>The marked sensitivity to distribution shift is a huge issue. If small ambiguities in reproductions lead to large losses in predictive performance, what happens when we take ML systems designed on static benchmarks and deploy them in important applications? A decade of AI fever has delivered piles of evidence that distribution shift is machine learning’s achilles heel. Algorithms run inside the big tech companies need to be constantly retrained with their huge computing resources. <a href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002683">Data-driven algorithms for radiology often fail if one changes the X-ray machine</a>. <a href="https://jamanetwork.com/journals/jamainternalmedicine/article-abstract/2781307">AI algorithms for sepsis fail if you change hospitals</a>. And self-driving car systems are readily confused in new environments (No citation needed. Keep your Tesla away from me.).</p>

<p>The only way forward is for machine learning to engage more broadly with other scientists who have been tackling similar issues for centuries. My first proposal is simple: let’s change our terminology to align with the rest of the sciences. The study of distribution shift in machine learning has always been insular and, while machine learning is particularly sensitive, all empirical science must deal with the jump from experiment to reality.</p>

<p>With this in mind, <a href="https://twitter.com/rajiinio">Deb Raji</a> and I have been digging through the scientific literature for a while now hoping to find some answers. In most other parts of science, “robustness to distribution shift” is called external validity. External validity quantifies how well a finding generalizes beyond a specific experiment. For example, a significant result on a particular cohort may not generalize to a broader population.</p>

<p>Predictive algorithms and experimental science both rely on repeatability. “The sun has always risen in the east.” “The apple always falls straight to the ground.” We expect that given the same contexts, the natural world more or less repeats itself. There is unfortunately a big leap from the sun rising in the morning, to an experimental finding in machine learning or biomedicine being reproducible. Why?</p>

<p>The experimental contexts under which predictions and inferences are designed are often far too narrow. The results of a study performed on young male college students in Maine may not help us understand properties of a retirement community in Arizona. These populations are different! However, it may give us insights into other cohorts of male college students: a study at Bates may generalize to Colby or Bowdoin.</p>

<p>Contexts can change in a myriad of ways. Some examples include the following:</p>

<ol>
  <li>The context can just be too narrow in the experiment. Do studies on adults generalize to children? Do studies on medications with only men generalize to women?</li>
  <li>The measured quantity may itself change. It is often easier to measure, detect, and control for exogenous disturbances in a lab setting than in the real world.</li>
  <li>Populations can change over time. For example, medical recommendations from the 1980s may no longer apply to the current population. Recent developments have led to <a href="https://www.npr.org/2021/10/13/1045746669/task-force-says-most-people-should-not-take-daily-aspirin-to-prevent-a-heart-att">not recommending aspirin to prevent heart attacks</a>.  Machine Learners like to call this <em>covariate shift</em>.</li>
  <li>Even more nefariously, the population can change in response to the intervention. A classic example of this is <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law">Goodhart’s Law</a> which states “Any observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes.”</li>
</ol>

<p>How can we grapple with these external validity challenges? Verifying external validity is daunting and the set of potential solutions remains quite limited. As I mentioned, Deb and I have been chatting about this for a year now, and we’ve now dragged the rest of the group into our investigations. So I’m going to share the blog with Deb for a few posts now, and we’ll both expand on what we’ve been reading and thinking about. In the next few posts, we’ll explore some of the intricacies of when external validity can fail and will also try to spell out some of the research directions that might help bridge the gaps between experiments and reality.</p></div>







<p class="date">
<a href="http://benjamin-recht.github.io/2022/03/15/external-validity/"><span class="datestr">at March 15, 2022 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=19736">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2022/03/14/are-these-the-last-digits-of-pi/">Are These the Last Digits of Pi?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>Ghoulish reflections on whether mathematics is emergent</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2022/03/14/are-these-the-last-digits-of-pi/kellerrolkepi/" rel="attachment wp-att-19738"><img width="222" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/03/KellerRolkePi.png?resize=222%2C95&amp;ssl=1" class="alignright wp-image-19738" height="95" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite of <a href="https://www.fhgr.ch/en/specialist-areas/applied-future-technologies/davis-centre/pi-challenge/">src1</a>, <a href="https://www.welt.de/wissenschaft/plus233700070/Zahl-Pi-Forscher-haben-62-8-Billionen-Stellen-ermittelt.html">src2</a></font></td>
</tr>
</tbody>
</table>
<p>
Thomas Keller and Heiko Rölke led a team at the University of Applied Sciences in Graubünden, Switzerland, that set a new record for the computation of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\pi}" class="latex" /> last August. They computed 62.8 trillion digits of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\pi}" class="latex" />. The last ten digits they obtained are 7817924264.</p>
<p>
Today, we wish people Happy Pi Day amid wishes for happier days overall.<br />
<span id="more-19736"></span></p>
<p>
Pi Day needs the day to be written American-style as 3/14/22. In international style it would be 31/4/22, but April does not have 31 days. This year involves the numerator of the simplest serviceable approximation to pi: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cpi+%5Capprox+%5Cfrac%7B22%7D%7B7%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \pi \approx \frac{22}{7} " class="latex" /></p>
<p>Because <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\pi}" class="latex" /> is irrational, any finite fraction or number of digits is an approximation. Because <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\pi}" class="latex" /> is computable—indeed highly efficiently computable in <a href="https://rjlipton.wpcomstaging.com/2009/03/15/cooks-class-contains-pi/">senses</a> we have <a href="https://rjlipton.wpcomstaging.com/2010/07/14/making-an-algorithm-an-algorithm-bbp/">covered</a>—we can adduce that the code for doing so represents <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\pi}" class="latex" /> exactly. Furthermore, the symbol <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\pi}" class="latex" /> lends itself to many other calculations that yield exact finite results. </p>
<p>
The digits, however, have their own mystique. We still do not know whether they are <a href="http://pi314.at/math/normal.html">normal</a> in any base, let alone base ten. Speaking as mathematical Platonists, we regard the infinite sequence as a completed, unchanging entity—one for which assertions like “it is normal” have currently-definite truth values. </p>
<p>
This is what events of the past few weeks have prompted Dick and me to question. If our world presently stops existing, 7817924264 will be the last digits of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\pi}" class="latex" /> that we know.</p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2022/03/14/are-these-the-last-digits-of-pi/pimug/" rel="attachment wp-att-19739"><img width="150" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/03/PiMug.jpg?resize=150%2C168&amp;ssl=1" class="alignright wp-image-19739" height="168" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Etsy Math Mug <a href="https://www.etsy.com/listing/497368583/math-mug-my-pin-is-the-last-4-digits-of">source</a></font>
</td>
</tr>
</tbody></table>
<p></p><h2> Not Just WW III </h2><p></p>
<p></p><p>
I already have a story of impermanent truth rooted in Russia. On my phone I have a free app for the <a href="https://chessok.com/?page_id=27966">Lomonosov Tablebases</a>, which give the perfect result of all chess positions with 7 or fewer pieces. Those tables took years to compile and exist only as 100+ terabytes on a machine at the Lomonosov Moscow State University computer center. </p>
<p>
Sometime over the new year, I noticed that the server stopped working. It was <a href="http://talkchess.com/forum3/viewtopic.php?f=2&amp;t=74046">reportedly</a> the victim of a ransomware attack. Is it meaningful to say that the tables currently exist? Mathematically, yes, and physically likely also yes—assuming the bits were merely blocked and not altered on the storage plates. </p>
<p>
Happily, there is a second 7-piece table called <a href="https://lichess.org/blog/W3WeMyQAACQAdfAL/7-piece-syzygy-tablebases-are-complete">Syzygy</a>, which uses only about 18 terabytes and exists in multiple dowloaded copies. However, this leads us to another question about correctness. The Syzygy computation reproduced some key extremal features of the Lomonosov compilation, such as the position with the longest number of moves needed to win. Not all have been verifiable, because Syzygy counts the time needed to make concrete progress in the form of a capture or pawn advance rather than the time to give checkmate. What I don’t know—and maybe now won’t know—is:</p>
<blockquote><p><b> </b> <em> Has Syzygy been used to verify every win/draw/loss (WDL) verdict computed by Lomonosov, and vice-versa? </em>
</p></blockquote>
<p></p><p>
Doing so would require cross-checking many trillions of chess positions. Of course, we should expect that the algorithms used to produce these tables have been verified as completely correct. </p>
<p>
That is worth saying again: The chess algorithms have been verified in themselves. This is not a case of a shock I had before Christmas, when a module in my own chess software threw an error for the first time since I wrote it in 2014, having worked perfectly on over 100 million moves in several million games. The function in question records not only the exact source and destination squares of a move but also the minimum information required by short-form notation systems to disambiguate it from other pieces that could move to the square. There was one game where the players horsed around until one side had 5 queens that could all move to the same square, and that was 1 more than my scheme had presumed possible. My code is thus not-quite correct.</p>
<p>
But even analytical correctness fails in cases of hardware error. A cosmic ray temporarily <a href="https://www.independent.co.uk/news/science/subatomic-particles-cosmic-rays-computers-change-elections-planes-autopilot-a7584616.html">changed</a> the outcome of an election in Belgium. Fortunately, the systems have cross-checks to catch these events. When the subject is <em>mathematical truths</em>, however, how are we checking? On what basis can we be satisfied by such checks?</p>
<p>
</p><p></p><h2> Emergence </h2><p></p>
<p></p><p>
Despite our Platonist convictions, as practitioners of mathematics we experience its truths as <a href="https://en.wikipedia.org/wiki/Emergence">emergent</a>. By “emergent” we mean more than saying theorems are unknown until the point in time where they are clearly proved. <em>Pace</em> our <a href="https://rjlipton.wpcomstaging.com/2019/04/21/pnp-proofs/">claimers</a>, P versus NP has not been proved either way, and we live in a world where even those who strongly believe <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{P &lt; NP}}" class="latex" /> will aver it is unknown. We <a href="https://rjlipton.wpcomstaging.com/2021/12/31/make-a-trillion-dollars/">covered</a> this recently.</p>
<p>
Our sense of <em>emergent</em> meets at least the “weak” criterion enunciated <a href="http://consc.net/papers/emergence.pdf">here</a> by David Chalmers, a sense of unexpectedness <a href="https://rjlipton.wpcomstaging.com/2015/10/29/guessing-conjectures/">that</a> we <a href="https://rjlipton.wpcomstaging.com/2011/04/13/even-great-mathematicians-guess-wrong/">have</a> often <a href="https://rjlipton.wpcomstaging.com/2010/06/19/guessing-the-truth/">discussed</a> going <a href="https://rjlipton.wpcomstaging.com/2009/09/27/surprises-in-mathematics-and-theory/">back</a> to the <a href="https://rjlipton.wpcomstaging.com/2009/02/19/we-all-guessed-wrong/">beginning</a> of the blog. </p>
<p>
Chalmers’s strong sense, when applied to mathematics, leads into independence results of the kind effected by Kurt Gödel, this blog’s eponym. We realize that none of our 1,000+ posts has yet attempted a deep appraisal of what these independence results <em>mean</em>. We will not do so now because we are questioning something more basic: Discussion of Gödelian independence is with regard to a truth value that is presumed to exist. What if it doesn’t exist?</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/03/14/are-these-the-last-digits-of-pi/provemewrong/" rel="attachment wp-att-19741"><img width="277" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/03/ProveMeWrong.jpg?resize=277%2C218&amp;ssl=1" class="aligncenter wp-image-19741" height="218" /></a></p>
<p>
We are not just catching the tension of Platonism with <a href="https://en.wikipedia.org/wiki/Logical_positivism">logical positivism</a> and related paths to asking, what is science? We are asking whether reality aligns with the position that we already feel is best for <em>practice</em>. </p>
<p>
“Emergent Mathematics” is a teaching movement that, in the words of one prominent <a href="https://www.jstor.org/stable/23434871">paper</a>, gets away from mathematics courses that “are focused on completed results that often hide the messiness and complication that led to their production.” In trying to explain much data showing that the most experienced mathematicians are not the most accomplished teachers, the paper’s two authors seem to identify the former with the position of putting emphasis on completed results. They seek the best attitude for childhood <em>learning</em> of mathematics, and believe it to be orthogonal to that of presenting finished mathematics.  But going another 90 degrees around the dial, perhaps their compass needle’s other end points to the best philosophical position for <em>creating</em> mathematics.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
I think we would all still agree that the next trillion digits of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\pi}" class="latex" /> currently exist. The tougher question is whether it is scientifically meaningful to postulate knowledge of them, without knowing them. We may get an opinion on that from a regular friend late Wednesday into Thursday.</p></font></font></div>







<p class="date">
by KWRegan <a href="https://rjlipton.wpcomstaging.com/2022/03/14/are-these-the-last-digits-of-pi/"><span class="datestr">at March 14, 2022 08:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://www.solipsistslog.com/?p=551">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/noah.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="http://www.solipsistslog.com/a-mysterious-constant-called-pi-arising-from-the-gaussian-integral-with-a-minor-application-to-circles/">A mysterious constant called pi, arising from the Gaussian integral (with a minor application to circles)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Hi, nerd blog! (This is a post that I wrote a long time ago and then never published. I figured it would be nice to publish it on March 14th.)</p>



<p>Today, we’re interested in the Gaussian integral </p><p style="line-height: 41px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-096cdf537e620c218a57694e9aca2bf4_l3.png" title="Rendered by QuickLaTeX.com" height="41" width="170" alt="\[f(C) := \int_{-\infty}^\infty e^{-Cx^2} {\rm d} x\]" class="ql-img-displayed-equation quicklatex-auto-format" /></p> for <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-87ddd944257c5bf57009a80226e5c414_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="47" alt="C &gt; 0" class="ql-img-inline-formula quicklatex-auto-format" />. This integral of course has lots of very serious practical applications, as it arises naturally in the study of the Gaussian/normal distribution. But, more importantly, it’s a lot of fun to play with and is simply beautiful. We’ll see a bit about what it makes it so pretty below. We start by simply trying to figure out the value of this thing, which isn’t super easy.<p></p>



<p>By a change of variables, we immediately see that <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-051ceeda9119a5fc9be66053f9dd40a3_l3.png" title="Rendered by QuickLaTeX.com" height="19" width="37" alt="f(C)" class="ql-img-inline-formula quicklatex-auto-format" /> is proportional to <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-94fd71739495e33d6c86fe0b70cdbcf1_l3.png" title="Rendered by QuickLaTeX.com" height="21" width="46" alt="1/\sqrt{C}" class="ql-img-inline-formula quicklatex-auto-format" />. But, what is the constant of proportionality? It’s actually nicer to ask a slightly different question: what is the unique value of <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-f34f74d98915e33f37a086f8cbfb996a_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="14" alt="C" class="ql-img-inline-formula quicklatex-auto-format" /> such that <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-a4141e7514a6f611842effecf51c1199_l3.png" title="Rendered by QuickLaTeX.com" height="18" width="70" alt="f(C) = 1" class="ql-img-inline-formula quicklatex-auto-format" />. A quick numerical computation shows that <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-24b068172655b4e41a85afb20c734882_l3.png" title="Rendered by QuickLaTeX.com" height="19" width="114" alt="f(3.14159) \approx 1" class="ql-img-inline-formula quicklatex-auto-format" />. E.g., here’s some Mathematica code to find this value:<br /><img src="http://www.solipsistslog.com/wp-content/uploads/2021/03/Screen-Shot-2021-03-27-at-3.25.48-PM.png" style="width: 400px;" height="110" width="1066" alt="" class="wp-image-552" />.</p>



<p>This constant <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-3351e9436eef51bd249c328e212df088_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="13" width="97" alt="C \approx 3.14159" class="ql-img-inline-formula quicklatex-auto-format" /> is so important for this blog post that it is worth giving it a name. So, I looked through the Greek alphabet for a nice letter that doesn’t get used much and chose the obscure lowercase letter <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="8" width="11" alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" />—spelled <em>pi</em> in English, and pronounced like “pie”. In other words, by definition <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-64cecd17d70571e9f6e5d0e701cda533_l3.png" title="Rendered by QuickLaTeX.com" height="19" width="67" alt="f(\pi) = 1" class="ql-img-inline-formula quicklatex-auto-format" />. (If this implicit definition bothers you, we can equivalently just define <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-fd24a2e8eab1e07787b7429db8502015_l3.png" title="Rendered by QuickLaTeX.com" height="20" width="80" alt="\pi := f(1)^{2}" class="ql-img-inline-formula quicklatex-auto-format" />. But, I find the implicit definition to be more elegant.)</p>



<p>So, we have this brand new mysterious constant <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="8" width="11" alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" />. What should we do with it? It is of course natural to try to find different expressions for it (though our integral expression can already be used to compute it to quite high precision). A first idea is to apply the change of variables <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-fcb6cf4e246632c794949355411b046b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="15" width="62" alt="u = \pi x^2" class="ql-img-inline-formula quicklatex-auto-format" /> to obtain </p><p style="line-height: 41px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-f10d9cf7ba99c79b8b6410cfbffbcb0d_l3.png" title="Rendered by QuickLaTeX.com" height="41" width="343" alt="\[1 = 2\int_{0}^{\infty} e^{-\pi x^2}{\rm d} x = \pi^{-1/2} \int_0^{\infty} e^{-u}/u^{1/2} {\rm d} u\; .\]" class="ql-img-displayed-equation quicklatex-auto-format" /></p> So, <p style="line-height: 41px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-a70346615d75d7bd8aad3e871d66f99c_l3.png" title="Rendered by QuickLaTeX.com" height="41" width="199" alt="\[\pi =\Big( \int_0^\infty e^{-u}/u^{1/2} {\rm d} u\Big)^2\; ,\]" class="ql-img-displayed-equation quicklatex-auto-format" /></p> which you might recognize as the square of the <a href="https://en.wikipedia.org/wiki/Gamma_function" target="_blank" rel="noreferrer noopener">Gamma function</a> evaluated at <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-872abde626b8cdf8e983a3345ee98925_l3.png" title="Rendered by QuickLaTeX.com" height="19" width="25" alt="1/2" class="ql-img-inline-formula quicklatex-auto-format" />, i.e., <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-f89e7a589afd5b47baf442a3c38013ee_l3.png" title="Rendered by QuickLaTeX.com" height="20" width="93" alt="\pi = \Gamma(1/2)^2" class="ql-img-inline-formula quicklatex-auto-format" />. (Recalling that <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-59278ef14f8421f91a9c38456784aa70_l3.png" title="Rendered by QuickLaTeX.com" height="19" width="118" alt="\Gamma(n) = (n-1)!" class="ql-img-inline-formula quicklatex-auto-format" /> for integer <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="8" width="11" alt="n" class="ql-img-inline-formula quicklatex-auto-format" />, one might interpret this as saying that <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-a0df3b7b3c88b80bd1b77220c28fb5ec_l3.png" title="Rendered by QuickLaTeX.com" height="18" width="25" alt="\sqrt{\pi}" class="ql-img-inline-formula quicklatex-auto-format" /> is “the factorial of <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b5205e45813b524be8d323d4b2d0fade_l3.png" title="Rendered by QuickLaTeX.com" height="19" width="39" alt="-1/2" class="ql-img-inline-formula quicklatex-auto-format" />.”) <p></p>



<p>This mysterious identity will play a key role later. We could, of course, find other identities involving this new constant <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="8" width="11" alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" />. But, I thought instead I’d jump ahead to a rather obscure fact about the relationship between <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="8" width="11" alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" /> and a circle.</p>



<h2>Our constant’s relationship with circles</h2>



<p>In my opinion, the Gaussian distribution is far more interesting in dimensions larger than one. In particular, consider the distribution on <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-f9868b4451c5811a288f7fdd10be5558_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="21" alt="\mathbb{R}^n" class="ql-img-inline-formula quicklatex-auto-format" /> given by the probability density function </p><p style="line-height: 24px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-cbeee2e0bc6e4a6f5288818cb4aa2a7a_l3.png" title="Rendered by QuickLaTeX.com" height="24" width="129" alt="\[\Pr[\mathbf{x}] = e^{-\pi \|\mathbf{x}\|^2}\; .\]" class="ql-img-displayed-equation quicklatex-auto-format" /></p> Notice that <p style="line-height: 41px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-8c88f8ef8350b5bf5e3924147dbed5de_l3.png" title="Rendered by QuickLaTeX.com" height="41" width="457" alt="\[\int_{\mathbb{R}^n}e^{-\pi \|\mathbf{x}\|^2} {\rm d} \mathbf{x} = \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} e^{-\pi x_1^2 -\cdots - \pi x_n^2} {\rm d}x_1 \ldots {\rm d} x_n = 1\; ,\]" class="ql-img-displayed-equation quicklatex-auto-format" /></p> so that this is in fact a distribution. <p></p>



<p>In fact, up to scaling, this distribution is the <em>unique</em> continuous radial product distribution—i.e., the unique distribution such that <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-17a91f072153c1ba54d02a280dccc979_l3.png" title="Rendered by QuickLaTeX.com" height="18" width="38" alt="\Pr[\mathbf{x}]" class="ql-img-inline-formula quicklatex-auto-format" /> can be written both as a function only of the norm of <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-bcda923e732ff6e429d93d0fa7ea8a47_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="8" width="11" alt="\mathbf{x}" class="ql-img-inline-formula quicklatex-auto-format" />, <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-2c1674e9adcb304d86d51e6e3e89bd93_l3.png" title="Rendered by QuickLaTeX.com" height="19" width="124" alt="\Pr[\mathbf{x}] = f^*(\|\mathbf{x}\|)" class="ql-img-inline-formula quicklatex-auto-format" /> for some continuous function <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-d1bc0c1e9f63254ef68d09d8ea3e316d_l3.png" title="Rendered by QuickLaTeX.com" height="16" width="17" alt="f^*" class="ql-img-inline-formula quicklatex-auto-format" />, <em>and</em> as a product of functions of its coordinates, <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-724a32e8712eb42c68f40510a4461d33_l3.png" title="Rendered by QuickLaTeX.com" height="19" width="188" alt="\Pr[\mathbf{x}] = f_1(x_1)\cdots f_n(x_n)" class="ql-img-inline-formula quicklatex-auto-format" />. This makes the Gaussian a uniquely powerful tool for reducing complicated multi-dimensional problems to one-dimensional problems. </p>



<p>For example, suppose that for some strange reason we wish to know the circumference of a circle with radius one. (If we were less civilized mathematicians, we might instead set the diameter to be equal to <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-4868771cbc422b5818f85500909ce433_l3.png" title="Rendered by QuickLaTeX.com" height="13" width="7" alt="1" class="ql-img-inline-formula quicklatex-auto-format" />, so that the radius would be <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-872abde626b8cdf8e983a3345ee98925_l3.png" title="Rendered by QuickLaTeX.com" height="19" width="25" alt="1/2" class="ql-img-inline-formula quicklatex-auto-format" />.) We can try to write this as some kind of path integral or something—and suffer quite a bit in the process—or we can use the following beautiful trick. We can write<br /></p><p style="line-height: 41px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-7d71e3575dad757294955cdf449254fa_l3.png" title="Rendered by QuickLaTeX.com" height="41" width="430" alt="\[1 = \int_{\mathbb{R}^2} e^{-\pi \|x\|^2} {\rm d} x = \int_0^\infty e^{-\pi r^2} \sigma_r {\rm d} r= \sigma_1 \int_0^\infty e^{-\pi r^2} r {\rm d} r\;,\]" class="ql-img-displayed-equation quicklatex-auto-format" /></p><br />where <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-192e8dc0de46182a65dec93ebf8b5081_l3.png" title="Rendered by QuickLaTeX.com" height="11" width="16" alt="\sigma_r" class="ql-img-inline-formula quicklatex-auto-format" /> is the circumference of a circle of with radius <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-c409433a9e2dfcdb83360a974d243f18_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="8" width="8" alt="r" class="ql-img-inline-formula quicklatex-auto-format" />. (The <em>only</em> facts that we have used here are our definition of <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="8" width="11" alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" /> together with the fact that <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-c8ca93d60deca29a9a89487ef2eddf50_l3.png" title="Rendered by QuickLaTeX.com" height="11" width="66" alt="\sigma_r = r \sigma_1" class="ql-img-inline-formula quicklatex-auto-format" />.) Fortunately, the last integral is easy to compute as <br /><p style="line-height: 41px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-f7b0b0433407f63b0c9f6682c331ab63_l3.png" title="Rendered by QuickLaTeX.com" height="41" width="303" alt="\[\int_0^\infty e^{-\pi r^2} r {\rm d} r = \frac{1}{2\pi} \cdot \int_0^\infty e^{-u} {\rm d} u = \frac{1}{2\pi} \;.\]" class="ql-img-displayed-equation quicklatex-auto-format" /></p> Rearranging, we see that <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-bceb0469014bc513c98176ca8e9baf1d_l3.png" title="Rendered by QuickLaTeX.com" height="15" width="61" alt="\sigma_1 = 2\pi" class="ql-img-inline-formula quicklatex-auto-format" />!<p></p>



<p>So, surprisingly, our mysterious constant <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="8" width="11" alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" /> is actually intimately related with the circumference of a circle. (If we were less civilized mathematicians, we might even have simply defined <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="8" width="11" alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" /> to be the circumference of a circle with radius <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-872abde626b8cdf8e983a3345ee98925_l3.png" title="Rendered by QuickLaTeX.com" height="19" width="25" alt="1/2" class="ql-img-inline-formula quicklatex-auto-format" />.)</p>



<h2>What’s so special about two dimensions? Surface area of n-spheres.</h2>



<p>But, why stop in dimension <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-e584dd0bab4e6c8efc164939c28db757_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="8" alt="2" class="ql-img-inline-formula quicklatex-auto-format" />? This same <em>one neat trick</em> is just as useful in higher dimensions. E.g., what is the surface area <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-e14b74f2c92c5d34a31da4576bfaecc9_l3.png" title="Rendered by QuickLaTeX.com" height="24" width="46" alt="\sigma_1^{(n-1)}" class="ql-img-inline-formula quicklatex-auto-format" /> of a unit sphere in <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="8" width="11" alt="n" class="ql-img-inline-formula quicklatex-auto-format" /> dimensions? (Conventionally, we write the <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="8" width="11" alt="n" class="ql-img-inline-formula quicklatex-auto-format" />-sphere as <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-8897f8b728befe1f7421e387094e6fd4_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="15" width="37" alt="S^{n-1}" class="ql-img-inline-formula quicklatex-auto-format" /> because it as an <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-8c10e83257454bc711811cc71f964a7e_l3.png" title="Rendered by QuickLaTeX.com" height="19" width="53" alt="(n-1)" class="ql-img-inline-formula quicklatex-auto-format" />-dimensional object that happens to be embedded in <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="8" width="11" alt="n" class="ql-img-inline-formula quicklatex-auto-format" />-dimensional space. This is why I write <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-1529a8cd27bcb6dcd64708282b0f0f0f_l3.png" title="Rendered by QuickLaTeX.com" height="24" width="46" alt="\sigma^{(n-1)}_1" class="ql-img-inline-formula quicklatex-auto-format" /> for its surface area.) Well, we have </p><p style="line-height: 41px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-f260e2a2d6410051fb74cc87df3c7792_l3.png" title="Rendered by QuickLaTeX.com" height="41" width="531" alt="\[1 = \int_{-\mathbb{R}^n} e^{-\pi \|x\|^2} {\rm d} x = \int_0^\infty e^{-\pi r^2} \sigma^{(n-1)}_r {\rm d} r= \sigma^{(n-1)}_1 \int_0^\infty e^{-\pi r^2} r^{n-1} {\rm d} r\; .\]" class="ql-img-displayed-equation quicklatex-auto-format" /></p> (Again, the only property that I have used here is that <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b61ebafad59fad72671514841700ebab_l3.png" title="Rendered by QuickLaTeX.com" height="24" width="153" alt="\sigma_r^{(n-1)} = r^{n-1} \sigma_1^{(n-1)}" class="ql-img-inline-formula quicklatex-auto-format" />.) This integral is a bit less pretty, but using the same approach, we see that  <p style="line-height: 42px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-65c828363c086f1b820b730be79a4c39_l3.png" title="Rendered by QuickLaTeX.com" height="42" width="437" alt="\[\int_0^\infty e^{-\pi r^2} r^{n-1} {\rm d} r = \frac{1}{2\pi^{n/2}} \cdot \int_0^{\infty} e^{-u} u^{n/2-1} {\rm d} u = \frac{\Gamma(n/2)}{2\pi^{n/2}}\; ,\]" class="ql-img-displayed-equation quicklatex-auto-format" /></p> where the last step is literally just plugging in the definition of the Gamma function. Rearranging, we see that the surface area of the unit sphere in <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="8" width="11" alt="n" class="ql-img-inline-formula quicklatex-auto-format" /> dimensions is exactly <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-50d143fa8519caff967068ce2d9de43f_l3.png" title="Rendered by QuickLaTeX.com" height="30" width="42" alt="\frac{2\pi^{n/2}}{\Gamma(n/2)}" class="ql-img-inline-formula quicklatex-auto-format" />.<p></p>



<p>If the Gamma function intimidates you, that’s fine. (It certainly intimidates me.) We can go a bit further by remembering that for integers <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-6b41df788161942c6f98604d37de8098_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="8" width="15" alt="m" class="ql-img-inline-formula quicklatex-auto-format" />, <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-d226e3a5659b079c31cbf51beb7d6f69_l3.png" title="Rendered by QuickLaTeX.com" height="19" width="128" alt="\Gamma(m) = (m-1)!" class="ql-img-inline-formula quicklatex-auto-format" />, while <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-5d2b39f9e1062dcc5fa00ec58d82eb91_l3.png" title="Rendered by QuickLaTeX.com" height="19" width="246" alt="\Gamma(m+1/2) = \sqrt{\pi}(2m)!/(4^m m!)" class="ql-img-inline-formula quicklatex-auto-format" />. (Both of these identities follow from the relation <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-fb2599738079fdbd25e5b92aa6f75617_l3.png" title="Rendered by QuickLaTeX.com" height="19" width="178" alt="\Gamma(x) = (x-1) \Gamma(x-1)" class="ql-img-inline-formula quicklatex-auto-format" />, which follows from integration by parts, together with the values <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-53c41acb4ea9695fbe3fbed4d494e73a_l3.png" title="Rendered by QuickLaTeX.com" height="19" width="65" alt="\Gamma(1) = 1" class="ql-img-inline-formula quicklatex-auto-format" /> and <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-119c76244632be47b3cddb44bcce032b_l3.png" title="Rendered by QuickLaTeX.com" height="19" width="101" alt="\Gamma(1/2) = \sqrt{\pi}" class="ql-img-inline-formula quicklatex-auto-format" />.) </p>



<p>Then, we see that the surface area of the unit sphere in <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="8" width="11" alt="n" class="ql-img-inline-formula quicklatex-auto-format" /> dimensions is </p><p style="line-height: 55px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-7093395acfb9701717087f97c5b73145_l3.png" title="Rendered by QuickLaTeX.com" height="55" width="318" alt="\[\sigma_1^{(n-1)} = \begin{cases}\pi^{n/2} \cdot \frac{2}{(n/2-1)!} &amp;n\text{ even}\\\pi^{(n-1)/2} \cdot \frac{2^n \cdot ((n-1)/2)!}{(n-1)!} &amp;n\text{ odd}.\end{cases}\]" class="ql-img-displayed-equation quicklatex-auto-format" /></p> In particular, from this formula, we immediately see the perhaps surprising fact that the surface area of the sphere in <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="8" width="11" alt="n" class="ql-img-inline-formula quicklatex-auto-format" /> dimensions rapidly approaches <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-a5e437be25f29374d30f66cd46adf81c_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="9" alt="0" class="ql-img-inline-formula quicklatex-auto-format" /> as <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-032ad3d6327a419ab33b269b23c31b7c_l3.png" title="Rendered by QuickLaTeX.com" height="10" width="55" alt="n \to \infty" class="ql-img-inline-formula quicklatex-auto-format" />. (I.e., “<img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="8" width="11" alt="n" class="ql-img-inline-formula quicklatex-auto-format" />-dimensional unit spheres are tiny.”) We also see the rather strange fact that <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-e14b74f2c92c5d34a31da4576bfaecc9_l3.png" title="Rendered by QuickLaTeX.com" height="24" width="46" alt="\sigma_1^{(n-1)}" class="ql-img-inline-formula quicklatex-auto-format" /> is a rational multiple of <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-362edd7540e9e43db2e65a0ddc45e684_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="17" width="43" alt="\pi^{\lfloor n/2 \rfloor}" class="ql-img-inline-formula quicklatex-auto-format" />.<p></p>



<p>We can also plug in low values of <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="8" width="11" alt="n" class="ql-img-inline-formula quicklatex-auto-format" /> to see what we get. E.g., I have heard that some people are interested in the case when <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-a47a3377f14fefc160d10b089a4aab45_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="42" alt="n = 2" class="ql-img-inline-formula quicklatex-auto-format" /> and <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-8eda9fc1983d40517cca42fa671a0f51_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="43" alt="n = 3" class="ql-img-inline-formula quicklatex-auto-format" />. Plugging in, one sees that the circumference of a circle with radius one is <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-5bfa2124624f767670227d1aeab8d85c_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="20" alt="2\pi" class="ql-img-inline-formula quicklatex-auto-format" /> (which, ok, we already saw before), and that the surface area of a sphere with radius one is <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-a64f86508ea52835b7fd42736282275d_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="20" alt="4\pi" class="ql-img-inline-formula quicklatex-auto-format" />. But, we can easily go farther: the surface area in four dimensions is <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-7249ec3f5fb52ad8207e0f9d873c7a4f_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="15" width="27" alt="2\pi^2" class="ql-img-inline-formula quicklatex-auto-format" />, and in five dimensions, it is <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-5b263bf1209b153f5824b853138f0510_l3.png" title="Rendered by QuickLaTeX.com" height="20" width="45" alt="8\pi^{2}/3" class="ql-img-inline-formula quicklatex-auto-format" />.</p>



<p>And, we can of course find the volume of the unit <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="8" width="11" alt="n" class="ql-img-inline-formula quicklatex-auto-format" />-ball by computing a simple integral </p><p style="line-height: 44px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-08b8932dcb424209e24a111106e869b2_l3.png" title="Rendered by QuickLaTeX.com" height="44" width="366" alt="\[V^{(n)} = \int_0^{1}\sigma_r^{(n)} {\rm} d r = \sigma_1^{(n)} \cdot \int_0^{1} r^{n-1} {\rm d} r = \sigma_1^{(n)}/n \; .\]" class="ql-img-displayed-equation quicklatex-auto-format" /></p><p></p>



<p>In short, I think this mysterious constant <img src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="8" width="11" alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" /> is rather nice. Perhaps it will find other applications.</p></div>







<p class="date">
by Noah Stephens-Davidowitz <a href="http://www.solipsistslog.com/a-mysterious-constant-called-pi-arising-from-the-gaussian-integral-with-a-minor-application-to-circles/"><span class="datestr">at March 14, 2022 02:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/039">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/039">TR22-039 |  Parallel Repetition For All 3-Player Games Over Binary Alphabet | 

	Kunal Mittal, 

	Uma Girish, 

	Justin Holmgren, 

	Wei Zhan, 

	Ran Raz</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove that for every 3-player (3-prover) game, with binary questions and answers and value less than $1$, the value of the $n$-fold parallel repetition of the game decays polynomially fast to $0$. That is, for every such game, there exists a constant $c&gt;0$, such that the value of the $n$-fold parallel repetition of the game is at most $n^{-c}$.

Along the way to proving this theorem, we prove two additional parallel repetition theorems for multiplayer (multiprover) games, that may be of independent interest:

$\textbf{Playerwise Connected Games (with any number of players and any Alphabet size):}$ We identify a large class of multiplayer games and prove that for every game with value less than $1$ in that class, the value of the $n$-fold parallel repetition of the game decays polynomially fast to $0$.

More precisely, our result applies for $\textit{playerwise connected games}$, with any number of players and any alphabet size:
For each player $i$, we define the graph $G_i$, whose vertices are the possible questions for that player and two questions $x,x'$ are connected by an edge if there exists a vector $y$ of questions for all other players, such that both $(x,y)$ and $(x',y)$ are asked by the referee with non-zero probability. We say that the game is $\textit{playerwise connected}$ if for every $i$, the graph $G_i$ is connected.

Our class of playerwise connected games is strictly larger than the class of connected games that was defined in [DHVY17] and for which exponentially fast decay bounds are known [DHVY17]. For playerwise connected games that are not connected, only inverse Ackermann decay bounds were previously known [Ver96].

$\textbf{Exponential Bounds for the Anti-Correlation Game:}$ In the 3-player $\textit{anti-correlation game}$, two out of three players are given $1$ as input, and the remaining player is given $0$. The two players who were given $1$ must produce different outputs in $\{0,1\}$. We prove that the value of the $n$-fold parallel repetition of that game decays exponentially fast to $0$. That is, there exists a constant $c&gt;0$, such that the value of the $n$-fold parallel repetition of the game is at most $2^{-cn}$. Only inverse Ackermann decay bounds were previously known [Ver96].

The 3-player anti-correlation game was studied and motivated in several previous works. In particular, Holmgren and Yang gave it as an example for a 3-player game whose non-signaling value (is smaller than $1$ and yet) does not decrease at all under parallel repetition [HY19].</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/039"><span class="datestr">at March 14, 2022 05:14 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/038">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/038">TR22-038 |  Lower bounds for Polynomial Calculus with extension variables over finite fields | 

	Sasank Mouli, 

	Russell Impagliazzo, 

	Toniann Pitassi</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
For every prime p &gt; 0, every n &gt; 0 and ? = O(logn), we show the existence
of an unsatisfiable system of polynomial equations over O(n log n) variables of degree O(log n) such that any Polynomial Calculus refutation over F_p with M extension variables, each depending on at most ? original variables requires size exp(?????(n2/(?^2*2^?*(M + ????nlog(n))))) .</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/038"><span class="datestr">at March 13, 2022 08:03 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/03/13/ukraine-student-senior-research-fellows-at-tel-aviv-university-apply-by-june-1-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/03/13/ukraine-student-senior-research-fellows-at-tel-aviv-university-apply-by-june-1-2022/">Ukraine student / senior research fellows at Tel Aviv University (apply by June 1, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Tel Aviv University offers emergency scholarship for research students from Ukraine (see attached). Further, I (Gil Cohen) have immediately available senior / students research fellows in theoretical computer science, coding theory, spectral graph theory, and adjacent mathematical branches.</p>
<p>Website: <a href="https://c1f423b8-ee8e-41b1-a3a7-2cfc865115ec.filesusr.com/ugd/d112fa_4de343bf5b3a410eae40b3853dcef087.pdf">https://c1f423b8-ee8e-41b1-a3a7-2cfc865115ec.filesusr.com/ugd/d112fa_4de343bf5b3a410eae40b3853dcef087.pdf</a><br />
Email: coheng@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/03/13/ukraine-student-senior-research-fellows-at-tel-aviv-university-apply-by-june-1-2022/"><span class="datestr">at March 13, 2022 11:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-527025314741039880">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2022/03/focs-2021-test-of-time-award-winners.html">FOCS 2021 Test-of-Time Award winners (and one deserving paper that missed out)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>As members of the TCS community will most likely know, FOCS established<a href="https://focs2021.cs.colorado.edu/test-of-time-awards/" target="_blank"> Test-of-Time Awards from its 2021 edition</a> to celebrate contributions published at that conference 30, 20 and 10 years before. The first list of selected winners is, as one might have expected, stellar: <br /></p><ul style="text-align: left;"><li>Uriel Feige, Shafi Goldwasser, László Lovász, Shmuel Safra, Mario Szegedy:<br />Approximating Clique is Almost NP-Complete.<br />FOCS 1991</li><li>David Zuckerman:<br />Simulating BPP Using a General Weak Random Source.<br />FOCS 1991</li><li>Serge A. Plotkin, David B. Shmoys, Éva Tardos:<br />Fast Approximation Algorithms for Fractional Packing and Covering Problems.<br />FOCS 1991</li><li>Ran Canetti:<br />Universally Composable Security: A New Paradigm for Cryptographic Protocols.<br />FOCS 2001</li><li>Boaz Barak:<br />How to Go Beyond the Black-Box Simulation Barrier.<br />FOCS 2001</li><li>Amit Chakrabarti, Yaoyun Shi, Anthony Wirth, Andrew Chi-Chih Yao:<br />Informational Complexity and the Direct Sum Problem for Simultaneous Message Complexity.<br />FOCS 2001</li><li>Zvika Brakerski, Vinod Vaikuntanathan:<br />Efficient Fully Homomorphic Encryption from (Standard) LWE.<br />FOCS 2011</li></ul><p>FWIW, I offer my belated congratulations to all the award recipients, whose work has had, and continues to have, a profound influence on the "Volume A" TCS community. </p><p>Apart from celebrating their achievement, the purpose of this post is to highlight a paper from FOCS 1991 that missed out on the Test-of-Time Award, but that, IMHO, would have fully deserved it. </p><p>I am fully aware that the number of deserving papers/scientists is typically larger, if not much larger, than the number of available awards. Awards are a scarce resource! My goal with this post is simply to remind our community (and especially its younger members) of a seminal contribution that they might want to read or re-read. <br /></p><p>The paper in question is "<a href="https://www.cs.cornell.edu/courses/cs6860/2019sp/Handouts/EmersonJutla91.pdf" target="_blank">Tree automata, mu-calculus and determinacy</a>" by <a href="https://www.cs.utexas.edu/~emerson/" target="_blank">Allen Emerson</a> and <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-csjutla" target="_blank">Charanjit S. Jutla</a>, which appeared at FOCS 1991. (Emerson shared the 2007 A.M. Turing Award for the invention of model checking and Jutla went on to doing path-breaking work in cryptography.) That paper is absolutely fundamental for the mu-calculus, but also for automata theory, and verification in general. It introduced many ideas and results that became the basis for extensive research. </p><p>As a first contribution, the article introduced <a href="https://en.wikipedia.org/wiki/Parity_game" target="_blank">parity games</a> and proved their fundamental properties. The parity condition was a missing link in automata theory on infinite objects. It made the whole theory much simpler than that proposed in earlier work.  Technically, the parity condition is both universal and positional. Universal means that tree automata with parity conditions are as expressive as those with Rabin or Muller conditions. Positional means that in the acceptance game if a player has a winning strategy then she has one depending only on the current position and not on the history of the play so far. This is a huge technical advance for all automata-theoretic constructions and for the analysis of  infinite-duration games. It allows one, for instance,  to avoid the complicated arguments of Gurevich and Harlington in <a href="https://doi.org/10.1145/800070.802177" target="_blank">their seminal STOC 1982 article</a>, which were already a huge simplification of <a href="https://www.ams.org/journals/tran/1969-141-00/S0002-9947-1969-0246760-1/S0002-9947-1969-0246760-1.pdf" target="_blank">Rabin's original argument from 1969</a> proving the decidability of the <a href="https://en.wikipedia.org/wiki/Monadic_second-order_logic#Decidability_and_complexity_of_satisfiability" target="_blank">monadic second-order theory of the infinite binary tree</a> and much more. In passing, let me remark that Rabin has gone on record saying that "I consider this to be the most difficult research I have ever done." See <a href="https://cacm.acm.org/magazines/2010/2/69370-an-interview-with-michael-rabin/fulltext" target="_blank">this interview</a> in CACM. </p><p>The second main contribution of that paper is the discovery of the relation between parity games and the mu-calculus. The authors show how a mu-calculus model-checking problem can be reduced to solving a parity game, and conversely, how the set of winning positions in a parity game can be described by a mu-calculus formula. This result is the birth of the "model-checking via games" approach. It also shows that establishing a winner in parity games is contained both in NP and in co-NP. As a corollary, the model-checking problem is as complex as solving games. It is still not known if the problem is in PTIME. A <a href="https://doi.org/10.1145/3055399.3055409" target="_blank">recent advance from STOC'17</a> gives a quasi-polynomial-time algorithm. (See <a href="https://blog.computationalcomplexity.org/2017/03/parity-games-in-quasipolynomial-time.html" target="_blank">this blog post</a> for a discussion of that result, which received the STOC 2017 best paper award and was immediately followed up by a flurry of related papers.) </p><p>Finally, the paper also shows how to prove Rabin's complementation lemma, which is the most difficult step in his celebrated aforementioned decidability result, with the help of parity conditions. The proof is radically simpler than previous approaches. The paper puts this contribution most prominently, but actually the conceptual and technical contributions presented later in the paper turned out to be most important for the community. </p><p>Overall, the above-mentioned paper by Emerson and Jutla is a truly seminal contribution that has stood the test of time, has sown the seeds for much research  over the last thirty years (as partly witnessed by the over 1,130 citations it has received so far) and is still stimulating advances at the cutting edge of theoretical computer science that bridge the Volume A-Volume B divide. </p><p>I encourage everyone to read it!</p><p><b>Acknowledgement:</b> I have learnt much of the content of this post from Igor Walukiewicz. The responsibility for any infelicity is mine alone. <br /></p></div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2022/03/focs-2021-test-of-time-award-winners.html"><span class="datestr">at March 12, 2022 12:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2022-03-12-dfinity-synchrony/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2022-03-12-dfinity-synchrony/">Consensus by Dfinity - Part I</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
This is part one of a two-part post on consensus protocols published by the Dfinity Foundation. Dfinity published two protocols: The first, published in 2018, is a BFT protocol under synchrony by Hanke, Movahedi, and Williams. We will call this protocol Dfinity’s Synchronous Consensus (DSC). An independent report called Dfinity...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2022-03-12-dfinity-synchrony/"><span class="datestr">at March 12, 2022 05:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2022-03-12-dfinity-partial-synchrony/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2022-03-12-dfinity-partial-synchrony/">Consensus by Dfinity - Part II (Internet Computer Consensus)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
This post is part two of a two-part post on consensus protocols published by the Dfinity Foundation; you can find part one here. This post will intuitively explain the Internet Computer Consensus. The differences between DSC and ICC are primarily due to the underlying network model that they assume —...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2022-03-12-dfinity-partial-synchrony/"><span class="datestr">at March 12, 2022 05:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://thmatters.wordpress.com/?p=1362">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sigact.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://thmatters.wordpress.com/2022/03/11/call-for-nominations-knuth-prize/">Call for nominations: Knuth Prize</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><strong>Deadline: March 31, 2022.</strong></p>



<p>The Donald E. Knuth Prize for outstanding contributions to the foundations of computer science is awarded for major research accomplishments and contributions to the foundations of computer science over an extended period of time. The Prize is awarded annually by the <a href="https://urldefense.com/v3/__https://acm.org/__;!!IBzWLUs!FhlJI8zMLEOwnp6yNYlHlUi7BTqdRYKuBcTGlpuO3upUQ3CeNXk5Et-Ykc0qrYCK$" target="_blank" rel="noreferrer noopener">ACM</a><a href="https://urldefense.com/v3/__https://www.sigact.org/__;!!IBzWLUs!FhlJI8zMLEOwnp6yNYlHlUi7BTqdRYKuBcTGlpuO3upUQ3CeNXk5Et-YkRfc620V$" target="_blank" rel="noreferrer noopener">Special Interest Group on Algorithms and Computation Theory</a> (SIGACT) and the <a href="https://urldefense.com/v3/__https://www.ieee.org/__;!!IBzWLUs!FhlJI8zMLEOwnp6yNYlHlUi7BTqdRYKuBcTGlpuO3upUQ3CeNXk5Et-YkSouJjK-$" target="_blank" rel="noreferrer noopener">IEEE</a><a href="https://urldefense.com/v3/__https://tc.computer.org/tcmf/__;!!IBzWLUs!FhlJI8zMLEOwnp6yNYlHlUi7BTqdRYKuBcTGlpuO3upUQ3CeNXk5Et-YkcMUrupP$" target="_blank" rel="noreferrer noopener">Technical Committee on the Mathematical Foundations of Computing</a> (TCMF).</p>



<p><strong>Nomination Procedure.</strong> Anyone in the Theoretical Computer Science community may nominate a candidate. To do so, please send nominations to <strong><a target="_blank" rel="noreferrer noopener">knuth.prize.2022@gmail.com</a></strong> by <strong>March 31, 2022</strong>. The nomination should state the nominee’s name, summarize their contributions in one or two pages, provide a CV for the nominee or a pointer to the nominee’s web page, and give telephone and email contact information for the nominator. Any supporting letters from other members of the community (up to a limit of 5) should be included in the package that the nominator submits. Supporting letters should contain substantial information not in the nomination. Others may endorse the nomination simply by adding their names to the nomination letter. If you have nominated a candidate in past years, you can re-nominate the candidate by sending a message to that effect to the above email address. (You may revise the nominating materials if you so desire.)</p>



<p><strong>Criteria for Selection.</strong> The winner is selected by a Prize Committee consisting of six people appointed by the SIGACT and TCMF Chairs, see below for the composition of the committee.</p>



<p>Previous nominations made or updated in the last 5 years will be considered. Older nominations must be updated for consideration. Note that the Knuth Prize is awarded to a single individual each year. Nominations of groups of researchers will not be considered.</p>



<p>In selecting the Knuth Prize winner, the Committee pays particular attention to a <em>sustained record</em> of high-impact, seminal contributions to the foundations of computer science. The selection may also be based partly on educational accomplishments and contributions such as fundamental textbooks and high-quality students. The award is not given for service to the theoretical computer science community, but service may be included in the citation for a winner if appropriate.</p>



<p>The 2022 prize committee consists of Harold Gabow (U. Colorado), Monika Henzinger (U. Vienna), Kurt Mehlhorn (Max Planck Institute), Dana Randall (Chair, Georgia Tech), Madhu Sudan (Harvard U.), and Andy Yao (Tsinghua U.).</p></div>







<p class="date">
by shuchic <a href="https://thmatters.wordpress.com/2022/03/11/call-for-nominations-knuth-prize/"><span class="datestr">at March 11, 2022 08:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://thmatters.wordpress.com/?p=1357">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sigact.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://thmatters.wordpress.com/2022/03/11/call-for-nominations-godel-prize/">Call for nominations: Godel Prize</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><strong>Deadline for nominations extended to March 31st 2022.</strong></p>



<p><a href="https://urldefense.com/v3/__https://www.sigact.org/prizes/g**Adel.html__;w7Y!!IBzWLUs!EguPGLYWUarkIsJJaLrhDzTZKGjc97LX_FB94NzWMobXbrXlbTKzzky4VPAE-mxj$" target="_blank" rel="noreferrer noopener">https://www.sigact.org/prizes/g%C3%B6del.html</a></p>



<p>The Gödel Prize for outstanding papers in the area of theoretical computer science is sponsored jointly by the European Association for Theoretical Computer Science (EATCS) and the Special Interest Group on Algorithms and Computation Theory of the Association for Computing Machinery (ACM SIGACT). This award is presented annually, with the presentation taking place alternately at the International Colloquium on Automata, Languages, and Programming (ICALP) and the ACM Symposium on Theory of Computing (STOC). The thirtieth Gödel Prize will be awarded at the forty-ninth International Colloquium on Automata, Languages and Programming (ICALP), which will be hybrid, happening both physically and virtually. The physical meeting will take place in Paris, France, July 4–8 2022.</p>



<p>The Prize is named in honor of Kurt Gödel in recognition of his major contributions to mathematical logic and of his interest, discovered in a letter he wrote to John von Neumann shortly before von Neumann’s death, in what has become the famous “P versus NP” question. The Prize includes an award of USD 5,000.</p>



<p><strong>Award Committee</strong></p>



<p>The 2022 Award Committee consists of Samson Abramsky (Chair, University College London), Nikhil Bansal (University of Michigan), Irit Dinur (Weizmann Institute), Anca Muscholl (University of Bordeaux), Ronitt Rubinfeld (Massachusetts Institute of Technology), and David Zuckerman (University of Texas at Austin).</p>



<p><strong>Eligibility</strong></p>



<p>The 2022 Prize rules are given below and they supersede any different interpretation of the generic rule to be found on websites of both SIGACT and EATCS. Any research paper or series of papers by a single author or by a team of authors is deemed eligible if:</p>



<p>• The main results were not published (in either preliminary or final form) in a journal or conference proceedings before January 1, 2009.</p>



<p>• The paper was published in a recognized refereed journal no later than December 31, 2021.<br />The research work nominated for the award should be in the area of theoretical computer science. Nominations are encouraged from the broadest spectrum of the theoretical computer science community so as to ensure that potential award winning papers are not overlooked. The Award Committee shall have the ultimate authority to decide whether a particular paper is eligible for the Prize.</p>



<p><strong>Nominations</strong></p>



<p>Nominations for the award should be submitted by email to the Award Committee Chair: <a target="_blank" rel="noreferrer noopener">s.abramsky@ucl.ac.uk</a>. Please make sure that the Subject line of all nominations and related messages begin with “Goedel Prize 2022”. To be considered, nominations for the 2022 Prize must be received by March 31, 2022.</p>



<p>A nomination package should include:</p>



<p>• A printable copy (or copies) of the journal paper(s) being nominated, together with a complete citation (or citations) thereof.</p>



<p>• A statement of the date(s) and venue(s) of the first conference or workshop publication(s) of the nominated work(s) or a statement that no such publication has occurred.</p>



<p>• A brief summary of the technical content of the paper(s) and a brief explanation of its significance.</p>



<p>• A support letter or letters signed by at least two members of the scientific community.<br />Additional support letters may also be received and are generally useful. The nominated paper(s) may be in any language. However, if a nominated publication is not in English, the nomination package must include an extended summary written in English.</p>



<p>Those intending to submit a nomination should contact the Award Committee Chair by email well in advance. The Chair will answer questions about eligibility, encourage coordination among different nominators for the same paper(s), and also accept informal proposals of potential nominees or tentative offers to prepare formal nominations. The committee maintains a database of past nominations for eligible papers, but fresh nominations for the same papers (especially if they highlight new evidence of impact) are always welcome.</p></div>







<p class="date">
by shuchic <a href="https://thmatters.wordpress.com/2022/03/11/call-for-nominations-godel-prize/"><span class="datestr">at March 11, 2022 08:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/03/11/postdoc-at-utrecht-university-apply-by-april-4-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/03/11/postdoc-at-utrecht-university-apply-by-april-4-2022/">Postdoc at Utrecht University (apply by April 4, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The ERC starting grant project “Finding Cracks in the Wall of NP-completeness” of PI Jesper Nederlof aims to improve classical algorithms for NP-hard problems. For example: Can the Bellman-Held-Karp dynamic programming algorithm from the 1960’s that solves TSP with n cities in 2^n time be improved to 1.9999^n time? Join the project as a postdoc now!</p>
<p>Website: <a href="https://www.uu.nl/en/organisation/working-at-utrecht-university/jobs/postdoc-position-in-algorithmic-theory-10-fte">https://www.uu.nl/en/organisation/working-at-utrecht-university/jobs/postdoc-position-in-algorithmic-theory-10-fte</a><br />
Email: j.nederlof@uu.nl</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/03/11/postdoc-at-utrecht-university-apply-by-april-4-2022/"><span class="datestr">at March 11, 2022 07:49 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://ptreview.sublinear.info/?p=1634">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/2022/03/news-for-february-2022/">News for February 2022</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>This month has seen a flurry of activity in sublinear algorithms and a diverse collection of papers have come up, with topics ranging from differentially private sublinear algorithms to local testers for multiplicity codes. Apologies to the readers for the delay in putting this post together!</p>



<p><strong>Almost-Optimal Sublinear-Time Edit Distance in the Low Distance Regime</strong> by Karl Bringmann, Alejandro Cassis, Nick Fischer and Vasileios Nakos (<a href="https://arxiv.org/abs/2202.08066">arXiv</a>)</p>



<p>This paper considers the problem of gap edit distance, i.e., of determining if the edit distance between two strings \(x\) and \(y\) is at most \(k\) or at least \(K\). Their main result is an algorithm that runs in time \(O(n/k + \text{poly}(k))\) and solves the problem for \(K =  k \cdot 2^{\tilde{O}(\sqrt{\log k})}\). The paper improves upon earlier results of Goldenberg, Krauthgamer and Saha (2019) and Kociumaka and Saha (2020) who solved the problem for \(K = k^2\) with the same asymptotic guarantee on the query complexity. </p>



<p>One of the interesting takeaways from the paper is that the complexity of solving the gap Hamming distance and gap edit distance are similar in the low distance regime. For both, the complexity of solving the \((k, k^{1+o(1)})\)-gap problem is \(n/k^{1 \pm o(1)}\). This needs to be contrasted with the fact that solving \((k, \Omega(n))\)-gap edit distance requires \(\Omega(\sqrt{k})\) queries as shown by Batu, Ergun, Kilian, Magen and Raskhodnikova (2003), whereas \((k, \Omega(n))\)-gap Hamming distance can be solved in \(O(1)\) time. </p>



<p>These results are incomparable to those obtained by Goldenberg, Kociumaka, Krauthgamer and Saha (which was discussed in our November 2021 post), where they give a nonadaptive algorithm with complexity \(O(n/k^{1.5})\) for \((k, k^2)\)-gap edit distance problem. The algorithm in the present paper is adaptive and works faster for smaller values of \(k\).</p>



<p></p>



<p><strong>Privately Estimating Graph Parameters in Sublinear time</strong> by Jeremiah Blocki, Elena Grigorescu, Tamalika Mukherjee (<a href="https://arxiv.org/abs/2202.05776">arXiv</a>)</p>



<p>Differentially private approximation algorithms for optimization problems on graphs is a well-studied topic. This paper opens up an exciting research direction by initiating a systematic study of the design of differentially private sublinear-time algorithms. The setting is that graphs are viewed as databases and two graphs are neighboring if they differ in an edge (or a node). An algorithm \(A\) is \(\epsilon\)-differentially private if for every pair of edge-neighboring(or node-neighboring) graphs \(G, G’\) and for every subset \(S\) of outputs, \(\Pr[A(G) \in S] \leq \exp(\epsilon) \cdot \Pr[A(G’) \in S]\).</p>



<p>The paper presents \(\epsilon\)-differentially private sublinear-time algorithms for well-studied problems such as estimating the average degree, the size of a min vertex cover and the size of a maximum matching. These algorithms access the input graphs via <em>neighbor queries</em> and <em>degree queries</em>. </p>



<p>In addition to providing a strong privacy guarantee, their algorithms nearly match the approximation and complexity guarantees of their non-differentially private counterparts. The main idea seems to be the formalization of a sensitivity notion, which they refer to as Global Coupled Sensitivity, and bounding it for the known sublinear-time algorithms for the aforementioned problems. Finally, they add Laplace noise calibrated with this sensitivity value to the output of the algorithms to make them differentially private.  </p>



<p></p>



<p><strong>Testability and Local Certification of Monotone Properties in Minor-closed Classes</strong> by Louis Esperet And Sergey Norin (<a href="https://arxiv.org/abs/2202.00543">arXiv</a>)</p>



<p>One of the major interests in graph property testing is to characterize which properties are testable, i.e, can be \(\epsilon\)-tested with query complexity that depends only on the parameter \(\epsilon\). The question of testability is well-understood in the dense graph model as well as the bounded degree model. This paper concerns itself with testability questions in the general model or the sparse model of graph property testing, where graphs are represented as adjacency lists with no bound on the maximum degree. </p>



<p>The authors prove that every monotone property of<strong> </strong>minor-closed graph classes is testable with one-sided error, where a property is monotone if it is closed under taking subgraphs and a graph class is minor-closed if it is closed under taking minors. A crucial fact to be noted here is that a tester is allowed to make only uniformly random nerighbor queries. </p>



<p>This result is a significant generalization of a 2019 result by Czumaj and Sohler, who proved that for every finite set of graphs \(\mathcal{H}\), every \(\mathcal{H}\)-free property of minor-closed graph classes is testable with one-sided error, where a graph satisfies \(\mathcal{H}\)-freeness if none of its subgraphs belong to \(\mathcal{H}\). </p>



<p>They show an interesting consequence of their results to designing a short local certification scheme for monotone properties of minor-closed graph classes. Roughly speaking, they show the existence of a prover-verifier system for the aforementioned testing problem where proofs of length \(O(\log n)\) are assigned to each vertex and that verifier needs to observe only the proofs assigned to a vertex and its neighbors.<br /></p>



<p><strong>The plane test is a local tester for Multiplicity Codes</strong> by Dan Karliner, Roie Salama, and Amnon Ta-Shma (<a href="https://eccc.weizmann.ac.il/report/2022/028/">ECCC</a>)</p>



<p>Multiplicity codes are a generalization of Reed-Muller codes and was first studied by Guruswami and Wang (2013) and Kopparty, Saraf and Yekhanin (2014). The messages here are polynomials of degree \(d\) over \(m\) variables and the codeword corresponding to a polynomial \(p\) is the evaluation of \(p\) and of all of its directional derivatives of order upto \(s\) over all the points in \(\mathbb{F}_q^m\), where \(q\) is a prime power. </p>



<p>Even though multiplicity codes are known to be locally decodable, it was open whether they are locally testable. Local testers for Reed Muller codes work by restricting the evaluations to a uniformly random line in \(\mathbb{F}_q^m\) and checking whether it corresponds to the evaluations of a degree \(d\) univariate polynomial. The authors first show that such a tester does not work for the case of multiplicity codes when \(d\) is large. They then show that a <em>plane test</em> is a good local tester for multiplicity codes even for larger values of \(d\). Specifically, a plane test checks whether the restriction of a given word, which is purportedly the evaluation of a polynomial of degree \(d\) and of its derivatives, to a uniformly random plane in \(\mathbb{F}_q^m\) is a bivariate multiplicity code of degree \(d\). </p>



<p></p>



<p>We conclude the post with a short note from Nader Bshouty and Oded Goldreich on a fundamental characterization result in property testing. </p>



<p><strong>On properties that are non-trivial to test</strong> by Nader H. Bshouty and Oded Goldreich (<a href="https://eccc.weizmann.ac.il/report/2022/013/">ECCC</a>)</p>



<p>A property on binary strings is nontrivial if for infinitely many \(n\), the property contains at least one string of length \(n\) and at most \(2^{n – \Omega(n)}\) strings of length \(n\). The note shows that every nontrivial property requires \(\Omega(1/\epsilon)\) queries to \(\epsilon\)-test. </p></div>







<p class="date">
by Nithin Varma <a href="https://ptreview.sublinear.info/2022/03/news-for-february-2022/"><span class="datestr">at March 11, 2022 04:09 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/03/10/postdoc-and-phd-student-in-graph-algorithms-at-tel-aviv-university-apply-by-april-15-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/03/10/postdoc-and-phd-student-in-graph-algorithms-at-tel-aviv-university-apply-by-april-15-2022/">Postdoc and PhD student in graph algorithms at Tel Aviv University (apply by April 15, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Applications are invited for postdoc and PhD positions at the group of Shay Solomon, Tel Aviv University, funded by an ERC starting grant.<br />
The selected candidates will confront fundamental problems in graph algorithms. Applications will be accepted until the positions are filled (flexible start date).<br />
To apply, send a CV and research statement, and arrange for three letters of recommendation.</p>
<p>Website: <a href="https://sites.google.com/site/soloshay/">https://sites.google.com/site/soloshay/</a><br />
Email: shayso@tauex.tau.ac.il</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/03/10/postdoc-and-phd-student-in-graph-algorithms-at-tel-aviv-university-apply-by-april-15-2022/"><span class="datestr">at March 10, 2022 09:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/03/10/differential-privacy-research-scientist-at-harvard-university-apply-by-april-30-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/03/10/differential-privacy-research-scientist-at-harvard-university-apply-by-april-30-2022/">Differential Privacy Research Scientist at Harvard University (apply by April 30, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The OpenDP project at Harvard University seeks to hire a Research Scientist to work with faculty directors Gary King and Salil Vadhan and the OpenDP Community to formulate and advance the scientific goals of OpenDP and solve research problems that are needed for its success.</p>
<p>Website: <a href="https://academicpositions.harvard.edu/postings/11093">https://academicpositions.harvard.edu/postings/11093</a><br />
Email: opendp@g.harvard.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/03/10/differential-privacy-research-scientist-at-harvard-university-apply-by-april-30-2022/"><span class="datestr">at March 10, 2022 08:56 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/037">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/037">TR22-037 |  Robust Radical Sylvester-Gallai Theorem for Quadratics | 

	Rafael Mendes de Oliveira, 

	Abhibhav Garg, 

	Akash Sengupta</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove a robust generalization of a Sylvester-Gallai type theorem for quadratic polynomials, generalizing the result in [S'20].
More precisely, given a parameter $0 &lt; \delta \leq 1$ and a finite collection $\mathcal{F}$ of irreducible and pairwise independent polynomials of degree at most 2, we say that $\mathcal{F}$ is a $(\delta, 2)$-radical Sylvester-Gallai configuration if for any polynomial $F_i \in \mathcal{F}$, there exist $\delta(|\mathcal{F}| -1)$ polynomials $F_j$ such that $|\mathrm{rad}(F_i, F_j) \cap \mathcal{F}| \geq 3$, that is, the radical of $F_i, F_j$ contains a third polynomial in the set.

In this work, we prove that any $(\delta, 2)$-radical Sylvester-Gallai configuration $\mathcal{F}$ must be of low dimension: that is 
$$\dim \mathrm{span}(\mathcal{F}) = \mathrm{poly}(1/\delta).$$</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/037"><span class="datestr">at March 10, 2022 06:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/03/10/visiting-position-at-williams-college-apply-by-march-28-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/03/10/visiting-position-at-williams-college-apply-by-march-28-2022/">Visiting Position at Williams College (apply by March 28, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science at Williams College invites applications for a one-year visiting faculty position beginning in the fall of 2022. Candidates should have a commitment to excellence in teaching and should have a Ph.D., or made significant progress towards completing a Ph.D., in computer science or a closely related discipline by September 2022.</p>
<p>Website: <a href="https://apply.interfolio.com/103820">https://apply.interfolio.com/103820</a><br />
Email: hiring@cs.williams.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/03/10/visiting-position-at-williams-college-apply-by-march-28-2022/"><span class="datestr">at March 10, 2022 02:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2022/036">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2022/036">TR22-036 |  Classes of Hard Formulas for QBF Resolution | 

	Agnes Schleitzer, 

	Olaf Beyersdorff</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
To date, we know only a few handcrafted quantified Boolean formulas (QBFs) that are hard for central QBF resolution systems such as Q and QU, and only one specific QBF family to separate Q and QU. 

Here we provide a general method to construct hard formulas for Q and  QU. The construction uses simple propositional formulas (e.g. minimally unsatisfiable formulas) in combination with easy QBF gadgets (Sigma_2^b formulas without constant winning strategies). 
This leads to a host of new hard formulas, including new classes of hard random QBFs. 

We further present generic constructions for formulas separating Q and QU, and for separating Q and LDQ.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2022/036"><span class="datestr">at March 10, 2022 01:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=118">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2022/03/10/wednesday-march-16th-2022-eric-tchetgen-tchetgen-from-penn/">Wednesday March 16th 2022 — Eric Tchetgen Tchetgen from Penn</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p class="has-text-align-justify">The second <a href="https://sites.google.com/view/dstheory/home" target="_blank" rel="noreferrer noopener">Foundations of Data Science</a> virtual talk of this year will take place on <strong>Wednesday, March 16th</strong> at <strong>11:00 PM Pacific Time</strong> (16:00 Eastern Time, 21:00 Central European Time, 20:00 UTC). <a href="https://statistics.wharton.upenn.edu/profile/ett/">Eric Tchetgen Tchetgen</a> from<strong> The Wharton School, University of Pennsylvania</strong> will speak about “An Introduction to Proximal Causal Learning<em>”</em>.</p>



<p><a href="https://sites.google.com/view/dstheory" target="_blank" rel="noreferrer noopener">Please register here to join the virtual talk.</a></p>



<p class="has-text-align-justify"><strong>Abstract</strong>:  A standard assumption for causal inference from observational data is that one has measured a sufficiently rich set of covariates to ensure that within covariates strata, subjects are exchangeable across observed treatment values. Skepticism about the exchangeability assumption in observational studies is often warranted because it hinges on one’s ability to accurately measure covariates capturing all potential sources of confounding. Realistically, confounding mechanisms can rarely if ever, be learned with certainty from measured covariates. One can therefore only ever hope that covariate measurements are at best proxies of true underlying confounding mechanisms operating in an observational study, thus invalidating causal claims made on basis of standard exchangeability conditions.</p>



<p>Causal learning from proxies is a challenging inverse problem which has to date remained unresolved. In this paper, we introduce a formal potential outcome framework for proximal causal learning, which while explicitly acknowledging covariate measurements as imperfect proxies of confounding mechanisms, offers an opportunity to learn about causal effects in settings where exchangeability based on measured covariates fails. Sufficient conditions for nonparametric identification are given, leading to the proximal g-formula and corresponding proximal g-computation algorithm for estimation, both generalizations of Robins’ foundational g-formula and g-computation algorithm, which account explicitly for bias due to unmeasured confounding. Both point treatment and time-varying treatment settings are considered, and an application of proximal g-computation of causal effects is given for illustration.</p>



<p class="has-text-align-justify"> The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2022/03/10/wednesday-march-16th-2022-eric-tchetgen-tchetgen-from-penn/"><span class="datestr">at March 10, 2022 06:57 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2022-03-10-eip1559/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2022-03-10-eip1559/">EIP-1559 In Retrospect</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
On August 5, 2021, Ethereum implemented Ethereum Improvement Proposal 1559 (EIP-1559) on its mainnet as part of the London Hardfork, which modified the transaction fee mechanism on Ethereum from a first price auction to one that involves blocks of varying sizes, separating transaction fees as history-dependent base fees and tips,...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2022-03-10-eip1559/"><span class="datestr">at March 10, 2022 05:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/03/09/postdoc-at-university-of-texas-at-austin-apply-by-march-23-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/03/09/postdoc-at-university-of-texas-at-austin-apply-by-march-23-2022/">Postdoc at University of Texas at Austin (apply by March 23, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>This Postdoctoral Fellowship in theoretical computer science is for the 2022-23 academic year to work with David Zuckerman. Research interests should overlap with his: pseudorandomness, computational complexity, coding theory, and more. Applications will be accepted until the position is filled. To apply, send a CV and research statement, and arrange for three letters of recommendation.</p>
<p>Website: <a href="https://www.cs.utexas.edu/~diz/Sub%20Websites/Postdoc.html">https://www.cs.utexas.edu/~diz/Sub%20Websites/Postdoc.html</a><br />
Email: maguilar@cs.utexas.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/03/09/postdoc-at-university-of-texas-at-austin-apply-by-march-23-2022/"><span class="datestr">at March 09, 2022 09:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2022/03/09/postdocs-at-aalto-university-apply-by-april-1-2022/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2022/03/09/postdocs-at-aalto-university-apply-by-april-1-2022/">Postdocs at Aalto University (apply by April 1, 2022)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Our research group at Aalto University (Helsinki, Finland) is hiring postdoctoral researchers to work on the foundations of distributed and parallel computing.</p>
<p>Website: <a href="https://research.cs.aalto.fi/da/jobs/">https://research.cs.aalto.fi/da/jobs/</a><br />
Email: jukka.suomela@aalto.fi</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2022/03/09/postdocs-at-aalto-university-apply-by-april-1-2022/"><span class="datestr">at March 09, 2022 06:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2022/03/08/mathematics-books-by-women">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2022/03/08/mathematics-books-by-women.html">Mathematics books by women, in need of more reviews</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>The last couple of years, for International Women’s Day, I’ve posted about <a href="https://11011110.github.io/blog/2021/03/08/more-mathematics-books.html">mathematics books by women covered by recently added Wikipedia articles</a>. I have a few more of those that I could list today, and a few more that I plan to add, but instead, I thought I’d list books on mathematics and related areas that look interesting enough to me that I want to add them to Wikipedia, but haven’t, because I haven’t found enough reviews to use as the basis for an article.</p>

<p>If you know of more reviews for these, please let me know! I need at least three in-depth reviews, published in journals or by major societies rather than just on a web site or someone’s blog, by three different reviewers, to be comfortable writing a Wikipedia article. It would also help convince other Wikipedians that the topic is a worthy one for Wikipedia if it had at least one review in an actual journal, not just in the MAA, EMS, AMS, and zbMATH review collections, for which one could argue that a review is “routine coverage” that doesn’t suggest any particular significance for a book. Even if you can’t find more reviews, you might find some of these titles and topics intriguing enough to read the books.</p>

<ul>
  <li>
    <p><em>A Course on Tug-of-War Games with Random Noise</em> (2020), <a href="https://en.wikipedia.org/wiki/Marta_Lewicka">Marta Lewicka</a>. Research monograph concerning a game-theoretic twist on Brownian motion. Reviewed by <a href="https://www.maa.org/press/maa-reviews/a-course-on-tug-of-war-games-with-random-noise">MAA</a> and <a href="https://zbmath.org/?q=an%3A1452.91002">zbMATH</a>. Listed by MathSciNet as “pending” so maybe there’ll be a third review soon.</p>
  </li>
  <li>
    <p><em>Alfonso’s Rectifying the Curved</em> (2021), Ruth Glasner and Avinoam Baraness. A history of medieval Spanish circle-squaring. Reviewed by <a href="https://www.maa.org/press/maa-reviews/alfonsos-rectifying-the-curved">MAA</a>.</p>
  </li>
  <li>
    <p><em>Bodies of Constant Width: An Introduction to Convex Geometry with Applications</em> (2019), Horst Martini, Luis Montejano, and Déborah Oliveros. A very thorough reference on <a href="https://en.wikipedia.org/wiki/Surface_of_constant_width">surfaces of constant width</a> and related topics, one that I’ve used as a reference in several other Wikipedia articles. Reviewed by <a href="https://www.ams.org/mathscinet-getitem?mr=3930585">MathSciNet</a> and <a href="https://zbmath.org/?q=an:06999635">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>Bubbles</em> (2018), <a href="https://en.wikipedia.org/wiki/Helen_Czerski">Helen Czerski</a>. A children’s book on the science of bubbles and foam. I haven’t found any reviews.</p>
  </li>
  <li>
    <p><em>Certificates of Positivity for Real Polynomials</em> (2021), <a href="https://en.wikipedia.org/wiki/Victoria_Powers">Victoria Powers</a>. <a href="https://en.wikipedia.org/wiki/Sum-of-squares_optimization">Sum-of-squares optimization</a> and the related Lasserre hierarchy has been a hot topic in theoretical computer science in recent years. A polynomial that can be represented as a sum of squares of other polynomials automatically takes only positive values, although the converse is untrue. This book concerns both the mathematical foundations and computational aspects of this topic. MathSciNet lists it only as “preliminary” and zbMATH gives only a publisher blurb.</p>
  </li>
  <li>
    <p><em>The Classification of Quadrilaterals: A Study in Definition</em> (2008), Zalman Usiskin and Jennifer Griffin. This calls itself a “micro-curricular analysis” in mathematics education, on the best way to organize basic definitions of four-sided shapes in elementary geometry. The mathematics education community is usually pretty good about reviewing their books, but I haven’t found any reviews for this one.</p>
  </li>
  <li>
    <p><em>Combinatorial Rigidity</em> (1993), Jack Graver, <a href="https://en.wikipedia.org/wiki/Brigitte_Servatius">Brigitte Servatius</a>, and Herman Servatius. Combinatorial and matroid-theoretic analysis of when a spatial structure made from rigid bars connected by flexible ball joints has some freedom of motion, and when its bars constrain it to a single configuration. I have three reviews for this one, in <em><a href="https://doi.org/10.1090%2FS0273-0979-96-00670-2">Bull. AMS</a></em>, <a href="https://www.ams.org/mathscinet-getitem?mr=1251062">MathSciNet</a>, and <a href="https://zbmath.org/?q=an:0788.05001">zbMATH</a>, but they only count as two because the first two are by the same reviewer.</p>
  </li>
  <li>
    <p><em>Computability Theory</em> (2012), Rebecca Weber. An undergraduate textbook on what can and cannot be computed by algorithms. Reviewed by <a href="https://www.maa.org/press/maa-reviews/computability-theory">MAA</a> and <a href="https://zbmath.org/?q=an:1246.03001">zbMATH</a>. It’s listed on MathSciNet, but only with a copy of the publisher’s blurb, not an actual review.</p>
  </li>
  <li>
    <p><em>Configurations from a Graphical Viewpoint</em> (2013), Tomo Pisanski and Brigitte Servatius. A <a href="https://en.wikipedia.org/wiki/Configuration_(geometry)">configuration</a>, in this context, means a finite system of points and lines with uniform numbers of points per line and lines per point, or more abstractly a <a href="https://en.wikipedia.org/wiki/Biregular_graph">biregular graph</a>. Reviewed in <a href="https://www.ams.org/mathscinet-getitem?mr=2978043">MathSciNet</a> and <a href="https://zbmath.org/?q=an:1277.05001">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>Do Not Erase</em> (2021), Jessica Wynne. I have this one in hardcopy. It’s a photo-essay of mathematicians’ blackboards, set side-by-side with personal reflections from each mathematician. Some of the boards are obviously set up with pretty diagrams specifically for the photo, while others show the blackboards as tools for recording work-in-progress. Reviewed by <a href="https://www.maa.org/press/maa-reviews/do-not-erase">MAA</a> and <em><a href="https://journals.uic.edu/ojs/index.php/fm/article/view/12284">First Monday</a></em>. MathSciNet has publisher’s blurb (surprising me a little that it was listed at all; usually they only cover graduate and research level mathematics). zbMATH lists it but with a blank review (they didn’t follow the instructions in the title).</p>
  </li>
  <li>
    <p><em>A Field Guide to Digital Color</em> (2003), <a href="https://en.wikipedia.org/wiki/Maureen_C._Stone">Maureen C. Stone</a>. When I used to work at Xerox PARC, Stone was the go-to researcher there on anything related to color. I suspect that, despite its age, there’s still plenty of useful material in her book. But I haven’t found any reviews.</p>
  </li>
  <li>
    <p><em>Fixed Point Theory and Applications</em> (2001), Ravi P. Agarwal, Maria Meehan, and Donal O’Regan. A well-cited reference work on the <a href="https://en.wikipedia.org/wiki/Brouwer_fixed-point_theorem">Brouwer fixed-point theorem</a> and its relatives. Reviewed in <a href="https://www.ams.org/mathscinet-getitem?mr=1825411">MathSciNet</a> and <a href="https://zbmath.org/?q=an%3A0960.54027">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>Geometric Group Theory: An Introduction</em> (2017), Clara Löh. A graduate-level textbook in group theory, studying groups by their actions as symmetries of metric spaces. Reviewed in <a href="https://zbmath.org/?q=an%3A1426.20001">zbMATH</a>, but MathSciNet has only the publisher’s blurb.</p>
  </li>
  <li>
    <p><em>Geometry of Cuts and Metrics</em> (1997), Michel Deza and <a href="https://en.wikipedia.org/wiki/Monique_Laurent">Monique Laurent</a>. The polyhedral combinatorics of cut polytopes, with connections to the Kahn–Kalai counterexample to <a href="https://en.wikipedia.org/wiki/Borsuk%27s_conjecture">Borsuk’s conjecture</a> on the existence of partitions of \(d\)-dimensional convex bodies into \(d+1\) pieces of smaller diameter. Reviewed in <a href="https://mathscinet.ams.org/mathscinet-getitem?mr=1460488">MathSciNet</a> and <a href="https://zbmath.org/?q=an:0885.52001">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>Geometry: The Line and the Circle</em> (2018),  Maureen T. Carroll and Elyn Rykken. An undergraduate textbook on axiomatic geometry and non-Euclidean geometry. Reviewed by <a href="https://www.maa.org/press/maa-reviews/geometry-the-line-and-the-circle">MAA</a> and <em><a href="https://doi.org/10.1007/s00605-021-01541-9">Monatsch. Math.</a></em> Publisher blurb on zbMATH. Nothing on MathSciNet.</p>
  </li>
  <li>
    <p><em>Geometry Through History: Euclidean, Hyperbolic, and Projective Geometries</em> (2018), Meighan Dillon. The title is a bit generic and makes this sound like a generic undergraduate textbook on non-Euclidean geometry. It could be used as that, but I think it goes beyond that as a thorough exploration of the history of thought that went into the development of non-Euclidean geometry. Reviewed by <a href="https://www.maa.org/press/maa-reviews/geometry-through-history-euclidean-hyperbolic-and-projective-geometries">MAA</a> and <a href="https://mathscinet.ams.org/mathscinet-getitem?mr=3753698">MathSciNet</a> but only very briefly by <a href="https://zbmath.org/?q=an%3A1395.51002">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>Gröbner Bases in Commutative Algebra</em> (2011), Viviana Ene and Jürgen Herzog. <a href="https://en.wikipedia.org/wiki/Gr%C3%B6bner_basis">Gröbner bases</a> are the key structure in a computational method for triangularizing systems of polynomial equations. In the worst case, this method has high complexity but in practice it often works. They’ve been rediscovered multiple times by multiple people, Gröbner not among them. This is one of several graduate textbooks on the subject. Reviewed by <a href="https://www.ams.org/mathscinet-getitem?mr=2850142">MathSciNet</a> and <a href="https://zbmath.org/?q=an%3A1242.13001">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>A Guide to Experimental Algorithmics</em> (2012), <a href="https://en.wikipedia.org/wiki/Catherine_McGeoch">Catherine McGeoch</a>. If you’re going to be doing experimental research on algorithms and their implementation, or guiding undergraduate research projects in this area, this is essential reading. I only know of the <a href="https://doi.org/10.1145/2744447.2744453">SIGACT News</a> review.</p>
  </li>
  <li>
    <p><em>Hyperbolic Knot Theory</em> (2020), <a href="https://en.wikipedia.org/wiki/Jessica_Purcell">Jessica Purcell</a>. The space surrounding a knot can often be given a uniform geometric structure, in which the knot itself recedes to infinity, as depicted in the well-known video <em><a href="https://en.wikipedia.org/wiki/Not_Knot">Not Knot</a></em>. For most knots the resulting geometry is hyperbolic. This book brings this material to the level of an undergraduate text. Reviewed by <a href="https://www.maa.org/press/maa-reviews/hyperbolic-knot-theory">MAA</a> and <a href="https://zbmath.org/?q=an%3A7258506">zbMATH</a>. Listed as pending by MathSciNet.</p>
  </li>
  <li>
    <p><em>Impossibility Results for Distributed Computing</em> (2014), <a href="https://en.wikipedia.org/wiki/Hagit_Attiya">Hagit Attiya</a> and <a href="https://en.wikipedia.org/wiki/Faith_Ellen">Faith Ellen</a>. Research monograph on applications of both information theory and combinatorics in showing that certain computations are impossible for distributed computing systems to perform. I only know of the <a href="https://zbmath.org/?q=an:1396.68004">zbMATH</a> review.</p>
  </li>
  <li>
    <p><em>Information and Coding Theory</em> (2000), Gareth Jones and J. Mary Jones. An undergraduate textbook on information theory, algebraic coding theory techniques, and their interrelations. Reviewed by <em><a href="https://www.jstor.org/stable/3622076">Math. Gaz.</a></em> and <a href="https://zbmath.org/?q=an%3A0956.68052">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>An Introduction to Lorentz Surfaces</em> (1996), <a href="https://en.wikipedia.org/wiki/Tilla_Weinstein">Tilla Weinstein</a>. The Wikipedia article on <a href="https://en.wikipedia.org/wiki/Lorentz_surface">Lorentz surfaces</a> is not very informative but apparently they’re two-dimensional analogues of four-dimensional relativistic spacetime. Reviewed by <a href="https://www.ams.org/mathscinet-getitem?mr=1405166">MathSciNet</a> and <a href="https://zbmath.org/?q=an:0881.53001">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>Leonardo’s Knots</em> (2019), Caroline Cocciardi. A general audience book on the mathematical and aesthetic analysis of the knotwork visible in the artworks of Leonardo Da Vinci. Reviewed by <a href="https://www.maa.org/press/maa-reviews/leonardos-knots">MAA</a>.</p>
  </li>
  <li>
    <p><em>Nonstandard Analysis in Practice</em> (1995), edited by Francine Diener and Marc Diener. The topics in this edited volume on <a href="https://en.wikipedia.org/wiki/Nonstandard_analysis">nonstandard analysis</a> range from pedagogical to researchy. Reviewed by <a href="https://www.ams.org/mathscinet-getitem?mr=1396794">MathSciNet</a> and briefly by <a href="https://zbmath.org/?q=an%3A0848.26015">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>Paradoxes and Sophisms in Calculus</em> (2013), Sergiy Klymchuk and Susan Staples. Published several years earlier in New Zealand? Possibly most useful as a collection of examples for instructors. Reviewed by <a href="https://www.maa.org/press/maa-reviews/paradoxes-and-sophisms-in-calculus">MAA</a> and <em><a href="https://www.jstor.org/stable/10.5951/mathteacher.108.5.0398">The Mathematics Teacher</a></em>. MathSciNet and zbMATH both only have the publisher’s blurb.</p>
  </li>
  <li>
    <p><em>Ptolemy’s Philosophy: Mathematics as a Way of Life</em> (2018), Jacqueline Feke. The context and motivation behind Ptolemy’s general philosophical system. Reviewed by <em><a href="https://doi.org/10.1111/meta.12397">Metaphilosophy</a></em> and <em><a href="https://doi.org/10.1163/18725473-12341470">Int. J. Platonic Trad.</a></em></p>
  </li>
  <li>
    <p><em>Selected Topics in Convex Geometry</em> (2006), Maria Moszyńska. This appears to be a graduate textbook in convex analysis, translated from a 2001 Polish version. Reviewed by <a href="https://www.ams.org/mathscinet-getitem?mr=2169492">MathSciNet</a> and <a href="https://zbmath.org/?q=an%3A1093.52001">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>The Theory of Remainders</em> (1995), Andrea Rothbart. A playfully-written introduction to number theory, aimed at middle school teachers. Reviewed in <em><a href="https://www.periodicos.rc.biblioteca.unesp.br/index.php/bolema/article/view/10652/7040">Bolema</a></em> (a Brazilian mathematics education journal) and telegraphically in <em><a href="https://www.jstor.org/stable/2975256">Amer. Math. Monthly</a></em>.</p>
  </li>
  <li>
    <p><em>Tolerance Graphs</em> (2004), Martin Charles Golumbic and <a href="https://en.wikipedia.org/wiki/Ann_Trenk">Ann Trenk</a>. Monograph on <a href="https://en.wikipedia.org/wiki/Tolerance_graph">tolerance graphs</a>, a class of perfect graphs generalizing interval graphs by requiring intervals to have a certain length of overlap in order to become adjacent. Reviewed by <a href="https://www.ams.org/mathscinet-getitem?mr=2051713">MathSciNet</a> and <a href="https://zbmath.org/?q=an:1091.05001">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>Topics from One-Dimensional Dynamics</em> (2004), <a href="https://en.wikipedia.org/wiki/Karen_Brucks">Karen Brucks</a> and Henk Bruin. Advanced undergraduate or introductory graduate text on dynamical systems theory. Reviewed by <a href="https://mathscinet.ams.org/mathscinet-getitem?mr=2080037">MathSciNet</a> and <a href="https://zbmath.org/?q=an:1074.37022">zbMATH</a>.</p>
  </li>
  <li>
    <p><em>Unitals in Projective Planes</em> (2008), Susan Barwick and Gary Ebert. Research monograph on structures in finite geometries. Reviewed in <a href="https://zbmath.org/?q=an%3A1156.51006">zbMATH</a>. MathSciNet has only the publisher’s blurb.</p>
  </li>
</ul>

<p>(<a href="https://mathstodon.xyz/@11011110/107920020659204328">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2022/03/08/mathematics-books-by-women.html"><span class="datestr">at March 08, 2022 12:30 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=6414">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/von-neumann-entropy/">Playing with positive definite matrices – II: entropy edition</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">Symmetric positive semi-definite (PSD) matrices come up in a variety of places in machine learning, statistics, and optimization, and more generally in most domains of applied mathematics. When estimating or optimizing over the set of such matrices, several geometries can be used. The most direct one is to consider PSD matrices as a convex set in the vector space of all symmetric matrices and thus to inherit the Euclidean geometry. Like for the positive orthant (which corresponds to diagonal PSD matrices), this is not natural for a variety of reasons. In this post we will consider <a href="https://en.wikipedia.org/wiki/Bregman_divergence">Bregman divergences</a> to define our notion of geometry.</p>



<p class="justify-text">A first natural and common geometry is to consider PSD matrices as <a href="https://en.wikipedia.org/wiki/Covariance_matrix">covariance matrices</a> (or their inverses, often referred to a precision or concentration matrices). They can thus be seen as parameters of zero-mean Gaussian random vectors, and classical notions from information theory, such as the Kullback-Leibler divergence, can be brought to bear to define a specific geometry. This happens to be equivalent to using the negative log determinant to define the Bregman divergence, and leads to the usual Stein divergence $$ – \log \det A + \log \det B \ – {\rm tr}[ – B^{\, -1} ( A \, – B) ] =  -\log \det (AB^{\, -1}) + {\rm tr}( A B^{\, -1}) + d, $$ which is the <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Kullback%E2%80%93Leibler_divergence">Kullback Leibler divergence</a> between two Gaussian random vectors with zero means and covariance matrices \(B\) and \(A\). </p>



<p class="justify-text">In this blog post, we consider a different but related matrix function, which will be \(A \mapsto {\rm tr }  [ A \log A]\) instead of \(– {\rm tr} [ \log A ]\). This will have stronger links with the Shannon entropy of discrete random variables [1]. In particular, we will be able to extend two classical notions related to discrete entropy:</p>



<ul class="justify-text"><li>The entropy function \(H(p) = \ – \sum_{i=1}^d p_i \log p_i\), defined on the simplex \(\Delta \subset \mathbb{R}^d\) (vectors with non-negative components that sum to one), is concave, while the associated Bregman divergence, here the usual <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leibler (KL) divergence</a>, is $$ \sum_{i=1}^d p_i \log \frac{p_i}{q_i}. $$ It is jointly convex in the vectors \(p\) and \(q\) (see this nice article [<a href="http://carma.newcastle.edu.au/resources/jon/Preprints/Books/CUP/CUPold/MaterialI/bregman.pdf">6</a>] for other jointly convex Bregman divergences). Note that the joint convexity is here a direct application of classical results regarding the convexity of the perspective of a convex function [<a href="http://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">10</a>, section 3.2.6].</li><li>The Fenchel-Legendre conjugate of \(-H\) (with the simplex as domain) is $$\sup_{p \in \Delta} \ \ p^\top z\ – \sum_{i=1}^d p_i \log p_i = \log \Big( \sum_{i=1}^d \exp(z_i) \Big).$$ With the positive orthant as the domain, we have $$\sup_{p \in \mathbb{R}_+^d }\ \ p^\top z \ – \sum_{i=1}^d p_i \log p_i = \sum_{i=1}^d \exp(z_i-1) .$$ This has many consequences in probabilistic modelling, notably the duality between maximum likelihood and maximum entropy [1, <a href="http://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf">7</a>].</li></ul>



<p class="justify-text">The extension to matrices will have several interesting consequences:</p>



<ul class="justify-text"><li>This defines a Bregman divergence that has interesting applications to mirror descent and smoothing techniques in optimization.</li><li>The non-trivial joint convexity of the associated Bregman divergence will lead to elegant methods for deriving concentration inequalities for symmetric matrices [<a href="http://arxiv.org/pdf/1501.01571">2</a>].</li><li>Application to covariance operators in infinite-dimensional spaces will lead to a whole range of information-theoretic results [<a href="http://arxiv.org/pdf/2202.08545.pdf">5</a>], but this is for next month!</li></ul>



<h2 id="von-neumann-entropy-and-relative-entropy">Von Neumann entropy and relative entropy</h2>



<p class="justify-text">Given a positive semi-definite symmetric matrix \(A \in \mathcal{S}_d\), the von Neumann entropy is defined as $$H(A) =\ – {\rm tr} [ A \log A],$$ as the trace of the matrix function \(– A \log A\), or equivalently as \(– \sum_{i=1}^d \! \lambda_i \log \lambda_i\) where \(\lambda_1,\dots,\lambda_d \geqslant 0\) are the \(d\) eigenvalues of \(A\) (note that we can have a zero eigenvalue and still be finite). </p>



<p class="justify-text">From <a href="https://francisbach.com/matrix-monotony-and-convexity/">last month blog post</a>, we get that \(H\) is concave. We can then naturally define on the <a href="https://en.wikipedia.org/wiki/Spectrahedron">spectrahedron</a> (set of positive semi-definite matrices with unit trace), the <em>relative entropy</em>, which is the Bregman divergence associated with the function \(A \mapsto {\rm tr} [ A \log A]\), whose gradient is \(\log A + I \), leading to: $$D(A\| B) = {\rm tr} [ A \log A ] \, – {\rm tr} [ B \log B] \, – {\rm tr} \big[ ( A \, – B) ( \log B + I ) \big],$$ which is equal to $$ D(A\| B) = {\rm tr} \big[ A ( \log A \,  – \log B ) \big] – {\rm tr}(A) + {\rm tr}(B).$$ This time we have to be careful to avoid infinite values: it is finite if and only if the null space of \(B\) is included in the one of \(A\), or, equivalently if \(B^{\, -1/2} A^{1/2} B^{\, -1/2}\) is finite. Note that when \(A\) and \(B\) have unit traces, the term in \(– {\rm tr}(A) + {\rm tr}(B)\) goes away.</p>



<p class="justify-text">As a Bregman divergence of a strictly convex function, \(D(A\|B)\) is non-negative and equal to zero if and only if \(A = B\).</p>



<p class="justify-text">When \(A = {\rm Diag}(p)\) and \(B = {\rm Diag}(q)\) are diagonal, we get $$D(A\| B) = \sum_{i=1}^d \Big\{ p_i \log \frac{p_i}{q_i} \, – p_i + q_i \Big\},$$ and we recover the traditional relative entropy. </p>



<p class="justify-text">Please refrain from writing \(D(A\| B) = {\rm tr} \big[ A \log ( A B^{\,-1} ) \big] – {\rm tr}(A) + {\rm tr}(B)\), which is not true unless \(A\) and \(B\) commute. See however below how using Kronecker products can make a similar equality always true.</p>



<h2>A geometry on matrices</h2>



<p class="justify-text">The von Neumann relative entropy (also referred to as matrix KL divergence) defines a new notion of geometry, which is different from the Euclidean geometry based on the squared Frobenius norm \(\| A – B \|_{\rm F}^2 = {\rm tr} [ ( A -B)^2]\). Below, we plot and compare balls for these various geometries. We first consider Euclidean balls, defined as $$\mbox{ Euclidean : } \  \big\{ B \in \mathcal{S}_d,  \| A – B \|_{\rm F}^2 \leqslant r \big\},$$ as well as the two KL balls (one for each side of the KL divergence): $$ \mbox{ Left KL : } \ \big\{ B \in \mathcal{S}_d,  D(B\|A)  \leqslant r \big\},$$ $$\mbox{ Right KL : } \big\{ B \in \mathcal{S}_d,  D(A\|B) \leqslant r \big\}.$$</p>



<p class="justify-text"><strong>Diagonal matrices with unit trace (isomorphic to the simplex).</strong> We consider diagonal matrices \(A = {\rm Diag}(p)\) with \(p\) in the simplex. We compare the three balls below (in yellow), with \(r = 1/16\) for the three of them.</p>



<div class="wp-block-image"><figure class="aligncenter size-full"><img width="1480" alt="" src="https://francisbach.com/wp-content/uploads/2022/03/simplex_geom.gif" class="wp-image-6794" height="340" /></figure></div>



<p class="justify-text">While the Euclidean balls have the same shapes at all positions, the KL balls adapt their shapes to their centers.</p>



<p class="justify-text"><strong>Unit trace PSD matrices in dimension two (isomorphic to the Euclidean ball).</strong> We consider matrices \(A = \Big( \begin{array}{cc} x &amp; y \\[-.15cm] y &amp; \!\!\!1-x\!\end{array} \Big)\) with the PSD constraint being equivalent to \(y^2 \leqslant x(1-x)\), that is, \((x,y)\) is in the disk of center \((\frac{1}{2},0)\) and radius \(\frac{1}{2}\). We compare the three balls below (in yellow), with \(r = \frac{1}{32}\) for the Euclidean ball, and \(r =\frac{1}{16}\) for the KL balls, with a similar difference between geometries.</p>



<div class="wp-block-image"><figure class="aligncenter size-full"><img width="1486" alt="" src="https://francisbach.com/wp-content/uploads/2022/03/ellipsoid_geom.gif" class="wp-image-6798" height="422" /></figure></div>



<h2 id="pinsker-inequality-1">Pinsker inequality</h2>



<p class="justify-text">In order to use the relative entropy within mirror descent, we need an extension of the <a href="https://en.wikipedia.org/wiki/Pinsker%27s_inequality">Pinsker inequality</a> $$\sum_{i=1}^d p_i \log \frac{p_i}{q_i} \geqslant \frac{1}{2} \Big(\sum_{i=1}^d | p_i – q_i| \Big)^2,$$ which is, in optimization terms, the \(1\)-strong-concavity of the entropy with respect to the \(\ell_1\)-norm. It turns out that the natural extension is true, that is, for PSD matrices with unit trace, $$D(A\| B) =  {\rm tr} \big[ A ( \log A\,   – \log B ) \big] \geqslant \frac{1}{2} \| A \ – B \|_\ast^2,$$ where \(\| \cdot \|_\ast\) denotes the trace norm, a.k.a. the nuclear norm, that is, the \(\ell_1\)-norm of eigenvalues. See [<a href="http://www.cs.cmu.edu/~yaoliang/mynotes/sc.pdf">3</a>] for a nice and simple proof.</p>



<p class="justify-text">This allows to use the relative entropy when optimizing over the spectrahedron using mirror descent [<a href="https://www2.isye.gatech.edu/~nemirovs/MLOptChapterI.pdf">11</a>]. Note that the diameter \(\sup_{A} {\rm tr} [ A \log A] \ –   \inf_{A} {\rm tr} [ A \log A] \) over PSD matrices with unit trace is equal to \(\log d\). Using duality, the relative entropy can also be used for smoothing the maximal eigenvalue of a matrix [<a href="https://link.springer.com/content/pdf/10.1007/s10107-006-0001-8.pdf">12</a>].</p>



<h2 id="joint-convexity-of-matrix-relative-entropy">Joint convexity of matrix relative entropy</h2>



<p class="justify-text">While the convexity with respect to \(A\) is straightforward, the relative entropy \(D(A \| B)\) is also <em>jointly</em> convex in \((A,B)\), which is crucial in the developments below. There exist several proofs, but one is particularly elegant [<a href="https://arxiv.org/pdf/quant-ph/0604206.pdf">4</a>] and will use the matrix-monotonicity that I presented in the <a href="https://francisbach.com/matrix-monotony-and-convexity/">last post</a>.</p>



<p class="justify-text"><strong>Kronecker products.</strong> We need to make a clever use of <a href="https://en.wikipedia.org/wiki/Kronecker_product">Kronecker products</a>. Given two matrices \(A\) and \(B\) of any sizes, \(A \otimes B\) is the matrix defined by blocks \(A_{ij} B\), thus of size the products of the number of rows and columns of \(A\) and \(B\). In the derivations below, both \(A\) and \(B\) will be of size \(d \times d\), and thus \(A \otimes B\) is then of size \(d^2 \times d^2\). </p>



<p class="justify-text">Kronecker products have very nice properties (they would probably deserve their own post!), such as (when dimensions are compatible), \((A \otimes B) ( C\otimes D) = (AC) \otimes (BD)\) or \((A \otimes B)^{\, -1} = A^{-1} \otimes B^{\, -1}\). What we will leverage is their use in characterizing linear operations on matrices. For a rectangular matrix \(X \in \mathbb{R}^{d_1 \times d_2}\), \({\rm vec}(X) \in \mathbb{R}^{d_1 d_2 \times 1}\) denotes the <em>vectorization</em> of \(X\) obtained by stacking all columns of \(X\) one after the other in a single column vector. Then, we have $$ (B^\top \otimes A){\rm vec}(X) = {\rm vec}(AXB)$$ for any \(A, X, B\) with compatible dimensions. </p>



<p class="justify-text"><strong>Reformulation of relative entropy.</strong> We can now use Kronecker products to get: $$ {\rm tr} [A \log A] = {\rm tr}[ A^{1/2}( \log A) A^{1/2} ] = {\rm vec}(A^{1/2})^\top [ I \otimes \log A ] {\rm vec}(A^{1/2}),$$ and $$ {\rm tr} [A \log B] = {\rm tr} [ A^{1/2}( \log B) A^{1/2} ] = {\rm vec}(A^{1/2})^\top [    \log B \otimes I  ] {\rm vec}(A^{1/2}),$$ leading to $$D(A\|B) = {\rm vec}(A^{1/2})^\top \big[ I \otimes \log A \ – \log B \otimes I \big]  {\rm vec}(A^{1/2}) \, – {\rm tr}(A) + {\rm tr}(B).$$ The key element is then to use the identity for PSD matrices $$\log(C \otimes D) = \log C \otimes I + I \otimes \log D$$ (which can be shown using the fact that eigenvalue decomposition of a Kronecker product of symmetric matrices can be obtained from the decompositions of the factors), leading to $$D(A\|B) = {\rm vec}(A^{1/2})^\top \big[ \log ( I \otimes A) \ – \log (B \otimes I) \big]  {\rm vec}(A^{1/2})\, – {\rm tr}(A) + {\rm tr}(B).$$ It seems we have not achieved much, but now, the matrices \( I \otimes A\) and \(B \otimes I\) commute! Therefore, we can now write $$D(A\|B) = {\rm vec}(A^{1/2})^\top \Big[ \log \big( ( I \otimes A) (B \otimes I)^{-1} \big) \Big]  {\rm vec}(A^{1/2})- {\rm tr}(A) + {\rm tr}(B).$$ This can then be rewritten as $$D(A\|B)  =\, – {\rm vec}(A^{1/2})^\top \big[ \log ( B \otimes A^{-1}) \big] {\rm vec}(A^{1/2}) \, – {\rm tr}(A) + {\rm tr}(B).$$ </p>



<p class="justify-text"><strong>Using matrix-convexity.</strong> We can now use the matrix-convexity of \(\lambda \mapsto \ – \log \lambda\), and the representation from <a href="http://francisbach.com/matrix-monotony-and-convexity/">last post</a> as $$ -\log \lambda = 1-\lambda + (\lambda-1)^2 \int_0^{+\infty} \!\!\frac{1}{\lambda +u} \frac{du}{(1+u)^2},$$ leading to $$D(A\|B) =  \int_0^{+\infty}  {\rm vec}(A^{1/2})^\top \Big[ ( B\otimes A^{-1} – I)^2 (B\otimes A^{-1} + u I)^{-1} \Big]  {\rm vec}(A^{1/2}) \frac{du}{(1+u)^2},$$ and then, by using  \(( B\otimes A^{-1} – I) ( I \otimes A^{1/2})  {\rm vec}(A^{1/2})  =  {\rm vec}(B-A)\), $$D(A\|B) =   \int_0^{+\infty}  {\rm vec}(A-B)^\top  (B\otimes I+ u I \otimes A)^{-1}   {\rm vec}(A-B)   \frac{du}{(1+u)^2}.$$ We can then use the joint convexity of the function \((C,D) \mapsto {\rm tr} \big[  C^\top D^{-1} C \big] \) to get the  joint convexity of \(D(A\|B)\).</p>



<p class="justify-text">This extends to all matrix-convex functions beyond the logarithm, such as \(\lambda \mapsto (\lambda-1)^2\), \(\lambda \mapsto 1 \, – \lambda^{1/2}\) or \(\displaystyle \lambda \mapsto \frac{1}{\lambda}(\lambda-1)^2\), leading to other <a href="https://en.wikipedia.org/wiki/F-divergence">\(f\)-divergences</a> between matrices. See [<a href="http://arxiv.org/pdf/2202.08545.pdf">5</a>, Appendix A.4] for details.</p>



<h2 id="lieb-concavity-theorems">Lieb concavity theorems</h2>



<p class="justify-text">Now that \((A,B) \mapsto D(A\|B)\) is jointly convex on all PSD matrices (not only unit trace), for any fixed symmetric matrix \(M\), the function $$ \varphi: B \mapsto \sup_{ A \succcurlyeq 0} \ {\rm tr}(AM) + {\rm tr}(A) \ – D(A\|B), $$ as the partial maximization (with respect to \(B\)) of a jointly concave function of \((A,B)\), is concave. We can maximize in closed form with respect to \(A\) by computing the gradient \(M \ – \log A  + \log B\), leading to \(A = \exp( M + \log B)\), and $$\varphi(B) = {\rm tr} \exp ( M + \log B),$$ which defines a concave function. This is one version of the classical Lieb concavity theorems [<a href="http://sciencedirect.com/science/article/pii/000187087390011X/pdf?md5=09b7ceaa543358cef395fa6af8de62aa&amp;pid=1-s2.0-000187087390011X-main.pdf">8</a>]. </p>



<p class="justify-text">As a consequence, we can obtain the “subadditivity of the matrix cumulant generative function”. That is, for any random <em>independent</em> symmetric matrices \(X_1,\dots,X_n\) of the same sizes, $$ \mathbb{E} \Big[  {\rm tr} \exp \big(   \sum_{i=1}^n X_i \big) \Big] \leqslant {\rm tr} \exp \big( \sum_{i=1}^n \log \big( \mathbb{E} [ \exp( X_i ) ] \big),$$ which can be shown by recursion using Jensen’s inequality for the function \(\varphi\) above with a well-chosen matrix \(M\). See [<a href="http://arxiv.org/pdf/1501.01571">2</a>] for details.</p>



<p class="justify-text">We are now ready to consider concentration inequalities for random matrices.</p>



<h2 id="application-to-concentration-inequalities">Application to concentration inequalities</h2>



<p class="justify-text">We consider random independent random matrices \(X_1,\dots,X_n\) such that \(\mathbb{E} [ X_i ] = 0\) for all \(i \in \{1,\dots,n\}\). Let \(S = \frac{1}{n} \sum_{i=1}^n \! X_i\) (which has zero mean). Our goal is to provide upper bounds on \(\mathbb{E} \big[ \lambda_{\max}(S) \big]\) and \(\mathbb{P} \big[ \lambda_{\max}(S) \geqslant t \big]\). We follow the outstanding work and presentation of <a href="http://users.cms.caltech.edu/~jtropp/">Joel Tropp</a> [<a href="http://arxiv.org/pdf/1501.01571">2</a>]: using the proper matrix tools presented above, we can get for matrix-valued random variables the almost exact same proofs of the usual concentration inequalities for real-valued random <em>variables</em>, such as <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">Hoeffding</a>, <a href="https://en.wikipedia.org/wiki/Bernstein_inequalities_(probability_theory)">Bernstein</a>, <a href="https://en.wikipedia.org/wiki/Azuma%27s_inequality">Azuma</a>, or Mac-Diarmid inequalities. </p>



<p class="justify-text"><strong>Expectation. </strong>We have, for any \(u &gt; 0\), by homogeneity of the largest eigenvalue: $$  \mathbb{E} \big[ \lambda_{\max}(S) \big] = \frac{1}{u}  \mathbb{E} \big[ \lambda_{\max}(uS) \big] =  \frac{1}{u}  \mathbb{E} \big[  \log \exp ( \lambda_{\max}(uS) ) \big].$$ We can then use Jensen’s inequality for the logarithm to get (together with the fact the largest eigenvalue of the exponential is the exponential of the largest eigenvalue): $$  \mathbb{E} \big[ \lambda_{\max}(S) \big] \leqslant \frac{1}{u}  \log \mathbb{E} \big[ \exp ( \lambda_{\max}(uS) ) \big]=\frac{1}{u}  \log \mathbb{E} \big[ ( \lambda_{\max}( \exp ( uS) ) \big], $$ and finally that for a PSD matrix the trace is larger than the largest eigenvalue, to get $$  \mathbb{E} \big[ \lambda_{\max}(S) \big] \leqslant \frac{1}{u}  \log \mathbb{E} \big[ {\rm tr} \exp (  uS ) \big].$$ Experts in concentration inequalities have probably recognized the usual argument to bound the expectation of the maximum of \(d\) random <em>real-valued</em> (non necessarily independent) variables \(X_1,\dots,X_d\), as $$ \mathbb{E} \big[ \max \{Y_1,\dots,Y_d\} \big] \leqslant \frac{1}{u} \log \mathbb{E} \big[ \exp(uY_1)+ \cdots + \exp(u Y_d) \big].$$</p>



<p class="justify-text"><strong>Tail bound.</strong> We have, using Markov’s inequality, for any \(u &gt; 0\): $$\mathbb{P} \big[ \lambda_{\max}(S) \geqslant t \big] = \mathbb{P} \big[ \exp(\lambda_{\max}(uS)) \geqslant \exp(ut) \big] \leqslant e^{-ut} \mathbb{E} \big[ \exp ( \lambda_{\max}(uS) ) \big],$$ thus leading to $$\mathbb{P} \big[ \lambda_{\max}(S) \geqslant t \big] \leqslant e^{-ut} \mathbb{E} \big[ {\rm tr} \exp (  uS ) \big].$$ The same experts have recognized $$ \mathbb{P} \big[ \max \{Y_1,\dots,Y_d\} \geqslant t \big] \leqslant  e^{-ut} \mathbb{E} \big[ \exp(uY_1)+ \cdots + \exp(u Y_d) \big].$$ </p>



<p class="justify-text">We can now consider random matrices such that \(a I \preccurlyeq X_i \preccurlyeq b I\) almost surely for all \(i=1,\dots,n\) (and still with zero mean). We first need a lemma whose proof is given at the end of the post, with the exact same proof as for the uni-dimensional case.</p>



<p class="justify-text"><strong>Lemma.</strong> If \(\mathbb{E} [ X ] = 0\) and \(a I \preccurlyeq X \preccurlyeq b I\) almost surely, then for all \(u &gt; 0\), $$ \mathbb{E} \big[ \exp( u X) \big] \preccurlyeq \exp\big( \frac{u^2}{8}(b-a)^2 \big) I.$$ </p>



<p class="justify-text">Together with the consequence of Lieb’s result presented above and the matrix-monotonicity of the logarithm, we get: $$ \mathbb{E} \big[ {\rm tr} \exp (  uS ) \big] \leqslant {\rm tr} \exp \Big( n \big( \frac{u^2}{8n^2}(b-a)^2 I \big) \Big) = d  \exp \big(   \frac{u^2}{8n}(b-a)^2 \big).$$ We can then get immediately, by optimizing with respect to \(u\) (with optimal value \(u=4 n t /(b-a)^2\)), $$\mathbb{P} \big[ \lambda_{\max}(S) \geqslant t \big] \leqslant d \exp\big( – \frac{2 n t^2 }{(b-a)^2} \big),$$ and, with optimal value \(u= {2 \sqrt{2 n \log d }}/ { (b-a) }\): $$  \mathbb{E} \big[ \lambda_{\max}(S) \big] \leqslant \frac{b-a}{\sqrt{2n}} \sqrt{ \log d}.$$</p>



<p class="justify-text">Note that these are the exact same results as when \(d=1\)! </p>



<p class="justify-text"><strong>Extensions.</strong> As nicely detailed in [<a href="https://arxiv.org/pdf/1501.01571">2</a>], there are extensions to all classical concentration inequalities. Moreover, this can be applied to rectangular matrices, where the largest eigenvalue is replaced by the largest singular value. Finally, for those who like to work in infinite dimensions and cannot bear the explicit dependence in dimension, it is possible to derive results that depend only on a well-defined notion of “intrinsic dimension” (the associated results can then be used for the study of kernel methods, see [<a href="http://di.ens.fr/~fbach/ltfp_book.pdf">13</a>, chapter 9]).</p>



<h2 id="conclusion">Conclusion</h2>



<p class="justify-text">Now that we know everything about the matrix relative entropy and its convexity properties, we will explore next month a new link between eigenvalues of covariance operators and the regular Shannon entropy of the generating probability distribution [<a href="http://arxiv.org/pdf/2202.08545.pdf">5</a>], thus mixing my favorite recent mathematical topic with my probably all-time favorite one: positive definite kernels!</p>



<h2 id="references">References</h2>



<p class="justify-text">[1] Thomas M. Cover and Joy A. Thomas. <em>Elements of Information Theory</em>. John Wiley &amp; Sons, 1999.<br />[2] Joel A. Tropp. <a href="https://arxiv.org/pdf/1501.01571">An introduction to matrix concentration inequalities</a>. <em>Foundations and Trends in Machine Learning</em>, 8(1-2):1–230, 2015.<br />[3] Yao-Liang Yu. <a href="http://www.cs.cmu.edu/~yaoliang/mynotes/sc.pdf">The strong convexity of von Neumann’s entropy</a>. Unpublished note, 2013.<br />[4] Mary Beth Ruskai. <a href="https://arxiv.org/pdf/quant-ph/0604206.pdf">Another short and </a><a href="https://arxiv.org/pdf/quant-ph/0604206.pdf" target="_blank" rel="noreferrer noopener">elementary</a><a href="https://arxiv.org/pdf/quant-ph/0604206.pdf"> proof of strong subadditivity of quantum entropy</a>. Reports on Mathematical Physics, 60(1):1–12, 2007.<br />[5] Francis Bach. <a href="https://arxiv.org/pdf/2202.08545.pdf">Information Theory with Kernel Methods</a>. Technical Report, arXiv-2202.08545, 2022.<br />[6] Heinz H. Bauschke, and Jonathan M. Borwein. <a href="https://carma.newcastle.edu.au/resources/jon/Preprints/Books/CUP/CUPold/MaterialI/bregman.pdf">Joint and separate convexity of the Bregman distance</a>. <em>Studies in Computational Mathematics</em>, 8:23-36, 2001.<br />[7] Martin J. Wainwright and Michael I. Jordan. <a href="https://people.eecs.berkeley.edu/~wainwrig/Papers/WaiJor08_FTML.pdf">Graphical Models, Exponential Families, and Variational Inference</a>. <em>Foundations and Trends in Machine Learning</em>, 1 (1–2):1–305, 2008. <br />[8] Elliott H. Lieb. <a href="https://www.sciencedirect.com/science/article/pii/000187087390011X/pdf?md5=09b7ceaa543358cef395fa6af8de62aa&amp;pid=1-s2.0-000187087390011X-main.pdf">Convex trace functions and the Wigner-Yanase-Dyson conjecture</a>.<br /><em>Advances in Mathematics</em>, 11(3):267–288, 1973.<br />[9] Stéphane Boucheron, Gabor Lugosi, and Pascal Massart. <em>Concentration Inequalities: A Nonasymptotic Theory of Independence</em>. Oxford University Press, 2013. <br />[10] Stephen Boyd, Lieven Vandenberghe. <em><a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Convex Optimization</a></em>. Cambridge University Press, 2004.<br />[11] Anatoli Juditsky, Arkadi Nemirovski. <a href="http://www2.isye.gatech.edu/~nemirovs/MLOptChapterI.pdf">First order methods for nonsmooth convex large-scale optimization, i: general purpose methods</a>. <em>Optimization for Machine Learning</em>, 121-148, 2012.<br />[12] Yurii Nesterov. <a href="https://link.springer.com/content/pdf/10.1007/s10107-006-0001-8.pdf">Smoothing technique and its applications in Semidefinite Optimization</a>. <em>Mathematical Programming</em>, 110(2):245-259, 2007.<br />[13] Francis Bach. <a href="https://www.di.ens.fr/~fbach/ltfp_book.pdf">Learning Theory From First Principles</a>. Book draft, 2021.</p>



<h2 id="proof-of-lemma-on-matrix-cumulant-generating-functions">Proof of lemma on matrix cumulant generating functions</h2>



<p class="justify-text"><strong>Lemma.</strong> If \(\mathbb{E} [ X ] = 0\) and \(a I \preccurlyeq X \preccurlyeq b I\) almost surely, then for all \(u &gt; 0\), $$ \mathbb{E} \big[ \exp( u X) \big] \preccurlyeq \exp\big( \frac{u^2}{8}(b-a)^2 I \big) =  \exp\big( \frac{u^2}{8}(b-a)^2 \big) I .$$ Note that we have \(a \leqslant 0 \leqslant b\).</p>



<p class="justify-text"><strong>Proof</strong>. We follow the standard proof [9] for real-valued random variables, simply transferred to the matrix case. On the interval \([a,b]\), by convexity of the exponential function $$ \exp(u\lambda) \leqslant \exp(ub) \frac{\lambda-a}{b-a} + \exp(ua) \frac{b \, – \lambda}{b-a}.$$ Thus (check why we can do this), almost surely: $$ \exp(uX) \preccurlyeq \exp(ub) \frac{X-a I }{b-a} + \exp(ua) \frac{bI  – X}{b-a},$$ leading to \(\displaystyle \mathbb{E} [ \exp(uX) ]  \preccurlyeq   \frac{b \exp(ua)\ – a \exp(ub)}{b-a} I\). We can then show that \(\displaystyle \frac{b \exp(ua)\ – a \exp(ub)}{b-a}  \leqslant \exp \Big(  \frac{u^2}{8}(b-a)^2 \Big)\) to conclude the proof. We thus need to show that \(\displaystyle \psi(u) = \log \Big( \frac{b \exp(ua)\ – a \exp(ub)}{b-a}  \Big) \leqslant \frac{u^2}{8}(b-a)^2\) for all \(u \geqslant 0\). We can simply express \(\psi(u)\) as a rescaled logistic function as $$\psi(u) =  ua + \log \frac{b}{b-a} + \log\Big[ 1 + \exp\Big( u(b-a) + \log\frac{-a}{b-a} \Big) \Big].$$ We have \(\psi(0) = 0\), and $$\psi'(u) =  \frac{ba \exp(ua)\ – ab \exp(ub)}{b \exp(ua)\ – a \exp(ub)}, $$ thus \(\psi'(0) = 0\), and through the link with the logistic function, the second-order derivarive is less than \((b-a)^2 / 4\). This leads to the desired result by Taylor’s formula.</p>



<p></p>



<p><br /></p></div>







<p class="date">
by Francis Bach <a href="https://francisbach.com/von-neumann-entropy/"><span class="datestr">at March 07, 2022 09:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2022-03-07-colordag-from-always-almost-to-almost-always-50-percent-selfish-mining-resilience/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2022-03-07-colordag-from-always-almost-to-almost-always-50-percent-selfish-mining-resilience/">Colordag: From always-almost to almost-always 50% selfish mining resilience</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The Selfish mining attack against blockchain protocols was discovered and formalized in 2013 by Eyal and Sirer (also see our blog post). The Bitcoin community has mentioned similar types of attacks in 2010. This attack remains a vulnerability of all operational blockchains we are aware of. For Bitcoin’s blockchain algorithm...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2022-03-07-colordag-from-always-almost-to-almost-always-50-percent-selfish-mining-resilience/"><span class="datestr">at March 07, 2022 03:19 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
