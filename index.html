<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wpcomstaging.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at April 03, 2021 03:39 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8067">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/04/03/natural-language-processing-guest-lecture-by-sasha-rush/">Natural Language Processing (guest lecture by Sasha Rush)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>Scribe notes by Benjamin Basseri and <a href="https://github.com/rxu18">Richard Xu</a></em></p>



<p><strong>Previous post:</strong> <a href="https://windowsontheory.org/2021/04/02/inference-and-statistical-physics/">Inference and statistical physics</a> <strong>Next post:</strong> TBD. See also <a href="https://windowsontheory.org/category/ml-theory-seminar/">all seminar posts</a> and <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">course webpage</a>.</p>



<p><a href="http://rush-nlp.com/">Alexander (Sasha) Rush</a> is a professor at Cornell working in in Deep Learning / NLP. He applies machine learning to problems of text generation, summarizing long documents, and interactions between character and word-based models. Sasha is previously at Harvard, where he taught an awesome NLP class, and we are excited to have him as our guest! (Note: some of the figures in this lecture are taken from other papers or presentations.)</p>



<p>The first half of the talk will focus on how NLP works and what makes it interesting: a bird’s-eye view of the field. The second half of the talk will focus on current research.</p>



<h2>Basics of NLP</h2>



<p>Textual data has many different challenges that differ from computer vision (CV), since it is a human phenomenon. There are methods that work in computer vision / other ML models that just don’t work for NLP (e.g. GANs). As effective methods were found for computer vision around 2009-2014, we thought that these methods would also work well for NLP. While this was sometimes the case, it has not been true in general.</p>



<p>What are the difficulties of working with natural language? Language works at different scales:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">word &lt; phrase &lt; sentence &lt; document &lt; ...
</pre></div>


<p>Here are examples of structure at each level:</p>



<ol><li>Zipf’s Law: The frequency of any word is inversely proportional to its popularity rank.</li><li>Given the last <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> symbols, it is often possible to predict the next one (The Shannon Game).</li><li>Linguists have found many rules about syntax and semantics of a language.</li><li>In a document, we have lots of discourse between different sentences. For example, “it” or other pronouns are context dependent.</li></ol>



<p>In NLP, we will talk about the <em>syntax</em> and <em>semantics</em> of a document. The syntax refers to how words can fit together, and semantics refers to the meaning of these words.</p>



<h2>Language Modeling</h2>



<p>There are many different NLP tasks such as sentiment analysis, question answering, named entity recognition, and translation. However, recent research shows that these tasks are often related to language modeling.</p>



<p>Language modeling, as explained in Shannon 1948, aims to answer the following question: <em>Think of language as a stochastic process producing symbols. Given the last <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> symbols, can we predict the next one?</em></p>



<p>This question is challenging as is. Consider the following example:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">A reading lamp on the desk shed glow on polished ___
</pre></div>


<p>There are many options: marble/desk/stone/engraving/etc., and it is already difficult to give a probability here. In general, language is hard to model because the next word can be connected to words from a long time ago.</p>



<p>Shannon proposes variants of Markov models to perform this prediction, based on the last couple characters or the context in general.</p>



<p>Since local context matters most, we assume that only the <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> most recent words matter. Then, we get the model<br /><img src="https://s0.wp.com/latex.php?latex=p%28w_%7Bt%2B1%7D%7Cw_%7Bt-n%7D%2C%5Cldots%2Cw_t%29%3D%5Cfrac%7Bp%28w_%7Bt-n%7D%2C%5Cldots%2Cw_t%2Cw_%7Bt%2B1%7D%29%7D%7Bp%28w_%7Bt-n%7D%2C%5Cldots%2Cw_t%29%7D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(w_{t+1}|w_{t-n},\ldots,w_t)=\frac{p(w_{t-n},\ldots,w_t,w_{t+1})}{p(w_{t-n},\ldots,w_t)}." class="latex" /></p>



<h3>Measuring Performance</h3>



<p>As <a href="https://windowsontheory.org/2021/02/24/unsupervised-learning-and-generative-models/">we have seen in the generative models lecture</a>, we can use cross entropy as a loss function for density estimation models. Given model density distribution <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" /> and true distribution <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" />, the cross entropy (which equals negative expected log-likelihood) is defined as follows:</p>



<p><img src="https://s0.wp.com/latex.php?latex=H%28p%2C+q%29+%3D+-+E_p+%5Clog+q%28w_t+%7C+w_1%2C+%5Cldots+%2C+w_%7Bt-1%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(p, q) = - E_p \log q(w_t | w_1, \ldots , w_{t-1})" class="latex" /></p>



<p>In NLP we tend to use the metric “perplexity”, which is the exponentiated negative cross entropy:</p>



<p><img src="https://s0.wp.com/latex.php?latex=ppl%28p%2C+q%29+%3D+%5Cexp+-H%28p%2C+q%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="ppl(p, q) = \exp -H(p, q)." class="latex" /></p>



<p>This corresponds to the equivalent vocabulary size of a uniformly distributed model. Lower perplexity means our model was closer to the underlying text distribution. As an example, the perplexity of the perfect dice-roll model would be 6.</p>



<p>Why do we care about perplexity anyway?</p>



<ol><li>With a good model we can determine the natural perplexity of a language, which is interesting.</li><li>Many NLP questions are language modeling with conditioning. Speech recognition is language modeling conditioned on some sound signal, and translation is language modeling conditioned on text from another language.</li><li>More importantly, we have found recent applications in <em>transfer learning</em>. That is, a language model can be trained on some (small) input data for a specific task. Then, such a model becomes effective at the given task!</li></ol>



<figure class="wp-block-image"><img src="https://i.imgur.com/euYVjaG.png" alt="Transferring" /></figure>



<p>A few years ago, the best perplexity on WSJ text was 150. Nowadays, it is about 20! To understand how we got here, we look at modern language modeling.</p>



<h2>Predicting the next word</h2>



<p>We start with the model</p>



<p><img src="https://s0.wp.com/latex.php?latex=p%28w_t+%7C+w_%7B1%3At-1%7D%3B+%5Ctheta%29+%3D+softmax%28%5Cmathbf%7BW%7D%5Cphi%28w_%7B1%3At-1%7D%3B+%5Ctheta%29%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(w_t | w_{1:t-1}; \theta) = softmax(\mathbf{W}\phi(w_{1:t-1}; \theta))." class="latex" /></p>



<p>(The softmax function maps a vector <img src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bv%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\vec{v}" class="latex" /> into the probability distribution <img src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bp%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\vec{p}" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=p_i+%5Cpropto+%5Cexp%28v_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_i \propto \exp(v_i)" class="latex" />. That is, <img src="https://s0.wp.com/latex.php?latex=p_i+%3D+%5Cexp%28v_i%29%2F%5Csum_j+%5Cexp%28v_j%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_i = \exp(v_i)/\sum_j \exp(v_j)" class="latex" />. Note that this is a Boltzman distribution of the type we saw in the <a href="https://windowsontheory.org/2021/04/02/inference-and-statistical-physics/">statistical physics and variational algorithms</a> lecture)</p>



<p>We call <strong>W</strong> the output word embeddings, <img src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi" class="latex" /> is some neural basis (e.g. <img src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi" class="latex" /> are all but the final layers of a neural net with weights <img src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\theta" class="latex" />, and <strong>W</strong> is the final layer). However, this means to use softmax to predict requires computing softmax over every word in your vocabulary (tens or hundreds of thousands). This was often infeasible until GPU computing became available.</p>



<p>As an aside, why not predict characters instead of words? The advantage is that there are much fewer characters than words. However, computation with characters is slower. Empirically, character-based models tend to perform worse than word-based. However, character-based models can handle words outside the vocabulary.</p>



<p><a href="https://en.wikipedia.org/wiki/Byte_pair_encoding">Byte-pair encoding</a> offers a bridge between character and word models. This greedily builds up new tokens as repetitive patterns are found in the original text.</p>



<p>In the last decade NLP has seen a few dominant architectures, all using SGD but with varying bases. First, we must cast the words as one-hot vectors, then embed them into vector space:<br /><img src="https://s0.wp.com/latex.php?latex=x_t+%3D+Vw_t%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_t = Vw_t," class="latex" /><br />where <img src="https://s0.wp.com/latex.php?latex=w_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_t" class="latex" /> is the one-hot encoded vector and <img src="https://s0.wp.com/latex.php?latex=V&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="V" class="latex" /> is the learned embedding transformation.</p>



<h3>NNLM</h3>



<p>NNLM (Neural Network Language Model) is like a CNN. The model predicts on possibly multiple NN transformations:<br /><img src="https://s0.wp.com/latex.php?latex=%5Cphi%28w_%7B1%3At-1%7D%3B+%5Ctheta%29+%3D+%5Csigma%28U%5Bx_%7Bt-k-1%7D+%5Coplus+%5Cldots+%5Coplus+x_%7Bt-1%7D%5D%5D%29%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi(w_{1:t-1}; \theta) = \sigma(U[x_{t-k-1} \oplus \ldots \oplus x_{t-1}]])," class="latex" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=%5Coplus&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\oplus" class="latex" /> denotes concatenation, <img src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="U" class="latex" /> is some convolutional filter and <img src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sigma" class="latex" /> is the activation function. This has the benefit of learning fast. The matrices it learns also transfer well.</p>



<p>As an example, GloVe is a NNLM-inspired model. It stores the words in 300-dimensional space. When we project the some words to 2-dimensions using PCA, we find semantic information in the language model.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/wesRAtM.png" alt="Language structure" /></figure>



<h3>Recurrent Models</h3>



<p>A recurrent model uses a fixed set of previous words to predict the next word. A recurrent network uses all previous words:<br /><img src="https://s0.wp.com/latex.php?latex=%5Cphi%28w_%7B1%3At-1%7D%3B+%5Ctheta%29+%3D+%5Csigma%28U%5Bx_%7Bt-1%7D%5Coplus+%5Cphi%28w_%7B1%3At-2%7D%3B%5Ctheta%29%5D%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi(w_{1:t-1}; \theta) = \sigma(U[x_{t-1}\oplus \phi(w_{1:t-2};\theta)])." class="latex" /></p>



<p>Previous information is ‘summarized’ in the <img src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi" class="latex" /> on the right, so this model uses finite memory. Below is an illustration of the recurrent neural network.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/iVl2dqZ.png" alt="RNN Explanation" /></figure>



<p>Since the recurrent model uses the full context, it is a more plausible model for how we really process language. Furthermore, the introduction of RNN saw drastically improved performance. In the graph below, the items in the chart are performances from previous NNLMs. The recurrent network performance is “off the chart”.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/v5rgNeQ.png" alt="RNN Performance" /></figure>



<p>However, the model grows with sequence length. This requires gradient flow to backpropagate over arbitrarily long sequences, and often required baroque network designs to facilitate longer sequences while avoiding exploding/vanishing gradients.</p>



<h3>Attentional Models</h3>



<p>To understand modern NLP we must look at <a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)">attention</a>. For a set of vectors <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" />, keys <img src="https://s0.wp.com/latex.php?latex=k_1%2C+%5Cldots%2C+k_T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k_1, \ldots, k_T" class="latex" /> and a query <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" />, we define attention as</p>



<p><img src="https://s0.wp.com/latex.php?latex=Att%28q%2C+k%2C+v%29+%3D+%5Csum_t+%5Calpha_t+v_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Att(q, k, v) = \sum_t \alpha_t v_t" class="latex" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+softmax%28q%5Ccdot+k_1+%5Coplus+%5Cldots+%5Coplus+q+%5Ccdot+k_T%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha = softmax(q\cdot k_1 \oplus \ldots \oplus q \cdot k_T)" class="latex" />.</p>



<p>Here, <img src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha" class="latex" /> can be considered a probability density of the model’s ‘memory’ of the sequence. The model decides which words are important by combining <img src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" />.</p>



<p>The <em>attentional model</em> can be fully autoregressive (use all previously seen words), and the query <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" /> can be learned or be specific to an input. In general, we have:<br /><img src="https://s0.wp.com/latex.php?latex=%5Cphi%28w_%7B1%3At-1%7D%3B+%5Ctheta%29+%3D+%5Csigma%28U%5BAtt%28q%2C+x_%7B1%3At-1%7D%2C+x_%7B1%3At-1%7D%29%5D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi(w_{1:t-1}; \theta) = \sigma(U[Att(q, x_{1:t-1}, x_{1:t-1})])" class="latex" /></p>



<p>Since we condense all previous information into the attention mechanism, it is simpler to backpropagate.<br />In particular attention enables looking at information from a large context without paying for this in the depth of the network and hence in the depth of back-propagation you need to cover. (Boaz’s note: With my crypto background, attention reminds me of the design of <a href="https://en.wikipedia.org/wiki/Block_cipher">block ciphers</a>, which use linear operations to mix between far away parts of the inputs, and then apply non-liearity locally to each small parts.)</p>



<p>Note that attention is defined with respect to a set of vectors. There is no idea of positional information in the attentional model. How do we encode positional information for the model? One way to do this is using <em>sinusoidal encoding</em> in the keys. We store the word <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> as <img src="https://s0.wp.com/latex.php?latex=%5Ccos%28%5Cpi+n%2Fk%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\cos(\pi n/k)" class="latex" /> for some period <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k" class="latex" />. Notice that if we choose many different periods, then the cosine ways will almost never meet at the same point. As a result, only recent points will have high dot products between the different cosine values.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/kAnYjAP.png" alt="Sinusoidal" /></figure>



<h3>Transformers</h3>



<p>A transformer is a stacked attention model. Computation in one layer becomes query, keys and values for the next layer. This is a multiheaded attention model. We learn <img src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H" class="latex" /> projections for each query/key (generally between 8 and 32) then do softmax across these projections:<br /><img src="https://s0.wp.com/latex.php?latex=%5Calpha%5Eh+%3D+softmax%28%28W%5E%7B%28h%29%7Dq%29%5Ccdot%28V%5E%7B%28h%29%7Dk_1%29%5Coplus+%5Cldots+%5Coplus+%28W%5E%7B%28h%29%7Dq%29%5Ccdot%28V%5E%7B%28h%29%7Dk_T%29%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha^h = softmax((W^{(h)}q)\cdot(V^{(h)}k_1)\oplus \ldots \oplus (W^{(h)}q)\cdot(V^{(h)}k_T))." class="latex" /></p>



<figure class="wp-block-image"><img src="https://i.imgur.com/uUp2elD.png" alt="Transformer Architecture" /></figure>



<p>These heads can be computed in parallel and can be implemented with batch matrix multiplication. As a result, transformers can be massively scaled and are extremely efficient in modern hardware. This has led these models to be very dominant in the field. Here are some example models:</p>



<ol><li>GPT-1,2,3 are able to generate text that is quite convincing to a human. They also handle the syntax and semantics of a language quite well.</li><li>BERT is a transformer-based model that examines text both forwards and backwards in making its predictions. It works well with transfer fining tuning: train on a large data set, then take the feature representations and train a task on top of the learned representation.</li></ol>



<h2>Scaling</h2>



<p>In recent years we have had larger and larger models, from GPT1’s 110 million to GPT3’s 175 billion.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/8drDknV.png" alt="Scaling Up" /></figure>



<p>On these massive scales, scaling has become a very interesting issue: how do we process more samples? How do we run distributed computation? How much autoregressive input should each model looks at? (Boaz’s note: To get a sense of how big these models are, GPT-3 was trained on about <a href="https://lambdalabs.com/blog/demystifying-gpt-3/">1000B tokens</a>. The total <a href="https://www.prb.org/howmanypeoplehaveeverlivedonearth/">number of people that ever lived</a> is about 100B, and only about half since the invention of the printing press, so this is arguably a non-negligible fraction of all text produced in human history.)</p>



<p>For a model like BERT, most of the cost still comes from feed-forward network — mostly matrix multiplications. These are tasks we are familiar with and can scale up.</p>



<p>One question is to have long-range attentional lookup, which is important for language modeling. For now, models often look at most 512 words back. Can we do longer range lookups? One approach to this is kernel feature attention.</p>



<h3>Kernel Feature Attention</h3>



<p>Recall that we have <img src="https://s0.wp.com/latex.php?latex=%5Calpha%3D%5Cmathrm%7Bsoftmax%7D%28q%5Ccdot+k_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha=\mathrm{softmax}(q\cdot k_i)" class="latex" />. Can we approximate this with some kernel <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K" class="latex" />? The main approach is that <img src="https://s0.wp.com/latex.php?latex=%5Calpha%5Cpropto+%5Cexp%28q%5Ccdot+k_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha\propto \exp(q\cdot k_i)" class="latex" />, which we approximate with the kernel <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28v_1%5Ccdot+v_2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\exp(v_1\cdot v_2)" class="latex" />. There is a rich literature on approximating <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K" class="latex" /> where<br /><img src="https://s0.wp.com/latex.php?latex=K%28v_1%2C+v_2%29+%5Capprox+%5Cphi%28v_1%29%5Ccdot%5Cphi%28v_2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K(v_1, v_2) \approx \phi(v_1)\cdot\phi(v_2)" class="latex" /></p>



<p>for some transfomration <img src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\phi" class="latex" />. Then, we can try to approximate <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K" class="latex" /> with linear features.</p>



<p>Practically, transformers do well but are slower. For longer texts, we have faster models that do slightly worse. A recent model called <a href="https://arxiv.org/abs/2009.14794">performer</a> is such an example.<br /><img src="https://i.imgur.com/S3vs2o5.png" alt="LRA Performance" /></p>



<h3>Scaling Down</h3>



<p>Ultimately, we want to make models run on “non Google scale” hardware once it has been trained to a specific task. This can often require scaling down.</p>



<p>One approach is to prune weights according to their magnitude. However, since models are often overparameterized and weights do not move much, the weights that get pruned according to this method are usually the weights that were simply initialized closest to 0. In the diagram below, we can consider only leaving the orange weights and cutting out the gray.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/MqY5jK2.png" alt="Pruning" /></figure>



<p>Another approach is to mask out the weights that are unnecessary for a specific task, if you’re trying to ship a model for a specific task.</p>



<h2>Research Questions</h2>



<p>Two major lines of research dominate NLP today:</p>



<ol><li>Attention / Transformer architecture,</li><li>Pretraining with language modeling as a deep learning paradigm.</li></ol>



<p>We are also in a space race to produce efficient models with more parameters, given how much scaling has been effective.</p>



<p>The paper <a href="https://arxiv.org/abs/2010.00854">Which BERT</a> classifies modern questions in NLP into the following categories:</p>



<ol><li><strong>Tasks:</strong> Is (masked) language modeling the best task for pretraining? Language modeling emphasizes local information. We could imagine doing other types of denoising. See also <a href="https://openai.com/blog/dall-e/">DALL-E</a>.</li><li><strong>Efficiency:</strong> We see that much fewer parameters are needed in practice after pruning. Does pruning lose something? Pruned models tend to do well on in-sample data, but out-of-sample data tends to make the pruned model do worse.</li><li><strong>Data:</strong> How does data used in pretraining impact task accuracy? How does the data of pretraining impact task bias?</li><li><strong>Interpretability:</strong> What does BERT know, and how can we determine this? Does interpretability need to come at a cost to performance?</li><li><strong>Multilinguality:</strong> Many languages don’t have the same amount of data as English. What methods apply when we have less data?</li></ol>



<h2>Q&amp;A</h2>



<p>We have many questions asked during and after lecture. Here are some of the questions.</p>



<ol><li><strong>Q:</strong> Should we say GANs fail at NLP or that other generative models are more advanced in NLP than in CV? <strong>A:</strong> One argument is that language is a human generated system, there are some inherent structures that help with generation. We can do language in left-to-right, but in CV this would be a lot more difficult. At the same time, this can change in the future!</li><li><strong>Q:</strong> Why are computer vision and NLP somewhat close to each other? <strong>A:</strong> classically, they are both perception-style tasks under AI. Also, around 2014 we had lots of ideas that come from porting CV ideas into NLP, and recently we have seen NLP ideas ported to CV.</li><li><strong>Q:</strong> Since languages have unique grammars, is NLP better at some languages? Do we have to translate language to an “NLP-effective” language and back? <strong>A:</strong> In the past, some languages are better. Ex: we used to struggle with Japanese to other languages but do well with English to other languages. However, modern models are <em>extremely</em> data driven, so we have needed much less hardcoding.</li><li><strong>Q:</strong> Have we done any scatter plot of the form (data available for language X, performance on X) to see if performance is just a function of available data? <strong>A:</strong> Not right now, but these plots can potentially be really cool! Multilinguality is a broad area of research in general.</li><li><strong>Q:</strong> What are some NLP techniques for low-resource languages? <strong>A:</strong> Bridging is commonly used. Iterative models (translate and translate back with some consistency) is also used to augment the data.</li><li><strong>Q:</strong> Do you think old-school parsers will make a comeback? <strong>A:</strong> Unlikely to deploy parsers, but the techniques of parsing is interesting.</li><li><strong>Q:</strong> Given the large number of possible “correct” answers, has there been work on which “contexts” are most informative? <strong>A:</strong> Best context is the closest context, which is expected. The other words matter but matter a lot less.</li><li><strong>Q:</strong> Is there any model that captures the global structure first (e.g. an outline) before writing sequentially, like humans do when they write longer texts? <strong>A:</strong> Currently no. Should be possible, but we do not have data about the global structure of writing.</li><li><strong>Q:</strong> Why is our goal density estimation? <strong>A:</strong> It is useful because it tells us how “surprising” the next word is. This is also related to why a language feels “fast” when you first learn it: because you are not familiar with the words, you cannot anticipate the next words.</li><li><strong>Q:</strong> Why is lower perplexity better? <strong>A:</strong> Recall from past talk that lower cross-entropy means less distance between <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" />, and intuitive you have more “certainty”.</li><li><strong>Q:</strong> Is the reason LM so important because evaluations are syntax-focused? <strong>A:</strong> Evaluations are actually more semantically focused, but syntax and semantics are quite connected.</li><li><strong>Q:</strong> Do we see attentional models in CV? <strong>A:</strong> Yes, we have seen more use of transformer in CV. In a standard model we use data only recently, and here we get to work with data across space and time. As such, we will need to encode time positionally.</li><li><strong>Q:</strong> Why is attention generalized convolution? <strong>A:</strong> If you have attention with all mass in the local area, that’s probably like convolution.</li><li><strong>Q:</strong> How do we compare heads with depth? e.g. Is having 5 heads better than 5x depth? <strong>A:</strong> when we use heads we add a lot less parameters. As such, we can parallelize heads and increase performance.</li><li><strong>Q:</strong> Do you think transformers are the be-all end-all of NLP models? <strong>A:</strong> Maybe. To dethrone transformers, you have to both show similar work on small-scale and show that it can be scaled easily.</li><li><strong>Q:</strong> How does simplicity bias affect these transferrable models? <strong>A:</strong> surprising and we are not sure. In CV we found that the models quickly notice peculiarities in the data (e.g. how mechanical turks are grouped), but the models do work.</li><li><strong>Q:</strong> We get bigger models every year and better performance. Will this end soon? <strong>A:</strong> Probably not, as it seems like having more parameters helps it recognize some additional features.</li><li><strong>Q:</strong> If we prune models to the same size, will they have the same performance? <strong>A:</strong> for small models we can seem to prune them, but for the bigger models it is hard to run them in academica given the computational resource constraints.</li><li><strong>Q:</strong> When we try to remember something from a long time ago we would look up a textbook / etc. Have we had similar approaches in practice? <strong>A:</strong> transformer training is static at first, and tasks happen later. So, we have to decide how to throw away information before we train on the tasks.</li><li><strong>Q:</strong> Are better evaluation metrics an important direction for future research? <strong>A:</strong> Yes — this has been the case for the past few years in academia.</li><li><strong>Q:</strong> What is a benchmark/task where you think current models show deep lack of capability? <strong>A:</strong> During generation, models don’t seem to distinguish between information that makes it “sound good” and factually correct information.</li></ol></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/04/03/natural-language-processing-guest-lecture-by-sasha-rush/"><span class="datestr">at April 03, 2021 02:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/04/02/islands">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/04/02/islands.html">Islands</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In the neighborhood where I live, fire safety regulations require the streets to be super-wide (so wide that two fire trucks can pass even with cars parked along both sides of the street), and to have even wider turnarounds at the ends of the culs-de-sac. To break up the resulting vast expanses of pavement, we have occasional islands of green, public gardens too small to name as a park. They come in several different types: medians to separate the incoming and outgoing lanes at junctions with larger roads,</p>

<p style="text-align: center;"><img src="https://www.ics.uci.edu/~eppstein/pix/islands/Frost-m.jpg" style="border-style: solid; border-color: black;" alt="Traffic island at Frost and Gabrielino, Irvine" /></p>

<p>barriers to separate small groups of houses from the main flow of the road,</p>

<p style="text-align: center;"><img src="https://www.ics.uci.edu/~eppstein/pix/islands/LosTrancos-m.jpg" style="border-style: solid; border-color: black;" alt="Traffic island on Los Trancos Drive, Irvine" /></p>

<p>or giving some shape to the turnaround at the end of a cul-de-sac.</p>

<p style="text-align: center;"><img src="https://www.ics.uci.edu/~eppstein/pix/islands/Harvey-m.jpg" style="border-style: solid; border-color: black;" alt="Traffic island on Harvey Court, Irvine" /></p>

<p>Most are ignored except by the community association’s gardeners and by passing cars, but I did find one set up as a neighborhood basketball court:</p>

<p style="text-align: center;"><img src="https://www.ics.uci.edu/~eppstein/pix/islands/Perkins-m.jpg" style="border-style: solid; border-color: black;" alt="Traffic island on Perkins Court, Irvine" /></p>

<p><a href="https://www.ics.uci.edu/~eppstein/pix/islands/">The rest of the gallery</a>.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/105998376302792384">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/04/02/islands.html"><span class="datestr">at April 02, 2021 04:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8054">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/04/02/inference-and-statistical-physics/">Inference and statistical physics</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>Scribe notes by Franklyn Wang</em></p>



<p><strong>Previous post:</strong> <a href="https://windowsontheory.org/2021/04/01/robustness-in-train-and-test-time/">Robustness in train and test time</a> <strong>Next post:</strong> <a href="https://windowsontheory.org/2021/04/03/natural-language-processing-guest-lecture-by-sasha-rush/">Natural Language Processing (guest lecture by Sasha Rush)</a>. See also <a href="https://windowsontheory.org/category/ml-theory-seminar/">all seminar posts</a> and <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">course webpage</a>.</p>



<p><a href="http://files.boazbarak.org/misc/mltheory/ML_seminar_lecture_5.pdf">lecture slides (pdf)</a> – <a href="http://files.boazbarak.org/misc/mltheory/ML_seminar_lecture_5.pptx">lecture slides (Powerpoint with animation and annotation)</a> – <a href="https://harvard.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=794333ec-0897-48ea-9bc5-ace600facb9f">video</a></p>



<h2>Digression: Frequentism vs Bayesianism</h2>



<p>Before getting started, we’ll discuss the difference between the two dominant schools of thought in probability: Frequentism and Bayesianism.</p>



<p>Frequentism holds that the probability of an event is the long-run average proportion of something that happens many, many times, so a coin has 50% probability of being heads if on average half of the flips are heads. One consequence of this framework is that a one-off event like Biden winning the election doesn’t have any probability as by definition they can only be observed once.</p>



<p>Bayesians reject this model of the world. For example, a famous Bayesian, Jaynes, even wrote that “probabilities are frequencies only in an imaginary universe.”</p>



<p>While these branches of thought are different, generally the answers to most questions are the same, so the distinction will not matter for this class. However, these branches of thought inspire different types of methods, which we now discuss.</p>



<p>For example, suppose that we have a probability distribution <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" /> that we can get samples from in the form of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" />. In statistics, our goal is to calculate a hypothesis <img src="https://s0.wp.com/latex.php?latex=h+%5Cin+%5Cmathcal%7BH%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h \in \mathcal{H}" class="latex" /> which minimizes <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D_%7BD%7D%28h%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{L}_{D}(h)" class="latex" /> (possibly a loss function, but any minimand will do) where we estimate <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D_D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{L}_D" class="latex" /> with our samples <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" />.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/Wvujokc.png" alt="" /></figure>



<p>A <strong>frequentist</strong> does this by defining a family <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{D}" class="latex" /> of potential distributions, and we need to find a transformation <img src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bx%7D+%5Cmapsto+h%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\vec{x} \mapsto h(\vec{x})" class="latex" /> which minimizes the cost for all <img src="https://s0.wp.com/latex.php?latex=D+%5Cin+%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D \in \mathcal{D}" class="latex" />, where</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/CHDrLRW.png" alt="" /></figure>



<p>These equations amount to saying that <img src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h" class="latex" /> is minimizing the worst-case loss over all distributions.</p>



<p>By contrast, a <strong>Bayesian</strong> approaches this task by assuming that <img src="https://s0.wp.com/latex.php?latex=D+%3D+D_%7B%5Cvec%7Bw%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D = D_{\vec{w}}" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bw%7D+%5Csim+P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\vec{w} \sim P" class="latex" /> are latent variables sampled from a prior.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/GDhqJTT.png" alt="" /></figure>



<p>Then, let <img src="https://s0.wp.com/latex.php?latex=Q_%7B%5Cvec%7Bx%7D%7D%28%5Cvec%7Bw%7D%29+%3D+P%28w+%7C+D_%7B%5Cvec%7Bw%7D%7D+%3D+x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q_{\vec{x}}(\vec{w}) = P(w | D_{\vec{w}} = x)" class="latex" /> be the <em>posterior</em> distribution of <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" /> conditioned on the observations <img src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bx%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\vec{x}" class="latex" />. We now minimize</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/dNTPavw.png" alt="" /></figure>



<p>In fact, Bayesian approaches extend beyond this, as if you can sample from a posterior you can do more than just minimize a loss function.</p>



<p>In Bayesian inference, chosing the prior <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P" class="latex" /> is often very important. One classical approach is a <em>maximum entropy prior</em>, because it’s the least informative (hence, the most entropy) and requires the fewest assumptions.</p>



<p>These two minimization equations con sometimes lead to identical results, but often in practice they work out differently once we introduce <strong>computational constraints</strong> into the picture. In the frequentist approach, we generally constrain the family of mappings <img src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bx%7D+%5Crightarrow+%5Cvec%7Bh%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\vec{x} \rightarrow \vec{h}" class="latex" /> to be efficiently computable. In the Bayesian approach, we typically approximate the posterior instead and either use approximate sampling or a restricted hypothesis class for the posterior to be able to efficiently sample from it.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/KgOrLqR.png" alt="" /></figure>



<h2>Part 1. An introduction to Statistical Physics</h2>



<figure class="wp-block-image"><img src="https://i.imgur.com/FOIcEVs.jpg" alt="" /></figure>



<p>Compare water and ice. Water is hotter, and the molecules move around more. In ice, by contrast, the molecules are more stationary. When the temperature increases, the objects move more quickly, and when they decrease the objects have less energy and stop moving. There are also phase transitions, where certain temperatures cause qualitative discontinuities in behavior, like solid to liquid or liquid to gas.<br /></p>



<p>See video from <a href="https://phet.colorado.edu/">here</a>:</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2021/04/ezgif.com-gif-maker-1.gif"><img src="https://windowsontheory.files.wordpress.com/2021/04/ezgif.com-gif-maker-1.gif?w=702" alt="" class="wp-image-8064" /></a></figure>



<p>An atomic state can be thought of as <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%7B0%2C+1%7D%5E%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x \in {0, 1}^{n}" class="latex" />. Now, how do we represent the system’s state? The crucial observation that makes statistical physics different from “vanilla physics” is the insight to represent the system state as a probability distribution <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" /> supported on <img src="https://s0.wp.com/latex.php?latex=%7B0%2C+1%7D%5E%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{0, 1}^{n}" class="latex" />, rather than in a single state.</p>



<p>Each atomic state has a negative energy function (which we will think of as a utility function) <img src="https://s0.wp.com/latex.php?latex=W%3A+%7B0%2C+1%7D%5E%7Bn%7D+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="W: {0, 1}^{n} \rightarrow \mathbb{R}" class="latex" />. In some sense, the system “wants” to have a high value of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bx+%5Csim+p%7D%5B+W%28x%29%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{x \sim p}[ W(x)]" class="latex" />. In addition, when the temperature is high, the system “wants” to have a high value of entropy.</p>



<p>Thus, an axiom of thermodynamics states that to find the true distribution, we need only look at the maximizer of</p>



<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%5Carg%5Cmax_%7Bq%7D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+q%7D%5BW%28x%29%5D+%2B+%5Ctau+%5Ccdot+H%28q%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p = \arg\max_{q} \mathbb{E}_{x \sim q}[W(x)] + \tau \cdot H(q)." class="latex" /></p>



<p>That is, it is the probability distribution which maximizes a linear combination of the expected negative energy and the entropy, with the temperature controlling the coefficient of entropy in this combination (the higher the temperature, the more the system prioritizes having high entropy).</p>



<p>The <strong>variational principle</strong>, which we prove later, states that the <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" /> which maximizes this satisfies</p>



<p><img src="https://s0.wp.com/latex.php?latex=p%28x%29+%5Cpropto+%5Cexp%28%5Ctau%5E%7B-1%7D+%5Ccdot+W%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(x) \propto \exp(\tau^{-1} \cdot W(x))" class="latex" /></p>



<p>which is known as the <strong>Boltzmann Distribution</strong>. We often write this with a normalizing constant, so <img src="https://s0.wp.com/latex.php?latex=p%28x%29+%3D+%5Cexp%28%5Ctau%5E%7B-1%7D+%5Ccdot+W%28x%29+-+A_%7B%5Ctau%7D%28W%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(x) = \exp(\tau^{-1} \cdot W(x) - A_{\tau}(W))" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=A_%7B%5Ctau%7D%28W%29+%3D+%5Clog+%28%5Cint+%5Cexp%28%5Ctau%5E%7B1%7D+%5Ccdot+W%28x%29%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A_{\tau}(W) = \log (\int \exp(\tau^{1} \cdot W(x)))" class="latex" />.</p>



<p>Before proving the variational principle, we will go through some examples of statistical physics.</p>



<h3>Example: Ising Model</h3>



<p>In the Ising model, we have <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> magnets that are connected in a square grid. The atomic state is each <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%7B%5Cpm+1%7D%5E%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x \in {\pm 1}^{n}" class="latex" />, which represents “spin”. The value of <img src="https://s0.wp.com/latex.php?latex=W%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="W(x)" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=J%5Csum_%7Bi+%5Csim+j%7D+x_i+x_j+%2B+J%27%5Csum_%7Bi%7D+x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="J\sum_{i \sim j} x_i x_j + J'\sum_{i} x_i" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=i+%5Csim+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i \sim j" class="latex" /> denotes that <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="j" class="latex" /> are adjacent magnets. This encourages adjacent magnets to have aligned spins. One important concept, which we will return to later, is that the values of <img src="https://s0.wp.com/latex.php?latex=%7Bx_i%2C+x_ix_j%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{x_i, x_ix_j}" class="latex" /> are sufficient to calculate the value of <img src="https://s0.wp.com/latex.php?latex=W%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="W(x)" class="latex" /> (these are known as <em>sufficient statistics</em>). Furthermore, if we wanted to calculate <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BW%28x%29%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}[W(x)]" class="latex" />, it would be enough to calculate the values of <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Bx_i%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}[x_i]" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Bx_i+x_j%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}[x_i x_j]" class="latex" /> and then apply linearity of expectation. An illustration of an Ising model that we cool down slowly can be found here:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/VG1ION8.jpg" alt="" /></figure>



<p>and a video can be found <a href="https://www.youtube.com/watch?v=kjwKgpQ-l1s">here</a>.</p>



<p>This is what a low-temperature Ising model looks like – note that <img src="https://s0.wp.com/latex.php?latex=W%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="W(x)" class="latex" /> is high because almost all adjacent spins are aligned (hence the well-defined regions).</p>



<p>The <a href="https://arxiv.org/abs/1211.1094">Sherrington-Kirkpatrick model</a> is a generalization of the Ising model to a random graph, which represents a disordered mean field model. Here, <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%7B%5Cpm+1%7D%5E%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x \in {\pm 1}^{n}" class="latex" /> still represents spin, and <img src="https://s0.wp.com/latex.php?latex=W%28x%29+%3D+%5Csum+w_%7Bi%2C+j%7D+x_i+x_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="W(x) = \sum w_{i, j} x_i x_j" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=w_%7Bi%2C+j%7D+%5Csim+%5Cmathcal%7BN%7D%280%2C+1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w_{i, j} \sim \mathcal{N}(0, 1)" class="latex" />. The SK-model is deeply influential in statistical physics. We say that it is <em>disordered</em> because the utility function <img src="https://s0.wp.com/latex.php?latex=W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="W" class="latex" /> is chosen at random and that it is a <em>mean field</em> model because, unlike the Ising model, there is no geometry in the sense that every pair of individual variables <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_i" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=x_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_j" class="latex" /> are equally likely to be connected.</p>



<p>Our third example is the posterior distribution, where <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> is a hidden variable with a uniform prior. We now makes, say, <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k" class="latex" /> independent observations <img src="https://s0.wp.com/latex.php?latex=O_1%2C+O_2%2C+%5Cldots+O_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="O_1, O_2, \ldots O_k" class="latex" />. The probability of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> is given by</p>



<p><img src="https://s0.wp.com/latex.php?latex=p%28x%29+%5Cpropto+p%28x%5Ctext%7B+satisfies+%7D+O_1%29+%5Ccdot+p%28x%5Ctext%7B+satisfies+%7D+O_2%29+%5Cldots+p%28x%5Ctext%7B+satisfies+%7D+O_k%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(x) \propto p(x\text{ satisfies } O_1) \cdot p(x\text{ satisfies } O_2) \ldots p(x\text{ satisfies } O_k)" class="latex" /></p>



<p>so <img src="https://s0.wp.com/latex.php?latex=p%28x%29+%5Cpropto+%5Cexp%28%5Clog+%5Csum+%5Cmathbb%7BP%7D%28x%5Ctext%7B+satisfies+%7D+O_i%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(x) \propto \exp(\log \sum \mathbb{P}(x\text{ satisfies } O_i))" class="latex" />. Note that the RHS is easy to calculate, but in practice the normalizing factor (also known as the partition function) can be difficult to calculate and often represents a large barrier, in the sense that computing the partition function makes many of these questions far easier.</p>



<h3>Proof of the Variational Principle</h3>



<p>Now, we prove the variational principle, which states that if <img src="https://s0.wp.com/latex.php?latex=p%28x%29+%3D+%5Cexp%28%5Ctau%5E%7B-1%7D+%5Ccdot+W%28x%29+-+A_%5Ctau%28W%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(x) = \exp(\tau^{-1} \cdot W(x) - A_\tau(W))" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=A_%5Ctau%28W%29+%3D+%5Clog+%5Cint+%5Cexp%28%5Ctau%5E%7B-1%7D+%5Ccdot+W%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A_\tau(W) = \log \int \exp(\tau^{-1} \cdot W(x))" class="latex" /> is the normalizing factor, then</p>



<p><img src="https://s0.wp.com/latex.php?latex=p+%3D+%5Carg%5Cmax_%7Bq%7D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+q%7D+W%28x%29+%2B+%5Ctau+%5Ccdot+H%28q%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p = \arg\max_{q} \mathbb{E}_{x \sim q} W(x) + \tau \cdot H(q)" class="latex" /></p>



<p>Before proving the variational principle, we make the following claim: if <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" /> is defined as <img src="https://s0.wp.com/latex.php?latex=p%28x%29+%3D+%5Cexp%28%5Ctau%5E%7B-1%7D+%5Ccdot+W%28x%29+-+A_%5Ctau%28W%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(x) = \exp(\tau^{-1} \cdot W(x) - A_\tau(W))" class="latex" /> then</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/yACUUCV.png" alt="" /></figure>



<p><strong>Proof of claim:</strong><br />Write<br /><img src="https://s0.wp.com/latex.php?latex=H%28p%29+%3D+-%5Cint+%5Clog+p%28x%29+p%28x%29+dx+%3D+-%5Cint+%28%5Ctau%5E%7B-1%7D+%5Ccdot+W%28x%29+-+A_%7B%5Ctau%7D%28W%29%29+p%28x%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(p) = -\int \log p(x) p(x) dx = -\int (\tau^{-1} \cdot W(x) - A_{\tau}(W)) p(x) dx" class="latex" /><br />(where we plugged in the definition of <img src="https://s0.wp.com/latex.php?latex=p%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(x)" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%5Clog+p%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log p(x)" class="latex" />).</p>



<p>We can now rewrite this as</p>



<p><img src="https://s0.wp.com/latex.php?latex=H%28p%29+%3D+-%5Ctau%5E%7B-1%7D+%5Ccdot+%5Cmathbb%7BE%7D_%7Bx+%5Csim+p%7D+W%28x%29+%2B+%5Cmathbb%7BE%7D_%7Bx+%5Csim+p%7D+A_%7B%5Ctau%7D%28W%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(p) = -\tau^{-1} \cdot \mathbb{E}_{x \sim p} W(x) + \mathbb{E}_{x \sim p} A_{\tau}(W)" class="latex" /></p>



<p>which means that</p>



<p><img src="https://s0.wp.com/latex.php?latex=H%28p%29+%3D+-%5Ctau%5E%7B-1%7D+%5Ccdot+%5Cmathbb%7BE%7D_%7Bx+%5Csim+p%7D+W%28x%29+%2B+A%7B%5Ctau%7D%28W%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(p) = -\tau^{-1} \cdot \mathbb{E}_{x \sim p} W(x) + A{\tau}(W)" class="latex" /></p>



<p>using the fact that <img src="https://s0.wp.com/latex.php?latex=A_%5Ctau%28W%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A_\tau(W)" class="latex" /> is a constant (independent of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" />). Multiplying by <img src="https://s0.wp.com/latex.php?latex=%5Ctau&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tau" class="latex" /> and rearranging, we obtain the claim. <img src="https://s0.wp.com/latex.php?latex=%5Cblacksquare&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\blacksquare" class="latex" /></p>



<p><strong>Proof of variational principle:</strong><br />Given the claim, we can now prove the variational principle.<br />Let <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" /> be any distribution. We write</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/KVij2G2.png" alt="" /></figure>



<p>where the first equation uses our likelihood expression. This implies that <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bx+%5Csim+q%7D+W%28x%29+%2B+%5Ctau+%5Ccdot+H%28q%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{x \sim q} W(x) + \tau \cdot H(q)" class="latex" /> is maximized at <img src="https://s0.wp.com/latex.php?latex=q+%3D+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q = p" class="latex" />, as desired. <img src="https://s0.wp.com/latex.php?latex=%5Cblacksquare&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\blacksquare" class="latex" /></p>



<p><strong>Remarks:</strong></p>



<ul><li>When <img src="https://s0.wp.com/latex.php?latex=%5Ctau+%3D+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tau = 1" class="latex" />, we have <img src="https://s0.wp.com/latex.php?latex=A%28W%29+%3D+%5Cmax_%7Bq%7D+H%28q%29+%2B+%5Cmathbb%7BE%7D_%7Bx+%5Csim+q%7D+W%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A(W) = \max_{q} H(q) + \mathbb{E}_{x \sim q} W(x)" class="latex" />.</li><li>In particular, for every value of <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" />, we have that<br /><img src="https://i.imgur.com/F0NNlGQ.png" alt="" /></li></ul>



<p>which <a href="https://windowsontheory.org/2021/02/24/unsupervised-learning-and-generative-models/">we’ve seen before</a>! Note that equality holds when <img src="https://s0.wp.com/latex.php?latex=q+%3D+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q = p" class="latex" />, but to approximate <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" /> we can simply take more tractable values of <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" />.</p>



<p>In many situations, we can compute <img src="https://s0.wp.com/latex.php?latex=W%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="W(x)" class="latex" />, but can’t compute <img src="https://s0.wp.com/latex.php?latex=A_%7B%5Ctau%7D%28W%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A_{\tau}(W)" class="latex" />, which stifles many applications. One upshot of this, though, is that we can calculate ratios of <img src="https://s0.wp.com/latex.php?latex=p%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(x)" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=p%28x%27%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(x')" class="latex" />, which is good enough for some applications, like Markov Chain Monte Carlo.</p>



<h3>Markov Chain Monte Carlo (MCMC)</h3>



<p>An important task in statistics is to sample from a distribution <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" />, for very complicated values of <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" />. MCMC does this by constructing a Markov Chain whose stationary distribution is <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" />. The most common instantiation of MCMC is Metropolis-Hastings, which we now describe. First, we assume that there exists an undirected graph on the states <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" />, so that <img src="https://s0.wp.com/latex.php?latex=x+%5Csim+x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x \sim x'" class="latex" /> iff <img src="https://s0.wp.com/latex.php?latex=x%27+%5Csim+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x' \sim x" class="latex" /> and that for <img src="https://s0.wp.com/latex.php?latex=x+%5Csim+x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x \sim x'" class="latex" />, the probabilities of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x'" class="latex" /> are similar in the sense that <img src="https://s0.wp.com/latex.php?latex=W%28x%29%2FW%28x%27%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="W(x)/W(x')" class="latex" /> is neither too small nor too large. Then the Metropolis-Hastings algortihm is as follows.</p>



<p><strong>Metropolis-Hastings Algorithm</strong>:</p>



<ol><li>Draw <img src="https://s0.wp.com/latex.php?latex=x_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_0" class="latex" /> at random.</li><li>For <img src="https://s0.wp.com/latex.php?latex=i+%3D+1%2C+2%2C+%5Cldots&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i = 1, 2, \ldots" class="latex" />, choose an arbitrary <img src="https://s0.wp.com/latex.php?latex=x%27+%5Csim+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x' \sim x" class="latex" />, and let</li></ol>



<figure class="wp-block-image"><img src="https://i.imgur.com/HQBwr77.png" alt="" /></figure>



<p>Then, <img src="https://s0.wp.com/latex.php?latex=x_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_t" class="latex" /> eventually is distributed as <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" />! To show that this samples from <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" />, we show that the stationary distribution is of this Markov chain is <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" />. In this case, this turns out to be easy, since we can check the <em>detailed balance conditions</em> <img src="https://s0.wp.com/latex.php?latex=p%28x%27+%7C+x%29p%28x%29+%3D+%5Cmin%7B%28p%28x%29%2C+p%28x%27%29%29%7D+%3D+p%28x+%7C+x%27%29p%28x%27%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(x' | x)p(x) = \min{(p(x), p(x'))} = p(x | x')p(x')" class="latex" /> so <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" /> is the stationary distribution of the Markov Chain.</p>



<p>The stationary distribution is often unique, so we’ve proven that this samples from <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" /> eventually. In MCMC algorithms, however, often an important question is how fast we converge to the stationary distribution. Often this is rather slow, which is especially dissappointing because if it were faster many very difficult problems could be solved much more easily, like generic optimization.<br />Indeed, there are examples where convergence to the stationary distribution would take exponential time.</p>



<p><strong>Note:</strong> One way to make MCMC faster is to let <img src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x'" class="latex" /> be really close from <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" />, because the likelihood ratio will be closer to <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_i" class="latex" /> will spend less time stuck at its original location. There’s a tradeoff, however. If <img src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x'" class="latex" /> is too close to <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" />, then the chain will not <em>mix</em> as quickly, where mixing is the convergence of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> to the stationary distribution, because generally to mix the value <img src="https://s0.wp.com/latex.php?latex=x_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_t" class="latex" /> must get close to every point, and making each of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" />‘s steps smaller will make that more difficult.</p>



<h3>Applications of MCMC: Simulated Annealing</h3>



<p>One application of MCMC is in simulated annealing. Suppose that we have <img src="https://s0.wp.com/latex.php?latex=W%3A+%7B0%2C+1%7D%5E%7Bn%7D+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="W: {0, 1}^{n} \rightarrow \mathbb{R}" class="latex" />, and we want to find <img src="https://s0.wp.com/latex.php?latex=x+%3D+%5Carg+%5Cmax_%7Bx%7D+W%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x = \arg \max_{x} W(x)" class="latex" />. The most direct attempt at solving this problem is creating a Markov Chain that simply samples from <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5Carg%5Cmax_%7Bx%7D+W%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x \in \arg\max_{x} W(x)" class="latex" />. However, this is impractical, at least directly. It is like we have a domino that is far away from us, and we can’t knock it down. What do we do in this case? We put some dominoes in between us!</p>



<p>To this end, we now create a sequence of Markov chains supported on <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" />, whose stationary distribution <img src="https://s0.wp.com/latex.php?latex=p_%7B%5Ctau%7D%28x%29+%5Csim+W%28x%29%5E%7B1%2F%5Ctau%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_{\tau}(x) \sim W(x)^{1/\tau}" class="latex" />, as <img src="https://s0.wp.com/latex.php?latex=%5Ctau&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tau" class="latex" /> gets smaller and smaller. This corresponds to cooling a system from a high-temperature to a low-temperature. Essentially, we want to sample from <img src="https://s0.wp.com/latex.php?latex=p_0%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_0(x)" class="latex" />.</p>



<p>Simulated annealing lets us begin by sampling from <img src="https://s0.wp.com/latex.php?latex=p_%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_{\infty}" class="latex" />, which is uniform on the support. Then we can slowly reduce <img src="https://s0.wp.com/latex.php?latex=%5Ctau&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tau" class="latex" /> from <img src="https://s0.wp.com/latex.php?latex=%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\infty" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" />. When cooling a system, it will be helpful to think of two stages. First, a stage in which the object cools down. Second, a settling stage in which the system settles into a more stable state. The settling stage is simulated via MCMC on <img src="https://s0.wp.com/latex.php?latex=p_%7B%5Ctau%7D%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_{\tau}(x)" class="latex" />, so the transition probability from <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x'" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=%5Cmin%281%2C+%5Ctext%7Bexp%7D%28%5Ctau%5E%7B-1%7D+%5Ccdot+%28W%28x%27%29+-+W%28x%29%29%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\min(1, \text{exp}(\tau^{-1} \cdot (W(x') - W(x))))" class="latex" />.</p>



<p><strong>Simulated Annealing Algorithm</strong>:</p>



<ol><li><em>Cooling</em> Begin <img src="https://s0.wp.com/latex.php?latex=%5Ctau&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tau" class="latex" /> at <img src="https://s0.wp.com/latex.php?latex=%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\infty" class="latex" /> (or very large), and lower the value of <img src="https://s0.wp.com/latex.php?latex=%5Ctau&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\tau" class="latex" /> to zero according to some schedule.<br />1a. <em>Settling</em> Now, repeatedly move from <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x'" class="latex" /> with probability <img src="https://s0.wp.com/latex.php?latex=%5Cmin%281%2C+%5Ctext%7Bexp%7D%28%5Ctau%5E%7B-1%7D+%5Ccdot+%28W%28x%27%29+-+W%28x%29%29%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\min(1, \text{exp}(\tau^{-1} \cdot (W(x') - W(x))))" class="latex" />.</li></ol>



<p>Since simulated annealing is inspired by physical intuition, it turns out that its shortcomings can be interpreted physically too. Namely, when you cool something too quickly it becomes a glassy state instead of a ground state, which often causes simulated annealing to fail, and for this algorithm to get stuck in local minima. Note how this is fundamentally a failure mode with MCMC – it can’t mix quickly enough.</p>



<p>See <a href="https://www.youtube.com/watch?v=iaq_Fpr4KZc">this video</a> for an illustration of simulated annealing.</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2021/04/ezgif.com-gif-maker.gif"><img src="https://windowsontheory.files.wordpress.com/2021/04/ezgif.com-gif-maker.gif?w=818" alt="" class="wp-image-8062" /></a></figure>



<h2>Part 2: Bayesian Analysis</h2>



<p>Note that the posterior of <img src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bw%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\vec{w}" class="latex" /> conditioned on <img src="https://s0.wp.com/latex.php?latex=x_1%2C+%5Cldots+x_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_1, \ldots x_n" class="latex" /> is</p>



<p><img src="https://s0.wp.com/latex.php?latex=p%28%5Cvec%7Bw%7D+%7C+x_1%2C+%5Cldots+x_n%29+%3D+%5Cfrac%7Bp%28%5Cvec%7Bw%7D%29%7D%7Bp%28%5Cvec%7Bx%7D%29%7D+%5Ccdot+p%28x_1+%7C+%5Cvec%7Bw%7D%29+p%28x_2+%7C+%5Cvec%7Bw%7D%29+%5Cldots+p%28x_n+%7C+%5Cvec%7Bw%7D%29+%5Cpropto+%5Cexp%5Cleft%28%5Csum+X_i%28%5Cvec%7Bw%7D%29%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(\vec{w} | x_1, \ldots x_n) = \frac{p(\vec{w})}{p(\vec{x})} \cdot p(x_1 | \vec{w}) p(x_2 | \vec{w}) \ldots p(x_n | \vec{w}) \propto \exp\left(\sum X_i(\vec{w})\right)" class="latex" /></p>



<p>We now have <img src="https://s0.wp.com/latex.php?latex=p_%7BW%7D%28x%29+%3D+%5Cexp%28W%28x%29+-+A%28W%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_{W}(x) = \exp(W(x) - A(W))" class="latex" />, an exponential distribution. Now suppose that <img src="https://s0.wp.com/latex.php?latex=W%28x%29+%3D+%5Clangle+w%2C+%5Chat%7Bx%7D+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="W(x) = \langle w, \hat{x} \rangle" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bx%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bm%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\hat{x} \in \mathbb{R}^{m}" class="latex" /> are the sufficient statistics of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" />. For example, the energy function could follow that of a tree, so <img src="https://s0.wp.com/latex.php?latex=W%28x%29+%3D+%5Csum_%7B%28i%2C+j%29+%5Cin+T%7D+w_%7Bi%2C+j%7D+x_i+x_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="W(x) = \sum_{(i, j) \in T} w_{i, j} x_i x_j" class="latex" />. If you want to find the expected value of $latex \mathbb{E}<em>{x \sim p</em>{W}}[W(x)]&amp;bg=ffffff$ its enough to know $latex \mu = \mathbb{E}<em>{x \sim p</em>{W} }[\hat{x}]&amp;bg=ffffff$. (By following a tree, we mean that the undirected graphical model of the distribution is that of a tree.)</p>



<h3>Sampling from a Tree-Structured Distribution</h3>



<p>Now how do we sample from a tree-structured distribution? The most direct way is by calculating the marginals. While marginalization is generally quite difficult, it is much more tractable on certain types of graphs. For a sense of what this entails, suppose that we have the following tree-structured statistical model:</p>



<p><img src="https://i.imgur.com/mnC1vkM.png" alt="" />,<br />whose joint PDF is <img src="https://s0.wp.com/latex.php?latex=W%28x%29+%3D+%5Cexp%28%5Csum_%7Bi%2C+j+%5Cin+E%7D+-w_%7Bi%2C+j%7D+%28x_i+-+x_j%29%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="W(x) = \exp(\sum_{i, j \in E} -w_{i, j} (x_i - x_j)^2)" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=x+%5Cin+%7B0%2C+1%7D%5E%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x \in {0, 1}^{n}" class="latex" />.<br />(The unusual notation is chosen so that the relationships between the marginals is clean. The log-density in question is a Laplacian Quadratic Form.)</p>



<p>Then one can show that if the marginal of <img src="https://s0.wp.com/latex.php?latex=x_3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_3" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=%5Cmu_3+%3D+%5Cmathbb%7BP%7D%28x_3+%3D+1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu_3 = \mathbb{P}(x_3 = 1)" class="latex" />, then we have that</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmu_6+%3D+%5Cmu_3+%5Ccdot+%5Cfrac%7Be%5E%7Bw_%7B3%2C6%7D%7D%7D%7B1+%2B+e%5E%7Bw_%7B3%2C+6%7D%7D%7D+%2B+%281+-+%5Cmu_3%29+%5Ccdot+%5Cfrac%7B1%7D%7B1+%2B+e%5E%7Bw_%7B3%2C+6%7D%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu_6 = \mu_3 \cdot \frac{e^{w_{3,6}}}{1 + e^{w_{3, 6}}} + (1 - \mu_3) \cdot \frac{1}{1 + e^{w_{3, 6}}}" class="latex" /></p>



<p>This will give six linear equations in six unknowns, which we can solve. Once we can marginalize a variable, say, <img src="https://s0.wp.com/latex.php?latex=x_6&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_6" class="latex" />, we can then simply sample from the marginal, then sample from the conditional distribution on <img src="https://s0.wp.com/latex.php?latex=x_6&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_6" class="latex" /> to sample from the entire distribution. This method is known as belief propogation.</p>



<p>This algorithm (with some modifications) also works for general graphs, but what it represents on general graphs is not the exact marginal, but rather an extension of that graph to a tree.</p>



<h3>General Exponential Distributions</h3>



<p>Now let’s try to develop more theory for exponential distributions. Let the PDF of this distribution be <img src="https://s0.wp.com/latex.php?latex=p_W%28x%29+%3D+%5Cexp%28%5Clangle+w%2C+%5Chat%7Bx%7D+%5Crangle+-+A%28w%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_W(x) = \exp(\langle w, \hat{x} \rangle - A(w))" class="latex" />. A few useful facts about these distributions that often appear in calculations: <img src="https://s0.wp.com/latex.php?latex=%5Cnabla+A%28w%29+%3D+%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla A(w) = \mu" class="latex" /> and $latex \nabla^2 (A(w)) = \text{Cov}<em>{p</em>{W}}(x) \succeq 0&amp;bg=ffffff$.</p>



<p>By the variational principle, we have that among all distributions with the same sufficient statistics as a given Boltzmann distribution, the true one maximizes the entropy, so</p>



<p><img src="https://s0.wp.com/latex.php?latex=p_w+%3D+%5Carg%5Cmax_%7B%5Cmathbb%7BE%7D_%7Bq%7D+%5Chat%7Bx%7D+%3D+%5Cmu%7D+H%28q%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_w = \arg\max_{\mathbb{E}_{q} \hat{x} = \mu} H(q)." class="latex" /></p>



<p>An analagous idea gives us<br /><img src="https://s0.wp.com/latex.php?latex=%5Cmu+%3D+%5Carg+%5Cmax_%7BH%28%5Cmu%29+%5Cge+H%28p_%7Bw%7D%29%7D+%5Clangle+w%2C+%5Cmu+%5Crangle%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu = \arg \max_{H(\mu) \ge H(p_{w})} \langle w, \mu \rangle," class="latex" /></p>



<p>so <img src="https://s0.wp.com/latex.php?latex=p_w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_w" class="latex" /> is the <em>maximum entropy</em> distribution consistent with observations <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu" class="latex" />.</p>



<p>The important consequence of these two facts is that in principle, <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" /> determines <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu" class="latex" /> and vice versa. (Up to fine print of using <em>minimal</em> sufficient statistics, which we ignore here.) We will refer to <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" /> as the “canonical parameter space” and <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu" class="latex" /> as the “mean representation space”. Now note that the first equation gives us a way of going from <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" />, and the second equation gives us a way to go from <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu" class="latex" />, at least information-theoretically.</p>



<p>Now, how do we do this algorithmically? Using <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" />, if were able to sample from <img src="https://s0.wp.com/latex.php?latex=p_w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_w" class="latex" /> (which is generally possible if we can evaluate <img src="https://s0.wp.com/latex.php?latex=A%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A(w)" class="latex" /> then we can estimate the mean <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu" class="latex" /> through samples. We can also obtain <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu" class="latex" /> from <img src="https://s0.wp.com/latex.php?latex=A%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A(w)" class="latex" /> by estimating <img src="https://s0.wp.com/latex.php?latex=%5Cnabla+A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla A" class="latex" />.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/qFKGiMa.png" alt="" /></figure>



<p>On the other hand, if you have <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu" class="latex" />, to obtain <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" /> you first consider <img src="https://s0.wp.com/latex.php?latex=A%5E%7B%5Cast%7D%28%5Cmu%29+%3D+%5Csup_%7Bw+%5Cin+%5COmega%7D+%7B%5Clangle+%5Cmu%2C+w+%5Crangle+-+A%28w%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A^{\ast}(\mu) = \sup_{w \in \Omega} {\langle \mu, w \rangle - A(w)}" class="latex" /> and then note that the desired value is <img src="https://s0.wp.com/latex.php?latex=%5Carg%5Cmax_%7Bw+%5Cin+%5COmega%7D%7B%5Clangle+%5Cmu%2C+w+%5Crangle+-+A%28w%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\arg\max_{w \in \Omega}{\langle \mu, w \rangle - A(w)}" class="latex" />, so solving this problem boils down to estimating <img src="https://s0.wp.com/latex.php?latex=%5Cnabla+A%5E%7B%5Cast%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla A^{\ast}" class="latex" /> efficiently, because setting it to zero will give us <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" />.</p>



<p>Thus going from <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu" class="latex" /> requires estimating <img src="https://s0.wp.com/latex.php?latex=%5Cnabla+A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla A" class="latex" />, whereas going from <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" /> requires estimating <img src="https://s0.wp.com/latex.php?latex=%5Cnabla+A%5E%7B%5Cast%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla A^{\ast}" class="latex" />.</p>



<p>When <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" /> is a <em>posterior distribution</em>, the observed data typically gives us the weights <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" />, and hence the inference problem becomes to use that to sample from the posterior.</p>



<h4>Examples of Exponential Distributions</h4>



<p>There are many examples of exponential distributions, which we now give.</p>



<ul><li>High-Dimensional Normals: <img src="https://s0.wp.com/latex.php?latex=x%5Cin+%5Cmathbb%7BR%7D%5E%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x\in \mathbb{R}^{d}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=W%28x%29+%3D+-%28x+-+%5Cmu%29%5E%7B%5Ctop%7D+%5CSigma%5E%7B-1%7D+%28x-%5Cmu%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="W(x) = -(x - \mu)^{\top} \Sigma^{-1} (x-\mu)" class="latex" /></li><li>Ising Model: <img src="https://s0.wp.com/latex.php?latex=x%5Cin+%7B0%2C+1%7D%5E%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x\in {0, 1}^{d}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=W%28x%29+%3D+%5Csum+w_i+x_I+%2B+%5Csum_%7Bi%2C+j+%5Cin+E%7D+w_%7Bi%2C+j%7D+x_i+x_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="W(x) = \sum w_i x_I + \sum_{i, j \in E} w_{i, j} x_i x_j" class="latex" />. A sufficient statistic for these is <img src="https://s0.wp.com/latex.php?latex=%5Chat%7Bx%7D+%3D+%28x%2C+xx%5E%7B%5Ctop%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\hat{x} = (x, xx^{\top})" class="latex" />, a fact which we will invoke repeatedly.</li><li>There’s many many more, including Gaussian Markov Random Fields, Latend Dirchlet Allocation, and Mixtures of Gaussians.</li></ul>



<h4>Going from canonical parameters to mean parameters</h4>



<p>Now, we show how to go from <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu" class="latex" /> in a special case, namely in the mean-field approximation. The mean field approximation is the approximation of distributions by product distributions over <img src="https://s0.wp.com/latex.php?latex=x_1%2C+%5Cldots%2C+x_d+%5Cin+%7B0%2C+1%7D%5E%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_1, \ldots, x_d \in {0, 1}^{d}" class="latex" />, for which</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BP%7D%28x_1%2C+%5Cldots+x_n%29+%3D+%5Cmathbb%7BP%7D%28x_1%29%5Cmathbb%7BP%7D%28x_2%29+%5Cldots+%5Cmathbb%7BP%7D%28x_n%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{P}(x_1, \ldots x_n) = \mathbb{P}(x_1)\mathbb{P}(x_2) \ldots \mathbb{P}(x_n)." class="latex" /></p>



<p>Recall that the partition function can be computed as</p>



<p><img src="https://s0.wp.com/latex.php?latex=A%28w%29+%3D+%5Cmax_%7Bq+%5Cin+%5Cmathcal%7BQ%7D%7D+%5Clangle+w%2C+%5Cmathbb%7BE%7D_%7Bq%7D+%5Chat%7Bx%7D+%5Crangle+%2B+H%28q%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A(w) = \max_{q \in \mathcal{Q}} \langle w, \mathbb{E}_{q} \hat{x} \rangle + H(q)." class="latex" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{Q}" class="latex" /> is the set of all probability distributions. If we instead write <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{Q}" class="latex" /> as the set of product distributions (parametrized by <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu" class="latex" />, or the probability that each variable is <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1" class="latex" />), we get</p>



<p><img src="https://s0.wp.com/latex.php?latex=A%28w%29+%5Cge+%5Cmax_%7B%5Cmu+%5Cin+%5Cmathcal%7BQ%7D%7D+%5Csum+w_i+%5Cmu_i+%2B+%5Csum+w_%7Bi%2Cj%7D+%5Cmu_i+%5Cmu_j+%2B+%5Csum+H%28%5Cmu_i%29%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A(w) \ge \max_{\mu \in \mathcal{Q}} \sum w_i \mu_i + \sum w_{i,j} \mu_i \mu_j + \sum H(\mu_i)," class="latex" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=H%28%5Cmu_i%29+%3D+-%5Cmu_i+%5Clog+%5Cmu_i+-+%281+-+%5Cmu_i%29+%5Clog%7B1+-+%5Cmu_i%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(\mu_i) = -\mu_i \log \mu_i - (1 - \mu_i) \log{1 - \mu_i}" class="latex" /> and it now suffices to maximize the right hand function over the set <img src="https://s0.wp.com/latex.php?latex=K+%3D+%7B+%5Cmu%5Cin+%5B0%2C1%5D%5Ed+%7C+%5Csum+%5Cmu_i+%3D+1+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K = { \mu\in [0,1]^d | \sum \mu_i = 1 }" class="latex" />.</p>



<p>We can generally maximize concave functions over a convex set such as <img src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K" class="latex" />. Unfortunately, this function is not concave. However, it is concave in every coordinate. This suggests the following algorithm: fix all but one variable and maximize over that variable, and repeat. This approach is known as Coordinate Ascent Variational Inference (CAVI), and its pseudocode is given below.</p>



<p><strong>CAVI Algorithm</strong></p>



<ol><li>Let <img src="https://s0.wp.com/latex.php?latex=%5Cmu+%3D+1%2F2+%5Ccdot+%281%2C+1%2C+%5Cldots+1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu = 1/2 \cdot (1, 1, \ldots 1)" class="latex" /></li><li>Repatedly choose values of <img src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="j" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%5Bn%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="[n]" class="latex" /> (possibly at random, or in a loop.)<br />2a. Update <img src="https://s0.wp.com/latex.php?latex=%5Cmu_%7Bj%7D+%3D+%5Carg%5Cmax_%7Bx_j%7D+%28f%28%5Cmu_%7B-j%7D%2C+x_j%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu_{j} = \arg\max_{x_j} (f(\mu_{-j}, x_j))" class="latex" /> (where <img src="https://s0.wp.com/latex.php?latex=%5Cmu_%7B-j%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu_{-j}" class="latex" /> represents the non-<img src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="j" class="latex" /> coordinates of <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu" class="latex" />)</li></ol>



<h2>Part 3. Solution landscapes and the replica method</h2>



<p>This part is best explained visually. Suppose that we have a Boltzman distribution. In the infinite temperature limit the this is the uniform distribution, distributed over the entire domain. As you decrease the temperature, the support’s size decreases. (<em>Note:</em> The gifs below are just cartoons. These pretend as if the probability distribution is always uniform over a subset. Also, in most cases of interest for learning, only higher order derivatives of entropy will be discontinuous.)<br /></p>



<figure class="wp-block-image"><img src="https://i.imgur.com/rPb67cG.gif" alt="" /></figure>



<p>Sometimes there’s a discontinuity in entropy, and the entropy “suddenly drops” in a discontinuity. Often the entropy function itself is continuous, but the derivatives are not continuous at that point – a higher order phase transition.</p>



<p>Sometimes the geometry of the solution space undergoes a phase transition as well, with it “shattering” to several distinct clusters.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/itUWxrB.gif" alt="" /></figure>



<h3>The replica method</h3>



<p><strong>Note:</strong> We will have an extended post on the replica method later on.</p>



<p>If you sampled <img src="https://s0.wp.com/latex.php?latex=x_1%2C+%5Cldots+x_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_1, \ldots x_n" class="latex" /> from <img src="https://s0.wp.com/latex.php?latex=p_%7Bw%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_{w}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=p_w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_w" class="latex" /> is a high-dimensional distribution, you’d expect the distances to all be approximately the same. The overlap matrix, or </p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Bbmatrix%7D+%5Clangle+x_1%2C+x_1+%5Crangle+%26+%5Ccdots+%26+%5Clangle+x_1%2C+x_n+%5Crangle+%5C+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%5C+%5Clangle+x_n%2C+x_1+%5Crangle+%26+%5Ccdots+%26+%5Clangle+x_n%2C+x_n+%5Crangle+%5Cend%7Bbmatrix%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\begin{bmatrix} \langle x_1, x_1 \rangle &amp; \cdots &amp; \langle x_1, x_n \rangle \ \vdots &amp; \ddots &amp; \vdots \ \langle x_n, x_1 \rangle &amp; \cdots &amp; \langle x_n, x_n \rangle \end{bmatrix}" class="latex" /> </p>



<p>we approximate as </p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Bbmatrix%7D+1+%26+%5Ccdots+%26+q+%5C+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%5C+q%26+%5Ccdots+%26+1+%5Cend%7Bbmatrix%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\begin{bmatrix} 1 &amp; \cdots &amp; q \ \vdots &amp; \ddots &amp; \vdots \ q&amp; \cdots &amp; 1 \end{bmatrix}" class="latex" /> </p>



<p>where <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" /> is some constant.</p>



<p>Suppose <img src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w" class="latex" /> comes from a probability distribution <img src="https://s0.wp.com/latex.php?latex=W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="W" class="latex" />. Then a very common problem is computing</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bw%7D%5BA%28w%29%5D+%3D+%5Cmathbb%7BE%7D_%7Bw%7D%5Cleft%5B+%5Clog+%5Cint+%5Cexp%28%5Clangle+w%2C+%5Chat%7Bx%7D+%5Crangle%29%5Cright%5D%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{w}[A(w)] = \mathbb{E}_{w}\left[ \log \int \exp(\langle w, \hat{x} \rangle)\right]," class="latex" /></p>



<p>which is the expected free energy. However, it turns out to be much easier to find <img src="https://s0.wp.com/latex.php?latex=%5Clog+%28%5Cint+E_%7Bw%7D+%5Cexp%28%5Clangle+w%2C+%5Chat%7Bx%7D+%5Crangle%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log (\int E_{w} \exp(\langle w, \hat{x} \rangle))" class="latex" />. So here’s what we do. We find</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bw%7D+%5BA%28w%29%5D+%3D+%5Clim%7Bn+%5Crightarrow+0%7D+%5Cfrac%7B%5Cmathbb%7BE%7D_%7Bw%7D+%5B%5Ctext%7Bexp%7D%28n+%5Ccdot+A%28w%29%29%5D+-+1%7D%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{w} [A(w)] = \lim{n \rightarrow 0} \frac{\mathbb{E}_{w} [\text{exp}(n \cdot A(w))] - 1}{n}" class="latex" /></p>



<p>This should already smell weird, because <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> is going to zero, an unusal notational choice. We can now write this as<br /><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%7Bw%7D+A%28w%29+%3D+%5Clim_%7Bn+%5Crightarrow+0%7D+%5Cfrac%7B%5Cmathbb%7BE%7D_%7Bw%7D%5Cleft%28+%5Cint%7Bx%7D+%5Cexp%28%5Clangle+w%2C+%5Chat%7Bx%7D+%5Crangle%29%5Cright%29%5E%7Bn%7D+-+1%7D%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}{w} A(w) = \lim_{n \rightarrow 0} \frac{\mathbb{E}_{w}\left( \int{x} \exp(\langle w, \hat{x} \rangle)\right)^{n} - 1}{n}" class="latex" /></p>



<p>which we can write as</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Clim_%7Bn+%5Crightarrow+0%7D+%5Cfrac%7B%5Cint_%7Bx_1%2C+%5Cldots+x_n%7D+%5Cmathbb%7BE%7D_%7Bw%7D+%5Cexp%28%5Clangle+w%2C+%5Chat%7Bx%7D_1+%2B+%5Ccdots+%2B+%5Chat%7Bx%7D_n+%5Crangle%29+-+1%7D%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lim_{n \rightarrow 0} \frac{\int_{x_1, \ldots x_n} \mathbb{E}_{w} \exp(\langle w, \hat{x}_1 + \cdots + \hat{x}_n \rangle) - 1}{n}" class="latex" /></p>



<p>Now these <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_i" class="latex" />‘s represent the replicas! Now, we’d hope <img src="https://s0.wp.com/latex.php?latex=%5Cpsi%28n%29+%3D+%5Cint_%7Bx_1%2C+%5Cldots+x_n%7D+%5Cmathbb%7BE%7D_w+%5Cexp%28%5Clangle+w%2C+%5Chat%7Bx%7D_1+%2B+%5Chat%7Bx%7D_2+%2B+%5Cldots+%2B+%5Chat%7Bx%7D_n+%5Crangle%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\psi(n) = \int_{x_1, \ldots x_n} \mathbb{E}_w \exp(\langle w, \hat{x}_1 + \hat{x}_2 + \ldots + \hat{x}_n \rangle)" class="latex" /> is an analytic function, and we can now write this as <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpsi%28n%29+-+1%7D%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{\psi(n) - 1}{n}" class="latex" /> as <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> goes to zero.</p>



<p>Generally speaking, <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bw%7D+%5B%5Cexp%28w%2C+%5Chat%7Bx%7D_1+%2B+%5Ccdots+%2B+%5Chat%7Bx%7D_n%29%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{w} [\exp(w, \hat{x}_1 + \cdots + \hat{x}_n)]" class="latex" /> only depends on overlaps of <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" />, so we often guess the value of <img src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Q" class="latex" /> and calculate this expectation.</p>



<h4>Examples:</h4>



<p>We’ll now give an example of how this is useful. Consider a spiked matrix/tensor model, where we observe <img src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y" class="latex" /> so that <img src="https://s0.wp.com/latex.php?latex=Y+%3D+%5Clambda+S+%2B+N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y = \lambda S + N" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="S" class="latex" /> is the signal and <img src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N" class="latex" /> is the noise. Thus here we have</p>



<p><img src="https://s0.wp.com/latex.php?latex=p%28S%27%2C+N%27+%7C+Y%29+%5Cpropto+%5Cexp%28%5Cbeta+%5Clangle+Y+-+%5Clambda+S%27%2C+N%27+%5Crangle%5E2%29+%5Ccdot+p%28S%27%29%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(S', N' | Y) \propto \exp(\beta \langle Y - \lambda S', N' \rangle^2) \cdot p(S')," class="latex" /></p>



<p>and we want to analyze <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7BS%27+%5Csim+p%28%5Ccdot+%7C+Y%29%7D+%5Clangle+S%2C+S%27+%5Crangle%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{S' \sim p(\cdot | Y)} \langle S, S' \rangle^2" class="latex" /> as a function of <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda" class="latex" />, which can be done by the <a href="https://arxiv.org/pdf/1612.07728.pdf">replica method</a> and exhibits a phase transition.<br /><img src="https://i.imgur.com/ec3tC8C.png" alt="" /></p>



<p>Other examples include Teacher-student models, where <img src="https://s0.wp.com/latex.php?latex=X%2C+Y+%3D+%28X%2C+f_S%28X%29+%2B+N%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X, Y = (X, f_S(X) + N)" class="latex" />. Then, <a href="https://arxiv.org/abs/2102.08127">a recent work</a> calculates the training losses and classification errors when training on this dataset, which closely match empirical values.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/x9bdDv7.png" alt="" /></figure>



<h3>Symmetry breaking</h3>



<p>Lastly, we’ll talk about replica symmetry breaking. Sometimes, discontinuities don’t just make the support smaller, but actually break the support into many different parts. For example, at this transition the circle becomes the three green circles.<br /><img src="https://i.imgur.com/ufck69X.png" alt="" /></p>



<p>When this happens, we can no longer make our overlap matrix assumption, as there are now asymmetries between the points. This leads to the overlap matrix being striped, in the fashion one would anticipate.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/0Zua7m9.png" alt="" /></figure>



<p>Sometimes, the support actually breaks into infinitely many parts, in a phenomenon called full replica symmetry breakng.</p>



<p>Finally, some historical notes. This “plugging in zero” trick was introduced by Parisi approximately 30 years ago, receiving a standing ovation at the ICM. Since then, some of those conjectures have been rigorously formalized, but many haven’t. It is still very impressive that Parisi was able to do it.</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/04/02/inference-and-statistical-physics/"><span class="datestr">at April 02, 2021 03:04 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4509">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2021/04/02/bocconi-hired-poorly-qualified-computer-scientist/">Bocconi Hired Poorly Qualified Computer Scientist</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Today I received an interesting email from our compliance office that is working on the accreditation of our PhD program in Statistics and Computer Science.</p>



<p>One of the requisites for accreditation is to have a certain number of affiliated faculty. To count as an affiliated faculty, however, one must pass certain minimal thresholds of research productivity, the same that are necessary to be promoted to Associate Professor, as quantified according to Italy’s well intentioned but questionably run initiative to conduct research evaluations using quantifiable parameters.</p>



<p>(For context, every Italian professor maintains a list of publications in a site run by the ministry. Although the site is linked to various bibliographic databases, one has to input each publication manually into a local site at one’s own university, then the ministry site fetches the data from the local site. The data in the ministry site is used for these research evaluations. At one point, a secretary and I spent long hours entering my publications from the past ten years, to apply for an Italian grant.)</p>



<p>Be that as it may, the compliance office noted that I did not qualify to be an affiliated faculty (or, for that matter, an Associate Professor) based on my 2016-2020 publication record. That would be seven papers in SoDA and two in FOCS: surely Italian Associate Professors are held to high standards! It turns out, however, that one of the criteria counts only journal publications.</p>



<p> Well, how about the paper in J. ACM and the two papers in SIAM J. on Computing published between 2016 and 2020? That would (barely) be enough, but one SICOMP paper has the same title of a SoDA paper (being, in fact, the same paper) and so the ministry site had rejected it. Luckily, the Bocconi administration was able to remove the SoDA paper from the ministry site, I added again the SICOMP version, and now I finally, if barely, qualify to be an Associate Professor and a PhD program affiliated faculty.</p>



<p>This sounds like the beginning of a long and unproductive relationship between me and the Italian system of research evaluation.</p>



<p>P.S. some colleagues at other Italian universities to whom I told this story argued that the Bocconi administration did not correctly apply the government rules, and that one should count conference proceedings indexed by Scopus; other colleagues said that indeed<a href="https://abilitazione.miur.it/public/documenti/2018/DM_Valori_Soglia_589_08082018.pdf"> the government decree n. 589 of August 8, 2018,</a> in article 2, comma 1, part a, only refers to journals. This of course only reinforces my impression that the whole set of evaluation criteria is a dumpster fire that is way too far gone.</p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2021/04/02/bocconi-hired-poorly-qualified-computer-scientist/"><span class="datestr">at April 02, 2021 02:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.00514">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.00514">Spectral Unions of Partial Deformable 3D Shapes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moschella:Luca.html">Luca Moschella</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Melzi:Simone.html">Simone Melzi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cosmo:Luca.html">Luca Cosmo</a>, Filippo Maggioli, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Litany:Or.html">Or Litany</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Ovsjanikov:Maks.html">Maks Ovsjanikov</a>, Leonidas Guibas, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rodol=agrave=:Emanuele.html">Emanuele Rodolà</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.00514">PDF</a><br /><b>Abstract: </b>Spectral geometric methods have brought revolutionary changes to the field of
geometry processing -- however, when the data to be processed exhibits severe
partiality, such methods fail to generalize. As a result, there exists a big
performance gap between methods dealing with complete shapes, and methods that
address missing geometry. In this paper, we propose a possible way to fill this
gap. We introduce the first method to compute compositions of non-rigidly
deforming shapes, without requiring to solve first for a dense correspondence
between the given partial shapes. We do so by operating in a purely spectral
domain, where we define a union operation between short sequences of
eigenvalues. Working with eigenvalues allows to deal with unknown
correspondence, different sampling, and different discretization (point clouds
and meshes alike), making this operation especially robust and general. Our
approach is data-driven, and can generalize to isometric and non-isometric
deformations of the surface, as long as these stay within the same semantic
class (e.g., human bodies), as well as to partiality artifacts not seen at
training time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.00514"><span class="datestr">at April 02, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.00415">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.00415">Learning with Neural Tangent Kernels in Near Input Sparsity Time</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zandieh:Amir.html">Amir Zandieh</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.00415">PDF</a><br /><b>Abstract: </b>The Neural Tangent Kernel (NTK) characterizes the behavior of infinitely wide
neural nets trained under least squares loss by gradient descent (Jacot et al.,
2018). However, despite its importance, the super-quadratic runtime of kernel
methods limits the use of NTK in large-scale learning tasks. To accelerate
kernel machines with NTK, we propose a near input sparsity time algorithm that
maps the input data to a randomized low-dimensional feature space so that the
inner product of the transformed data approximates their NTK evaluation.
Furthermore, we propose a feature map for approximating the convolutional
counterpart of the NTK (Arora et al., 2019), which can transform any image
using a runtime that is only linear in the number of pixels. We show that in
standard large-scale regression and classification tasks a linear regressor
trained on our features outperforms trained NNs and Nystrom method with NTK
kernels.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.00415"><span class="datestr">at April 02, 2021 10:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.00406">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.00406">The complete classification for quantified equality constraints</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhuk:Dmitriy.html">Dmitriy Zhuk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Martin:Barnaby.html">Barnaby Martin</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.00406">PDF</a><br /><b>Abstract: </b>We prove that QCSP$(\mathbb{N};x=y\rightarrow y=z)$ is PSpace-complete,
settling a question open for more than ten years. This completes the complexity
classification for quantified equality languages as a trichotomy between
Logspace, NP-complete and PSpace-complete.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.00406"><span class="datestr">at April 02, 2021 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.00207">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.00207">The $k$-Colorable Unit Disk Cover Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Monith S. Reyunuru, Kriti Jethlia, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Basappa:Manjanna.html">Manjanna Basappa</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.00207">PDF</a><br /><b>Abstract: </b>In this article, we consider colorable variations of the Unit Disk Cover
({\it UDC}) problem as follows. {\it $k$-Colorable Discrete Unit Disk Cover
({\it $k$-CDUDC})}: Given a set $P$ of $n$ points, and a set $D$ of $m$ unit
disks (of radius=1), both lying in the plane, and a parameter $k$, the
objective is to compute a set $D'\subseteq D$ such that every point in $P$ is
covered by at least one disk in $D'$ and there exists a function
$\chi:D'\rightarrow C$ that assigns colors to disks in $D'$ such that for any
$d$ and $d'$ in $D'$ if $d\cap d'\neq\emptyset$, then $\chi(d)\neq\chi(d')$,
where $C$ denotes a set containing $k$ distinct colors.
</p>
<p>For the {\it $k$-CDUDC} problem, our proposed algorithms approximate the
number of colors used in the coloring if there exists a $k$-colorable cover. We
first propose a 4-approximation algorithm in $O(m^{7k}n\log k)$ time for this
problem, where $k$ is a positive integer. The previous best known result for
the problem when $k=3$ is due to the recent work of Biedl et al. [CCCG 2019],
who proposed a 2-approximation algorithm in $O(m^{25}n)$ time. For $k=3$, our
algorithm runs in $O(m^{21}n)$ time, faster than the previous best algorithm,
but gives a 4-approximate result. We then generalize our approach to yield a
family of $\rho$-approximation algorithms in $O(m^{\alpha k}n\log k)$ time,
where $(\rho,\alpha)\in \{(4, 7), (6,5), (7, 5), (9,4)\}$. We further
generalize this to exhibit a $O(\frac{1}{\tau})$-approximation algorithm in
$O(m^{\alpha k}n\log k)$ time for a given grid width $1 \leq \tau \leq 2$,
where $\alpha=O(\tau^2)$. We also extend our algorithm to solve the {\it
$k$-Colorable Line Segment Disk Cover ({\it $k$-CLSDC})} and {\it $k$-Colorable
Rectangular Region Cover ({\it $k$-CRRC})} problems, in which instead of the
set $P$ of $n$ points, we are given a set $S$ of $n$ line segments, and a
rectangular region $\cal R$, respectively.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.00207"><span class="datestr">at April 02, 2021 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.00104">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.00104">Vertex Connectivity in Poly-logarithmic Max-flows</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jason.html">Jason Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nanongkai:Danupon.html">Danupon Nanongkai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Panigrahi:Debmalya.html">Debmalya Panigrahi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saranurak:Thatchaphol.html">Thatchaphol Saranurak</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yingchareonthawornchai:Sorrachai.html">Sorrachai Yingchareonthawornchai</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.00104">PDF</a><br /><b>Abstract: </b>The vertex connectivity of an $m$-edge $n$-vertex undirected graph is the
smallest number of vertices whose removal disconnects the graph, or leaves only
a singleton vertex. In this paper, we give a reduction from the vertex
connectivity problem to a set of maxflow instances. Using this reduction, we
can solve vertex connectivity in $\tilde O(m^{\alpha})$ time for any $\alpha
\ge 1$, if there is a $m^{\alpha}$-time maxflow algorithm. Using the current
best maxflow algorithm that runs in $m^{4/3+o(1)}$ time (Kathuria, Liu and
Sidford, FOCS 2020), this yields a $m^{4/3+o(1)}$-time vertex connectivity
algorithm. This is the first improvement in the running time of the vertex
connectivity problem in over 20 years, the previous best being an $\tilde
O(mn)$-time algorithm due to Henzinger, Rao, and Gabow (FOCS 1996). Indeed, no
algorithm with an $o(mn)$ running time was known before our work, even if we
assume an $\tilde O(m)$-time maxflow algorithm. Our new technique is robust
enough to also improve the best $\tilde O(mn)$-time bound for directed vertex
connectivity to $mn^{1-1/12+o(1)}$ time
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.00104"><span class="datestr">at April 02, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2104.00034">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2104.00034">Approximation Schemes for Multiperiod Binary Knapsack Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gao:Zuguang.html">Zuguang Gao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Birge:John_R=.html">John R. Birge</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Varun.html">Varun Gupta</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2104.00034">PDF</a><br /><b>Abstract: </b>An instance of the multiperiod binary knapsack problem (MPBKP) is given by a
horizon length $T$, a non-decreasing vector of knapsack sizes $(c_1, \ldots,
c_T)$ where $c_t$ denotes the cumulative size for periods $1,\ldots,t$, and a
list of $n$ items. Each item is a triple $(r, q, d)$ where $r$ denotes the
reward of the item, $q$ its size, and $d$ its time index (or, deadline). The
goal is to choose, for each deadline $t$, which items to include to maximize
the total reward, subject to the constraints that for all $t=1,\ldots,T$, the
total size of selected items with deadlines at most $t$ does not exceed the
cumulative capacity of the knapsack up to time $t$. We also consider the
multiperiod binary knapsack problem with soft capacity constraints (MPBKP-S)
where the capacity constraints are allowed to be violated by paying a penalty
that is linear in the violation. The goal is to maximize the total profit,
i.e., the total reward of selected items less the total penalty. Finally, we
consider the multiperiod binary knapsack problem with soft stochastic capacity
constraints (MPBKP-SS), where the non-decreasing vector of knapsack sizes
$(c_1, \ldots, c_T)$ follow some arbitrary joint distribution but we are given
access to the profit as an oracle, and we choose a subset of items to maximize
the total expected profit, i.e., the total reward less the total expected
penalty. For MPBKP, we exhibit a fully polynomial-time approximation scheme
with runtime
$\tilde{\mathcal{O}}\left(\min\left\{n+\frac{T^{3.25}}{\epsilon^{2.25}},n+\frac{T^{2}}{\epsilon^{3}},\frac{nT}{\epsilon^2},\frac{n^2}{\epsilon}\right\}\right)$
that achieves $(1+\epsilon)$ approximation; for MPBKP-S, the $(1+\epsilon)$
approximation can be achieved in $\mathcal{O}\left(\frac{n\log
n}{\epsilon}\cdot\min\left\{\frac{T}{\epsilon},n\right\}\right)$; for MPBKP-SS,
a greedy algorithm is a 2-approximation when items have the same size.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2104.00034"><span class="datestr">at April 02, 2021 10:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8044">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/04/01/robustness-in-train-and-test-time/">Robustness in train and test time</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>Scribe notes by <a href="https://www.media.mit.edu/people/vepakom/overview/">Praneeth Vepakomma</a></em></p>



<p><strong>Previous post:</strong> <a href="https://windowsontheory.org/2021/02/24/unsupervised-learning-and-generative-models/">Unsupervised learning and generative models</a> <strong>Next post:</strong> <a href="https://windowsontheory.org/2021/04/02/inference-and-statistical-physics/">Inference and statistical physics</a>. See also <a href="https://windowsontheory.org/category/ml-theory-seminar/">all seminar posts</a> and <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">course webpage</a>.</p>



<p><a href="http://files.boazbarak.org/misc/mltheory/ML_seminar_lecture4.pdf">lecture slides (pdf)</a> – <a href="http://files.boazbarak.org/misc/mltheory/ML_seminar_lecture4.pptx">lecture slides (Powerpoint with animation and annotation)</a> – <a href="https://harvard.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=1186415c-f0f1-445c-886f-acd701757cb7">video</a></p>



<p>In this blog post, we will focus on the topic of robustness – how well (or not so well) do machine learning algorithms perform when either their training or testing/deployment data differs from our expectations.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/jmDvjav.png" alt="" /></figure>



<p>We will cover the following areas:</p>



<ol><li>Math/stat refresher</li><li>Multiplicative weights algorithm</li><li>Robustness<ul><li>train-time<ul><li>robust statistics</li><li>robust mean estimation</li><li>data poisoning</li></ul></li><li>test-time<ul><li>distribution shifts</li><li>adversarial perturbations</li></ul></li></ul></li></ol>



<h2><img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f4dd.png" style="height: 1em;" class="wp-smiley" alt="📝" />Math/stat refresher</h2>



<h3>KL refresher</h3>



<p>The KL-divergence between the probability distributions is the expectation of the log of probability ratios:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/aWTh2Lc.png" alt="" /></figure>



<p>KL-divergence is always non-negative and can be decomposed as the difference between negative entropy and cross-entropy. The non-negativity property thereby implies that <img src="https://s0.wp.com/latex.php?latex=%5Cforall&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\forall" class="latex" /> distributions <img src="https://s0.wp.com/latex.php?latex=p%2Cq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p,q" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=H%28p%2Cq%29+%5Cgeq+H%28p%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(p,q) \geq H(p)." class="latex" /></p>



<h3>Introduction to concentration</h3>



<p>Consider a Gaussian random variable <img src="https://s0.wp.com/latex.php?latex=X+%5Csim+N%28%5Cmu%2C+%5Csigma%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X \sim N(\mu, \sigma^2)" class="latex" />. Concentration is the phenomenon that if you have i.i.d random variables <img src="https://s0.wp.com/latex.php?latex=X_1%2CX_2%2C+%5Cldots%2CX_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X_1,X_2, \ldots,X_n" class="latex" /> with expectation <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu" class="latex" />, then the empirical average <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Csum_i+X_i%7D%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{\sum_i X_i}{n}" class="latex" /> is distributed approximately like <img src="https://s0.wp.com/latex.php?latex=N%28%5Cmu%2C+%5Cfrac%7B1%7D%7Bn%7D%29%3D%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7DN%28%5Cmu%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(\mu, \frac{1}{n})=\frac{1}{\sqrt{n}}N(\mu,1)" class="latex" /><br />Therefore the standard deviation of the empricial average is smaller than the standard deviation of each of the <img src="https://s0.wp.com/latex.php?latex=Y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y_i" class="latex" />‘s. The central limit theorem ensure this asymptotically w.r.t <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" />, while non-asymptotic versions of this phenomenon can be seen via popular inequalities such as the Chernoff/Hoeffding/Bernstein styled inequalities. These roughly have the form <img src="https://s0.wp.com/latex.php?latex=P%5B%7C%5Csum_i+X_i+-+%5Cmu.n%7C+%5Cgeq+%5Cepsilon+n%5D+%5Capprox+%5Cexp%28-%5Cepsilon%5E2+n%29+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P[|\sum_i X_i - \mu.n| \geq \epsilon n] \approx \exp(-\epsilon^2 n) " class="latex" /></p>



<h3>Matrix notations</h3>



<h4>Norms</h4>



<p>We denote the spectral norm of a matrix <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf{A}" class="latex" /> by <img src="https://s0.wp.com/latex.php?latex=%5Cleft+%7C+%5Cmathbf%7BA%7D+%5Cright%7C+%3D+%5Cunderset%7B%7C+v%7C+%3D+1%7D%7B%5Cmax%7D%5Cleft%7C+%5Cmathbf%7BA%7Dv+%5Cright%7C+%3D+%5Clambda_%7B%5Cmax%7D%28%5Cmathbf%7BA%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left | \mathbf{A} \right| = \underset{| v| = 1}{\max}\left| \mathbf{A}v \right| = \lambda_{\max}(\mathbf{A})" class="latex" /> and the Frobenius norm by <img src="https://s0.wp.com/latex.php?latex=%5Cleft+%7C+%5Cmathbf%7BA%7D+%5Cright%7C_F+%3D+%5Csqrt%7B%5Csum%5Cmathbf%7BA%7D_%7Bij%7D%5E2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left | \mathbf{A} \right|_F = \sqrt{\sum\mathbf{A}_{ij}^2}" class="latex" />.</p>



<h4>PSD Ordering</h4>



<p>We use the matrix ordering <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D+%5Cpreceq+%5Cmathbf%7BB%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf{A} \preceq \mathbf{B}" class="latex" /> if <img src="https://s0.wp.com/latex.php?latex=v%5ET%5Cmathbf%7BA%7D+v+%5Cleq+v%5ET%5Cmathbf%7BB%7D+v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v^T\mathbf{A} v \leq v^T\mathbf{B} v" class="latex" /> for all vectors <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" />. We similarly refer to <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D+%5Cin+%5Ba%2Cb%5D+%5Cmathbf%7BI%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf{A} \in [a,b] \mathbf{I}" class="latex" /> if <img src="https://s0.wp.com/latex.php?latex=a%5Cmathbf%7BI%7D+%5Cpreceq+%5Cmathbf%7BA%7D+%5Cpreceq+b%5Cmathbf%7BI%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a\mathbf{I} \preceq \mathbf{A} \preceq b\mathbf{I}" class="latex" />.</p>



<h4>Matrix/vector valued random variables</h4>



<p><strong>Vector valued normals:</strong> If <img src="https://s0.wp.com/latex.php?latex=%5Cmu+%5Cin+%5Cmathbb%7BR%7D%5E%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu \in \mathbb{R}^{d}" class="latex" /> is a vector, and <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BV%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bd+%5Ctimes+d%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf{V} \in \mathbb{R}^{d \times d}" class="latex" /> is a psd covariance matrix, with <img src="https://s0.wp.com/latex.php?latex=x+%5Csim+N%28%5Cmu%2C+%5Cmathbf%7BV%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x \sim N(\mu, \mathbf{V})" class="latex" /> normal over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{R}^{d}" class="latex" /> with</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Cleft%5Bx_%7Bi%7D%5Cright%5D%3D%5Cmu_%7Bi%7D%2C+%5C%3B%5C%3B+%5Cmathbb%7BE%7D%5Cleft%5B%5Cleft%28x_%7Bi%7D-%5Cmu_%7Bi%7D%5Cright%29%5Cleft%28x_%7Bj%7D-%5Cmu_%7Bj%7D%5Cright%29%5Cright%5D%3D%5Cmathbf%7BV%7D_%7Bi%2C+j%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}\left[x_{i}\right]=\mu_{i}, \;\; \mathbb{E}\left[\left(x_{i}-\mu_{i}\right)\left(x_{j}-\mu_{j}\right)\right]=\mathbf{V}_{i, j}" class="latex" /></p>



<p>For every psd matrix V, there exists a Normal distribution where<br /><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Cleft%5B%5Cleft%28x_%7Bi%7D-%5Cmu_%7Bi%7D%5Cright%29%5Cleft%28x_%7Bj%7D-%5Cmu_%7Bj%7D%5Cright%29%5Cright%5D%3D%5Cmathbf%7BV%7D_%7Bi%2C+j%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}\left[\left(x_{i}-\mu_{i}\right)\left(x_{j}-\mu_{j}\right)\right]=\mathbf{V}_{i, j}" class="latex" /></p>



<p><strong>Standard vector-valued normal:</strong> This refers to the case when <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bx%7D+%5Csim+N%5Cleft%280%5E%7Bd%7D%2C+I_%7Bd%7D%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathrm{x} \sim N\left(0^{d}, I_{d}\right)" class="latex" /> (or <img src="https://s0.wp.com/latex.php?latex=%5Cleft.%5Cmathrm%7Bx%7D+%5Csim+N%280%2C+I%29%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left.\mathrm{x} \sim N(0, I)\right)" class="latex" /> and</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D+x%3D%5Coverrightarrow%7B0%7D%2C%5C%3B%5C%3B+%5Cmathbb%7BE%7D%5Cleft%5Bx+x%5E%7B%5Ctop%7D%5Cright%5D%3DI+%5Cquad%5Cleft%28%5Cbegin%7Barray%7D%7Bl%7D+%5Cmathbb%7BE%7D+x_%7Bi%7D%5E%7B2%7D%3D1+%5C+%5Cmathbb%7BE%7D+x_%7Bi%7D+x_%7Bj%7D%3D0+%5Ctext+%7B+for+%7D+i+%5Cneq+j+%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E} x=\overrightarrow{0},\;\; \mathbb{E}\left[x x^{\top}\right]=I \quad\left(\begin{array}{l} \mathbb{E} x_{i}^{2}=1 \ \mathbb{E} x_{i} x_{j}=0 \text { for } i \neq j \end{array}\right)" class="latex" /></p>



<h3>Matrix concentration</h3>



<p>For scalar random variables, we saw that if <img src="https://s0.wp.com/latex.php?latex=Y_%7B1%7D%2C+%5Cldots%2C+Y_%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y_{1}, \ldots, Y_{n}" class="latex" /> are i.i.d over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{R}" class="latex" /> bounded with expectation <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu" class="latex" /> then</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BPr%7D%5Cleft%5B%5Cleft%7C%5Csum+Y_%7Bi%7D-%5Cmu+%5Ccdot+n%5Cright%7C+%5Cgeq+%5Cepsilon+n%5Cright%5D+%5Capprox+%5Cexp+%5Cleft%28-%5Cepsilon%5E%7B2%7D+n%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathrm{Pr}\left[\left|\sum Y_{i}-\mu \cdot n\right| \geq \epsilon n\right] \approx \exp \left(-\epsilon^{2} n\right)" class="latex" /></p>



<p>If the random variables were vectors instead, we can easily generalize this to <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> samples of <img src="https://s0.wp.com/latex.php?latex=Y_i+%5Cin+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y_i \in \mathbb{R}^d" class="latex" />. Generalizing to matrices is far more interesting, especially when the norm under consideration is the spectral norm instead of the Frobenius norm.</p>



<h4>Matrix Bernstein inequality:</h4>



<p>If <img src="https://s0.wp.com/latex.php?latex=Y_%7B1%7D%2C+%5Cldots%2C+Y_%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y_{1}, \ldots, Y_{n}" class="latex" /> i.i.d symmetric matrices in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7Bd+%5Ctimes+d%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{R}^{d \times d}" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D+Y_%7Bi%7D%3D%5Cmu%2C%5Cleft%7CY_%7Bi%7D%5Cright%7C+%5Cleq+O%281%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E} Y_{i}=\mu,\left|Y_{i}\right| \leq O(1)" class="latex" /> (i.e bounded in spectral norm), then</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BPr%7D%5Cleft%5B%5Cleft%7C%5Csum+Y_%7Bi%7D-%5Cmu+%5Ccdot+n%5Cright%7C+%5Cgeq+%5Cepsilon+n%5Cright%5D+%5Capprox+d+%5Ccdot+%5Cexp+%5Cleft%28-%5Cepsilon%5E%7B2%7D+n%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathrm{Pr}\left[\left|\sum Y_{i}-\mu \cdot n\right| \geq \epsilon n\right] \approx d \cdot \exp \left(-\epsilon^{2} n\right)" class="latex" /></p>



<p>Note that <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu" class="latex" /> is a matrix in this case, instead. The norm under consideration is the spectral norm. Note that the difference in the inequality w.r.t the scalar case is the additional multiplicative factor of d or an additive factor of <img src="https://s0.wp.com/latex.php?latex=%5Clog%28d%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log(d)" class="latex" /> in the exponent as in the equations above. Please refer to Tropp, 2015 Chapter 6 for formally precise statements.</p>



<p>Another property that follow is the the expected norm of the difference follows<br /><img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Cleft%7C%5Csum+Y_%7Bi%7D-%5Cmu%5Cright%7C+%5Cleq+O%28%5Csqrt%7Bn+%5Clog+d%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}\left|\sum Y_{i}-\mu\right| \leq O(\sqrt{n \log d})" class="latex" /></p>



<p>There are some long-standing conjectures and results on cases when one can get rid of this additional factor of <img src="https://s0.wp.com/latex.php?latex=%5Clog%28d%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log(d)" class="latex" />.</p>



<p><strong>Trivia on log factors:</strong> For example as a tangent (as well as some trivia!) on some results of this flavor, where a <img src="https://s0.wp.com/latex.php?latex=%5Clog&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log" class="latex" /> factor was replaced by a constant, includes Spencer’s paper from 1985 titled <a href="https://www.ams.org/journals/tran/1985-289-02/S0002-9947-1985-0784009-0/home.html">‘Six standard deviations suffice’</a>, which showed that a constant of 6 would suffice in certain bounds where a naive result would instead give a <img src="https://s0.wp.com/latex.php?latex=%5Clog%28n%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log(n)" class="latex" /> dependency. The <a href="https://en.wikipedia.org/wiki/Kadison%E2%80%93Singer_problem">Kadison-singer problem</a> (by Spielman-Srivastava) and the <a href="https://arxiv.org/abs/1809.04726">Paulsen problem</a> are two other examples of works in this flavor.</p>



<h4>Random matrices</h4>



<p>For a random <img src="https://s0.wp.com/latex.php?latex=d+%5Ctimes+d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d \times d" class="latex" /> symmetric matrix matrix <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf{A}" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D_%7Bi%2Cj%7D+%5Csim+N%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf{A}_{i,j} \sim N(0,1)" class="latex" />, the spectrum of eigenvalues is distributed according to <strong>Wigner semi-circle law</strong> on a support between <img src="https://s0.wp.com/latex.php?latex=%5B-2%5Csqrt%7Bd%7D%2C%2B2%5Csqrt%7Bd%7D%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="[-2\sqrt{d},+2\sqrt{d}]" class="latex" /> as shown in the figure below. Note that most mass is observed close to <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" />.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/mRmGRUG.png" alt="" /></figure>



<h4>Marchenko-Pastur distribution (for random empirical covariance matrices)</h4>



<p>For <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX+%3D+%5Cfrac%7B1%7D%7Bn%7D+AA%5ET%2C+A%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bd%5Ctimes+n%7D%2C+%5Cmathbf%7BA%7D_%7Bij%7D+%5Csim+N%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf{X = \frac{1}{n} AA^T, A} \in \mathbb{R}^{d\times n}, \mathbf{A}_{ij} \sim N(0,1)" class="latex" /><br />we have <img src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbf{X}" class="latex" /> is the empirical estimate for covariance of <img src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n+%5Csim++N%280%2C%5Cmathbf%7BI%7D_d%29+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_1,\ldots,x_n \sim  N(0,\mathbf{I}_d) " class="latex" /><br />the eigenvalues are distributed according to the Marchenko-Pastur distribution. In the case when, <img src="https://s0.wp.com/latex.php?latex=d+%3C+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d &lt; n" class="latex" /> , the eigenvalues will be bounded away from zero. When <img src="https://s0.wp.com/latex.php?latex=d+%5Capprox+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d \approx n" class="latex" /> , then it has a lot more mass close to <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" /> (way more than the semi-circle law). When <img src="https://s0.wp.com/latex.php?latex=d+%3E+n+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d &gt; n " class="latex" /> , then there is a spike of <img src="https://s0.wp.com/latex.php?latex=d-n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d-n" class="latex" /> eigenvalues at <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" /> and the rest are bounded away from <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" />.</p>



<p><strong>Connections to the stability of linear regression</strong><br />In the regime, when <img src="https://s0.wp.com/latex.php?latex=d+%5Capprox+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d \approx n" class="latex" />, linear regression is most unstable, which is the case when most of the eigenvalues of the empirical covariance are close to <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" />. When <img src="https://s0.wp.com/latex.php?latex=d%3En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d&gt;n" class="latex" />, although linear regression is over-parametrized, the solution is not exact, but the approximate solution is still pretty stable as in the case of <img src="https://s0.wp.com/latex.php?latex=d+%3C+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d &lt; n" class="latex" />. When <img src="https://s0.wp.com/latex.php?latex=d%5Cgg+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d\gg n" class="latex" />, the condition number is infinite, and therefore we use a pseudo-inverse as opposed to the usual inverse in computing the solution for linear regression. This ignores the subspace on which the matrix has <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" /> eigenvalues while the inverse is performed. The condition number of the subspace of the non-zero eigenvectors is finite.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/nXdo5vB.png" alt="" /></figure>



<h2>Digression: Multiplicative Weights Algorithm</h2>



<p><strong>Note:</strong> Its variants/connections include techniques/topics of, Follow The Regularized Leader / Regret Minimization / Mirror Descent. Elad Hazan’s <a href="https://arxiv.org/abs/1909.03550">lecture notes</a> on online convex optimization and <a href="https://theoryofcomputing.org/articles/v008a006/">Arora, Hazan, Kale’s article</a> on multiplicative and matrix multiplicative weights are a great reading source for these topics.</p>



<p>The following is a general setup in online optimization and online learning.</p>



<p><strong>Setup:</strong> <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n" class="latex" /> possible actions <img src="https://s0.wp.com/latex.php?latex=a_%7B1%7D%2C+%5Cldots%2C+a_%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a_{1}, \ldots, a_{n}" class="latex" /></p>



<p>At time <img src="https://s0.wp.com/latex.php?latex=t%3D1%2C2%2C+%5Cldots%2C+T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t=1,2, \ldots, T" class="latex" />, we incur loss <img src="https://s0.wp.com/latex.php?latex=L_%7Bi%2C+t%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L_{i, t}" class="latex" /> for action <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i" class="latex" /> at time <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t" class="latex" />. After this action, we also learn the loss for actions we did not take. (This is referred to as the <em>experts</em> model in online learning, in contrast to the <em>bandit</em> model where we only learn the loss for the action taken; the experts model is an easier setup than the bandit model.)</p>



<p>The following is the overall approach for learning to optimize in this online setup.</p>



<p><strong>Overall Approach:</strong></p>



<ul><li>Initialize <img src="https://s0.wp.com/latex.php?latex=p_%7B0%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_{0}" class="latex" /> distribution over action space <img src="https://s0.wp.com/latex.php?latex=%5Bn%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="[n]" class="latex" />. We then take a step based on this distribution and observe the incurred loss.</li><li>Then the distribution is updated as <img src="https://s0.wp.com/latex.php?latex=p_%7Bt%2B1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_{t+1}" class="latex" /> by letting <img src="https://s0.wp.com/latex.php?latex=p_%7Bt%2B1%7D%28i%29+%5Cpropto+p_%7Bt%7D%28i%29+%5Cexp+%5Cleft%28-%5Ceta+L_%7Bi%2C+t%7D%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_{t+1}(i) \propto p_{t}(i) \exp \left(-\eta L_{i, t}\right)" class="latex" />. i.e, if an action gave a bad outcome in terms of loss, we downweight it’s probability for the next draw of action to be taken. <img src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta" class="latex" /> is the penalty terms that governs the aggresiveness of the downweighting.</li></ul>



<p>The <em>hope</em> is that this approach converges to a “good aggregation or strategy”. We measure the quality of the strategy using <strong>regret</strong>.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/wkWExIB.png" alt="" /></figure>



<p>The difference between the cost we incur and the optimal action (or probability distribution over actions) in hindsight is known as the (average) <em>regret</em>.</p>



<p>Note that we compare the loss we incur with the best loss over a <em>fixed</em> (i.e., nonadaptive) probability distribution over the set of possible actions. It can be shown that the best such loss can always be achieved by a distribution that puts all the weight on a single action.</p>



<p>The following theorem bounds the regret.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/waOIq4F.png" alt="" /></figure>



<p>The ‘prior ignorance’ term captures how good the initialization is. The ‘sensitivity per step’ term captures how aggressive the exploration is (i.e this term governs the potential difference between loss incurred at <img src="https://s0.wp.com/latex.php?latex=p_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_t" class="latex" /> versus <img src="https://s0.wp.com/latex.php?latex=p_%7Bt-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_{t-1}" class="latex" />). As <img src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta" class="latex" /> also occurs in the denominator of the prior ignorance term, it needs to be carefully chosen to balance the two terms properly.</p>



<p>The first inequality of this theorem is always true for any <img src="https://s0.wp.com/latex.php?latex=p%5Cast%2C+p_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p\ast, p_0" class="latex" /> whether <img src="https://s0.wp.com/latex.php?latex=p%5Cast&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p\ast" class="latex" /> is optimal strategy or not. The second (right-most) inequality is true when optimal <img src="https://s0.wp.com/latex.php?latex=p%5Cast&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p\ast" class="latex" /> is the delta function (which it will be in optimal strategy), <img src="https://s0.wp.com/latex.php?latex=p_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_0" class="latex" /> is initialized to uniform distribution, and <img src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\eta" class="latex" /> is set to be <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B%5Cfrac%7B%5Clog%28n%29%7D%7Bt%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sqrt{\frac{\log(n)}{t}}" class="latex" />. Note that in this case, the divergence <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%5Cast+%7C+p_0%29+%3D+%5Clog+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(p\ast | p_0) = \log n" class="latex" />.</p>



<p>To state the ‘overall approach’, more precisely, there needs to be a normalization <img src="https://s0.wp.com/latex.php?latex=Z_%7Bt%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z_{t}" class="latex" /> at every step as below<br /><img src="https://s0.wp.com/latex.php?latex=p_%7Bt%2B1%7D%28i%29%3Dp_%7Bt%7D%28i%29+%5Cexp+%5Cleft%28-%5Ceta+L_%7Bi%2C+t%7D%5Cright%29+%2F+Z_%7Bt%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_{t+1}(i)=p_{t}(i) \exp \left(-\eta L_{i, t}\right) / Z_{t}" class="latex" /><br />The above can be rearranged as follows upon taking log<br /><img src="https://s0.wp.com/latex.php?latex=L_%7Bt%2C+i%7D%3D%5Cfrac%7B1%7D%7B%5Ceta%7D+%5Clog+%5Cfrac%7Bp_%7Bt%7D%28i%29%7D%7Bp_%7Bt%2B1%7D%28i%29+%5Ccdot+Z_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L_{t, i}=\frac{1}{\eta} \log \frac{p_{t}(i)}{p_{t+1}(i) \cdot Z_{i}}" class="latex" /></p>



<p>By substituting this in below</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/jtzs3cB.png" alt="" /></figure>



<p>we get the following expanded version of what the regret amounts to be:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/sGwn0Xq.png" alt="" /></figure>



<p>The last equation above is due to the telescoping sum where adjacent terms cancel out except the first and last terms.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/kU2Mmfr.png" alt="" /></figure>



<p>The last inequality here is because the cross-entropy is always at least as large as the entropy.</p>



<p>So now we have this so far</p>



<p>PF: Regret <img src="https://s0.wp.com/latex.php?latex=%5Cleq+%5Cfrac%7B%5CDelta_%7BK+L%7D%5Cleft%28p%5E%7B%5Cast%7D+%7C+p_%7B0%7D%5Cright%29%7D%7B%5Ceta+%5Ccdot+T%7D%2B%5Cfrac%7B1%7D%7B%5Ceta+%5Ccdot+T%7D+%5Csum_%7Bt%7D+%5CDelta_%7BK+L%7D%5Cleft%28p_%7Bt%7D%2C+p_%7Bt%2B1%7D%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\leq \frac{\Delta_{K L}\left(p^{\ast} | p_{0}\right)}{\eta \cdot T}+\frac{1}{\eta \cdot T} \sum_{t} \Delta_{K L}\left(p_{t}, p_{t+1}\right)" class="latex" /></p>



<p>Now there is this property that if <img src="https://s0.wp.com/latex.php?latex=p%2C+q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p, q" class="latex" /> s.t. <img src="https://s0.wp.com/latex.php?latex=p%28i%29+%5Cpropto+q%28i%29+%5Crho_%7Bi%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(i) \propto q(i) \rho_{i}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=%5Crho_%7Bi%7D+%5Cin%5B1-%5Ceta%2C+1%2B%5Ceta%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\rho_{i} \in[1-\eta, 1+\eta]" class="latex" /> then <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BK+L%7D%28p+%7C+q%29+%5Cleq+O%5Cleft%28%5Ceta%5E%7B2%7D%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{K L}(p | q) \leq O\left(\eta^{2}\right)" class="latex" /><br />Therefore upon substituting this, we prove the upper bound stated over the regret.</p>



<p>This subclaim that was used can be proved as follows</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/pgcdG5S.png" alt="" /></figure>



<h3>Generalization of multiplicative weights: Follow The Regularized Leader (FTRL)</h3>



<p>When the set K of actions is convex (set of probability distributions on discrete actions is convex),</p>



<p>At time <img src="https://s0.wp.com/latex.php?latex=t%2B1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t+1" class="latex" />, it makes a choice <img src="https://s0.wp.com/latex.php?latex=x_%7Bt%2B1%7D+%5Cin+K&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_{t+1} \in K" class="latex" /> and learns cost function <img src="https://s0.wp.com/latex.php?latex=L_%7Bt%2B1%7D%3A+K+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L_{t+1}: K \rightarrow \mathbb{R}" class="latex" /> (so the setup is again like the experts model unlike the bandit model).</p>



<p>Now the new action in FTRL is based on the following optimization which is a regularized (with <img src="https://s0.wp.com/latex.php?latex=R%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="R(x)" class="latex" />) loss:<br />FTRL: <img src="https://s0.wp.com/latex.php?latex=x_%7Bt%2B1%7D%3D%5Carg+%5Cmin_%7Bx+%5Cin+R%7D%5Cleft%28R%28x%29%2B%5Csum_%7Bi%3D1%7D%5E%7Bt%7D+L_%7Bi%7D%28x%29%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_{t+1}=\arg \min_{x \in R}\left(R(x)+\sum_{i=1}^{t} L_{i}(x)\right)" class="latex" /></p>



<p>In this case, the regret bound is given by the following theorem.<br /><img src="https://i.imgur.com/lglCvZh.png" alt="" /></p>



<p><img src="https://s0.wp.com/latex.php?latex=x%5E%5Cast&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^\ast" class="latex" /> refers to the optimal choice. This indeed has a similar flavor to the theorem we saw for multiplicative weights. To be precise when<br /><img src="https://s0.wp.com/latex.php?latex=K%3D%7B%5Ctext+%7B+set+of+all+distributions+on+action+space+%7D%5Bn%5D%7D%2C+%5Ctext+%7Band%7D+%5C%3B+R%28x%29%3D-%5Cfrac%7B1%7D%7B%5Ceta%7D+H%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="K={\text { set of all distributions on action space }[n]}, \text {and} \; R(x)=-\frac{1}{\eta} H(x)" class="latex" /> we have that the multiplicate weights becomes FTRL. Similarly there is a view that connects multiplicative weights to mirror descent. (Nisheeth Vishnoi has <a href="https://nisheethvishnoi.wordpress.com/convex-optimization/">notes</a> on this.)</p>



<h2>Train-Time Robustness</h2>



<p>We now look at some robustness issues specific to the <em>training</em> phase in machine learning.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/7DlRj3k.png" alt="" /></figure>



<p>For example, during training, there could be adversarially poisoned examples in the training dataset that damage the trained model’s performance.</p>



<p><strong>Setup:</strong> Suppose <img src="https://s0.wp.com/latex.php?latex=1-%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1-\epsilon" class="latex" /> samples are generated from a genuine data distribution and <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon" class="latex" /> samples are maliciously chosen from an arbitrary distribution. i.e, in formal notation: <img src="https://s0.wp.com/latex.php?latex=x_%7B1%7D%2C+%5Cldots%2C+x_%7B%281-%5Cepsilon%29+n%7D+%5Csim+X+%5Csubseteq+%5Cmathbb%7BR%7D%5E%7Bd%7D%2C+%5C%3B%5C%3B+%5Ctext+%7Band+%7D+x_%7B%281-%5Cepsilon%29+n%2B1%7D%2C+%5Cldots%2C+x_%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_{1}, \ldots, x_{(1-\epsilon) n} \sim X \subseteq \mathbb{R}^{d}, \;\; \text {and } x_{(1-\epsilon) n+1}, \ldots, x_{n}" class="latex" /> are arbitrary. (While for convenience we assume here that the last <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon n" class="latex" /> items are maliciously chosen, the learning algorithm does not get the data in order.)</p>



<p>Assume <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7Cx_%7Bi%7D%5Cright%7C%5E%7B2%7D+%5Capprox+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left|x_{i}\right|^{2} \approx 1" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=i%3C%281-%5Cepsilon%29+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i&lt;(1-\epsilon) n" class="latex" /></p>



<p>Let us start with the task of estimating the mean under this setup.</p>



<p><strong>Mean estimation:</strong> Estimate <img src="https://s0.wp.com/latex.php?latex=%5Cmu+%3D+%5Cmathbb%7BE%7DX&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu = \mathbb{E}X" class="latex" /></p>



<p><strong>Noiseless case:</strong> In the noiseless case, the best estimate for the mean under many measures is the <em>empirical mean</em> <img src="https://s0.wp.com/latex.php?latex=%5Chat%7B%5Cmu%7D%3D%5Cfrac%7B1%7D%7Bn%7D%5Csum_i+X_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\hat{\mu}=\frac{1}{n}\sum_i X_i" class="latex" /></p>



<p>Standard concentration results show that for <img src="https://s0.wp.com/latex.php?latex=d%3D1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d=1" class="latex" /> <img src="https://s0.wp.com/latex.php?latex=%7C%5Chat%7B%5Cmu%7D-%5Cmu%7C+%5Cleq+O%281+%2F+%5Csqrt%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|\hat{\mu}-\mu| \leq O(1 / \sqrt{n})" class="latex" /> and for a general <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" /> that <img src="https://s0.wp.com/latex.php?latex=%7C%5Chat%7B%5Cmu%7D-%5Cmu%7C+%5Cleq+O%28%5Csqrt%7Bd+%2F+n%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|\hat{\mu}-\mu| \leq O(\sqrt{d / n})" class="latex" />.</p>



<p><strong>Adversarial case:</strong> If there is even a single malicious sample (that can be arbitrarily large or small) then it is well known that the empirical mean can give an arbitrarily bad approximation of the true population mean. For <img src="https://s0.wp.com/latex.php?latex=d%3D1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d=1" class="latex" /> we can use the <em>empirical median</em> <img src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%7B%5Cast%7D%3D%5Cmathrm%7Bsort%7D%5Cleft%28x_%7B1%7D%2C+%5Cldots%2C+x_%7Bn%7D%5Cright%29_%7Bn+%2F+2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu^{\ast}=\mathrm{sort}\left(x_{1}, \ldots, x_{n}\right)_{n / 2}" class="latex" />. This is guaranteed to lie in the <img src="https://s0.wp.com/latex.php?latex=%5Cleft%28%5Cfrac%7B1%7D%7B2%7D-%5Cepsilon%2C+%5Cfrac%7B1%7D%7B2%7D%2B%5Cepsilon%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left(\frac{1}{2}-\epsilon, \frac{1}{2}+\epsilon\right)" class="latex" /> quantile of real data. This is most optimal because of the property that <img src="https://s0.wp.com/latex.php?latex=X+%5Csim+N%28%5Cmu%2C+1%29%2C%5Cleft%7C%5Cmu-%5Cmu%5E%7B%5Cast%7D%5Cright%7C+%5Cleq+O%28%5Cepsilon%2B1+%2F+%5Csqrt%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X \sim N(\mu, 1),\left|\mu-\mu^{\ast}\right| \leq O(\epsilon+1 / \sqrt{n})" class="latex" />.</p>



<p>In a higher-dimension, this story instead goes as follows:</p>



<p><strong>Setup:</strong> <img src="https://s0.wp.com/latex.php?latex=x_%7B1%7D%2C+%5Cldots%2C+x_%7B%281-%5Cepsilon%29+n%7D+%5Csim+X+%5Csubseteq+%5Cmathbb%7BR%7D%5E%7Bd%7D%2C+%5Cquad+x_%7B%281-%5Cepsilon%29+n%2B1%7D%2C+%5Cldots%2C+x_%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_{1}, \ldots, x_{(1-\epsilon) n} \sim X \subseteq \mathbb{R}^{d}, \quad x_{(1-\epsilon) n+1}, \ldots, x_{n}" class="latex" /> arbitrary.</p>



<p><strong>Median of coordinates (a starter solution):</strong> When <img src="https://s0.wp.com/latex.php?latex=%28d+%5Cgeq+1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(d \geq 1)" class="latex" />, we have <img src="https://s0.wp.com/latex.php?latex=%5Cmu_%7Bi%7D%5E%7B%5Cast%7D%3D%5Cmathrm%7Bsort%7D%5Cleft%28x_%7B1%2C+i%7D%2C+%5Cldots%2C+x_%7Bn%2C+i%7D%5Cright%29_%7Bn+%2F+2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu_{i}^{\ast}=\mathrm{sort}\left(x_{1, i}, \ldots, x_{n, i}\right)_{n / 2}" class="latex" /> per coordinate as an initial naive solution. (i.e a median of coordinates). In this case, say if we have <img src="https://s0.wp.com/latex.php?latex=X+%5Csim+N%28%5Cmu%2C+I%29%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X \sim N(\mu, I)," class="latex" /> then an adversary can perurb the data to make <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon" class="latex" /> fraction of the points be <img src="https://s0.wp.com/latex.php?latex=%28%5Cmu_1%2BM%2C%5Cmu_2%2BM%2C%5Cldots%2C%5Cmu_d%2BM%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(\mu_1+M,\mu_2+M,\ldots,\mu_d+M)" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="M" class="latex" /> is some large number. This will shift in each coordinate <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i" class="latex" /> the distribution to have median <img src="https://s0.wp.com/latex.php?latex=%5Cmu_i+%2B+%5COmega%28%5Cepsilon%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu_i + \Omega(\epsilon)" class="latex" /> instead of median <img src="https://s0.wp.com/latex.php?latex=%5Cmu_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu_i" class="latex" />, and hence make <img src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%7B%5Cast%7D+%5Capprox+%5Cmu+%2B+c%5Ccdot+%28%5Cepsilon%2C+%5Cldots%2C+%5Cepsilon%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu^{\ast} \approx \mu + c\cdot (\epsilon, \ldots, \epsilon)" class="latex" /> which implies that <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Cmu%5E%7B%5Cast%7D-%5Cmu%5Cright%7C+%5Capprox+c%5Ccdot+%5Cepsilon+%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left|\mu^{\ast}-\mu\right| \approx c\cdot \epsilon \sqrt{d}" class="latex" /> for some constant <img src="https://s0.wp.com/latex.php?latex=c%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="c&gt;0" class="latex" />.</p>



<p><strong>The obvious next question is: Can we do better?</strong> i.e, can we avoid paying this dimension-dependent <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sqrt{d}" class="latex" /> price?</p>



<p>Yes we can! We can use the ‘Tukey Median’!.<br /><img src="https://i.imgur.com/pbKZ8U6.png" alt="" /></p>



<p><strong>What is a Tukey median?</strong></p>



<p>Informally via the picture below, if you have a Tukey median (red), no matter which direction you take from it and look at the half-space, it exactly partitions the data into about half the # of points.<br /><img src="https://i.imgur.com/NaGRWbB.png" alt="" /></p>



<p><strong>Formal definition of Tukey median</strong> is given as follows (fixing some parameters):</p>



<p>A Tukey median* of <img src="https://s0.wp.com/latex.php?latex=x_%7B1%7D%2C+%5Cldots%2C+x_%7Bn%7D+%5Cin+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_{1}, \ldots, x_{n} \in \mathbb{R}^d" class="latex" /> is a vector <img src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%7B%5Cast%7D+%5Cin+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu^{\ast} \in \mathbb{R}^d" class="latex" /> s.t. for every nonzero <img src="https://s0.wp.com/latex.php?latex=v+%5Cin+%5Cmathbb%7BR%7D%5E%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v \in \mathbb{R}^{d}" class="latex" /></p>



<figure class="wp-block-image"><img src="https://i.imgur.com/bBy6GZE.png" alt="" /></figure>



<p>A Tukey median need not always exist for a given data. However, we will show that if the data is generated at random according to a nice distribution, and then at most <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon" class="latex" /> fraction of it is perturbed, a Tukey median will exist with high probability. In fact, the true population mean will be such a median.</p>



<p><strong>Existence of Tukey median</strong></p>



<p><strong>THM:</strong> If <img src="https://s0.wp.com/latex.php?latex=x_%7B1%7D%2C+%5Cldots%2C+x_%7B%281-%5Cepsilon%29+n%7D+%5Csim+N%28%5Cmu%2C+I%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_{1}, \ldots, x_{(1-\epsilon) n} \sim N(\mu, I)" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7Bd+%2F+n%7D+%5Cll+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sqrt{d / n} \ll \epsilon" class="latex" /> then</p>



<ol><li>Tukey median <img src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%7B%5Cast%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu^{\ast}" class="latex" /> exists and</li><li>For <em>every</em> Tukey median <img src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%7B%5Cast%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu^{\ast}" class="latex" /> , <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Cmu-%5Cmu%5E%7B%5Cast%7D%5Cright%7C+%5Cleq+O%28%5Cepsilon%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left|\mu-\mu^{\ast}\right| \leq O(\epsilon)" class="latex" />.</li></ol>



<p>Together 1. and 2. mean that if we search over all vectors and output the first Tukey median that we find, then (a) this process will terminate and (b) its output will be a good approximation for the population mean. In particular, we do not have to pay the extra <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sqrt{d}" class="latex" /> cost needed in the median of coordinates!</p>



<p>** Proof of 1 (existence):**<br />The mean itself is the Tukey median in this case because, <img src="https://s0.wp.com/latex.php?latex=%5Cforall+v+%5Cneq+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\forall v \neq 0" class="latex" />, if we define <img src="https://s0.wp.com/latex.php?latex=Y_%7B1%7D%3D%5Cmathrm%7Bsign%7D%5Cleft%28%5Cleft%5Clangle+x_%7B1%7D-%5Cmu%2C+v%5Cright%5Crangle%5Cright%29%2C+%5Cldots%2C+Y_%7Bk%7D%3D%5Cmathrm%7Bsign%7D%5Cleft%28%5Cleft%5Clangle+x_%7Bk%7D-%5Cmu%2C+v%5Cright%5Crangle%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y_{1}=\mathrm{sign}\left(\left\langle x_{1}-\mu, v\right\rangle\right), \ldots, Y_{k}=\mathrm{sign}\left(\left\langle x_{k}-\mu, v\right\rangle\right)" class="latex" /> then these are i.i.d ±1 vars of mean zero, and thus:</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BPr%7D%5Cleft%5B%5Csum+Y_%7Bi%7D%3E%5Cepsilon+k%5Cright%5D%3C%5Cexp+%5Cleft%28-%5Cepsilon%5E%7B2%7D+n%5Cright%29+%5Cll+%5Cexp+%28-d%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathrm{Pr}\left[\sum Y_{i}&gt;\epsilon k\right]&lt;\exp \left(-\epsilon^{2} n\right) \ll \exp (-d)" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7Bd+%2F+n%7D+%5Cll+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sqrt{d / n} \ll \epsilon" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=k%3D%281-%5Cepsilon%29n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k=(1-\epsilon)n" class="latex" />. Through discretization, we can pretend (losing some constants) that there are only <img src="https://s0.wp.com/latex.php?latex=2%5E%7BO%28d%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="2^{O(d)}" class="latex" /> unit vectors in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{R}^d" class="latex" />. Hence we can use the union bound to conclude that for <em>every</em> unit <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" />, if we restrict attention to the “good” (i.e., unperturbed) vectors <img src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2C+x_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_1,\ldots, x_k" class="latex" />, then the fraction of them satisfying <img src="https://s0.wp.com/latex.php?latex=%5Clangle+x_i+-%5Cmu+%2C+v+%5Crangle+%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle x_i -\mu , v \rangle &gt;0" class="latex" /> will be in <img src="https://s0.wp.com/latex.php?latex=1%2F2+%5Cpm+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/2 \pm \epsilon" class="latex" />. Since the adversary can perturb at most <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon n" class="latex" /> vectors, the overall frection of <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_i" class="latex" />‘s such that <img src="https://s0.wp.com/latex.php?latex=%5Clangle+x_i+-%5Cmu+%2C+v+%5Crangle+%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle x_i -\mu , v \rangle &gt;0" class="latex" /> will be in <img src="https://s0.wp.com/latex.php?latex=1%2F2+%5Cpm+2%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/2 \pm 2\epsilon" class="latex" />. QED(1)</p>



<p>** Proof of 2:**<br />Let <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu" class="latex" /> be the population mean (i.e., the “good” <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_i" class="latex" />‘s are distributed according to <img src="https://s0.wp.com/latex.php?latex=N%28%5Cmu%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(\mu,I)" class="latex" />). Suppose for simplicity, and toward a contradiction, that <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Cmu-%5Cmu%5E%7B%5Cast%7D%5Cright%7C+%3D+10+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left|\mu-\mu^{\ast}\right| = 10 \epsilon" class="latex" />.</p>



<p>Let <img src="https://s0.wp.com/latex.php?latex=v%3D%5Cmu-%5Cmu%5E%7B%5Cast%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v=\mu-\mu^{\ast}" class="latex" />. Then,<br /><img src="https://s0.wp.com/latex.php?latex=%5Cleft%5Clangle+x_%7Bi%7D-%5Cmu%5E%7B%5Cast%7D%2C+v+%2F%7Cv%7C%5Cright%5Crangle%3D%5Cleft%5Clangle+x_%7Bi%7D-%5Cmu%2Bv%2C+v+%2F%7Cv%7C%5Cright%5Crangle%3D%5Cleft%5Clangle+x_%7Bi%7D-%5Cmu%2C+v+%2F%7Cv%7C%5Cright%5Crangle+%2B%7Cv%7C+%3D+%5Cleft%5Clangle+x_%7Bi%7D-%5Cmu%2C+v+%2F%7Cv%7C%5Cright%5Crangle+%2B10+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left\langle x_{i}-\mu^{\ast}, v /|v|\right\rangle=\left\langle x_{i}-\mu+v, v /|v|\right\rangle=\left\langle x_{i}-\mu, v /|v|\right\rangle +|v| = \left\langle x_{i}-\mu, v /|v|\right\rangle +10 \epsilon" class="latex" />.</p>



<p>Note that <img src="https://s0.wp.com/latex.php?latex=%5Cleft%5Clangle+x_%7Bi%7D-%5Cmu%2C+v+%2F%7Cv%7C%5Cright%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left\langle x_{i}-\mu, v /|v|\right\rangle" class="latex" /> is distributed as <img src="https://s0.wp.com/latex.php?latex=N%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(0,1)" class="latex" />, and so we get that <img src="https://s0.wp.com/latex.php?latex=%5Clangle+x_i+-+%5Cmu%5E%2A+%2C+v%2F%7Cv%7C%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle x_i - \mu^* , v/|v|\rangle" class="latex" /> is distributed as <img src="https://s0.wp.com/latex.php?latex=N%2810%5Cepsilon%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(10\epsilon,1)" class="latex" />.<br />Hence, if <img src="https://s0.wp.com/latex.php?latex=Y_%7Bi%7D%3D%5Cmathrm%7Bsign%7D%5Cleft%28%5Cleft%5Clangle+x_%7Bi%7D-%5Cmu%5E%7B%5Cast%7D%2C+v+%2F%7Cv%7C%5Cright%5Crangle%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y_{i}=\mathrm{sign}\left(\left\langle x_{i}-\mu^{\ast}, v /|v|\right\rangle\right)" class="latex" /> then <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BPr%7D%5Cleft%5BY_%7Bi%7D%3D-1%5Cright%5D%3D%5Cmathrm%7BPr%7D%5BN%2810%5Cepsilon+%2C1%29+%3C+0%5D+%5Cleq+1+%2F+2-5+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathrm{Pr}\left[Y_{i}=-1\right]=\mathrm{Pr}[N(10\epsilon ,1) &lt; 0] \leq 1 / 2-5 \epsilon" class="latex" />.<br />This implies that via a similar concentration argument as above, for every <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v" class="latex" />, there will be with high probability at most <img src="https://s0.wp.com/latex.php?latex=1%2F2+-+4%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/2 - 4\epsilon" class="latex" /> fraction of <img src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i" class="latex" />‘s such that <img src="https://s0.wp.com/latex.php?latex=Y_%7Bi%7D%3D-1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Y_{i}=-1" class="latex" />, contradicting our assumption that <img src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%5Cast&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu^\ast" class="latex" /> was a Tukey median. QED(2)</p>



<p>Exactly computing the Tukey median is NP-hard, but efficient algorithms for robust mean estimation of normals and other distributions exist as referred to in <a href="https://jerryzli.github.io/robust-ml-fall19.html">Jerry Li’s lecture notes</a>. In particular, we can use the following approach using <em>spectral signatures</em> and <em>filtering</em>.</p>



<p><strong>Spectral Signatures</strong> can efficiently <em>certify</em> that a given vector is in fact a robust mean estimator.<br />Let <img src="https://s0.wp.com/latex.php?latex=x_%7B1%7D%2C+%5Cldots%2C+x_%7B%281-%5Cepsilon%29+n%7D+%5Csim+N%28%5Cmu%2C+I%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_{1}, \ldots, x_{(1-\epsilon) n} \sim N(\mu, I)" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=x_%7B%281-%5Cepsilon%29+n%2B1%7D%2C+%5Cldots%2C+x_%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_{(1-\epsilon) n+1}, \ldots, x_{n}" class="latex" /> arbitrary<br />Let <img src="https://s0.wp.com/latex.php?latex=%5Chat%7B%5Cmu%7D%3D%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+x_%7Bi%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\hat{\mu}=\frac{1}{n} \sum_{i=1}^{n} x_{i}" class="latex" /> be the empirical mean and <img src="https://s0.wp.com/latex.php?latex=%5Cwidehat%7B%5CSigma%7D%3D%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi-1%7D%5E%7Bn%7D%5Cleft%28x_%7Bi%7D-%5Chat%7B%5Cmu%7D%5Cright%29%5Cleft%28x_%7Bi%7D-%5Chat%7B%5Cmu%7D%5Cright%29%5E%7B%5Ctop%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\widehat{\Sigma}=\frac{1}{n} \sum_{i-1}^{n}\left(x_{i}-\hat{\mu}\right)\left(x_{i}-\hat{\mu}\right)^{\top}" class="latex" /> be the empirical co-variance. Then we can bound the error of <img src="https://s0.wp.com/latex.php?latex=%5Chat%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\hat{\mu}" class="latex" /> as follows:</p>



<p><strong>Claim:</strong> <img src="https://s0.wp.com/latex.php?latex=%7C%5Chat%7B%5Cmu%7D-%5Cmu%7C+%5Cleq+O%5Cleft%28%5Csqrt%7B%5Cfrac%7Bd%7D%7Bn%7D%7D%2B%5Csqrt%7B%5Cepsilon%7D%7C%5Chat%7B%5CSigma%7D%7C%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|\hat{\mu}-\mu| \leq O\left(\sqrt{\frac{d}{n}}+\sqrt{\epsilon}|\hat{\Sigma}|\right)" class="latex" /></p>



<p>In other words, if the spectral norm of the empirical covariance matrix is small, then the empirical mean is a good estimator for the population mean.</p>



<p><strong>Note<img src="https://s0.wp.com/latex.php?latex=%7B+%7D%5E%7B%5Cast%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{ }^{\ast}" class="latex" />:</strong> If all points are from <img src="https://s0.wp.com/latex.php?latex=N%28%5Cmu%2C+I%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(\mu, I)" class="latex" /> then <img src="https://s0.wp.com/latex.php?latex=%7C%5Chat%7B%5CSigma%7D%7C%3DO%28%5Csqrt%7Bd+%2F+n%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|\hat{\Sigma}|=O(\sqrt{d / n})" class="latex" />.</p>



<p>The Proof for this claim is given below<br /><img src="https://i.imgur.com/1tl31kK.png" alt="" /></p>



<p><strong>Explanation:</strong> Here, we assume the <img src="https://s0.wp.com/latex.php?latex=%5Cmu%3D0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu=0" class="latex" /> for simplicity without loss of generality. The norm can be split into additive terms on good points (green text above) and malicious points (red text above). The first term of this inequality follows from standard concentration. For the second (red) term, we can modify it by adding and subtracting <img src="https://s0.wp.com/latex.php?latex=%5Chat%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\hat{\mu}" class="latex" />. We can then apply the Cauchy-Schwartz (cs) inequality to prove it. Upon rearranging the terms and dividing by the norm of <img src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu" class="latex" />, we get the desired result.</p>



<p><strong>Filtering</strong> is an approach to turn the certificate into an algorithm to actually find the estimator. The idea is that the same claim holds for non-uniform reweighting of data points to estimate the empirical mean and covariance. Hence we can use a violation of the spectral norm condition (the existence of a large eigenvalue) to assign “blame” to some data points and down-weigh their contributions until we reach a probability distribution over points that on the one hand is spread on roughly <img src="https://s0.wp.com/latex.php?latex=1-O%28%5Cepsilon%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1-O(\epsilon)" class="latex" /> fraction of the points and on the other hand leads to a weighted empirical covariance matrix with small spectral norm. The above motivates the following robust mean estimation algorithm in the online case as below:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/XtpiBrm.png" alt="" /></figure>



<p><strong>Explanation:</strong> We first compute the mean and covariance based on uniform weighting based, and the certificate is checked via the spectral norm of the covariance. If the quality isn’t good enough, then the blame is given to the largest eigenvector of the covariance that contributes to the most error. The weighting is now improved from the uniform initialization via the multiplicative weights styled update as given in step 3. <a href="https://jerryzli.github.io/robust-ml-fall19.html">Jerry Li</a> and <a href="https://www.stat.berkeley.edu/~jsteinhardt/stat260/notes.pdf">Jacob Steinhardt</a> have wonderful lecture notes on these topics.<br />The algorithm above is computationally efficient while the bounds are not that tight.</p>



<p><strong>SoS algorithms:</strong> Another approach is via sum of squares algorithms where the guarantees for statistical robustness are much tighter, but they are computationally not very efficient although they are polynomial time. A hybrid approach might as well give a balance of these based on the problem at hand to bridge this gap between computational efficiency vs. statistical efficiency.</p>



<p>A list of relevant references is given below.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/G2dFbrm.png" alt="" /></figure>



<h2>Test-time robustness</h2>



<p>We now cover robustness issues with distribution shift and adversarial data poisoning during the testing phase in machine learning.</p>



<h3>Warm-up with pictures</h3>



<figure class="wp-block-image"><img src="https://i.imgur.com/nym9PHw.png" alt="" /></figure>



<figure class="wp-block-image"><img src="https://i.imgur.com/WjF0kUi.png" alt="" /></figure>



<p>As shown in <a href="https://arxiv.org/abs/1706.03691">Steinhardt, Koh, Liang, 2017</a> the images below illustrate how poisoning samples can drastically alter the performance of a classifier from good to bad.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/8fOuCw8.png" alt="" /></figure>



<figure class="wp-block-image"><img src="https://i.imgur.com/EjNcU3X.png" alt="" /></figure>



<p><a href="https://arxiv.org/abs/1804.00792">Shafahi , Huang, Najibi, Suciu, Studer, Dumitras, Goldstein, 2018</a> showed how poisoning images look perfectly fine to the human perception, while they flip the model’s performance where a fish is considered to be a dog and vice versa as shown below.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/kVrVMSA.png" alt="" /></figure>



<p>Another problem with regards to test-time robustness is the issue of domain shift where the distribution of test data is different from the distribution of training data.</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cunderset%7Bx%2Cy+%5Csim+D%7D%7B%5Cmathbb%7BE%7D%7D+L%28f%28x%29%2Cy%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\underset{x,y \sim D}{\mathbb{E}} L(f(x),y)" class="latex" /> vs. <img src="https://s0.wp.com/latex.php?latex=%5Cunderset%7Bx%2Cy+%5Csim+D%27%7D%7B%5Cmathbb%7BE%7D%7D+L%28f%28x%29%2Cy%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\underset{x,y \sim D'}{\mathbb{E}} L(f(x),y)" class="latex" /> . If L is bounded, then if Lipschitz constant is known then distances like earth mover’s distance, T.V distance can be used to bound this. But one needs to be quite careful or these bounds are too large or close to vacuous.</p>



<p>For example, images of dogs taken say in a forest vs. images of dogs taken on roads have huge distance in measures like the ones mentioned above, as many pixels (although not important pixels) across the images are very different and there could be a classifier that performs terribly across the test while it does good on train. But magically, there appears to be a linear relationship between the accuracy on <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" /> vs accuracy on <img src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D'" class="latex" />. Typically one would expect that the line would be below the x=y (45 degree) line.</p>



<p>i.e say if it was trained on cifar 10 (<img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" />) and tested on cifar10.2 (<img src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D'" class="latex" />) then finding this line to be a bit lower than the <img src="https://s0.wp.com/latex.php?latex=x%3Dy&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x=y" class="latex" /> line is not surprising but this linear correlation is quite surprising. It is even more surprising if the datasets are different-say in the case when performance on photos vs illustrations or cartoons of these photos was considered.</p>



<p>What are the potential reasons (intuitively)?</p>



<p>i) Overfitting on cifar test,</p>



<p>ii) Fewer human annotators per image <a href="https://arxiv.org/abs/2005.09619">introduces skew</a> towards hardness of dataset,</p>



<p>iii) Running out of images by human annotators as they might end up choosing images that are easier to annotate.</p>



<p>If we achieve better accuracy on <img src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D'" class="latex" /> than we achieved on <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" />, it is a strong indicator of a drop in hardness. If the errors are in the other direction, then this reasoning isn’t as clear.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/Ns4uH9i.png" alt="" /></figure>



<h3>A toy theoretical model</h3>



<p>Here is a toy model can demonstrate this surprising linear relationship between accuracies under domain shift.<br />Let us do a thought experiment where things are linear. Let us assume there was a true vector cat direction in terms of the representation(feature) as shown in the cartoon below. Let there be some correlated idiosyncratic correlations. An example, idiosyncrasy is say due to cats tending to be photographed more in indoors than in outdoors.</p>



<p>Consider <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> to be a point that needs to be labeled.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/TUKbBTy.png" alt="" /></figure>



<p>In some dataset <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" /> consider the probability that <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> is labeled as a cat is proportional to the following exponential as:<br /><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BPr%7D%5Bx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathrm{Pr}[x" class="latex" /> labeled cat <img src="https://s0.wp.com/latex.php?latex=%5D+%5Cpropto+%5Cexp+%28%5Cbeta%5Clangle+x%2C+C+A+T%2B%5Calpha+I%5Crangle%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="] \propto \exp (\beta\langle x, C A T+\alpha I\rangle)" class="latex" /></p>



<p>where <img src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\alpha" class="latex" /> to denote the theidiosyncratic correlation factor. This is the exponent of the dot product of the image to be labeled <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> with the CAT direction and the idiosyncratic direction. The same can be done for a dataset <img src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D'" class="latex" /> as</p>



<p>Dataset <img src="https://s0.wp.com/latex.php?latex=D%5E%7B%5Cprime%7D%3A+%5Cmathrm%7BPr%7D%5Bx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D^{\prime}: \mathrm{Pr}[x" class="latex" /> labeled cat <img src="https://s0.wp.com/latex.php?latex=%5D+%5Cpropto+%5Cexp+%5Cleft%28%5Cbeta%5E%7B%5Cprime%7D%5Cleft%5Clangle+x%2C+C+A+T%2B%5Calpha%5E%7B%5Cprime%7DI%5E%7B%5Cprime%7D%5Cright%5Crangle%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="] \propto \exp \left(\beta^{\prime}\left\langle x, C A T+\alpha^{\prime}I^{\prime}\right\rangle\right)" class="latex" /></p>



<p>Intuitively <img src="https://s0.wp.com/latex.php?latex=%5Cbeta%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta'" class="latex" /> is like the signal to noise ratio. That is, if <img src="https://s0.wp.com/latex.php?latex=%5Cbeta%27+%3C+%5Cbeta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta' &lt; \beta" class="latex" /> then <img src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D'" class="latex" /> is a harder dataset to classify than <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" />.</p>



<p>So in this toy model, the best accuracy that can be reached for any linear classifier is given by the following, where the softmax of the RHS is the classification probability.</p>



<p><img src="https://s0.wp.com/latex.php?latex=A+c+c_%7BD%7D%28C%29%3D%5Cbeta%5Clangle+C%2C+C+A+T%5Crangle%2B%5Cbeta+%5Calpha%5Clangle+C%2C+I%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A c c_{D}(C)=\beta\langle C, C A T\rangle+\beta \alpha\langle C, I\rangle" class="latex" /><br /><img src="https://s0.wp.com/latex.php?latex=A+c+c_%7BD+%5Cprime%7D%28C%29%3D%5Cbeta%5E%7B%5Cprime%7D%5Clangle+C%2C+C+A+T%5Crangle%2B%5Cbeta%5E%7B%5Cprime%7D+%5Calpha%5E%7B%5Cprime%7D%5Cleft%5Clangle+C%2C+I%5E%7B%5Cprime%7D%5Cright%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A c c_{D \prime}(C)=\beta^{\prime}\langle C, C A T\rangle+\beta^{\prime} \alpha^{\prime}\left\langle C, I^{\prime}\right\rangle" class="latex" /><br />The first term of this accuracy is the universal and transferable part and the second term is the idiosyncratic part.</p>



<p>The following is an For the <img src="https://s0.wp.com/latex.php?latex=A+c+c_%7BD+%5Cprime%7D%28C%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A c c_{D \prime}(C)" class="latex" /> we assume that if <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="C" class="latex" /> is trained on <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" /> then we assume <img src="https://s0.wp.com/latex.php?latex=%5Capprox+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\approx 0" class="latex" />. This is because if <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="C" class="latex" /> is trained on <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" />, then idiosyncratic directions of <img src="https://s0.wp.com/latex.php?latex=D%2CD%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D,D'" class="latex" /> are orthogonal to each other. So the <img src="https://s0.wp.com/latex.php?latex=%5Capprox+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\approx 0" class="latex" /> and the accuracy will be just this term of <img src="https://s0.wp.com/latex.php?latex=%5Cbeta%5E%7B%5Cprime%7D%5Clangle+C%2C+C+A+T%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta^{\prime}\langle C, C A T\rangle" class="latex" />.</p>



<p>If the model is learnt by gradient descent then the gradient direction will always be proportional to <img src="https://s0.wp.com/latex.php?latex=CAT+%2B+%5Calpha+I+%2B+Noise&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="CAT + \alpha I + Noise" class="latex" /> as the gradient is of the form<br /><img src="https://s0.wp.com/latex.php?latex=%5Cnabla%28%5Cbeta%5Clangle+C%2C+C+A+T%5Crangle%2B%5Calpha+%5Cbeta%5Clangle+C%2C+I%5Crangle%29%3D%5Cbeta+%5Ccdot+C+A+T%2B%5Cbeta+%5Calpha+%5Ccdot+I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla(\beta\langle C, C A T\rangle+\alpha \beta\langle C, I\rangle)=\beta \cdot C A T+\beta \alpha \cdot I" class="latex" /><br />So, if <img src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="C" class="latex" /> is trained on <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" />, then <img src="https://s0.wp.com/latex.php?latex=C+%5Cpropto+C+A+T%2B%5Calpha+%5Ccdot+I%2B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="C \propto C A T+\alpha \cdot I+" class="latex" /> Noise <img src="https://s0.wp.com/latex.php?latex=%3D%5Cfrac%7B%5Cgamma%7D%7B%7CC+A+T%2B%5Calpha+%5Ccdot+I%7C%5E%7B2%7D%7D%28C+A+T%2B%5Calpha+%5Ccdot+I%29%2B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="=\frac{\gamma}{|C A T+\alpha \cdot I|^{2}}(C A T+\alpha \cdot I)+" class="latex" /> Noise. Here, <img src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta" class="latex" /> is given by <img src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cgamma%7D%7B%7CC+A+T%2B%5Calpha+%5Ccdot+I%7C%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\frac{\gamma}{|C A T+\alpha \cdot I|^{2}}" class="latex" />.<br />Therefore the accuracies will be as follows</p>



<p><img src="https://s0.wp.com/latex.php?latex=A+c+c_%7BD%7D%28C%29%3D%5Cbeta%5Clangle+C%2C+C+A+T%2B%5Calpha+%5Ccdot+I%5Crangle%3D%5Cbeta+%5Cgamma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A c c_{D}(C)=\beta\langle C, C A T+\alpha \cdot I\rangle=\beta \gamma" class="latex" /><br /><img src="https://s0.wp.com/latex.php?latex=A+c+c_%7BD%5E%7B%5Cprime%7D%7D%28C%29%3D%5Cbeta%5E%7B%5Cprime%7D%5Cleft%5Clangle+C%2C+C+A+T%2B%5Calpha+%5Ccdot+I%5E%7B%5Cprime%7D%5Cright%5Crangle%3D%5Cbeta%5E%7B%5Cprime%7D+%5Cgamma+%5Ccdot+%5Cfrac%7B%7CC+A+T%7C%5E%7B2%7D%7D%7B%7CC+A+T%7C%5E%7B2%7D%2B%5Calpha%5E%7B2%7D%7CI%7C%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A c c_{D^{\prime}}(C)=\beta^{\prime}\left\langle C, C A T+\alpha \cdot I^{\prime}\right\rangle=\beta^{\prime} \gamma \cdot \frac{|C A T|^{2}}{|C A T|^{2}+\alpha^{2}|I|^{2}}" class="latex" /></p>



<p>Therefore, we see this form of a linear relationship:<br /><img src="https://s0.wp.com/latex.php?latex=A+c+c_%7BD%5E%7B%5Cprime%7D%7D%28C%29%3D%5Cfrac%7B%5Cbeta%5E%7B%5Cprime%7D%7D%7B%5Cbeta%5Cleft%281%2B%5Ctheta%5E%7B2%7D%5Cright%29%7D+%5Ccdot+%5Cmathrm%7BAcc%7D_%7BD%7D%28C%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A c c_{D^{\prime}}(C)=\frac{\beta^{\prime}}{\beta\left(1+\theta^{2}\right)} \cdot \mathrm{Acc}_{D}(C)" class="latex" /></p>



<p>Note that:</p>



<ul><li><img src="https://s0.wp.com/latex.php?latex=%5Cbeta%5E%7B%5Cprime%7D+%2F+%5Cbeta%3C1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\beta^{\prime} / \beta&lt;1" class="latex" /> iff <img src="https://s0.wp.com/latex.php?latex=D%5E%7B%5Cprime%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D^{\prime}" class="latex" /> harder than <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" /></li><li><img src="https://s0.wp.com/latex.php?latex=%5Ctheta%5E%7B2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\theta^{2}" class="latex" /> grows with idiosyncratic component of <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" /></li></ul>



<p>Although this is a toy theoretical model, it explains a linear relationship. However, finding a model that explains this linear relationship in real-life will be an interesting project to think of.</p>



<h3>Adversarial perturbations</h3>



<p>We now move to the last (yet another active) topic of our blog: adversarial perturbations. As an introductory example (taken from <a href="https://adversarial-ml-tutorial.org/">this tutorial</a>), the hog image <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> was originally classified as a hog with <img src="https://s0.wp.com/latex.php?latex=%5Capprox+99.6+%5C%25&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\approx 99.6 \%" class="latex" /> probability. A small amount of noise <img src="https://s0.wp.com/latex.php?latex=%5CDelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta" class="latex" /> was added to get the <img src="https://s0.wp.com/latex.php?latex=x%2B%5CDelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x+\Delta" class="latex" /> image which to the human eye perceptually looks indistinguishable. That said the model now ends up misclassifying the noised hog to something that is not a hog.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/4jjdwwL.png" alt="" /></figure>



<p>Should we be surprised with the efficacy of such adversarial perturbations? Originally-certainly yes, but not as much now in hindsight!</p>



<p>In this example it turns out that the magnitude of each element satisfies <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7C%5CDelta_%7Bi%7D%5Cright%7C+%5Capprox+%5Cfrac%7B1%7D%7B64%7D%5Cleft%7Cx_%7Bi%7D%5Cright%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left|\Delta_{i}\right| \approx \frac{1}{64}\left|x_{i}\right|" class="latex" /> and the 2-norm of this vector is roughly as <img src="https://s0.wp.com/latex.php?latex=%7C%5CDelta%7C+%5Capprox+%5Cfrac%7B1%7D%7B64%7D%7Cx%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|\Delta| \approx \frac{1}{64}|x|" class="latex" />. Note that the Resnet50 model outputs <img src="https://s0.wp.com/latex.php?latex=r%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r(x)" class="latex" /> at the penultimate layer has a dimension <img src="https://s0.wp.com/latex.php?latex=d%3D2048&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d=2048" class="latex" />. We scale <img src="https://s0.wp.com/latex.php?latex=%7Cx%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|x|" class="latex" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Cr%28x%29%7C+%5Capprox+%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|r(x)| \approx \sqrt{d}" class="latex" />. There is a Lipschitz constant <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L" class="latex" /> w.r.t the preservation of the following norms: <img src="https://s0.wp.com/latex.php?latex=%7Cr%28x%2B%5CDelta%29-r%28x%29%7C+%5Capprox+L%7C%5CDelta%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|r(x+\Delta)-r(x)| \approx L|\Delta|" class="latex" />. The final classification decision is made by looking at <img src="https://s0.wp.com/latex.php?latex=C+%5Ccdot+H+O+G&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="C \cdot H O G" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=HOG&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="HOG" class="latex" /> is a unit vector such that <img src="https://s0.wp.com/latex.php?latex=H+O+G%3A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H O G:" class="latex" /> unit vector s.t. <img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BPr%7D%5Bx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathrm{Pr}[x" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=h+o+g%5D+%5Cpropto%5Clangle+H+O+G%2C+r%28x%29%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="h o g] \propto\langle H O G, r(x)\rangle" class="latex" />. We assume some randomness in the decision as being <img src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B1-c%5E%7B2%7D+%2F+d%7D+%5Ccdot+N%280%2C+I%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sqrt{1-c^{2} / d} \cdot N(0, I)" class="latex" /> for simplicity. So we have<br /><img src="https://s0.wp.com/latex.php?latex=r%28x%29%3DC+%5Ccdot+H+O+G%2B%5Csqrt%7B1-c%5E%7B2%7D+%2F+d%7D+%5Ccdot+N%280%2C+I%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r(x)=C \cdot H O G+\sqrt{1-c^{2} / d} \cdot N(0, I)" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=C%3D%5Clangle+x%2C+HOG+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="C=\langle x, HOG \rangle" class="latex" />.We now have the probability that <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> is not a hog as</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BPr%7D%5Bx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathrm{Pr}[x" class="latex" /> is not hog <img src="https://s0.wp.com/latex.php?latex=%5D%3D%5Cfrac%7B%5Cexp+%28-C%29%7D%7B%5Cexp+%28C%29%2B%5Cexp+%28-C%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="]=\frac{\exp (-C)}{\exp (C)+\exp (-C)}" class="latex" />.</p>



<p>As we know that the observed probability of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" /> being not a hog is 0.0996, we can calculate that <img src="https://s0.wp.com/latex.php?latex=C+%5Capprox+3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="C \approx 3" class="latex" />.<br />Upon normalizing <img src="https://s0.wp.com/latex.php?latex=%7Cr%28x%29%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|r(x)|^2" class="latex" /> to be <img src="https://s0.wp.com/latex.php?latex=d%3D2048&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d=2048" class="latex" />, We can expect the following square of this dot product w.r.t the representation <img src="https://s0.wp.com/latex.php?latex=r%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r(x)" class="latex" /> as:<img src="https://s0.wp.com/latex.php?latex=%5Clangle+r%28x%29%2C+H+O+G%5Crangle%5E%7B2%7D+%5Capprox+3+%5Capprox+%7Cr%28x%29%7C%5E2+%2F+700&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle r(x), H O G\rangle^{2} \approx 3 \approx |r(x)|^2 / 700" class="latex" /><br />Therefore the norm of the projection of <img src="https://s0.wp.com/latex.php?latex=r%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r(x)" class="latex" /> to the HOG direction is given by <img src="https://s0.wp.com/latex.php?latex=%5Cleft%7Cr%28x%29_%7BH+O+G%7D%5Cright%7C+%5Capprox+%5Cfrac%7B1%7D%7B25%7D%7Cr%28x%29%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\left|r(x)_{H O G}\right| \approx \frac{1}{25}|r(x)|" class="latex" />.</p>



<p>So say if the 2048 vector had one larger element which accounts for the HOG direction, although it still accounts for a small proportion of its total norm. Therefore it wouldn’t need too much noise to flip one class to its wrong class label as shown in the cartoons below.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/TzWJX5X.png" alt="" /></figure>



<figure class="wp-block-image"><img src="https://i.imgur.com/5j65j1P.png" alt="" /></figure>



<p>So if the Lipschitz constant is greater than around 2.5 or 3, then a fraction of 1/25 is enough to zero out the hog direction (as <img src="https://s0.wp.com/latex.php?latex=L+%5Cgg+%5Cfrac%7B64%7D%7B25%7D+%5Capprox+2.5&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L \gg \frac{64}{25} \approx 2.5" class="latex" />).</p>



<h3>Robust loss</h3>



<p>What are some strategies for training neural networks that are robust to perturbations?</p>



<ul><li>A set of transformation that do not change the true nature of the data such as for e.g:<br /><img src="https://s0.wp.com/latex.php?latex=t_%7B%5CDelta%7D%28x%29%3Dx%2B%5CDelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t_{\Delta}(x)=x+\Delta" class="latex" /> where the set is</li></ul>



<figure class="wp-block-image"><img src="https://i.imgur.com/kbq4bol.png" alt="" /></figure>



<p>i.e, they only perturb the image or data sample by atmost <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon" class="latex" /> upon applying that transformation.<br />Now, given a loss function <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%3A+Y+%5Ctimes+Y+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{L}: Y \times Y \rightarrow \mathbb{R}" class="latex" /> and a classifier <img src="https://s0.wp.com/latex.php?latex=f%3A+X+%5Crightarrow+Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f: X \rightarrow Y" class="latex" />, a <strong>robust loss</strong> function of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" /> at point <img src="https://s0.wp.com/latex.php?latex=%28x%2C+y%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(x, y)" class="latex" /> is defined as</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/9cghGEA.png" alt="" /></figure>



<h3>Robust training:</h3>



<p>Given <img src="https://s0.wp.com/latex.php?latex=x_%7B1%7D%2C+y_%7B1%7D%2C+%5Cldots%2C+x_%7Bn%7D%2C+y_%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_{1}, y_{1}, \ldots, x_{n}, y_{n}" class="latex" /> and arobust loss function, a robust training would involve the minimization of this loss which gives us:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/JKMY5NL.png" alt="" /></figure>



<p>Now for the subgoal of finding <img src="https://s0.wp.com/latex.php?latex=%5Cnabla_%7Bf%7D+%5Cmax_%7Bt+%5Cin+%5Cmathcal%7BT%7D%7D+%5Cmathcal%7BL%7D%5Cleft%28f%5Cleft%28t%28x%7Bi%7D%29%5Cright%29%2C+y_%7Bi%7D%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\nabla_{f} \max_{t \in \mathcal{T}} \mathcal{L}\left(f\left(t(x{i})\right), y_{i}\right)" class="latex" /> for the sake of optimization, invoking <a href="https://en.wikipedia.org/wiki/Danskin%27s_theorem">Danskin’s Theorem</a> will greatly help. The theorem basically says that if <img src="https://s0.wp.com/latex.php?latex=g%28f%2C+t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g(f, t)" class="latex" /> is nice (diff, continuous) and if <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BT%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{T}" class="latex" /> is compact then we have that:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/XXRYA2k.png" alt="" /></figure>



<p>i.e for any function <img src="https://s0.wp.com/latex.php?latex=g%28f%2Ct%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g(f,t)" class="latex" />, that depends on <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t" class="latex" /> as long as the function space <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BT%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{T}" class="latex" /> from which <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t" class="latex" /> needs to be chosen is nice (diff, continuous) and compact, then to find the gradient of the maximum of the function <img src="https://s0.wp.com/latex.php?latex=g%28f%2Ct%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g(f,t)" class="latex" /> then by the theorem one can find maximizer <img src="https://s0.wp.com/latex.php?latex=t%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t^*" class="latex" /> for any particular <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" />, then that can give the required gradient (after some fine print that is given in note below).<br />Note: The paper below extends to the case when <img src="https://s0.wp.com/latex.php?latex=t%5E%7B%5Cast%7D%28f%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t^{\ast}(f)" class="latex" /> non unique though there is other fine print. See Appendix A of <a href="https://arxiv.org/abs/1706.06083">(Madry Makelov Schmidt Tsipras Vladu 2017)</a>.</p>



<p>On the empirical side, there seems to be a trade-off. If one wants to achieve adversarial robustness, it can be achieved by letting go of some model accuracy. See discussion <a href="https://distill.pub/2019/advex-bugs-discussion/">here</a> and the papers cited below.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/xFo1uDK.png" alt="" /></figure></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/04/01/robustness-in-train-and-test-time/"><span class="datestr">at April 01, 2021 11:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-2369988409049011731">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/04/want-to-buy-theorem.html">Want to Buy a Theorem?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>This is embarrassing to admit but after a few badly timed trades on GameStop options I find myself a bit tight on money. To raise some cash, I reluctantly decided to sell one of my prized possessions, one of my theorems.</p><p><b>For Sale</b>: Boolean formula satisfiability cannot be solved in both logarithmic space and quasilinear time. For a more formal and slightly more general statement and a proof, see <a href="https://doi.org/10.1006/jcss.1999.1671">this paper</a>.</p><p><b>Bidding starts</b> at 12 BTC (about $705,000). </p><p><b>The winning bid, upon verified payment, will receive:</b></p><p></p><ul style="text-align: left;"><li>The ability to give the theorem the name of your choice such as your own name, your best friend's mother's name or "Teddy McTheoremface".</li><li>A non-fungible token (NFT) attesting ownership of the theorem and the name you have chosen for it.</li><li>Anyone citing this result will be required to note that you own it and use the name you chose above. You cannot, however, limit the use of the theorem or receive compensation for its use. </li><li>By virtue of owning this theorem you will a Fortnow number of zero. This immediately gives you an Erdős number of 2. If you have previously written a paper with Paul Erdős then both of us will now have an Erdős number of 1.</li></ul><div><b>Frequently Asked Questions</b></div><div><b><br /></b></div><div><b>Q: </b>Why this theorem?</div><div><b><br /></b></div><div><b>A: </b>The theorem is in one of my few solely authored papers and I can't afford to share the proceeds of the sale. </div><div><br /></div><div><b>Q: </b>Doesn't Ryan Williams and others have <a href="http://pages.cs.wisc.edu/~dieter/Papers/sat-lb-survey-fttcs.pdf">stronger theorems</a>?</div><div><br /></div><div><b>A: </b>The results are incomparable. Ryan gives bounds on a single algorithm with low time and space. My theorem allows different machines for the time and space bounds.</div><div><br /></div><div>Also, to the best of my knowledge, Ryan's theorem is not for sale.</div><div><br /></div><div><b>Q: </b>Doesn't the proof of the theorem rely on other people's theorems such as Nepomnjascii? Shouldn't he get some of the value from this sale?</div><div><br /></div><div><b>A: </b>I'm not selling the proof of the theorem, just the theorem itself.</div><div><br /></div><div><b>Q: </b>If I purchase this theorem will I get to write next year's April Fools post?</div><div><br /></div><div><b>A: </b>No.</div><p></p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/04/want-to-buy-theorem.html"><span class="datestr">at April 01, 2021 01:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/049">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/049">TR21-049 |  Kolmogorov complexity and  nondeterminism versus determinism  for polynomial time computations | 

	Juraj Hromkovic</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We call any consistent and sufficiently powerful formal theory that enables to algorithmically in polynomial time verify whether a text is a proof  \textbf{efficiently verifiable mathematics} (ev-mathematics). We study the question whether nondeterminism is more powerful than determinism for polynomial time computations in the framework of ev-mathematics. Our main results are as follows. \\
"$\P \subsetneq \NP$ or for any deterministic, polynomial time compression algorithm $A$ there exists a nondeterministic, polynomial time compression machine $M$ that reduces infinitely many binary strings logarithmically stronger than $A$." \\
"$\P \subsetneq \NP$ or f-time resource bounded Kolmogorov complexity of any binary string $x$ can be computed in deterministic polynomial time for each polynomial time constructible function $f$."</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/049"><span class="datestr">at April 01, 2021 12:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18471">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/04/01/computer-science-gets-noted/">Computer Science Gets Noted</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>All non-metallic UK currency to feature Computing and Data Science pioneers</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/01/computer-science-gets-noted/babbagelovelacenote/" rel="attachment wp-att-18474"><img width="250" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/BabbageLovelaceNote.jpg?resize=250%2C134&amp;ssl=1" class="alignright wp-image-18474" height="134" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from BBC <a href="https://www.bbc.com/news/uk-34710261?ocid=socialflow_facebook&amp;">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Charles Babbage and Ada Lovelace will be reunited. Their work on designing and programming a computing device, a century before any machine was built, is being honored by their appearance on the reverse of the 5-pound note in a new series authorized by the Bank of England. Royal Statistical Society Fellow Florence Nightingale will appear on the 10, EDSAC creator Maurice Wilkes on the 20, and Alan Turing on the 50. </p>
<p>
Today, April 1, we are delighted to convey this news and show previews of the banknotes. </p>
<p>
We have mentioned Babbage and Lovelace several times on this blog, the latter first <a href="https://rjlipton.wpcomstaging.com/2010/03/23/its-ada-lovelace-day/">here</a>. Our 2015 <a href="https://rjlipton.wpcomstaging.com/2015/02/17/ada-the-amplifier/">post</a> on her work on his project sought to engage a scholarly consensus that tends to minimize it. We argue that it should be judged on the plane of a doctoral or masters advisory relationship. Our point is that she greatly <em>amplified</em> the technical content of his work as shown to the scientific community. </p>
<p>
The design shown above makes a leap from their Victorian world to our online age. Being “online” back then probably meant being reached by the new railroad system centered on London. The one indelible connection between our world and theirs is shown by the obverse of the banknote: both worlds have a long-serving British queen.</p>
<p></p><h2> Nightingale On The 10 </h2><p></p>
<p>
Besides a long life (1820–1910), Nightingale has a long entry as a <a href="https://en.wikipedia.org/wiki/Founders_of_statistics">founder</a> of statistics. Her actions and influence extended for more than five decades after her iconic “Lady With the Lamp” ministry to soldiers of the Crimean War depicted on the 10-pound note:</p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/01/computer-science-gets-noted/nightingalespecimen2021/" rel="attachment wp-att-18475"><img width="450" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/NightingaleSpecimen2021.jpg?resize=450%2C252&amp;ssl=1" class="aligncenter wp-image-18475" height="252" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Nightingale 200 Years <a href="https://www.florence-nightingale.co.uk/10-bank-note-featuring-florence-nightingale-1975-1992/">source</a></font>
</td>
</tr>
</tbody></table>
<p>
The picture at right is based on a <a href="https://www.amdigital.co.uk/about/blog/item/nightingale">photo</a> by William Kilburn, one of few series of photos she allowed to be taken. We have not found a definitive word on what she is holding—not even from this long <a href="https://theclassicphotomag.com/florence-nightingale-the-mysteries-behind-her-iconic-photographs/">article</a>—but we would like to believe it is a letter or paper regarding medical practice. Already in 1854, the <em>Oxford Chronicle and Reading Gazette</em> noted her bringing much more than a lamp:</p>
<blockquote><p><b> </b> <em> In a knowledge of the ancient languages and of the higher branches of mathematics, in general art, science, and literature, her attainments are extraordinary. There is scarcely a modern language which she does not understand; and she speaks French, German, and Italian as fluently as her native English. </em>
</p></blockquote>
<p>
Not only did she popularize the pie chart and invent other statistical visualization techniques, she was among the first people, period, to bring statistical inference into policy. This included demonstrating with data the higher exposure of nurses to multiple diseases among those they tended. She was inducted into the Royal Statistical Society in <b>1858</b>. </p>
<p>
Her medical conduct principles have been applied expressly during the pandemic, as <a href="https://nursing.vanderbilt.edu/news/lessons-from-florence-nightingale-are-primary-tools-in-covid-19-nursing/">recognized</a> by the time of the 200th anniversary of her birth last May 12, and field hospitals in the UK for Covid-19 treatment are <a href="https://en.wikipedia.org/wiki/COVID-19_hospitals_in_the_United_Kingdom">named</a> for her. Her recognition on the currency was considered a way to honor first-line caregivers and was a unanimous vote of the Bank of England commission. It has the earliest release <a href="https://en.numista.com/catalogue/note203311.html">date</a> of all the banknotes.</p>
<p></p><h2> Wilkes On The 20 </h2><p></p>
<p>
Maurice Wilkes had an even longer life, 1913 to 2010. This marks a stark contrast to Turing. Wilkes was the second winner, in 1967, of the Turing Award. The Turing Award <a href="https://amturing.acm.org/award_winners/wilkes_1001395.cfm">citation</a> said he was</p>
<blockquote><p><b> </b> <em> “the builder and designer of the EDSAC, the first computer with an internally stored program.” </em>
</p></blockquote>
<p>
He was thus the first realizer of the full vision of Babbage and Lovelace, although many claim the stored-program distinction for the Manchester <a href="https://en.wikipedia.org/wiki/Manchester_Baby">Baby</a> one year earlier, in 1948.</p>
<p>
There was controversy about the order of him and Turing. The banknotes were intended to go in chronological order. Turing’s famous paper conceiving the computer dates to 1936–37. Wilkes’s first brush with the project that became the <a href="https://en.wikipedia.org/wiki/EDSAC">EDSAC</a> came in 1945, when he spent an iconic 24 hours with a loaned copy of John von Neumann’s <a href="https://en.wikipedia.org/wiki/First_Draft_of_a_Report_on_the_EDVAC">First EDVAC Report</a> that he was not able to mimeograph. However, Wilkes was involved with analog computing devices before 1937 and his EDSAC reached full operation in May 1949, whereas Turing’s own main project, the <a href="https://en.wikipedia.org/wiki/Automatic_Computing_Engine">ACE</a>, launched its <a href="https://en.wikipedia.org/wiki/Pilot_ACE">pilot</a> version only in 1950. </p>
<p>
The ultimate determiner was that the 20-pound note was already recently revised to feature the artist Joseph Turner in February 2020. The Turner issue will have a five-year term, so Wilkes will appear in 2025. The banknote design has not yet been executed.</p>
<p></p><h2> Turing On The 50 </h2><p></p>
<p>
Of course, this is the highlight for us, and we are delighted to be able to show the Turing design in full glory. It was formally <a href="https://www.bankofengland.co.uk/news/2021/march/the-new-50-note-unveiled">unveiled</a> last week:</p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/01/computer-science-gets-noted/turingbanknote/" rel="attachment wp-att-18476"><img width="550" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/TuringBanknote.jpg?resize=550%2C295&amp;ssl=1" class="aligncenter wp-image-18476" height="295" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Belfast Telegraph <a href="https://www.belfasttelegraph.co.uk/business/uk-world/birth-date-in-binary-code-among-features-of-new-alan-turing-banknote-40238025.html">source</a></font>
</td>
</tr>
</tbody></table>
<p>
The Pilot ACE machine is shown overlaid with Turing machine code from his 1936 paper. Turing tapes with binary code appear in three places. The big one represents his June 23, 1912 birth date—exactly how, we leave as a puzzle.  We’ll just hint that all our friends across the Pond write dates differently from how we do. The new banknotes are being released on Turing’s birthday this coming June.</p>
<p>
The <a href="https://en.wikipedia.org/wiki/GCHQ">GCHQ</a>, which grew out of Turing’s employers at Bletchley Park, has just released its own puzzle <a href="https://www.gchq.gov.uk/information/turing-challenge">challenge</a>, which references Easter-egg features of the Turing banknote. The binary code puzzle is 11th of 12, but to our surprise, when you click it, it gives the answer to the birth date part right away.  Then it proceeds to something far more cryptic—well, it’s from GCHQ after all. </p>
<p></p><h2> Other Notes </h2><p></p>
<p>
The Turing release will also complete the conversion from paper to polymer of all Bank of England notes, which are also the only series issued for Wales. Scotland and Northern Ireland issue their own pound-denominated banknotes with different designs. They have yet to convert to polymer, and that will delay any consideration of adopting the new series.</p>
<p>
Turing’s family on his father’s side came from Scotland, so there is some sentiment for the Royal Bank of Scotland adopting the design. There is also discussion of recognizing Robin Milner and/or Donald Michie, though both were born in England. </p>
<p>
Over in Ireland, both sides of the Northern Ireland border are considering the statistician William Gosset, who as Head Brewer of Guinness over a century ago conceived the <a href="https://en.wikipedia.org/wiki/Student's_t-test">Student t-test</a> among many innovations. However, Gosset was also born in England. The Bushmills distillery in Northern Ireland has already <a href="https://web.archive.org/web/20150717073525/http://boini.bankofireland.com/about-boi-group/bank-notes/note/16">appeared</a> on their 5-pound note.</p>
<p>
In fact, Gosset and Milner and Michie were all originally <a href="https://www.bankofengland.co.uk/-/media/boe/files/banknotes/50-character-selection-names.pdf">nominated</a> alongside Turing for the 50-pound note. We spot Bertrand Russell and William Tutte and Karen Spärck-Jones on the list. In physics, we notice Paul Dirac and John Bell and Stephen Hawking, and also the astronomer Vera Rubin—who was only American, no? Bell was from Northern Ireland. </p>
<p>
Sir Michael Atiyah is not on the list because he was living at the time—no living person other than the monarch may appear on a banknote. This law does not apply to e-currency however, and this leads to what for us is the most exciting aspect of the Bank of England initiative.</p>
<p></p><h2> Semi-Fungible Tokens </h2><p></p>
<p>
The following is a GLL exclusive. It was conveyed to us by Dr. Lofa Polir, who since our <a href="https://rjlipton.wpcomstaging.com/2020/04/01/research-at-home/">story</a> a year ago has resided in England. It comes from a confluence of several facts:</p>
<ul>
<li>National banks regularly issue limited editions of currency apart from the main series.
</li><li>Government banks have largely refrained from direct involvement with digital currencies, but <a href="https://www.federalreserve.gov/newsevents/speech/brainard20200813a.htm">observe</a> them closely.
</li><li>Large sections of the public have come to ascribe intrinsic value to <em>non-fungible tokens</em> (<a href="https://en.wikipedia.org/wiki/Non-fungible_token">NFTs</a>), which come with a blockchain-verifiable certificate of unique ownership.
</li><li>A non-fungible token by-definition cannot be used as money but only bought and sold as a (unique) commodity.
</li><li>Playing cards in games such as <a href="https://en.wikipedia.org/wiki/Magic:_The_Gathering">Magic: the Gathering</a> are regularly traded online to the extent of <a href="https://en.wikipedia.org/wiki/Mt._Gox">melding</a> with cryptocurrencies or verging on being currency themselves.
</li></ul>
<p>
The Bank of England recently voted to create an active response to this situation: the <em>semi-fungible token</em> (SFT). Like Bitcoin and other cryptocurrencies, SFTs limit their supply, but unlike them, SFTs are maintained by a central body and backed by its assets. An SFT is fungible by dint of having a fixed redemption rate to standard currency, but its own price may go higher.  As with NFTs, each SFT is associated to a piece of digital artwork. </p>
<p>
The new issues are based on the Royal Mail’s 2015 <a href="https://www.collectgbstamps.co.uk/explore/issues/?issue=22720">Inventive Britain</a> collection, whose honorees include Sir Tim Berners-Lee for the World Wide Web. The main stamps were abstract, but associated issues are not subject to the law against likenesses of living persons:</p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/01/computer-science-gets-noted/bernersleewww/" rel="attachment wp-att-18477"><img width="534" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/BernersLeeWWW.png?resize=534%2C212&amp;ssl=1" class="aligncenter wp-image-18477" height="212" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite of <a href="https://gb-site.netlify.app/tim-berners-lee-biography/">src1</a>, <a href="https://www.bfdc.co.uk/2007/world_of_invention/sir_tim_berners_lee.html">src2</a></font>
</td>
</tr>
</tbody></table>
<p>
Berners-Lee will be the first honoree in the SFT series. We are not authorized to reveal the art design before its release. But Lofa sent us a list of nominees that includes Geoffrey Hinton, Tony Hoare, and Leslie Valiant on the CS side, plus David Spiegelhalter and Demis Hassabis in data science. Unlike with the banknotes, the Bank of England can “catch them all.” Imagine, then, you will soon be able to pay for groceries with cards of some of your favorite computer and data scientists—at least if they are British and you buy the groceries in Britain. </p>
<p></p><h2> Open Problems </h2><p></p>
<p>
If you see a market for NFTs, do you see one for SFTs—or are they both April Fools?</p>
<p></p><p><br />
[added a sentence to the description of SFTs.]</p></font></font></div>







<p class="date">
by KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/04/01/computer-science-gets-noted/"><span class="datestr">at April 01, 2021 08:36 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/03/31/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/03/31/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>As is often the case, not all of these are links; they’re copies of posts that I made over on my Mastodon account because they were too short to make full posts here.</p>

<ul>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Borromean_rings">Borromean rings</a> (<a href="https://mathstodon.xyz/@11011110/105898319690639869">\(\mathbb{M}\)</a>), now a Good Article on Wikipedia. You can’t make the Borromean rings with geometric circles: as Tverberg observed, an inversion would make one of them a line. But then each of the other two circles would span an angle less than \(\pi\) as viewed from the line, leaving an unspanned direction along which the line could escape to infinity, contradicting the inseparability of the rings.</p>

    <p>The same proof shows that <a href="https://en.wikipedia.org/wiki/Brunnian_link">Brunnian links</a> cannot be made from four geometric circles. But the original proof that Borromean rings are not circular, by <a href="https://doi.org/10.4310/jdg/1214440725">Freedman and Skora</a> using 4d hyperbolic geometry, applies more generally to any Brunnian link no matter how many components it has.</p>
  </li>
  <li>
    <p><a href="https://twitter.com/mathpuzzle/status/1366756737882218500">Squares can be divided into \(45^\circ\)–\(60^\circ\)–\(75^\circ\) triangles</a> (<a href="https://mathstodon.xyz/@11011110/105909672693909508">\(\mathbb{M}\)</a>), as Ed Pegg posts on twitter. This is an interesting variant on something I looked at a long time ago, <a href="https://www.ics.uci.edu/~eppstein/junkyard/acute-square/">triangulating the square to minimize the maximum angle</a>. But Pegg’s subdivision is not edge-to-edge. And the triangles on my page are not all similar. Do squares have edge-to-edge triangulations by similar acute triangles?</p>
  </li>
  <li>
    <p><a href="https://blogs.sciencemag.org/editors-blog/2021/02/18/a-new-name-change-policy/"><em>Science</em> joins other publishers in making it easy for authors to change their names on past publications</a> (<a href="https://mathstodon.xyz/@11011110/105915141700667095">\(\mathbb{M}\)</a>, <a href="https://www.metafilter.com/190813/Making-it-easier-for-published-scientists-to-change-their-names">via</a>). Transgender scientists made this change necessary but I think it benefits all of us. I’ve seen plenty of name changes in the literature for other reasons (marriage, divorce, escaping prejudice, …) and making it easier to find your old papers is generally a good thing.</p>
  </li>
  <li>
    <p>Cairo in Denmark? (<a href="https://mathstodon.xyz/@11011110/105920191343711124">\(\mathbb{M}\)</a>).  This is a screenshot from Google Maps of Havnepromenade in Copenhagen, where it meets the end of Cort Adelers Gade. I’m sad to have missed this spot when I was in Copenhagen a couple years ago; it’s a nice example of a <a href="https://en.wikipedia.org/wiki/Cairo_pentagonal_tiling">Cairo pentagonal tiling</a>.</p>

    <p style="text-align: center;"><img width="80%" style="border-style: solid; border-color: black;" alt="Pentagonal street pavers on Havnepromenade at Cort Adelers Gade in Copenhagen, taken as a screenshot from Google Maps" src="https://11011110.github.io/blog/assets/2021/Cairo-in-Copenhagen.jpg" /></p>
  </li>
  <li>
    <p>My latest illustration trick: make complex non-Boolean combinations of shapes by overlaying simple shapes, exploding the overlay into simple regions, and unioning the regions into the shapes I want (<a href="https://mathstodon.xyz/@11011110/105929236692195138">\(\mathbb{M}\)</a>). Exploding and unioning are single clicks in Illustrator’s pathfinder tool. I used two levels of this technique to make this diagram of a <a href="https://en.wikipedia.org/wiki/Brunnian_link">Brunnian link</a>: once to make the shape of a single component, and again to make the over-under relation between components.</p>

    <p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2021/six-rubberband-link.svg" alt="Six-rubberband link" /></p>
  </li>
  <li>
    <p><a href="https://mattwthomas.com/blog/induction-on-reals/">Real induction</a> (<a href="https://mathstodon.xyz/@mwt/105862096547025004">\(\mathbb{M}\)</a>). An induction principle for proving statements about all real numbers in intervals.</p>
  </li>
  <li>
    <p><a href="https://www.patreon.com/posts/new-print-49071226">Recamán Sequence</a> (<a href="https://mastodon.social/@joshmillard/105935104495594723">\(\mathbb{M}\)</a>). Linocut art by Josh Millard in a style drawn from a <em>Numberphile</em> video.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/mathematicians-inch-closer-to-matrix-multiplication-goal-20210323/"><em>Quanta</em> surveys recent developments in fast matrix multiplication</a> (<a href="https://mathstodon.xyz/@11011110/105943717367375547">\(\mathbb{M}\)</a>), including <a href="https://arxiv.org/abs/2010.05846">a paper from last SODA by Josh Alman and Virginia Williams</a> improving the exponent from 2.3728639 to 2.3728596. Known barriers to the widely-expressed hope that the exponent can be reduced to 2 (see the SODA paper’s introduction) are, I think too specific to be convincing. On the other hand at this rate there’s still a long way from here to 2…</p>
  </li>
  <li>
    <p><a href="https://fredrikj.net/blog/2021/03/printing-algebraic-numbers/">Printing algebraic numbers</a> (<a href="https://mathstodon.xyz/@11011110/105946537048589616">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=26566090">via</a>). So your code can handle symbolic representations of exact algebraic numbers rather than just approximating everything as complex/float. How do you get those representations out to human users in a readable way? From ‪Fredrik Johansson‬, long-time developer of open symbolic algebra and exact real arithmetic software including SymPy and Calcium.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Fermat%27s_right_triangle_theorem">Fermat’s right triangle theorem</a> (<a href="https://mathstodon.xyz/@11011110/105951398938627194">\(\mathbb{M}\)</a>), now a Good Article on Wikipedia. As Fibonacci observed and Fermat proved (as his only surviving proof), integer right triangles cannot have an area that is a perfect square. One corollary of this is the exponent-4 case of Fermat’s Last Theorem.</p>
  </li>
  <li>
    <p><a href="https://mastodon.social/@tomharris/105956972687307734">Tom Harris asks: What’s your favorite OEIS sequence?</a> Mine for this week at least is the <a href="https://en.wikipedia.org/wiki/Moser%E2%80%93De_Bruijn_sequence">Moser–De Bruijn sequence</a>, OEIS <a href="https://oeis.org/A000695">A000695</a>.</p>
  </li>
  <li>
    <p>The most recent image of me on my site was from 2009 and getting pretty out-of-date. So I took a screenshot of myself as I look these days on Zoom (<a href="https://mathstodon.xyz/@11011110/105964230654539136">\(\mathbb{M}\)</a>). This is far from the first time I’ve let my hair go at least this long, but I’m still looking forward to a time when I will feel more comfortable getting a haircut.
It occurs to me that there has been a reversal: now, instead of <a href="https://en.wikipedia.org/wiki/Almost_Cut_My_Hair">showing off rebelliousness</a>, I am visibly conforming to safety rules.</p>

    <p style="text-align: center;"><img width="80%" style="border-style: solid; border-color: black;" alt="David Eppstein, self-portrait, Zoom screenshot" src="https://11011110.github.io/blog/assets/2021/zoom.jpg" /></p>
  </li>
  <li>
    <p><a href="https://www.eff.org/deeplinks/2021/03/free-climbing-rock-climbers-open-data-project-threatened-bogus-copyright-claims">Fight over open data in rock climbing</a> (<a href="https://mathstodon.xyz/@11011110/105971892719302236">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=26609945">via</a>).</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/q/387543/440">Weird probability distributions on dyadic rationals from a simple averaging process</a> (<a href="https://mathstodon.xyz/@11011110/105977625108931414">\(\mathbb{M}\)</a>). Start with the two-element multiset \(\{0,1\}\), repeatedly draw two-element samples (without replacement) from the multiset and include the average into the multiset. The result tends to cluster around some value, not the same value every time but itself drawn from a bimodal distribution, with the clustering sort of looking Cauchy but not quite. Interesting new MathOverflow question.</p>
  </li>
  <li>
    <p><a href="https://blog.computationalcomplexity.org/2021/03/slicing-hypercube.html">How many hyperplanes does it take to slice all the edges of an -dimensional hypercube</a> (<a href="https://mathstodon.xyz/@11011110/105982764812531388">\(\mathbb{M}\)</a>)? Two if you can skim the ends of the edges or include entire edges in hyperplanes but more to really slice them. The obvious answer of axis-parallel cuts can be improved to \(5n/6\), and Lance Fortnow reports that the newest lower bound is \(\Omega(n^{0.57})\), by Gal Yehuda and Amir Yehudayoff, improving <a href="https://arxiv.org/abs/2102.05536">their own new preprint</a>.</p>
  </li>
  <li>
    <p><a href="https://rjlipton.wpcomstaging.com/2021/03/30/all-the-news-that-fits-we-print/">A list of current blogs on computer theory and related math</a> (<a href="https://mathstodon.xyz/@11011110/105986862218758838">\(\mathbb{M}\)</a>), from <em>Gödel’s Lost Letter and P=NP</em>.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/03/31/linkage.html"><span class="datestr">at March 31, 2021 05:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18443">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/03/30/all-the-news-that-fits-we-print/">All The News That Fits We Print</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>The New York Times is a great newspaper: it is also No Fun—Molly Ivins</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/03/30/all-the-news-that-fits-we-print/nyt/" rel="attachment wp-att-18445"><img width="300" alt="" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/03/nyt.png?resize=300%2C164&amp;ssl=1" class="alignright size-medium wp-image-18445" height="164" /></a></p>
<p>
The Gray Lady, as the New York Times is usually called, believes in her real slogan: </p>
<blockquote><p><b> </b> <em> All the news that’s fit to print. </em>
</p></blockquote>
<p><a href="https://rjlipton.wpcomstaging.com/2021/03/30/all-the-news-that-fits-we-print/gl/" rel="attachment wp-att-18446"><img width="300" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/03/gl.jpg?resize=300%2C158&amp;ssl=1" class="aligncenter size-medium wp-image-18446" height="158" /><br />
</a></p>
<p>
This slogan started about 115 years ago, and is on the <a href="https://www.bbc.com/news/world-us-canada-16918787">masthead</a> of the paper. </p>
<p>
Today I thought, for something different, we would supply just news.<br />
<span id="more-18443"></span></p>
<p>
That is links to blogs on computer theory and related math. Links that are current, that are interesting, and that are fit to refer to. All the blogs that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\dots}" class="latex" title="{\dots}" /></p>
<p>
</p><p></p><h2> The Links </h2><p></p>
<p>Our rule is we include a link provided it is current. It must have a recent post—say at least this year. See <a href="http://cstheory-feed.org">all</a> for a complete list. But we decided to emphasis those that are up to date. </p>
<p>
<i>What’s new</i> by Terence Tao<br />
It is <a href="https://terrytao.wordpress.com">here</a>. A recent article is “Boosting the van der Corput inequality using the tensor power trick”.</p>
<p>
<i>Gower’s Weblog</i> by Timothy Gowers<br />
It is <a href="https://gowers.wordpress.com">here</a>. A recent article is “Leicester mathematics under threat again”.</p>
<p>
<i>Not Even Wrong</i> by Peter Woit<br />
It is <a href="http://www.math.columbia.edu/~woit/wordpress/">here</a>. A recent article is “The Future of Fundamental Physics”. </p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/03/30/all-the-news-that-fits-we-print/pure/" rel="attachment wp-att-18449"><img src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/03/pure.png?w=400&amp;ssl=1" alt="" class="aligncenter size-medium wp-image-18449" /></a>





</p><p>
<i>Shtetl-Optimized</i> by  Scott Aaronson<br />
It is <a href="https://www.scottaaronson.com/blog/">here</a>. A recent article is “QC ethics and hype: the call is coming from inside the house”. </p>
<p>
<i>Some Plane Truths</i> by Adam Sheffer<br />
It is <a href="https://adamsheffer.wordpress.com">here</a>. A recent article is “The 2021 Polymath Jr Program”.</p>
<p>
<i>Computational Complexity</i> by Lance Fortnow and Bill Gasarch<br />
It is <a href="https://blog.computationalcomplexity.org">here</a>. A recent article is “The key to my Taylor series problem: Buddy can you spare a penny, nickel, dime, or quarter”. </p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/03/30/all-the-news-that-fits-we-print/book-4/" rel="attachment wp-att-18455"><img width="189" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/03/book-1.png?resize=189%2C300&amp;ssl=1" class="aligncenter size-medium wp-image-18455" height="300" /></a></p>
<p></p><p><br />
<i>11011110</i> by David Eppstein<br />
It is <a href="https://11011110.github.io/blog/">here</a>. A recent article is “More mathematics books by women”.</p>
<p>
<i>Decentralized Thoughts</i> by Ittai Abraham, Ittay Eyal, Kartik Nayak, Ling Ren, Alin Tomescu, Avishay Yanai<br />
It is <a href="https://decentralizedthoughts.github.io">here</a>. A recent article is “Living with Asynchrony: the Gather protocol”.</p>
<p>
<i>Foundation of Data Science </i> by dstheory<br />
It is <a href="https://dstheory.wordpress.com">here</a>. A recent <a href="https://dstheory.wordpress.com/2021/03/28/thursday-april-1st-ingrid-daubechies-from-duke-university/">article</a> previews a talk this Thu. Apr, 1 (at 1pm ET) by Ingrid Daubechies on “Discovering low-dimensional manifolds in high-dimensional data sets.”</p>
<p>
<i>Machine Learning Research Blog</i> by Francis Bach<br />
It is <a href="https://francisbach.com">here</a>. A recent article “Going beyond least-squares – II : Self-concordant analysis for logistic regression”.</p>
<p>
<i>Combinatorics and more</i> by Gil Kalai<br />
It is <a href="https://gilkalai.wordpress.com">here</a>. A recent article “What is mathematics (or at least, how it feels)”. </p>
<p>
<i>Kamathematics</i> by Gautam Kamath<br />
It is <a href="https://kamathematics.wordpress.com">here</a>. A recent article “Learning Theory Alliance and Mentoring Workshop”.</p>
<p>
<i>Process Algebra Diary</i> by Luca Aceto<br />
It is <a href="http://processalgebra.blogspot.com">here</a>. A recent article “Article by Sergey Kitaev and Anthony Mendes in Jeff Remmel’s memory”.</p>
<p>
<i>in Theory</i> by Luca Trevisan<br />
It is <a href="https://lucatrevisan.wordpress.com">here</a>. A recent article is an announcement for accepted papers <a href="http://acm-stoc.org/stoc2021/accepted-papers.html">STOC 2021</a>. </p>
<blockquote><p><b> </b> <em> “Marge, I agree with you — in theory. In theory, communism works. In theory.” — Homer Simpson. </em>
</p></blockquote>
<p></p><p>
<i>Algorithms, Nature, and Society</i> by Nisheeth Vishnoi<br />
It is <a href="https://nisheethvishnoi.wordpress.com">here</a>. A recent article is an announcement for <a href="https://www.cs.yale.edu/homes/vishnoi/focs-2021-cfp.html">FOCS 2022</a>.</p>
<p></p><h2> Open Problems </h2><p></p>
<p>Did we leave any out? Did we include any we should have <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\dots}" class="latex" title="{\dots}" /></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2021/03/30/all-the-news-that-fits-we-print/"><span class="datestr">at March 30, 2021 06:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8037">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/03/29/new-summer-school-in-tcs/">New summer school in TCS</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Shuchi Chawla, Madhur Tulsiani, and I are organizing <a href="https://boazbk.github.io/tcs-summerschool/">a new summer school </a>aimed at exposing undergraduate students to research directions in theoretical computer science and its applications. The school will take place from <strong>May 31 till June 4, 2021</strong>. This first iteration will be online, but we hope it will become a recurring and lasting event.  See <a href="https://boazbk.github.io/tcs-summerschool/" target="_blank" rel="noreferrer noopener">https://boazbk.github.io/tcs-summerschool/</a> for the webpage. We already have 5 exciting instructors signed up – <a href="http://www.cse.psu.edu/~azb1015/">Antonio Blanca</a>, <a href="https://www.ashiawilson.com/">Ashia Wlson</a>, <a href="https://people.eecs.berkeley.edu/~minilek/">Jelani Nelson</a>, <a href="https://immorlica.com/">Nicole Immorlica</a>, and <a href="https://www.microsoft.com/en-us/research/people/yael/">Yael Kalai</a>. We plan to have both technical tutorials / mini-courses and networking/social events.<br /><br />This course is aimed at undergraduate students, and in particular students from groups that are currently under-represented in our field. If you are a faculty member in a university, we would be grateful if you can spread the announcement below to both your colleagues, and any mailing lists for undergraduate students, and in particular chapters of affinity groups such as the National Society of Black Engineers, Society of Hispanic Professional Engineers, National Center for Women &amp; Information Technology, etc.</p>



<hr class="wp-block-separator" />



<p>The committee for advancement of theoretical computer science (<a href="https://thmatters.wordpress.com/catcs/">CATCS</a>) is organizing an online  summer course that will take place on May 31 till June 4, 2021. New horizons in theoretical computer science is a week-long online summer school which will expose undergraduates to exciting research areas in the area of theoretical computer science and its applications. The school will contain several mini-courses from top researchers in the field. The course is free of charge,and we welcome applications from undergraduates majoring in computer science or related fields. We particularly encourage applications from students that are members of groups that are currently under-represented in theoretical computer science. </p>



<p>The course is intended for currently enrolled undergraduate students that are majoring in computer science or related fields. Students will be expected to be familiar with the material typically taught in introductory algorithms and discrete mathematics / mathematics for computer science courses. If you are unsure if you are prepared for the course, please write to us at <a href="mailto:summer-school-admin@boazbarak.org" target="_blank" rel="noreferrer noopener">summer-school-admin@boazbarak.org</a><br /><br />To apply for the course, please visit <a href="https://boazbk.github.io/tcs-summerschool/" target="_blank" rel="noreferrer noopener">https://boazbk.github.io/tcs-summerschool/</a> and fill in the application form by <strong>April 15, 2021</strong>. <br /><br /><strong>Course organizers:</strong> Boaz Barak (Harvard), Shuchi Chawla (UT Austin), Madhur Tulsiani (TTI-Chicago)</p>



<p><strong>Current list of confirmed instructors:</strong>  Antonio Blanca (Penn State University), Ashia Wilson (MIT), Jelani Nelson (UC Berkeley), Nicole Immorlica (Microsoft Research), Yael Kalai (Microsoft research).</p>



<p>Please email  <a href="mailto:summer-school-admin@boazbarak.org" target="_blank" rel="noreferrer noopener">summer-school-admin@boazbarak.org</a> with any questions.</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/03/29/new-summer-school-in-tcs/"><span class="datestr">at March 29, 2021 02:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-7850404914913698515">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/03/slicing-hypercube.html">Slicing the Hypercube</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Here's a neat result I heard about at <a href="https://www.dagstuhl.de/en/program/calendar/semhp/?semnr=21121">virtual Dagstuhl</a> last week, a new lower bound on the number of hyperplanes that cuts all the edges of a hypercube.</p><p>A n-dimensional hypercube has 2<sup>n</sup> vertices corresponding to the binary strings of length n. Edges are between two vertices that differ in exactly one bit, for a total of (n/2) 2<sup>n</sup> edges. Hypercubes played a <a href="https://blog.computationalcomplexity.org/2019/07/degree-and-sensitivity.html">large role</a> in Hao Huang's recent proof of the sensitivity conjecture. </p><p>A hyperplane is just a linear inequality in x<sub>1</sub>,…,x<sub>n</sub> the bits of the string corresponding to a vertex. An edge is cut if the inequality is true for one vertex and false for the other.</p><p>With n hyperplanes you can cut all the edges in two very different ways. </p><p></p><ul style="text-align: left;"><li>The hyperplanes x<sub>i</sub> &gt; 0 for each i. These are n orthogonal hyperplanes.</li><li>The hyperplanes x<sub>1</sub>+…+x<sub>n</sub> &gt; i for each i from 0 to n-1. These are n parallel hyperplanes.</li></ul><div>Do you need n hyperplanes to cut all the edges? Mike Paterson found 5 explicit hyperplanes that cuts all the edges of a 6-dimensional hypercube (see <a href="https://doi.org/10.1017/CBO9780511662089.009">survey</a> by Mike Saks). Scaling that up means you need 5n/6 hyperplanes to cut an n-dimensional hypercube. That remains the best known upper bound.</div><div><br /></div><div>For the lower bound, in 1971 Patrick O'Neil <a href="https://doi.org/10.1016/0012-365X(71)90025-2">showed</a> that any hyperplane can cut at most O(n<sup>0.5</sup>) fraction of the edges (sharp by the hyperplane x<sub>1</sub>+…+x<sub>n</sub> &gt; n/2). Thus you need at least O(n<sup>0.5</sup>) hyperplanes which for 50 years was the best known bound.</div><div><br /></div><div>Gil Yehuda and Amir Yehudayoff have <a href="https://arxiv.org/abs/2102.05536">given a new lower bound</a> of O(n<sup>0.57</sup>). The paper gives a O(n<sup>0.51</sup>) bound but Yehudayoff said in a talk last week the bound has been improved.</div><div><br /></div><div>Yehudayoff didn't go into much details in his 20 minute talk but the proofs uses geometry, probability and antichains. </div><div><br /></div><div>The result has some applications to complexity, namely you need at least n<sup>1.57</sup> wires in a depth-two threshold circuit for parity. But the main fun is the question itself, that we finally made progress and there is still a long way to go.</div><p></p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/03/slicing-hypercube.html"><span class="datestr">at March 29, 2021 12:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://dstheory.wordpress.com/?p=91">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://dstheory.wordpress.com/2021/03/28/thursday-april-1st-ingrid-daubechies-from-duke-university/">Thursday April 1st — Ingrid Daubechies  from Duke University</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p class="has-text-align-justify">The next <a href="https://sites.google.com/view/dstheory/home" target="_blank" rel="noreferrer noopener">Foundations of Data Science</a> virtual talk will take place on <strong>Thursday, April 1</strong>st at <strong>10:00 AM Pacific Time</strong> (13:00 Eastern Time, 19:00 Central European Time, 17:00 UTC).  <strong><a href="https://ece.duke.edu/faculty/ingrid-daubechies" target="_blank" rel="noreferrer noopener">Ingrid Daubechies</a></strong> from <strong>Duke Univeristy</strong> will speak about “Discovering low-dimensional manifolds in high-dimensional data sets.”</p>



<p><a href="https://sites.google.com/view/dstheory" target="_blank" rel="noreferrer noopener">Please register here to join the virtual talk.</a></p>



<p class="has-text-align-justify"><strong>Abstract</strong>: This talk reviews diffusion methods to identify low-dimensional manifolds underlying high-dimensional datasets, and illustrates that by pinpointing additional mathematical structure, improved results can be obtained. Much of the talk draws on a case study from a collaboration with biological morphologists, who compare different phenotypical structures to study relationships of living or extinct animals with their surroundings and each other. This is typically done from carefully defined anatomical correspondence points (landmarks) on e.g. bones; such landmarking draws on highly specialized knowledge. To make possible more extensive use of large (and growing) databases, algorithms are required for automatic morphological correspondence maps, without any preliminary marking of special features or landmarks by the user.</p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>







<p class="date">
by dstheory <a href="https://dstheory.wordpress.com/2021/03/28/thursday-april-1st-ingrid-daubechies-from-duke-university/"><span class="datestr">at March 28, 2021 03:16 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/048">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/048">TR21-048 |  Better Pseudodistributions and Derandomization for Space-Bounded Computation | 

	William Hoza</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Three decades ago, Nisan constructed an explicit pseudorandom generator (PRG) that fools width-$n$ length-$n$ read-once branching programs (ROBPs) with error $\varepsilon$ and seed length $O(\log^2 n + \log n \cdot \log(1/\varepsilon))$ (Combinatorica 1992). Nisan's generator remains the best explicit PRG known for this important model of computation. However, a recent line of work starting with Braverman, Cohen, and Garg (Braverman, Cohen, and Garg SICOMP 2020; Chattopadhyay and Liao CCC 2020; Cohen, Doron, Renard, Sberlo, and Ta-Shma ECCC 2021; Pyne and Vadhan ECCC 2021) has shown how to construct *weighted* pseudorandom generators (WPRGs, aka pseudorandom pseudodistribution generators) with better seed lengths than Nisan's generator when the error parameter $\varepsilon$ is small.
		
In this work, we present an explicit WPRG for width-$n$ length-$n$ ROBPs with seed length $O(\log^2 n + \log(1/\varepsilon))$. Our seed length eliminates $\log \log$ factors from prior constructions, and our generator completes this line of research in the sense that further improvements would require beating Nisan's generator in the standard constant-error regime. Our technique is a variation of a recently-discovered reduction that converts moderate-error PRGs into low-error WPRGs (Cohen et al. ECCC 2021; Pyne and Vadhan ECCC 2021). Our version of the reduction uses averaging samplers.
		
We also point out that as a consequence of the recent work on WPRGs, any randomized space-$S$ decision algorithm can be simulated deterministically in space $O(S^{3/2} / \sqrt{\log S})$. This is a slight improvement over Saks and Zhou's celebrated $O(S^{3/2})$ bound (JCSS 1999). For this application, our improved WPRG is not necessary.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/048"><span class="datestr">at March 27, 2021 08:27 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18424">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/03/26/congrats-avi-and-laci-on-the-abel-prize/">Congrats Avi and Laci on the Abel Prize</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><font color="#0044cc"><br />
<em>The joy of knowing, understanding, and creating is common to all the sciences.—Vera Sós.</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/03/da.png?ssl=1"><img width="150" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/03/da.png?resize=150%2C155&amp;ssl=1" class="alignright wp-image-18426" height="155" /></a></p>
<p>
Dorit Aharonov is a researcher in quantum computation. She was a Ph.D. student of Michael Ben-Or and Avi Wigderson. Her 1999 <a href="https://www.cs.huji.ac.il/~doria/thesis.ps">thesis</a>, entitled “Noisy Quantum Computation,” included her role with Ben-Or as one of several superposed groups who discovered the quantum fault tolerance threshold <a href="https://en.wikipedia.org/wiki/Quantum_threshold_theorem">theorem</a>.</p>
<p>
Today Ken and I wish to send our congratulations to Avi and Laci—László Lovász—on winning the 2021 <a href="https://www.abelprize.no">Abel Prize</a>.<br />
<span id="more-18424"></span></p>
<p>
The prize citation is: </p>
<blockquote><p><b> </b> <em> For their foundational contributions to theoretical computer science and discrete mathematics, and their leading role in shaping them into central fields of modern mathematics. </em>
</p></blockquote>
<p></p><h2> Genealogy </h2><p></p>
<p>
You might wonder why we are talking about Aharonov? She was a student of Avi and he was a student of mine—see <a href="https://www.mathgenealogy.org/id.php?id=82100">this</a> for more details. I am proud of Avi for his terrific results and also for his great students. I guess Aharonov is a grand-student of mine. </p>
<p>
Lovász and Ken’s advisor, Dominic Welsh of Oxford, partnered with Dominic’s student Magnus Bordewich and Michael Freedman of Microsoft on a 2005 <a href="http://community.dur.ac.uk/m.j.r.bordewich/papers/Bordewich2005-c.pdf">paper</a> titled, “Approximate counting and quantum computation.” The paper translates criteria for approximating the <a href="https://en.wikipedia.org/wiki/Jones_polynomial">Jones polynomial</a> into simulations of quantum circuits. It references how topological quantum field theory (TQFT) implicitly embodies a quantum algorithm that meets the criteria, hence showing that the Jones approximation problem is complete for <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BBQP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{BQP}}" class="latex" title="{\mathsf{BQP}}" /> in a suitable promise-problem sense. </p>
<p>
The genealogy of the Jones polynomial is that it was discovered by Vaughan Jones. Aharonov, with Jones and his student Zeph Landau, wrote a 2009 <a href="https://link.springer.com/content/pdf/10.1007/s00453-008-9168-0.pdf">paper</a> that makes the quantum algorithm for approximating the Jones polynomial explicit. To quote the converse of a statement in their abstract, removing the dependence on TQFT makes the algorithm “accessible to computer scientists.”</p>
<p>
Aharonov’s work thus represents the mixing of elements of classical mathematics and computing theory (including quantum, i.e. non-classical, computing) that this year’s Abel Prize recognizes. <em>Mixing</em> is indeed the word, since results on the mixing rates of Markov chains by Lovász and many others undergird some of this work. This is true even more directly of a notable <a href="https://arxiv.org/pdf/quant-ph/0012090.pdf">paper</a> by her with Andris Ambainis, Julia Kempe, and Umesh Vazirani on quantum walks. </p>
<p></p><h2> More Quantum </h2><p></p>
<p>
When we hosted the 2012 <a href="https://rjlipton.wpcomstaging.com/2012/01/30/perpetual-motion-of-the-21st-century/">debate</a> between Gil Kalai and Aram Harrow on the feasibility of large-scale quantum computing, Gil led off with the following photo of the physicist Robert Alicki, Ben-Or, Aharonov, and himself, which was taken in Jerusalem in 2005.</p>
<p></p><p>
<a href="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/03/BAAK.png?ssl=1"><img width="557" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/03/BAAK.png?resize=557%2C415&amp;ssl=1" class="aligncenter wp-image-18428" height="415" /></a></p>
<p></p><p>
Aharonov wrote an influential long <a href="https://arxiv.org/pdf/quant-ph/9812037.pdf">survey</a> in 1998 titled “Quantum Computation,” which grew out of her thesis and is sometimes <a href="https://www.researchgate.net/publication/2864868_Noisy_Quantum_Computation">confused</a> with it. Ken and I owe a small debt to diagrams she used in the survey, for which she in turn alluded to Richard Feynman’s path integrals and their visualizations: </p>
<p></p><p>
<a href="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/03/AharonovSnip.png?ssl=1"><img width="487" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/03/AharonovSnip.png?resize=487%2C231&amp;ssl=1" class="aligncenter size-full wp-image-18429" height="231" /></a></p>
<p></p><p>
We expanded on this kind of diagram in our textbook on quantum computing with MIT Press, whose <a href="https://mitpress.mit.edu/books/introduction-quantum-algorithms-linear-algebra-second-edition">second edition</a> is scheduled to appear next month. </p>
<p></p><h2> Abel Prize and Lower Bounds </h2><p></p>
<p>
Whereas the Fields Medals are awarded only every four years, the <a href="https://en.wikipedia.org/wiki/Abel_Prize">Abel Prize</a> is annual. It commemorates the Norwegian mathematician Niels Abel. Although it has been awarded only since 2003, it was originally proposed in 1899 expressly as a counterpart to the Nobel Prizes—and fell through only because of the 1905 separation of Sweden and Norway. We most recently <a href="https://rjlipton.wpcomstaging.com/2019/03/21/the-shortest-path-to-the-abel-prize/">covered</a> the 2019 prize going to Karen Uhlenbeck.</p>
<p>
Abel of course is the mathematician who gave his name to abelian groups and much <a href="https://en.wikipedia.org/wiki/List_of_things_named_after_Niels_Henrik_Abel">else</a>. About his short life of twenty-seven years, Charles Hermite said: </p>
<blockquote><p><b> </b> <em> Abel has left mathematicians enough to keep them busy for five hundred years. </em>
</p></blockquote>
<p>
What we note is that Abel’s most famous result is a lower bound type result. He showed the impossibility of solving the general quintic equation in radicals. Finding clever identities, clever inequities, clever algorithms is hard. But not as hard as showing that something cannot be done. </p>
<p>
Abel’s result is perhaps like proving today, in the 21st century, a lower bound on the complexity of some concrete problem. Some of Avi’s and also Laci’s best work has been directed toward this quest: Prove some non-trivial lower bound. Avi has worked on numerous approaches, including the <a href="https://www.math.ias.edu/avi/node/817">fusion method</a> in 1993. Although its lack of breakthroughs has caused jibes about “<a href="https://en.wikipedia.org/wiki/Cold_fusion">cold fusion</a>,” Avi noted the following in a recent <a href="https://simons.berkeley.edu/talks/tbd-9">talk</a> at the Simons Institute: </p>
<blockquote><p><b> </b> <em> I will review (and hope to revive) a collection of old works, which suggest obtaining circuit lower bounds by “fusing” many correct computations (of a circuit too small) into an incorrect computation.</em></p><em>
</em><p><em>
In a couple of models, this viewpoint led to (slight) superlinear lower bounds, which nonetheless have not been improved upon for 25 years. </em>
</p></blockquote>
<p>
We hope that the Abel Prize award will promote efforts to achieve not-so-slightly super-linear lower bounds on circuit size, running time, algebraic size, quantum effort, anything like that…</p>
<p></p><h2> Open Problems </h2><p></p>
<p>
We also tip our hat to others who have reported on the Abel Prize, including some of our fellow bloggers: Gil Kalai <a href="https://gilkalai.wordpress.com/2021/03/17/cheerful-news-in-difficult-times-the-abel-prize-is-awarded-to-laszlo-lovasz-and-avi-wigderson/">here</a>, Scott Aaronson <a href="https://www.scottaaronson.com/blog/?p=5388">here</a>, Lance Fortnow and Bill Gasarch <a href="https://blog.computationalcomplexity.org/2021/03/i-read-news-today-oh-boy.html">here</a>, plus Kevin Hartnett for <i>Quanta</i> <a href="https://www.quantamagazine.org/avi-wigderson-and-laszlo-lovasz-win-abel-prize-20210317/">here</a>.</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wpcomstaging.com/2021/03/26/congrats-avi-and-laci-on-the-abel-prize/"><span class="datestr">at March 26, 2021 04:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2021-03-26-living-with-asynchrony-the-gather-protocol/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2021-03-26-living-with-asynchrony-the-gather-protocol/">Living with Asynchrony: the Gather protocol</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A very useful tool in Asynchronus distributed computing is Reliable Broadcast, or simply called Broadcast. It allows a leader to send a message, knowing that all parties will eventually receive the same message, even if a malicious adversary control $f$ parties and $f&lt;n/3$. Broadcast is deterministic and takes just a...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2021-03-26-living-with-asynchrony-the-gather-protocol/"><span class="datestr">at March 26, 2021 10:54 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/047">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/047">TR21-047 |  Random restrictions and PRGs for PTFs in Gaussian Space | 

	Zander Kelley, 

	Raghu Meka</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A polynomial threshold function (PTF) $f:\mathbb{R}^n \rightarrow \mathbb{R}$ is a function of the form $f(x) = sign(p(x))$ where $p$ is a polynomial of degree at most $d$. PTFs are a classical and well-studied complexity class with applications across complexity theory, learning theory, approximation theory, quantum complexity and more. We address the question of designing pseudorandom generators (PRG) for polynomial threshold functions (PTFs) in the gaussian space: design a PRG that takes a seed of few bits of randomness and outputs a $n$-dimensional vector whose distribution is indistinguishable from a standard multivariate gaussian by a degree $d$ PTF. 
     
     Our main result is a PRG that takes a seed of $d^{O(1)}\log ( n / \varepsilon)\log(1/\varepsilon)/\varepsilon^2$ random bits with output that cannot be distinguished from $n$-dimensional gaussian distribution with advantage better than $\varepsilon$ by degree $d$ PTFs. The best previous generator due to O'Donnell, Servedio, and Tan (STOC'20) had a quasi-polynomial dependence (i.e.,  seedlength of $d^{O(\log d)}$) in the degree $d$. Along the way we prove a few nearly-tight  structural properties of restrictions of PTFs that may be of independent interest.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/047"><span class="datestr">at March 26, 2021 10:15 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-6040846843118046934">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/03/the-key-to-my-taylor-series-problem.html">The key to my Taylor series problem: Buddy can you spare a penny, nickel, dime, or quarter</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> In my last blog post I posed a question about finding the coeff of x^100 in a particular Taylor Series. The question and answer are given  <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/taylorcoins.pdf">here</a>:</p><p><br /></p><p>The key to the problem was to recognize that it was asking how many ways you can make change of a dollar using pennies, nickels, dimes, and quarters. This can be done by hand (its 242). </p><p>1) Someone who I gave the problem to solved it by available software, but when he saw the answer was 242 he realized how to do it via making change.</p><p><br /></p><p>2) How hard would this problem me to do completely by hand- say as a Putnam Problem? Its hard for me to say since I started with the trick and then found the problem. </p><p><br /></p><p>3) Is this a well known trick? </p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/03/the-key-to-my-taylor-series-problem.html"><span class="datestr">at March 26, 2021 02:58 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://toc4fairness.org/?p=1551">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/fair.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://toc4fairness.org/videos-up/">Videos Up</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>With a bit of a delay, we are starting to upload the videos of our seminar’s talks. The <a href="https://toc4fairness.org/inaugural-meeting-toc4fairness-seminar-annette-zimmermann/">Inaugural Meeting</a> of our seminar was devoted to a wonderful talk by Dr. <a href="https://www.annette-zimmermann.com/">Annette Zimmermann</a>.  I highly recommend watching <a href="https://toc4fairness.org/inaugural-meeting-toc4fairness-seminar-annette-zimmermann-2/">the video</a>, and following up with some additional reading (some pointers below).</p>



<p>I won’t try to summarize the talk because I doubt that I can do it justice, but one of the themes (which I fully support) is that it is not enough to consider “fair” implementations of specific tasks. Instead, we (also) want to explore the right task to implement and if it is appropriate to implement any algorithmic task in any specific context. </p>



<p>As a side note, I loved Annette’s statement on ethics,  “if we can choose, we’re on the hook.” For me, it beautifully complements the paradigm that “ought implies can.” In other words, ethical imperatives only exist when the expected action is possible but every choice has ethical implications.   </p>



<hr class="wp-block-separator" />



<p>Some resources for additional reading.</p>



<p><strong>Resources on exploitation</strong></p>



<p><strong>Introductory / very accessible for an interdisciplinary audience</strong></p>



<p>Nicholas Vrousalis, “Exploitation: A Primer,” <em>Philosophy Compass</em> 13, no. 2 (2018).</p>



<p><strong>Background</strong></p>



<p>G.A. Cohen, “The Labor Theory of Value and the Concept of Exploitation,” <em>Philosophy and Public Affairs</em> 8, no. 4 (1979): 338–360.</p>



<p>Joel Feinberg, <em>Harmless Wrongdoing</em>, Oxford: Oxford University Press (1988).`</p>



<p>Robert E. Goodin, “Exploiting a Situation and Exploiting a Person,” in Andrew Reeve (ed.), <em>Modern Theories of Exploitation</em>, London: Sage (1987), 166–200.</p>



<p>Ruth Sample, <em>Exploitation, What It Is and Why it is Wrong</em>, Lanham, MD: Rowman and Littlefield (2003).</p>



<p>Nicholas Vrousalis, “Exploitation, Vulnerability, and Social Domination,” <em>Philosophy and Public Affairs</em>, 41, no. 2 (2013): 131–157.</p>



<p>Alan Wertheimer, <em>Exploitation</em>, Princeton: Princeton University Press (1996).</p>



<p>Iris Marion Young, “Five Faces of Oppression,” in Thomas Wartenberg (ed.), <em>Rethinking Power</em>, Albany, NY: SUNY Press (1992).</p>



<p><strong>Resource on the political philosophy of AI (for a general audience)</strong></p>



<p>Annette Zimmermann, Elena Di Rosa, Hochan Kim, “<a href="http://bostonreview.net/science-nature-politics/annette-zimmermann-elena-di-rosa-hochan-kim-technology-cant-fix-algorithmic">Technology Can’t Fix Algorithmic Injustice</a>, <em>Boston Review</em> </p></div>







<p class="date">
by Omer Reingold <a href="https://toc4fairness.org/videos-up/"><span class="datestr">at March 25, 2021 10:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2021/03/25/beyondNTK/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2021/03/25/beyondNTK/">When are Neural Networks more powerful than Neural Tangent Kernels?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>The empirical success of deep learning has posed significant challenges to machine learning theory: Why can we efficiently train neural networks with gradient descent despite its highly non-convex optimization landscape? Why do over-parametrized networks generalize well? The recently proposed Neural Tangent Kernel (NTK) theory offers a powerful framework for understanding these, but yet still comes with its limitations.</p>

<p>In this blog post, we explore how to analyze wide neural networks beyond the NTK theory, based on our recent <a href="https://arxiv.org/abs/1910.01619">Beyond Linearization paper</a> and follow-up <a href="https://arxiv.org/abs/2006.13436">paper on understanding hierarchical learning</a>. (This blog post is also cross-posted at the <a href="https://blog.einstein.ai/beyond-ntk/">Salesforce Research blog</a>.)</p>

<h3 id="neural-tangent-kernels">Neural Tangent Kernels</h3>
<p>The Neural Tangent Kernel (NTK) is a recently proposed theoretical framework for establishing provable convergence and generalization guarantees for wide (over-parametrized) neural networks <a href="https://arxiv.org/abs/1806.07572">(Jacot et al. 2018)</a>. Roughly speaking, the NTK theory shows that</p>

<ul>
  <li>A sufficiently wide neural network trains like a linearized model governed by the derivative of the network with respect to its parameters.</li>
  <li>At the infinite-width limit, this linearized model becomes a kernel predictor with the Neural Tangent Kernel (the NTK).</li>
</ul>

<p>Consequently, a wide neural network trained with small learning rate converges to 0 training loss and generalize as well as the infinite-width kernel predictor. For a detailed introduction to the NTK, please refer to the earlier <a href="http://www.offconvex.org/2019/10/03/NTK/">blog post</a> by Wei and Simon.</p>

<h3 id="does-ntk-fully-explain-the-success-of-neural-networks">Does NTK fully explain the success of neural networks?</h3>
<p>Although the NTK yields powerful theoretical results, it turns out that real-world deep learning <em>do not operate in the NTK regime</em>:</p>

<ul>
  <li>Empirically, infinite-width NTK kernel predictors perform slightly worse (though competitive) than fully trained neural networks on benchmark tasks such as CIFAR-10 <a href="https://arxiv.org/abs/1904.11955">(Arora et al. 2019b)</a>. For finite width networks in practice, this gap is even more profound, as we see in Figure 1: The linearized network is a rather poor approximation of the fully trained network at practical optimization setups such as large initial learning rate <a href="https://arxiv.org/abs/2002.04010">(Bai et al. 2020)</a>.</li>
  <li>Theoretically, the NTK has poor <em>sample complexity for learning certain simple functions</em>. Though the NTK is a universal kernel that can interpolate any finite, non-degenerate training dataset <a href="https://arxiv.org/abs/1810.02054">(Du et al. 2018</a><a href="https://arxiv.org/abs/1811.03804">, 2019)</a>, the test error of this kernel predictor scales with the RKHS norm of the ground truth function. For certain non-smooth but simple functions such as a single ReLU, this norm can be exponentially large in the feature dimension <a href="https://arxiv.org/abs/1904.00687">(Yehudai &amp; Shamir 2019)</a>. Consequently, NTK analyses yield poor sample complexity upper bounds for learning such functions, whereas empirically neural nets only require a mild sample size <a href="https://arxiv.org/abs/1410.1141">(Livni et al. 2014)</a>.</li>
</ul>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/taylor-plot.png" style="width: 700px;" />
<br />
<i><b>Figure 1.</b>
Linearized model does not closely approximate the training trajectory of neural networks with practical optimization setups, whereas higher order Taylor models offer a substantially better approximation.
</i>
<br />
<br />
</div>

<p>These gaps urge us to ask the following</p>
<blockquote>
  <p><strong>Question</strong>: How can we theoretically study neural networks beyond the NTK regime? Can we prove that neural networks outperform the NTK on certain learning tasks?</p>
</blockquote>

<p>The key technical question here is to mathematically understand neural networks operating <em>outside of the NTK regime</em>.</p>

<h2 id="higher-order-taylor-expansion">Higher-order Taylor expansion</h2>
<p>Our main tool for going beyond the NTK is the <em>Taylor expansion</em>. Consider a two-layer neural network with $m$ neurons, where we only train the “bottom” nonlinear layer $W$:</p>

\[f_{W_0 + W}(x) = \frac{1}{\sqrt{m}} \sum_{r=1}^m a_r \sigma( (w_{0,r} + w_r)^\top x).\]

<p>(Here, $W_0+W$ is an $m\times d$ weight matrix, where $W_0$ denotes the random initialization and $W$ denotes the trainable “movement” matrix initialized at zero). For small enough $W$, we can perform a Taylor expansion of the network around $W_0$ and get</p>

\[f_{W_0+W}(x) = \frac{1}{\sqrt{m}} \sum_{r=1}^m a_r \sigma(w_{0,r}^\top x) + \sum_{k=1}^\infty \frac{1}{\sqrt{m}} \sum_{r=1}^m a_r \frac{\sigma^{(k)} (w_{0,r}^\top x)}{k!} (w_r^\top x)^k\]

<p>Let us denote the $k$-th order term as $ f^{(k)}_{W_0, W}$, and rewrite this as</p>

\[f_{W_0+W}(x) = f^{(0)}_{W_0}(x) + \sum_{k=1}^\infty f^{(k)}_{W_0, W}(x).\]

<p>Above, term $f^{(k)}$ is a $k$-th order polynomial of the trainable parameter $W$. For the moment assume that $f^{(0)}(x)=0$ (this can be achieved via techniques such as the symmetric initialization).</p>

<p>The key insight of the NTK theory can be described as the following <strong>linearized approximation</strong> property</p>
<blockquote>
  <p>For small enough $W$, the neural network $f_{W_0,W}$ is closely approximated by the linear model $f^{(1)}$.</p>
</blockquote>

<p>Towards moving beyond the linearized approximation, in our <a href="https://arxiv.org/abs/1910.01619">Beyond Linearization paper</a>, we start by asking</p>
<blockquote>
  <p>Why just $f^{(1)}$? Can we also utilize the higher-order term in the Taylor series such as $f^{(2)}$?</p>
</blockquote>

<p>At first sight, this seems rather unlikely, as in Taylor expansions we always expect the linear term $f^{(1)}$ to dominate the whole expansion and have a larger magnitude than $f^{(2)}$ (and subsequent terms).</p>

<h3 id="killing-the-ntk-term-by-randomized-coupling">“Killing” the NTK term by randomized coupling</h3>
<p>We bring forward the idea of <em>randomization</em>, which helps us escape the “domination” of $f^{(1)}$ and couple neural networks with their quadratic Taylor expansion term $f^{(2)}$. This idea appeared first in <a href="https://arxiv.org/abs/1811.04918">Allen-Zhu et al. (2018)</a> for analyzing three-layer networks, and as we will show also applies to two-layer networks in a perhaps more intuitive fashion.</p>

<p>Let us now assign each weight movement $w_r$ with a <em>random sign</em> $s_r\in\{\pm 1\}$, and consider the randomized weights $\{s_rw_r\}$. The random signs satisfy the following basic properties:</p>

\[E[s_r]=0 \quad {\rm and} \quad s_r^2 \equiv 1.\]

<p>Therefore, let $SW\in\mathbb{R}^{m\times d}$ denote the randomized weight matrix, we can compare the first and second order terms in the Taylor expansion at $SW$:</p>

\[E_{S} \left[f^{(1)}_{W_0, SW}(x)\right] = E_{S} \left[ \frac{1}{\sqrt{m}}\sum_{r\le m} a_r \sigma'(w_{0,r}^\top x) (s_rw_r^\top x) \right] = 0,\]

<p>whereas</p>

\[f^{(2)}_{W_0, SW}(x) = \frac{1}{\sqrt{m}}\sum_{r\le m} a_r \frac{\sigma^{(2)}(w_{0,r}^\top x)}{2} (s_rw_r^\top x)^2 = \frac{1}{\sqrt{m}}\sum_{r\le m} a_r \frac{\sigma^{(2)}(w_{0,r}^\top x)}{2} (w_r^\top x)^2 = f^{(2)}_{W_0, W}(x).\]

<p>Observe that the sign randomization keeps the quadratic term $f^{(2)}$ unchanged, but “kills” the linear term $f^{(1)}$ in expectation!</p>

<p>If we train such a randomized network with freshly sampled signs $S$ at each iteration, the linear term $f^{(1)}$ will keep oscillating around zero and does not have any power in fitting the data, whereas the quadratic term is not affected at all and thus becomes the leading force for fitting the data. (The keen reader may notice that this randomization is similar to Dropout, with the key difference being that we randomize the weight <em>movement</em> matrix, whereas vanilla Dropout randomizes the weight matrix itself.)</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/beyond-ntk.png" style="width: 700px;" />
<br />
<i><b>Figure 2.</b>
The NTK regime operates in the "NTK ball" where the network is approximately equal to the linear term. The quadratic regime operates in a larger ball where the network is approximately equal to the sum of first two terms, but the linear term dominates and can blow up at large width. Our randomized coupling technique resolves this by introducing the random sign matrix that in expectation "kills" the linear term but always preserves the quadratic term.
</i>
<br />
<br />
</div>

<p>Our first result shows that networks with sign randomization can still be efficiently optimized, despite its now non-convex optimization landscape:</p>
<blockquote>
  <p><strong>Theorem</strong>: Any escaping-saddle algorithm (e.g. noisy SGD) on the regularized loss function $E_S[L(W_0+SW)]+R(W)$, with freshly sampled sign $S=S_t$ per iteration, can find the global minimum in polynomial time.</p>
</blockquote>

<p>The proof builds on the quadratic approximation $E_S[f]\approx f^{(2)}$ and recent understandings on neural networks with quadratic activation, e.g. <a href="https://arxiv.org/abs/1707.04926">Soltanolkotabi et al. (2017)</a> &amp; <a href="https://arxiv.org/abs/1803.01206">Du and Lee (2018)</a>.</p>

<h3 id="generalization-and-sample-complexity-case-study-on-learning-low-rank-polynomials">Generalization and sample complexity: Case study on learning low-rank polynomials</h3>
<p>We next study the generalization of these networks in the context of learning <em>low-rank degree-$p$ polynomials</em>:</p>

\[f_\star(x) = \sum_{s=1}^{r_\star} \alpha_s (\beta_s^\top x)^{p_s}, \quad |\alpha_s|\le 1,\|(\beta_s^\top x)^{p_s}\|_{L_2} \le 1, p_s\le p \quad \textrm{for all } s.\]

<p>We are specifically interested in the case where $r_\star$ is small (e.g. $O(1)$), so that $y$ only depends on the projection of $x$ on a few directions. This for example captures teacher networks with polynomial activation of bounded degree and analytic activation (approximately), as well as constant depth teacher networks with polynomial activations.</p>

<p>For the NTK, the sample complexity of learning polynomials have been studied extensively in <a href="https://arxiv.org/abs/1901.08584">(Arora et al. 2019a)</a>, <a href="https://arxiv.org/abs/1904.12191">(Ghorbani et al. 2019)</a>, and many concurrent work. Combined, they showed that the sample complexity for learning degree-$p$ polynomials is $\Theta(d^p)$, with matching lower and upper bounds:</p>
<blockquote>
  <p><strong>Theorem (NTK)</strong> : Suppose $x$ is uniformly distributed on the sphere, then the NTK requires $O(d^p)$ samples in order to achieve a small test error for learning any degree-$p$ polynomial, and there is a matching lower bound of $\Omega(d^p)$ for any inner-product kernel method.</p>
</blockquote>

<p>In our <a href="https://arxiv.org/abs/1910.0161">Beyond Linearization paper</a>, we show that the quadratic Taylor model achieves an improved sample complexity of  $\tilde{O}(d^{p-1})$ with isotropic inputs:</p>
<blockquote>
  <p><strong>Theorem (Quadratic Model)</strong>: For mildly isotropic input distributions, the two-layer quadratic Taylor model (or two-layer NN with sign randomization) only requires $\tilde{O}({\rm poly}(r_\star, p)d^{p-1})$ samples in order to achieve a small test error for learning a low-rank degree-$p$ polynomial.</p>
</blockquote>

<p>In our <a href="https://arxiv.org/abs/2006.13436">follow-up paper on understanding hierarchical learning</a>, we further design a “hierarchical learner” using a specific three-layer network, and show the following</p>
<blockquote>
  <p><strong>Theorem (Three-layer hierarchical model)</strong>: Under mild input distribution assumptions, a three-layer network with a fixed representation layer of width $D=d^{p/2}$ and a trainable quadratic Taylor layer can achieve a small test error using only $\tilde{O}({\rm poly}(r_\star, p)d^{p/2})$ samples.</p>
</blockquote>

<p>When $r_\star,p=O(1)$, the quadratic Taylor model can improve over the NTK by a multiplicative factor of $d$, and we can further get a substantially larger improvement of $d^{p/2}$ by using the three-layer hierarchical learner. Here we briefly discuss the proof intuitions, and refer the reader to our papers for more details.</p>

<ul>
  <li>
    <p><strong>Generalization bounds</strong>: We show that, while the NTK and quadratic Taylor model expresses functions using similar random feature constructions, their generalization depends differently on the norm of the input. In the NTK, the generalization depends on the L2 norm of the features (as well as the weights), whereas generalization of the quadratic Taylor model depends on the operator norm of the input matrix features $\frac{1}{n}\sum x_ix_i^\top$ times the nuclear norm of $\sum w_rw_r^\top$. It turns out that this decomposition can match the one given by the NTK (it is never worse), and in addition be better by a factor of $O(\sqrt{d})$ if the input distribution is mildly isotropic so that $\|\frac{1}{n}\sum x_ix_i^\top\|_{\rm op} \le 1/\sqrt{d} \cdot \max \|x_i\|_2^2$, leading to the $O(d)$ improvement in the sample complexity.</p>
  </li>
  <li>
    <p><strong>Hierarchical learning</strong>: The key intuition behind the hierarchical learner is that we can utilize the $O(d)$ sample complexity gain to its fullest, by applying quadratic Taylor model to not the input $x$, but a feature representation $h(x)\in \mathbb{R}^D$ where $D\gg d$. This yields a gain as long as $h$ is rich enough to express $f_\star$ and also isotropic enough to let the operator norm $\|\frac{1}{n}\sum h(x_i)h(x_i)^\top\|_{\rm op}$ be nice. In particular, for learning degree-$p$ polynomials, the best we can do is to choose $D=d^{p/2}$, leading to a sample complexity saving of $\tilde{O}(D)=\tilde{O}(d^{p/2})$.</p>
  </li>
</ul>

<h3 id="concluding-thoughts">Concluding thoughts</h3>
<p>In this post, we explored higher-order Taylor expansions (in particular the quadratic expansion) as an approach to deep learning theory beyond the NTK regime. The Taylorization approach has several advantages:</p>

<ul>
  <li>Non-convex but benign optimization landscape;</li>
  <li>Provable generalization benefits over NTKs;</li>
  <li>Ability of modeling hierarchical learning;</li>
  <li>Convenient API for expeirmentation (cf. the <a href="https://github.com/google/neural-tangents">Neural Tangents</a> package and the <a href="https://arxiv.org/abs/2002.04010">Taylorized training</a> paper).</li>
</ul>

<p>We believe these advantages make the Taylor expansion a powerful tool for deep learning theory, and our results are just a beginning. We also remark that there are other theoretical frameworks such as the <a href="https://arxiv.org/abs/1909.08156">Neural Tangent Hierarchy</a> or the <a href="https://arxiv.org/abs/1804.06561">Mean-Field Theory</a> that go beyond the NTK with their own advantages in various angles, but without computational efficiency guarantees. See the <a href="https://jasondlee88.github.io/slides/beyond_ntk.pdf">slides</a> for more on going beyond NTK. Making progress on any of these directions (or coming up with new ones) would be an exciting direction for future work.</p></div>







<p class="date">
<a href="http://offconvex.github.io/2021/03/25/beyondNTK/"><span class="datestr">at March 25, 2021 02:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/046">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/046">TR21-046 |  Fourier Growth of Parity Decision Trees | 

	Uma Girish, 

	Avishay Tal, 

	Kewen Wu</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove that for every parity decision tree of depth $d$ on $n$ variables, the sum of absolute values of Fourier coefficients at level $\ell$ is at most $d^{\ell/2} \cdot O(\ell \cdot \log(n))^\ell$. 
Our result is nearly tight for small values of $\ell$ and extends a previous Fourier bound for standard decision trees by Sherstov, Storozhenko, and Wu (STOC, 2021). 

As an application of our Fourier bounds, using the results of Bansal and Sinha (STOC, 2021), we show that the $k$-fold Forrelation problem has (randomized) parity decision tree complexity  $\tilde{\Omega}\left(n^{1-1/k}\right)$, while having quantum query complexity $\lceil k/2\rceil$. 

Our proof follows a random-walk approach, analyzing the contribution of a random path in the decision tree to the level-$\ell$ Fourier expression. 
To carry the argument, we apply a careful cleanup procedure to the parity decision tree, ensuring that the value of the random walk is bounded with high probability. We observe that step sizes for the level-$\ell$ walks can be computed by the intermediate values of level $\le \ell-1$ walks, which calls for an inductive argument.
Our approach differs from previous proofs of Tal (FOCS, 2020) and Sherstov, Storozhenko, and Wu (STOC, 2021) that relied on decompositions of the tree. In particular, for the special case of standard decision trees we view our proof as slightly simpler and more intuitive.

In addition, we prove a similar bound for noisy decision trees of cost at most $d$ -- a model that was recently introduced by Ben-David and Blais (FOCS, 2020).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/046"><span class="datestr">at March 25, 2021 03:37 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=539">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2021/03/23/tcs-talk-wednesday-march-31-jasper-lee-brown-university/">TCS+ talk: Wednesday, March 31 — Jasper Lee, Brown University</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, March 31th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="http://cs.brown.edu/~clee10/"><strong>Jasper Lee</strong></a> from Brown University will speak about “<em>Optimal Sub-Gaussian Mean Estimation in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" alt="\mathbb{R}" class="latex" /></em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards, so people who did not sign up will still be able to watch the talk)</p>
<p>As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: We revisit and settle a fundamental problem in statistics: given access to independent samples from a 1D random variable (with finite but unknown mean and variance), what is the best way to estimate the mean in the high probability regime, in terms of error convergence with respect to sample size? The conventional wisdom is to use the empirical mean as our estimate. However, it is known that the empirical mean can in fact have exponentially sub-optimal convergence for certain heavy-tailed distributions. On the other hand, the median-of-means estimator (invented and reinvented in various literature) does have sub-Gaussian convergence for all finite-variance distributions, albeit only in the big-O sense with a sub-optimal multiplicative constant. The natural remaining question then, is whether it is possible to bridge the gap, and have an estimator that has optimal convergence with the right constant for all finite-variance distributions.</p>
<p>In this talk, we answer the question affirmatively by giving an estimator that converges with the optimal constant inside the big-O, up to a 1+o(1) multiplicative factor. The estimator is also easy to compute. The convergence analysis involves deriving tail bounds using linear and convex-concave programming dualities, which may be of independent interest.</p>
<p>Based on joint work with Paul Valiant.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2021/03/23/tcs-talk-wednesday-march-31-jasper-lee-brown-university/"><span class="datestr">at March 24, 2021 03:37 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18389">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/03/23/moby-dick-meet-theory/">Moby Dick Meet Theory</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>People seem to be afraid of mathematics. And I think that’s such a shame, because I don’t think it’s as hard as people seem to think it is. —Heidi Hammel</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/?attachment_id=18328" rel="attachment wp-att-18328"><img src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/03/sh2.jpg?w=250&amp;ssl=1" alt="" class="alignright  wp-image-18328" /></a></p>
<p>
Sarah Hart holds a <a href="https://en.wikipedia.org/wiki/Gresham_Professor_of_Geometry">chair</a> that dates back to 1597—Gresham Professor of Geometry at Birkbeck, University of London.  She is a British mathematician specialising in group theory. The British <a href="https://en.wikipedia.org/wiki/American_and_British_English_spelling_differences">modifier</a> is redundant since I spelled specialising with a “s” not a “z”: </p>
<blockquote><p><b> </b> <em> Today’s British English spellings mostly follow Samuel Johnson’s <a href="https://en.wikipedia.org/wiki/A_Dictionary_of_the_English_Language">A Dictionary of the English Language</a> (1755), while many American English spellings follow Webster’s <a href="https://en.wikipedia.org/wiki/Webster%27s_Dictionary">An American Dictionary of the English Language</a> (1828). </em>
</p></blockquote>
<p>
Today we will discuss history, making history, and more.<br />
<span id="more-18389"></span></p>
<p>
Hart is an excellent writer about math with a special interest in history and in non-standard applications of math to things like Moby-Dick. See the recent <a href="https://www.nytimes.com/2021/03/06/science/math-gresham-sarah-hart.html">article</a> in the science section of the New York Times about Hart: </p>
<blockquote><p><b> </b> <em> For the mathematician Sarah Hart, a close reading of “Moby-Dick” reveals not merely “one of the strangest and most wonderful books in the world” and “the greatest book of the sea ever written,” but also a work awash in mathematical metaphors. </em>
</p></blockquote>
<p>
<a href="https://rjlipton.wpcomstaging.com/?attachment_id=18329" rel="attachment wp-att-18329"><img width="259" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/03/wt.png?resize=259%2C194&amp;ssl=1" class="alignright size-full wp-image-18329" height="194" /></a></p>
<p>
Hart’s paper on Moby-Dick is titled <a href="https://arxiv.org/pdf/1903.12102.pdf">Ahab’s Arithmetic</a>. I bet few articles on the arxiv website are on fictional works like Moby-Dick: </p>
<blockquote><p><b> </b> <em> Herman Melville’s novel Moby-Dick contains a surprising number of mathematical allusions. In this article we explore some of these, as well as discussing the questions that naturally follow: why did Melville choose to use so much mathematical imagery? How did Melville come to acquire the level of mathematical knowledge shown in the novel? And is it commensurate with the general level of mathematical literacy at that time? </em>
</p></blockquote>
<p>
Hart also has given many talks on math from a unique point of <a href="https://www.gresham.ac.uk/professorships/geometry-professorship/">view</a>.</p>
<p></p><h2> Group Theory History </h2><p></p>
<p>
Hart is the current <a href="https://www.bshm.ac.uk/where-our-members-are">president</a> of the British Society for the History of Mathematics: </p>
<blockquote><p><b> </b> <em> Mathematics has been part of human culture since the beginnings of civilization. Its study and practice has gone hand in hand with the evolution and development of commerce, architecture, legal theory, cosmology, astrology, and countless other activities. The history of mathematics today addresses a rich cultural heritage across continents, peoples, and ages, taking in the practical traditions of merchants’ accounts and surveying just as it does the elevated traditions of learned scholars at court or the modern day university professor. </em>
</p></blockquote>
<p>
Hart adds: </p>
<blockquote><p><b> </b> <em> In this <a href="https://eprints.bbk.ac.uk/id/eprint/26953/">article</a> we explore mathematical allusions in Herman Melville’s novel Moby-Dick. We argue that both the quantity and sophistication of these allusions are evidence for Melville’s high level of mathematical knowledge and ability. We discuss some of the most compelling mathematical imagery, as well as giving background on the several mathematicians and mathematics books mentioned in the novel. We also include some biographical details supporting the assertion that Melville had an unusually good mathematical education. </em>
</p></blockquote>
<p>
Another view of history is given by Alma <a href="https://history.columbia.edu/person/alma-steingart/">Steingart</a> of the history department of Columbia. Steingart has written, for example, an <a href="https://sites.tufts.edu/histmath/files/2015/11/steingart-group-theory.pdf">article</a> on the history of the classification of all simple groups: </p>
<blockquote><p><b> </b> <em> Over a period of more than 30 years, more than 100 mathematicians worked on a project to classify mathematical objects known as finite simple groups. The Classification, when officially declared completed in 1981, ranged between 300 and 500 articles and ran somewhere between 5,000 and 10,000 journal pages. Mathematicians have hailed the project as one of the greatest mathematical achievements of the 20th century, and it surpasses, both in scale and scope, any other mathematical proof of the 20th century. </em>
</p></blockquote>
<p>
Decades ago as soon as we started to think about the <a href="https://www.uni-ulm.de/fileadmin/website_uni_ulm/iui.inst.190/Mitarbeiter/wagner/GroupIso_040712.pdf">group isomorphism</a> problem we realized there is an application of the classification theorem: </p>
<blockquote><p><b> </b> <em> We can use it to prove that isomorphism for finite simple groups is in polynomial time. The proof is simple—bad pun: Just use the fact that all simple groups are generated by at most <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2}" class="latex" title="{2}" /> elements. So just try all possible pairs. See <a href="https://rjlipton.wordpress.com/2013/05/11/advances-on-group-isomorphism/">GLL</a> for some more comments. </em>
</p></blockquote>
<p></p><h2> Making Group Theory History </h2><p></p>
<p>
Hart may be interested in the history of math, but proving new theorems is perhaps the best way to affect history. The <a href="https://www.combinatorics.org/ojs/index.php/eljc/article/view/v16i1r59/pdf">paper</a> titled Small maximal sum-free sets by Michael Giudici and Hart solves a previously open problem. The site <i>Theorem of day</i> highlights this paper <a href="https://www.theoremoftheday.org/GroupTheory/SumFree/TotDSumFree.pdf">here</a>. Also Tim <a href="https://arxiv.org/pdf/0710.3877.pdf">Gowers</a> proved some related results.</p>
<p>
Hart’s main theorem is: </p>
<blockquote><p><b>Theorem 1</b> <em> Let <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" title="{G}" /> be a finite group and let <img src="https://s0.wp.com/latex.php?latex=%7B%3CS%3E%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{&lt;S&gt;}" class="latex" title="{&lt;S&gt;}" /> be the subgroup generated by a subset <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" title="{S}" /> of the elements of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" title="{G}" />. Then unless <img src="https://s0.wp.com/latex.php?latex=%7B%7CS+%7C+%5Cle+2%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{|S | \le 2}" class="latex" title="{|S | \le 2}" />, <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" title="{S}" /> cannot be both maximally sum-free and minimal with respect to generating <img src="https://s0.wp.com/latex.php?latex=%7B%3CS%3E%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{&lt;S&gt;}" class="latex" title="{&lt;S&gt;}" />. </em>
</p></blockquote>
<p>
Consider a set <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" title="{S}" /> of elements in a finite group <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" title="{G}" />. There are two properties that <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" title="{S}" /> can have, and these properties act in opposite directions.</p>
<ul>
<li>Force <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" title="{S}" /> to be smaller: The set <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" title="{S}" /> is sum-free. This means that for <img src="https://s0.wp.com/latex.php?latex=%7Ba%2C+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a, b}" class="latex" title="{a, b}" /> in <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" title="{S}" />,
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++ab+%5Cnot%5Cin+S.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  ab \not\in S. " class="latex" title="\displaystyle  ab \not\in S. " /></p>
</li><li>Force <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" title="{S}" /> to be larger: The set is minimal generator for <img src="https://s0.wp.com/latex.php?latex=%7B%3CS%3E%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{&lt;S&gt;}" class="latex" title="{&lt;S&gt;}" />. This means that it is not possible to generate <img src="https://s0.wp.com/latex.php?latex=%7B%3CS%3E%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{&lt;S&gt;}" class="latex" title="{&lt;S&gt;}" /> with less elements than <img src="https://s0.wp.com/latex.php?latex=%7B%7CS%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{|S|}" class="latex" title="{|S|}" />.  Intuitively these properties work against each other. Adding an element to <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" title="{S}" /> could violate sum-free; deleting an element from <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" title="{S}" /> could violate generation.
</li></ul>
<p></p><h2> Open Problems </h2><p></p>
<p>
Here is an <a href="https://www.newton.ac.uk/science/outreach/women/six-questions/hart-sarah">interview</a> with Hart—take a look.</p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2021/03/23/moby-dick-meet-theory/"><span class="datestr">at March 23, 2021 02:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=21227">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2021/03/23/to-cheer-you-up-in-difficult-times-22-some-mathematical-news-part-1/">To cheer you up in difficult times 22: some mathematical news! (Part 1)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>To cheer you up, in these difficult times, here are (in two parts) some mathematical news that I heard in personal communications or on social media. (Maybe I will write later, in more details,  about few of them that are closer to combinatorics.)</p>
<h2>Mathematics and the real world</h2>
<h3>Zvi Artstein  <a href="https://gilkalai.files.wordpress.com/2021/03/c30.pdf">The pendulum under vibrations revisited</a></h3>
<p>Zvi (Zvika) was one of the first people that I met in 1971 at the university as a high school student and he gave me a lot of good advice and help. Here he shed light on an old mystery!</p>
<blockquote>
<p><span style="color: #0000ff;"><em>Abstract: A simple intuitive physical explanation is offered, of the stability of the inverted pendulum under fast violent vibrations. The direct description allows to analyze, both intuitively and rigorously, the effect of vibrations in similar, and in more general, situations. The rigorous derivations in the paper follow a singular perturbations model of mixed slow and fast dynamics. The approach allows applications beyond the classical inverted pendulum model.</em></span></p>
</blockquote>
<p>I first heard about the stability of the inverted pendulum under fast violent vibrations from Sylvia Serfaty in connection with <a href="https://gilkalai.wordpress.com/2009/04/20/the-intermediate-value-theorem-applied-to-football/">my study of “forehead-football.” See this post</a>. Let me mention with pride that Endre Szemeredi, who is a fan of football in general (and a capable player), is also a fan of my forehead-football idea.</p>
<h3>Imre Bárány, William Steiger, and Sivan Toledo <a href="https://arxiv.org/abs/2007.06838">the cocked hat</a></h3>
<blockquote>
<p><span style="color: #0000ff;"><em>Abstract (arXive): We revisit the cocked hat — an old problem from navigation — and examine under what conditions its old solution is valid.</em></span></p>
</blockquote>
<h2><a href="https://gilkalai.files.wordpress.com/2021/03/cokedhat.png"><img width="426" alt="" src="https://gilkalai.files.wordpress.com/2021/03/cokedhat.png" class="alignnone size-full wp-image-21402" height="296" /></a></h2>
<p>This is a great story, read it in <a href="https://www.cambridge.org/core/journals/journal-of-navigation/article/abs/cocked-hat-formal-statements-and-proofs-of-the-theorems/5264C1491A61116CAF7B890161CFD091">The Journal of Navigation</a>, or <a href="https://arxiv.org/abs/2007.06838">on the arXive.</a></p>
<h2>Rigidity</h2>
<h3>Katie Clinch, Bill Jackson, Shin-ichi Tanigawa: Maximal Matroid Problem on Graphs</h3>
<div>In mid January, we had a <strong>great lecture</strong> by <b></b>Shin-ichi Tanigawa (University of Tokyo). I certainly plan to blog about it. Here is the abstract.</div>
<div> </div>
<blockquote>
<div><span style="color: #0000ff;"><em>The problem of characterizing the 3-dimensional generic rigidity of graphs is one of the major open problems in graph rigidity theory. Walter <span class="il">Whiteley</span> conjectured that the 3-dimensional generic matroid coincides with a matroid studied in the context of bivariate splines.  In this talk I will show a solution to the characterization problem for the latter matroid. </em></span></div>
<div> </div>
<div><span style="color: #0000ff;"><em>I will explain the idea of our characterization from the view point of constructing maximal matroids on complete graphs. Specifically, for a graph H, a matroid on the edge set of a complete graph is called an H-matroid if every edge set of each subgraph isomorphic to H is a circuit. A main theme of my talk will be about identifying and constructing a maximal H-matroid with respect to the weak order. This talk is based on a joint work with Bill Jackson and Katie Clinch.</em></span>
<p> </p>
<p><a href="https://huji.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=a588e411-26c9-4f35-9e1b-acb400bcf8de">Here is the video</a></p>
</div>
</blockquote>
<div>The papers on the arxive</div>
<div>Katie Clinch, Bill Jackson, Shin-ichi Tanigawa, <a href="https://arxiv.org/abs/1911.00205">Abstract 3-Rigidity and Bivariate <span class="MathJax" id="MathJax-Element-19-Frame"><span class="math" id="MathJax-Span-52"><span class="mrow" id="MathJax-Span-53"><span class="msubsup" id="MathJax-Span-54"><span class="mi" id="MathJax-Span-55">C</span><span class="mn" id="MathJax-Span-56">1</span><span class="mn" id="MathJax-Span-57">2</span></span></span></span></span>-Splines I: Whiteley’s Maximality Conjecture </a></div>
<div>Katie Clinch, Bill Jackson, Shin-ichi Tanigawa <a href="https://arxiv.org/abs/1911.00207">Abstract 3-Rigidity and Bivariate <span class="MathJax" id="MathJax-Element-15-Frame"><span class="math" id="MathJax-Span-34"><span class="mrow" id="MathJax-Span-35"><span class="msubsup" id="MathJax-Span-36"><span class="mi" id="MathJax-Span-37">C</span><span class="mn" id="MathJax-Span-38">1</span><span class="mn" id="MathJax-Span-39">2</span></span></span></span></span>-Splines II: Combinatorial Characterization</a></div>
<div> </div>
<h2>Polytopes</h2>
<h3>Ardila and Escobar:  The harmonic polytope</h3>
<blockquote>
<p><span style="color: #0000ff;"><em><strong><span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:</strong> <span class="abstract-full has-text-grey-dark mathjax" id="2006.03078v1-abstract-full">We study the harmonic polytope, which arose in Ardila, Denham, and Huh’s work on the Lagrangian geometry of matroids. We show that it is a <span class="MathJax" id="MathJax-Element-5-Frame"><span class="math" id="MathJax-Span-40"><span class="mrow" id="MathJax-Span-41"><span class="mo" id="MathJax-Span-42">(</span><span class="mn" id="MathJax-Span-43">2</span><span class="mi" id="MathJax-Span-44">n</span><span class="mo" id="MathJax-Span-45">−</span><span class="mn" id="MathJax-Span-46">2</span><span class="mo" id="MathJax-Span-47">)</span></span></span></span>-dimensional polytope with $latex <span class="MathJax" id="MathJax-Element-6-Frame"><span class="math" id="MathJax-Span-48"><span class="mrow" id="MathJax-Span-49"><span class="mo" id="MathJax-Span-50">(</span><span class="mi" id="MathJax-Span-51">n</span><span class="mo" id="MathJax-Span-52">!</span><span class="msubsup" id="MathJax-Span-53"><span class="mo" id="MathJax-Span-54">)^</span><span class="mn" id="MathJax-Span-55">2 </span></span><span class="mo" id="MathJax-Span-56">(</span><span class="mn" id="MathJax-Span-57">1</span><span class="mo" id="MathJax-Span-58">+</span><span class="mfrac" id="MathJax-Span-59"><span class="mn" id="MathJax-Span-60">1/</span><span class="mn" id="MathJax-Span-61">2</span></span><span class="mo" id="MathJax-Span-62">+</span><span class="mo" id="MathJax-Span-63">⋯</span><span class="mo" id="MathJax-Span-64">+</span><span class="mfrac" id="MathJax-Span-65"><span class="mn" id="MathJax-Span-66">1/</span><span class="mi" id="MathJax-Span-67">n</span></span><span class="mo" id="MathJax-Span-68">)$</span></span></span></span> vertices and $latex <span class="MathJax" id="MathJax-Element-7-Frame"><span class="math" id="MathJax-Span-69"><span class="mrow" id="MathJax-Span-70"><span class="msubsup" id="MathJax-Span-71"><span class="mn" id="MathJax-Span-72">3^</span><span class="mi" id="MathJax-Span-73">n</span></span><span class="mo" id="MathJax-Span-74">−</span><span class="mn" id="MathJax-Span-75">3$</span></span></span></span> facets. We give a formula for its volume: it is a weighted sum of the degrees of the projective varieties of all the toric ideals of connected bipartite graphs with <span class="MathJax" id="MathJax-Element-8-Frame"><span class="math" id="MathJax-Span-76"><span class="mrow" id="MathJax-Span-77"><span class="mi" id="MathJax-Span-78">n</span></span></span></span> edges; or equivalently, a weighted sum of the lattice point counts of all the corresponding trimmed generalized permutahedra.</span></em></span></p>
</blockquote>
<p>These polytopes look truly great!</p>
<p>Federico Ardila, Laura Escobar, <a href="https://arxiv.org/abs/2006.03078">The <span class="search-hit mathjax">harmonic</span> <span class="search-hit mathjax">polytope</span></a></p>
<h2>Percolation</h2>
<p> I am thankful to Itai Benjamini who told me about these results.</p>
<h3>At last: Rotation invariance theorem for planar percolation for the square grid.</h3>
<p class="title is-5 mathjax"><a href="https://arxiv.org/abs/2012.11672"><span class="search-hit mathjax">Rotational</span> invariance in critical planar lattice models</a></p>
<p>This is very big news: twenty years after Smirnov’s result for the triangular grid, finally rotational invariance is proved for critical percolation on the square grid.</p>
<p class="authors"><span class="has-text-black-bis has-text-weight-semibold">Authors:</span> Hugo Duminil-Copin, Karol Kajetan Kozlowski, Dmitry Krachun, Ioan Manolescu, Mendes Oulamara</p>
<blockquote>
<p><span style="color: #0000ff;"><em><strong>Abstract: </strong> In this paper, we prove that the large scale properties of a number of two-dimensional lattice models are rotationally invariant. More precisely, we prove that the random-cluster model on the square lattice with cluster-weight <span class="MathJax" id="MathJax-Element-1-Frame"><span class="math" id="MathJax-Span-1"><span class="mrow" id="MathJax-Span-2"><span class="mn" id="MathJax-Span-3">1</span><span class="mo" id="MathJax-Span-4">≤</span><span class="mi" id="MathJax-Span-5">q</span><span class="mo" id="MathJax-Span-6">≤</span><span class="mn" id="MathJax-Span-7">4</span></span></span></span> exhibits rotational invariance at large scales. This covers the case of Bernoulli percolation on the square lattice as an important example. We deduce from this result that the correlations of the Potts models with <span class="MathJax" id="MathJax-Element-2-Frame"><span class="math" id="MathJax-Span-8"><span class="mrow" id="MathJax-Span-9"><span class="mi" id="MathJax-Span-10">q</span><span class="mo" id="MathJax-Span-11">∈</span><span class="mo" id="MathJax-Span-12">{</span><span class="mn" id="MathJax-Span-13">2</span><span class="mo" id="MathJax-Span-14">,</span><span class="mn" id="MathJax-Span-15">3</span><span class="mo" id="MathJax-Span-16">,</span><span class="mn" id="MathJax-Span-17">4</span><span class="mo" id="MathJax-Span-18">}</span></span></span></span> colors and of the six-vertex height function with <span class="MathJax" id="MathJax-Element-3-Frame"><span class="math" id="MathJax-Span-19"><span class="mrow" id="MathJax-Span-20"><span class="mi" id="MathJax-Span-21">Δ</span><span class="mo" id="MathJax-Span-22">∈</span><span class="mo" id="MathJax-Span-23">[</span><span class="mo" id="MathJax-Span-24">−</span><span class="mn" id="MathJax-Span-25">1</span><span class="mo" id="MathJax-Span-26">,</span><span class="mo" id="MathJax-Span-27">−</span><span class="mn" id="MathJax-Span-28">1</span><span class="texatom" id="MathJax-Span-29"><span class="mrow" id="MathJax-Span-30"><span class="mo" id="MathJax-Span-31">/</span></span></span><span class="mn" id="MathJax-Span-32">2</span><span class="mo" id="MathJax-Span-33">]</span></span></span></span> are rotationally invariant at large scales.</em></span></p>
</blockquote>
<h3>Noise sensitivity for planar percolation without Fourier</h3>
<p>Endre Szemeredi sometimes say that we should try to avoid the use of his regularity lemma “at all costs”. Similarly, we should try to avoid Fourier tools if we can. Vincent and Hugo could!</p>
<p class="title mathjax"><a href="https://arxiv.org/abs/2011.04572">Noise sensitivity of percolation via differential inequalities</a>, Vincent Tassion and Hugo Vanneuville</p>
<blockquote>
<p><span style="color: #0000ff;"><em><strong>Abstract:</strong> Consider critical Bernoulli percolation in the plane. We give a new proof of the sharp noise sensitivity theorem shown by Garban, Pete and Schramm. Contrary to the previous approaches, we do not use any spectral tool. We rather study differential inequalities satisfied by a dynamical four-arm event, in the spirit of Kesten’s proof of scaling relations. We also obtain new results in dynamical percolation. In particular, we prove that the Hausdorff dimension of the set of times with both primal and dual percolation equals <span class="MathJax" id="MathJax-Element-1-Frame"><span class="math" id="MathJax-Span-1"><span class="mrow" id="MathJax-Span-2"><span class="mn" id="MathJax-Span-3">2</span><span class="texatom" id="MathJax-Span-4"><span class="mrow" id="MathJax-Span-5"><span class="mo" id="MathJax-Span-6">/</span></span></span><span class="mn" id="MathJax-Span-7">3</span></span></span></span> a.s.</em></span></p>
</blockquote>
<h3 class="title mathjax"><a href="https://arxiv.org/abs/2011.11903">Plaquette Percolation on the Torus</a> by  Paul Duncan, Matthew Kahle, and Benjamin Schweinhart</h3>
<p>Harry Kesten famously proved that the critical probability for planar percolation is 1/2. Planar duality is crucial.  Many people expected or speculated that a similar statement for <img src="https://s0.wp.com/latex.php?latex=2d&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="2d" class="latex" />-dimensions holds if we replace “connectivity” by some statement about <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="d" class="latex" /> dimensional homology. This is what the new paper does.</p>
<p>(Next part: Topology, graph theory, algebra, Boolean functions, and Mathematics over the media.)</p>
<p> </p>


<p></p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2021/03/23/to-cheer-you-up-in-difficult-times-22-some-mathematical-news-part-1/"><span class="datestr">at March 23, 2021 08:53 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-7448114190319986834">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/03/a-taylor-series-problem.html">A Taylor Series Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p> I post a problem today and its solution on Thursday.</p><p>Comments are fine, though if you don't want to get a hint, don't read them. </p><p><br /></p><p>Find the coefficient of x<sup>100</sup> in the Taylor series for the rational function which has </p><p><br /></p><p>numerator 1 </p><p>and denominator</p><p><br /></p><p>x<sup>41</sup> - x<sup>40</sup> - x<sup>36</sup> + x<sup>35</sup> -x<sup>31</sup> + x<sup>30</sup> + x<sup>26</sup> - x<sup>25</sup>- x<sup>16</sup> + x<sup>15</sup> + x<sup>11</sup> - x<sup>10</sup> + x<sup>6</sup> - x<sup>5</sup> -x+1</p><p><br /></p><p>For better readability see my pdf file with the problem in it <a href="http://www.cs.umd.edu/~gasarch/BLOGPAPERS/taylorcoinsprob.pdf">here</a></p><p><br /></p><p>Is there a clever way to do the problem?  If the way to do it was to actually do the Taylor series then </p><p>1) I wouldn't post it</p><p>2) I probably could not do it (or it would take too long to bother)  though maybe there are freely available programs that could. </p><p><br /></p><p>So yes, there is a clever solution. At least I think it's clever. </p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/03/a-taylor-series-problem.html"><span class="datestr">at March 22, 2021 07:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/045">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/045">TR21-045 |  Reconstruction Algorithms for Low-Rank Tensors and Depth-3 Multilinear Circuits | 

	Vishwas Bhargava, 

	Shubhangi Saraf, 

	Ilya Volkovich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We give new and efficient black-box reconstruction algorithms for some classes of depth-$3$ arithmetic circuits. As a consequence, we obtain the first efficient algorithm for computing the tensor rank and for finding the optimal tensor decomposition as a sum of rank-one tensors when then input is a {\it constant-rank} tensor. More specifically, we provide efficient learning algorithms that run in randomized polynomial time over general fields and in deterministic polynomial time over $\mathbb R$ and $\mathbb C$ for the following classes:

\begin{enumerate}
    \item {\it Set-multilinear depth-$3$ circuits of constant top fan-in ($\Sigma\Pi\Sigma_{{\sqcup_j X_j}}(k)$ circuits)}. As a consequence of our algorithm, we obtain the first polynomial time algorithm for tensor rank computation and optimal tensor decomposition of constant-rank tensors. This result holds for $d$ dimensional tensors for any $d$, but is interesting even for $d=3$.
    \item {\it Sums of powers of constantly many linear forms ($\Sigma\!\wedge\!\Sigma(k)$ circuits)}.  As a consequence we obtain the first polynomial-time algorithm for tensor rank computation and optimal tensor decomposition of constant-rank symmetric tensors.
    \item {\it Multilinear depth-3 circuits of constant top fan-in (multilinear $\Sigma\Pi\Sigma(k)$ circuits)}. Our algorithm works over all fields of characteristic 0 or large enough characteristic. Prior to our work the only efficient algorithms known were over polynomially-sized finite fields \cite{KarninShpilka09}. 
\end{enumerate}
Prior to our work, the only polynomial-time or even subexponential-time algorithms known (deterministic or randomized) for subclasses of $\Sigma\Pi\Sigma(k)$ circuits that also work over large/infinite fields were for the setting when the top fan-in $k$ is at most $2$ \cite{Sin16, Sin20}.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/045"><span class="datestr">at March 22, 2021 06:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/044">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/044">TR21-044 |  SAT-based Circuit Local Improvement | 

	Alexander Kulikov, 

	Nikita Slezkin</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Finding exact circuit size is a notorious optimization problem in practice. Whereas modern computers and algorithmic techniques allow to find a circuit of size seven in blink of an eye, it may take more than a week to search for a circuit of size thirteen. One of the reasons of this behavior is that the search space is enormous: the number of circuits of size $s$ is $s^{\Theta(s)}$, the number of Boolean functions on $n$ variables is $2^{2^n}$.

In this paper, we explore the following natural heuristic idea for decreasing the size of a given circuit: go through all its subcircuits of moderate size and check whether any of them can be improved by reducing to SAT. This may be viewed as a local search approach: we search for a smaller circuit in a ball around a given circuit. We report the results of experiments with various symmetric functions.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/044"><span class="datestr">at March 22, 2021 04:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/03/22/postdoctoral-associate-at-rutgers-university-dimacs-apply-by-april-5-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/03/22/postdoctoral-associate-at-rutgers-university-dimacs-apply-by-april-5-2021/">Postdoctoral Associate at Rutgers University – DIMACS (apply by April 5, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>DIMACS, the Center for Discrete Mathematics and Theoretical Computer Science, based at Rutgers University, invites applications for a transdisciplinary DATA-INSPIRE postdoc<br />
Applicants should be recent PhDs in computer science, math, or statistics, with a demonstrated interest in working across these disciplines.</p>
<p>Website: <a href="https://jobs.rutgers.edu/postings/128123">https://jobs.rutgers.edu/postings/128123</a><br />
Email: application@dimacs.rutgers.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/03/22/postdoctoral-associate-at-rutgers-university-dimacs-apply-by-april-5-2021/"><span class="datestr">at March 22, 2021 03:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5387">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5387">QC ethics and hype: the call is coming from inside the house</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>For years, I’d sometimes hear discussions about the <em>ethics</em> of quantum computing research.  Quantum ethics!</p>



<p>When the debates weren’t purely semantic, over the <a href="https://www.scottaaronson.com/blog/?p=4450">propriety</a> of terms like “quantum supremacy” or “ancilla qubit,” they were always about chin-strokers like “but what if cracking RSA encryption gives governments more power to surveil their citizens?  or what if only a few big countries or companies get quantum computers, thereby widening the divide between haves and have-nots?”  Which, OK, conceivably these will someday be issues.  But, besides barely depending on any specific facts about quantum computing, these debates always struck me as oddly <em>safe</em>, because the moral dilemmas were so hypothetical and far removed from us in time.</p>



<p>I confess I may have even occasionally poked fun when asked to expound on quantum ethics.  I may have commented that quantum computers probably won’t kill anyone unless a dilution refrigerator tips over onto their head.  I may have asked forgiveness for feeding custom-designed oracles to <a href="https://en.wikipedia.org/wiki/BQP">BQP</a> and <a href="https://en.wikipedia.org/wiki/QMA">QMA</a>, without first consulting an ethics committee about the long-term effects on those complexity classes.</p>



<p>Now fate has punished me for my flippancy.  These days, I really <em>do</em> feel like quantum computing research has become an ethical minefield—but not for any of the reasons mentioned previously.  What’s new is that millions of dollars are now potentially available to quantum computing researchers, along with equity, stock options, and whatever else causes “ka-ching” sound effects and bulging eyes with dollar signs.  And in many cases, to have a shot at such riches, all an expert needs to do is profess optimism that quantum computing will have revolutionary, world-changing applications and have them <em>soon</em>.  Or at least, not object too strongly when others say that.</p>



<p>Some of today’s rhetoric will of course remind people of the D-Wave saga, which first brought this blog to prominence when it began in earnest in 2007.  Quantum computers, we hear now as then, will soon leave the Earth’s fastest supercomputers in the dust.  They’re going to harness superposition to try all the exponentially many possible solutions at once.  They’ll crack the Traveling Salesman Problem, and will transform machine learning and AI beyond recognition.  Meanwhile, simulations of quantum systems will be key to solving global warming and cancer.</p>



<p>Despite the parallels, though, this new gold rush <em>doesn’t</em> feel to me like the D-Wave one, which seems in retrospect like just a little dry run.  If I had to articulate what’s new in one sentence, it’s that this time “the call is coming from inside the house.”  Many of the companies making wildly overhyped claims are recognized leaders of the field.  They have brilliant quantum computing theorists and experimentalists on their staff with impeccable research records.  Some of those researchers are among my best friends.  And even when I wince at the claims of near-term applications, in many cases (especially with quantum simulation) the claims aren’t <em>obviously</em> false—we won’t know for certain until we try it and see!  It’s genuinely gotten harder to draw the line between defensible optimism and exaggerations verging on fraud.</p>



<p>Indeed, this time around virtually <em>everyone</em> in QC is “complicit” to a greater or lesser degree.  I, too, have accepted compensation to consult on quantum computing topics, to give talks at hedge funds, and in a few cases to serve as a scientific adviser to quantum computing startups.  I tell myself that, by 2021 standards, this stuff is all trivial chump change—a few thousands of dollars here or there, to expound on the same themes that I already discuss free of charge on this blog.  I actually get paid to <em>dispel</em> hype, rather than propagate it!  I tell myself that I’ve turned my back on the orders of magnitude more money available to those willing to hitch their scientific reputations to the aspirations of this or that specific QC company.  (Yes, this blog, and my desire to preserve its intellectual independence and credibility, might well be costing me millions!)</p>



<p>But, OK, some would argue that accepting <em>any</em> money from QC companies or QC investors just puts you at the top of a slope with unabashed snake-oil salesmen at the bottom.  With the commercialization of our field that started around 2015, there’s no bright line anymore marking the boundary between pure scientific curiosity and the pursuit of filthy lucre; it’s all just points along a continuum.  I’m not sure that these people are wrong.</p>



<p>As some of you might’ve seen already, IonQ, the trapped-ion QC startup that originated from the University of Maryland, is poised to have the <a href="https://www.reuters.com/article/us-ionq-m-a-dmy-technology/quantum-computing-provider-ionq-to-go-public-via-2-billion-spac-deal-idUSKBN2B013Z">first-ever quantum computing IPO</a>—a so-called “SPAC IPO,” which while I’m a financial ignoramus, apparently involves merging with a shell company and thereby bypassing the SEC’s normal IPO rules.  Supposedly they’re seeking $650 million in new funding and a $2 billion market cap.  If you want to see what IonQ is saying about QC to prospective investors, <a href="https://static1.squarespace.com/static/5e33152a051d2e7588f7571c/t/60459578b8c075444a656357/1615173012167/IonQ+Investor+Presentation+030721+vFF.pdf">click here</a>.  Lacking any choice in the matter, I’ll probably say more about these developments in a future post.</p>



<p>Meanwhile, <a href="https://psiquantum.com/">PsiQuantum</a>, the Palo-Alto-based optical QC startup, has <a href="https://www.ft.com/content/a5af3039-abbf-4b25-92e2-c40e5957c8cd">said</a> that it’s soon going to leave “stealth mode.”  And Amazon, Microsoft, Google, IBM, Honeywell, and other big players continue making large investments in QC—treating it, at least rhetorically, not at all like blue-sky basic research, but like a central part of their future business plans.</p>



<p>All of these companies have produced or funded excellent QC research.  And of course, they’re all heterogeneous, composed of individuals who might vehemently disagree with each other about the near- or long-term prospects of QC.  And yet all of them have, at various times, inspired reflections in me like the ones in this post.</p>



<p>I regret that this post has no clear conclusion.  I’m still hashing things out, solicing thoughts from my readers and friends.  Speaking of which: this coming Monday, March 22, at 8-10pm US Eastern time, I’ve decided to hold a discussion around these issues on <a href="https://www.joinclubhouse.com/">Clubhouse</a>—my “grand debut” on that app, and an opportunity to see whether I like it or not!  My friend Adam Brown will moderate the discussion; other likely participants will be John Horgan, George Musser, Michael Nielsen, and Matjaž Leonardis.  If you’re on Clubhouse, I hope to see you there!</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update (March 22):</span></strong> Read <a href="https://www.scottaaronson.com/blog/?p=5387#comment-1883356">this comment</a> by “FB” if you’d like to understand how we got to this point.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5387"><span class="datestr">at March 21, 2021 01:18 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://rjlipton.wpcomstaging.com/?p=18396">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://rjlipton.wpcomstaging.com/2021/03/19/closing-an-erdos-problem/">Closing an Erdős Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wpcomstaging.com" title="Gödel's Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>
<font color="#0044cc"><br />
<em>Whatever the problem, be part of the solution. Don’t just sit around raising questions and pointing out obstacles.—Tina Fey</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/03/19/closing-an-erdos-problem/dh2/" rel="attachment wp-att-18399"><img width="180" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/03/dh2.jpg?resize=180%2C240&amp;ssl=1" class="alignright swp-image-18399" height="240" /></a></p>
<p>
Daniela Kühn is the Mason Professor in Mathematics at the University of Birmingham. She works in extremal and probabilistic combinatorics. She has many honors, including giving an invited lecture at the International Congress of Mathematicians in 2014.</p>
<p>
Today I thought we should look at a new result of hers.</p>
<p>
By the way, as I wrote this draft, I was excited to see that it was the second time we highlighted a researcher from Bi*ngham*. <a href="https://rjlipton.wordpress.com">Previously</a> we highlighted Jessica Fridrich, from Binghamton University. OK, they are different places, in different countries, but the closeness in edit distance caught my eye.</p>
<p>
Kühn says <a href="https://www.birmingham.ac.uk/staff/profiles/maths/kuhn-daniela.aspx">here</a>: </p>
<blockquote><p><b> </b> <em> Recently I have focused on sufficient conditions for Hamilton cycles in directed graphs, i.e. cycles which contain all the vertices of the directed graph. It is unlikely that there is a good characterization of all (directed) graphs containing a Hamilton cycle since the corresponding decision problem is NP-complete. So it is natural to ask for sufficient conditions for the existence of a Hamilton cycle. </em>
</p></blockquote>
<p>
Here is her <a href="https://silo.tips/download/hamiltonian-degree-sequences-in-digraphs">paper</a> on degree sequences on directed graphs that force Hamilton cycles. It is joint with Deryk Osthus and Andrew Treglown. </p>
<p></p><h2> The Conjecture </h2><p></p>
<p>
Paul Erdős is known for his conjectures as well as his countless results. Here are two of his open <a href="https://en.wikipedia.org/wiki/List_of_conjectures_by_Paul_Erdos">ones</a> that are directly about graphs: </p>
<ol>
<li>The Erdős-Faber-Lovász conjecture on coloring unions of cliques.
</li><li>The Erdős-Hajnal conjecture that in a family of graphs defined by an excluded induced subgraph, every graph has either a large clique or a large independent set.
</li></ol>
<p>Of course math is tricky: A conjecture like the—</p>
<blockquote><p><b> </b> <em> Erdős-Szekeres conjecture on the number of points needed to ensure that a point set contains a large convex polygon. </em>
</p></blockquote>
<p>—could still be about graphs. Consider the graph that is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\dots}" class="latex" title="{\dots}" /></p>
<p>
Well, the conjecture (1) is now claimed to be solved, and so the list of open conjectures is one less. Kühn with co-authors Dong Yeap Kang, Tom Kelly, Abhishek Methuku, and Deryk Osthus have claimed to resolve it. I have no reason to doubt these top researchers—it’s just until it is refereed. Their <a href="https://arxiv.org/pdf/2101.04698.pdf">paper</a> is cleverly titled: A Proof Of The Erdős-Faber-Lovász Conjecture. </p>
<p><a href="https://rjlipton.wpcomstaging.com/2021/03/19/closing-an-erdos-problem/five/" rel="attachment wp-att-18401"><img width="388" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/03/five.png?resize=388%2C259&amp;ssl=1" class="aligncenter wp-image-18401" height="259" /></a></p>
<blockquote><p><b> </b> <em>  By the way: See this <a href="https://sites.google.com/view/epcwebinar/">talk</a> on the paper by Kühn’s co-author Abhishek Methuku on this coming Monday March 22, 2021, 14:00 UTC. To see the talk on zoom you need to know the first six primes. Here is some help: </em>
</p></blockquote>
<p><a href="https://rjlipton.wpcomstaging.com/2021/03/19/closing-an-erdos-problem/primes/" rel="attachment wp-att-18402"><img width="269" alt="" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/03/primes.png?resize=269%2C187&amp;ssl=1" class="aligncenter wp-image-18402" height="187" /></a></p>
<p></p><p><br />
This conjecture, now almost 50 years old, states that a graph that consists of complete graphs of size <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" /> that barely overlap can be colored with <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" /> colors. Here barely overlap is they have no edge in common. </p>
<p>
That is: If <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" title="{k}" /> complete graphs, each having exactly <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" title="{k}" /> vertices, have the property that every pair of complete graphs has at most one shared vertex, then the union of the graphs can be properly colored with <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" title="{k}" /> colors. The following picture should help you understand the conjecture:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/03/19/closing-an-erdos-problem/cj/" rel="attachment wp-att-18403"><img width="390" alt="" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/03/cj.png?resize=390%2C418&amp;ssl=1" class="aligncenter wp-image-18403" height="418" /></a></p>
<p>
The conjecture can be stated various ways:<br />
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" title="{\bullet}" /> If <img src="https://s0.wp.com/latex.php?latex=%7BA_1%2C%5Cdots%2C+A_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_1,\dots, A_n}" class="latex" title="{A_1,\dots, A_n}" /> are sets of size <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" /> such that every pair of them shares at most one element, then the elements of each <img src="https://s0.wp.com/latex.php?latex=%7BA_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_i}" class="latex" title="{A_i}" /> can be colored by <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" /> colors so that all colors appear in each <img src="https://s0.wp.com/latex.php?latex=%7BA_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_i}" class="latex" title="{A_i}" />.<br />
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet}" class="latex" title="{\bullet}" /> If H is a linear hypergraph with <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" /> vertices, then the chromatic index of <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" title="{H}" /> is at most <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" />.</p>
<p>
The chromatic index of a hypergraph <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" title="{H}" /> is the smallest number of colors needed to color the edges of <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{H}" class="latex" title="{H}" /> so that any two edges that share a vertex have different colors.</p>
<p></p><h2> Previous Results </h2><p></p>
<p>
While Kühn’s paper solves the conjecture, there were many partial results before. These prefer to view the conjecture as one about hypergraphs. </p>
<blockquote><p><b> </b> <em> A linear hypergraph (also known as partial linear space) is a hypergraph with the property that every two hyperedges have at most one vertex in common. A hypergraph is said to be uniform if all of its hyperedges have the same number of vertices as each other. The <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" /> cliques of size <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" /> in the Erdős-Faber-Lovász conjecture may be interpreted as the hyperedges of an <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" />-uniform linear hypergraph that has the same vertices as the underlying graph. In this language, the Erdős-Faber-Lovász conjecture states that, given any <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" />-uniform linear hypergraph with <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" /> hyperedges, one may <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" />-color the vertices such that each hyperedge has one vertex of each color.</em></p><em>
</em><p><em>
A simple hypergraph is a hypergraph in which at most one hyperedge connects any pair of vertices and there are no hyperedges of size at most one. In the graph coloring formulation of the Erdős-Faber-Lovász conjecture, it is safe to remove vertices that belong to a single clique, as their coloring presents no difficulty; once this is done, the hypergraph that has a vertex for each clique, and a hyperedge for each graph vertex, forms a simple hypergraph. And, the hypergraph dual of vertex coloring is edge coloring. Thus, the Erdős-Faber-Lovász conjecture is equivalent to the statement that any simple hypergraph with <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" /> vertices has chromatic index (edge coloring number) at most <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" />. </em>
</p></blockquote>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> William Chang and Gene Lawler in their <a href="https://link.springer.com/content/pdf/10.1007/BF02126801.pdf">paper</a> proved one of the first bounds. The goal is <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" /> and they prove <img src="https://s0.wp.com/latex.php?latex=%7B3%2F2n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3/2n}" class="latex" title="{3/2n}" />. </p>
<blockquote><p><b> </b> <em> Call a hypergraph simple if for any pair <img src="https://s0.wp.com/latex.php?latex=%7Bu%2C+v%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{u, v}" class="latex" title="{u, v}" /> of distinct vertices, there is at most one edge incident to both <img src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{u}" class="latex" title="{u}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" title="{v}" />, and there are no edges incident to exactly one vertex. A conjecture of Erdős, Faber and Lovász is equivalent to the statement that the edges of any simple hypergraph on <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" /> vertices can be colored with at most <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" /> colors. We present a simple proof that the edges of a simple hypergraph on <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" title="{n}" /> vertices can be colored with at most <img src="https://s0.wp.com/latex.php?latex=%7B3%2F2n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3/2n}" class="latex" title="{3/2n}" /> colors. </em>
</p></blockquote>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bullet }" class="latex" title="{\bullet }" /> Jeff Kahn showed that the bound could be improved to <img src="https://s0.wp.com/latex.php?latex=%7Bn+%2B+o%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n + o(n)}" class="latex" title="{n + o(n)}" /> colors in this <a href="https://www.sciencedirect.com/science/article/pii/009731659290096D?via%3Dihub">paper</a>.</p>
<p></p><h2> Open Problems </h2><p></p>
<p>
See Gil Kalai for his wonderful <a href="https://gilkalai.wordpress.com/2021/01/14/to-cheer-you-up-in-difficult-times-17-amazing-the-erdos-faber-lovasz-conjecture-for-large-n-was-proved-by-dong-yeap-kang-tom-kelly-daniela-kuhn-abhishek-methuku-and-deryk-osthus/">comments</a> on this paper of Kühn and her co-authors.</p>
<p>
Of course we <a href="https://www.nytimes.com/2021/03/17/science/abel-prize-mathematics.html">congratulate</a> Laci <a href="https://www.nature.com/articles/d41586-021-00694-9">plus</a> Avi Wigderson on winning the <a href="https://www.abelprize.no/">2021 Abel Prize</a>.  We will write more about this in an upcoming post.</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wpcomstaging.com/2021/03/19/closing-an-erdos-problem/"><span class="datestr">at March 19, 2021 10:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/043">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/043">TR21-043 |  Promise Problems Meet Pseudodeterminism | 

	Peter Dixon, 

	A.  Pavan, 

	N. V. Vinodchandran</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The  Acceptance Probability Estimation Problem (APEP) is to additively approximate the acceptance probability of a Boolean circuit. This problem admits a probabilistic approximation scheme. A central question is whether we can design a pseudodeterministic approximation algorithm for this problem: a probabilistic polynomial-time algorithm that outputs a canonical approximation with high probability. Recently, it was shown that such an algorithm would imply that every approximation algorithm can be made pseudodeterministic (Dixon, Pavan, Vinodchandran; (ITCS 2021).    

The main conceptual contribution of this work is to establish that the existence of a pseudodeterministic algorithm for APEP is fundamentally connected to the relationship between probabilistic promise classes and the corresponding standard complexity classes. In particular, we show the following equivalence:  every promise problem in PromiseBPP has a solution in BPP if and only if APEP has a pseudodeterministic algorithm. Based on this intuition, we show that pseudodeterministic algorithms for APEP  can shed light on a few central topics in complexity theory such as circuit lowerbounds, probabilistic hierarchy theorems, and multi-pseudodeterminism.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/043"><span class="datestr">at March 18, 2021 06:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4504">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2021/03/18/finally-a-joy/">Finally, a joy</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>In Rome we have an expression, <em>mai una gioia</em> (literally, “never (a moment of) joy”) that applies well to the present times. Yesterday, there was, finally, something to be joyous about: the announcement that two of my heroes, Laszlo Lovasz and Avi Wigderson, will share the 2021 Abel Prize, one of the highest honors of mathematics.</p>
<p>The reader can find a very good article about them on <a href="https://www.quantamagazine.org/avi-wigderson-and-laszlo-lovasz-win-abel-prize-20210317/">Quanta Magazine</a>.</p>
<p>Instead of talking about their greatest accomplishment, here I would like to recall two beautiful and somewhat related results, that admit a short treatment.</p>
<p><span id="more-4504"></span></p>
<p>Avi’s first paper was on the 3-coloring problem. He described a polynomial time algorithm that, given a 3-colorable graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />, finds a valid coloring of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(\sqrt n)}" class="latex" /> colors, where <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> is the number of vertices. The starting point is that if every vertex in <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> had degree <img src="https://s0.wp.com/latex.php?latex=%7B%5Cleq+%5Csqrt+n+-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\leq \sqrt n -1}" class="latex" /> then there is an easy way to color the graph with <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sqrt n}" class="latex" /> colors (look at the vertices in any order, choose for each vertex a color not already used for the neighbors that have already been considered).</p>
<p>What if there is some vertex <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> of degree <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgeq+%5Csqrt+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\geq \sqrt n}" class="latex" />? Well, and this is the idea of the paper, the <em>neighbors of <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> must induce a bipartite subgraph</em>, so we can color the neighbors of <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> with 2 colors in a way that is consistent with all the edges between neighbors of <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" />, then remove the neighbors of <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" /> and continue with the rest of the graph, committing to never use those two colors again. Every time we do this, we consume two colors but we get rid of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cgeq+%5Csqrt+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\geq \sqrt n}" class="latex" /> vertices, so this costs us at most <img src="https://s0.wp.com/latex.php?latex=%7B2%5Csqrt+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2\sqrt n}" class="latex" /> colors. When we are done, all remaining vertices have degree <img src="https://s0.wp.com/latex.php?latex=%7B%5Cleq+%5Csqrt+n-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\leq \sqrt n-1}" class="latex" /> and we just need <img src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\sqrt n}" class="latex" /> more colors as observed above.</p>
<p>In total, we used at most <img src="https://s0.wp.com/latex.php?latex=%7B3%5Csqrt+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3\sqrt n}" class="latex" /> colors.</p>
<p>This remained the state of the art for approximate graph coloring for a few years, until Avrim Blum improved the bound for 3-colorable graphs to <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%7B0.4%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(n^{0.4})}" class="latex" /> and then to <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%7B3%2F8%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(n^{3/8})}" class="latex" /> colors, using more sophisticated combinatorial arguments. Later, Karger, Motwani and Sudan devised an approximation algorithm that uses <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde+O%28n%5E%7B1%2F4%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\tilde O(n^{1/4})}" class="latex" /> colors to color 3-colorable graphs, and is based on a semidefinite programming relaxation of the coloring problem, which is related to the Lovasz <em>theta function</em>. It is still open whether there is a polynomial time algorithm that uses <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7Bo%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n^{o(1)}}" class="latex" /> colors.</p>
<p>The Lovasz theta function <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctheta+%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\theta (G)}" class="latex" /> of a graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> is a convex relaxation of the maximum independent set problem in <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />. It is, or it can be formulated as, a semidefinite programming relaxation of the maximum independent set problem, and so it can be computed (up to arbitrarily good accuracy) in polynomial time.</p>
<p>Lovasz defined the theta function to study the <em>Shannon capacity of a graph</em>. This is a question that arises when one wants to find error-correcting codes for a channel that may introduce errors. Suppose that the channel carries an element of an alphabet <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{V}" class="latex" /> of size <img src="https://s0.wp.com/latex.php?latex=%7Bn%3D%7CV%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n=|V|}" class="latex" />, and that certain pairs of alphabet elements can be “confused” by the channel, meaning that for certain pairs <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+a%2Cb+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{ a,b \}}" class="latex" /> the channel may output <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{b}" class="latex" /> given <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a}" class="latex" /> or viceversa. If we construct the graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> whose vertex set is <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{V}" class="latex" /> and whose edges are the pairs of “confusable” pairs, then an independent set in this graph gives a set of length-1 codewords that are not confusable, and <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog_2+%5Calpha%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log_2 \alpha(G)}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\alpha(G)}" class="latex" /> is the size of a maximal independent set in <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />, is the number of bits that we can transmit in an errorless way by making one use of the channel. If we can access the channel <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" /> times, then an errorless code is an independent set in <img src="https://s0.wp.com/latex.php?latex=%7BG%5Ek%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G^k}" class="latex" />, the graph that has vertex set <img src="https://s0.wp.com/latex.php?latex=%7BV%5Ek%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{V^k}" class="latex" /> and such that <img src="https://s0.wp.com/latex.php?latex=%7B%28%28a_1%2C%5Cldots%2Ca_k%29%2C%28b_1%2C%5Cldots%2Cb_k%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{((a_1,\ldots,a_k),(b_1,\ldots,b_k))}" class="latex" /> is an edge if and only if all the pairs <img src="https://s0.wp.com/latex.php?latex=%7B%28a_i%2Cb_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(a_i,b_i)}" class="latex" /> are edges of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />. The number of bits of information per channel use that we can send is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac+1k+%5Clog+%5Calpha%28G%5Ek%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \frac 1k \log \alpha(G^k) " class="latex" /></p>
<p> and so we are interested in</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clim_%7Bk%5Crightarrow+%5Cinfty%7D+%28%5Calpha+%28G%5Ek%29%29%5E%7B1%2Fk%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \lim_{k\rightarrow \infty} (\alpha (G^k))^{1/k} " class="latex" /></p>
<p> Nicely, we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ctheta+%28G%5Ek%29+%3D+%28%5Ctheta%28G%29%29%5Ek+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \theta (G^k) = (\theta(G))^k " class="latex" /></p>
<p> so we have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ctheta%28G%29+%5Cgeq+%5Clim_%7Bk%5Crightarrow+%5Cinfty%7D+%28%5Calpha+%28G%5Ek%29%29%5E%7B1%2Fk%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \theta(G) \geq \lim_{k\rightarrow \infty} (\alpha (G^k))^{1/k} " class="latex" /></p>
<p> and the theta function provides an upper bound to the Shannon capacity of a graph.</p>
<p>If we compute <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctheta+%28%5Cbar+G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\theta (\bar G)}" class="latex" />, where <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbar+G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\bar G}" class="latex" /> is the complement of the graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> (that is, the graph whose edges are all the non-edges of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />), then we see that <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctheta%28%5Cbar+G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\theta(\bar G)}" class="latex" /> is a relaxation of the maximum clique problem in <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />. Remarkably, <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctheta%28%5Cbar+G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\theta(\bar G)}" class="latex" /> is also a <em>lower bound</em> to the chromatic number of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" />, and so it is <em>sandwiched</em> between the clique number and the chromatic number. Don Knuth has used this fact as a starting point for a wonderful <a href="https://arxiv.org/abs/math/9312214">49-page treatise</a> on convex relaxations of the chromatic number and the clique number; here we will see a quick proof of this fact.</p>
<p>As you can see in Knuth’s survey, there are several equivalent ways to define the theta function. Below we define <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctheta%28%5Cbar+G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\theta(\bar G)}" class="latex" /> as a natural semidefinite programming relaxation of the maximum clique problem (we have a variable <img src="https://s0.wp.com/latex.php?latex=%7Bx_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_0}" class="latex" /> plus one variable <img src="https://s0.wp.com/latex.php?latex=%7Bx_v%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_v}" class="latex" /> for every vertex <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v}" class="latex" />; all variables are vectors).</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cbegin%7Barray%7D%7Blr%7D+%5Cmax+%5Csum_%7Bv%5Cin+V%7D+%7C%7C+x_v%7C%7C%5E2%5C%5C+s.t.%5C%5C+%7C%7Cx_0%7C%7C%5E2+%3D+1%5C%5C+%5Clangle+x_0%2Cx_v+%5Crangle+%3D+%7C%7Cx_v%7C%7C%5E2+%26+%5Cforall+v%5Cin+V%5C%5C+%5Clangle+x_u%2Cx_v+%5Crangle+%3D+0+%26+%5Cforall+u%5Cneq+v%3A+%28u%2Cv%29+%5Cnot%5Cin+E%5C%5C+%5Cend%7Barray%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle \begin{array}{lr} \max \sum_{v\in V} || x_v||^2\\ s.t.\\ ||x_0||^2 = 1\\ \langle x_0,x_v \rangle = ||x_v||^2 &amp; \forall v\in V\\ \langle x_u,x_v \rangle = 0 &amp; \forall u\neq v: (u,v) \not\in E\\ \end{array}" class="latex" /></p>
<p>To see that it is a relaxation, given a clique <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{K}" class="latex" />, let <img src="https://s0.wp.com/latex.php?latex=%7Bx_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_0}" class="latex" /> be any unit vector, and let <img src="https://s0.wp.com/latex.php?latex=%7Bx_v+%3D+x_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_v = x_0}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=%7Bv%5Cin+K%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v\in K}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bx_v+%3D+%7B%5Cbf+0%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_v = {\bf 0}}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=%7Bv%5Cnot%5Cin+K%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{v\not\in K}" class="latex" />. We satisfy all constraints and the objective function is <img src="https://s0.wp.com/latex.php?latex=%7B%7CK%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{|K|}" class="latex" />.</p>
<p>If we write the semidefinite dual of the above relaxation, we can see that we get a minimization problem which is a relaxation of the chromatic number problem (and which is equivalent to the relaxation used by Karger, Motwani and Sudan to improve on Avi’s result on 3-coloring), so that the “sandwich theorem” follows from weak duality.</p>
<p>The reader should try to write down the semidefinite dual and reason about why it relaxes the chromatic number problem. Below, we collapse the construction of the dual, the way of embedding colorings into dual solutions, and the proof of weak duality for semidefinite programming, into a short but mystifying “sum of squares” proof.</p>
<p>Suppose that <img src="https://s0.wp.com/latex.php?latex=%7BV+%3D+%5C%7B+v_1%2C%5Cldots%2Cv_n+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{V = \{ v_1,\ldots,v_n \}}" class="latex" /> and let <img src="https://s0.wp.com/latex.php?latex=%7Bx_0%2Cx_%7Bv_1%7D%2C%5Cldots%2Cx_%7Bv_n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_0,x_{v_1},\ldots,x_{v_n}}" class="latex" /> be a feasible solution to the max clique relaxation defined above. Suppose that the graph is <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{k}" class="latex" />-colorable and let <img src="https://s0.wp.com/latex.php?latex=%7B%28C_1%2C%5Cldots%2CC_k%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(C_1,\ldots,C_k)}" class="latex" /> be a partition of <img src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{V}" class="latex" /> into color classes. We have</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++0+%5Cleq+%5Csum_%7Bi%3D1%7D%5Ek+%5Cleft+%5C%7C+-x_0+%2B+%5Csum_%7Bv%5Cin+C_i%7D+x_v+%5Cright+%5C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  0 \leq \sum_{i=1}^k \left \| -x_0 + \sum_{v\in C_i} x_v \right \|^2 " class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_%7Bi%3D1%7D%5Ek+%5Cleft%28+%7C%7C+x_0%7C%7C%5E2+-+2+%5Csum_%7Bv%5Cin+C_i%7D+%5Clangle+x_0+%2C+x_v+%5Crangle+%2B+%5Csum_%7Bu%2Cv+%5Cin+C_i%7D+%5Clangle+x_u%2C+x_v%5Crangle+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  = \sum_{i=1}^k \left( || x_0||^2 - 2 \sum_{v\in C_i} \langle x_0 , x_v \rangle + \sum_{u,v \in C_i} \langle x_u, x_v\rangle \right) " class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_%7Bi%3D1%7D%5Ek+%5Cleft%28+1+-+%5Csum_%7Bv%5Cin+C_i%7D+%7C%7C+x_v%7C%7C%5E2+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  = \sum_{i=1}^k \left( 1 - \sum_{v\in C_i} || x_v||^2 \right) " class="latex" /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+k+-+%5Csum_%7Bv%5Cin+V%7D+%7C%7Cx_v%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  = k - \sum_{v\in V} ||x_v||^2 " class="latex" /></p>
<p> so that the objective function always satisfies</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bv%5Cin+V%7D+%7C%7Cx_v%7C%7C%5E2+%5Cleq+k+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \sum_{v\in V} ||x_v||^2 \leq k " class="latex" /></p>
<p> In particular, this is true for an optimal solution <img src="https://s0.wp.com/latex.php?latex=%7Bx_0%2Cx_%7Bv_1%7D%2C%5Cldots%2Cx_%7Bv_k%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x_0,x_{v_1},\ldots,x_{v_k}}" class="latex" /> and for an optimal coloring, so we have that <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctheta%28%5Cbar+G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\theta(\bar G)}" class="latex" /> is upper bounded by the chromatic number of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{G}" class="latex" /> as promised.</p>
<p>Above, we used various properties of feasible solutions, such as <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7Cx_0%7C%7C%5E2+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||x_0||^2 = 1}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+x_0%2C+x_v+%5Crangle+%3D+%7C%7Cx_v%7C%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\langle x_0, x_v \rangle = ||x_v||^2}" class="latex" /> and</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bu%2Cv+%5Cin+C_i+%7D+%5Clangle+x_u+%2C+x_v+%5Crangle+%3D+%5Csum_%7Bv%5Cin+C_i%7D+%7C%7C+x_v%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  \sum_{u,v \in C_i } \langle x_u , x_v \rangle = \sum_{v\in C_i} || x_v||^2 " class="latex" /></p>
<p> which follows from the fact that each color class is an independent set, and so all inner products <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+x_u%2C+x_v+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\langle x_u, x_v \rangle}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=%7Bu%2Cv+%5Cin+C_i%2C+u%5Cneq+v%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{u,v \in C_i, u\neq v}" class="latex" /> have to be equal to zero, because <img src="https://s0.wp.com/latex.php?latex=%7B%28u%2Cv%29+%5Cnot%5Cin+E%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(u,v) \not\in E}" class="latex" />.</p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2021/03/18/finally-a-joy/"><span class="datestr">at March 18, 2021 01:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/03/18/professor-at-tu-hamburg-apply-by-april-29-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/03/18/professor-at-tu-hamburg-apply-by-april-29-2021/">Professor at TU Hamburg (apply by April 29, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>TU Hamburg invites applications for a professorship (assistant professorship/W1 with tenure track to full professorship/W3) in Theoretical Computer Science. The focus is on logic and computability, models and algorithms for new computing architectures (cloud, in-memory, fine-grained complexity and others), model theory and game theory in verification, algorithmic foundations of quantum computing.</p>
<p>Website: <a href="https://stellenportal.tuhh.de/jobposting/a25baa5724882150161c93dc5ad0f48777de95ad">https://stellenportal.tuhh.de/jobposting/a25baa5724882150161c93dc5ad0f48777de95ad</a><br />
Email: berufungen@tuhh.de</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/03/18/professor-at-tu-hamburg-apply-by-april-29-2021/"><span class="datestr">at March 18, 2021 08:43 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5388">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5388">Abel to win</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>Many of you will have seen the happy news today that Avi Wigderson and László Lovász <a href="https://www.abelprize.no/c76389/seksjon/vis.html?tid=76390&amp;strukt_tid=76389">share this year’s Abel Prize</a> (which now contends with the Fields Medal for the highest award in pure math).  This is only the second time that the Abel Prize has been given wholly or partly for work in theoretical computer science, after <a href="https://www.abelprize.no/c54147/seksjon/vis.html?tid=54148">Szemerédi</a> in 2012.  See also the articles in <em><a href="https://www.quantamagazine.org/avi-wigderson-and-laszlo-lovasz-win-abel-prize-20210317/">Quanta</a></em> or the <a href="https://www.nytimes.com/2021/03/17/science/abel-prize-mathematics.html">NYT</a>, which actually say most of what I would’ve said for a lay audience about Wigderson’s and Lovász’s most famous research results and their importance (except, no, Avi hasn’t <em>yet</em> proved P=BPP, just taken some major steps toward it…).</p>



<p>On a personal note, Avi was both my and my wife Dana’s postdoctoral advisor at the Institute for Advanced Study in Princeton.  He’s been an <em>unbelievably</em> important mentor to both of us, as he’s been for dozens of others in the CS theory community.  Back in 2007, I also had the privilege of working closely with Avi for months on our <a href="https://www.scottaaronson.com/papers/alg.pdf">Algebrization</a> paper.  Now would be a fine time to revisit <a href="https://www.scottaaronson.com/blog/?p=2925">Avi’s Permanent Impact on Me</a> (or <a href="https://www.youtube.com/watch?v=BLxC3rGeWBI">watch the YouTube video</a>), which is the talk I gave at IAS in 2016 on the occasion of Avi’s 60th birthday.</p>



<p>Huge congratulations to both Avi and László!</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5388"><span class="datestr">at March 18, 2021 03:19 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
