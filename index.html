<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="https://blogs.oregonstate.edu:443/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="403: forbidden">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" class="message" title="internal server error">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="https://toc4fairness.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://toc4fairness.org" title="TOC for Fairness">TOC for Fairness</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at March 06, 2021 05:23 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/032">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/032">TR21-032 |  Fiat-Shamir via List-Recoverable Codes (or: Parallel Repetition of GMW is not Zero-Knowledge) | 

	Ron Rothblum, 

	Justin Holmgren, 

	Alex Lombardi</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Shortly after the introduction of zero-knowledge proofs, Goldreich, Micali and Wigderson (CRYPTO '86) demonstrated their wide applicability by constructing  zero-knowledge proofs for the NP-complete problem of graph 3-coloring. A long-standing open question has been whether parallel repetition of their protocol preserves zero knowledge. In this work, we answer this question in the negative, assuming a a standard cryptographic assumption (i.e., the hardness of learning with errors (LWE)).

Leveraging a connection observed by Dwork, Naor, Reingold, and Stockmeyer (FOCS '99), our negative result is obtained by making positive progress on a related fundamental problem in cryptography: securely instantiating the Fiat-Shamir heuristic for eliminating interaction in public-coin interactive protocols. A recent line of works has shown how to instantiate the heuristic securely, albeit only for a limited class of protocols.

Our main result shows how to instantiate Fiat-Shamir for parallel repetitions of much more general interactive proofs. In particular, we construct hash functions that, assuming LWE, securely realize the Fiat-Shamir transform for the following rich classes of protocols:

- The parallel repetition of any ``commit-and-open'' protocol (such as the GMW protocol mentioned above), when a specific (natural) commitment scheme is used.  Commit-and-open protocols are a ubiquitous paradigm for constructing general purpose public-coin zero knowledge proofs.

- The parallel repetition of any base protocol that (1) satisfies a stronger notion of soundness called round-by-round soundness, and (2) has an efficient procedure, using a suitable trapdoor, for recognizing ``bad verifier randomness'' that would allow the prover to cheat.

Our results are obtained by establishing a new connection between the Fiat-Shamir transform and  list-recoverable codes.  In contrast to the usual focus in coding theory, we focus on a parameter regime in which the input lists are extremely large, but the rate can be small.  We give a (probabilistic) construction based on Parvaresh-Vardy codes (FOCS '05) that suffices for our applications.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/032"><span class="datestr">at March 05, 2021 11:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/031">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/031">TR21-031 |  Upper Bound for Torus Polynomials | 

	Vaibhav Krishan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove that all functions that have low degree torus polynomials approximating them with small error also have $MidBit^+$ circuits computing them. This serves as a partial converse to the result that all $ACC$ functions have low degree torus polynomials approximating them with small error, by Bhrushundi, Hosseini, Lovett and Rao (ITCS 2019).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/031"><span class="datestr">at March 05, 2021 02:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=18238">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2021/03/04/wsj-meets-group-algorithms/">WSJ Meets Group Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Our whole life is solving puzzles. — Ernő Rubik</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2021/03/jf-1.png"><img width="163" alt="" src="https://rjlipton.files.wordpress.com/2021/03/jf-1.png?w=163&amp;h=120" class="alignright wp-image-18244" height="120" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from <a href="http://www.ws.binghamton.edu/fridrich/pressconnects_com%20%2009-11-03%20%20News%20Story.htm">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Jessica Fridrich is a Distinguished Professor of Electrical and Computer Engineering at Binghamton University. She is an expert on data hiding, that is, <a href="https://en.wikipedia.org/wiki/Steganography">steganography</a>. She has over 34,000 citations—impressive. A lot more than most of us. She also has <a href="http://www.ws.binghamton.edu/fridrich/cube.html">worked</a> on the famous Rubik’s cube.</p>
<p>
Today we look at her work on Rubik’s cube, the WSJ’s interest in Rubik’s cube, and what both say—and don’t say—about fundamental algorithms.</p>
<p>
By the way, WSJ stands for the Wall Street Journal—the American <a href="https://en.wikipedia.org/wiki/The_Wall_Street_Journal">newspaper</a> of business. The WSJ has shown great interest in the Rubik’s cube puzzle and has run many articles over the years on it.</p>
<p>
Recall the cube puzzle was invented…of course you know all about Rubik’s cube. You probably have owned one at one time. Right. Just for a <a href="https://en.wikipedia.org/wiki/Rubik%27s_Cube">refresher</a>: </p>
<blockquote><p><b> </b> <em> The Rubik’s Cube is a 3-D combination puzzle invented in 1974 by Hungarian sculptor and professor of architecture Ernő Rubik. As of January 2009, 350 million cubes had been sold worldwide, making it the world’s top-selling puzzle game. It is widely considered to be the world’s best-selling toy. </em>
</p></blockquote>
<p>
But you may not know all about Fridrich.</p>
<p></p><h2> Speed Solving </h2><p></p>
<p>
Fridrich was one of the progenitors of <em>speed cubing</em>. She took part in the First World Championship in 1982 in Budapest, next-door to her native Czechoslovakia. She finished in the middle of the pack with a time of <b>29.11</b> seconds from a randomly well-mixed starting cube position. Her thoughts on how the cubes could be better prepared for speed are recorded on her <a href="http://www.ws.binghamton.edu/fridrich/cubewrld.html">page</a> about the tournament.</p>
<p>
At the Second World Championship, she improved her average time to <b>20.48</b> seconds and placed <a href="https://www.worldcubeassociation.org/competitions/WC2003">2nd</a>. She had the two fastest solves in the finals but lost on average-of-median-three-of-five.  That championship took place in Toronto—in <b>2003</b>. She is at a loss to explain why there was such a gap. Usually an athlete—in this case a mathlete?—is on the downswing nearing age 40, but even as a self-described “<a href="http://ws2.binghamton.edu/fridrich/history.html">old-timer</a>,” she fended off all but one of a whole next generation. </p>
<p>
Much of the credit goes to her solving method. She originated the “O” and “P” parts of the <a href="https://en.wikipedia.org/wiki/CFOP_method">CFOP</a> method. CFOP stands for: Cross, First 2 Layers, Orient Last Layer, Permute Last Layer. Versions of this are used my most top “cubers” to this day, and her name is often affixed to the method. In a 2008 profile of her, the New York Times <a href="https://www.nytimes.com/2008/12/16/science/16prof.html?_r=1&amp;em">quoted</a> the 2003 winner as saying that Fridrich found the route up the mountain while the rest of the cubers optimize traversing ledges along it. And in 2012, the NYT <a href="https://london2012.blogs.nytimes.com/2012/06/25/master-of-the-shot-put-and-the-cube/?searchResultPosition=5">quoted</a> Olympic shot-putter Reese Hoffa as wanting “to learn the Fridrich Method of solving the puzzle, ‘which is what all of the best cubers use.'” </p>
<p>
At this point, knowing our interest in chess, you might expect a <a href="https://en.wikipedia.org/wiki/The_Queen's_Gambit_(novel)"><i>Queen’s</i></a> <a href="https://en.wikipedia.org/wiki/The_Queen's_Gambit_(miniseries)"><i>Gambit</i></a> reference. But what we have here is not a story of Beth Harmon coming back from a life <a href="https://www.thereviewgeek.com/thequeensgambit-e6review/">adjournment</a> or Roy Hobbs in <a href="https://en.wikipedia.org/wiki/The_Natural"><i>The</i></a> <a href="https://en.wikipedia.org/wiki/The_Natural_(film)"><i>Natural</i></a> rejoining baseball almost 20 years after being shot. It’s about going overseas, earning a PhD, getting two research positions, writing early papers (under the <a href="https://dblp.org/pid/29/4038.html">name</a> Jiri Fridrich), transitioning, then getting a faculty position leading to tenure while developing mathematical formulas and writing tons of code for systems to <a href="https://www.nytimes.com/2004/07/22/technology/what-s-next-for-doctored-photos-a-new-flavor-of-digital-truth-serum.html?searchResultPosition=16">source</a> photos and catch digital pirates and pornographers and other image fraudsters, then coming back to light up an <img src="https://s0.wp.com/latex.php?latex=%7B8+%5Ctimes+8%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{8 \times 8}" class="latex" /> or <img src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3 \times 3 \times 3}" class="latex" /> universe. Not to mention doing her own stunning <a href="https://www.jessicafridrich.com/">photo art</a> of the American Southwest.</p>
<p></p><h2> Quicker Times and Cubes </h2><p></p>
<p>
Since 2003, the <a href="https://en.wikipedia.org/wiki/Speedcubing#Competitions">championships</a> have been held every other year, thought the 2021 championships set for the Netherlands are uncertain owing to the pandemic. The youngsters soon broke through en-masse, and it strikes me that the cube technology improved so that the cubes are springier and lighter. The winning time fell almost 5 seconds to <b>15.10</b> in 2005 and hit <b>6.74</b> in 2019. That was not the world record, however—an incredible <b>3.47</b> seconds in 2018 by Yusheng Du, beating the previous record of Feliks Zemdegs by a whopping 3/4 of a second.</p>
<p>
Fridrich, however, must claim a distinction no one may ever match. She learned how to solve the cube and traced out the performance of methods of doing so in 1981, months before she saw a cube, let alone owned one. Despite the “Bűvös Kocka” (“Magic Cube,” as Rubik called it) having been on shelves in neighboring Hungary for four years, with worldwide marketing by early 1980, they were hard to come by in her home city, Ostrava. </p>
<p>
She found an article on solving the cube in a Russian magazine. It laid out the concept of group theory and the role of group commutators, which she learned to apply creatively in order to streamline actions. The first time she touched a cube was to help a friend put his back the way it was. A family visiting from France let her keep one, and later in 1981 she was finally able to purchase a few more. This invites analogy to working out chess without a board on a bedroom ceiling as depicted in <em>The Queen’s Gambit</em>.</p>
<p>
We—Dick and Ken—must admit that neither of us has ever done this with the cubes we own, not fast, not slow. Yet we do understand the theory behind it. We believe we do. </p>
<p></p><h2> Group Theory of the Cube </h2><p></p>
<p>
I (Dick) plan on explaining the theory by using a new toy that I have invented: The <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathit%7Bslider%7D%5E%7BTM%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathit{slider}^{TM}}" class="latex" />. 	</p>
<p><a href="https://rjlipton.files.wordpress.com/2021/03/slidercubes.png"><img width="96" alt="" src="https://rjlipton.files.wordpress.com/2021/03/slidercubes.png?w=96&amp;h=45" class="aligncenter wp-image-18247" height="45" /></a></p>
<p>
We will write the state as <img src="https://s0.wp.com/latex.php?latex=%7Bxyz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{xyz}" class="latex" /> where each of <img src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%2Cz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{x,y,z}" class="latex" /> is one of <font color="red">1</font>, <font color="green">2</font>, or <font color="blue">3</font>. The operations allowed are the <i>cyclic shift</i> <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{C}" class="latex" />, which does 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++xyz+%5Crightarrow+zxy%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  xyz \rightarrow zxy, " class="latex" /></p>
<p>and the <i>flip</i> <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{F}" class="latex" /> of the initial two elements: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++xyz+%5Crightarrow+yxz.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  xyz \rightarrow yxz. " class="latex" /></p>
<p>
Note there are 6 possible states. For the real Rubik’s cube, the number of states is just a little bit larger: <b>43,252,003,274,489,856,000</b>. But the basic concept is the same. Suppose we are given the state 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++132.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  132. " class="latex" /></p>
<p>How fast can you get the initial state <img src="https://s0.wp.com/latex.php?latex=%7B123%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{123}" class="latex" />? Apply <img src="https://s0.wp.com/latex.php?latex=%7BFCC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{FCC}" class="latex" />: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++132+%5Crightarrow+312+%5Crightarrow+231+%5Crightarrow+123+.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  132 \rightarrow 312 \rightarrow 231 \rightarrow 123 . " class="latex" /></p>
<p>
This is a special case of the general <a href="https://kconrad.math.uconn.edu/blurbs/grouptheory/genset.pdf">result</a> that any symmetric group is <a href="https://groupprops.subwiki.org/wiki/Symmetric_group_on_a_finite_set_is_2-generated">generated</a> by two operations: a full cycle and a single flip. The key with the actual Rubik’s cube is since the group is larger and it has more operations that can be applied finding the group operations may be more difficult. But there are algorithms that can find them. See <a href="https://kconrad.math.uconn.edu/blurbs/grouptheory/gpaction.pdf">this</a> for another article by Keith Conrad. </p>
<p>
There are many more pages like that on the cube. But Fridrich still shows the <a href="http://www.ws.binghamton.edu/fridrich/system.html">seminal page</a> she posted in “Winter 1996/97.” It links to other pages, ones that also credit other people, such as <a href="http://www.ws.binghamton.edu/fridrich/Mike/middle.html">this</a> explaining the algorithms in great pictorial detail. This was in the infancy of the Internet. Her pages are often credited with spurring the turn-of-the-millennium boom in Rubik’s cube which led to the revival of the championships in 2003. A 2016 New York Post <a href="https://nypost.com/2016/10/31/how-the-internet-brought-the-rubiks-cube-back-to-life/">article</a> whose URL is titled, “how the Internet brought the Rubik’s cube back to life,” says: </p>
<blockquote><p><b> </b> <em> The seeds for Rubik’s Cube’s rediscovery were sown on the internet. In the mid-1990s, a Rubik’s Cube champion-turned-computer-science professor at SUNY Binghamton posted her secrets of the Cube on a primitive Web 1.0 site on the university’s servers. Jessica Fridrich’s method spread and is today the most widely used technique to solve the puzzle. </em>
</p></blockquote>
<p>
See also this <a href="https://uncletyson.wordpress.com/tag/dan-knights/">telling</a> by the 2003 winner, Dan Knights. This shows how one person using spare time on the Internet can power up business.</p>
<p></p><h2> Enter the WSJ </h2><p></p>
<p>
The WSJ has had an interest in Rubik’s cube for years. They had a long feature <a href="https://www.wsj.com/articles/how-to-teach-professors-humility-hand-them-a-rubiks-cube-11614352261">article</a> last week titled, “How to Teach Professors Humility? Hand Them a Rubik’s Cube,” by Melissa Korn. It describes a faculty development challenge among several small colleges in which professors became students again. Last month they also had an <a href="https://www.wsj.com/articles/seeing-things-with-the-power-of-symmetry-11612461325">article</a> on symmetry by the mathematician Eugenia Cheng that mentioned the cube.</p>
<p>
I recall several features the WSJ has run on the cube and its solvers. The 2011 <a href="https://www.wsj.com/articles/SB10001424052970204319004577088513615125328">article</a>, “One Cube, Many Knockoffs, Quintillions of Possibilities,” led off with the Polish teenager Michal Pleskowicz winning the 2011 world championship with a time of <b>8.65</b> seconds, then discussed the performance of pirated cubes: “One reason Mr. Pleskowicz and a new generation of Rubik’s fanatics can solve the notoriously difficult puzzle in record time: They don’t use Rubik’s Cubes at all, instead substituting souped-up Chinese knockoffs engineered for speed…” Their 2014 <a href="https://www.wsj.com/articles/SB10001424052702304518704579523513594900696">article</a>, “Rubik’s Cube Proves It’s Hip to Be Square,” profiled both Rubik and speed-solvers. </p>
<p>
The <a href="https://www.wsj.com/articles/a-thinking-persons-guide-to-the-rubiks-cube-1517586702">feature</a> I recall best was in 2018. It was titled, “A Thinking Person’s Guide to the Rubik’s Cube,” and subtitled, “What’s the best solution method—theory, algorithms or chance?” It was also by Eugenia Cheng. She begins by confessing, “I have always loved playing with a Rubik’s Cube, which combines logic with a satisfying tactile activity. I can solve it—getting each of the six sides to be one color—but not particularly quickly or cleverly.” </p>
<p>
They also like its use for analogies. Scrolling through their advanced search—both Ken and I subscribe to the WSJ—we find:</p>
<ul>
<li><a href="https://www.wsj.com/articles/close-reopen-repeat-restaurants-dont-know-what-covid-19-will-dish-out-next-11613138412">2/12/21</a>: “Running restaurants is now ‘a bit of a Rubik’s Cube,’ said Mr. Mosier, who reopened his casual cafes in late January.”
</li><li><a href="https://www.wsj.com/articles/reopening-schools-is-so-complicated-new-york-struggles-to-schedule-classes-11597939473">8/20/20</a>, headline: “Reopening Schools Is So Complicated, New York Is Struggling to Schedule Classes Nation’s largest district is still hashing out basic details about the school day; ‘a multidimensional Rubik’s Cube’ of challenges.”
</li><li><a href="https://www.wsj.com/articles/new-u-s-rules-on-foreign-students-put-universitiesin-dilemma-11594149280">7/7/20</a>: “The new [pandemic] rules have created a Rubik’s Cube of decisions for schools, which face unique challenges with each of their international student populations.”
</li><li><a href="https://www.wsj.com/articles/an-l-a-home-asking-62-million-includes-a-playful-perk-a-model-racetrack-11590523214">5/26/20</a>, about a home selling for $62 million: “Designed by Seattle-based architecture firm Olson Kundig, the house has interlocking boxes and planes resembling a Rubik’s cube…”
</li><li><a href="https://www.wsj.com/articles/president-trump-announces-19-billion-relief-program-for-farmers-11587165759">4/17/20</a>, quoting Agriculture Secretary Sonny Perdue on the coronavirus relief program for farmers: “It will be a logistical Rubik’s Cube.”
</li><li><a href="https://www.wsj.com/articles/he-wanted-something-more-from-retirement-so-he-got-three-jobs-11573743922">11/14/19</a>, about a retiree who started teaching business classes, keeping books for a non-profit business, and working on a ferry dock: “My society consists of able-bodied seamen, boat captains, truckers hauling bait and lobsters, fishermen, islanders and wide-eyed vacationers,” says Mr. Marshall. It’s “a constant Rubik’s cube. You never know what you’ll find.”
</li></ul>
<p>
In all, using the WSJ advanced search, we find 239 hits for “Rubik” going back to 1980. We should mention in-passing that one of them is their 7/17/20 <a href="https://www.wsj.com/articles/ron-graham-dazzled-admirers-with-math-and-juggling-feats-11594994403">obituary</a> for Ron Graham. We also find 7 hits for “Fridrich” over the same span. But they are all about the housing market, involving the Nashville-based realty Fridrich and Clarke.</p>
<p></p><h2> Open Problems </h2><p></p>
<p>
I am happy to see that the WSJ has published multiple articles on a particular algorithmic task. I like that algorithms have been the center of articles. I wish they would talk more about important algorithms. Solving a Rubik’s cube is not an algorithm that is used every day: What about: </p>
<ul>
<li>Sorting
</li><li>Searching
</li><li>Dynamic Programming
</li><li>Fast Arithmetic
</li></ul>
<p>They do have Eugenia Cheng, who wrote a <a href="https://www.wsj.com/articles/algorithms-arent-just-for-computers-11557407055">column</a> comparing sorting algorithms. And they have written on algorithms used in <a href="https://www.wsj.com/graphics/journey-inside-a-real-life-trading-algorithm/">trading</a> and on <a href="https://www.wsj.com/articles/social-media-algorithms-rule-how-we-see-the-world-good-luck-trying-to-stop-them-11610884800">social</a>–<a href="https://www.wsj.com/articles/how-google-interferes-with-its-search-algorithms-and-changes-your-results-11573823753">media</a> <a href="https://www.wsj.com/articles/how-to-win-friends-and-influence-algorithms-11555246800">platforms</a> and for <a href="https://www.wsj.com/articles/algorithms-used-in-policing-face-policy-review-11591003801">policing</a> and <a href="https://www.wsj.com/articles/SB10001424052702304626104579121251595240852">parole</a> and <a href="https://www.wsj.com/articles/algorithm-helps-new-york-decide-who-goes-free-before-trial-11600610400">bail</a> decisions. But that tends away from <em>fundamental algorithms</em> where the math is the matter.</p>
<p>
A 2018 WSJ <a href="https://www.wsj.com/articles/dont-believe-the-algorithm-1536157620">article</a> by Hannah Fry titled “Don’t Believe the Algorithm,” which begins with flaws in using facial recognition to find wanted suspects, brings us back toward Fridrich’s research. Might this all also raise discussion of “algorithms” for what and whom to cover?</p>
<p>
[fixed name at end]</p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2021/03/04/wsj-meets-group-algorithms/"><span class="datestr">at March 05, 2021 12:37 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.03238">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.03238">On the Complexity of Equilibrium Computation in First-Price Auctions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Filos=Ratsikas:Aris.html">Aris Filos-Ratsikas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Giannakopoulos:Yiannis.html">Yiannis Giannakopoulos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hollender:Alexandros.html">Alexandros Hollender</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lazos:Philip.html">Philip Lazos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Po=ccedil=as:Diogo.html">Diogo Poças</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.03238">PDF</a><br /><b>Abstract: </b>We consider the problem of computing a (pure) Bayes-Nash equilibrium in the
first-price auction with continuous value distributions and discrete bidding
space. We prove that when bidders have independent subjective prior beliefs
about the value distributions of the other bidders, computing an
$\varepsilon$-equilibrium of the auction is PPAD-complete, and computing an
exact equilibrium is FIXP-complete.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.03238"><span class="datestr">at March 05, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.03228">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.03228">One for One, or All for All: Equilibria and Optimality of Collaboration in Federated Learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blum:Avrim.html">Avrim Blum</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haghtalab:Nika.html">Nika Haghtalab</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Phillips:Richard_Lanas.html">Richard Lanas Phillips</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shao:Han.html">Han Shao</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.03228">PDF</a><br /><b>Abstract: </b>In recent years, federated learning has been embraced as an approach for
bringing about collaboration across large populations of learning agents.
However, little is known about how collaboration protocols should take agents'
incentives into account when allocating individual resources for communal
learning in order to maintain such collaborations. Inspired by game theoretic
notions, this paper introduces a framework for incentive-aware learning and
data sharing in federated learning. Our stable and envy-free equilibria capture
notions of collaboration in the presence of agents interested in meeting their
learning objectives while keeping their own sample collection burden low. For
example, in an envy-free equilibrium, no agent would wish to swap their
sampling burden with any other agent and in a stable equilibrium, no agent
would wish to unilaterally reduce their sampling burden.
</p>
<p>In addition to formalizing this framework, our contributions include
characterizing the structural properties of such equilibria, proving when they
exist, and showing how they can be computed. Furthermore, we compare the sample
complexity of incentive-aware collaboration with that of optimal collaboration
when one ignores agents' incentives.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.03228"><span class="datestr">at March 05, 2021 10:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.03035">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.03035">Computing Subset Feedback Vertex Set via Leafage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Papadopoulos:Charis.html">Charis Papadopoulos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tzimas:Spyridon.html">Spyridon Tzimas</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.03035">PDF</a><br /><b>Abstract: </b>Chordal graphs are characterized as the intersection graphs of subtrees in a
tree and such a representation is known as the tree model. Restricting the
characterization results in well-known subclasses of chordal graphs such as
interval graphs or split graphs. A typical example that behaves computationally
different in subclasses of chordal graph is the \textsc{Subset Feedback Vertex
Set} (SFVS) problem: given a graph $G=(V,E)$ and a set $S\subseteq V$, SFVS
asks for a minimum set of vertices that intersects all cycles containing a
vertex of $S$. SFVS is known to be polynomial-time solvable on interval graphs,
whereas SFVS remains \NP-complete on split graphs and, consequently, on chordal
graphs. Towards a better understanding of the complexity of SFVS on subclasses
of chordal graphs, we exploit structural properties of a tree model in order to
cope with the hardness of SFVS. Here we consider variants of the \emph{leafage}
that measures the minimum number of leaves in a tree model. We show that SFVS
can be solved in polynomial time for every chordal graph with bounded leafage.
In particular, given a chordal graph on $n$ vertices with leafage $\ell$, we
provide an algorithm for SFVS with running time $n^{O(\ell)}$. Pushing further
our positive result, it is natural to consider a slight generalization of
leafage, the \emph{vertex leafage}, which measures the smallest number among
the maximum number of leaves of all subtrees in a tree model. However, we show
that it is unlikely to obtain a similar result, as we prove that SFVS remains
\NP-complete on undirected path graphs, i.e., graphs having vertex leafage at
most two. Moreover, we strengthen previously-known polynomial-time algorithm
for SFVS on directed path graphs that form a proper subclass of undirected path
graphs and graphs of mim-width one.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.03035"><span class="datestr">at March 05, 2021 10:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.02980">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.02980">Construction of approximate $C^1$ bases for isogeometric analysis on two-patch domains</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Pascal Weinmüller, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Takacs:Thomas.html">Thomas Takacs</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.02980">PDF</a><br /><b>Abstract: </b>In this paper, we develop and study approximately smooth basis constructions
for isogeometric analysis over two-patch domains. One key element of
isogeometric analysis is that it allows high order smoothness within one patch.
However, for representing complex geometries, a multi-patch construction is
needed. In this case, a $C^0$-smooth basis is easy to obtain, whereas
$C^1$-smooth isogeometric functions require a special construction. Such spaces
are of interest when solving numerically fourth-order PDE problems, such as the
biharmonic equation and the Kirchhoff-Love plate or shell formulation, using an
isogeometric Galerkin method.
</p>
<p>With the construction of so-called analysis-suitable $G^1$ (in short,
AS-$G^1$) parametrizations, as introduced in (Collin, Sangalli, Takacs; CAGD,
2016), it is possible to construct $C^1$ isogeometric spaces which possess
optimal approximation properties. These geometries need to satisfy certain
constraints along the interfaces and additionally require that the regularity
$r$ and degree $p$ of the underlying spline space satisfy $1 \leq r \leq p-2$.
The problem is that most complex geometries are not AS-$G^1$ geometries.
Therefore, we define basis functions for isogeometric spaces by enforcing
approximate $C^1$ conditions following the basis construction from (Kapl,
Sangalli, Takacs; CAGD, 2017). For this reason, the defined function spaces are
not exactly $C^1$ but only approximately.
</p>
<p>We study the convergence behavior and define function spaces that converge
optimally under $h$-refinement, by locally introducing functions of higher
polynomial degree and lower regularity. The convergence rate is optimal in
several numerical tests performed on domains with non-trivial interfaces. While
an extension to more general multi-patch domains is possible, we restrict
ourselves to the two-patch case and focus on the construction over a single
interface.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.02980"><span class="datestr">at March 05, 2021 10:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.02972">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.02972">Weisfeiler--Leman, Graph Spectra, and Random Walks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rattan:Gaurav.html">Gaurav Rattan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Seppelt:Tim.html">Tim Seppelt</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.02972">PDF</a><br /><b>Abstract: </b>The Weisfeiler--Leman algorithm is a ubiquitous tool for the Graph
Isomorphism Problem with various characterisations in e.g. descriptive
complexity and convex optimisation. It is known that graphs that are not
distinguished by the two-dimensional variant have cospectral adjacency
matrices. We tackle a converse problem by proposing a set of matrices called
Generalised Laplacians that characterises the expressiveness of WL in terms of
spectra. As an application to random walks, we show using Generalised
Laplacians that the edge colours produced by 2-WL determine commute distances.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.02972"><span class="datestr">at March 05, 2021 10:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.02939">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.02939">Quad layouts with high valence singularities for flexible quad meshing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Jovana Jezdimirovi\' c, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chemin:Alexandre.html">Alexandre Chemin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reberol:Maxence.html">Maxence Reberol</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Henrotte:Fran=ccedil=ois.html">François Henrotte</a>, Jean François Remacle <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.02939">PDF</a><br /><b>Abstract: </b>A novel algorithm that produces a quad layout based on imposed set of
singularities is proposed. In this paper, we either use singularities that
appear naturally, e.g., by minimizing Ginzburg-Landau energy, or use as an
input user-defined singularity pattern, possibly with high valence
singularities that do not appear naturally in cross-field computations. The
first contribution of the paper is the development of a formulation that allows
computing a cross-field from a given set of singularities through the
resolution of two linear PDEs. A specific mesh refinement is applied at the
vicinity of singularities to accommodate the large gradients of cross
directions that appear in the vicinity of singularities of high valence. The
second contribution of the paper is a correction scheme that repairs limit
cycles and/or non-quadrilateral patches. Finally, a high quality
block-structured quad mesh is generated from the quad layout and per-partition
parameterization.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.02939"><span class="datestr">at March 05, 2021 10:45 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.02936">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.02936">On subgraph complementation to H-free graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Dhanyamol Antony, Jay Garchar, Sagartanu Pal, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sandeep:R=_B=.html">R. B. Sandeep</a>, Sagnik Sen, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Subashini:R=.html">R. Subashini</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.02936">PDF</a><br /><b>Abstract: </b>For a class $\mathcal{G}$ of graphs, the problem SUBGRAPH COMPLEMENT TO
$\mathcal{G}$ asks whether one can find a subset $S$ of vertices of the input
graph $G$ such that complementing the subgraph induced by $S$ in $G$ results in
a graph in $\mathcal{G}$. We investigate the complexity of the problem when
$\mathcal{G}$ is $H$-free for $H$ being a complete graph, a star, a path, or a
cycle. We obtain the following results:
</p>
<p>- When $H$ is a $K_t$ (a complete graph on $t$ vertices) for any fixed $t\geq
1$, the problem is solvable in polynomial-time. This applies even when
$\mathcal{G}$ is a subclass of $K_t$-free graphs recognizable in
polynomial-time, for example, the class of $(t-2)$-degenerate graphs.
</p>
<p>- When $H$ is a $K_{1,t}$ (a star graph on $t+1$ vertices), we obtain that
the problem is NP-complete for every $t\geq 5$. This, along with known results,
leaves only two unresolved cases - $K_{1,3}$ and $K_{1,4}$.
</p>
<p>- When $H$ is a $P_t$ (a path on $t$ vertices), we obtain that the problem is
NP-complete for every $t\geq 7$, leaving behind only two unresolved cases -
$P_5$ and $P_6$.
</p>
<p>- When $H$ is a $C_t$ (a cycle on $t$ vertices), we obtain that the problem
is NP-complete for every $t\geq 8$, leaving behind four unresolved cases -
$C_4, C_5, C_6,$ and $C_7$.
</p>
<p>Further, we prove that these hard problems do not admit subexponential-time
algorithms (algorithms running in time $2^{o(|V(G)|)}$), assuming the
Exponential Time Hypothesis. A simple complementation argument implies that
results for $\mathcal{G}$ are applicable for $\overline{\mathcal{G}}$, thereby
obtaining similar results for $H$ being the complement of a complete graph, a
star, a path, or a cycle. Our results generalize two main results and resolve
one open question by Fomin et al. (Algorithmica, 2020).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.02936"><span class="datestr">at March 05, 2021 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.02919">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.02919">A Simple Algorithm for the Constrained Sequence Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Francis Yuk Lun Chin, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Ho:Ngai_Lam.html">Ngai Lam Ho</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Santis:Alfredo_De.html">Alfredo De Santis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kim:S=_K=.html">S. K. Kim</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.02919">PDF</a><br /><b>Abstract: </b>In this paper we address the constrained longest common subsequence problem.
Given two sequences $X$, $Y$ and a constrained sequence $P$, a sequence $Z$ is
a constrained longest common subsequence for $X$ and $Y$ with respect to $P$ if
$Z$ is the longest subsequence of $X$ and $Y$ such that $P$ is a subsequence of
$Z$. Recently, Tsai \cite{Tsai} proposed an $O(n^2 \cdot m^2 \cdot r)$ time
algorithm to solve this problem using dynamic programming technique, where $n$,
$m$ and $r$ are the lengths of $X$, $Y$ and $P$, respectively. In this paper,
we present a simple algorithm to solve the constrained longest common
subsequence problem in $O(n \cdot m \cdot r)$ time and show that the
constrained longest common subsequence problem is equivalent to a special case
of the constrained multiple sequence alignment problem which can also be
solved.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.02919"><span class="datestr">at March 05, 2021 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.02916">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.02916">Consensus in Blockchain Systems with Low Network Throughput: A Systematic Mapping Study</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Henrik Knudsen, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Notland:Jakob_Svennevik.html">Jakob Svennevik Notland</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haro:Peter_Halland.html">Peter Halland Haro</a>, Truls Bakkejord Ræder, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jingyue.html">Jingyue Li</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.02916">PDF</a><br /><b>Abstract: </b>Blockchain technologies originate from cryptocurrencies. Thus, most
blockchain technologies assume an environment with a fast and stable network.
However, in some blockchain-based systems, e.g., supply chain management (SCM)
systems, some Internet of Things (IOT) nodes can only rely on the low-quality
network sometimes to achieve consensus. Thus, it is critical to understand the
applicability of existing consensus algorithms in such environments. We
performed a systematic mapping study to evaluate and compare existing consensus
mechanisms' capability to provide integrity and security with varying network
properties. Our study identified 25 state-of-the-art consensus algorithms from
published and preprint literature. We categorized and compared the consensus
algorithms qualitatively based on established performance and integrity metrics
and well-known blockchain security issues. Results show that consensus
algorithms rely on the synchronous network for correctness cannot provide the
expected integrity. Such consensus algorithms may also be vulnerable to
distributed-denial-of-service (DDOS) and routing attacks, given limited network
throughput. Conversely, asynchronous consensus algorithms, e.g.,
Honey-BadgerBFT, are deemed more robust against many of these attacks and may
provide high integrity in asynchrony events.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.02916"><span class="datestr">at March 05, 2021 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.02914">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.02914">The Bounded Acceleration Shortest Path problem: complexity and solution algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Stefano Ardizzoni, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Consolini:Luca.html">Luca Consolini</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Laurini:Mattia.html">Mattia Laurini</a>, Marco Locatelli <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.02914">PDF</a><br /><b>Abstract: </b>The purpose of this work is to introduce and characterize the Bounded
Acceleration Shortest Path (BASP) problem, a generalization of the Shortest
Path (SP) problem. This problem is associated to a graph: the nodes represent
positions of a mobile vehicle and the arcs are associated to pre-assigned
geometric paths that connect these positions. BASP consists in finding the
minimum-time path between two nodes. Differently from SP, we require that the
vehicle satisfy bounds on maximum and minimum acceleration and speed, that
depend on the vehicle position on the currently traveled arc. We prove that
BASP is NP-hard and define solution algorithm that achieves polynomial
time-complexity under some additional hypotheses on problem data.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.02914"><span class="datestr">at March 05, 2021 10:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.02749">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.02749">Introduction to Periodic Geometry and Topology</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Olga Anosova, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kurlin:Vitaliy.html">Vitaliy Kurlin</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.02749">PDF</a><br /><b>Abstract: </b>This paper introduces the key concepts and problems of the new research area
of Periodic Geometry and Topology for applications in Materials Science.
Periodic structures such as solid crystalline materials or textiles were
previously studied as isolated structures without taking into account the
continuity of their configuration spaces. The key new problem in Periodic
Geometry is an isometry classification of periodic point sets. A required
complete invariant should continuously change under point perturbations,
because atoms always vibrate in real crystals. The main objects of Periodic
Topology are embeddings of curves in a thickened plane that are invariant under
lattice translations. Such periodic knots were classified in the past up to
continuous deformations (isotopies) that keep a fixed lattice structure, hence
are realized in a fixed thickened torus. The more practical equivalence is a
periodic isotopy in a thickened plane without fixing a lattice basis. The paper
states the first results in the new area and proposes further problems and
directions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.02749"><span class="datestr">at March 05, 2021 10:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2103.02605">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2103.02605">On Fast Computation of a Circulant Matrix-Vector Product</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rosowski:Andreas.html">Andreas Rosowski</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2103.02605">PDF</a><br /><b>Abstract: </b>This paper deals with circulant matrices. It is shown that a circulant matrix
can be multiplied by a vector in time O(n log(n)) in a ring with roots of unity
without making use of an FFT algorithm. With our algorithm we achieve a speedup
of a factor of about 2.25 for the multiplication of two polynomials with
integer coefficients compared to multiplication by an FFT algorithm. Moreover
this paper discusses multiplication of large integers as further application.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2103.02605"><span class="datestr">at March 05, 2021 10:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5359">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5359">The Zen Anti-Interpretation of Quantum Mechanics</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>As I lay bedridden this week, knocked out by my second dose of the Moderna vaccine, I decided I should blog some more half-baked ideas because what the hell?  It feels therapeutic, I have tenure, and anyone who doesn’t like it can close their broswer tab.</p>



<p>So: although I’ve written tens of thousands <a href="https://www.pbs.org/wgbh/nova/article/can-quantum-computing-reveal-the-true-meaning-of-quantum-mechanics/">of</a> <a href="https://arxiv.org/abs/1306.0159">words</a>, <a href="https://www.scottaaronson.com/papers/philos.pdf">on</a> <a href="https://www.scottaaronson.com/blog/?p=1103">this</a> <a href="https://www.scottaaronson.com/blog/?p=3628">blog</a> <a href="https://www.scottaaronson.com/democritus/">and</a> <a href="https://www.scottaaronson.com/qclec.pdf">elsewhere</a>, about interpretations of quantum mechanics, again and again I’ve dodged the question of which interpretation (if any) I <em>really believe myself</em>.  Today, at last, I’ll emerge from the shadows and tell you precisely where I stand.</p>



<p>I hold that all interpretations of QM are just crutches that are better or worse at helping you along to the Zen realization that <strong>QM is what it is and doesn’t need an interpretation</strong>.  As Sidney Coleman <a href="https://arxiv.org/abs/2011.12671">famously argued</a>, what needs reinterpretation is not QM itself, but all our <em>pre</em>-quantum philosophical baggage—the baggage that leads us to demand, for example, that a wavefunction |ψ⟩ either be “real” like a stubbed toe or else “unreal” like a dream.  Crucially, because this philosophical baggage differs somewhat from person to person, the “best” interpretation—meaning, the one that leads most quickly to the desired Zen state—can also differ from person to person.  Meanwhile, though, thousands of physicists (and chemists, mathematicians, quantum computer scientists, etc.) have approached the Zen state merely by spending decades working with QM, never worrying much about interpretations at all.  This is probably the truest path; it’s just that most people lack the inclination, ability, or time.</p>



<p>Greg Kuperberg, one of the smartest people I know, once told me that the problem with the Many-Worlds Interpretation is not that it says anything wrong, but only that it’s “melodramatic” and “overwritten.”  Greg is far along the Zen path, probably further than me.</p>



<p>You shouldn’t confuse the Zen Anti-Interpretation with “Shut Up And Calculate.”  The latter phrase, mistakenly attributed to Feynman but really due to David Mermin, is something one might say at the <em>beginning</em> of the path, when one is as a baby.  I’m talking here only about the <em>endpoint</em> of path, which one can approach but never reach—the endpoint where you intuitively understand exactly what a Many-Worlder, Copenhagenist, or Bohmian would say about any given issue, and also how they’d respond to each other, and how they’d respond to the responses, etc. but after years of study and effort you’ve <em>returned</em> to the situation of the baby, who just sees the thing for what it is.</p>



<p>I don’t mean to say that the interpretations are all interchangeable, or equally good or bad.  If you had to, you could call even me a “Many-Worlder,” but <em>only</em> in the following limited sense: that in fifteen years of teaching quantum information, my experience has consistently been that for <em>most</em> students, <a href="https://en.wikipedia.org/wiki/Many-worlds_interpretation">Everett’s crutch</a> is the best one currently on the market.  At any rate, it’s the one that’s the most like a straightforward <em>picture</em> of the equations, and the least like a wobbly tower of words that might collapse if you utter any wrong ones.  Unlike Bohr, Everett will never make you feel stupid for asking the questions an inquisitive child would ask; he’ll simply give you answers that are as clear, logical, and internally consistent as they are metaphysically extravagant.  That’s a start.</p>



<p>The <a href="https://en.wikipedia.org/wiki/Copenhagen_interpretation">Copenhagen Interpretation</a> retains a place of honor as the <em>first</em> crutch, for decades the <em>only</em> crutch, and the one closest to the spirit of positivism.  Unfortunately, <em>wielding</em> the Copenhagen crutch requires mad philosophical skillz—which parts of the universe should you temporarily regard as “classical”?  which questions should be answered, and which deflected?—to the point where, if you’re capable of all that verbal footwork, then why do you even <em>need</em> a crutch in the first place?  In the hands of amateurs—meaning, alas, nearly everyone—Copenhagen often leads <em>away</em> <em>from</em> rather than toward the Zen state, as one sees with the generations of New-Age bastardizations about “observations creating reality.”</p>



<p>As for <a href="https://en.wikipedia.org/wiki/De_Broglie%E2%80%93Bohm_theory">deBroglie-Bohm</a>—well, that’s a weird, interesting, baroque crutch, one whose actual details (the preferred basis and the guiding equation) are historically contingent and tied to specific physical systems.  It’s probably the right crutch for <em>someone</em>—it gets eternal credit for having led Bell to discover the Bell inequality—but its quirks definitely need to be discarded along the way.</p>



<p>Note that, among those who approach the Zen state, many might still call themselves Many-Worlders or Copenhagenists or Bohmians or whatever—just as those far along in spiritual enlightenment might still call themselves Buddhists or Catholics or Muslims or Jews (or atheists or agnostics)—even though, by that point, they might have more in common with each other than they do with their supposed coreligionists or co-irreligionists.</p>



<p>Alright, but isn’t all this Zen stuff just a way to dodge the <em>actual, substantive</em> questions about QM, by cheaply claiming to have transcended them?  If that’s your charge, then please help yourself to the following FAQ about the details of the Zen Anti-Interpretation.</p>



<ol><li><strong>What is a quantum state?</strong>  It’s a unit vector of complex numbers (or if we’re talking about mixed states, then a trace-1, Hermitian, positive semidefinite matrix), which encodes everything there is to know about a physical system.<br /></li><li><strong>OK, but are the quantum states “ontic” (really out in the world), or “epistemic” (only in our heads)?</strong>  Dude.  Do “basketball games” really exist, or is that just a phrase we use to summarize our knowledge about certain large agglomerations of interacting quarks and leptons?  Do even the “quarks” and “leptons” exist, or are those just words for excitations of the more fundamental fields?  Does “jealousy” exist?  Pretty much<em> all</em> our concepts are complicated grab bags of “ontic” and “epistemic,” so it shouldn’t surprise us if quantum states are too.  Bad dichotomy.<br /></li><li><strong>Why are there probabilities in QM?</strong>  Because QM <em>is</em> a (the?) generalization of probability theory to involve complex numbers, whose squared absolute values are probabilities.  It <em>includes</em> probability as a special case.<br /></li><li><strong>But why do the probabilities obey the Born rule?</strong>  Because, once the unitary part of QM has picked out the 2-norm as being special, for the probabilities <em>also</em> to be governed by the 2-norm is pretty much the only possibility that makes mathematical sense; there are many nice theorems formalizing that intuition under reasonable assumptions.<br /></li><li><strong>What is an “observer”?</strong>  It’s exactly what modern decoherence theory says it is: a particular kind of quantum system that interacts with other quantum systems, becomes entangled with them, and thereby records information about them—reversibly in principle but irreversibly in practice.<br /></li><li><strong>Can observers be manipulated in coherent superposition, as in the <a href="https://en.wikipedia.org/wiki/Wigner%27s_friend">Wigner’s Friend</a> scenario?</strong>  If so, they’d be radically unlike any physical system we’ve ever had direct experience with.  So, are you asking whether such “observers” would be <em>conscious</em>, or if so what they’d be conscious of?  Who the hell knows?<br /></li><li><strong>Do “other” branches of the wavefunction—ones, for example, where my life took a different course—exist in the same sense this one does?</strong>  If you start with a quantum state for the early universe and then time-evolve it forward, then yes, you’ll get not only “our” branch but also a proliferation of other branches, in the overwhelming majority of which Donald Trump was never president and civilization didn’t grind to a halt because of a bat near Wuhan.  But how could we possibly know whether anything “breathes fire” into the other branches and makes them real, when we have no idea what breathes fire into <em>this</em> branch and makes <em>it</em> real?  This is not a dodge—it’s just that a simple “yes” or “no” would fail to do justice to the enormity of such a question, which is above the pay grade of physics as it currently exists. <br /></li><li><strong>Is this it?  Have you brought me to the end of the path of understanding QM?</strong>  No, I’ve just pointed the way toward the <em>beginning</em> of the path.  The most fundamental tenet of the Zen Anti-Interpretation is that there’s no shortcut to actually <a href="https://www.scottaaronson.com/qclec.pdf">working</a> <a href="https://www.amazon.com/Quantum-Mechanics-Theoretical-Leonard-Susskind/dp/0465062903/ref=asc_df_0465062903/?tag=hyprod-20&amp;linkCode=df0&amp;hvadid=312014159412&amp;hvpos=&amp;hvnetw=g&amp;hvrand=7852945785672685485&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9028280&amp;hvtargid=pla-435140302691&amp;psc=1">through</a> the Bell inequality, quantum teleportation, Shor’s algorithm, the Kochen-Specker and PBR theorems, possibly even a … <em>photon</em> or a <em>hydrogen atom</em>, so you can see quantum probability in action and be enlightened.  I’m further along the path than I was twenty years ago, but not as far along as some of my colleagues.  Even the greatest quantum Zen masters will be able to get further when new quantum phenomena and protocols are discovered in the future.  All the same, though—and this is another major teaching of the Zen Anti-Interpretation—there’s more to life than achieving greater and greater clarity about the foundations of QM.  And on that note…</li></ol>



<p>To those who asked me about Claus Peter Schnorr’s <a href="https://eprint.iacr.org/2021/232">claim</a> to have discovered a fast <em>classical</em> factoring algorithm, thereby “destroying” (in his words) the RSA cryptosystem, see (e.g.) <a href="https://twitter.com/inf_0_/status/1367376526300172288?fbclid=IwAR19Ip7XyoPjHfm9WBzqiUkQpxUVLGfVTgLGQmmncgrkUsvcLIrkzbOPw_U">this Twitter thread by Keegan Ryan</a>, which explains what certainly <em>looks</em> like a fatal error in Schnorr’s paper.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5359"><span class="datestr">at March 04, 2021 11:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2021-03-03-2-round-bft-smr-with-n-equals-4-f-equals-1/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2021-03-03-2-round-bft-smr-with-n-equals-4-f-equals-1/">2-round BFT SMR with n=4, f=1</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Guest post by Zhuolun Xiang In the previous post, we presented a summary of our good-case latency results for Byzantine broadcast and Byzantine fault tolerant state machine replication (BFT SMR), where the good case measures the latency to commit given that the leader/broadcaster is honest. In this post, we describe...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2021-03-03-2-round-bft-smr-with-n-equals-4-f-equals-1/"><span class="datestr">at March 03, 2021 11:37 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/030">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/030">TR21-030 |  Hardness of Constant-round Communication Complexity | 

	Rahul Ilango, 

	Shuichi Hirahara, 

	Bruno Loff</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
How difficult is it to compute the communication complexity of a two-argument total Boolean function $f:[N]\times[N]\to\{0,1\}$, when it is given as an $N\times N$ binary matrix? In 2009, Kushilevitz and Weinreb showed that this problem is cryptographically hard, but it is still open whether it is NP-hard. 

In this work, we show that it is NP-hard to approximate the size (number of leaves) of the smallest constant-round protocol for a two-argument total Boolean function $f:[N]\times[N]\to\{0,1\}$, when it is given as an $N\times N$ binary matrix. Along the way to proving this, we show a new *deterministic* variant of the round elimination lemma, which may be of independent interest.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/030"><span class="datestr">at March 02, 2021 09:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/03/02/faculty-at-universidad-catolica-de-chile-apply-by-april-10-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/03/02/faculty-at-universidad-catolica-de-chile-apply-by-april-10-2021/">Faculty at Universidad Católica de Chile (apply by April 10, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Institute for Mathematical and Computational Engineering at Universidad Católica de Chile offers one or more full-time positions. We invite applications from candidates in the areas of Data Science, Machine Learning, Optimization, Statistics and Stochastic, although other areas from Computational Science and Engineering, Optimization and Applied Mathematics will also be considered.</p>
<p>Website: <a href="http://imc.uc.cl/index.php/noticias/183-open-position-at-the-institute-for-mathematical-and-computational-engineering-uc">http://imc.uc.cl/index.php/noticias/183-open-position-at-the-institute-for-mathematical-and-computational-engineering-uc</a><br />
Email: pbarcelo@uc.cl</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/03/02/faculty-at-universidad-catolica-de-chile-apply-by-april-10-2021/"><span class="datestr">at March 02, 2021 03:50 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1842">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2021/03/02/automated-design-of-error-correcting-codes-part-1/">Automated Design of Error-Correcting Codes, Part 1</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><strong>Introduction. </strong>For nearly a century, error-correcting codes (ECCs) have been used for allowing communication even when the used communication channel is corrupted by noise. Beyond communication, error-correcting codes have found a variety of other uses, from<a href="https://en.wikipedia.org/wiki/Multiclass_classification"> multiclass learning</a> to even <a href="https://arxiv.org/pdf/1002.3864.pdf">showing hardness of approximation</a>. As such, understanding the rich world of error-correcting codes is essential for progress in all of these domains.</p>



<p>In our setting, imagine Alice wants to send a message to Bob of length <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" />, but the channel between them is corrupted by noise. To overcome this, Alice uses an <em>encoder</em> to turn her message into a longer, redundant string of length <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="n" class="latex" title="n" />. Then, Bob receives this transmission and uses a <em>decoder</em> to (hopefully) recover the original message of length <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" />. Two important properties are the <em>rate</em> <img src="https://s0.wp.com/latex.php?latex=k%2Fn&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k/n" class="latex" title="k/n" /> of the code (essentially what fraction of the transmission is “information”) and the <em>bit error rate (BER)</em> which is the (expected) number of decoding errors divided by <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" />. Desirable properties are to make the rate as large as possible and the BER as small as possible.</p>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img width="526" alt="" src="https://lh3.googleusercontent.com/9y96GkfFf6FBCUKoufmsqUGfXt7VmrDqAuCQI1IaOfy4DB-VmJWEvSwL9c1mj9QP9gVYq49haRBI96eNMx5qPpr3BFhhGuWvTv4wixNbLLuTuSnI3xv36xaVa64D3DvshMs_XwmT" height="353" /></figure></div>



<p class="has-text-align-center">Basic ECC paradigm.</p>



<p>Since the days of <a href="https://en.wikipedia.org/wiki/Claude_Shannon">Claude Shannon</a>, many error-correcting codes have been discovered, such as <a href="https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction">Reed-Solomon codes</a> and <a href="https://en.wikipedia.org/wiki/BCH_code">BCH codes</a>. Each error-correcting code has its own tradeoffs (e.g., some have higher rate, some are more resistant to special kinds of channel corruptions, etc.). With the large number of ECCs which have been discovered, it can sometimes be overwhelming what the proper error correcting code is for a given application. Further, if the application is sufficiently specialized there may be <em>no </em>known ECC which meets your needs. Such concerns motivate the <em>automation</em> <em>of error correcting codes</em>, which is the main topic of this blog post. </p>



<p>I’m using the word “automation” to cover a variety of tasks which various computational methods could assist with in the study of ECCs:</p>



<ol><li>Existence — Does the code I want even exist?</li><li>Encoding — What is the “best” way to convert my messages into a code?</li><li>Decoding — How do I recover from noisy transmissions?</li><li>Verification — Is the proposed ECC design provably correct?</li><li>Selection — Which ECC from a given class should I use for a given application?</li></ol>



<p>Each of these facets of the automation of ECCs is a whole field of research! In this and the subsequent post, I will discuss at a high level two types of techniques which have been used to approach these questions: “Formal Methods” and “Machine Learning.” We’ll cover formal methods in this post, and in the next post we will cover machine learning methods.</p>



<p><strong>Formal Methods. </strong>The field of Formal Methods strives to give <em>provable guarantees</em> for various computational questions by reducing them to formal logic. Although formal methods are mostly used for software and hardware verification (that is, making sure they are “bug free”), such tools are also used by mathematicians to show the validity of mathematical statements that would be difficult to prove by hand. For example, the <a href="https://en.wikipedia.org/wiki/Kepler%27s_conjecture">Kepler conjecture</a>, a question of what is the best way to pack spheres in three dimensions–essentially finding an optimal error-correcting code in Euclidean space–was only firmly proved by <a href="https://github.com/flyspeck/flyspeck">Thomas Hales and his team</a> through the use of automated theorem-proving tools.</p>



<p>A line of work for using formal methods to directly construct practical ECCs was initiated by <a href="https://ieeexplore.ieee.org/abstract/document/5699220">Shamshiri and Cheng</a> in 2010. In their work, they are motivated by designing error-correcting codes for static random-access memory (SRAM), the kind that is often used for CPU caches. When using SRAM (pictured below), a worry is that cosmic rays could hit some of the bits, causing them to flip. Further, it is not uncommon for a group of consecutive bits to flip. As such, it is desirable to esure that the error correcting code can correct either <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="g" class="latex" title="g" /> <em>global </em>errors or <img src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="l" class="latex" title="l" /> <em>local</em> errors. Correcting global errors is a common property of error correcting codes, such as BCH codes or the <a href="https://en.wikipedia.org/wiki/Binary_Golay_code">Golay code</a>. However, local error correction is a much less common property to guarantee. Thus, the authors use a <em>SAT solver</em> to construct error correcting codes with the properties they desire.</p>



<div class="wp-block-image"><figure class="aligncenter is-resized"><img width="529" alt="" src="https://lh4.googleusercontent.com/qQRzeu2F2XzrjAl6DyPPWAnBVOKOSODIBX4RPBuz0aAMqErYxC2ZyAbtWy3z8ORU4rvMT9UEMI8-C7boHeKEijFwrKY2pRaneEm1lsAoKBUHpQQ1rcMpaLBZs8zAwr2yviMGJPyG" height="318" /></figure></div>



<p class="has-text-align-center">Static random-access memory (source: <a href="https://en.wikipedia.org/wiki/File:Hyundai_RAM_HY6116AP-10.jpg" rel="prettyphoto">Wikipedia</a>)</p>



<p>Assuming that the code to be construction is linear (the encoding map is a linear function over the field <img src="https://s0.wp.com/latex.php?latex=mathbb+F_2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="mathbb F_2" class="latex" title="mathbb F_2" />), then the error correcting code can be described by a <a href="https://en.wikipedia.org/wiki/Parity-check_matrix">parity check matrix</a> M in <img src="https://s0.wp.com/latex.php?latex=%7B0%2C1%7D%5E%7B%28n-k%29+times+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0,1}^{(n-k) times n}" class="latex" title="{0,1}^{(n-k) times n}" />. The key observation the authors make is that for M to be a proper error correcting code, every error pattern <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> (i.e., vectors with hamming weight at most <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="g" class="latex" title="g" /> or consecutive errors in a block of length <img src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="l" class="latex" title="l" />) must have <img src="https://s0.wp.com/latex.php?latex=Mp&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="Mp" class="latex" title="Mp" /> be a distinct vector. For instance  <img src="https://s0.wp.com/latex.php?latex=M%280%2C+1%2C+0%2C+0%2C+1%29+neq+M%281%2C1%2C1%2C0%2C0%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="M(0, 1, 0, 0, 1) neq M(1,1,1,0,0)" class="latex" title="M(0, 1, 0, 0, 1) neq M(1,1,1,0,0)" /> if <img src="https://s0.wp.com/latex.php?latex=g+%3D+2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="g = 2" class="latex" title="g = 2" /> and <img src="https://s0.wp.com/latex.php?latex=l+%3D+3&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="l = 3" class="latex" title="l = 3" />.</p>



<p>These Boolean constraints can be expressed in conjunctive normal form, i.e., a SAT instance. As such a SAT-solver can be used to determine if there exists a matrix M with the given properties for a given k and n. For instance, they are able to find an error correcting code with parameters <img src="https://s0.wp.com/latex.php?latex=k+%3D+16%2C+n+%3D+26%2C+g+%3D+2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="k = 16, n = 26, g = 2" class="latex" title="k = 16, n = 26, g = 2" /> and <img src="https://s0.wp.com/latex.php?latex=l+%3D+4&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="l = 4" class="latex" title="l = 4" />. In their follow-up work [<a href="https://ieeexplore.ieee.org/abstract/document/6139156">Shamshi, Ghofrani, Cheng, 2011]</a>, they use this error-correcting code for modeling an “on-chip network” between CPU cores in a multi-core processor<strong>.</strong></p>



<p>Another line of work led by Ben Curtis (see the <a href="https://cs.uwaterloo.ca/~cbright/reports/cacm-preprint.pdf">survey by Curtis, Kotsireas, and Ganesh</a>) has been seeking to construct ECC-like combinatorial objects. An example of such an object is a Hadamard matrix: a square matrix with <img src="https://s0.wp.com/latex.php?latex=%5Cpm+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\pm 1" class="latex" title="\pm 1" /> entries such that every row and column is orthogonal in <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\mathbb R^n" class="latex" title="\mathbb R^n" />. In fact, the authors search for a special type of Hadamard matrix made up of a quartet Williamson matrices which have an intricate algebraic structure. They find these objects by using an algorithm which goes back-and-forth between a SAT solver with a CAS (computer algebraic system) to help narrow the search space. </p>



<p>Formal Methods have further applications in error-correcting codes for <a href="https://ieeexplore.ieee.org/abstract/document/6649704">distributed cloud storage</a> and <a href="https://arxiv.org/pdf/1804.02317.pdf">value-deviation-bounded codes</a>.</p>



<p>This concludes our first post. In the next post, we will cover machine learning methods. </p>



<p>Are you aware of other examples or applications of automation to error-correcting codes? If so, please leave a comment.</p>



<p><strong>Acknowledgments. </strong>I would like to thank my quals committee, Aviad Rubinstein, Moses Charikar, and Mary Wootters for valuable feedback. </p></div>







<p class="date">
by jbrakensiek <a href="https://theorydish.blog/2021/03/02/automated-design-of-error-correcting-codes-part-1/"><span class="datestr">at March 02, 2021 03:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/03/01/associate-professor-at-kth-royal-institute-of-technology-apply-by-april-15-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/03/01/associate-professor-at-kth-royal-institute-of-technology-apply-by-april-15-2021/">Associate professor at KTH Royal Institute of Technology (apply by April 15, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>KTH Royal Institute of Technology, School of Electrical Engineering and Computer Science has a vacancy for one Associate Professor with specialization in Foundations of Data Science. The position will be permanent and full time, to start as soon as possible.</p>
<p>Website: <a href="https://www.kth.se/en/om/work-at-kth/lediga-jobb/what:job/jobID:366029/where:4/">https://www.kth.se/en/om/work-at-kth/lediga-jobb/what:job/jobID:366029/where:4/</a><br />
Email: tenuretrack@eecs.kth.se</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/03/01/associate-professor-at-kth-royal-institute-of-technology-apply-by-april-15-2021/"><span class="datestr">at March 01, 2021 09:54 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/029">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/029">TR21-029 |  Public-Coin Statistical Zero-Knowledge Batch Verification against Malicious Verifiers | 

	Inbar Kaslasi, 

	Ron Rothblum, 

	Prashant Nalini Vasudevan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Suppose that a problem $\Pi$ has a statistical zero-knowledge (SZK) proof with communication complexity $m$. The question of batch verification for SZK asks whether one can prove that $k$ instances $x_1,\ldots,x_k$ all belong to $\Pi$ with a statistical zero-knowledge proof whose communication complexity is better than $k \cdot m$ (which is the complexity of the trivial solution of executing the original protocol independently on each input).

In a recent work, Kaslasi et al. (TCC, 2020) constructed such a batch verification protocol for any problem having a non-interactive SZK (NISZK) proof-system. Two drawbacks of their result are that their protocol is private-coin and is only zero-knowledge with respect to the honest verifier.

In this work, we eliminate these two drawbacks by constructing a public-coin malicious-verifier SZK protocol for batch verification of NISZK. Similarly to the aforementioned prior work, the communication complexity of our protocol is $\big(k+poly(m) \big) \cdot polylog(k,m)$</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/029"><span class="datestr">at March 01, 2021 04:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2021/03/01/beyondlogconcave2/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2021/03/01/beyondlogconcave2/">Beyond log-concave sampling (Part 2)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In our previous <a href="http://www.offconvex.org/2020/09/19/beyondlogconvavesampling">blog post</a>, we introduced the challenges of sampling distributions beyond log-concavity. 
We first introduced the problem of sampling from a distibution $p(x) \propto e^{-f(x)}$ given value or gradient oracle access to $f$, as an analogous problem to black-box optimization with oracle access. We introduced the natural algorithm for sampling in this setup: Langevin Monte Carlo, a Markov Chain reminiscent of noisy gradient descent,</p>

\[x_{t+\eta} = x_t - \eta \nabla f(x_t) + \sqrt{2\eta}\xi_t,\quad \xi_t\sim N(0,I).\]

<p>Finally, we laid out the challenges when $f$ is not convex; in particular, LMC can suffer from slow mixing.</p>

<p>In this and the coming post, we describe two of our recent works tackling this problem. We identify two kinds of structure beyond log-concavity under which we can design provably efficient algorithms:  <em>multi-modality</em> and <em>manifold structure in the level sets</em>. These structures commonly occur in practice, especially in problems involving statistical inference and posterior sampling in generative models.</p>

<p>In this post, we will focus on multimodality, covered by the paper <a href="https://arxiv.org/abs/1812.00793">Simulated tempering Langevin Monte Carlo</a> by Rong Ge, Holden Lee, and Andrej Risteski.</p>

<h1 id="sampling-multimodal-distributions-with-simulated-tempering">Sampling multimodal distributions with simulated tempering</h1>

<p>The classical scenario in which Langevin takes exponentially long to mix is when $p$ is a mixture of two well-separated gaussians. In broadest generality, this was considered by <a href="http://www.ems-ph.org/journals/show_abstract.php?issn=1435-9855%20&amp;vol=6&amp;iss=4&amp;rank=1">Bovier et al. 2004</a> who used tools from metastable processes to show that transitioning from one peak to another can take exponential time. Roughly speaking, they show the transition time is proportional to the “energy barrier” a particle has to cross. If the gaussians have unit variance and means at distance $2r$, then the probability density at a point midway in between is $\propto e^{-r^2/2}$, and this energy barrier is $\propto e^{r^2/2}$. Thus, the mixing time is exponential. Qualitatively, the intuition for this phenomenon is simple to describe: if started at point A, the drift (i.e. gradient) term will push the walk towards A, so long as it’s close to the basin around A; hence, to transition from A to B (through C) the Gaussian noise must persistenly counteract the gradient term.</p>

<center>
<img width="500" src="http://www.andrew.cmu.edu/user/aristesk/animation_bovier.gif" />
</center>

<p>Hence Langevin on its own will not work even in very simple multimodal settings.</p>

<p>In <a href="https://arxiv.org/abs/1812.00793">our paper</a>, we show that combining Langevin Monte Carlo with a temperature-based heuristic called <em>simulated tempering</em> can significantly speed up mixing for multimodal distributions, where the number of modes is not too large, and the modes “look similar.”</p>

<p>More precisely, we show:</p>

<blockquote>
  <p><strong>Theorem (Ge, Lee, Risteski ‘18, informal)</strong>: If $p(x)$ is a mixture of $k$ shifts of a strongly log-concave distribution in $d$ dimensions (e.g. Gaussian), an algorithm based on simulated tempering and Langevin Monte Carlo that runs in time poly($d,k, 1/\varepsilon$) produces samples from a distribution $\varepsilon$-close to $p$ in total variation distance.</p>
</blockquote>

<p>The main idea is to create a meta-Markov chain (the simulated tempering chain) which has two types of moves: change the current “temperature” of the sample, or move “within” a temperature. The main intuition behind this is that at higher temperatures, the distribution is flatter, so the chain explores the landscape faster (see the figure below).</p>

<center> 
<img src="http://www.andrew.cmu.edu/user/aristesk/animation_tempering.gif" />
</center>

<p>More formally, the distribution at inverse temperature $\beta$ is given by $p_\beta(x) \propto e^{-\beta f(x)}$. The Langevin chain which corresponds to $\beta$ is given by</p>

\[x_{t+\eta} = x_t - \eta \beta \nabla f(x_t) + \sqrt{2\eta}\xi_t,\quad \xi_t\sim N(0,I).\]

<p>As in the figure above, a high temperature (low $\beta&lt;1$) flattens out the distribution and causes the chain to mix faster (top distribution in figure). However, we can’t merely run Langevin at a higher temperature, because the stationary distribution of the high-temperature chain is wrong: it’s $p_\beta(x)$. The idea behind simulated tempering is to run Langevin chains at different temperatures, sometimes swapping to another temperature to help lower-temperature chains explore. To maintain the right stationary distributions at each temperature, we use a Metropolis-Hastings filtering step.</p>

<p>More formally, choosing a suitable sequence $0&lt; \beta_1&lt; \cdots &lt;\beta_L=1$, we define the simulated tempering chain as follows.</p>

<p><img width="300" style="float: right;" src="http://holdenlee.github.io/pics/stl.png" /></p>

<ul>
  <li>The <em>state space</em> is a pair of a temperature and location in space $(i, x), i \in [L], x \in \mathbb{R}^d$.<br />
</li>
  <li>The <em>transitions</em> are defined as follows.
    <ul>
      <li>If the current point is $(i,x)$, then <em>evolve</em> $x$ according to Langevin diffusion with inverse temperature $\beta_i$.</li>
      <li>Propose swaps with some rate $\lambda &gt;0$. Proposing a swap means attempting to move to a neighboring chain, i.e. change $i$ to $i’=i\pm 1$. With probability $\min{p_{i’}(x)/p_i(x), 1}$, the transition is accepted. Otherwise, stay at the same point. This is a <em>Metropolis-Hastings step</em>; its purpose is to preserve the stationary distribution.</li>
    </ul>
  </li>
</ul>

<p>Finally, it’s not too hard to see that at the stationary distribution, the samples at the $L$th level ($\beta_L=1$) are the desired samples.</p>

<h2 id="proof-idea-decomposition-theorem">Proof idea: decomposition theorem</h2>

<p>The main strategy is inspired by Madras and Randall’s <a href="https://www.jstor.org/stable/2699896">Markov chain decomposition theorem</a>, which gives a criterion for a Markov chain to mix rapidly: partition the state space into sets, and show that</p>

<ol>
  <li>The Markov chain mixes rapidly when restricted to each set of the partition.</li>
  <li>The <em>projected</em> Markov Chain, which we define momentarily, mixes rapidly. If there are $m$ sets, the projected chain $\overline M$ is defined on the state space ${1,\ldots, m}$, and transition probabilities are given by average probability flows between the corresponding sets.</li>
</ol>

<p>To implement this strategy, we first have to specify the partition. In fact, we roughly show that there is a partition of $[L] \times \mathbb{R}^d$ in which:</p>

<ol>
  <li>The simulated tempering Langevin chain mixes fast within each of the sets.</li>
  <li>The “volume” of the sets (under the stationary distribution of the tempering chain) is not too small.
</li>
</ol>

<p>In applying the Madras-Randall framework with this partition, it’s clear that point (1) above satisfies requirement (1) for the framework; point (2) ensures that the projected Markov chain has no “bottlenecks” and hence that it mixes rapidly (requirement (2)). More precisely, we can show rapid mixing either through the method of canonical paths or Cheeger’s inequality. To do this, we exhibit a “good-probability” path between any two sets in the partition, going through the highest temperature.</p>

<p>The intuition for why this path works is illustrated in the figure below: when transitioning from the set corresponding to the left mode at level $L$ to the right mode at level $L$, each of the steps up/down the temperatures are accepted with good probability if the neighboring temperatures are not too different; at the highest temperature, the chain mixes fast by point (1), and since each of the sets are not too small by point (2), there is a reasonable probability to end at the right mode at the highest temperature.</p>

<center>
<img src="http://www.andrew.cmu.edu/user/aristesk/animation_conductance.gif" />
</center>



<p>Intuitively, the partition should track the “modes” of the distribution, but a technical hurdle in implementing this plan is in defining the partition when the modes overlap. One can either do this spectrally (i.e. showing that the Langevin chain has a spectral gap, and use theorems about <a href="https://arxiv.org/abs/1309.3223">spectral graph partitioning</a>, as we did in the <a href="https://arxiv.org/abs/1710.02736">first version</a> of the paper), or use a functional “soft decomposition theorem” which is a more flexible version of the classical decomposition theorem, which we use in a <a href="https://arxiv.org/abs/1812.00793">later version</a> of the paper.</p></div>







<p class="date">
<a href="http://offconvex.github.io/2021/03/01/beyondlogconcave2/"><span class="datestr">at March 01, 2021 02:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/03/01/phd-thesis-at-lamsade-paris-dauphine-apply-by-april-30-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/03/01/phd-thesis-at-lamsade-paris-dauphine-apply-by-april-30-2021/">PhD. Thesis at LAMSADE (Paris Dauphine) (apply by April 30, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>PhD. Thesis offer in Paris Dauphine University “Algorithmic aspects of intersection graphs”</p>
<p>Website: <a href="https://www.lamsade.dauphine.fr/fileadmin/mediatheque/lamsade/documents/propositions_theses_2020/murat.pdf">https://www.lamsade.dauphine.fr/fileadmin/mediatheque/lamsade/documents/propositions_theses_2020/murat.pdf</a><br />
Email: florian.sikora@dauphine.fr</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/03/01/phd-thesis-at-lamsade-paris-dauphine-apply-by-april-30-2021/"><span class="datestr">at March 01, 2021 10:35 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2021-02-28-good-case-latency-of-byzantine-broadcast-a-complete-categorization/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2021-02-28-good-case-latency-of-byzantine-broadcast-a-complete-categorization/">Good-case Latency of Byzantine Broadcast: a Complete Categorization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Guest post by Zhuolun Xiang State Machine Replication and Broadcast Many existing permission blockchains are built using Byzantine fault-tolerant state machine replication (BFT SMR), which ensures all honest replicas agree on the same sequence of client inputs. Most of the practical solutions for BFT SMR are based on the Primary-Backup...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2021-02-28-good-case-latency-of-byzantine-broadcast-a-complete-categorization/"><span class="datestr">at February 28, 2021 06:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-5663884325046890461">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/02/using-number-of-phds-as-measure-of.html">Using number-of-PhD's as a measure of smartness is stupid.</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In <i>Thor:Ragnorak</i> Bruce Banner mentions that he has 7 PhDs. Gee, I wonder how he managed to slip that into a conversation casually.  Later in the movie:</p><p><br /></p><p>Bruce: I don't know how to fly one of those (it an Alien Spacecraft)</p><p>Thor: You're a scientist. Use one of your PhD's </p><p>Bruce: None of them are for flying alien spaceships.</p><p><br /></p><p>On the episode <i>Double Date </i>of Archer (Season 11, Episode 6) Gabrielle notes that she has 2 PhD's whereas Lana only has 1 PhD. </p><p><br /></p><p>I am sure there are other examples of a work of fiction using <i>number of PhDs </i>as a way to say that someone is smart. In reality the number of PhD's one has is... not really a thing. </p><p>In reality if a scientist wants to do work in another field they... do work in that field.</p><p>Godel did research in Physics in the 1950's, but it would have been silly to go back and get a PhD in it.</p><p>Fortnow did research in Economics, but it would have been silly to go back and get a PhD in it. </p><p>Amy Farrah Fowler worked in neurobiology and then in Physics. Her Nobel prize in physics (with Sheldon Cooper) is impressive, getting a PhD in Physics would be ... odd. Imagine someone looking at here resume: <i>She has a Nobel Prize in Physics, but does she have a PhD? Did she pass her qualifying</i> <i>exams?</i>  This is the flip side of what I mentioned in a prior post about PhD's: <i>Not only does Dr. Doom want to take over the world, but his PhD is from The University of Latveria, which is not accredited. </i></p><p>There are other examples.</p><p>There ARE some people who get two PhDs for reasons of job market or other such things. That's absolutely fine of course. However, I wonder if in the real world they brag about it. I doubt it. </p><p>Is there anyone who has 3 PhDs? I would assume yes, but again, I wonder if they brag about it. Or should. </p><p>WHY do TV and movies use number-of-PhDs as a sign of genius? I do not know- especially since there are BETTER ways say someone is a genius in a way the audience can understand:  number-of-Nobel-prizes, number-of-times-mentioned-in-complexityblog,  number of Dundie's (see <a href="https://theoffice.fandom.com/wiki/Dundie">here</a>), etc. </p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p><p><br /></p></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/02/using-number-of-phds-as-measure-of.html"><span class="datestr">at February 28, 2021 05:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2021/02/28/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2021/02/28/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="http://acm-stoc.org/stoc2021/accepted-papers.html">STOC 2021 accepted papers</a> (<a href="https://mathstodon.xyz/@11011110/105748480557219533">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://randomascii.wordpress.com/2021/02/16/arranging-invisible-icons-in-quadratic-time/">Arranging invisible icons in quadratic time</a> (<a href="https://mathstodon.xyz/@11011110/105756917532905626">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=26152335">via</a>). Yet another instance where using a too-slow algorithm causes a UI hang, with the twist that the better solution would not be to replace it with a faster algorithm, but instead to not do the useless thing that the bad algorithm does at all.</p>
  </li>
  <li>
    <p><a href="https://joshdata.me/iceberger.html">Fun with shapes: draw an iceberg and see which way up and how deep it would float</a> (<a href="https://mathstodon.xyz/@11011110/105768276511155377">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=26201160">via</a>, <a href="https://www.metafilter.com/190533/Iceberger">via2</a>, <a href="https://boingboing.net/2021/02/20/make-your-own-iceberg-with-iceberger.html">via3</a>). Inspired by <a href="https://mobile.twitter.com/GlacialMeg/status/1362557149147058178">a twitter thread by Megan Thompson-Munson</a> pointing out that many supposed photos or illustrations of icebergs are fake and wrong.</p>
  </li>
  <li>
    <p>Draw an infinite subgraph of the 3d integer lattice in which each vertex has four co-planar neighbors, in a perpendicular plane to each of its neighbors (<a href="https://mathstodon.xyz/@11011110/105771494222747316">\(\mathbb{M}\)</a>). This completely determines the subgraph, which is 4-regular and highly symmetric. It is the graph of adjacencies of the cubes in the <a href="https://en.wikipedia.org/wiki/Tetrastix">tetrastix structure</a>. Does this graph have a name and history?</p>
  </li>
  <li>
    <p><a href="https://cacm.acm.org/magazines/2021/3/250708-gender-trends-in-computer-science-authorship">Gender trends in computer science authorship</a> (<a href="https://mathstodon.xyz/@11011110/105781841287243050">\(\mathbb{M}\)</a>). Takeaways for me (mostly from the barely-readable Fig. 4) are:</p>

    <ul>
      <li>
        <p>Roughly one in four coauthors of CS research publications are currently female, up from a big dip of one in seven in the 1970s to 1990s.</p>
      </li>
      <li>
        <p>Mathematics started lower and is currently more or less the same.</p>
      </li>
      <li>
        <p>We are not on track to gender parity.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>I’m sad that the only way to find a viewable version of the 1991 short film <em><a href="https://en.wikipedia.org/wiki/Not_Knot">Not Knot</a></em> (on the hyperbolic geometry of knot complements) seems to be through pirate copies (<a href="https://mathstodon.xyz/@11011110/105785401264334824">\(\mathbb{M}\)</a>). Or you could pay $45 to Amazon for a copy on DVD. Do most people still have DVD players? At least they’re not still trying to sell it on VHS only.</p>
  </li>
  <li>
    <p><a href="https://cscresearchblog.wordpress.com/2018/11/16/karp-sipser-heuristic-and-reductions/">On the slow spread of knowledge of nice theorems</a> (<a href="https://mathstodon.xyz/@11011110/105793165233864617">\(\mathbb{M}\)</a>), an amusing cartoon at the end of a longer blog post on fast graph matching heuristics.</p>
  </li>
  <li>
    <p>Today’s LaTeX formatting tip (<a href="https://mathstodon.xyz/@11011110/105796107362586793">\(\mathbb{M}\)</a>): You know that bug where amsthm + hyperref, with one numbering for theorems and lemmas and corollaries and whatever, causes <code class="language-plaintext highlighter-rouge">\autoref</code> to call them theorems even when they’re really lemmas and corollaries and whatever? If you don’t, you’re lucky. Anyway, there’s a very simple workaround: after loading amsthm and hyperref, add one more package:</p>

    <p><code class="language-plaintext highlighter-rouge">\usepackage[capitalize,nameinlink]{cleveref}</code></p>

    <p>Then, just use <code class="language-plaintext highlighter-rouge">\cref</code> everywhere you were using <code class="language-plaintext highlighter-rouge">\autoref</code>. Problem solved!</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Lloyd%27s_algorithm">Lloyd’s algorithm</a> animated for 3d points (<a href="https://mathstodon.xyz/@tpfto/105553548210257285">\(\mathbb{M}\)</a>). See also <a href="https://mathstodon.xyz/@tpfto/105803635782297523">the spherical version</a>.</p>
  </li>
  <li>
    <p><a href="https://rjlipton.wordpress.com/2021/02/27/new-old-ancient-results/">Applications of the no-3-in-line problem and cap-sets to complexity theory</a> (<a href="https://mathstodon.xyz/@11011110/105807834096788492">\(\mathbb{M}\)</a>). “What is most curious to us is that for matrix multiplication, the cap-set related technique frustrates a better complexity upper bound, whereas [for linear algebraic circuits] it frustrates a better lower bound.”</p>
  </li>
  <li>
    <p><a href="https://www.bldgblog.com/2013/08/tensioned-suspension/">Tensioned suspension</a> (<a href="https://mathstodon.xyz/@11011110/105811049795181041">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=9093187">via</a>): sculptures by Dan Grayber in which the weight of mechanical linkages causes them to push out against the sides of their glass enclosures, seemingly causing them to hang suspended in air. More at <a href="http://www.dangrayber.com/">Grayber’s web site</a>.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2021/02/28/linkage.html"><span class="datestr">at February 28, 2021 04:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=18225">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2021/02/27/new-old-ancient-results/">New, Old, Ancient Results</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Nonexistence theorems and attempts at lower bounds</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2021/02/jgcropped.jpg"><img width="141" alt="" src="https://rjlipton.files.wordpress.com/2021/02/jgcropped.jpg?w=141&amp;h=168" class="alignright wp-image-18228" height="168" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from <a href="https://home.cs.colorado.edu/~jgrochow/">src</a></font></td>
</tr>
</tbody>
</table>
<p>
Joshua Grochow is an assistant professor in Computer Science and Mathematics at the University of Colorado at Boulder. He was a student of Ketan Mulmuley and Lance Fortnow at Chicago; his <a href="https://home.cs.colorado.edu/~jgrochow/grochow-thesis.pdf">dissertation</a> and <a href="https://arxiv.org/pdf/1304.6333.pdf">some</a> <a href="https://arxiv.org/pdf/1112.2012.pdf">subsequent</a> <a href="https://arxiv.org/pdf/1605.02815.pdf">papers</a> did much to widen the horizons of the “Geometric Complexity Theory” (GCT) program. He is also a gifted expositor.</p>
<p>
Today we will highlight some of his work and some of his exposition of new and old theorems.<br />
<span id="more-18225"></span></p>
<p>
An ancient one is Stephen Mahaney’s famous theorem on the nonexistence of sparse <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP}}" class="latex" />-complete sets (unless <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP+%3D+P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathsf{NP = P}}" class="latex" />). Grochow <a href="https://arxiv.org/pdf/1610.05825.pdf">discusses</a> a simpler proof of the theorem by Manindra Agrawal and gives some further impacts on GCT. </p>
<p>
A recent <a href="https://drops.dagstuhl.de/opus/volltexte/2021/13570/pdf/LIPIcs-ITCS-2021-31.pdf">one</a> with Youming Qiao is on an old topic and is in the 2021 Innovations in Theoretical Computer Science conference. It is titled, “On the Complexity of Isomorphism Problems for Tensors, Groups, and Polynomials I: Tensor Isomorphism-Completeness,” and grows out of a 2019 <a href="https://arxiv.org/abs/1907.00309">paper</a> by the same authors. </p>
<p>
This came to my attention through communications with Grochow’s <a href="https://michaellevet.github.io">student</a>, Michael Levet. Indeed, Levet is the reason for my putting this all together. He raised through email some questions about an ancient result of mine on group isomorphism. I reported <a href="https://rjlipton.wordpress.com/2013/05/11/advances-on-group-isomorphism/">previously</a>:</p>
<blockquote><p><b> </b> <em> Long ago Bob Tarjan and Zeke Zalcstein and I made a simple observation: Group isomorphism could be done in time 	</em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++n%5E%7B%5Clog_%7B2%7D+n+%2B+O%281%29%7D.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  n^{\log_{2} n + O(1)}. " class="latex" /></p>
</em><p><em>This relies on the easy-to-prove fact that every group has at most <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog_%7B2%7D+n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\log_{2} n}" class="latex" /> generators. We have discussed this idea earlier <a href="https://rjlipton.wordpress.com/2011/10/08/an-annoying-open-problem/">here</a>. </em>
</p></blockquote>
<p>
Levet raised an issue about related observations of mine—ones that were misleading at best. I think he has a good point and we are still trying to unravel exactly what I meant back then. I applaud him for reading ancient stuff, for trying to extend it, and for working on such problems. I wish him well.</p>
<p>
</p><h2> No Three In a Row </h2><p></p>
<p>While Levet and I work that out and think about Grochow’s paper on isomorphism problems with Qiao, Ken and I want to highlight a different expository <a href="https://www.ams.org/journals/bull/2019-56-01/S0273-0979-2018-01648-0/S0273-0979-2018-01648-0.pdf">paper</a> by Grochow on news from 2016 that we <a href="https://rjlipton.wordpress.com/2016/06/15/polynomial-prestidigitation/">covered</a> then. Grochow’s paper appeared in the <em>AMS Bulletin</em> and is titled, “New Applications Of The Polynomial Method: The Cap Set Conjecture And Beyond.” </p>
<p>
To lead in to the subject, here is a <a href="https://archive.org/stream/amusementsinmath00dude#page/94/mode/2up">problem</a> from 1917 by the English puzzlemaster Henry Dudeney titled, “A Puzzle With Pawns”:</p>
<blockquote><p><b> </b> <em> Place two pawns in the middle of the chess- board, one at Q 4 and the other at K 5. Now, place the remaining fourteen pawns (sixteen in all) so that no three shall be in a straight line in any possible direction. Note that I purposely do not say queens, because by the words ” any possible direction ” I go beyond attacks on diagonals. The pawns must be regarded as mere points in space — at the centres of the squares. </em>
</p></blockquote>
<p>
Sixteen is obviously the maximum possible for a standard <img src="https://s0.wp.com/latex.php?latex=%7B8+%5Ctimes+8%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{8 \times 8}" class="latex" /> chessboard because a seventeenth pawn would make three in some row and some column. For an <img src="https://s0.wp.com/latex.php?latex=%7Br+%5Ctimes+r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r \times r}" class="latex" /> board, the limit is <img src="https://s0.wp.com/latex.php?latex=%7B2r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2r}" class="latex" /> by similar reasoning—this is an example of the <em>pigeonhole principle</em> which we just <a href="https://rjlipton.wordpress.com/2021/02/15/pigenhole-principle/">mentioned</a>. </p>
<p>
It is possible to achieve the maximum for all <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r}" class="latex" /> up to <img src="https://s0.wp.com/latex.php?latex=%7B46%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{46}" class="latex" /> and then the only other cases known are <img src="https://s0.wp.com/latex.php?latex=%7B48%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{48}" class="latex" />, <img src="https://s0.wp.com/latex.php?latex=%7B50%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{50}" class="latex" />, and <img src="https://s0.wp.com/latex.php?latex=%7B52%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{52}" class="latex" />. That’s it. Here are solutions for <img src="https://s0.wp.com/latex.php?latex=%7Br+%3D+10%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r = 10}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Br+%3D+52%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r = 52}" class="latex" />. The latter was found by Achim Flammenkamp, whose <a href="http://wwwhomes.uni-bielefeld.de/achim/no3in/readme.html">page</a> has encyclopedic information. On the former, the pieces are positioned on gridpoints like stones in Go, which seems a better context for this problem than chess. </p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2021/02/3inarow20and52.jpg"><img width="550" alt="" src="https://rjlipton.files.wordpress.com/2021/02/3inarow20and52.jpg?w=550&amp;h=257" class="aligncenter wp-image-18230" height="257" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite of <a href="https://en.wikipedia.org/wiki/No-three-in-line_problem">src1</a>, <a href="https://mathworld.wolfram.com/No-Three-in-a-Line-Problem.html">src2</a></font>
</td>
</tr>
</tbody></table>
<p>
The conjecture is not only that a <img src="https://s0.wp.com/latex.php?latex=%7B2n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2n}" class="latex" />-size solution exists for only finitely many <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />, but also that the maximum size for all sufficiently large <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r}" class="latex" /> is bounded by <img src="https://s0.wp.com/latex.php?latex=%7Bcr%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{cr}" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=%7Bc+%3C+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c &lt; 2}" class="latex" />, indeed, with <img src="https://s0.wp.com/latex.php?latex=%7Bc+%3C+1.815%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c &lt; 1.815}" class="latex" />. It is known that <img src="https://s0.wp.com/latex.php?latex=%7B%281.5-%5Cepsilon_r%29r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(1.5-\epsilon_r)r}" class="latex" /> stones can always be placed with no three collinear, where the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_r%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon_r}" class="latex" /> depends on the closeness of a prime to <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7Br%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\frac{r}{2}}" class="latex" />. </p>
<p>
The problem can be taken to dimensions <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cge+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n \ge 3}" class="latex" /> that are beyond the plane. We can also extend what is meant by a “line” via various notions of wrapping-around. Then the question is how close the maximum size can stay to being linear in the size of the space—as <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> and/or the size <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r}" class="latex" /> of an individual dimension increase.</p>
<p>
</p><h2> Not As Easy As Tic-Tac-Toe </h2><p></p>
<p>
The theme of the no-three-in-a-line problem is fundamental to combinatorics. There are tons of problems of the form: </p>
<blockquote><p><b> </b> <em> How many objects can one place, so that no pattern of some certain type exists? </em>
</p></blockquote>
<p>
In <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />-dimensional space the smallest <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r}" class="latex" /> of interest is <img src="https://s0.wp.com/latex.php?latex=%7Br+%3D+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r = 3}" class="latex" />. This means playing on higher-dimensional versions of the <img src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3 \times 3}" class="latex" /> grid and <img src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3 \times 3 \times 3}" class="latex" /> cube. Then the only Euclidean lines are the kind we know from tic-tac-toe: straight across or down, or diagonal. </p>
<p>
For dimension <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cgeq+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n \geq 3}" class="latex" /> there are other kinds of diagonals, such as within a face or through the center of the cube, but they all win at <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />-dimensional tic-tac-toe. So the problem becomes: what is the maximum number of moves you can make by yourself without creating a win at tic-tac-toe? The <em>cap-set problem</em> adds a twist by extending the notion of what is a <em>line</em>. It is like playing tic-tac-toe on a floor of <img src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3 \times 3}" class="latex" /> tiles where a play in one tile is replicated in all of them. Then you can make a line by playing in a corner and in the middle of the two opposite edges, as shown at left in the following diagram (original drawing).</p>
<p>
<a href="https://rjlipton.files.wordpress.com/2021/02/lineandcapset3x3.png"><img width="550" alt="" src="https://rjlipton.files.wordpress.com/2021/02/lineandcapset3x3.png?w=550&amp;h=222" class="aligncenter wp-image-18231" height="222" /></a></p>
<p>
The four orange O’s at right have no 3-in-a-line even with this extended notion of line. Note that the four blank cells in the top two rows also avoid putting 3 in a line. Four is the maximum, however: it is not possible to have a drawn game in extended <img src="https://s0.wp.com/latex.php?latex=%7B3+%5Ctimes+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3 \times 3}" class="latex" /> tic-tac-toe. </p>
<p>
The theorem <a href="https://arxiv.org/pdf/1605.09223v1.pdf">proved</a> by Jordan Ellenberg and Dion Gijswijt in 2016 is that the upper bound is not only a vanishing fraction of the size <img src="https://s0.wp.com/latex.php?latex=%7B3%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3^n}" class="latex" /> of the space as <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> grows, it is bounded by <img src="https://s0.wp.com/latex.php?latex=%7Bc%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c^n}" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=%7Bc+%3C+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c &lt; 3}" class="latex" />. Namely:</p>
<blockquote><p><b>Theorem 1</b> <em> Every cap set in the <img src="https://s0.wp.com/latex.php?latex=%7B3%5En%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3^n}" class="latex" />-cube has size at most <img src="https://s0.wp.com/latex.php?latex=%7B2.756%5En%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2.756^n}" class="latex" />. </em>
</p></blockquote>
<p>
</p><h2> Using Polynomials </h2><p></p>
<p>There is a simple way to express the extended notion of “line” that works for all dimensions <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />: Number the coordinates of each dimension <img src="https://s0.wp.com/latex.php?latex=%7B0%2C1%2C2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{0,1,2}" class="latex" />. Make the space <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%2C2%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{0,1,2\}^n}" class="latex" /> with addition modulo <img src="https://s0.wp.com/latex.php?latex=%7Bq+%3D+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{q = 3}" class="latex" />, that is, make it <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_3%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbb{Z}_3^n}" class="latex" />. Then the condition for three points <img src="https://s0.wp.com/latex.php?latex=%7BA%2CB%2CC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A,B,C}" class="latex" /> to be in a line is simply </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A+%2B+B+%3D+2C.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  A + B = 2C. " class="latex" /></p>
<p>It is easy to write polynomial equations over the field <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbb{F}_3}" class="latex" /> to express the property of a set having such a line. What was unexpected, until Ernie Croot, Vsevolod Lev, and Péter Pál Pach solved a related problem with <img src="https://s0.wp.com/latex.php?latex=%7Bq+%3D+4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{q = 4}" class="latex" />, was that there would be</p>
<blockquote><p><b> </b> <em> “an ingeniously simple way to split the polynomial[s] into pieces with smaller exponents, which led to a bound on the size of collections with no [lines].” </em>
</p></blockquote>
<p>
The quotation comes from an <a href="https://www.quantamagazine.org/set-proof-stuns-mathematicians-20160531">article</a> by Erica Klarreich for <em>Quanta</em> right then in 2016. A 2016 AMS Feature <a href="http://www.ams.org/publicoutreach/feature-column/fc-2016-08">column</a> by David Austin covers how to make this say a set <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> of points is a cap set modulo 3: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S+%5Cuplus+S+%5Ccap+2S+%3D+%5Cemptyset%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  S \uplus S \cap 2S = \emptyset, " class="latex" /></p>
<p>where we (not Austin) write <img src="https://s0.wp.com/latex.php?latex=%7BS+%5Cuplus+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S \uplus S}" class="latex" /> to mean the set of sums <img src="https://s0.wp.com/latex.php?latex=%7Ba+%2B+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a + b}" class="latex" /> where <img src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb+%5Cin+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a,b \in S}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bb+%5Cneq+a%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{b \neq a}" class="latex" />. If there is an element <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r}" class="latex" /> in the intersection then <img src="https://s0.wp.com/latex.php?latex=%7Ba+%2B+b+%3D+r+%3D+2c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a + b = r = 2c}" class="latex" />, and since <img src="https://s0.wp.com/latex.php?latex=%7B3c+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{3c = 0}" class="latex" />, we get <img src="https://s0.wp.com/latex.php?latex=%7Ba+%2B+b+%2B+c+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a + b + c = 0}" class="latex" /> with <img src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2Cc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a,b,c}" class="latex" /> in <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S}" class="latex" /> and all distinct, a contradiction. (If <img src="https://s0.wp.com/latex.php?latex=%7Bc+%3D+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{c = b}" class="latex" /> then <img src="https://s0.wp.com/latex.php?latex=%7Ba+%2B+2b+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a + 2b = 0}" class="latex" />, so <img src="https://s0.wp.com/latex.php?latex=%7Ba+%3D+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{a = b}" class="latex" />.) Let <img src="https://s0.wp.com/latex.php?latex=%7Bm_d%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{m_d}" class="latex" /> stand for the number of monomials of degree at most <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" /> in the <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> variables. The key first insight is:</p>
<blockquote><p><b>Lemma 2</b> <em> If a polynomial <img src="https://s0.wp.com/latex.php?latex=%7Bp%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p(x_1,\dots,x_n)}" class="latex" /> of degree <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" /> vanishes on <img src="https://s0.wp.com/latex.php?latex=%7BS%5Cuplus+S%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S\uplus S}" class="latex" />, then <img src="https://s0.wp.com/latex.php?latex=%7Bp%28x%29+%3D+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p(x) = 0}" class="latex" /> for all but at most <img src="https://s0.wp.com/latex.php?latex=%7B2m_%7Bd%2F2%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2m_{d/2}}" class="latex" /> points of <img src="https://s0.wp.com/latex.php?latex=%7B2S%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2S}" class="latex" />. </em>
</p></blockquote>
<p>
One could first try to interpret this as saying that <img src="https://s0.wp.com/latex.php?latex=%7BS+%5Cuplus+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{S \uplus S}" class="latex" /> “looks like” <img src="https://s0.wp.com/latex.php?latex=%7B2S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2S}" class="latex" /> to polynomials <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" /> of “low” degree <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" />. However, if <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" /> stays low relative to <img src="https://s0.wp.com/latex.php?latex=%7B%7CS%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{|S|}" class="latex" /> then the “if” part would hold vacuously, opposing the goal of bounding <img src="https://s0.wp.com/latex.php?latex=%7B%7CS%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{|S|}" class="latex" /> and making the whole idea self-defeating. In fact, the important tension comes when <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" /> is intermediate: <img src="https://s0.wp.com/latex.php?latex=%7Bd+%3D+%28q-1%29n%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d = (q-1)n/3}" class="latex" />, which for <img src="https://s0.wp.com/latex.php?latex=%7Bq+%3D+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{q = 3}" class="latex" /> makes <img src="https://s0.wp.com/latex.php?latex=%7Bd%2F2+%3D+n%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d/2 = n/3}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bd+%3D+2n%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d = 2n/3}" class="latex" /> neatly occupy the middle of the range <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B1%2C%5Cdots%2Cn%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\{1,\dots,n\}}" class="latex" />.</p>
<p>
The proof also uses the trick that if a product of two monomials has degree <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{d}" class="latex" /> then one of them must have degree at most <img src="https://s0.wp.com/latex.php?latex=%7B%5Clfloor+d%2F2%5Crfloor%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\lfloor d/2\rfloor}" class="latex" />. As I (Ken writing these sections) <a href="https://rjlipton.wordpress.com/2016/06/15/polynomial-prestidigitation/">wrote</a> about it back in 2016, this reminds of Roman Smolensky’s degree-halving <a href="https://rjlipton.wordpress.com/2012/03/11/a-note-on-distributions-and-approximation/">trick</a> in his celebrated 1987 <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.463.883&amp;rep=rep1&amp;type=pdf">theorem</a> on lower bounds for mod-<img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" /> versus mod-<img src="https://s0.wp.com/latex.php?latex=%7Bq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{q}" class="latex" />. This trick, however, runs from <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" /> to <img src="https://s0.wp.com/latex.php?latex=%7Bn%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n/2}" class="latex" /> for all moduli. </p>
<p>
In any event, the 2016 papers were a new form of the polynomial method that led to striking new results. What Grochow’s survey does for us now is bring out wider implications of this ingenuity.</p>
<p>
</p><h2> Applications in Complexity </h2><p></p>
<p>Grochow’s four application areas in section 4 of his survey are:</p>
<ol>
<li>
Progress on various forms of `sunflower’ conjectures. <p></p>
</li><li>
Barriers to attempts to show that the exponent of matrix multiplication is <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{2}" class="latex" />. <p></p>
</li><li>
Removing edges to make graphs triangle-free. <p></p>
</li><li>
Matrix rigidity and lower bounds.
</li></ol>
<p>
We say a little more about the last of these. For any <img src="https://s0.wp.com/latex.php?latex=%7BN+%5Ctimes+N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{N \times N}" class="latex" /> matrix <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7Br+%5Cleq+N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r \leq N}" class="latex" /> define the <em>rigidity</em> <img src="https://s0.wp.com/latex.php?latex=%7BR_A%28r%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R_A(r)}" class="latex" /> to be the minimum number of entries in which <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A}" class="latex" /> differs from some matrix of rank (at most) <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r}" class="latex" />. The highest possible rigidity for rank <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r}" class="latex" /> is <img src="https://s0.wp.com/latex.php?latex=%7B%28N+-+r%29%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(N - r)^2}" class="latex" />, since zeroing out an <img src="https://s0.wp.com/latex.php?latex=%7B%28n-r%29%5Ctimes%28n-r%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{(n-r)\times(n-r)}" class="latex" /> block leaves a matrix of rank at most <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r}" class="latex" />. Sufficiently random matrices meet this upper bound with high probability, but the best lower bounds for explicit families of matrices are <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28%5Cfrac%7BN%5E2%7D%7Br%7D%5Clog%5Cfrac%7BN%7D%7Br%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Omega(\frac{N^2}{r}\log\frac{N}{r})}" class="latex" />, which is only quasi-linear when <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r}" class="latex" /> is close to <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{N}" class="latex" />. The question is whether we can inch this up to <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n%5E%7B1%2B%5Cepsilon%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Omega(n^{1+\epsilon})}" class="latex" /> for some <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon &gt; 0}" class="latex" />.</p>
<blockquote><p><b>Definition 3</b> <em> A family of matrices <img src="https://s0.wp.com/latex.php?latex=%7BA_N%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_N}" class="latex" /> is <em>significantly rigid</em> if there is an <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon &gt; 0}" class="latex" /> such that taking <img src="https://s0.wp.com/latex.php?latex=%7Br+%3D+%5Cfrac%7BN%7D%7B%5Clog%5Clog+N%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r = \frac{N}{\log\log N}}" class="latex" /> makes <img src="https://s0.wp.com/latex.php?latex=%7BR_%7BA_N%7D%28r%29+%3D+%5COmega%28N%5E%7B1%2B%5Cepsilon%7D%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R_{A_N}(r) = \Omega(N^{1+\epsilon})}" class="latex" />. </em>
</p></blockquote>
<p>
The interest in this definition comes from a lack of lower bounds on linear algebraic circuits computing natural families <img src="https://s0.wp.com/latex.php?latex=%7BA_N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_N}" class="latex" /> of linear transformations that seems even more extreme than our lack of super-linear lower bounds on Boolean circuits, nor better than <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28N%5Clog+N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\Omega(N\log N)}" class="latex" /> for general algebraic circuits computing polynomials in <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{N}" class="latex" /> variables of degree <img src="https://s0.wp.com/latex.php?latex=%7BB%5E%7BO%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{B^{O(1)}}" class="latex" />. It is still consistent with our knowledge that every natural family <img src="https://s0.wp.com/latex.php?latex=%7BA_N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_N}" class="latex" /> can be computed by linear algebraic circuits of <img src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(N)}" class="latex" /> size <b>and</b> <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Clog+N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{O(\log N)}" class="latex" /> depth. Leslie Valiant in 1977 proved the following sufficient condition to improve this state of affairs.</p>
<blockquote><p><b>Theorem 4</b> <em> Every significantly rigid family <img src="https://s0.wp.com/latex.php?latex=%7BA_N%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{A_N}" class="latex" /> cannot be computed by linear algebraic circuits of linear size and logarithmic depth. </em>
</p></blockquote>
<p>
So for coming on half a century the question has been:</p>
<blockquote><p><b> </b> <em> Can we construct a natural explicit family of significantly rigid matrices? </em>
</p></blockquote>
<p>
Beliefs that the Hadamard matrices provided such a family were <a href="https://arxiv.org/pdf/1611.05558.pdf">refuted</a> by Josh Alman and Ryan Williams at STOC 2017, and <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.38.3100">known</a> <a href="https://core.ac.uk/download/pdf/82556808.pdf">results</a> for Vandermonde matrices do not have <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r}" class="latex" /> close enough to <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />. </p>
<p>
One hope had been to derive such matrices from explicit functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_n%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f_n(x_1,\dots,x_n)}" class="latex" /> over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\mathbb{Z}_p}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{p}" class="latex" /> prime by taking <img src="https://s0.wp.com/latex.php?latex=%7BN+%3D+p%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{N = p^n}" class="latex" /> and defining </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A_%7Bf_n%7D%5B%5Cvec%7Bx%7D%2C%5Cvec%7By%7D%5D+%3D+f%28x_1+%2B+y_1%2C%5Cdots%2Cx_n+%2B+y_n%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  A_{f_n}[\vec{x},\vec{y}] = f(x_1 + y_1,\dots,x_n + y_n). " class="latex" /></p>
<p>Unfortunately, the polynomial method for cap sets shows that no such attempt can work. Zeev Dvir and Benjamin Edelman <a href="https://theoryofcomputing.org/articles/v015a008/">proved</a> that no matter how <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{f}" class="latex" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\epsilon}" class="latex" /> are chosen, there is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\delta &gt; 0}" class="latex" /> such that for all large enough <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{n}" class="latex" />, </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R_%7BA_%7Bf_n%7D%7D%28N%5E%7B1-%5Cdelta%7D%29+%3C+n%5E%7B1%2B%5Cepsilon%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="\displaystyle  R_{A_{f_n}}(N^{1-\delta}) &lt; n^{1+\epsilon}. " class="latex" /></p>
<p>This means we cannot get <img src="https://s0.wp.com/latex.php?latex=%7BR_%7BA_%7Bf_n%7D%7D%28r%29+%3D+%5COmega%28n%5E%7B1%2B%5COmega%281%29%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{R_{A_{f_n}}(r) = \Omega(n^{1+\Omega(1)})}" class="latex" /> for <img src="https://s0.wp.com/latex.php?latex=%7Br+%3D+%5Cfrac%7BN%7D%7B%5Clog%5Clog+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{r = \frac{N}{\log\log N}}" class="latex" />, indeed, far from it. What is most curious to us is that for matrix multiplication, the cap-set related technique frustrates a better complexity upper bound, whereas here it frustrates a better lower bound.</p>
<p>
</p><h2> Open Problems </h2><p></p>
<p>What further applications can we find for the polynomial method?</p>
<p></p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2021/02/27/new-old-ancient-results/"><span class="datestr">at February 27, 2021 08:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/028">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/028">TR21-028 |  Branching Programs with Bounded Repetitions and $\mathrm{Flow}$ Formulas | 

	Anastasia Sofronova, 

	Dmitry Sokolov</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Restricted branching programs capture various complexity measures like space in Turing machines or length of proofs in proof systems. In this paper, we focus on the application in the proof complexity that was discovered by Lovasz et al. '95 who showed the equivalence between regular Resolution and read-once branching programs for ``unsatisfied clause search problem'' ($\mathrm{Search}_{\varphi}$). This connection is widely used, in particular, in the recent breakthrough result about the Clique problem in regular Resolution by Atserias et al. '18.

We study the branching programs with bounded repetitions, so-called $(1, +k)$-BPs (Sieling '96) in application to the $\mathrm{Search}_{\varphi}$ problem. On the one hand, it is a natural generalization of read-once branching programs. On the other hand, this model gives a powerful proof system that can efficiently certify the unsatisfiability of a wide class of formulas that is hard for Resolution (Knop '17).


We deal with $\mathrm{Search}_{\varphi}$ that is ``relatively easy'' compared to all known hard examples for the $(1, +k)$-BPs. We introduce the first technique for proving exponential lower bounds for the $(1, +k)$-BPs on $\mathrm{Search}_{\varphi}$. To do it we combine a well-known technique for proving lower bounds on the size of branching programs (Sieling '96; Sieling, Wegener '94; Jukna, Razborov '98) with the modification of the ``closure'' technique (Alekhnovich et al. 04; Alekhnovich, Razborov '03). In contrast with the most Resolution lower bounds, our technique uses not only ``local'' properties of the formula, but also a ``global'' structure. Our hard examples are based on the $\mathrm{Flow}$ formulas introduced in (Alekhnovich, Razborov '03).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/028"><span class="datestr">at February 27, 2021 07:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/02/26/phd-position-at-university-of-amsterdam-apply-by-march-18-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/02/26/phd-position-at-university-of-amsterdam-apply-by-march-18-2021/">PhD position at University of Amsterdam (apply by March 18, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The University of Amsterdam encourages applications for an open PhD position in the theory of quantum computing and quantum networks. Potential research topics include multi-party quantum computation, secure positioning, and multi-party communication complexity.</p>
<p>Website: <a href="https://www.uva.nl/shared-content/uva/en/vacancies/2021/02/21-069-phd-position-on-the-theory-of-quantum-networks.html">https://www.uva.nl/shared-content/uva/en/vacancies/2021/02/21-069-phd-position-on-the-theory-of-quantum-networks.html</a><br />
Email: f.speelman@uva.nl</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/02/26/phd-position-at-university-of-amsterdam-apply-by-march-18-2021/"><span class="datestr">at February 26, 2021 10:38 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/02/26/faculty-at-krea-university-india-apply-by-may-1-2021-2/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/02/26/faculty-at-krea-university-india-apply-by-may-1-2021-2/">Faculty at KREA University, India (apply by May 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Krea University, an upcoming liberal arts university located near Chennai, India, is looking for dynamic tenure track faculty across disciplines and experience levels in computer science.</p>
<p>Open House, 27th Feb 2021 9:00 AM [IST]:<br />
<a href="https://krea.edu.in/wp-content/uploads/2021/02/cshiringopenhouse.pdf">https://krea.edu.in/wp-content/uploads/2021/02/cshiringopenhouse.pdf</a></p>
<p>Website: <a href="https://jobs.acm.org/jobs/assistant-associate-professor-computer-science-sri-city-andhra-pradesh-517646-121295350-d">https://jobs.acm.org/jobs/assistant-associate-professor-computer-science-sri-city-andhra-pradesh-517646-121295350-d</a><br />
Email: sias.chair_sciences@krea.edu.in</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/02/26/faculty-at-krea-university-india-apply-by-may-1-2021-2/"><span class="datestr">at February 26, 2021 08:49 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/02/25/tenure-track-open-rank-at-university-of-illinois-urbana-champaign-apply-by-june-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/02/25/tenure-track-open-rank-at-university-of-illinois-urbana-champaign-apply-by-june-1-2021/">Tenure Track (Open Rank) at University of Illinois, Urbana-Champaign (apply by March 15, 2021; or as soon as possible)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science at the University of Illinois Urbana-Champaign invites applications for full-time tenure-track faculty positions at all levels (Assistant Professor, Associate Professor, Full Professor). We particularly encourage applications in quantum computing, but also welcome applications from exceptional candidates in other areas.</p>
<p>Website: <a href="https://cs.illinois.edu/about/positions/faculty-positions">https://cs.illinois.edu/about/positions/faculty-positions</a><br />
Email: chekuri@illinois.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/02/25/tenure-track-open-rank-at-university-of-illinois-urbana-champaign-apply-by-june-1-2021/"><span class="datestr">at February 25, 2021 10:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-2202248828009562800">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2021/02/complexity-is-enemy-of-speed.html">Complexity is the Enemy of Speed</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>The title of this post came from an <a href="https://www.wsj.com/articles/connecticuts-covid-vaccine-lesson-11614124012">opinion piece</a> in the Wall Street Journal yesterday on vaccine distribution. Many attempts to get the vaccines to the right groups first have slowed down distribution and sometime even caused <a href="https://www.nbcnews.com/news/us-news/thousands-covid-19-vaccines-wind-garbage-because-fed-state-guidelines-n1254364">vaccines to go to waste</a>. Rules to help spread vaccines across minority groups often backfire. Often when some rules lead to inequity, we try to fix it with more rules when we need less much less. Attempts to distribute vaccines to multiple medical and pharmacy sites have made it difficult to get appointments even if you are eligible.</p><p>Randomness is the simplest way to fairness. The movie Contagion got it right, just choose birthdays by picking balls from a bin to distribute the vaccine. Then people can just show up at a few chosen sites with proof of birthday. No need to sign up.</p><p>You could argue to add back conditions like age, medical conditions, jobs but that just leads you down the same problematic path. The fastest way to get past this pandemic is to get vaccines into arms. Trust the randomness.</p></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2021/02/complexity-is-enemy-of-speed.html"><span class="datestr">at February 25, 2021 02:19 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2021/02/25/faculty-at-krea-university-india-apply-by-may-1-2021/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2021/02/25/faculty-at-krea-university-india-apply-by-may-1-2021/">Faculty at KREA University, India (apply by May 1, 2021)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Krea University, an upcoming liberal arts university located near Chennai, India, is looking for dynamic tenure track faculty across disciplines and experience levels in computer science.</p>
<p>Open House, 27th Feb 2021 9:00 AM [IST]:<br />
<a href="https://krea.edu.in/wp-content/uploads/2021/02/cshiringopenhouse.pdf">https://krea.edu.in/wp-content/uploads/2021/02/cshiringopenhouse.pdf</a></p>
<p>Website: <a href="https://jobs.acm.org/jobs/assistant-associate-professor-computer-science-sri-city-andhra-pradesh-517646-121295350-d">https://jobs.acm.org/jobs/assistant-associate-professor-computer-science-sri-city-andhra-pradesh-517646-121295350-d</a><br />
Email: sias.chair_sciences@krea.edu.in</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2021/02/25/faculty-at-krea-university-india-apply-by-may-1-2021/"><span class="datestr">at February 25, 2021 09:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=532">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2021/02/24/tcs-talk-wednesday-march-3-steve-hanneke-ttic/">TCS+ talk: Wednesday, March 3 — Steve Hanneke, TTIC</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, March 3th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Steve Hanneke</strong> from TTIC will speak about “<em>A Theory of Universal Learning</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our website</a> aftwerwards, so people who did not sign up will still be able to watch the talk.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: How quickly can a given class of concepts be learned from examples? It is common to measure the performance of a supervised machine learning algorithm by plotting its “learning curve”, that is, the decay of the error rate as a function of the number of training examples. However, the classical theoretical framework for understanding learnability, the PAC model of Vapnik-Chervonenkis and Valiant, does not explain the behavior of learning curves: the distribution-free PAC model of learning can only bound the upper envelope of the learning curves over all possible data distributions. This does not match the practice of machine learning, where the data source is typically fixed in any given scenario, while the learner may choose the number of training examples on the basis of factors such as computational resources and desired accuracy.</p>
<p>In this work, we study an alternative learning model that better captures such practical aspects of machine learning, but still gives rise to a complete theory of the learnable in the spirit of the PAC model. More precisely, we consider the problem of universal learning, which aims to understand the performance of learning algorithms on every data distribution, but without requiring uniformity over the distribution. The main result of this work is a remarkable trichotomy: there are only three possible rates of universal learning. More precisely, we show that the learning curves of any given concept class decay either at an exponential, linear, or arbitrarily slow rates. Moreover, each of these cases is completely characterized by appropriate combinatorial parameters, and we exhibit optimal learning algorithms that achieve the best possible rate in each case.</p>
<p>Joint work with Olivier Bousquet, Shay Moran, Ramon van Handel, and Amir Yehudayoff.</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2021/02/24/tcs-talk-wednesday-march-3-steve-hanneke-ttic/"><span class="datestr">at February 25, 2021 02:03 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=8008">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2021/02/24/unsupervised-learning-and-generative-models/">Unsupervised Learning and generative models</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>Scribe notes by <a href="https://github.com/rxu18">Richard Xu</a></em></p>



<p><strong>Previous post:</strong> <a href="https://windowsontheory.org/2021/02/17/what-do-deep-networks-learn-and-when-do-they-learn-it/">What do neural networks learn and when do they learn it</a> <strong>Next post:</strong> TBD. See also <a href="https://windowsontheory.org/category/ml-theory-seminar/?order=asc">all seminar posts</a> and <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">course webpage</a>.</p>



<p><a href="http://files.boazbarak.org/misc/mltheory/ML_seminar_lecture_3.pdf">lecture slides (pdf)</a> – <a href="http://files.boazbarak.org/misc/mltheory/ML_seminar_lecture_3.pptx">lecture slides (Powerpoint with animation and annotation)</a> – <a href="https://harvard.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=70cafab0-bdea-412b-a353-acc90173fd61">video</a></p>



<p>In this lecture, we move from the world of supervised learning to unsupervised learning, with a focus on generative models. We will</p>



<ul><li>Introduce unsupervised learning and the relevant notations.</li><li>Discuss various approaches for generative models, such as PCA, VAE, Flow Models, and GAN.</li><li>Discuss theoretical and practical results we currently have for these approaches.</li></ul>



<h2>Setup for Unsupervised Learning</h2>



<p>In <em>supervised learning</em>, we have data <img src="https://s0.wp.com/latex.php?latex=x_i%5Csim+p%5Csubset+%5Cmathbb+R%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_i\sim p\subset \mathbb R^d" class="latex" title="x_i\sim p\subset \mathbb R^d" /> and we want to understand the distribution <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" />. For example,</p>



<ol><li><em>Probability estimation:</em> Given <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />, can we compute/approximate <img src="https://s0.wp.com/latex.php?latex=p%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(x)" class="latex" title="p(x)" /> (the probability that <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> is output under <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" />)?</li><li><em>Generation:</em> Can we sample from <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" />, or from a “nearby” distribution?</li><li><em>Encoding:</em> Can we find a representation <img src="https://s0.wp.com/latex.php?latex=E%3A%5Cmathrm%7BSupport%7D%28p%29+%5Crightarrow+%5Cmathbb%7BR%7D%5Er&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E:\mathrm{Support}(p) \rightarrow \mathbb{R}^r" class="latex" title="E:\mathrm{Support}(p) \rightarrow \mathbb{R}^r" /> such that for <img src="https://s0.wp.com/latex.php?latex=x+%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x \sim p" class="latex" title="x \sim p" />, <img src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E(x)" class="latex" title="E(x)" /> makes it easy to answer semantic questions on <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" />? And such that <img src="https://s0.wp.com/latex.php?latex=%5Clangle+E%28x%29+%2C+E%28x%27%29+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle E(x) , E(x') \rangle" class="latex" title="\langle E(x) , E(x') \rangle" /> corresponds to “semantic similarity” of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> and <img src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x'" class="latex" title="x'" />?</li><li><em>Prediction:</em> We would like to be able to predict (for example) the second half of <img src="https://s0.wp.com/latex.php?latex=x+%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x \sim p" class="latex" title="x \sim p" /> from the first half. More generally, we want to solve the <em>conditional generation</em> task, where given some function <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> (e.g., the projection to the first half) and some value <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y" class="latex" title="y" />, we can sample from the conditional probability distribution <img src="https://s0.wp.com/latex.php?latex=p%7Cf%28x%29%3Dy&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p|f(x)=y" class="latex" title="p|f(x)=y" />.</li></ol>



<p>Our “dream” is to solve all of those by the following setup:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/EPyXsSW.png" alt="" /></figure>



<p>There is an “encoder” <img src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E" class="latex" title="E" /> that maps <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> into a representation <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> in the latent space, and then a “decoder” <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" title="D" /> that can transform such a representation back into <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />. We would like it to be the case that:</p>



<ol><li><em>Generation:</em> For <img src="https://s0.wp.com/latex.php?latex=x%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x\sim p" class="latex" title="x\sim p" />, the induced distribution <img src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E(x)" class="latex" title="E(x)" /> is “nice” and efficiently sampleable (e.g., the standard normal <img src="https://s0.wp.com/latex.php?latex=N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(0,I)" class="latex" title="N(0,I)" /> over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5Er&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{R}^r" class="latex" title="\mathbb{R}^r" />) such that we can (approximately) sample from <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> by sampling <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> and outputting <img src="https://s0.wp.com/latex.php?latex=D%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D(z)" class="latex" title="D(z)" />.</li><li><em>Density estimation:</em> We would like to be able to evaluate the probability that <img src="https://s0.wp.com/latex.php?latex=D%28z%29%3Dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D(z)=x" class="latex" title="D(z)=x" />. For example, if <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" title="D" /> is the inverse of <img src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E" class="latex" title="E" />, and <img src="https://s0.wp.com/latex.php?latex=z+%5Csim+N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z \sim N(0,I)" class="latex" title="z \sim N(0,I)" /> we could do so by computing <img src="https://s0.wp.com/latex.php?latex=%7C+E%28x%29+%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="| E(x) |" class="latex" title="| E(x) |" />.</li><li><em>Semantic representation:</em> We would like the latent representation <img src="https://s0.wp.com/latex.php?latex=E%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E(z)" class="latex" title="E(z)" /> to map <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> into meaningful latent space. Ideally, linear directions in this space will correspond to semantic attributes.</li><li><em>Conditional sampling:</em> We would like to be able to do conditional generation, and in particular for some functions <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> and values <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y" class="latex" title="y" />, be able to sample from the set of <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" />‘s such that <img src="https://s0.wp.com/latex.php?latex=f%28E%28z%29%29%3Dy&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(E(z))=y" class="latex" title="f(E(z))=y" /></li></ol>



<p>Ideally, if we could map images to the latent variables used to generate them and vice versa (as in the cartoon from the last lecture), then we could achieve these goals:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/3X4aqfl.png" alt="" /></figure>



<p>At the moment, we do not have a single system that can solve all these problems for a natural domain such as images or language, but we have several approaches that achieve part of the dream.</p>



<p><strong>Digressions.</strong> Before discussing concrete models, we make three digressions. One will be non-technical, and the other three technical. The three technical digressions are the following:</p>



<ol><li>If we have multiple objectives, we want a way to interpolate between them.</li><li>To measure how good our models are, we have to measure distances between statistical distributions.</li><li>Once we come up with generating models, we would <em>metrics</em> for measuring how good they are.</li></ol>



<h2>Non-technical digression: Is deep learning a cargo cult science? (spoiler: no)</h2>



<p>In an <a href="https://calteches.library.caltech.edu/51/2/CargoCult.htm">influential essay</a>, Richard Feynman coined the term “cargo cult science” for the activities that have superficial similarities to science but do not follow the scientific method. Some of the tools we use in machine learning look suspiciously close to “cargo cult science.” We use the tools of classical learning, but in a setting in which they were not designed to work in and on which we have no guarantees that they will work. For example, we run (stochastic) gradient descent – an algorithm designed to minimize a convex function – to minimize convex loss. We also write use <em>empirical risk minimization</em> – minimizing loss on our training set – in a setting where we have no guarantee that it will not lead to “overfitting.”</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/tBw6UsX.png" alt="" /></figure>



<figure class="wp-block-image"><img src="https://i.imgur.com/s5G6xfj.png" alt="" /></figure>



<p>And yet, unlike the original cargo cults, in deep learning, “the planes do land”, or at least they often do. When we use a tool <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" title="A" /> in a situation <img src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X" class="latex" title="X" /> that it was not designed to work in, it can play out in one (or mixture) of the following scenarios:</p>



<ul><li><strong>Murphy’s Law:</strong> “Anything that can go wrong will go wrong.” As computer scientists, we are used to this scenario. The natural state of our systems is that they have bugs and errors. There is a reason why software engineering talks about “contracts”, “invariants”, preconditions” and “postconditions”: typically, if we try to use a component <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" title="A" /> in a situation that it wasn’t designed for, it will not turn out well. This is doubly the case in security and cryptography, where people have learned the hard way time and again that Murphy’s law holds sway.</li><li><strong>“Marley’s Law”:</strong> “Every little thing gonna be alright”. In machine learning, we sometimes see the opposite phenomenon- we use algorithms outside the conditions under which they have been analysed or designed to work in, but they still produce good results. Part of it could be because ML algorithms are already robust to certain errors in their inputs, and their output was only guaranteed to be approximately correct in the first place.</li></ul>



<p>Murphy’s law does occasionally pop up, even in machine learning. We will see examples of both phenomena in this lecture.</p>



<h2>Technical digression 1: Optimization with Multiple Objectives</h2>



<p>During machine learning, we often have multiple objectives to optimize. For example, we may want both an efficient encoder and an effective decoder, but there is a tradeoff between them.</p>



<p>Suppose we have 2 loss functions <img src="https://s0.wp.com/latex.php?latex=L_1%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L_1(w)" class="latex" title="L_1(w)" /> and <img src="https://s0.wp.com/latex.php?latex=L_2%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L_2(w)" class="latex" title="L_2(w)" />, but there can be a trade off between them. The <em>pareto curve</em> is the set <img src="https://s0.wp.com/latex.php?latex=P%3D%7B%28a%2Cb%29%3A+%5Cforall+w%5Cin+W%2C+L_1%28w%29%5Cge+a%5Cvee+L_2%28w%29%5Cge+b.%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P={(a,b): \forall w\in W, L_1(w)\ge a\vee L_2(w)\ge b.}" class="latex" title="P={(a,b): \forall w\in W, L_1(w)\ge a\vee L_2(w)\ge b.}" /></p>



<figure class="wp-block-image"><img src="https://i.imgur.com/QbPRQtR.jpg" alt="Pareto curve for 2 loss functions" /></figure>



<p>If a model is above the curve, it is not optimal. If it is below the curve, the model is infeasible.</p>



<p>When the set <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P" class="latex" title="P" /> is convex, we can reach any point on the curve <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P" class="latex" title="P" /> by minimizing <img src="https://s0.wp.com/latex.php?latex=L_1%28w%29%2B%5Clambda+L_2%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L_1(w)+\lambda L_2(w)" class="latex" title="L_1(w)+\lambda L_2(w)" />. The proof is by the picture above: for any point <img src="https://s0.wp.com/latex.php?latex=%28a_0%2Cb_0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(a_0,b_0)" class="latex" title="(a_0,b_0)" /> on the curve, there is a tangent line at <img src="https://s0.wp.com/latex.php?latex=%28a_0%2Cb_0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(a_0,b_0)" class="latex" title="(a_0,b_0)" /> that is strictly below the curve. If <img src="https://s0.wp.com/latex.php?latex=a%2B%5Clambda+b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a+\lambda b" class="latex" title="a+\lambda b" /> is the normal vector for this line, then the global minimum of <img src="https://s0.wp.com/latex.php?latex=a%2B%5Clambda+b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="a+\lambda b" class="latex" title="a+\lambda b" /> on the feasible set will be <img src="https://s0.wp.com/latex.php?latex=%28a_0%2Cb_0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(a_0,b_0)" class="latex" title="(a_0,b_0)" />.<br />This motivates the common practice of minimizing two introducing a hyperparameter <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda" class="latex" title="\lambda" /> to aggregate two objectives into one.</p>



<p>When <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P" class="latex" title="P" /> is not convex, it may well be that:</p>



<ul><li>Some points on <img src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="P" class="latex" title="P" /> are not minima of <img src="https://s0.wp.com/latex.php?latex=L_1+%2B+%5Clambda+L_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L_1 + \lambda L_2" class="latex" title="L_1 + \lambda L_2" /></li><li><img src="https://s0.wp.com/latex.php?latex=L_1+%2B+%5Clambda+L_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L_1 + \lambda L_2" class="latex" title="L_1 + \lambda L_2" /> might have multiple minima</li><li>Depending on the path one takes, it is possible to get “stuck” in a point that is <em>not</em> a global minima</li></ul>



<p>The following figure demonstrates all three possibilities</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/Rjg4iZU.png" alt="" /></figure>



<p>Par for the course, this does not stop people in machine learning from using this approach to minimize different objectives, and often “Marley’s Law” holds, and this works fine. But this is not always the case. A <a href="https://engraved.ghost.io/why-machine-learning-algorithms-are-hard-to-tune/">nice blog post by Degrave and Kurshonova</a> discusses this issue and why sometimes we do in fact, see “Murphy’s law” when we combine objectives. They also detail some other approaches for combining objectives, but there is no single way that will work in all cases.</p>



<p>Figure from Degrave-Kurshonova demonstrating where the algorithm could reach in the non-convex case depending on initialization and <img src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\lambda" class="latex" title="\lambda" />:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/4VZZaRR.gif" alt="" /></figure>



<h2>Technical digression 2: Distances between probability measures</h2>



<p>Suppose we have two distributions <img src="https://s0.wp.com/latex.php?latex=p%2Cq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p,q" class="latex" title="p,q" /> over <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" title="D" />. There are two common ways of measuring the distances between them.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/5JTzg6D.png" alt="" /></figure>



<p>The <em>Total Variance (TV)</em> (also known as statistical distance) between <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> and <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" title="q" /> is equal to</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BTV%7D%28p%2Cq%29%3D%5Cfrac12+%5Csum_%7Bx%5Cin+D%7D%7Cp%28x%29-q%28x%29%7C+%3D+%5Cmax_%7Bf%3AD%5Cto+%7B0%2C1%7D%7D+%7C+%5Cmathbb%7BE%7D_p%28f%29-%5Cmathbb%7BE%7D_q%28f%29%7C.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{TV}(p,q)=\frac12 \sum_{x\in D}|p(x)-q(x)| = \max_{f:D\to {0,1}} | \mathbb{E}_p(f)-\mathbb{E}_q(f)|." class="latex" title="\Delta_{TV}(p,q)=\frac12 \sum_{x\in D}|p(x)-q(x)| = \max_{f:D\to {0,1}} | \mathbb{E}_p(f)-\mathbb{E}_q(f)|." /></p>



<p>The second equality can be proved by constructing <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> that outputs 1 on <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> where <img src="https://s0.wp.com/latex.php?latex=p%28x%29-q%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(x)-q(x)" class="latex" title="p(x)-q(x)" /> and vice versa. The <img src="https://s0.wp.com/latex.php?latex=%5Cmax_f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\max_f" class="latex" title="\max_f" /> definition has a crypto-flavored interpretation: For any adversary <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" />, the TV measures the advantage they can have over half of determining whether <img src="https://s0.wp.com/latex.php?latex=x%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x\sim p" class="latex" title="x\sim p" /> or <img src="https://s0.wp.com/latex.php?latex=x%5Csim+q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x\sim q" class="latex" title="x\sim q" />.</p>



<p>Second, the <em>Kullback–Leibler (KL) Divergence</em> between <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> and <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" title="q" /> is equal to</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29%3D%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7D%28%5Clog+p%28x%29%2Fq%28x%29%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(p||q)=\mathbb{E}_{x\sim p}(\log p(x)/q(x))." class="latex" title="\Delta_{KL}(p||q)=\mathbb{E}_{x\sim p}(\log p(x)/q(x))." /></p>



<p>(The total variation distance is symmetric, in the sense that <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BTV%7D%28p%2Cq%29%3D%5CDelta_%7BTV%7D%28q%2Cp%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{TV}(p,q)=\Delta_{TV}(q,p)" class="latex" title="\Delta_{TV}(p,q)=\Delta_{TV}(q,p)" />, but the KL divergence is not. Both have the property that they are non-negative and equal to zero if and only if <img src="https://s0.wp.com/latex.php?latex=p%3Dq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p=q" class="latex" title="p=q" />.)</p>



<p>Unlike the total variation distance, which is bounded between <img src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0" class="latex" title="0" /> and <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1" class="latex" title="1" />, the KL divergence can be arbitrarily large and even infinite (though it can be shown using the concavity of log that it is always non-negative). To interpret the KL divergence, it is helpful to separate between the case that <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(p||q)" class="latex" title="\Delta_{KL}(p||q)" /> is close to zero and the case where it is a large number. If <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(p||q) \approx \delta" class="latex" title="\Delta_{KL}(p||q) \approx \delta" /> for some <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%5Cll+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta \ll 1" class="latex" title="\delta \ll 1" />, then we would need about <img src="https://s0.wp.com/latex.php?latex=1%2F%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/\delta" class="latex" title="1/\delta" /> samples to distinguish between samples of <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> and samples of <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" title="q" />. In particular, suppose that we get <img src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_1,\ldots,x_n" class="latex" title="x_1,\ldots,x_n" /> and we want to distinguish between the case that we they were independently sampled from <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> and the case that they were independently sampled from <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" title="q" />. A natural (and as it turns out, optimal) approach is to use a <em>likelihood ratio test</em> where we decide the samples came from <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="T" class="latex" title="T" /> if <img src="https://s0.wp.com/latex.php?latex=%5CPr_p%5Bx_1%2C%5Cldots%2Cx_n%5D%2F%5CPr_q%5Bx_1%2C%5Cldots%2Cx_n%5D%3ET&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Pr_p[x_1,\ldots,x_n]/\Pr_q[x_1,\ldots,x_n]&gt;T" class="latex" title="\Pr_p[x_1,\ldots,x_n]/\Pr_q[x_1,\ldots,x_n]&gt;T" />. For example, if we set <img src="https://s0.wp.com/latex.php?latex=T%3D20&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="T=20" class="latex" title="T=20" /> then this approach will guarantee that our “false positive rate” (announcing that samples came from <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> when they really came from <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" title="q" />) will be most <img src="https://s0.wp.com/latex.php?latex=1%2F20%3D5%5C%25&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/20=5\%" class="latex" title="1/20=5\%" />. Taking logs and using the fact that the probability of these independent samples is the product of probabilities, this amounts to testing whether <img src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bi%3D1%7D%5En+%5Clog+%5Cleft%28%5Ctfrac%7Bp%28x_i%29%7D%7Bq%28x_i%29%7D%5Cright%29+%5Cgeq+%5Clog+T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sum_{i=1}^n \log \left(\tfrac{p(x_i)}{q(x_i)}\right) \geq \log T" class="latex" title="\sum_{i=1}^n \log \left(\tfrac{p(x_i)}{q(x_i)}\right) \geq \log T" />. When samples come from <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" />, the expectation of the righthand side is <img src="https://s0.wp.com/latex.php?latex=n%5Ccdot+%5CDelta_%7BKL%7D%28p%7C%7Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="n\cdot \Delta_{KL}(p||q)" class="latex" title="n\cdot \Delta_{KL}(p||q)" />, so we see that to ensure <img src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="T" class="latex" title="T" /> is larger than <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1" class="latex" title="1" /> we need the number samples to be at least <img src="https://s0.wp.com/latex.php?latex=1%2F%5CDelta_%7BKL%7D%28p%7C%7Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1/\Delta_{KL}(p||q)" class="latex" title="1/\Delta_{KL}(p||q)" /> (and as it turns out, this will do).</p>



<p>When the <img src="https://s0.wp.com/latex.php?latex=KL&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="KL" class="latex" title="KL" /> divergence is a large number <img src="https://s0.wp.com/latex.php?latex=k%3E1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k&gt;1" class="latex" title="k&gt;1" />, we can think of it as the number of bits of “surprise” in <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" title="q" /> as opposed to <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" />. For example, in the common case where <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" title="q" /> is obtained by conditioning <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> on some event <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" title="A" />, <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(p||q)" class="latex" title="\Delta_{KL}(p||q)" /> will typically be <img src="https://s0.wp.com/latex.php?latex=%5Clog+1%2F%5CPr%5BA%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log 1/\Pr[A]" class="latex" title="\log 1/\Pr[A]" /> (some fine print applies). In general, if <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" title="q" /> is obtained from <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> by revealing <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" /> bits of information (i.e., by conditioning on a random variable whose mutual information with <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> is <img src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="k" class="latex" title="k" />) then <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29%3Dk&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(p||q)=k" class="latex" title="\Delta_{KL}(p||q)=k" />.</p>



<p><strong>Generalizations:</strong> The total variation distance is a special case of metrics of the form <img src="https://s0.wp.com/latex.php?latex=%5CDelta%28p%2Cq%29+%3D+%5Cmax_%7Bf+%5Cin+%5Cmathcal%7BF%7D%7D+%7C%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7D+f%28x%29+-+%5Cmathbb%7BE%7D_%7Bx+%5Csim+q%7D+f%28x%29%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta(p,q) = \max_{f \in \mathcal{F}} |\mathbb{E}_{x\sim p} f(x) - \mathbb{E}_{x \sim q} f(x)|" class="latex" title="\Delta(p,q) = \max_{f \in \mathcal{F}} |\mathbb{E}_{x\sim p} f(x) - \mathbb{E}_{x \sim q} f(x)|" />. These are known as <a href="https://arxiv.org/abs/0901.2698">integral probability metrics</a> and include examples such as the Wasserstein distance, Dudley metric, and Maximum Mean Discrepancy. KL divergence is a special case of divergence measures known as <a href="https://en.wikipedia.org/wiki/F-divergence"><img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" />-divergence</a>, which are measures of the form <img src="https://s0.wp.com/latex.php?latex=%5CDelta_f%28p%7C%7Cq%29%3D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+q%7D+f%5Cleft%28%5Ctfrac%7Bp%28x%29%7D%7Bq%28x%29%7D%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_f(p||q)= \mathbb{E}_{x \sim q} f\left(\tfrac{p(x)}{q(x)}\right)" class="latex" title="\Delta_f(p||q)= \mathbb{E}_{x \sim q} f\left(\tfrac{p(x)}{q(x)}\right)" />. The KL divergence is obtained by setting <img src="https://s0.wp.com/latex.php?latex=f%28t%29+%3D+t+%5Clog+t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(t) = t \log t" class="latex" title="f(t) = t \log t" />. (In fact even the TV distance is a special case of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> divergence by setting <img src="https://s0.wp.com/latex.php?latex=f%28t%29%3D%7Ct-1%7C%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(t)=|t-1|/2" class="latex" title="f(t)=|t-1|/2" />.)</p>



<p><strong>Normal distributions:</strong> It is a useful exercise to calculate the TV and KL distances for normal random variables. If <img src="https://s0.wp.com/latex.php?latex=p%3DN%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p=N(0,1)" class="latex" title="p=N(0,1)" /> and <img src="https://s0.wp.com/latex.php?latex=q%3DN%28-%5Cepsilon%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q=N(-\epsilon,1)" class="latex" title="q=N(-\epsilon,1)" />, then since most probability mass in the regime where <img src="https://s0.wp.com/latex.php?latex=p%28x%29+%5Capprox+%281%5Cpm+%5Cepsilon%29+q%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(x) \approx (1\pm \epsilon) q(x)" class="latex" title="p(x) \approx (1\pm \epsilon) q(x)" />, <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BTV%7D%28p%2Cq%29+%5Capprox+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{TV}(p,q) \approx \epsilon" class="latex" title="\Delta_{TV}(p,q) \approx \epsilon" /> (i.e., up to some multiplicative constant). For KL divergence, if we selected <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> from a normal between <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> and <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" title="q" /> then with probability about half we’ll have <img src="https://s0.wp.com/latex.php?latex=p%28x%29+%5Capprox+q%28x%29%281%2B%5Cepsilon%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(x) \approx q(x)(1+\epsilon)" class="latex" title="p(x) \approx q(x)(1+\epsilon)" /> and with probability about half we will have <img src="https://s0.wp.com/latex.php?latex=p%28q%29+%5Capprox+q%28x%29%281-%5Cepsilon%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(q) \approx q(x)(1-\epsilon)" class="latex" title="p(q) \approx q(x)(1-\epsilon)" />. By selecting <img src="https://s0.wp.com/latex.php?latex=x%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x\sim p" class="latex" title="x\sim p" />, we increase probability of the former to <img src="https://s0.wp.com/latex.php?latex=%5Capprox+1%2F2%2B%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\approx 1/2+\epsilon" class="latex" title="\approx 1/2+\epsilon" /> and the decrease the probability of the latter to <img src="https://s0.wp.com/latex.php?latex=%5Capprox+1%2F2+-+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\approx 1/2 - \epsilon" class="latex" title="\approx 1/2 - \epsilon" />. So we have <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\epsilon" class="latex" title="\epsilon" /> bias towards <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />‘s where <img src="https://s0.wp.com/latex.php?latex=p%28x%29%2Fq%28x%29+%5Capprox+1%2B%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(x)/q(x) \approx 1+\epsilon" class="latex" title="p(x)/q(x) \approx 1+\epsilon" />, or <img src="https://s0.wp.com/latex.php?latex=%5Clog+p%28x%29%2Fq%28x%29+%5Capprox+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log p(x)/q(x) \approx \epsilon" class="latex" title="\log p(x)/q(x) \approx \epsilon" />. Hence <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+%5Cepsilon%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(p||q) \approx \epsilon^2" class="latex" title="\Delta_{KL}(p||q) \approx \epsilon^2" />. The above generalizes to higher dimensions. If <img src="https://s0.wp.com/latex.php?latex=p%3D+N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p= N(0,I)" class="latex" title="p= N(0,I)" /> is a <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" />-variate normal, and <img src="https://s0.wp.com/latex.php?latex=q%3DN%28%5Cmu%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q=N(\mu,I)" class="latex" title="q=N(\mu,I)" /> for <img src="https://s0.wp.com/latex.php?latex=%5Cmu+%5Cin+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu \in \mathbb{R}^d" class="latex" title="\mu \in \mathbb{R}^d" />, then (for small <img src="https://s0.wp.com/latex.php?latex=%7C%5Cmu%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="|\mu|" class="latex" title="|\mu|" />) <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BTV%7D%28p%2Cq%29+%5Capprox+%7C%5Cmu%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{TV}(p,q) \approx |\mu|" class="latex" title="\Delta_{TV}(p,q) \approx |\mu|" /> while <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29%5Capprox+%7C%5Cmu%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(p||q)\approx |\mu|^2" class="latex" title="\Delta_{KL}(p||q)\approx |\mu|^2" />.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/w3aXr99.png" alt="" /></figure>



<p>If <img src="https://s0.wp.com/latex.php?latex=p%3DN%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p=N(0,1)" class="latex" title="p=N(0,1)" /> and <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" title="q" /> is a “narrow normal” of the form <img src="https://s0.wp.com/latex.php?latex=q%3DN%280%2C%5Cepsilon%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q=N(0,\epsilon^2)" class="latex" title="q=N(0,\epsilon^2)" /> then their TV distance is close to <img src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="1" class="latex" title="1" /> while <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+1%2F%5Cepsilon%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(p||q) \approx 1/\epsilon^2" class="latex" title="\Delta_{KL}(p||q) \approx 1/\epsilon^2" />. In the <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> dimensional case, if <img src="https://s0.wp.com/latex.php?latex=p%3DN%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p=N(0,I)" class="latex" title="p=N(0,I)" /> and <img src="https://s0.wp.com/latex.php?latex=q%3DN%280%2CV%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q=N(0,V)" class="latex" title="q=N(0,V)" /> for some covariance matrix <img src="https://s0.wp.com/latex.php?latex=V&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="V" class="latex" title="V" />, then <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+%5Cmathrm%7BTr%7D%28V%5E%7B-1%7D%29+-+d+%2B+%5Cln+%5Cdet+V&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(p||q) \approx \mathrm{Tr}(V^{-1}) - d + \ln \det V" class="latex" title="\Delta_{KL}(p||q) \approx \mathrm{Tr}(V^{-1}) - d + \ln \det V" />. The two last terms are often less significant. For example if <img src="https://s0.wp.com/latex.php?latex=V+%3D+%5Cepsilon%5E2+I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="V = \epsilon^2 I" class="latex" title="V = \epsilon^2 I" /> then <img src="https://s0.wp.com/latex.php?latex=%5Cdelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+d%2F%5Cepsilon%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta_{KL}(p||q) \approx d/\epsilon^2" class="latex" title="\delta_{KL}(p||q) \approx d/\epsilon^2" />.</p>



<h2>Technical digression 3: benchmarking generative models</h2>



<p>Given a distribution <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> of natural data and a purported generative model <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g" class="latex" title="g" />, how do we measure the quality of <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g" class="latex" title="g" />?</p>



<p>A natural measure is the KL divergence <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cg%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(p||g)" class="latex" title="\Delta_{KL}(p||g)" /> but it can be hard to evaluate, since it involves the term <img src="https://s0.wp.com/latex.php?latex=p%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(x)" class="latex" title="p(x)" /> which we cannot evaluate. However, we can rewrite the KL divergence as <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7D%28%5Clog+p%28x%29%29+-+%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7D%28%5Clog+q%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{x\sim p}(\log p(x)) - \mathbb{E}_{x\sim p}(\log q(x))" class="latex" title="\mathbb{E}_{x\sim p}(\log p(x)) - \mathbb{E}_{x\sim p}(\log q(x))" />. The term <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%7Bx%5Csim+p%7D+%5Clog+p%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}{x\sim p} \log p(x)" class="latex" title="\mathbb{E}{x\sim p} \log p(x)" /> is equal to <img src="https://s0.wp.com/latex.php?latex=-H%28p%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-H(p)" class="latex" title="-H(p)" /> where <img src="https://s0.wp.com/latex.php?latex=H%28p%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(p)" class="latex" title="H(p)" /> is the <em>entropy</em> of <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" />. The term <img src="https://s0.wp.com/latex.php?latex=-%5Cmathbb%7BE%7D_%7Bx+%5Csim+p%7D+%5Clog+q%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="-\mathbb{E}_{x \sim p} \log q(x)" class="latex" title="-\mathbb{E}_{x \sim p} \log q(x)" /> is known as the <em>cross entropy </em>of <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> and <img src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q" class="latex" title="q" />. Note that the cross-entropy of <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> and <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g" class="latex" title="g" /> is simply the expectation of the negative log likelihood of <img src="https://s0.wp.com/latex.php?latex=g%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g(x)" class="latex" title="g(x)" /> for <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> sampled from <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" />.</p>



<p>When <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> is fixed, minimizing <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cg%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(p||g)" class="latex" title="\Delta_{KL}(p||g)" /> corresponds to minimizing the cross entropy <img src="https://s0.wp.com/latex.php?latex=H%28p%2Cg%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(p,g)" class="latex" title="H(p,g)" /> or equivalently, maximizing the log likelihood. This is useful since often is the case that we can sample elements from <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> (e.g., natural images) but can only evaluate the probability function for <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g" class="latex" title="g" />. Hence a common metric in such cases is minimizing the cross-entropy / negative log likelihood <img src="https://s0.wp.com/latex.php?latex=H%28p%2Cg%29%3D+-%5Cmathbb%7BE%7D_%7Bx+sim+p%7D+%5Clog+g%28x%29+%3D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+p%7D+%5Clog+%281%2Fg%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(p,g)= -\mathbb{E}_{x sim p} \log g(x) = \mathbb{E}_{x \sim p} \log (1/g(x))" class="latex" title="H(p,g)= -\mathbb{E}_{x sim p} \log g(x) = \mathbb{E}_{x \sim p} \log (1/g(x))" />. For images, a common metric is “bits per pixel” which simply equals <img src="https://s0.wp.com/latex.php?latex=H%28p%2Cq%29%2Fd&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(p,q)/d" class="latex" title="H(p,q)/d" /> where <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> is the length of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />. Another metric (often used in natural language processing) is perplexity, which interchanges the expectation and the logarithm. The logarithm of the perplexity of <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g" class="latex" title="g" /> is <img src="https://s0.wp.com/latex.php?latex=-+%5Ctfrac%7B1%7D%7Bd%7D%5Clog+%5Cmathbb%7BE%7D_%7Bx+%5Csim+p%7D+g%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="- \tfrac{1}{d}\log \mathbb{E}_{x \sim p} g(x)" class="latex" title="- \tfrac{1}{d}\log \mathbb{E}_{x \sim p} g(x)" /> where <img src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d" class="latex" title="d" /> is the length of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> (e.g., in tokens). Another way to write this is that log of the perplexity is the average of <img src="https://s0.wp.com/latex.php?latex=%5Clog+g%28x_i%7Cx%7B%3Ci%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log g(x_i|x{&lt;i})" class="latex" title="\log g(x_i|x{&lt;i})" /> where <img src="https://s0.wp.com/latex.php?latex=g%28x_i%7Cx_%7B%3Ci%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g(x_i|x_{&lt;i})" class="latex" title="g(x_i|x_{&lt;i})" /> is the probability of <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_i" class="latex" title="x_i" /> under <img src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="g" class="latex" title="g" /> conditioned on the first <img src="https://s0.wp.com/latex.php?latex=i-1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i-1" class="latex" title="i-1" /> parts of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />.</p>



<p><strong>Memorization for log-likelihood.</strong> The issue of “overfitting” is even more problematic for generative models than for classifiers. Given samples <img src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_1,\ldots,x_n" class="latex" title="x_1,\ldots,x_n" /> and enough parameters, we can easily come up with a model corresponding to the uniform distribution <img src="https://s0.wp.com/latex.php?latex=%7B+x_1%2C%5Cldots%2C+x_n+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="{ x_1,\ldots, x_n }" class="latex" title="{ x_1,\ldots, x_n }" />. This is obviously a useless model that will never generate new examples. However, this model will not only get a large log likelihood value on the training set, in fact, it will get <em>even better log likelihood</em> than the true distribution! For example, any reasonable natural distribution on images would have at least tens of millions, if not billions or trillions of potential images. In contrast, a typical training set might have fewer than 1M samples. Hence, unlike in the classification setting, for generation, the “overfitting” model will not only match but can, in fact, beat the ground truth. (This is reminiscent of the following quote from <a href="https://etc.usf.edu/lit2go/86/peter-pan/1602/chapter-12-the-children-are-carried-off/">Peter and Wendy</a>: <em>“Not a sound is to be heard, save when they give vent to a wonderful imitation of the lonely call of the coyote. The cry is answered by other braves; and some of them do it even better than the coyotes, who are not very good at it.”</em>)</p>



<p>If we cannot compute the density function, then benchmarking becomes more difficult. What often happens in practice is an “I know it when I see it” approach. The paper includes a few pictures generated by the model, and if the pictures look realistic, we think it is a good model. However, this can be deceiving. After all, we are feeding in good pictures into the model, so generating a good photo may not be particularly hard (e.g. the model might memorize some good pictures and use those as outputs).</p>



<p>There is another metric called the <em>inception score</em>, which loosely corresponds to how similar the “inception” neural network finds the GAN model to ImageNet (in the sense that inception thinks it covers many of the ImageNet classes and that produces images on which inception has high confidence)  but it too has its problems. <a href="https://arxiv.org/pdf/1905.10887.pdf">Ravuri-Vinyalis 2019</a> used a GAN model with a good inception score used its outputs to train a different model on ImageNet. Despite the high inception score (which should have indicated that the GANs output are as good as ImageNets) the accuracy when training on the GAN output dropped from the original value of <img src="https://s0.wp.com/latex.php?latex=74%5C%25&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="74\%" class="latex" title="74\%" /> to as low as <img src="https://s0.wp.com/latex.php?latex=5%5C%25&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="5\%" class="latex" title="5\%" />!  (Even in the best case, accuracy dropped by at least 30 points.) Compare this with the  11-14% drop when we train on ImageNet and test on <a href="https://arxiv.org/abs/1902.10811">ImageNet v2</a>.</p>



<p>This figure from <a href="https://arxiv.org/abs/1701.00160">Goodfellow’s tutorial</a> describes generative models where we know and don’t know how to compute the density function:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/3hVJBPl.png" alt="" /></figure>



<h1>Auto Encoder / Decoder</h1>



<p>We now shift our attention to the encoder/decoder architecture mentioned above.</p>



<p>Recall that we want to understand <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" />, generate new elements <img src="https://s0.wp.com/latex.php?latex=x%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x^*" class="latex" title="x^*" />, and find a good representation of the elements. Our dream is to solve all of the issues with auto encoder/decoder, whose setup is as follows:</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/udIY089.png" alt="Setup for Auto Encoder/Decoder" /></figure>



<p>That is, we want <img src="https://s0.wp.com/latex.php?latex=E%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D%5Er&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E:\mathbb{R}^d \rightarrow \mathbb{R}^r" class="latex" title="E:\mathbb{R}^d \rightarrow \mathbb{R}^r" />, <img src="https://s0.wp.com/latex.php?latex=D%3A%5Cmathbb%7BR%7D%5Er+%5Crightarrow+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D:\mathbb{R}^r \rightarrow \mathbb{R}^d" class="latex" title="D:\mathbb{R}^r \rightarrow \mathbb{R}^d" /> such that</p>



<ul><li><img src="https://s0.wp.com/latex.php?latex=D%28E%28x%29%29+%5Capprox+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D(E(x)) \approx x" class="latex" title="D(E(x)) \approx x" /></li><li>The representation <img src="https://s0.wp.com/latex.php?latex=E%2CD&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E,D" class="latex" title="E,D" /> enables us to solve tasks such as generation, classification, etc..</li></ul>



<p>To each the first point, we can aim to minimize <img src="https://s0.wp.com/latex.php?latex=%5Csum_i+%7C%7Cx_i+-+D%28E%28x_i%29%29%7C%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sum_i ||x_i - D(E(x_i))||^2" class="latex" title="\sum_i ||x_i - D(E(x_i))||^2" />. However, we can of course, make this loss zero by letting <img src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E" class="latex" title="E" /> and <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" title="D" /> be the identity function. Much of the framework of generative models can be considered as placing some restrictions on the “communication channel” that rule out this trivial approach, with the hope that would require the encoder and decoder to “intelligently” correspond to the structure of the natural data.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/XW6cDE7.png" alt="" /></figure>



<h2>Auto Encoders: noiseless short <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /></h2>



<p>A natural idea is to simply restrict the dimension of the latent space to be small (<img src="https://s0.wp.com/latex.php?latex=r+%5Cll+d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r \ll d" class="latex" title="r \ll d" />). In principle, the optimal compression scheme for a probability distribution will require knowing the distribution. Moreover, the optimal compression will maximize the entropy of the latent data <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" />. Since the maximum entropy distribution is uniform (in the discrete case), we could easily sample from it. (In the continuous setting, the standard normal distribution plays the role of the uniform distribution.)</p>



<p>For starter, consider the case of picking <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r" class="latex" title="r" /> to be small and minimizing <img src="https://s0.wp.com/latex.php?latex=%5Csum+%7C%7Cx_i+-+D%28E%28x_i%29%29%7C%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\sum ||x_i - D(E(x_i))||^2" class="latex" title="\sum ||x_i - D(E(x_i))||^2" /> for <em>linear</em> <img src="https://s0.wp.com/latex.php?latex=E%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D%5Er&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E:\mathbb{R}^d \rightarrow \mathbb{R}^r" class="latex" title="E:\mathbb{R}^d \rightarrow \mathbb{R}^r" />, <img src="https://s0.wp.com/latex.php?latex=D%3A%5Cmathbb%7BR%7D%5Er+%5Crightarrow+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D:\mathbb{R}^r \rightarrow \mathbb{R}^d" class="latex" title="D:\mathbb{R}^r \rightarrow \mathbb{R}^d" />. Since <img src="https://s0.wp.com/latex.php?latex=DE&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="DE" class="latex" title="DE" /> is a rank <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r" class="latex" title="r" /> matrix, we can write this as finding a rank <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r" class="latex" title="r" /> matrix <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L" class="latex" title="L" /> that minimizes <img src="https://s0.wp.com/latex.php?latex=%7C+%28I-L%29X%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="| (I-L)X|^2" class="latex" title="| (I-L)X|^2" /> where <img src="https://s0.wp.com/latex.php?latex=X+%3D+%28x_1%2C%5Cldots%2Cx_n%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="X = (x_1,\ldots,x_n)" class="latex" title="X = (x_1,\ldots,x_n)" /> is our input data. It can be shown that <img src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L" class="latex" title="L" /> that would minimize this will be the projection to the top <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r" class="latex" title="r" /> eigenvectors of <img src="https://s0.wp.com/latex.php?latex=XX%5E%5Ctop&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="XX^\top" class="latex" title="XX^\top" /> which exactly corresponds to <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis (PCA)</a>.</p>



<p>In the nonlinear case, we can obtain better compression. However, we do not achieve our other goals:</p>



<ul><li>It is not the case that we can generate realistic data by sampling uniform/normal <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> and output <img src="https://s0.wp.com/latex.php?latex=D%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D(z)" class="latex" title="D(z)" /></li><li>It is not the case that semantic similarity between <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> and <img src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x'" class="latex" title="x'" /> corresponds to large dot product between <img src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E(x)" class="latex" title="E(x)" /> and <img src="https://s0.wp.com/latex.php?latex=E%28x%27%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E(x')" class="latex" title="E(x')" />.</li></ul>



<p>It seems that model just rediscovers a compression algorithm like JPEG. We do not expect the JPEG encoding of an image to be semantically informative, and JPEG decoding of a random file will not be a good way to generate realistic images. It turns out that sometimes “Murphy’s law” does hold and if it’s possible to minimize the loss in a not very useful way then that will indeed be the case.</p>



<h2>Variational Auto Encoder (VAE)</h2>



<p>We now discuss <em>variational auto encoders</em> (VAEs). We can think of these as generalization auto-encoders to the case where the channel has some Gaussian noise. We will describe VAEs in two nearly equivalent ways:</p>



<ul><li>We can think of VAEs as trying to optimize two objectives: both the auto-encoder objective of minimizing <img src="https://s0.wp.com/latex.php?latex=%7C+D%28E%28x%29%29-x%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="| D(E(x))-x|^2" class="latex" title="| D(E(x))-x|^2" /> and another objective of minimizing the KL divergence between <img src="https://s0.wp.com/latex.php?latex=D%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D(x)" class="latex" title="D(x)" /> and the standard normal distribution <img src="https://s0.wp.com/latex.php?latex=N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(0,I)" class="latex" title="N(0,I)" />.</li><li>We can think of VAEs as trying to maximize a proxy for the log-likelihood. This proxy is a quantity known as the “Evidence Lower Bound (ELBO)” which we can evaluate using <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" title="D" /> and <img src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E" class="latex" title="E" /> and is always smaller or equal to the log-likelihood.</li></ul>



<p>We start with the first description. One view of VAEs is that we search for a pair <img src="https://s0.wp.com/latex.php?latex=E%2CD&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E,D" class="latex" title="E,D" /> of encoder and decoder that are aimed at minimizing the following two objectives:</p>



<ul><li><img src="https://s0.wp.com/latex.php?latex=%7C+x+-+D%28E%28x%29%29%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="| x - D(E(x))|^2" class="latex" title="| x - D(E(x))|^2" /> (standard AE objective)</li><li><img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28+E%28x%29+%7C%7C+N%280%2CI%29+%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}( E(x) || N(0,I) )" class="latex" title="\Delta_{KL}( E(x) || N(0,I) )" /> (distance of latent from the standard normal)</li></ul>



<p>To make the second term a function of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />, we consider <img src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E(x)" class="latex" title="E(x)" /> as a probability distribution with respect to a <em>fixed</em> <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />. To ensure this makes sense, we need to make <img src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E" class="latex" title="E" /> <em>randomized</em>. A randomized Neural network has “sampling neurons” that take no input, have parameters <img src="https://s0.wp.com/latex.php?latex=%5Cmu%2C%5Csigma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu,\sigma" class="latex" title="\mu,\sigma" /> and produce an element <img src="https://s0.wp.com/latex.php?latex=v+%5Csim+N%28%5Cmu%2C%5Csigma%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="v \sim N(\mu,\sigma^2)" class="latex" title="v \sim N(\mu,\sigma^2)" />. We can train such a network by fixing a random <img src="https://s0.wp.com/latex.php?latex=t+%5Csim+N%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="t \sim N(0,1)" class="latex" title="t \sim N(0,1)" /> and defining the neuron to simply output <img src="https://s0.wp.com/latex.php?latex=%5Cmu+%2B+%5Csigma+t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mu + \sigma t" class="latex" title="\mu + \sigma t" />.</p>



<p><strong>ELBO derivation:</strong> Another view of VAEs is that they aim at maximizing a term known as the evidence lower bound or ELBO. We start by deriving this bound. Let <img src="https://s0.wp.com/latex.php?latex=Z%3DN%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z=N(0,I)" class="latex" title="Z=N(0,I)" /> be the standard normal distribution over the latent space. Define <img src="https://s0.wp.com/latex.php?latex=p_x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_x" class="latex" title="p_x" /> to be the distribution of <img src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z" class="latex" title="Z" /> conditioned on <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> decoding to <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> (i.e., <img src="https://s0.wp.com/latex.php?latex=Z%3D+z%5Csim+Z%7CD%28z%29%3Dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z= z\sim Z|D(z)=x" class="latex" title="Z= z\sim Z|D(z)=x" />, and define <img src="https://s0.wp.com/latex.php?latex=q_x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q_x" class="latex" title="q_x" /> be the distribution <img src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E(x)" class="latex" title="E(x)" />. Since <img src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28q_x%7C%7Cp_x%29+%5Cgeq+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Delta_{KL}(q_x||p_x) \geq 0" class="latex" title="\Delta_{KL}(q_x||p_x) \geq 0" />, we know that</p>



<p><img src="https://s0.wp.com/latex.php?latex=0+%5Cleq+-H%28q_x%29-+%5Cmathbb%7BE%7D_%7Bz+%5Csim+q_x%7D+%5Clog+p_x%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0 \leq -H(q_x)- \mathbb{E}_{z \sim q_x} \log p_x(z)" class="latex" title="0 \leq -H(q_x)- \mathbb{E}_{z \sim q_x} \log p_x(z)" /></p>



<p>By the definition of <img src="https://s0.wp.com/latex.php?latex=p_x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_x" class="latex" title="p_x" />, <img src="https://s0.wp.com/latex.php?latex=p_x%28z%29+%3D+%5CPr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D+%2F+%5CPr%5BD%28Z%29%3Dx%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p_x(z) = \Pr[ Z=z \;\wedge\; D(z)=x ] / \Pr[D(Z)=x]" class="latex" title="p_x(z) = \Pr[ Z=z \;\wedge\; D(z)=x ] / \Pr[D(Z)=x]" />. Hence we can derive that</p>



<p><img src="https://s0.wp.com/latex.php?latex=0+%5Cleq+-H%28q_x%29+-+%5Cmathbb%7BE%7D_%7Bz+%5Csim+q_x%7D+%5Clog+%5CPr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D+%2B+%5Clog+%5CPr%5B+D%28Z%29%3Dx%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="0 \leq -H(q_x) - \mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ] + \log \Pr[ D(Z)=x]" class="latex" title="0 \leq -H(q_x) - \mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ] + \log \Pr[ D(Z)=x]" /><br />(since <img src="https://s0.wp.com/latex.php?latex=%5CPr%5B+D%28Z%29%3Dx%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\Pr[ D(Z)=x]" class="latex" title="\Pr[ D(Z)=x]" /> depends only on <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />, given that <img src="https://s0.wp.com/latex.php?latex=Z%3DN%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z=N(0,I)" class="latex" title="Z=N(0,I)" />.)</p>



<p>Rearranging, we see that</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Clog+Pr%5B+D%28Z%29%3Dx%5D+%5Cgeq+%5Cmathbb%7BE%7D_%7Bz+%5Csim+q_x%7D+%5Clog+%5CPr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D+%2B+H%28q_x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log Pr[ D(Z)=x] \geq \mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ] + H(q_x)" class="latex" title="\log Pr[ D(Z)=x] \geq \mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ] + H(q_x)" /></p>



<p>or in other words, we have the following theorem:</p>



<p><strong>Theorem (ELBO):</strong> For every (possibly randomized) maps <img src="https://s0.wp.com/latex.php?latex=E%3A%5Cmathcal%7BX%7D+%5Crightarrow+%5Cmathcal%7BZ%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E:\mathcal{X} \rightarrow \mathcal{Z}" class="latex" title="E:\mathcal{X} \rightarrow \mathcal{Z}" /> and <img src="https://s0.wp.com/latex.php?latex=D%3A%5Cmathcal%7BZ%7D+%5Crightarrow+%5Cmathcal%7BX%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D:\mathcal{Z} \rightarrow \mathcal{X}" class="latex" title="D:\mathcal{Z} \rightarrow \mathcal{X}" />, distribution <img src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z" class="latex" title="Z" /> over <img src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BZ%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathcal{Z}" class="latex" title="\mathcal{Z}" /> and <img src="https://s0.wp.com/latex.php?latex=x%5Cin+%5Cmathcal%7BX%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x\in \mathcal{X}" class="latex" title="x\in \mathcal{X}" />,</p>



<p><img src="https://s0.wp.com/latex.php?latex=%5Clog+%5CPr%5B+D%28Z%29%3Dx%5D+%5Cgeq+%5CPr_%7Bz+%5Csim+E%28x%29%2C+z%27++%5Csim+Z%7D%5B+D%28z%29+%3D+x+%5Cwedge+z%3Dz%27+%5D+%2B+H%28E%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log \Pr[ D(Z)=x] \geq \Pr_{z \sim E(x), z'  \sim Z}[ D(z) = x \wedge z=z' ] + H(E(x))" class="latex" title="\log \Pr[ D(Z)=x] \geq \Pr_{z \sim E(x), z'  \sim Z}[ D(z) = x \wedge z=z' ] + H(E(x))" /></p>



<p>The left-hand side of this inequality is simply the log-likelihood of <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" />. The right-hand side (which, as the inequality shows, is always smaller or equal to it) is known as the <em>evidence lower bound</em> or ELBO. We can think of VAEs as trying to maximize the ELBO.</p>



<p>The reason that the two views are roughly equivalent is the follows:</p>



<ul><li>The first term of the ELBO, known as the <em>reconstruction term</em>, is <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bz+%5Csim+q_x%7D+%5Clog+%5CPr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ]" class="latex" title="\mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ]" /> if we assume some normal noise, then the probabiility taht <img src="https://s0.wp.com/latex.php?latex=D%28z%29%3Dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D(z)=x" class="latex" title="D(z)=x" /> will be proportional to <img src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%7Cx-D%28z%29%7C%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\exp(-|x-D(z)|^2)" class="latex" title="\exp(-|x-D(z)|^2)" /> since for <img src="https://s0.wp.com/latex.php?latex=q_x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q_x" class="latex" title="q_x" />, <img src="https://s0.wp.com/latex.php?latex=z%3DE%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z=E(x)" class="latex" title="z=E(x)" /> we get that <img src="https://s0.wp.com/latex.php?latex=%5Clog+Pr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D+%5Capprox+-%7C+x-+D%28E%28x%29%29%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\log Pr[ Z=z \;\wedge\; D(z)=x ] \approx -| x- D(E(x))|^2" class="latex" title="\log Pr[ Z=z \;\wedge\; D(z)=x ] \approx -| x- D(E(x))|^2" /> and hence maximizing this term corresponds to minimizing the square distance.</li><li>The second term of the ELBO, known as the <em>divergence term</em>, is <img src="https://s0.wp.com/latex.php?latex=H%28q_x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="H(q_x)" class="latex" title="H(q_x)" /> which is roughly equal to <img src="https://s0.wp.com/latex.php?latex=r+-%5CDelta_%7BKL%7D%28q_x%7C%7CN%280%2CI%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r -\Delta_{KL}(q_x||N(0,I))" class="latex" title="r -\Delta_{KL}(q_x||N(0,I))" />, where <img src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="r" class="latex" title="r" /> is the dimension of the latent space. Hence maximizing this term corresponds to minimizing the KL divergence between <img src="https://s0.wp.com/latex.php?latex=q_x%3DE%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="q_x=E(x)" class="latex" title="q_x=E(x)" /> and the standard normal distribution.</li></ul>



<p>How well does VAE work? First of all, we can actually generate images using them. We also find that similar inputs will have similar encodings, which is good. However, sometimes VAEs can still “cheat” (as in auto encoders). There is a risk that the learned model will split <img src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="Z" class="latex" title="Z" /> to two parts of the form <img src="https://s0.wp.com/latex.php?latex=%28N%280%2CI%29%2C+JPEG%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(N(0,I), JPEG(x))" class="latex" title="(N(0,I), JPEG(x))" />. The first part of the data is there to minimize divergence, while the second part is there for reconstruction. Such a model is similarly uninformative.</p>



<p>However, VAEs have found practical success. For example, <a href="https://arxiv.org/pdf/1610.00291.pdf">Hou et. al 2016</a> used VAE to create an encoding where two dimensions seem to correspond to “sunglasses” and “blondness”, as illustrated below. We do note that “sunglasses” and “blondness” are somewhere between “semantic” and “syntactic” attributes. They do correspond to relatively local changes in “pixel space”.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/O48nnWB.jpg" alt="VAE Example 1" /></figure>



<p>The picture can be blurry because of the noise we injected to make <img src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E" class="latex" title="E" /> random. However, recent models have used new techniques (e.g. <a href="https://arxiv.org/abs/1906.00446">vector quantized VAE</a> and <a href="https://arxiv.org/abs/2007.03898">hierarchical VAE</a>) to resolve the blurriness and significantly improve on state of art.</p>



<h2>Flow Models</h2>



<p>In a flow model, we flip the order of <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" title="D" /> and <img src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E" class="latex" title="E" /> and set <img src="https://s0.wp.com/latex.php?latex=E%3DD%5E%7B-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E=D^{-1}" class="latex" title="E=D^{-1}" /> (so <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" title="D" /> must be invertible). The input <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> to <img src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="D" class="latex" title="D" /> will come from the standard normal distribution <img src="https://s0.wp.com/latex.php?latex=N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="N(0,I)" class="latex" title="N(0,I)" />. The idea is that we obtain <img src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="E" class="latex" title="E" /> by a composition of simple invertible functions. We use the fact that if we can compute the density function of a distribution <img src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p" class="latex" title="p" /> over <img src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\mathbb{R}^d" class="latex" title="\mathbb{R}^d" /> and <img src="https://s0.wp.com/latex.php?latex=f%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f:\mathbb{R}^d \rightarrow \mathbb{R}^d" class="latex" title="f:\mathbb{R}^d \rightarrow \mathbb{R}^d" /> is invertible and differentiable, then we can compute the density function of <img src="https://s0.wp.com/latex.php?latex=f%5Ccirc+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f\circ p" class="latex" title="f\circ p" /> (i.e., the distribution obtained by sampling <img src="https://s0.wp.com/latex.php?latex=w+%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="w \sim p" class="latex" title="w \sim p" /> and outputting <img src="https://s0.wp.com/latex.php?latex=f%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f(w)" class="latex" title="f(w)" />). To see why this is the case, consider the setting when <img src="https://s0.wp.com/latex.php?latex=d%3D2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="d=2" class="latex" title="d=2" /> and a small <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%5Ctimes+%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta \times \delta" class="latex" title="\delta \times \delta" /> rectangle <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" title="A" />. If <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta" class="latex" title="\delta" /> is small enough, <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> will be roughly linear and hence will map <img src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="A" class="latex" title="A" /> into a parallelogram <img src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="B" class="latex" title="B" />. Shifting the <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x" class="latex" title="x" /> coordinate by <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta" class="latex" title="\delta" /> corresponds to shifting the output of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> by the vector <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%28%5Ctfrac%7Bd+f_x%7D%7Bdx%7D%2C+%5Ctfrac%7Bd+f_y%7D%7Bdx%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta (\tfrac{d f_x}{dx}, \tfrac{d f_y}{dx})" class="latex" title="\delta (\tfrac{d f_x}{dx}, \tfrac{d f_y}{dx})" /> and shifting the <img src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="y" class="latex" title="y" /> coordinate by <img src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta" class="latex" title="\delta" /> corresponds to shifting the output of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> by the vector <img src="https://s0.wp.com/latex.php?latex=%5Cdelta+%28%5Ctfrac%7Bd+f_x%7D%7Bdy%7D%2C+%5Ctfrac%7Bd+f_y%7D%7Bdy%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\delta (\tfrac{d f_x}{dy}, \tfrac{d f_y}{dy})" class="latex" title="\delta (\tfrac{d f_x}{dy}, \tfrac{d f_y}{dy})" />. For every <img src="https://s0.wp.com/latex.php?latex=z+%5Cin+B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z \in B" class="latex" title="z \in B" />, the density of <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> under <img src="https://s0.wp.com/latex.php?latex=f%5Ccirc+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f\circ p" class="latex" title="f\circ p" /> will be proportional to the density of <img src="https://s0.wp.com/latex.php?latex=f%5E%7B-1%7D%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f^{-1}(z)" class="latex" title="f^{-1}(z)" /> with the proportionality fector being <img src="https://s0.wp.com/latex.php?latex=vol%28A%29%2Fvol%28B%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="vol(A)/vol(B)" class="latex" title="vol(A)/vol(B)" />.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/pet8tBU.png" alt="" /></figure>



<p>Overall we the density of <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /> under <img src="https://s0.wp.com/latex.php?latex=f+%5Ccirc+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f \circ p" class="latex" title="f \circ p" /> will equal <img src="https://s0.wp.com/latex.php?latex=p%28f%5E%7B-1%7D%28z%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="p(f^{-1}(z))" class="latex" title="p(f^{-1}(z))" /> times the inverse determinant of the <em>Jacobian</em> of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> at the point <img src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="z" class="latex" title="z" /></p>



<figure class="wp-block-image"><img src="https://i.imgur.com/ItOPqSX.png" alt="" /></figure>



<p>There are different ways to compose together simple reversible functions to compute a complex one. Indeed, this issue also arises in cryptography and quantum computing (e.g., the <a href="https://en.wikipedia.org/wiki/Feistel_cipher">Fiestel cipher</a>). Using similar ideas, it is not hard to show that any probability distribution can be approximated by a (sufficiently big) combination of simple reversible functions.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/AlYrWJx.png" alt="" /></figure>



<p>In practice, we have some recent succcessful flow models. A few examples of these models are in the lecture slides.</p>



<h1>Giving up on the dream</h1>



<p>In section 2, we had a dream of doing both representation and generation at once. So far, we have not been able to find success with these models. What if we do each goal separately?</p>



<p>The tasks of representation becomes self-supervised learning with approaches such SIMCLR. The task of generation can be solved by GANs. Both areas have had recent success.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/D0CpobJ.jpg" alt="Model after we separate E and D" /></figure>



<p>Open-AI <a href="https://openai.com/blog/clip">CLIP</a> and <a href="https://openai.com/blog/dall-e/">DALL-E</a> is a pair of models that perform each part of these tasks well, and suggest an approach to merge them.<br />CLIP does representation for both texts and images where the two encoders are aligned, i.e. <img src="https://s0.wp.com/latex.php?latex=%5Clangle+E%28%5Ctext%7B%27cat%27%7D%29%2C+E%28%5Ctext%7Bimg+of+cat%29%7D%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="\langle E(\text{'cat'}), E(\text{img of cat)}\rangle" class="latex" title="\langle E(\text{'cat'}), E(\text{img of cat)}\rangle" /> is large. DALL-E, given some text, generates an image corresponding to the text. Below are images generated by DALL-E when asked for an armchair in the shape of an avocado.</p>



<figure class="wp-block-image"><img src="https://i.imgur.com/ZcsHXKE.png" alt="DALL-E Example" /></figure>



<h2>Contrastive learning</h2>



<p>The general approach used in CLIP is called contrastive learning.</p>



<p>Suppose we have some representation function <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> and inputs <img src="https://s0.wp.com/latex.php?latex=u_i%2Cv_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u_i,v_i" class="latex" title="u_i,v_i" /> which represent similar objects. Let <img src="https://s0.wp.com/latex.php?latex=M_%7Bi%2Cj%7D%3Df%28u_i%5Ccdot+v_j%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="M_{i,j}=f(u_i\cdot v_j)" class="latex" title="M_{i,j}=f(u_i\cdot v_j)" />, then we want <img src="https://s0.wp.com/latex.php?latex=M_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="M_{i,j}" class="latex" title="M_{i,j}" /> to be large when <img src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i=j" class="latex" title="i=j" />, but small when <img src="https://s0.wp.com/latex.php?latex=i%5Cneq+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="i\neq j" class="latex" title="i\neq j" />. So, let the loss function be <img src="https://s0.wp.com/latex.php?latex=L%28M%29%3D%5Csum+M_%7Bi%2Ci%7D+%2F+%5Csum_%7Bi%5Cneq+j%7D+M_%7Bi%2Cj%7D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L(M)=\sum M_{i,i} / \sum_{i\neq j} M_{i,j}." class="latex" title="L(M)=\sum M_{i,i} / \sum_{i\neq j} M_{i,j}." /> How do we create similar <img src="https://s0.wp.com/latex.php?latex=u_i%2Cv_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u_i,v_i" class="latex" title="u_i,v_i" />? In SIMCLR, <img src="https://s0.wp.com/latex.php?latex=u_i%2Cv_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="u_i,v_i" class="latex" title="u_i,v_i" /> are augmentations of the same image <img src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="x_i" class="latex" title="x_i" />. In CLIP, <img src="https://s0.wp.com/latex.php?latex=%28u_i%2Cv_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="(u_i,v_i)" class="latex" title="(u_i,v_i)" /> is an image and a text that describes it.</p>



<p>CLIPs representation space does seem to have nice properties such as correspondence between semantic attributes and linear directions, which enables doing some “semantic linear algebra” on representations: (see this based on <a href="https://github.com/haltakov/natural-language-image-search">Vladimir Hatlakov’s code</a> – in the snippet below <code>tenc</code> maps text to its encoding/representation and <code>get_img</code> finds nearest image to representation in a the unsplash dataset):</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2021/02/image-6.png"><img src="https://windowsontheory.files.wordpress.com/2021/02/image-6.png?w=854" alt="" class="wp-image-8017" /></a></figure>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2021/02/image-7.png"><img src="https://windowsontheory.files.wordpress.com/2021/02/image-7.png?w=444" alt="" class="wp-image-8019" /></a></figure>



<h2>GANs</h2>



<p>The theory of GANs is currently not well-developed. As an objective, we want images that “look real” (which is not well defined), and we have no posterior distribution. If we just define the distribution based on real images, our GAN might memorize the photos to beat us.</p>



<p>However, we know that Neural Networks are good at discriminating real vs. fake images. So, we add in a discriminator <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="f" class="latex" title="f" /> and define the loss function <img src="https://s0.wp.com/latex.php?latex=L%28D%29+%3D+%5Cmax_%7Bf%3A%5Cmathbb+R%5Ed%5Cto+%5Cmathbb+R%7D+%7C%5Cmathbb%7BE%7D_%7B%5Chat+x%5Csim+D%28z%29%7Df%28%5Chat+x%29-%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7Df%28x%29%7C.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" alt="L(D) = \max_{f:\mathbb R^d\to \mathbb R} |\mathbb{E}_{\hat x\sim D(z)}f(\hat x)-\mathbb{E}_{x\sim p}f(x)|." class="latex" title="L(D) = \max_{f:\mathbb R^d\to \mathbb R} |\mathbb{E}_{\hat x\sim D(z)}f(\hat x)-\mathbb{E}_{x\sim p}f(x)|." /></p>



<p>The generator model and discriminator model form a 2-player game, which are often harder to train and very delicate. We typically train by changing a player’s action to the best response. However, we need to be careful if the two players have very different skill levels. They may be stuck in a setting where no change of strategies will make much difference, since the stronger player always dominates the weaker one. In particular in GANs we need to ensure that the generator is not cheating by using a degenerate distribution that still succeeds with respect to the discriminator.</p>



<p>If a 2-player model makes training more difficult, why do we use it? If we fix the discriminator, then the generator can find a picture that the discriminator thinks is real and only output that one, obtaining low loss. As a result, the discriminator needs to update along with the generator. This example also highlights that the discriminator’s job is often harder. To fix this, we have to somehow require the generator to give us good entropy.</p>



<p>Finally, how good are GANs in practice? Recently, we have had GANs that make great images as well as audios. For example, modern deepfake techniques often use GANs in their architecture. However, it is still unclear how rich the images are.</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2021/02/24/unsupervised-learning-and-generative-models/"><span class="datestr">at February 24, 2021 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=18187">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2021/02/24/a-quiz-of-quotes/">A Quiz of Quotes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Everything that can be invented has been invented—Charles Duell, Commissioner, U.S. Office of Patents, 1899</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/24/a-quiz-of-quotes/cantormathquote/" rel="attachment wp-att-18201"><img width="150" alt="" src="https://rjlipton.files.wordpress.com/2021/02/cantormathquote.jpg?w=150&amp;h=150" class="alignright wp-image-18201" height="150" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">MathQuotes <a href="https://www.facebook.com/mathsqoutes/posts/the-mathematician-does-not-study-pure-mathematics-because-it-is-useful-he-studie/144012037239958/">src</a></font></td>
</tr>
</tbody>
</table>
<p>
George Cantor has been featured <a href="https://rjlipton.wordpress.com/2014/07/31/the-cantor-bernstein-schroder-theorem/">here</a> and <a href="https://rjlipton.wordpress.com/2009/04/18/cantors-non-diagonal-proof/">here</a> and <a href="https://rjlipton.wordpress.com/2012/09/04/thinking-out-of-the-notation-box/">here</a> before on GLL. Of course, he invented modern set theory and changed math forever. His birthday is soon, so we thought we would talk about him now—he was born on March 3rd in 1845.</p>
<p>
Today we thought it might be fun to have a quiz on math quotes.<br />
<span id="more-18187"></span></p>
<p>
Wait. Cantor did not invent quotation marks, nor is he known for many quotes. He does of course have many famous results, and they will live forever. But his results were subject to immediate horrible criticism and therefore memorable quotes. </p>
<p>Leopold Kronecker was a particular source of barbs.  For example: “What good is your beautiful proof on the transcendence of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{\pi}" class="latex" />? Why investigate such problems, given that irrational numbers do not even exist?” </p>
<p>
As a complexity theorist I must say that Kronecker has a point when he also said: </p>
<blockquote><p><b> </b> <em> “Definitions must contain the means of reaching a decision in a finite number of steps, and existence proofs must be conducted so that the quantity in question can be calculated with any degree of accuracy.” </em>
</p></blockquote>
<p>David Hilbert defended Cantor and said: “No one shall expel us from the paradise that Cantor has created.”</p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/24/a-quiz-of-quotes/cantor/" rel="attachment wp-att-18190"><img width="600" alt="" src="https://rjlipton.files.wordpress.com/2021/02/cantor.png?w=600&amp;h=205" class="aligncenter wp-image-18190" height="205" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">BBVA Open Mind <a href="https://www.bbvaopenmind.com/en/science/mathematics/georg-cantor-the-man-who-discovered-different-infinities/">src</a>
</font></td>
</tr>
</tbody></table>
<p></p><h2> Quotes Quiz </h2><p></p>
<p></p><p>
On to the quiz. Each quote is followed by two possible authors in alphabetical order. You should pick the one you think is correct. The players are: </p>
<blockquote><p><b> </b> <em> 1. Douglas Adams  2. Bernard Baruch  3. Eric Temple Bell  4. Raoul Bott<br />
5. Paul Erdős  6. Richard Hamming  7. Godfrey Hardy  8. David Hilbert<br />
9. Admiral Grace Hooper  10. Alan Kay  11. Donald Knuth  12. John von Neumann<br />
13. Alan Perlis  14. Henri Poincaré  15. Srinivasa Ramanujan  16. Marcus du Sautoy<br />
17. Raymond Smullyan  18. Alan Turing  19. Moshe Vardi  20. Andrew Wiles<br />
</em>
</p></blockquote>
<p>
</p><ol>
<p></p><li>
Those who can imagine anything, can create the impossible.<br />
—Kay <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" /> Turing<p></p>
<p></p></li><li>
I really didn’t foresee the Internet. But then, neither did the computer industry. Not that that tells us very much of course–the computer industry didn’t even foresee that the century was going to end.<br />
— Adams <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" /> Knuth<p></p>
<p></p></li><li>
One man’s constant is another man’s variable.<br />
—Perlis <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" /> du Sautoy<p></p>
<p></p></li><li>
The most damaging phrase in the language is: “It’s always been done that way.”<br />
—-Hopper <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" /> Perlis<p></p>
<p></p></li><li>
The best way to predict the future is to invent it.<br />
—Kay <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" /> Turing<p></p>
<p></p></li><li>
The purpose of computing is insight, not numbers.<br />
Adams <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" /> Hamming<p></p>
<p></p></li><li>
Beware of bugs in the above code; I have only proved it correct, not tried it.<br />
—Knuth <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" /> Vardi<p></p>
<p></p></li><li>
No, it is a very interesting number, it is the smallest number expressible as a sum of two cubes in two different ways.<br />
—Bell <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" /> Ramanujan <p></p>
<p></p></li><li>
Beauty is the first test: there is no permanent place in the world for ugly mathematics. <br />
—Erdős <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" /> Hardy<p></p>
<p></p></li><li>
Mathematics is the art of giving the same name to different things. <br />
—Hooper <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" /> Poincaré <p></p>
<p></p></li><li>
There’s no sense in being precise when you don’t even know what you’re talking about.<br />
—Bott <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" /> von Neumann<p></p>
<p></p></li><li>
I hope we’ll be able to solve these problems before we leave. <br />
—Erdős <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" /> Perlis<p></p>
<p></p></li><li>
Some people are always critical of vague statements. I tend rather to be critical of precise statements; they are the only ones which can correctly be labeled ‘wrong’. <br />
—Knuth <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" /> Smullyan<p></p>
<p></p></li><li>
Everything that humans can do a machine can do. <br />
—Perlis <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" /> Vardi<p></p>
<p></p></li><li>
“Obvious” is the most dangerous word in mathematics.<br />
— Bell <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" /> Hooper<p></p>
<p></p></li><li>
Just because we can’t find a solution, it doesn’t mean there isn’t one.<br />
— Adams <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" /> Wiles<p></p>
<p></p></li><li>
Mathematics is a place where you can do things which you can’t do in the real world.<br />
— du Sautoy <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" /> Turing<p></p>
<p></p></li><li>
Millions saw the apple fall, but Newton asked why.<br />
— Baruch <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" /> Hopper<p></p>
<p></p></li><li>
The definition of a good mathematical problem is the mathematics it generates rather than the problem itself.<br />
— Hilbert <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" /> Wiles<p></p>
<p></p></li><li>
There are two ways to do great mathematics. The first is to be smarter than everybody else. The second way is to be stupider than everybody else – but persistent.<br />
— Bott <img src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" alt="{||}" class="latex" /> Knuth<p></p>
</li></ol>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
“I always have a quotation for everything—it saves original thinking.”<br />
—Dorothy Sayers</p>
<p>
Here are the answers:</p>
<p><a href="https://rjlipton.wordpress.com/2021/02/24/a-quiz-of-quotes/ans/" rel="attachment wp-att-18194"><img width="600" alt="" src="https://rjlipton.files.wordpress.com/2021/02/ans.png?w=600&amp;h=702" class="alignright size-full wp-image-18194" height="702" /></a></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2021/02/24/a-quiz-of-quotes/"><span class="datestr">at February 24, 2021 05:44 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/027">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/027">TR21-027 |  Almost Optimal Super-Constant-Pass Streaming Lower Bounds for Reachability | 

	Lijie Chen, 

	Gillat Kol, 

	Dmitry Paramonov, 

	Raghuvansh Saxena, 

	Zhao Song, 

	Huacheng Yu</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We give an almost quadratic $n^{2-o(1)}$ lower bound on the space consumption of any $o(\sqrt{\log n})$-pass streaming algorithm solving the (directed) $s$-$t$ reachability problem. This means that any such algorithm must essentially store the entire graph. As corollaries, we obtain almost quadratic space lower bounds for additional fundamental problems, including maximum matching, shortest path, matrix rank, and linear programming.

Our main technical contribution is the definition and construction of set hiding graphs, that may be of independent interest: we give a general way of encoding a set $S \subseteq [k]$ as a directed graph with $n = k^{ 1 + o( 1 ) }$ vertices, such that deciding whether $i \in S$ boils down to deciding if $t_i$ is reachable from $s_i$, for a specific pair of vertices $(s_i,t_i)$ in the graph. Furthermore, we prove that our graph ``hides'' $S$, in the sense that no low-space streaming algorithm with a small number of passes can learn (almost) anything about $S$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/027"><span class="datestr">at February 24, 2021 02:18 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=5350">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=5350">Stop emailing my utexas address</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>A month ago, UT Austin changed its email policies—banning auto-forwarding from university accounts to Gmail accounts, apparently as a way to force the faculty and other employees to separate their work email from their personal email, and thereby comply with various government regulations.  Ever since that change, the email part of my life has been a <em>total, unmitigated disaster</em>.  I’ve missed (or been late to see) dozens of important work emails, with the only silver lining being that that’s arguably UT’s problem more than it is mine!</p>



<p>And yes, I’ve already gone to technical support; the only answer I’ve gotten is that (in so many words) there <em>is</em> no answer.  Other UT faculty are somehow able to deal with this because they are them; I am unable to deal with it because I am me.  As a mere PhD in computer science, I’m utterly unqualified to set up a technical fix for this sort of problem.</p>



<p>So the bottom line is: <strong>from now on, if you want me to see an email, send it to scott@scottaaronson.com</strong>.  Really.  If you try sending it to aaronson@cs.utexas.edu, it will land in a separate inbox that I can access only with great inconvenience.  And if, God forbid, you try sending it to aaronson@utexas.edu, the email will bounce and I’ll never see it at all.  Indeed, a central purpose of this post is just to have a place to point the people who contact me every day, shocked that their emails to me bounced.</p>



<p>This whole episode has given me <em>immense</em> sympathy for Hillary Clinton, and for the factors that led her to set up clintonemail.com from her house.  It’s not merely that her private email server was a laughably trivial reason to end the United States’ 240-year run of democratic government.  Rather it’s that, even on the narrow question of emails, I now feel certain that <em>Hillary was 100% right</em>.  Bureaucracy that impedes communication is a cancer on human civilization.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update:</span></strong> Thanks so much to commenter Avraham and to my colleague Etienne Vouga, who quickly gave me the crucial information that tech support would not, and thereby let me solve this problem.  I can once again easily read emails sent to aaronson@cs.utexas.edu … well, at least for now!  I’m now checking about aaronson@utexas.edu.  Again, though, <strong>scott@scottaaronson.com to be safe</strong>.</p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=5350"><span class="datestr">at February 23, 2021 09:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2021/026">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2021/026">TR21-026 |  Conditional Dichotomy of Boolean Ordered Promise CSPs | 

	Joshua Brakensiek, 

	Venkatesan Guruswami, 

	Sai Sandeep</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Promise Constraint Satisfaction Problems (PCSPs) are a generalization of Constraint Satisfaction Problems (CSPs) where each predicate has a strong and a weak form and given a CSP instance, the objective is to distinguish if the strong form can be satisfied vs. even the weak form cannot be satisfied. Since their formal introduction by Austrin, Guruswami, and Håstad, there has been a flurry of works on PCSPs, including recent breakthroughs in approximate graph coloring. The key tool in studying PCSPs is the algebraic framework developed in the context of CSPs where the closure properties of the satisfying solutions known as *polymorphisms* are analyzed. 
    
    The polymorphisms of PCSPs are significantly richer than CSPs---this is illustrated by the fact that even in the Boolean case, we still do not know if there exists a dichotomy result for PCSPs analogous to Schaefer's dichotomy result for CSPs. In this paper, we study a special case of Boolean PCSPs, namely Boolean *Ordered* PCSPs where the Boolean PCSPs have the predicate $x \leq y$. In the algebraic framework, this is the special case of Boolean PCSPs when the polymorphisms are *monotone* functions. We prove that Boolean Ordered PCSPs exhibit a computational dichotomy assuming the Rich $2$-to-$1$ Conjecture due to Braverman, Khot, and Minzer, which is a perfect completeness surrogate of the Unique Games Conjecture. 
    
    In particular, assuming the Rich $2$-to-$1$ Conjecture, we prove that a Boolean Ordered PCSP can be solved in polynomial time if for every $\epsilon &gt;0$, it has polymorphisms where each coordinate has *Shapley value* at most $\epsilon$, else it is NP-hard. The algorithmic part of our dichotomy result is based on a structural lemma showing that Boolean monotone functions with each coordinate having low Shapley value have arbitrarily large threshold functions as minors. The hardness part proceeds by showing that the Shapley value is consistent under a uniformly random $2$-to-$1$ minor. As a structural result of independent interest, we construct an example to show that the Shapley value can be inconsistent under an adversarial $2$-to-$1$ minor.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2021/026"><span class="datestr">at February 23, 2021 05:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
