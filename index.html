<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/?tag=tcs&amp;feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://www.blogger.com/feeds/27705661/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" class="message" title="403: forbidden">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://kintali.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kintali.wordpress.com" title="My Brain is Open">Shiva Kintali</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at June 08, 2019 09:23 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/06/07/little-knowledge-can">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/06/07/little-knowledge-can.html">A little knowledge can make the next step harder</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Suppose you have a fill-in-the-unknowns puzzle, like Sudoku. Can making some deductions and filling in those parts of the puzzle make the whole thing harder to solve than it was before you started? Sometimes, yes!</p>

<p>I have in mind human-style puzzle-deduction rules, where you see a piece of the puzzle that matches some pattern and use that to deduce what one of the unknowns should be. And by “harder” I mean that a puzzle that was previously possible to solve by some set of deduction rules, after the deduction, stopped being possible for those rules to solve. Of course this is possible if you have a bad set of deduction rules. But normally, at least for the kinds of patterns I think about when solving puzzles, if a pattern matches in a partially completed puzzle, then it or a simplification of it will continue to match no matter how I fill in more of the unknowns. Most of the deduction pattern that I use are monotonic, in this sense. If you had a collection of patterns that was not monotonic, you could add all ways of partially filling them in to your collection, and get a better set of patterns, right?</p>

<p>Wrong! There can be valid deduction patterns for which this extension to monotonic sets would produce invalid patterns. I’m pretty sure this can happen in Sudoku, actually, but the example I have in mind comes from a different puzzle game I’ve been playing lately, part of Simon Tatham’s puzzle collection, where it’s called “map”. It’s based on the problem of <a href="https://en.wikipedia.org/wiki/Precoloring_extension">precoloring extension</a>: you’re given a partially 4-colored planar map, and you have to fill in the rest of the colors.
And it’s trivially NP-complete, by a reduction from planar 3-coloring (augment a 3-coloring instance by extra vertices of the fourth color, preventing any of the given instance vertices from having that color) but the puzzles usually presented by the puzzle app are solvable by hand, even when they’re large and at the highest of its levels of difficulty.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/map-screenshot.png" alt="Screenshot of the map puzzle from Simon Tatham's puzzle collection" /></p>

<p>If that were all, then I think all deduction rules could be made monotonic. But I’m going to tell you one more thing about the puzzle, and this one thing makes it non-monotonic. It is that, like Sudoku, every puzzle has a unique solution.
And <a href="https://11011110.github.io/blog/2005/10/15/assuming-uniqueness-in.html">like Sudoku, the assumption of uniqueness leads to new deduction rules</a>.
You can infer that certain regions have to have certain colors, because if they could be colored anything else then there would be more than one solution.</p>

<p>To see how this works, suppose I had a map like the one shown below (where the white squares have not yet been colored, and I’m only showing a small piece of a larger puzzle):</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/nonmon1.svg" alt="Uncolored pocket in a map puzzle" /></p>

<p>There’s a pocket of uncolored squares extending into the colored region on the left. If I colored the square at the mouth of the pocket yellow, the inner square of the puzzle would be ambiguous: it has only yellow and blue neighbors, so it could be either red or black.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/nonmon2.svg" alt="Ambiguously colored pocket in a map puzzle" /></p>

<p>To prevent this ambiguity, the square at the mouth of the pocket must be black. And to force it to be black, the square one step beyond the mouth must be yellow. So from the initial state and the assumption of a unique solution, it’s possible to infer the colors of three previously-blank squares:</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/nonmon3.svg" alt="Ambiguously colored pocket in a map puzzle" /></p>

<p>But if I have multiple rules at hand, it’s natural for me to try the weaker and easier ones first. Suppose I had done this, and used a weaker inference rule telling me that the square at the mouth was black.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/nonmon4.svg" alt="Partially colored pocket in a map puzzle" /></p>

<p>Or suppose I had used a rule that produced the valid (but even weaker) inference that the inner square must be red.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/nonmon5.svg" alt="Even more partially colored pocket in a map puzzle" /></p>

<p>Now I can’t use my strong inference rule and color the outer squares! I’ve lost the information about why I colored the inner pocket squares the way I did, and so I’ve lost the ability to make deductions about how the outer squares should be colored to avoid ambiguities. It would not be a valid pattern to see a puzzle in these states and deduce the color of the remaining squares associated with the pocket. So the extension from my initial (valid) rule, which filled in all three squares when they were all blank, to a monotonic rule that fills in the partially-filled-in pocket in the same way, would be invalid. Of course, if the puzzle solution was unique before, it must still be unique after partially filling in the pocket. But with fewer squares colored, my deductive abilities might not be up to the task of reasoning from the remaining parts of the puzzle to its unique solution.</p>

<p>For the same reason, I might not actually want to color yellow the square beyond the mouth, forgetting why it needs to be yellow. Because what I can infer from the initial state is not merely that it should be colored yellow: it’s that the three outer neighbors of this square must have a permutation of the three other colors, so that this square is forced to be yellow, so that the rest of the pocket will have an unambiguous coloring.</p>

<p>I think what this means is that my knowledge representation (consisting only of blank or filled-in puzzle regions) is inadequate. In practice, I actually use a more complex knowledge representation where (either in my head or with markers provided in the puzzle app) I keep track of which colors are still available for the blank regions, but it’s still inadequate, in the same way. It’s not clear to me what the right knowledge representation is, to allow me to keep track of chains of inferences like “one of these squares must be red to prevent this square from becoming yellow to prevent its neighbor from becoming ambiguous” without the complexity of what I remember for each square blowing up to non-constant.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/102234384857906663">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/06/07/little-knowledge-can.html"><span class="datestr">at June 07, 2019 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:typepad.com,2003:post-6a00d83452383469e20240a48c6624200d">
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><br />
<br />
<a style="display: inline;" href="https://3dpancakes.typepad.com/.a/6a00d83452383469e20240a4b0ee61200b-pi" class="asset-img-link"> <img src="https://3dpancakes.typepad.com/.a/6a00d83452383469e20240a4b0ee61200b-800wi" alt="It-exists" border="0" class="asset  asset-image at-xid-6a00d83452383469e20240a4b0ee61200b image-full img-responsive" title="It-exists" /> </a><br /></p></div>







<p class="date">
by Jeff Erickson <a href="https://3dpancakes.typepad.com/ernie/2019/06/my-entry.html"><span class="datestr">at June 07, 2019 06:17 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15957">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/06/07/a-rank-problem/">A Rank Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>More on restricted quantum circuits</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/06/07/a-rank-problem/images-191/" rel="attachment wp-att-15961"><img src="https://rjlipton.files.wordpress.com/2019/06/images.png?w=600" alt="" class="alignright size-full wp-image-15961" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ The Daily Grail ]</font></td>
</tr>
</tbody>
</table>
<p>
Ken Regan just wrote about his paper with Chaowen Guan (GR). The <a href="https://arxiv.org/abs/1904.00101">paper</a> is titled, “Stabilizer Circuits, Quadratic Forms, and Computing Matrix Rank.”</p>
<p>
Today I thought I would add some additional comments on their result.</p>
<p>
The post Ken wrote is thorough, detailed, and geared for an expert in quantum computation. I think he did a service to the field of quantum complexity theory with his write-up. But I thought too that there some things that he neglected to say: things that could be interesting to a wider community. So with all due respect I hope this short discussion is useful.</p>
<p>
</p><p></p><h2> Quantum Circuits </h2><p></p>
<p></p><p>
In studying classic computations, we can restrict our attention to Boolean circuits. Moreover, these circuits can be assumed to consist of one type of gate. A formal way to say this is: Circuits that use NAND gates are <i>universal</i>. Any computation can be converted into a circuit that only uses this type of gate. It is now viewed as a trivial observation, but was not obvious in the beginning. Note, AND and OR and NOT can be implemented with just NAND gates.</p>
<p>
The understanding of what gate types are universal is not only of interest to theorists. Some technologies support directly some types of gates, while others support different types. For example the dominant CMOS technology efficiently implements NAND gates. </p>
<p></p><p></p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/06/07/a-rank-problem/cmos/" rel="attachment wp-att-15958"><img src="https://rjlipton.files.wordpress.com/2019/06/cmos.png?w=150&amp;h=300" alt="" width="150" class="aligncenter size-medium wp-image-15958" height="300" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ Wikipedia ]</font>
</td>
</tr>
</tbody></table>
<p>
In studying quantum computations, we can also restrict our attention to <i>quantum circuits</i>. These circuits need only use gates from a small family. These gate types are more complex than just NAND gates, as you might expect since quantum circuits manipulate quits and not just bits. I believe that it is fair to say that the fact that there are universal gates for quantum circuits is not trivial. There are many types of gates that are studied: Clifford, NOT, Fredkin, Hadamard, Toffoli, and many others.</p>
<p>
Again the understanding of what gate types are universal is not only of interest to theorists. Quantum circuits are not easy to build, the technology is still evolving. There is yet no CMOS answer to the question: How do we build quantum hardware? Errors may be the limiting factor for large quantum circuits. We will see. One consequence of the difficulty of building universal quantum circuits is the interest in circuits that use gates that are from a family that is <i>not universal</i>.</p>
<p>
The hope is several fold: </p>
<ul>
<li>
Perhaps understanding gates that are not universal will help us understand general quantum computations. <p></p>
</li><li>
Perhaps some important problems in chemistry, for example, may be solvable with these non-universal circuits. <p></p>
</li><li>
Perhaps the mathematics of these restricted circuits will be interesting in its own right.
</li></ul>
<p>
The third point is where GR’s work falls.</p>
<p>
</p><p></p><h2> Stabilizer Circuits </h2><p></p>
<p></p><p>
The class of quantum circuits that GR studied are the <i>stabilizer circuits</i>. See this for an earlier discussion on these <a href="https://rjlipton.wordpress.com/2012/07/08/grilling-quantum-circuits/">circuits</a>. The key is that these circuits are not universal. And more importantly they can be simulated by classical computers in polynomial time. In the upside-down world of quantum complexity theory, the ability to efficiently simulate them is bad. In order to show that quantum computations are new and exciting, being able to simulate them on your laptop is not good.</p>
<p>
The punch-line is: how efficient is this simulation? There are polynomial algorithms and there are useful polynomial algorithms. The 2004 <a href="https://arxiv.org/abs/quant-ph/0406196">paper</a> by Scott Aaronson and Daniel Gottesman (AG) says that the time complexity is <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" />. Looks good.</p>
<p>
</p><p></p><h2> A Breakthrough? </h2><p></p>
<p></p><p>
For a few hours, maybe days, GR thought they could show that stabilizer circuits solve the matrix rank problem. In part this was based on a misunderstanding, but it was also based on not needing the full generality of maintaining the system between single-qubit measurements. If they could calculate or even estimate the probability of just one all-qubits measurement in time <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> then the same time would apply to computing matrix rank over the field <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{F}_2}" class="latex" title="{\mathbb{F}_2}" />. This would have been an immense result. The best known is that the rank of a matrix can be computed in <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B%5Comega%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^{\omega}}" class="latex" title="{n^{\omega}}" /> where <img src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\omega}" class="latex" title="{\omega}" /> is the current exponent for multiplying <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n \times n}" class="latex" title="{n \times n}" /> matrices—over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{F}_2}" class="latex" title="{\mathbb{F}_2}" /> or any field. Today <img src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\omega}" class="latex" title="{\omega}" /> stands at <img src="https://s0.wp.com/latex.php?latex=%7B2.3728%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2.3728\dots}" class="latex" title="{2.3728\dots}" />. </p>
<p>
Clearly, they were excited. I still recall Ken calling me with the possible news. I thought that it was not a crazy idea. Their plan was: </p>
<ol>
<li>
Reduce matrix rank to a quantum problem; <p></p>
</li><li>
Show that problem could be computed by stabilizer circuits; <p></p>
</li><li>
Then invoke the simulation theorem for these circuits to get a classical algorithm.
</li></ol>
<p>Wow. This trip from a classic problem, to a quantum one, and back was intriguing. </p>
<p>
</p><p></p><h2> Saving Grace </h2><p></p>
<p></p><p>
Quickly GR figured out the issue. They could in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> total time determine the probability of each individual qubit being <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />. That probability would be either <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> or <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac{1}{2}}" class="latex" title="{\frac{1}{2}}" /> since their reduction from rank gives a circuit in which the probability of getting all <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />s is guaranteed to be positive. If <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" /> values are <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac{1}{2}}" class="latex" title="{\frac{1}{2}}" /> that does not make the answer <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%5Ek%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac{1}{2^k}}" class="latex" title="{\frac{1}{2^k}}" />, because of entanglements. They had thought that the Aaronson-Gottesman algorithm took care of the entanglement bookkeeping in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> time, but it services only one qubit in that time. Ironically, AG expressly note that they improved Gottesman’s earlier <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" /> time for this task to <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" />—exactly what GR thought they would get—but AG still only get <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^3)}" class="latex" title="{O(n^3)}" /> time for operations involving all the qubits.</p>
<p>
GR worked out the full picture from a more-recent <a href="https://arxiv.org/abs/1712.03554">paper</a> by Héctor García-Ramírez and Igor Markov of Michigan, which describes detailed software for simulating stabilizer circuits. Thus the quantum method would only put matrix rank into cubic time—not new. But then they realized that the goalposts for a result in the <em>other</em> direction were only <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^3)}" class="latex" title="{O(n^3)}" />. This they could beat with most of their hard work already done. Thus they can improve <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^3)}" class="latex" title="{O(n^3)}" /> cases by AG and some subsequent papers to <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" />. In theory, that is—the <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" /> algorithms are galactic. But what GR also have is a pretty result that is not galactic at all:</p>
<blockquote><p><b>Theorem 1</b> <em> If membership in a certain class of undirected <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-vertex graphs can be decided in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> time, then the following problems all have the same time complexity: </em></p><em>
<ol>
<li>
The strong simulation of quantum stabilizer circuits; <p></p>
</li><li>
The computation of the rank of matrices over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\mathbb{F}_2}" class="latex" title="{\mathbb{F}_2}" />; <p></p>
</li><li>
The counting of solutions to classical quadratic forms modulo <img src="https://s0.wp.com/latex.php?latex=%7B4%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{4}" class="latex" title="{4}" />.
</li></ol>
</em><p><em></em>
</p></blockquote>
<p></p><p>
Note that all these problems have upper bounds of <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^\omega}" class="latex" title="{n^\omega}" /> by GR; the point is that their bounds would coincide even if matrix rank turns out to be easier than matrix multiplication. The next post will talk about the class of graphs. Just to be clear:</p>
<ul>
<li>
If the graphs are in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> time, this does not mean the problems are all in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> time—they could still need <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^\omega}" class="latex" title="{n^\omega}" /> time. <p></p>
</li><li>
The graphs are in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" /> time. This is for dense graphs with order-<img src="https://s0.wp.com/latex.php?latex=%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^2}" class="latex" title="{n^2}" /> edges. <p></p>
</li><li>
The reductions from rank to the other problems run in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> time unconditionally—the graphs involved are all bipartite so their status is known.
</li></ul>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
I hope that this discussion helps shed some light on the new result of GR. I hope that raising the covers on how they found their theorem is useful. Research is not smooth, is not a straight-line from start to finish, and their journey is not atypical.</p>
<p>[Edited wrong word: thorough]</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2019/06/07/a-rank-problem/"><span class="datestr">at June 07, 2019 12:58 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.02511">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.02511">On the distribution of runners on a circle</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hrubes:Pavel.html">Pavel Hrubes</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.02511">PDF</a><br /><b>Abstract: </b>Consider $n$ runners running on a circular track of unit length with constant
speeds such that $k$ of the speeds are distinct. We show that, at some time,
there will exist a sector $S$ which contains at least $|S|n+ \Omega(\sqrt{k})$
runners. The result can be generalized as follows. Let $f(x,y)$ be a complex
bivariate polynomial whose Newton polytope has $k$ vertices. Then there exists
$a\in {\mathbb C}\setminus\{0\}$ and a complex sector $S=\{re^{\imath \theta}:
r&gt;0, \alpha\leq \theta \leq \beta\}$ such that the univariate polynomial
$f(x,a)$ contains at least $\frac{\beta-\alpha}{2\pi}n+\Omega(\sqrt{k})$
non-zero roots in $S$ (where $n$ is the total number of such roots and $0\leq
(\beta-\alpha)\leq 2\pi$). This shows that the Real $\tau$-Conjecture of Koiran
implies the conjecture on Newton polytopes of Koiran et al.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.02511"><span class="datestr">at June 07, 2019 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.02315">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.02315">Greed is Not Always Good: On Submodular Maximization over Independence Systems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuhnle:Alan.html">Alan Kuhnle</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.02315">PDF</a><br /><b>Abstract: </b>In this work, we consider the maximization of submodular functions
constrained by independence systems. Because of the wide applicability of
submodular functions, this problem has been extensively studied in the
literature. When the independence system is a $p$-system, prior literature has
claimed that the greedy algorithm achieves a $1/(p+1)$-approximation if the
submodular function is monotone. We show that, on the contrary, for any
$\epsilon &gt; 0$, the problem is hard to approximate within $(2/n)^{1-\epsilon}$,
where $n$ is the size of the ground set, even when the independence system is a
$1$-system. This result invalidates prior work on constant-factor algorithms
for non-monotone submodular maximization over $p$-systems as well. On the
positive side, we provide the first nearly linear-time algorithm for
maximization of non-monotone submodular functions over $p$-extendible
independence systems, which are a subclass of $p$-systems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.02315"><span class="datestr">at June 07, 2019 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.02229">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.02229">Quantum Algorithms for Solving Dynamic Programming Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ronagh:Pooya.html">Pooya Ronagh</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.02229">PDF</a><br /><b>Abstract: </b>We present quantum algorithms for solving finite-horizon and infinite-horizon
dynamic programming problems. The infinite-horizon problems are studied using
the framework of Markov decision processes. We prove query complexity lower
bounds for classical randomized algorithms for the same tasks and consequently
demonstrate a polynomial separation between the query complexity of our quantum
algorithms and best-case query complexity of classical randomized algorithms.
Up to polylogarithmic factors, our quantum algorithms provide quadratic
advantage in terms of the number of states $|S|$, and the number of actions
$|A|$, in the Markov decision process when the transition kernels are
deterministic. This covers all discrete and combinatorial optimization problems
solved classically using dynamic programming techniques. In particular, we show
that our quantum algorithm solves the travelling salesperson problem in
$O^*(c^4 \sqrt{2^n})$ where $n$ is the number of nodes of the underlying graph
and $c$ is the maximum edge-weight of it. For stochastic transition kernels the
quantum advantage is again quadratic in terms of the numbers of actions but
less than quadratic (from $|S|^2$ to $|S|^{3/2}$) in terms of the numbers of
states. In all cases, the speed-up achieved is at the expense of appearance of
other polynomial factors in the scaling of the algorithm. Finally we prove
lower bounds for the query complexity of our quantum algorithms and show that
no more-than-quadratic speed-up in either of $|S|$ or $|A|$ can be achieved for
solving dynamic programming and Markov decision problems using quantum
algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.02229"><span class="datestr">at June 07, 2019 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/085">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/085">TR19-085 |  Approximate Degree-Weight and Indistinguishability | 

	Emanuele Viola, 

	Xuangui Huang</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We prove that the Or function on $n$ bits can be point-wise approximated with error $\eps$ by a polynomial of degree $O(k)$ and weight $2^{O(n \log (1/\eps)/k)}$, for any $k \geq \sqrt{n \log 1/\eps}$.  This result is tight for all $k$.  Previous results were either not tight or had $\eps = \Omega(1)$.  In general we obtain an approximation result for any symmetric function, also tight.  Building on this we also obtain an approximation result for bounded-width CNF.  For these two classes no such result was known.

One motivation for such results comes from the study of indistinguishability.
Two distributions $P$, $Q$ over $n$-bit strings are $(k,\delta)$-indistinguishable if their projections on any $k$ bits have statistical distance at most $\delta$.
The above approximations give values of $(k,\delta)$ that suffice to fool symmetric functions and bounded-width CNF, and the first result is tight.
Finally, we show that any two $(k, \delta)$-indistinguishable distributions are $O(n)^{k/2}\delta$-close to two distributions that are $(k,0)$-indistinguishable, improving the previous bound of $O(n)^k \delta$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/085"><span class="datestr">at June 06, 2019 11:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3397">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2019/06/06/workshop-on-simplicity-and-robustness-in-complex-markets-stony-brook-university-july-11-12-2019/">Workshop on Simplicity and Robustness in Complex Markets, Stony Brook University, July 11-12, 2019</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p style="font-weight: 400;">As part of the <a href="http://www.gtcenter.org/?page=Conference.html">30<sup>th</sup> Stony Brook International Conference on Game Theory</a>, we will be holding a workshop on Simplicity and Robustness in Complex Markets.  The workshop will be held July 11-12 at Stony Brook University.</p>
<p style="font-weight: 400;">The goal of this workshop is to bring together researchers from Computer Science, Game Theory, Economics, and Operations Research to focus on issues related to simplicity, robustness, and approximation in economic problems.  For more information, including the workshop program, please visit</p>
<p style="font-weight: 400;"><a href="http://www.gtcenter.org/?page=Workshops.html">http://www.gtcenter.org/?page=Workshops.html</a></p>
<p style="font-weight: 400;">Early registration is open until June 16 at <a href="http://www.gtcenter.org/?page=Registration.html">http://www.gtcenter.org/?page=Registration.html</a></p>
<p style="font-weight: 400;">We hope to see you there!</p>
<p style="font-weight: 400;">Michal Feldman and Brendan Lucier</p></div>







<p class="date">
by michalfeldman <a href="https://agtb.wordpress.com/2019/06/06/workshop-on-simplicity-and-robustness-in-complex-markets-stony-brook-university-july-11-12-2019/"><span class="datestr">at June 06, 2019 06:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=357">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2019/06/06/tcs-talk-wednesday-june-12th-john-wright-mit/">TCS+ talk: Wednesday, June 12th — John Wright, MIT</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk—and last of the season!—will take place this coming Wednesday, June 12th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 19:00 Central European Summer Time, 17:00 UTC). <strong>John Wright</strong> from MIT will speak about “<em>NEEXP in MIP*</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote>
<p style="text-align: justify;">Abstract: A long-standing puzzle in quantum complexity theory is to understand the power of the class <img src="https://s0.wp.com/latex.php?latex=%5Ctextsf%7BMIP%2A%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\textsf{MIP*}" class="latex" title="\textsf{MIP*}" /> of multiprover interactive proofs with shared entanglement. This question is closely related to the study of entanglement through non-local games, which dates back to the pioneering work of Bell. In this work we show that <img src="https://s0.wp.com/latex.php?latex=%5Ctextsf%7BMIP%2A%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\textsf{MIP*}" class="latex" title="\textsf{MIP*}" /> contains <img src="https://s0.wp.com/latex.php?latex=%5Ctextsf%7BNEEXP%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\textsf{NEEXP}" class="latex" title="\textsf{NEEXP}" /> (non-deterministic doubly-exponential time), exponentially improving the prior lower bound of <img src="https://s0.wp.com/latex.php?latex=%5Ctextsf%7BNEXP%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\textsf{NEXP}" class="latex" title="\textsf{NEXP}" /> due to Ito and Vidick. Our result shows that shared entanglement exponentially increases the power of these proof systems, as the class <img src="https://s0.wp.com/latex.php?latex=%5Ctextsf%7BMIP%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\textsf{MIP}" class="latex" title="\textsf{MIP}" /> of multiprover interactive proofs without shared entanglement is known to be equal to <img src="https://s0.wp.com/latex.php?latex=%5Ctextsf%7BNEXP%7D&amp;bg=fff&amp;fg=444444&amp;s=0" alt="\textsf{NEXP}" class="latex" title="\textsf{NEXP}" />.</p>
</blockquote>
<p> </p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2019/06/06/tcs-talk-wednesday-june-12th-john-wright-mit/"><span class="datestr">at June 06, 2019 04:02 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-1381501092894849452">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/06/what-happened-to-surprising-theorems.html">What Happened to the Surprising Theorems?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Twenty-five years ago Peter Shor presented a polynomial-time factoring algorithms for quantum computers. For Peter, it was a simple translation of a <a href="https://en.wikipedia.org/wiki/Simon%27s_problem">quantum algorithm</a> due to Dan Simon. For the rest of us, it was a shock, while we knew quantum could do some seemingly artificial problems exponentially faster, no one expected a natural problem like factoring to fall so quickly. I remember remarking at the time that Shor bought quantum computing twenty years, now I would say fifty.<br />
<div>
<br /></div>
<div>
That may have been the last time I was truly shocked by a theorem in theoretical computer science. I've been shocked by proofs, that Primes are in P, Undirected connectivity in Log space, NEXP not in ACC<sup>0</sup>, Graph Isomorphism in quasi-polynomial time. But the theorems themselves all went in the directions we expected.<br />
<br />
In the ten years before Shor we had plenty of surprises, interactive proofs, zero-knowledge proofs, probabilistically checkable proofs, nondeterministic space closed under complementation, hardness versus randomness, the permanent hard for the polynomial-time hierarchy. It seemed to come to a hard stop after Shor.<br />
<br />
There have been some mild surprises, the Hadamard isn't rigid, holographic algorithms, the complexity of Nash equilibrium, QIP = PSPACE, and many others. But nothing that has made us  rethink the complexity world.<br />
<br />
This reflects the maturity of our field. How many shocking theorems have we seen recently in math in general? We're shocked by proofs of the Poincaré conjecture and Fermat's last theorem but both went in the expected direction.<br />
<br />
We will have some shocking theorem in the future, maybe Factoring in P or L = NL. To be truly shocked it would have to be something I can't even imagine being true today.</div></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/06/what-happened-to-surprising-theorems.html"><span class="datestr">at June 06, 2019 02:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gradientscience.org/robust_apps/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://gradientscience.org/robust_apps/">Robustness beyond Security&amp;#58; Computer Vision Applications</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left;" href="https://gradientscience.org/robust-apps.pdf" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Paper
</a>
<a style="float: left;" href="http://git.io/robust-apps" class="bbutton">
<i class="fab fa-github"></i>
   Notebooks
</a>
<a style="float: left;" href="http://bit.ly/robustness_demo" class="bbutton">
<i class="fas fa-code"></i>
   Live Demo
</a></p>

<p><i>We discuss our <a href="https://gradientscience.org/robust-apps.pdf">latest paper</a>
on computer vision applications of robust classifiers.
We are able to leverage the features learned by a single classifier to 
develop a rich toolkit for diverse computer vision applications.
Our results suggest the robust classification framework as a viable alternative
to more complex or task-specific approaches.
</i></p>

<p>In our <a href="https://gradientscience.org/robust_reps/">previous post</a>, we saw how robust models 
capture high-level, human-aligned features that can be directly manipulated through 
gradient descent. In this post, we demonstrate how to leverage these robust models 
to perform a wide range of computer vision tasks. In fact, we perform all these tasks 
by simply optimizing the predicted class scores of a <i>single</i> robustly trained 
classifier (per dataset). The resulting toolkit is simple, versatile and 
reliable—to highlight its consistency, in this post we visualize the performance 
of our method using <i>random</i> (not cherry-picked) samples.</p>

<p>Our corresponding paper can be found <a href="https://gradientscience.org/robust-apps.pdf">here</a>, and you 
can reproduce all of our results using open-source IPython notebooks 
<a href="http://git.io/robust-apps">here</a>.
We also have a live <a href="http://bit.ly/robustness_demo">demo</a> where you play with a 
trained robust model like this:</p>

<video align="right" style="width: 90%;">
    <source src="http://gradientscience.org/assets/rf-vision/robust_apps_demo.mp4" type="video/mp4">
    <source src="http://gradientscience.org/assets/rf-vision/robust_apps_demo.webm" type="video/webm">
</source></source></video>

<h3 id="robust-models-as-a-tool-for-input-manipulation">Robust Models as a Tool for Input Manipulation</h3>
<p>The key primitive of our approach is <i>class maximization</i>: maximization of class 
log-probabilities (scores) from an <a href="https://gradientscience.org/robust_opt_pt1/">adversarially robust model</a> 
using gradient descent in input space.</p>

<p>As discussed in our <a href="https://gradientscience.org/robust_reps/">last post</a>, robust models learn 
representations that are more aligned with human perception. As it turns out, 
performing class maximization on these models actually introduces class-relevant 
characteristics in the corresponding input (after all, these class log-probabilities 
are just linear combinations of learned representations). To visualize this, here is 
the result of class maximization for a few random inputs:</p>

<p><img src="http://gradientscience.org/assets/rf-vision/targeted.jpg" alt="Targeted adversarial examples for a robust model" /></p>
<div class="footnote"> For a robust
model, maximizing the predicted probability of a specific class enhances key
features of that class in the input. </div>

<p>These results are also in line with <a href="https://arxiv.org/abs/1805.12152">our previous
work</a>, where we observed that large
adversarial perturbations for robust models often actually resemble natural
examples of the corresponding incorrect class.</p>

<p>In the rest of this post, we will explore how to perform a variety of computer
vision tasks using only class maximization. It turns out that robustness is all
you need!</p>

<h3 id="generation">Generation</h3>
<p>We begin by leveraging robust models to generate diverse,
realistic images. As we saw in the previous section, it is possible to introduce
salient features of a target class into an input through class maximization.
This simple operation alone turns out to also suffice to (class-conditionally)
generate images.</p>

<p>To generate an image, we randomly sample a starting point (seed) and then
execute (starting from that seed) the projected gradient ascent operation
underlying class maximization. The key question here is: how do we sample the
seed input? A natural idea is to just fit a Gaussian distribution to each class
(in image space), and sample seeds from that distribution.. Despite its
simplicity, this approach already leads to fairly diverse and realistic samples:</p>

<div class="widget">
    <div class="choices_one_full" id="gen">
    <span class="widgetheading" id="genclass">Choose an Image</span>
    </div>
    <div style="border-right: 3px white solid;">
        <img style="width: 38%; margin: 8px;" class="image-container" id="gen1" />
        <img style="width: 60%;" class="image-container" id="gen2" />
    </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Interactive demo</strong>: select any image in the top two rows to see additional
samples of that class.
</div>

<p>We expect that there is still room for improvement; for example, one could
replace Gaussians with a sophisticated class of distributions for seed sampling.</p>

<h3 id="image-to-image-translation">Image-to-Image translation</h3>

<p>The ability to introduce perceptually meaningful, class-relevant features in
image space (via class maximization) enables a very intuitive approach to
performing <a href="https://arxiv.org/abs/1703.10593">image-to-image translation</a>, the
task of transforming inputs from a source to a target domain (e.g., transforming
horses into zebras in photos). To perform this task, we first train a classifier
to <em>robustly</em> distinguish between the two domains. This process encourages the
classifier to learn key characteristics of each domain. We then perform image
translation on an image from a given domain simply by using class maximization
towards the target domain—this suffices! Here are some instances of our
approach applied to typical datasets:</p>

<div class="widget">
    <div class="choices_one" id="translate">
    <span class="widgetheading">Choose an Image</span>
    </div>
    <span class="widgetheading" id="translatedclass">Translated Image</span>
    <div class="beer-slider selected_one" id="translate_slider">
    <img style="width: 336px;" id="translate1" />
    <div style="border-right: 3px white solid;" class="beer-reveal">
        <img class="slider_img" id="translate2" />
    </div>
    </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Interactive demo</strong>: choose an image on the left to see the
result of image-to-image translation.
</div>
<p>This is what a few particularly nice samples produced by our
method look like:</p>

<p><img src="http://gradientscience.org/assets/rf-vision/translation.jpeg" alt="Select image-to-image translations" /></p>

<p>Just as with image generation, we find reasonable solutions to the task using
only class maximization on a robust classifier.</p>

<h3 id="inpainting">Inpainting</h3>

<p>Next, we consider the task of image inpainting—recovering images with large
missing or corrupted regions. At a high level, we would like to fill in the
damaged regions with features that are human-meaningful and consistent with the
rest of the image. Within our framework, the most natural way to do this is to
perform class maximization (towards the original class)  while also penalizing
large changes to the uncorrupted regions of the image. The intuition being that
this process restores the “missing” features while only minimally modifying the
rest of the image. This is how a few random inpainted images produced by our
method look like:</p>

<div class="widget">
    <div class="choices_one" id="inpaint">
    <span class="widgetheading">Choose an Image</span>
    </div>
    <span class="widgetheading">Inpainted Image</span>
    <div class="beer-slider selected_one" id="inpaint_slider">
    <img style="width: 336px;" id="inpaint1" />
    <div style="border-right: 3px white solid;" class="beer-reveal">
        <img class="slider_img" id="inpaint2" />
    </div>
    </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Interactive demo</strong>: click on an image on the left to see
an instance of inpainting for that image.
</div>

<p>Interestingly, even when our method produces reconstructions that differ from
the original, they are often still perceptually plausible to a human. Here are a
few select samples:</p>

<p><img src="http://gradientscience.org/assets/rf-vision/inpainting_errors.jpeg" alt="Failure modes of inpainting using robust models" /></p>
<div class="footnote">
Even when inpainting fails to recover the original uncorrupted image, the result is 
often still perceptually plausible to a human.
</div>

<h3 id="superresolution">Superresolution</h3>

<p>To perform inpainting, we used class maximization to restore relevant features
in corrupted images. The exact same intuition applies to the task of
superresolution, i.e., improving the resolution of an image in a human-
meaningful way. Specifically, using class maximization towards the underlying
true class, we can accentuate image features that are distorted in the low-
resolution image. Here we apply this method to images from the CIFAR10 dataset
(32x32 pixels) to a resolution of 224x224 (7-fold upsampling):</p>

<div class="widget">
    <div class="choices_one" id="upsample">
    <span class="widgetheading">Choose an Image</span>
    </div>
    <span class="widgetheading">Upsampled Image</span>
    <div class="beer-slider selected_one" id="upsample_slider">
    <img style="width: 336px;" id="upsample1" />
    <div style="border-right: 3px white solid;" class="beer-reveal">
        <img class="slider_img" id="upsample2" />
    </div>
    </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Interactive demo</strong>: click on any of the images on the left to see its
7-fold superresolution.
</div>

<p>Since the starting point of the underlying class maximization comes from a crude
upsampling (i.e., nearest neighbor interpolation) of the low-resolution image,
the final images exhibit some pixelation artifacts. We expect, however, that
combining this approach with a more sophisticated initialization will yield even
more realistic samples.</p>

<h3 id="interactive-image-manipulation">Interactive Image Manipulation</h3>

<p>Finally, using this simple primitive, one can build an interactive toolkit for
performing input space manipulations.</p>

<h4 id="sketch-to-image">Sketch-to-image</h4>

<p>Class maximization with robust classifiers turns out to yield human-meaningful
transformations even for <em>arbitrary</em> inputs. This enables us to use this
primitive to even transform hand-drawn sketches into realistic images. Here is
the result of maximizing a chosen class probability from a very crude sketch:</p>

<div class="widget">
    <div class="choices_one" id="sketch">
    <span class="widgetheading">Choose an Image</span>
    </div>
    <span class="widgetheading" id="sketchclass">Enhanced Image</span>
    <div class="beer-slider selected_one" id="sketch_slider">
    <img style="width: 336px;" id="sketch1" />
    <div style="border-right: 3px white solid;" class="beer-reveal">
        <img class="slider_img" id="sketch2" />
    </div>
    </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Interactive demo</strong>: select any of the sketches on the left to see it 
converted into a realistic image.
</div>

<p>You can draw realistic looking images with this method interactively
<a href="http://bit.ly/robustness_demo">here</a>, without any artistic skill!</p>

<h4 id="paint-with-features">Paint-with-Features</h4>

<p>In fact, we can achieve an even more fine-grained level of manipulation if we
directly perform maximization on the <em>representations</em> learned by the robust
model, instead of the class probabilities. (Recall that in the <a href="https://gradientscience.org/robust_reps/">last
post</a> we saw how individual components can
correspond to human-level features such as “stripes”.) By adding a human in the
loop, we can choose particular regions of the image to modify and specific
features to add. This leads to a versatile paint tool (inspired by
<a href="http://gandissect.res.ibm.com/ganpaint.html">GANpaint</a>) that can perform
manipulation such as this:</p>

<div class="widget">
    <div class="choices_one_paint" id="paint_left1">
    <span class="widgetheading">Choose an Image</span>
    </div>
    <div class="selected_three" id="paintclass">
    
    <video style="width: 100%;" id="paint_video">
        <source type="video/mp4" id="paint_selected_mp4">
        <source type="video/webm" id="paint_selected_webm">
    </source></source></video>
    </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Interactive demo</strong>: Choose one of the images on the left to see
a demonstration of painting high-level features onto the image.
</div>

<p>Here, we added a feature to a given region of the image by simply maximize the
corresponding activation (a single component of the robust representation
vector) while penalizing changes to the rest of the image. By successively
performing such <i>activation maximization</i> in different parts of the image,
we can paint with high-level concepts (e.g., grass or stripes).</p>

<h3 id="takeaways">Takeaways</h3>

<p>In this blog post, we applied simple, first-order manipulations of the
representation learned by a <em>single</em> robust classifier to perform a number of
computer vision tasks. This is contrast to prior approaches that
often required specialized
and sophisticated techniques. Crucially, to highlight the potential of the core
methodology itself, we used the same simple toolkit for all tasks and datasets,
and with minimal tuning and no task-specific optimizations. We expect that the
addition of domain knowledge and leveraging more perceptually-aligned notions of
robustness will further boost the performance of this toolkit. Importantly,
the models we use here are truly off-the-shelf and
are trained in a standard (and stable) manner (via
<a href="https://gradientscience.org/robust_opt_pt1/">robust optimization</a>).</p>

<p>Furthermore, our results highlight the utility of the basic classification
toolkit outside of classification tasks. We hope that our framework will expand to
offer ways to perform other vision tasks, on par with the existing
state-of-the-art techniques (e.g., based on generative models). Finally, our
findings highlight the merits of adversarial robustness as a goal that goes
beyond the security and reliability contexts this goal was considered in so far.</p></div>







<p class="date">
<a href="http://gradientscience.org/robust_apps/"><span class="datestr">at June 06, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/084">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/084">TR19-084 |  Resolution Lower Bounds for Refutation Statements | 

	Michal Garlik</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
For any unsatisfiable CNF formula we give an exponential lower bound on the size of resolution refutations of a propositional statement that the formula has a resolution refutation. We describe three applications. (1) An open question in [Atserias-Müller,2019] asks whether a certain natural propositional encoding of the above statement is hard for Resolution. We answer by giving an exponential size lower bound. (2) We show exponential resolution size lower bounds for reflection principles, thereby improving a result in [Atserias-Bonet,2004]. (3) We provide new examples of CNFs that exponentially separate Res(2) from Resolution (an exponential separation of these two proof systems was originally proved in [Segerlind-Buss-Impagliazzo,2004]).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/084"><span class="datestr">at June 05, 2019 10:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/06/05/postdoc-position-in-tcs-at-lund-university-apply-by-june-14-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/06/05/postdoc-position-in-tcs-at-lund-university-apply-by-june-14-2019/">Postdoc position in TCS at Lund University (apply by June 14, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The CS Department at Lund University invites applications for a postdoc position in TCS. The application deadline is June 14, 2019. See <a href="https://lu.mynetworkglobal.com/en/what:job/jobID:270663/">https://lu.mynetworkglobal.com/en/what:job/jobID:270663/</a> for the full announcement with more information and instructions for how to apply. Informal enquiries are welcome and may be sent to jakob.nordstrom@cs.lth.se.</p>
<p>Website: <a href="https://lu.mynetworkglobal.com/en/what:job/jobID:270663/">https://lu.mynetworkglobal.com/en/what:job/jobID:270663/</a><br />
Email: jakob.nordstrom@cs.lth.se</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/06/05/postdoc-position-in-tcs-at-lund-university-apply-by-june-14-2019/"><span class="datestr">at June 05, 2019 02:48 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/083">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/083">TR19-083 |  Testing Graphs against an Unknown Distribution | 

	Lior Gishboliner, 

	Asaf Shapira</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The area of graph property testing seeks to understand the relation between the global properties of a graph and its local statistics. In the classical model, the local statistics of a graph is defined relative to a uniform distribution over the graph’s vertex set. A graph property $\mathcal{P}$ is said to be testable if the local statistics of a graph can allow one to distinguish between graphs satisfying $\mathcal{P}$ and those that are far from satisfying it.

Goldreich recently introduced a generalization of this model in which one endows the vertex set of the input graph with an arbitrary and unknown distribution, and asked which of the properties that can be tested in the classical model can also be tested in this more general setting. We completely resolve this problem by giving a (surprisingly ``clean'') characterization of these properties. To this end, we prove a removal lemma for vertex weighted graphs which is of independent interest.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/083"><span class="datestr">at June 04, 2019 09:17 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7512">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2019/06/04/itcs-20-call-for-papers-guest-post-by-thomas-vidick/">ITCS 20 call for papers (guest post by Thomas Vidick)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>

We invite you to submit your papers to the 11th Innovations in<br />Theoretical Computer Science (ITCS). The conference will be held at<br />the University of Washington in Seattle, Washington from January 12-14,<br />2020.</p>



<p>ITCS seeks to promote research that carries a strong conceptual message<br />(e.g., introducing a new concept, model or understanding, opening a new<br />line of inquiry within traditional or interdisciplinary areas,<br />introducing new mathematical techniques and methodologies, or new<br />applications of known techniques). ITCS welcomes both conceptual and<br />technical contributions whose contents will advance and inspire the<br />greater theory community.</p>



<p>Submission deadline: September 9, 2019 (05:59pm PDT)<br />Notification to authors: October 31, 2019<br />Conference dates: January 12-14, 2020</p>



<p>See the website at <a href="http://itcs-conf.org/itcs20/itcs20-cfp.html" target="_blank" rel="noreferrer noopener">http://itcs-conf.org/itcs20/itcs20-cfp.html</a> for<br />detailed information regarding submissions.</p>



<p>Program committee</p>



<p>Nikhil Bansal, CWI + TU Eindhoven<br />Nir Bitansky, Tel-Aviv University<br />Clement Canonne, Stanford<br />Timothy Chan, University of Ilinois at Urbana-Champaign<br />Edith Cohen, Google and Tel-Aviv University<br />Shaddin Dughmi, University of Southern California<br />Sumegha Garg, Princeton<br />Ankit Garg, Microsoft research<br />Ran Gelles, Bar-Ilan University<br />Elena Grigorescu, Purdue<br />Tom Gur, University of Warwick<br />Sandy Irani, UC Irvine<br />Dakshita Khurana, University of Illinois at Urbana-Champaign<br />Antonina Kolokolova, Memorial University of Newfoundland.<br />Pravesh Kothari, Carnegie Mellon University<br />Rasmus Kyng, Harvard<br />Katrina Ligett, Hebrew University<br />Nutan Limaye, IIT Bombay<br />Pasin Manurangsi, UC Berkeley<br />Tamara Mchedlidze, Karlsruhe Institute of Technology<br />Dana Moshkovitz, UT Austin<br />Jelani Nelson, UC Berkeley<br />Merav Parter, Weizmann Institute<br />Krzysztof Pietrzak, IST Austria<br />Elaine Shi, Cornell<br />Piyush Srivastava, Tata Institute of Fundamental Research, Mumbai<br />Li-Yang Tan, Stanford<br />Madhur Tulsiani, TTIC<br />Gregory Valiant, Stanford<br />Thomas Vidick, California Institute of Technology (chair)<br />Virginia Vassilevska Williams, MIT<br />Ronald de Wolf, CWI and University of Amsterdam<br />David Woodruff, Carnegie Mellon University

</p></div>







<p class="date">
by windowsontheory <a href="https://windowsontheory.org/2019/06/04/itcs-20-call-for-papers-guest-post-by-thomas-vidick/"><span class="datestr">at June 04, 2019 09:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15923">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/">A Quantum Connection For Matrix Rank</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><font color="#0044cc"><br />
<em>A new paper with Chaowen Guan</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/chaowenwhiteboard/" rel="attachment wp-att-15934"><img src="https://rjlipton.files.wordpress.com/2019/06/chaowenwhiteboard.jpg?w=180&amp;h=130" alt="" width="180" class="alignright wp-image-15934" height="130" /></a></p>
<p>
Chaowen Guan is a PhD student at Buffalo. After a busy end to the Spring 2019 term at UB, we are getting time to write about our <a href="https://arxiv.org/abs/1904.00101">paper</a>, “Stabilizer Circuits, Quadratic Forms, and Computing Matrix Rank.”</p>
<p>
Today we emphasize new connections we have found between simulating special quantum circuits and computing matrix rank over the field <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{F}_2}" class="latex" title="{\mathbb{F}_2}" />.<span id="more-15923"></span></p>
<p>
The quantum circuits involved have been known as polynomial-time solvable <a href="https://en.wikipedia.org/wiki/Gottesman-Knill_theorem">since</a> <a href="https://arxiv.org/abs/quant-ph/9807006v1">1998</a>. They are not universal but form important building blocks of quantum systems people intend to build. They impact the problem of showing quantum circuits are more powerful than classical circuits—the <i>quantum advantage problem</i>—in terms of how much harder quantum stuff must be added to them. </p>
<p>
The question is: How efficiently can we simulate these special circuits? Our answer improves the bound from order-<img src="https://s0.wp.com/latex.php?latex=%7Bn%5E3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^3}" class="latex" title="{n^3}" /> to <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B%5Comega%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^{\omega}}" class="latex" title="{n^{\omega}}" />, where <img src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\omega}" class="latex" title="{\omega}" /> here means the current best-known exponent for multiplying <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n \times n}" class="latex" title="{n \times n}" /> matrices (over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{F}_2}" class="latex" title="{\mathbb{F}_2}" /> or any field). Today <img src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\omega}" class="latex" title="{\omega}" /> stands at <img src="https://s0.wp.com/latex.php?latex=%7B2.3728%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2.3728\dots}" class="latex" title="{2.3728\dots}" />. The non-quantum problem of counting solutions to a quadratic polynomial <img src="https://s0.wp.com/latex.php?latex=%7Bf%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(x_1,\dots,x_n)}" class="latex" title="{f(x_1,\dots,x_n)}" /> modulo 2 is likewise improved from the <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^3)}" class="latex" title="{O(n^3)}" /> <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.9881">shown</a> by Andrzej Ehrenfeucht and Marek Karpinski to <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" />.</p>
<p>
This comes at a price, however, because the matrix multiplication algorithms that optimize the exponent are <a href="https://en.wikipedia.org/wiki/Galactic_algorithm">galactic</a>. In this post we’ll emphasize what is <em>not</em> galactic: reductions to and from the problem of computing matrix rank that run in linear time—meaning <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> time for dense matrices—except for the need to check a yes/no condition in one of them. All this builds on the algebraic methods in our <a href="https://link.springer.com/chapter/10.1007/978-3-662-56499-8_4">paper</a> last year with Amlan Chakrabarti of the University of Calcutta.</p>
<p>
Chaowen has contributed a <a href="https://rjlipton.wordpress.com/2018/07/02/local-hams-in-la-jolla/">post</a> and some other materials for this blog. His work first came up in a <a href="https://rjlipton.wordpress.com/2016/06/29/getting-to-the-roots-of-factoring/">post</a> three years ago that saluted Dick and Kathryn’s wedding. Today is their third anniversary—so this post also comes with happy anniversary wishes.</p>
<p>
</p><p></p><h2> Strong Simulation Problems </h2><p></p>
<p></p><p>
We have <a href="https://rjlipton.wordpress.com/2010/08/02/quantum-algorithms-via-linear-algebra/">covered</a> <a href="https://rjlipton.wordpress.com/2010/08/25/quantum-algorithms-a-different-view-again/">quantum</a> <a href="https://rjlipton.wordpress.com/2011/10/26/quantum-chocolate-boxes/">algorithms</a> <a href="https://rjlipton.wordpress.com/2011/11/14/more-quantum-chocolate-boxes/">several</a> <a href="https://rjlipton.wordpress.com/2015/04/08/a-quantum-two-finger-exercise/">times</a>. We discussed <em>stabilizer circuits</em> in an early <a href="https://rjlipton.wordpress.com/2012/07/08/grilling-quantum-circuits/">post</a> on the work with Amlan and <a href="https://rjlipton.wordpress.com/2017/11/20/a-magic-madison-visit">covered</a> them more recently in connection with the work of Jin-Yi Cai’s group. Suffice it to say that stabilizer circuits—which extend Clifford circuits by allowing intermediate measurement gates—form the most salient case that classical computers can simulate in polynomial time.</p>
<p>
The simulation time is sometimes cited as <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> going back to a 2004 <a href="https://arxiv.org/abs/quant-ph/0406196">paper</a> by Scott Aaronson and Daniel Gottesman, but there is a catch: this is only for one measurement of one qubit. For general (non-sparse) instances, all of <a href="https://arxiv.org/abs/quant-ph/0504117">various</a> <a href="https://arxiv.org/abs/1305.6190">other</a> <a href="https://web.eecs.umich.edu/~imarkov/pubs/conf/iccd13-quipu.pdf">algorithms</a> need order-<img src="https://s0.wp.com/latex.php?latex=%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^2}" class="latex" title="{n^2}" /> time to re-organize their data structures after each single-qubit measurement. This is so even if one merely wants to measure all <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> qubits in one shot: the time becomes <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^3)}" class="latex" title="{O(n^3)}" />. This is one case of what is generally called a <em>strong</em> simulation. It is precisely this time that Chaowen and I improved to <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" />.</p>
<p>
In wider contexts, strong simulation of a quantum circuit <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> means the ability to compute the probability of a given output to high precision. When the input and output are both in <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{0,1\}^n}" class="latex" title="{\{0,1\}^n}" /> we may suppose both are <img src="https://s0.wp.com/latex.php?latex=%7B0%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0^n}" class="latex" title="{0^n}" /> since we can prepend and append <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNOT%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{NOT}}" class="latex" title="{\mathsf{NOT}}" /> gates to <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" />. Then strong simulation means computing the amplitude <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle 0^n |C| 0^n \rangle}" class="latex" title="{\langle 0^n |C| 0^n \rangle}" /> (or computing <img src="https://s0.wp.com/latex.php?latex=%7B%7C%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|\langle 0^n |C| 0^n \rangle|^2}" class="latex" title="{|\langle 0^n |C| 0^n \rangle|^2}" /> which is the output probability) to <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-place precision. It doesn’t take much for this to be <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{NP}}" class="latex" title="{\mathsf{NP}}" />-hard, often <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{\#P}}" class="latex" title="{\mathsf{\#P}}" />-complete. If we take the Clifford generating set </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BH%7D+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Cbegin%7Bbmatrix%7D+1+%26+1+%5C%5C+1+%26+-1+%5Cend%7Bbmatrix%7D%2C%5Cquad+%5Cmathsf%7BCZ%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%26+0+%5C%5C+0+%26+1+%26+0+%26+0+%5C%5C+0+%26+0+%26+1+%26+0+%5C%5C+0+%26+0+%26+0+%26+-1+%5Cend%7Bbmatrix%7D%2C%5Cquad+%5Cmathsf%7BS%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%5C%5C+0+%26+i+%5Cend%7Bbmatrix%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{H} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{bmatrix},\quad \mathsf{CZ} = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; -1 \end{bmatrix},\quad \mathsf{S} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; i \end{bmatrix}, " class="latex" title="\displaystyle  \mathsf{H} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{bmatrix},\quad \mathsf{CZ} = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; -1 \end{bmatrix},\quad \mathsf{S} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; i \end{bmatrix}, " /></p>
<p>then we can get universal circuits by adding any any one of the following gates: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BT%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%5C%5C+0+%26+%5Csqrt%7Bi%7D+%5Cend%7Bbmatrix%7D%2C%5Cquad+%5Cmathsf%7BCS%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%26+0+%5C%5C+0+%26+1+%26+0+%26+0+%5C%5C+0+%26+0+%26+1+%26+0+%5C%5C+0+%26+0+%26+0+%26+i+%5Cend%7Bbmatrix%7D%2C%5Cquad+%5Cmathsf%7BTof%7D+%3D+%5Cmathit%7Bdiag%7D%281%2C1%2C1%2C1%2C1%2C1%2C%5Cbegin%7Bbmatrix%7D+0+%26+1+%5C%5C+1+%26+0+%5Cend%7Bbmatrix%7D%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{T} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; \sqrt{i} \end{bmatrix},\quad \mathsf{CS} = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; i \end{bmatrix},\quad \mathsf{Tof} = \mathit{diag}(1,1,1,1,1,1,\begin{bmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{bmatrix}). " class="latex" title="\displaystyle  \mathsf{T} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; \sqrt{i} \end{bmatrix},\quad \mathsf{CS} = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; i \end{bmatrix},\quad \mathsf{Tof} = \mathit{diag}(1,1,1,1,1,1,\begin{bmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{bmatrix}). " /></p>
<p>In the last one we’ve portrayed the <img src="https://s0.wp.com/latex.php?latex=%7B8+%5Ctimes+8%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{8 \times 8}" class="latex" title="{8 \times 8}" /> matrix of the <em>Toffoli gate</em> as being <em>block-diagonal</em>. We will later consider block-diagonal matrices permuted so that all <img src="https://s0.wp.com/latex.php?latex=%7B2+%5Ctimes+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2 \times 2}" class="latex" title="{2 \times 2}" /> “blocks” are at upper left.</p>
<p>
There is <a href="https://arxiv.org/abs/1601.07601">much</a> <a href="https://arxiv.org/pdf/1808.00128.pdf">recent</a> <a href="https://arxiv.org/pdf/1712.03554.pdf">literature</a> on trying to simulate circuits with limited numbers of non-Clifford gates, and on how many such gates may be needed for <a href="https://arxiv.org/pdf/1902.04764.pdf">exponential</a> lower bounds—even just to tell whether <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle+%5Cneq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle 0^n |C| 0^n \rangle \neq 0}" class="latex" title="{\langle 0^n |C| 0^n \rangle \neq 0}" />. This plays against a wider context of <a href="https://arxiv.org/abs/1608.00263">efforts</a> <a href="https://arxiv.org/abs/1807.10749">toward</a> <a href="https://arxiv.org/pdf/1905.00444.pdf">quantum</a> <a href="https://arxiv.org/abs/1203.5813">advantage</a>. Chaowen and I have been trying to apply algebraic-geometric techniques for new lower bounds at the high end, but this time we found new upper bounds at the low end.</p>
<p>
</p><p></p><h2> From Matrix Rank to Quantum </h2><p></p>
<p></p><p>
It is not known how to compute the rank <img src="https://s0.wp.com/latex.php?latex=%7Brk%28A%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{rk(A)}" class="latex" title="{rk(A)}" /> of a dense matrix <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> in better than matrix-multiplication time, even over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{F}_2}" class="latex" title="{\mathbb{F}_2}" />. We may suppose <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> is square and symmetric, since we can always form the block matrix </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A%27+%3D+%5Cbegin%7Bbmatrix%7D+0+%26+A%5E%5Ctop+%5C%5C+A+%26+0+%5Cend%7Bbmatrix%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  A' = \begin{bmatrix} 0 &amp; A^\top \\ A &amp; 0 \end{bmatrix} " class="latex" title="\displaystyle  A' = \begin{bmatrix} 0 &amp; A^\top \\ A &amp; 0 \end{bmatrix} " /></p>
<p>and then <img src="https://s0.wp.com/latex.php?latex=%7Brk%28A%29+%3D+%5Cfrac%7B1%7D%7B2%7Drk%28A%27%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{rk(A) = \frac{1}{2}rk(A')}" class="latex" title="{rk(A) = \frac{1}{2}rk(A')}" />. In the case of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{F}_2}" class="latex" title="{\mathbb{F}_2}" />, <img src="https://s0.wp.com/latex.php?latex=%7BA%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A'}" class="latex" title="{A'}" /> is the adjacency matrix <img src="https://s0.wp.com/latex.php?latex=%7BA_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A_G}" class="latex" title="{A_G}" /> of an undirected bipartite graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />. The rank of <img src="https://s0.wp.com/latex.php?latex=%7BA_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A_G}" class="latex" title="{A_G}" /> for any undirected graph <img src="https://s0.wp.com/latex.php?latex=%7BG+%3D+%28V%2CE%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G = (V,E)}" class="latex" title="{G = (V,E)}" /> must be even. Whereas the rank of the <img src="https://s0.wp.com/latex.php?latex=%7B%7CV%7C+%5Ctimes+%7CE%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|V| \times |E|}" class="latex" title="{|V| \times |E|}" /> vertex-edge incidence matrix always equals <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> minus the number of connected components of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />, less is <a href="http://web.cs.elte.hu/~lovasz/kurzusok/adjrank16.pdf">known</a> about characterizing <img src="https://s0.wp.com/latex.php?latex=%7Brk%28A_G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{rk(A_G)}" class="latex" title="{rk(A_G)}" />. Our first main theorem brings quantum strong simulation into the picture. Let <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N}" class="latex" title="{N}" /> stand for <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^2}" class="latex" title="{n^2}" />.</p>
<blockquote><p><b>Theorem 1</b> <em><a name="rank2QC"></a> Given any <img src="https://s0.wp.com/latex.php?latex=%7BA+%5Cin+%5Cmathbb%7BF%7D_2%5E%7Bn+%5Ctimes+n%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{A \in \mathbb{F}_2^{n \times n}}" class="latex" title="{A \in \mathbb{F}_2^{n \times n}}" /> we can construct in <img src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{O(N)}" class="latex" title="{O(N)}" /> time a stabilizer circuit <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> on <img src="https://s0.wp.com/latex.php?latex=%7B2n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{2n}" class="latex" title="{2n}" /> qubits such that </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++rk%28A%29+%3D+%5Clog_2%28%7C%5Clangle+0%5E%7B2n%7D+%7CC%7C+0%5E%7B2n%7D+%5Crangle%7C%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="\displaystyle  rk(A) = \log_2(|\langle 0^{2n} |C| 0^{2n} \rangle|). " class="latex" title="\displaystyle  rk(A) = \log_2(|\langle 0^{2n} |C| 0^{2n} \rangle|). " /></p>
</em><p><em></em>
</p></blockquote>
<p></p><p>
One interpretation is that if you believe matrix rank is a “mildly hard” function (with regard to <img src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(N)}" class="latex" title="{O(N)}" />-time computability) then predicting the result of measuring all the qubits in a stabilizer circuit is also “mildly hard.” Such mild hardness would represent a <em>gap</em> between the <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> time for weak simulation and the time for strong simulation. Such gaps have been noted and proved for extensions of stabilizer circuits but those are between “polynomial” and an intractable hardness notion.</p>
<p>
One can also view Theorem <a href="https://rjlipton.wordpress.com/feed/#rank2QC">1</a> as a possible avenue toward computing matrix rank without doing either matrix multiplication or Gaussian elimination. This is the view Chaowen and I have had all along. </p>
<p>
</p><p></p><h2> From Quantum to Rank </h2><p></p>
<p></p><p>
The distinguishing point of our converse reduction to the rank <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> is <em>knowledge of normal forms that depend on <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /></em> where one can use the knowledge <em>to delay or avoid computing them explicitly</em>. The normal forms are for polynomials <img src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_C}" class="latex" title="{f_C}" /> associated to quantum circuits <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> in our <a href="https://rjlipton.wordpress.com/2012/07/08/grilling-quantum-circuits/">earlier</a> <a href="https://link.springer.com/chapter/10.1007/978-3-662-56499-8_4">work</a>. Stabilizer circuits yield <img src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_C}" class="latex" title="{f_C}" /> as a <em>classical quadratic form</em> over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_4}" class="latex" title="{\mathbb{Z}_4}" />, the integers modulo <img src="https://s0.wp.com/latex.php?latex=%7B4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{4}" class="latex" title="{4}" />. That is, all cross terms <img src="https://s0.wp.com/latex.php?latex=%7Bx_i+x_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_i x_j}" class="latex" title="{x_i x_j}" /> in <img src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_C}" class="latex" title="{f_C}" /> have even coefficients—here, <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> or <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />. Thus quantum computing enters a debate that occupied Carl Gauss and others over two hundred years ago:</p>
<blockquote><p><b> </b> <em> Should every homogeneous quadratic polynomial <img src="https://s0.wp.com/latex.php?latex=%7Bf%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{f(x_1,\dots,x_n)}" class="latex" title="{f(x_1,\dots,x_n)}" /> with integer coefficients be called a <b>quadratic form</b>, or only those whose cross terms <img src="https://s0.wp.com/latex.php?latex=%7Bc_%7Bi%2Cj%7Dx_i+x_j%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{c_{i,j}x_i x_j}" class="latex" title="{c_{i,j}x_i x_j}" /> all have even coefficients <img src="https://s0.wp.com/latex.php?latex=%7Bc_%7Bi%2Cj%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{c_{i,j}}" class="latex" title="{c_{i,j}}" />? </em>
</p></blockquote>
<p></p><p>
The point of even coefficients is that they enable having a symmetric <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n \times n}" class="latex" title="{n \times n}" /> <em>integer</em> matrix <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> such that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%29+%3D+x%5E%5Ctop+S+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f(x) = x^\top S x " class="latex" title="\displaystyle  f(x) = x^\top S x " /></p>
<p>for all <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />. Without that condition, <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> might only be half-integral. This old difference turns out to mirror that between universal quantum computing and classical, because the non-Clifford <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BCS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{CS}}" class="latex" title="{\mathsf{CS}}" />-gate noted above yields circuits <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> whose <img src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_C}" class="latex" title="{f_C}" /> over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_4}" class="latex" title="{\mathbb{Z}_4}" /> have terms <img src="https://s0.wp.com/latex.php?latex=%7Bx_i+x_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_i x_j}" class="latex" title="{x_i x_j}" /> and/or <img src="https://s0.wp.com/latex.php?latex=%7B3+x_i+x_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3 x_i x_j}" class="latex" title="{3 x_i x_j}" />. While counting solutions in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_4^n}" class="latex" title="{\mathbb{Z}_4^n}" /> for those polynomials is in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{P}}" class="latex" title="{\mathsf{P}}" />, counting their <em>binary</em> solutions is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{\#P}}" class="latex" title="{\mathsf{\#P}}" />-complete—an amazing dichotomy we expounded <a href="https://rjlipton.wordpress.com/2017/11/20/a-magic-madison-visit/">here</a>.</p>
<p>
We hasten to add that for <img src="https://s0.wp.com/latex.php?latex=%7Bk+%3D+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k = 2}" class="latex" title="{k = 2}" /> the classical forms coincide with those over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_%7B2%5Ek%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_{2^k}}" class="latex" title="{\mathbb{Z}_{2^k}}" /> whose nonzero cross terms all have coefficient <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bk-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{k-1}}" class="latex" title="{2^{k-1}}" />. Those are called <em>affine</em> in the work by Jin-Yi and others noted above, and our above-mentioned <a href="https://rjlipton.wordpress.com/2017/11/20/a-magic-madison-visit">post</a> noted his 2017 <a href="https://arxiv.org/abs/1705.00942">paper</a> with Heng Guo and Tyson Williams giving another proof of polynomial-time simulation of stabilizer circuits via <img src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_C}" class="latex" title="{f_C}" /> being affine. Our work improving the polynomial bounds, however, draws on a 2009 <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.730.2154">paper</a> by Kai-Uwe Schmidt and further theory of classical quadratic forms. This paper uses work going back to 1938 that decomposes a classical (affine) quadratic form <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Z}_4}" class="latex" title="{\mathsf{Z}_4}" /> further as <a name="repn"></a></p><a name="repn">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%29+%3D+f_0%28x%29+%2B+2%28x+%5Cbullet+v%29+%5Cquad%5Ctext%7Bwith%7D%5Cquad+f_0%28x%29+%3D+x%5E%5Ctop+B+x%2C+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f(x) = f_0(x) + 2(x \bullet v) \quad\text{with}\quad f_0(x) = x^\top B x, \ \ \ \ \ (1)" class="latex" title="\displaystyle  f(x) = f_0(x) + 2(x \bullet v) \quad\text{with}\quad f_0(x) = x^\top B x, \ \ \ \ \ (1)" /></p>
</a><p><a name="repn"></a> for binary arguments <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />. Here <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v}" class="latex" title="{v}" /> is a binary vector with <img src="https://s0.wp.com/latex.php?latex=%7Bv_i+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v_i = 1}" class="latex" title="{v_i = 1}" /> if <img src="https://s0.wp.com/latex.php?latex=%7BS%5Bi%2Ci%5D+%3D+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S[i,i] = 2}" class="latex" title="{S[i,i] = 2}" /> or <img src="https://s0.wp.com/latex.php?latex=%7BS%5Bi%2Ci%5D+%3D+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S[i,i] = 3}" class="latex" title="{S[i,i] = 3}" />, <img src="https://s0.wp.com/latex.php?latex=%7Bv_i+%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v_i =0}" class="latex" title="{v_i =0}" /> otherwise, and the operations including the inner product <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet}" class="latex" title="{\bullet}" /> are mod-2 except that the final <img src="https://s0.wp.com/latex.php?latex=%7B%2B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{+}" class="latex" title="{+}" /> is in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_4}" class="latex" title="{\mathbb{Z}_4}" />. Then <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> is <em>alternating</em> if the diagonal of <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> is all-zero, <em>non-alternating</em> otherwise. Now take <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> to be the rank of <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" />. The key normal-form lemma is:</p>
<blockquote><p><b>Lemma 2</b> <em> There is a change of basis to <img src="https://s0.wp.com/latex.php?latex=%7By_1%2C%5Cdots%2Cy_n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{y_1,\dots,y_n}" class="latex" title="{y_1,\dots,y_n}" /> such that if <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> is non-alternating then <img src="https://s0.wp.com/latex.php?latex=%7Bf_0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{f_0}" class="latex" title="{f_0}" /> is transformed to </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%27_0%28y%29+%3D+y_1+%2B+y_2+%2B+%5Ccdots+%2B+y_r%2C+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="\displaystyle  f'_0(y) = y_1 + y_2 + \cdots + y_r, " class="latex" title="\displaystyle  f'_0(y) = y_1 + y_2 + \cdots + y_r, " /></p>
<p>whereas if <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> is alternating then <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> is even and <img src="https://s0.wp.com/latex.php?latex=%7Bf_0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{f_0}" class="latex" title="{f_0}" /> is transformed to </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%27_0%28y%29+%3D+2y_1+y_2+%2B+2y_3+y_4+%2B+%5Ccdots+%2B+2y_%7Br-1%7D+y_r.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="\displaystyle  f'_0(y) = 2y_1 y_2 + 2y_3 y_4 + \cdots + 2y_{r-1} y_r. " class="latex" title="\displaystyle  f'_0(y) = 2y_1 y_2 + 2y_3 y_4 + \cdots + 2y_{r-1} y_r. " /></p>
</em><p><em>In either case, there is a binary vector <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" /> so that <img src="https://s0.wp.com/latex.php?latex=%7Bf%28y%29+%3D+f%27_0%28y%29+%2B+2%28y+%5Cbullet+w%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{f(y) = f'_0(y) + 2(y \bullet w)}" class="latex" title="{f(y) = f'_0(y) + 2(y \bullet w)}" /> for all <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />. </em>
</p></blockquote>
<p></p><p>
The point is that to evaluate the quantum circuit <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" />, we don’t need to evaluate <img src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_C}" class="latex" title="{f_C}" />, but can make inferences about the structure of the solution sets to <img src="https://s0.wp.com/latex.php?latex=%7Bf_C%28x%29+%3D+a%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_C(x) = a}" class="latex" title="{f_C(x) = a}" /> for <img src="https://s0.wp.com/latex.php?latex=%7Ba+%3D+0%2C1%2C2%2C3+%5Cpmod%7B4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a = 0,1,2,3 \pmod{4}}" class="latex" title="{a = 0,1,2,3 \pmod{4}}" />, where <img src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cin+%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x \in \{0,1\}^n}" class="latex" title="{x \in \{0,1\}^n}" />. Given the knowledge of <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" />, the normal form goes a long way to this. The vector <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" /> is also needed, but the fact of its having only <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> bits gives hope of finding it in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29+%3D+O%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2) = O(N)}" class="latex" title="{O(n^2) = O(N)}" /> time. That—plus an analysis of the normal form <img src="https://s0.wp.com/latex.php?latex=%7Bf%27_0%2Cw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f'_0,w}" class="latex" title="{f'_0,w}" /> itself of course—would complete an <img src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(N)}" class="latex" title="{O(N)}" />-time reduction from computing the amplitude to computing <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" />.</p>
<p>
</p><p></p><h2> The Needed Piece—For Now </h2><p></p>
<p></p><p>
Chaowen took the lead all through the Fall 2018 term in trying multiple attacks. In the non-alternating case, the change of basis converts <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> into a diagonal matrix <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{F}_2}" class="latex" title="{\mathbb{F}_2}" />. In the alternating case, the same process makes <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> a block-diagonal matrix of the kind we mentioned above. The conversion <img src="https://s0.wp.com/latex.php?latex=%7BD+%3D+Q+B+Q%5E%5Ctop%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D = Q B Q^\top}" class="latex" title="{D = Q B Q^\top}" /> in both cases also yields <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" />. Of course <img src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q}" class="latex" title="{Q}" /> can be computed by Gaussian elimination in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^3)}" class="latex" title="{O(n^3)}" /> time, but this is what we wanted to avoid.</p>
<p>
After poring over older literature on <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^\omega}" class="latex" title="{n^\omega}" />-time methods, including a 1974 <a href="https://www.ams.org/journals/mcom/1974-28-125/S0025-5718-1974-0331751-8/">paper</a> by James Bunch and John Hopcroft (see also <a href="http://renatoppl.com/blog/2014/08/12/solving-linear-systems-and-inverting-a-matrix-is-equivalent-to-matrix-multiplication/">this</a>), we found a <a href="https://arxiv.org/abs/1802.10453">paper</a> from last year by Jean-Guillaume Dumas and Clément Pernet that gives exactly what we needed: an LDU-type decomposition that yields <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" /> time. We only needed to apply the change-of-basis analysis in Schmidt’s paper to this decomposition and combine with the normal-form analysis to establish our algorithm for computing the amplitude <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle 0^n |C| 0^n \rangle}" class="latex" title="{\langle 0^n |C| 0^n \rangle}" />: </p>
<ol>
<li>
Convert <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> to the classical quadratic form <img src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_C}" class="latex" title="{f_C}" /> with matrix <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" /> over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_4}" class="latex" title="{\mathbb{Z}_4}" /> and associate the <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n \times n}" class="latex" title="{n \times n}" /> matrix <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_2}" class="latex" title="{\mathbb{Z}_2}" /> as above. This needs only <img src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(N)}" class="latex" title="{O(N)}" /> time. <p></p>
</li><li>
Compute the Dumas-Pernet decomposition <img src="https://s0.wp.com/latex.php?latex=%7BB+%3D+PLDL%5E%5Ctop+P%5E%5Ctop%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B = PLDL^\top P^\top}" class="latex" title="{B = PLDL^\top P^\top}" /> over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_2}" class="latex" title="{\mathbb{Z}_2}" /> where <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" /> is a permutation matrix, <img src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{L}" class="latex" title="{L}" /> is lower-triangular, and <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> is block-diagonal with blocks that are either <img src="https://s0.wp.com/latex.php?latex=%7B1+%5Ctimes+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1 \times 1}" class="latex" title="{1 \times 1}" /> or <img src="https://s0.wp.com/latex.php?latex=%7B2+%5Ctimes+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2 \times 2}" class="latex" title="{2 \times 2}" />. Of course, this involves computing the rank <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> of <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> and takes <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" /> time. Think of it as <img src="https://s0.wp.com/latex.php?latex=%7BD+%3D+QBQ%5E%5Ctop%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D = QBQ^\top}" class="latex" title="{D = QBQ^\top}" />. This takes <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" /> time—indeed, <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2+r%5E%7B%5Comega+-+2%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2 r^{\omega - 2})}" class="latex" title="{O(n^2 r^{\omega - 2})}" /> time according to Dumas and Pernet. <p></p>
</li><li>
Compute <img src="https://s0.wp.com/latex.php?latex=%7BD%27+%3D+Q+S+Q%5E%5Ctop%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D' = Q S Q^\top}" class="latex" title="{D' = Q S Q^\top}" /> over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_4}" class="latex" title="{\mathbb{Z}_4}" />. This, too, takes <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" /> time. <p></p>
</li><li>
If any diagonal <img src="https://s0.wp.com/latex.php?latex=%7B1+%5Ctimes+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1 \times 1}" class="latex" title="{1 \times 1}" /> block of the original <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> has become <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> in <img src="https://s0.wp.com/latex.php?latex=%7BD%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D'}" class="latex" title="{D'}" />, output <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle 0^n |C| 0^n \rangle = 0}" class="latex" title="{\langle 0^n |C| 0^n \rangle = 0}" />. Else, <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle 0^n |C| 0^n \rangle}" class="latex" title="{\langle 0^n |C| 0^n \rangle}" /> is nonzero and we have enough information about <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" /> to find it—in only <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n)}" class="latex" title="{O(n)}" /> time, in fact.
</li></ol>
<p>
This proves our main theorem:</p>
<blockquote><p><b>Theorem 3</b> <em><a name="rank2QC"></a> For stabilizer circuits <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" />, <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\langle 0^n |C| 0^n \rangle}" class="latex" title="{\langle 0^n |C| 0^n \rangle}" /> is computable in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" /> time. So is counting binary solutions to a classical quadratic form over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_4}" class="latex" title="{\mathbb{Z}_4}" />, or any quadratic polynomial mod 2. </em>
</p></blockquote>
<p></p><p>
Because we use the decomposition, the above is not a clean <img src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(N)}" class="latex" title="{O(N)}" />-time reduction to computing <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" />. It does not make Theorem <a href="https://rjlipton.wordpress.com/feed/#rank2QC">3</a> into a linear-time equivalence. By further analysis, however, we show that the only impediment is needing <img src="https://s0.wp.com/latex.php?latex=%7BD%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D'}" class="latex" title="{D'}" /> in step 4 of our algorithm to tell whether <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\langle 0^n |C| 0^n \rangle = 0}" class="latex" title="{\langle 0^n |C| 0^n \rangle = 0}" />. If we are <a href="https://rjlipton.wordpress.com/2010/09/05/promise-problems-and-twopaths/">promised</a> that it is nonzero, then we obtain the probability <img src="https://s0.wp.com/latex.php?latex=%7B%7C%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|\langle 0^n |C| 0^n \rangle|^2}" class="latex" title="{|\langle 0^n |C| 0^n \rangle|^2}" /> in <img src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(N)}" class="latex" title="{O(N)}" /> time from <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> alone. This is actually where the power of Chaowen’s analysis of the normal forms is brightest and neatest. We will devote further posts to this and to illuminating further connections in graph and matroid theory.</p>
<p>
</p><p></p><h2> A Three-Part Example </h2><p></p>
<p></p><p>
Consider the following quantum circuit <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" />. OK, this is a very low-tech drawing. Besides the six Hadamard gates it has two <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BCZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{CZ}}" class="latex" title="{\mathsf{CZ}}" /> gates, which are shown as simple bars since they are symmetric:</p>
<p>
<a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/c1/" rel="attachment wp-att-15927"><img src="https://rjlipton.files.wordpress.com/2019/06/c1.png?w=240&amp;h=112" alt="" width="240" class="aligncenter wp-image-15927" height="112" /></a></p>
<p>
By the rules given <a href="https://rjlipton.wordpress.com/2012/07/08/grilling-quantum-circuits/">here</a>, the three Hadamard gates at left introduce “nondeterministic variables” <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%2Cx_2%2Cx_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1,x_2,x_3}" class="latex" title="{x_1,x_2,x_3}" />. The three Hadamard gates at right also give nondeterministic variables, but they are immediately equated to the output variables <img src="https://s0.wp.com/latex.php?latex=%7Bz_1%2Cz_2%2Cz_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{z_1,z_2,z_3}" class="latex" title="{z_1,z_2,z_3}" /> so we skip them. The polynomial <img src="https://s0.wp.com/latex.php?latex=%7Bq_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_C}" class="latex" title="{q_C}" /> is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2u_1+x_1+%2B+2u_2+x_2+%2B+2u_3+x_3+%2B+2x_1+x_2+%2B+2+x_2+x_3+%2B+2+x_1+z_1+%2B+2+x_2+z_2+%2B+2+x_3+z_3.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  2u_1 x_1 + 2u_2 x_2 + 2u_3 x_3 + 2x_1 x_2 + 2 x_2 x_3 + 2 x_1 z_1 + 2 x_2 z_2 + 2 x_3 z_3. " class="latex" title="\displaystyle  2u_1 x_1 + 2u_2 x_2 + 2u_3 x_3 + 2x_1 x_2 + 2 x_2 x_3 + 2 x_1 z_1 + 2 x_2 z_2 + 2 x_3 z_3. " /></p>
<p>Upon substituting <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> for all of <img src="https://s0.wp.com/latex.php?latex=%7Bu_1%2Cu_2%2Cu_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u_1,u_2,u_3}" class="latex" title="{u_1,u_2,u_3}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bz_1%2Cz_2%2Cz_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{z_1,z_2,z_3}" class="latex" title="{z_1,z_2,z_3}" /> this gives simply <img src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29+%3D+2x_1+x_2+%2B+2+x_2+x_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(x) = 2x_1 x_2 + 2 x_2 x_3}" class="latex" title="{f(x) = 2x_1 x_2 + 2 x_2 x_3}" />. This is an alternating form with </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S+%3D+B+%3D+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  S = B = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}, " class="latex" title="\displaystyle  S = B = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}, " /></p>
<p>which is the adjacency matrix of the path graph of length 2 on <img src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n = 3}" class="latex" title="{n = 3}" /> vertices. Gaussian elimination does not need any prior swaps, so the permutation matrix <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" /> in the decomposition is the identity and we get </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5Cend%7Bbmatrix%7D%2C+%5Cquad%5Ctext%7Bgiving%7D%5Cquad+D+%3D+QBQ%5E%5Ctop+%3D+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+0+%5C%5C+0+%26+0+%26+0+%5Cend%7Bbmatrix%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  Q = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix}, \quad\text{giving}\quad D = QBQ^\top = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{bmatrix} " class="latex" title="\displaystyle  Q = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix}, \quad\text{giving}\quad D = QBQ^\top = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{bmatrix} " /></p>
<p>as the block-diagonal matrix over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_2}" class="latex" title="{\mathbb{Z}_2}" />. Now we re-compute the products over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_4}" class="latex" title="{\mathbb{Z}_4}" /> to get </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q+S+Q%5E%7B%5Ctop%7D+%5C%21%5C%21%3D%5C%21%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot+Q%5E%5Ctop+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+2+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5C%5C+0+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+2+%5C%5C+0+%26+2+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+D%27.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^\top \!=\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 2 &amp; 0 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 2 \\ 0 &amp; 2 &amp; 0 \end{bmatrix} \!=\! D'. " class="latex" title="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^\top \!=\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 2 &amp; 0 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 2 \\ 0 &amp; 2 &amp; 0 \end{bmatrix} \!=\! D'. " /></p>
<p>Now <img src="https://s0.wp.com/latex.php?latex=%7BD%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D'}" class="latex" title="{D'}" /> has entries that are <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> but they are off-diagonal, and hence cancel when <img src="https://s0.wp.com/latex.php?latex=%7By%5E%5Ctop+D%27+y%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y^\top D' y}" class="latex" title="{y^\top D' y}" /> is computed in the <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />-basis. Since <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" /> is likewise the zero vector, this gives the transformed form as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%27_0%28y_1%2Cy_2%2Cy_3%29+%3D+2y_1+y_2.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f'_0(y_1,y_2,y_3) = 2y_1 y_2. " class="latex" title="\displaystyle  f'_0(y_1,y_2,y_3) = 2y_1 y_2. " /></p>
<p>It is easy to compute that <img src="https://s0.wp.com/latex.php?latex=%7Bf%27_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f'_0}" class="latex" title="{f'_0}" /> has six values of 0 and two values of 2, which gives the amplitude as the difference <img src="https://s0.wp.com/latex.php?latex=%7B6+-+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{6 - 2}" class="latex" title="{6 - 2}" /> divided by the square root of <img src="https://s0.wp.com/latex.php?latex=%7B2%5E6%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^6}" class="latex" title="{2^6}" />, so <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac{1}{2}}" class="latex" title="{\frac{1}{2}}" />, The probability of getting <img src="https://s0.wp.com/latex.php?latex=%7B000%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{000}" class="latex" title="{000}" /> as the result of the measurement is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac{1}{4}}" class="latex" title="{\frac{1}{4}}" />.</p>
<p>
Now suppose we insert a <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Z}}" class="latex" title="{\mathsf{Z}}" />-gate <img src="https://s0.wp.com/latex.php?latex=%7B%5Cbegin%7Bbmatrix%7D+1+%26+0+%5C%5C+0+%26+-1+%5Cend%7Bbmatrix%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -1 \end{bmatrix}}" class="latex" title="{\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -1 \end{bmatrix}}" /> on the first qubit to make a new circuit <img src="https://s0.wp.com/latex.php?latex=%7BC_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_2}" class="latex" title="{C_2}" />. Since <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Z}}" class="latex" title="{\mathsf{Z}}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BCZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{CZ}}" class="latex" title="{\mathsf{CZ}}" /> are diagonal in the standard basis it does not matter where between the Hadamard gates it goes, say:</p>
<p></p><p><br />
<a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/c2/" rel="attachment wp-att-15928"><img src="https://rjlipton.files.wordpress.com/2019/06/c2.png?w=240&amp;h=106" alt="" width="240" class="aligncenter wp-image-15928" height="106" /></a></p>
<p></p><p><br />
After substituting zeroes the form over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}}" class="latex" title="{\mathbb{Z}}" /> is <img src="https://s0.wp.com/latex.php?latex=%7Bg+%3D+2x_1+x_2+%2B+2+x_2+x_3+%2B+2x_1%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g = 2x_1 x_2 + 2 x_2 x_3 + 2x_1^2}" class="latex" title="{g = 2x_1 x_2 + 2 x_2 x_3 + 2x_1^2}" />. This gives </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S+%3D+%5Cbegin%7Bbmatrix%7D+2+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D%2C%5Cquad+v+%3D+%281%2C0%2C0%29%2C+%5Cquad+B+%3D+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  S = \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix},\quad v = (1,0,0), \quad B = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}. " class="latex" title="\displaystyle  S = \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix},\quad v = (1,0,0), \quad B = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}. " /></p>
<p>The matrix <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> is the same as in the first example, hence so are the matrices <img src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q}" class="latex" title="{Q}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" /> and the alternating status of <img src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{g}" class="latex" title="{g}" />. The difference made by <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v}" class="latex" title="{v}" /> and the resulting <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" /> makes itself felt when we re-compute over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_4}" class="latex" title="{\mathbb{Z}_4}" />: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q+S+Q%5E%7B%5Ctop%7D+%5C%21%5C%21%3D%5C%21%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21+%5Cbegin%7Bbmatrix%7D+2+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot+Q%5E%5Ctop+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+2+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+2+%26+2+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5C%5C+0+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+2+%26+1+%26+2+%5C%5C+1+%26+0+%26+2+%5C%5C+2+%26+2+%26+2+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+D%27.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^\top \!=\! \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 2 &amp; 2 &amp; 0 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 2 &amp; 1 &amp; 2 \\ 1 &amp; 0 &amp; 2 \\ 2 &amp; 2 &amp; 2 \end{bmatrix} \!=\! D'. " class="latex" title="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^\top \!=\! \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 2 &amp; 2 &amp; 0 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 2 &amp; 1 &amp; 2 \\ 1 &amp; 0 &amp; 2 \\ 2 &amp; 2 &amp; 2 \end{bmatrix} \!=\! D'. " /></p>
<p>Well, <img src="https://s0.wp.com/latex.php?latex=%7BD%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D'}" class="latex" title="{D'}" /> is far from diagonal—perhaps we shouldn’t use that name—but again the off-diagonal <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />s are innocuous so we really have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%27%27+%3D+%5Cbegin%7Bbmatrix%7D+2+%26+1+%26+0+%5C%5C+1+%26+0+%26+0+%5C%5C+0+%26+0+%26+2+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  D'' = \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 2 \end{bmatrix}. " class="latex" title="\displaystyle  D'' = \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 2 \end{bmatrix}. " /></p>
<p>The <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> at upper left does not zero out the amplitude, because it is within a <img src="https://s0.wp.com/latex.php?latex=%7B2+%5Ctimes+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2 \times 2}" class="latex" title="{2 \times 2}" /> block. The <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> at lower right, however, constitutes a <img src="https://s0.wp.com/latex.php?latex=%7B1+%5Ctimes+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1 \times 1}" class="latex" title="{1 \times 1}" /> block of <img src="https://s0.wp.com/latex.php?latex=%7BD%27%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D''}" class="latex" title="{D''}" />, so it signifies that <img src="https://s0.wp.com/latex.php?latex=%7B000%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{000}" class="latex" title="{000}" /> is not a possible measurement outcome. Essentially what has happened is that in the <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />-basis the form has become </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%27%28y%29+%3D+2y_1%5E2+%2B+2y_1+y_2+%2B+2y_3%5E2.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  g'(y) = 2y_1^2 + 2y_1 y_2 + 2y_3^2. " class="latex" title="\displaystyle  g'(y) = 2y_1^2 + 2y_1 y_2 + 2y_3^2. " /></p>
<p>The isolated term in <img src="https://s0.wp.com/latex.php?latex=%7By_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y_3}" class="latex" title="{y_3}" /> contributes <img src="https://s0.wp.com/latex.php?latex=%7B%2B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{+2}" class="latex" title="{+2}" /> mod <img src="https://s0.wp.com/latex.php?latex=%7B4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{4}" class="latex" title="{4}" /> to half the <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />–<img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> assignments so as to cancel the other half, leaving a difference of <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> in the numerator of the amplitude.</p>
<p>
For the third example, let us insert a phase gate <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{S}}" class="latex" title="{\mathsf{S}}" /> after the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Z}}" class="latex" title="{\mathsf{Z}}" /> to make a circuit <img src="https://s0.wp.com/latex.php?latex=%7BC_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_3}" class="latex" title="{C_3}" />:</p>
<p></p><p><br />
<a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/c3/" rel="attachment wp-att-15929"><img src="https://rjlipton.files.wordpress.com/2019/06/c3.png?w=240&amp;h=90" alt="" width="240" class="aligncenter wp-image-15929" height="90" /></a></p>
<p></p><p><br />
The <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{ZS}}" class="latex" title="{\mathsf{ZS}}" /> combination is the same as <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BS%5E%2A%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{S^*}}" class="latex" title="{\mathsf{S^*}}" />, the adjoint (and inverse) of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{S}}" class="latex" title="{\mathsf{S}}" />. Now after substitutions we have <img src="https://s0.wp.com/latex.php?latex=%7Bh_%7BC_3%7D%28x%29+%3D+2x_1+x_2+%2B+2+x_2+x_3+%2B+3x_1%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{h_{C_3}(x) = 2x_1 x_2 + 2 x_2 x_3 + 3x_1^2}" class="latex" title="{h_{C_3}(x) = 2x_1 x_2 + 2 x_2 x_3 + 3x_1^2}" />, giving: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S+%3D+%5Cbegin%7Bbmatrix%7D+3+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D%2C%5Cquad+v+%3D+%281%2C0%2C0%29%2C+%5Cquad+B+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  S = \begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix},\quad v = (1,0,0), \quad B = \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}. " class="latex" title="\displaystyle  S = \begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix},\quad v = (1,0,0), \quad B = \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}. " /></p>
<p>Note that <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> is still a 0-1 matrix. This <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> has full rank. Again it helps our exposition that <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> is diagonalizable without swaps (and that the inverse of an invertible lower-triangular matrix is lower-triangular), so we can find <img src="https://s0.wp.com/latex.php?latex=%7BQBQ%5E%5Ctop+%3D+D+%3D+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{QBQ^\top = D = I}" class="latex" title="{QBQ^\top = D = I}" /> with </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+1+%26+1+%26+0+%5C%5C+1+%26+1+%26+1+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  Q = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \end{bmatrix}. " class="latex" title="\displaystyle  Q = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \end{bmatrix}. " /></p>
<p>In the <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />-basis we get <img src="https://s0.wp.com/latex.php?latex=%7Bh%27%28y%29+%3D+y_1+%2B+y_2+%2B+y_3+%2B+2%28y+%5Cbullet+w%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{h'(y) = y_1 + y_2 + y_3 + 2(y \bullet w)}" class="latex" title="{h'(y) = y_1 + y_2 + y_3 + 2(y \bullet w)}" /> for some <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" />. To test for zero amplitude—before we know what <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" /> is—we compute in <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_4}" class="latex" title="{\mathbb{Z}_4}" />: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q+S+Q%5E%7B%5Ctop%7D+%5C%21%5C%21%3D%5C%21%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+1+%26+1+%26+0+%5C%5C+0+%26+1+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21%5Cbegin%7Bbmatrix%7D+3+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot+Q%5E%7B%5Ctop%7D+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+3+%26+1+%26+0+%5C%5C+0+%26+1+%26+1+%5C%5C+0+%26+2+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+1+%26+1+%5C%5C+0+%26+1+%26+1+%5C%5C+0+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+3+%26+0+%26+0+%5C%5C+0+%26+1+%26+2+%5C%5C+0+%26+2+%26+3+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+D%27.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \end{bmatrix} \!\cdot\!\begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^{\top} \!=\! \begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 2 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 3 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 2 \\ 0 &amp; 2 &amp; 3 \end{bmatrix} \!=\! D'. " class="latex" title="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \end{bmatrix} \!\cdot\!\begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^{\top} \!=\! \begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 2 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 3 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 2 \\ 0 &amp; 2 &amp; 3 \end{bmatrix} \!=\! D'. " /></p>
<p>Again we can ignore the off-diagonal <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />‘s. There is no <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> on the main diagonal, so we know the amplitude is non-zero. To compute it, we only need the information on the diagonal, which tells us <img src="https://s0.wp.com/latex.php?latex=%7Bh%27_0%28y%29+%3D+y_1+%2B+y_2+%2B+y_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{h'_0(y) = y_1 + y_2 + y_3}" class="latex" title="{h'_0(y) = y_1 + y_2 + y_3}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bw+%3D+%281%2C0%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w = (1,0,1)}" class="latex" title="{w = (1,0,1)}" /> in the transformed basis. Note that we could have written <img src="https://s0.wp.com/latex.php?latex=%7Bh%27_0%28y%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{h'_0(y)}" class="latex" title="{h'_0(y)}" /> down the moment we learned that <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> has rank <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3}" class="latex" title="{3}" /> over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{F}_2}" class="latex" title="{\mathbb{F}_2}" />, so <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" /> is the only rigmarole. The final analysis—using a recursion detailed in the appendix of our paper—gives the amplitude as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B2+-+2i%7D%7B8%7D+%3D+%5Cfrac%7B1+-+i%7D%7B4%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \frac{2 - 2i}{8} = \frac{1 - i}{4}, " class="latex" title="\displaystyle  \frac{2 - 2i}{8} = \frac{1 - i}{4}, " /></p>
<p>and so the probability of the output <img src="https://s0.wp.com/latex.php?latex=%7B000%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{000}" class="latex" title="{000}" /> is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B8%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac{1}{8}}" class="latex" title="{\frac{1}{8}}" />. </p>
<p>
We remark finally that <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" /> is generally not the same as <img src="https://s0.wp.com/latex.php?latex=%7BQv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Qv}" class="latex" title="{Qv}" />. To see where it comes from, let us now compute <img src="https://s0.wp.com/latex.php?latex=%7BQ+B+Q%5E%7B%5Ctop%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q B Q^{\top}}" class="latex" title="{Q B Q^{\top}}" /> (not <img src="https://s0.wp.com/latex.php?latex=%7BQSQ%5E%5Ctop%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{QSQ^\top}" class="latex" title="{QSQ^\top}" />) over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_4}" class="latex" title="{\mathbb{Z}_4}" /> to get <img src="https://s0.wp.com/latex.php?latex=%7BQBQ%5E%5Ctop+%3D+D+%2B+2U%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{QBQ^\top = D + 2U}" class="latex" title="{QBQ^\top = D + 2U}" />. Then </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++f%28x%29+%26%3D%26+x%5E%5Ctop+B+x+%2B+2x%5E%5Ctop+v+%3D+x%5E%5Ctop+Q%5E%7B-1%7D+%28D%2B2U%29+%28Q%5E%5Ctop%29%5E%7B-1%7D+x+%2B+2x%5E%5Ctop+%28Q%5E%7B-1%7D+Q%29+v%5C%5C+%26%3D%26+y%5E%5Ctop+%28D+%2B+2U%29+y+%2B+2+y%5E%5Ctop+Qv%2C+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \begin{array}{rcl}  f(x) &amp;=&amp; x^\top B x + 2x^\top v = x^\top Q^{-1} (D+2U) (Q^\top)^{-1} x + 2x^\top (Q^{-1} Q) v\\ &amp;=&amp; y^\top (D + 2U) y + 2 y^\top Qv, \end{array} " class="latex" title="\displaystyle  \begin{array}{rcl}  f(x) &amp;=&amp; x^\top B x + 2x^\top v = x^\top Q^{-1} (D+2U) (Q^\top)^{-1} x + 2x^\top (Q^{-1} Q) v\\ &amp;=&amp; y^\top (D + 2U) y + 2 y^\top Qv, \end{array} " /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7By+%3D+%28Q%5E%5Ctop%29%5E%7B-1%7D+x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y = (Q^\top)^{-1} x}" class="latex" title="{y = (Q^\top)^{-1} x}" />. Now off-diagonal elements in <img src="https://s0.wp.com/latex.php?latex=%7B2U%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2U}" class="latex" title="{2U}" /> will cancel when taking <img src="https://s0.wp.com/latex.php?latex=%7B2+y%5E%5Ctop+U+y%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2 y^\top U y}" class="latex" title="{2 y^\top U y}" /> modulo 4, so we need only retain the diagonal <img src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u}" class="latex" title="{u}" /> of <img src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{U}" class="latex" title="{U}" /> as a binary vector. Since <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" /> is binary, <img src="https://s0.wp.com/latex.php?latex=%7By%5E%5Ctop+%5Cmathit%7Bdiag%7D%28u%29+y+%3D+y%5E%5Ctop+u%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y^\top \mathit{diag}(u) y = y^\top u}" class="latex" title="{y^\top \mathit{diag}(u) y = y^\top u}" />. This finally gives </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%29+%3D+y%5E%5Ctop+D+y+%2B+2y%5E%5Ctop+%28u+%2B+Qv%29+%3D+y%5E%5Ctop+D+y+%2B+2%28y+%5Cbullet+w%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f(x) = y^\top D y + 2y^\top (u + Qv) = y^\top D y + 2(y \bullet w) " class="latex" title="\displaystyle  f(x) = y^\top D y + 2y^\top (u + Qv) = y^\top D y + 2(y \bullet w) " /></p>
<p>with <img src="https://s0.wp.com/latex.php?latex=%7Bw+%3D+u+%2B+Qv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w = u + Qv}" class="latex" title="{w = u + Qv}" />. In the third example we have <img src="https://s0.wp.com/latex.php?latex=%7BQv+%3D+%281%2C1%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Qv = (1,1,1)}" class="latex" title="{Qv = (1,1,1)}" /> and </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++QBQ%5E%7B%5Ctop%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+1+%26+1+%26+0+%5C%5C+0+%26+1+%26+1+%5Cend%7Bbmatrix%7D+%5Ccdot+%5Cbegin%7Bbmatrix%7D+1+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D+%5Ccdot+Q%5E%5Ctop+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+1+%26+0+%5C%5C+2+%26+1+%26+1+%5C%5C+2+%26+2+%26+1+%5Cend%7Bbmatrix%7D+%5Ccdot+%5Cbegin%7Bbmatrix%7D+1+%26+1+%26+1+%5C%5C+0+%26+1+%26+1+%5C%5C+0+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+2+%26+2+%5C%5C+2+%26+3+%26+0+%5C%5C+2+%26+0+%26+1+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  QBQ^{\top} = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \cdot Q^\top = \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 2 &amp; 1 &amp; 1 \\ 2 &amp; 2 &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} = \begin{bmatrix} 1 &amp; 2 &amp; 2 \\ 2 &amp; 3 &amp; 0 \\ 2 &amp; 0 &amp; 1 \end{bmatrix}. " class="latex" title="\displaystyle  QBQ^{\top} = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \cdot Q^\top = \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 2 &amp; 1 &amp; 1 \\ 2 &amp; 2 &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} = \begin{bmatrix} 1 &amp; 2 &amp; 2 \\ 2 &amp; 3 &amp; 0 \\ 2 &amp; 0 &amp; 1 \end{bmatrix}. " /></p>
<p>The diagonal gives <img src="https://s0.wp.com/latex.php?latex=%7Bu+%3D+%280%2C1%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u = (0,1,0)}" class="latex" title="{u = (0,1,0)}" /> and so <img src="https://s0.wp.com/latex.php?latex=%7Bw+%3D+u+%2B+Qv+%5Cpmod%7B2%7D+%3D+%281%2C0%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w = u + Qv \pmod{2} = (1,0,1)}" class="latex" title="{w = u + Qv \pmod{2} = (1,0,1)}" />. This agrees with what we read off above by comparing <img src="https://s0.wp.com/latex.php?latex=%7BD%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D'}" class="latex" title="{D'}" /> with <img src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{D}" class="latex" title="{D}" />. There is a different worked-out example for the triangle graph on three vertices in the paper.</p>
<p>
</p><p></p><h2> Looking Ahead </h2><p></p>
<p></p><p>
Chaowen and I continue to be interested in shortcuts to computing the amplitude and/or probability. Here we take a cue from how Volker Strassen titled his famous 1969 <a href="https://eudml.org/doc/131927">paper</a> on matrix multiplication:</p>
<blockquote><p><b> </b> <em> “Gaussian Elimination is not Optimal.” </em>
</p></blockquote>
<p></p><p>
We would like to find cases where we can say, “Matrix Multiplication is not Optimal.” In view of recent papers blunting efforts to show <img src="https://s0.wp.com/latex.php?latex=%7B%5Comega+%3D+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\omega = 2}" class="latex" title="{\omega = 2}" />—see this <a href="https://rjlipton.wordpress.com/2018/08/30/limits-on-matrix-multiplication/">post</a>—the question may shift to which computations may not need the full power of matrix multiplication and be achievable in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> time after all. This applies to computing the rank (over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_2}" class="latex" title="{\mathbb{Z}_2}" />) itself, and the question extends to sparse cases like those considered in the <a href="http://www-scf.usc.edu/~hoyeeche/papers/matrix-rank.pdf">paper</a>, “Fast Matrix Rank Algorithms and Applications,” by Ho Yee Cheung, Tsz Chiu Kwok, and Lap Chi Lau.</p>
<p>
The second circuit in the above example corresponds to a graph with a self-loop at node 1—or, depending on how one counts incidence of self-loops in undirected graphs, one could call it a double self-loop. It exemplifies circuits used to create quantum <a href="https://en.wikipedia.org/wiki/Graph_state">graph states</a>, and those circuits are representative of stabilizer circuits in general. The third circuit can be said to have a “triple loop,” or maybe better, a “3/2-loop”—while if the original <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Z}}" class="latex" title="{\mathsf{Z}}" />-gate were a single <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{S}}" class="latex" title="{\mathsf{S}}" />-gate giving the form <img src="https://s0.wp.com/latex.php?latex=%7Bx_1%5E2+%2B+2x_1+x_2+%2B+2+x_2+x_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_1^2 + 2x_1 x_2 + 2 x_2 x_3}" class="latex" title="{x_1^2 + 2x_1 x_2 + 2 x_2 x_3}" />, we would face the ambiguity of calling it a “loop” or a “half-loop.” Sorting this out properly needs going beyond graph theory. In upcoming posts, Chaowen and I will say more about how all this yields new problems in graph theory and new connections between quantum computing and <em>matroid theory</em>.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
What do our results say about the problem of computing the rank of a matrix, and possibly separating it from dependence on matrix multiplication?</p>
<p>
We hope that we have begun to convey how our paper uncovers a lot of fun computational mathematics. We are grateful for communications from people we’ve approached (some acknowledged in our paper) about possible known connections, but there may be more we don’t know. Our next posts will say more about combinatorial aspects of quantum circuits.</p>
<p>
[fixed name]</p></font></font></div>







<p class="date">
by KWRegan <a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/"><span class="datestr">at June 04, 2019 07:55 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://theorydish.blog/?p=1500">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/theorydish.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://theorydish.blog/2019/06/04/itcs20-call-for-papers/">ITCS’20 Call for Papers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p style="text-align: justify;"><em>ITCS is one of my favorite (if not my favorite) conferences, with not only great and insightful papers but also a friendly atmosphere. This year should be no exception!</em></p>
<p><em><strong>tl;dr:</strong> the ITCS’20 CFP has been <a href="http://itcs-conf.org/itcs20/itcs20-cfp.html" target="_blank" rel="noopener">posted</a>. Read it, and submit your work there!</em></p>
<hr />
<p> </p>
<p style="text-align: justify;">We invite you to submit your papers to the <a href="http://itcs-conf.org/" target="_blank" rel="noopener">11th Innovations in</a> <a href="http://itcs-conf.org/" target="_blank" rel="noopener">Theoretical Computer Science</a> (ITCS). The conference will be held at the University of Washington in Seattle, Washington from January 12-14, 2020.</p>
<p style="text-align: justify;">ITCS seeks to promote research that carries a strong conceptual message (e.g., introducing a new concept, model or understanding, opening a new line of inquiry within traditional or interdisciplinary areas, introducing new mathematical techniques and methodologies, or new applications of known techniques). ITCS welcomes both conceptual and technical contributions whose contents will advance and inspire the<br />
greater theory community.</p>
<p><strong>Important dates</strong></p>
<ul>
<li><em>Submission deadline:</em> September 9, 2019 (05:59pm PDT)</li>
<li><em>Notification to authors:</em> October 31, 2019</li>
<li><em>Conference dates:</em> January 12-14, 2020</li>
</ul>
<p>See the website at <a href="http://itcs-conf.org/itcs20/itcs20-cfp.html">http://itcs-conf.org/itcs20/itcs20-cfp.html</a> for detailed information regarding submissions.</p>
<p><strong>Program committee</strong></p>
<p>Nikhil Bansal, CWI + TU Eindhoven<br />
Nir Bitansky, Tel-Aviv University<br />
Clement Canonne, Stanford<br />
Timothy Chan, University of Ilinois at Urbana-Champaign<br />
Edith Cohen, Google and Tel-Aviv University<br />
Shaddin Dughmi, University of Southern California<br />
Sumegha Garg, Princeton<br />
Ankit Garg, Microsoft research<br />
Ran Gelles, Bar-Ilan University<br />
Elena Grigorescu, Purdue<br />
Tom Gur, University of Warwick<br />
Sandy Irani, UC Irvine<br />
Dakshita Khurana, University of Illinois at Urbana-Champaign<br />
Antonina Kolokolova, Memorial University of Newfoundland.<br />
Pravesh Kothari, Carnegie Mellon University<br />
Rasmus Kyng, Harvard<br />
Katrina Ligett, Hebrew University<br />
Nutan Limaye, IIT Bombay<br />
Pasin Manurangsi, UC Berkeley<br />
Tamara Mchedlidze, Karlsruhe Institute of Technology<br />
Dana Moshkovitz, UT Austin<br />
Jelani Nelson, UC Berkeley<br />
Merav Parter, Weizmann Institute<br />
Krzysztof Pietrzak, IST Austria<br />
Elaine Shi, Cornell<br />
Piyush Srivastava, Tata Institute of Fundamental Research, Mumbai<br />
Li-Yang Tan, Stanford<br />
Madhur Tulsiani, TTIC<br />
Gregory Valiant, Stanford<br />
Thomas Vidick, California Institute of Technology (chair)<br />
Virginia Vassilevska Williams, MIT<br />
Ronald de Wolf, CWI and University of Amsterdam<br />
David Woodruff, Carnegie Mellon University</p></div>







<p class="date">
by ccanonne <a href="https://theorydish.blog/2019/06/04/itcs20-call-for-papers/"><span class="datestr">at June 04, 2019 06:58 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3116995407343145604">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/06/imus-non-controversial-changing-name-of.html">IMU's non-controversial changing the name of the Nevanlinna Prize</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
(I want to thank Alexander Soifer for supplying me with some of the documents I point to in this post. We should all thank him for getting the ball rolling on changing the name of the Nevanlinna Prize.)<br />
<br />
The <i>Nevanlinna Prize </i>was essentially a Fields Medal for Theoretical Computer Science.  I do not know why it is a<i> Prize </i>instead of a <i>Medal.</i><br />
<div>
<br /></div>
<div>
It has been renamed <i>The Abacus Medal. </i>If you want to know why the IMU (International Mathematics Union) thinks the new name is good <i>but do not </i><i>care even a little about why the original name was bad</i> then see this article: <a href="https://www.heidelberg-laureate-forum.org/blog/imu-abacus-medal/">here</a>.</div>
<div>
<br /></div>
<div>
So why is <i>The Nevanlinna Prize</i> a bad name? In brief, Rolf Nevanlinna was an enthusiastic Nazi sympathizer. How enthused? He served as the chair of the Finish SS recruitment committee.<br />
<br />
That would seem like enough to get the name changed. In fact, it makes one wonder why the prize originally had the name.<br />
<br />
1) Why the change now?  It began when Alexander Soifer came across this information about Nevanlinna while working on his book<br />
<br />
<i>The Scholar and the State: In Search of Van der Waerdan</i> (see <a href="https://amzn.to/2WnfDYh">here</a> to buy it, see <a href="https://mathcs.clarku.edu/~fgreen/SIGACTReviews/bookrev/47-1.pdf">here</a> for a book review column that includes my review of it).<br />
<br />
He then wrote a letter to the IMU which sponsors the <i>Nevanlinna Prize</i>. The letter is <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/letterToImu.pdf">here</a>. Note that Alexander offered to pay for the prize ($15,000 every four years) if that will help get the name changed.<br />
<br />
After a response that lamely said (I paraphrase): <i>Gee, we didn't know. Oh well</i>. Alex wrote another letter which is <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/letterToImu2.pdf">here</a>.<br />
<br />
The story has a happy ending: the name was changed.  (No, Alexander is not paying for the award.)<br />
<br />
2) For a full summary of why the award was originally named Nevanlinna  and why it was changed see the article, <i>Yes We Can,  </i>by Alexander Soifer,<i> </i>in an issue of the journal <i>Mathematical Competition</i>s, see <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/yeswecan.pdf">here</a>.</div>
<div>
<br /></div>
<div>
3) When is change possible?<br />
<br /></div>
<div>
 Assume Y did X and X is awful (e.g., I assume for most of my readers believing and spreading Nazi propaganda). Assume there is a Y-prize. What does it take to have the name changed?<br />
<br /></div>
<div>
<br /></div>
<div>
a) You need someone pushing hard for it. Kudos to Alexander Soifer who started this.</div>
<div>
<br /></div>
<div>
b) There is no really good reason to use that name in the first place. </div>
<div>
<br /></div>
<div>
What was Nevanlinna's contribution to mathematical aspects of computer science? The IMU (International Mathematics Union) internet page answers:</div>
<div>
<br /></div>
<div>
<i>The prize was named in honors of Rolf Nevanlinna ... who in the 1950's had taken the initiative to the computer organization at Finnish Universities. </i></div>
<div>
<i><br /></i></div>
<div>
That's all. If there was a Gauss Prize (actually there IS a Gauss Prize) and we later found out that Gauss was X, I doubt we would change the name of the award. Gauss's name is on it since he is a great mathematician. </div>
<div>
<br /></div>
<div>
c) The person on the award is not the one giving the money. If we found out that Nobel was an X,  I doubt the name would change since he is paid for it. </div>
<div>
<br /></div>
<div>
d) If the award name is well known then it might not change. Nobel is a good example. I think the Nevanlinna prize is mostly unknown to the public. The Field's medal is better known, though still not that well known. The general public became briefly aware of the Field's medal twice: when it was mentioned in the movie <i>Good Will Hunting,</i> and when Perelman turned it down. Fame is fleeting for both prizes and people.</div>
<div>
<br /></div>
<div>
e) Organizations don't like to change things. Hence X would need to be particularly bad to warrant a name change. </div>
<div>
<br /></div>
<div>
OTHER THOUGHTS</div>
<div>
<br /></div>
<div>
1) Why <i>The Abacus Medal</i>? Perhaps they are worried that if they name it after someone and that someone turns out to be an X they'll have to change it again. I find the explanation given <a href="https://www.heidelberg-laureate-forum.org/blog/imu-abacus-medal/">here</a> to be unsatisfying. I find the fact that they make <b>NO MENTION</b> of why they are no longer naming it <i>The</i> <i>Nevanlinna prize </i>appalling and insulting.</div>
<div>
<br /></div>
<div>
2) Lets turn to people who get the awards. If someone solved two Millennium problems and clearly deserved a Field's Medal, but was an X, should they be denied the prize on that basis. I would tend to think no (that is, they should get the prize) but it does trouble me. What would happen?  I honestly don't know.  </div>
<div>
<br /></div>
<div>
3) X will change over time.</div>
<div>
<br /></div></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/06/imus-non-controversial-changing-name-of.html"><span class="datestr">at June 04, 2019 03:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-32902056.post-2757759457141834790">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/goldberg.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://paulwgoldberg.blogspot.com/2019/06/plans-for-wine-conferences.html">plans for WINE conferences</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div style="text-align: left;" dir="ltr">Update on the annual Conference on Web and Internet Economics (I am on the steering committee).<br /><br /><a href="http://wine2019.cs.columbia.edu/">WINE 2019</a> (the 15th) will be at Columbia University, December 10-12. We could not avoid the clash with NeurIPS, due to Columbia’s exam schedule. Submission deadline is July 15th.<br /><br />The plan is for WINE 2020 to take place at Peking University, following the tradition to rotate between Europe, USA, and Asia.<br /><br />WINE 2021 is under discussion; one idea it to hold it in Addis Ababa, Ethiopia, which is not as novel as it may seem at first sight, it would be following ICLR 2020 (see <a href="https://venturebeat.com/2018/11/19/major-ai-conference-is-moving-to-africa-in-2020-due-to-visa-issues/">this link</a> (noting the visa issues) and others). The rationale is that Africa has a burgeoning AI community including people who are interested in algorithmic game theory, and for them, Ethiopia is an easy destination (administratively, in particular). WINE 2018 (at Oxford, UK) had (I think) 5 African participants, and about 10 more would have liked to come but were denied visas. These participants brought home to me the point that there is this developing AI community in Africa. At WINE 2018, Eric Sodomka (from Facebook) gave a well-received presentation on the idea of holding a future WINE in Africa. Are there other places in Africa we should be thinking of? I welcome feedback and comments!<br /><br /></div></div>







<p class="date">
by Paul Goldberg (noreply@blogger.com) <a href="http://paulwgoldberg.blogspot.com/2019/06/plans-for-wine-conferences.html"><span class="datestr">at June 04, 2019 10:46 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2019/06/03/trajectories/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2019/06/03/trajectories/">Is Optimization a Sufficient Language for Understanding Deep Learning?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In this Deep Learning era, machine learning usually boils down to defining a suitable objective/cost function for the learning task at hand, and then optimizing this function using some variant of gradient descent (implemented via backpropagation).  Little wonder that hundreds of ML papers each year are devoted to various aspects of optimization. Today I will suggest that if our goal is mathematical understanding of deep learning, then  the optimization viewpoint is potentially insufficient —at least in the conventional view:</p>

<blockquote>
  <p><strong>Conventional View (CV) of Optimization</strong>: Find a solution of minimum possible value of the objective, as fast as possible.</p>
</blockquote>

<p>Note that <em>a priori</em> it is not obvious if all learning should involve optimizing a single objective. Whether or not this is true for  learning in the brain is a longstanding open question in neuroscience. Brain components appear to have been repurposed/cobbled together through various accidents of evolution and the whole assemblage may or may not boil down to optimization of an objective. See <a href="https://arxiv.org/pdf/1606.03813.pdf">this survey by Marblestone et al</a>.</p>

<p>I am suggesting that deep learning algorithms also have important properties that are not always reflected in the objective value. Current deep nets, being vastly overparametrized, have multiple optima. They are trained until the objective is almost zero (i.e., close to optimality) and training is said to succeed if the optimum (or near-optimum) model thus found also performs well on unseen/held-out data —i.e., <em>generalizes.</em> The catch here is that the value of the objective may imply nothing about generalization (see <a href="https://arxiv.org/abs/1611.03530">Zhang et al.</a>).</p>

<p>Of course experts will now ask: “Wasn’t generalization theory invented precisely for this reason as the “second leg” of machine learning,  where optimization is the first leg?” For instance this theory shows how to add regularizers to the training objective to ensure the solution generalizes. Or that <em>early stopping</em> (i.e., stopping before reaching the optimum) or even adding noise to the gradient (e.g. by playing with batch sizes and learning rates) can be preferable to perfect optimization, even in simple settings such as regression.</p>

<p>However, in practice explicit regularizers  and noising tricks can’t prevent deep nets from attaining low training objective even on data with random labels; see <a href="https://arxiv.org/abs/1611.03530">Zhang et al.</a>. Current generalization theory is designed to give <em>post hoc</em> explanations for why a particular model generalized. It is agnostic about <em>how</em> the solution was obtained, and thus makes few prescriptions —apart from recommending some regularization— for optimization.   (See my earlier <a href="http://www.offconvex.org/2017/12/08/generalization1/">blog post</a>, which explains the distinction between descriptive and prescriptive methods, and  that generalization theory is primarily descriptive.) The fundamental mystery is:</p>

<blockquote>
  <p>Even vanilla gradient descent (GD) is good at finding models with reasonable generalization. Furthermore, methods to speed up gradient descent (e.g., acceleration or adaptive regularization) can sometimes lead to worse generalization.</p>
</blockquote>

<p>In other words, GD has an innate bias towards finding solutions with good generalization. Magic happens along the GD trajectory and is not captured in the objective value per se. We’re reminded of the old adage.</p>

<blockquote>
  <p>The journey matters more than the destination.</p>
</blockquote>

<p>I will illustrate this viewpoint by sketching new  rigorous analyses of gradient descent in two simple but suggestive settings. I  hope more  detailed writeups will appear in future blog posts.</p>

<p>Acknowledgements: My views on this topic were initially shaped by the excellent papers from TTI Chicago group regarding the implicit bias of gradient descent (<a href="https://arxiv.org/pdf/1709.01953.pdf">Behnam Neyshabur’s thesis</a> is a good starting point), and then of course by  various coauthors.</p>

<h2 id="computing-with-infinitely-wide-deep-nets">Computing with Infinitely Wide Deep Nets</h2>

<p>Since overparametrization does not appear to hurt deep nets too much, researchers have wondered what happens in the infinite limit of overparametrization: use a fixed training set such as CIFAR10 to train a classic deep net architecture like AlexNet or VGG19 whose “width” —namely, number of channels in the convolutional filters, and number of nodes in fully connected internal layers—- is allowed to increase to <strong>infinity</strong>. Note that initialization (using sufficiently small Gaussian weights) and training makes sense for any finite width, no matter how large. We assume $\ell_2$ loss at the output.</p>

<p>Understandably, such questions can seem hopeless and pointless: all the computing in the world is insufficient to train an infinite net, and we theorists already have our hands full trying to figure out finite nets.  But sometimes in math/physics one can derive insight into questions by studying them in the infinite limit.  Here where an infinite net is training on a finite dataset like CIFAR10, the number of optima is infinite and we are trying to understand what GD does.</p>

<p>Thanks to insights in recent papers on provable learning by overparametrized deep nets (some of the key papers are: <a href="https://arxiv.org/abs/1811.04918">Allen-Zhu et al 1</a>, <a href="https://arxiv.org/abs/1811.03962">Allen-Zhu et al 2</a> <a href="https://arxiv.org/abs/1811.03804">Du et al</a>, <a href="https://arxiv.org/abs/1811.08888">Zou et al</a>) researchers have realized that a nice limiting structure emerges:</p>

<blockquote>
  <p>As width $\rightarrow \infty$, trajectory approaches the trajectory of GD for a kernel regression problem, where the (fixed) kernel in question is the so-called  <em>Neural Tangent Kernel</em> (NTK). (For convolutional nets the kernel is <em>Convolutional NTK or CNTK.</em> )</p>
</blockquote>

<p>The kernel was identified and named by <a href="https://arxiv.org/abs/1806.07572">Jacot et al.</a>, and also implicit in some of the above-mentioned papers on overparametrized nets, e.g. <a href="https://arxiv.org/abs/1810.02054">Du et al</a>.</p>

<p>The definition of this fixed kernel uses the infinite net at its random initialization. For  two inputs $x_i$ and $x_j$ the kernel inner product  $K(x_i, x_j)$  is the inner product of the gradient $\nabla_x$ of the output with respect to the input, evaluated at $x=x_i$, and $x= x_j$ respectively. As the net size increases to infinity this kernel inner product can be shown to converge to a limiting value (there is a technicality about how to define the limit, and the series of new papers have improved the formal statement here; eg <a href="https://arxiv.org/abs/1902.04760">Yang2019</a> and our paper below.).</p>

<p>Our <a href="https://arxiv.org/abs/1904.11955">new paper with Simon Du, Wei Hu, Zhiyuan Li, Russ Salakhutdinov and Ruosang Wang</a> shows that the CNTK can be efficiently computed via dynamic programming, giving us a way to efficiently compute the answer of the trained net for any desired input,  <em>even though training the infinite net directly is of course computationally infeasible.</em> (Aside: Please do not confuse these new results with some earlier papers which view infinite nets as kernels or Gaussian Processes —see citations/discussion in our paper—  since they correspond to training only the top layer while freezing the lower layers to a random initialization.) Empirically we find that this infinite net (aka kernel regression with respect to the NTK) yields better performance on CIFAR10 than any previously known kernel —not counting kernels that were  hand-tuned or designed by training on image data. For instance we can compute the kernel corresponding to a 10-layer convolutional net (CNN) and obtain 77.4% success rate on CIFAR10.</p>

<h2 id="deep-matrix-factorization-for-solving-matrix-completion">Deep Matrix Factorization for solving Matrix Completion</h2>

<p><a href="https://en.wikipedia.org/wiki/Matrix_completion">Matrix completion</a>, motivated by design of recommender systems, is well-studied for over a decade: given $K$ random entries of an unknown matrix, we wish to recover the unseen entries. Solution is not unique in general. But if the unknown matrix is low rank or approximately low rank and satisfies some additional technical assumptions (eg <em>incoherence</em>) then various algorithms can recover the unseen entries approximately or even exactly. A famous algorithm  based upon <a href="https://en.wikipedia.org/wiki/Matrix_norm#Schatten_norms">nuclear/trace norm</a>  minimization is as follows: find matrix that fits all the known observations and has minimum nuclear norm. (Note that nuclear norm is a convex relaxation of rank.) It is also possible to rephrase this as a single objective in the form required by the Conventional View as follows where $S$ is the subset of indices of revealed entries,  $\lambda$ is a multiplier:</p>



<p>In case you didn’t know about nuclear norms, you will like the interesting suggestion made by <a href="http://papers.nips.cc/paper/7195-implicit-regularization-in-matrix-factorization">Gunasekar et al. 2017</a>: let us just forget about the nuclear norm penalty term  altogether. Instead try to recover the missing entries by  simply training (via simple gradient descent/backpropagation) a linear net with two layers on the first term in the loss. This linear net is just a multiplication of two $n\times n $ matrices (you can read about linear deep nets in this <a href="http://www.offconvex.org/2018/03/02/acceleration-overparameterization/">earlier blog post by Nadav Cohen</a>) so we obtain the following  where $e_i$ is the vector with all entries $0$ except for $1$ in the $i$th position:</p>



<p>The “data” now corresponds to indices $(i, j) \in S$, and the training loss captures how well the end-to-end model $M_2M_1$ fits the revealed entries.  Since $S$ was chosen randomly among all entries,  “generalization” corresponds exactly to doing well at predicting the remaining entries. Empirically, soving matrix completion this way via deep learning  (i.e., gradient descent to solve for $M_1, M_2$, and entirely forgetting about ensuring low rank) works as well as the classic algorithm, leading to the following conjecture, which if true would imply that the implicit regularization effect of gradient descent in this case is captured exactly by the nuclear norm.</p>

<blockquote>
  <p>(Conjecture by Gunasekar et al.; Rough Statement) When solving matrix completion as above using a depth-$2$ linear net, the solution obtained is exactly the  one obtained by the nuclear norm minimization method.</p>
</blockquote>

<p>But as you may have already guessed, this turns out to be too simplistic. In <a href="https://arxiv.org/abs/1905.13655">a new paper with Nadav Cohen, Wei Hu and Yuping Luo</a>, we report new experiments suggesting that the above conjecture is false. (I hedge by saying “suggest” because some fine print in the conjecture statement makes it pretty hard to refute definitively.) More interesting, we find that if we overparametrize the problem by further increasing the number of layers from two to $3$ or even higher —which we call Deep Matrix Factorization—then this empirically solves matrix completion even better than nuclear norm minimization. (Note that we’re working in the regime where $S$ is slightly smaller than what it needs to be for nuclear norm algorithm to exactly recover the matrix. Inductive bias is most important precisely in such data-poor settings!) We provide partial analysis for this improved performance of depth $N$ nets by analysing —surprise surprise!—the trajectory of gradient descent and showing how it biases strongly toward finding solutions of low rank, and this bias is stronger than simple nuclear norm. Furthermore our analysis suggests that this bias toward low rank  cannot be captured by nuclear norm or any obvious Schatten quasi-norm of the end-to-end matrix.</p>

<p>NB: Empirically we find that Adam, the celebrated  acceleration method for deep learning, speeds up optimization a lot here as well, but slightly hurts generalization. This relates to what I said above about the  Conventional View being insufficient to capture generalization.</p>

<h2 id="conclusionstakeways">Conclusions/Takeways</h2>

<p>Though the above settings are simple, they suggest that to understand deep learning we have to go beyond the Conventional View of optimization, which focuses only on the value of the objective and the rate of convergence.</p>

<p>(1): Different optimization strategies —GD, SGD, Adam, AdaGrad etc. —-lead to different learning algorithms. They induce different trajectories, which may lead to solutions with different generalization properties.</p>

<p>(2) We need to develop a new vocabulary (and mathematics) to reason about trajectories. This goes beyond the usual “landscape view” of stationary points, gradient norms, Hessian norms, smoothness etc. Caution: trajectories depend on initialization!</p>

<p>(3): I wish I had learnt a few tricks about ODEs/PDEs/Dynamical Systems/Lagrangians in college, to be in better shape to reason about trajectories!</p></div>







<p class="date">
<a href="http://offconvex.github.io/2019/06/03/trajectories/"><span class="datestr">at June 03, 2019 10:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gradientscience.org/robust_reps/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://gradientscience.org/robust_reps/">Robustness beyond Security&amp;#58; Representation Learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><a style="float: left;" href="https://arxiv.org/abs/1906.00945" class="bbutton">
<i class="fas fa-file-pdf"></i>
    Read the paper
</a>
<a style="float: right;" href="http://git.io/robust-reps" class="bbutton">
<i class="fab fa-github"></i>
   Download the notebooks
</a></p>

<p><i>This post discusses our <a href="https://arxiv.org/abs/1906.00945">latest paper</a>
on deep network representations—while representations of standard
networks are brittle and thus not fully reflective of the input geometry,
we find that the representations of robust networks are amenable to all
sorts of manipulation, and can truly be thought of (and dealt with) as just
high-level feature representations. Our work suggests that robustness might
be more broadly useful than just protection against adversarial examples.
</i></p>

<p>One of the most promising aspects of deep neural networks is their
potential to learn high-level <i>features</i> that are useful beyond the
classification task at hand. Our mental model of deep
learning classifiers is often similar to the following diagram, in which
the networks learns progressively higher-level features until the final
layer, which acts as a linear classifier over these high-level features:</p>

<p><img src="http://gradientscience.org/assets/rf1_images/visualization.png" alt="" /></p>
<div class="footnote">
A conceptual picture of our understanding of modern deep neural networks
(NVIDIA).
</div>

<p>This picture is consistent with the surprising versatility of deep neural
network <i>feature representations</i>—learned representations for one
task are useful for many others
(as in <a href="https://papers.nips.cc/paper/959-learning-many-related-tasks-at-the-same-time-with-backpropagation.pdf">transfer learning</a>), and
distance in representation space has often been proposed as a perceptual
metric on natural images (as in <a href="http://arxiv.org/abs/1801.03924">VGG distance</a>).</p>

<div class="footnote">
<strong>Note:</strong> In <a href="https://arxiv.org/abs/1906.00945">our paper</a> and in this blog post, we refer to the
<i>representation</i> $R(x)$ of an input $x$ for a network as the
values of the penultimate layer in the network for that input.
</div>

<p>But to what extent is this picture accurate? It turns out that it is
rather simple to (consistently) construct images that are <i>completely</i>
different to a human, but share very similar representations:</p>

<p><img src="http://gradientscience.org/assets/rf1_images/standard_brittleness.png" alt="Standard representations are brittle" /></p>
<div class="footnote">
The above two images, despite seeming completely different to humans, share very
similar representations.
</div>

<p>This phenomenon is somewhat troubling for our conceptual picture: if
feature representations actually encode high-level, human-meaningful
features, we should not be able to find two images with totally different
features that the model “sees” as very similar.</p>

<p>The phenomenon at play here turns out to be more fundamental than just
pairs of images with similar representations. Indeed, the
representations of neural networks seem to be <i>pervasively brittle</i>:
they can be manipulated arbitrarily without meaningful change to the input.
(In fact, this brittleness is similar to the phenomenon that we exploit
when making <a href="https://gradientscience.org/intro_adversarial">adversarial examples</a>.)</p>

<p>Clearly, this brittleness precludes standard representations from acting
how we want them to—in particular, distance in representation space is
not fully <i>aligned</i> with our human perception of distance in feature
space. So, how might we go about fixing this issue?</p>

<h3 id="adversarial-robustness-as-a-feature-prior">Adversarial Robustness as a Feature Prior</h3>

<p>Unfortunately, we don’t have a way to explicitly control which features
models learn (or in what way they learn them). We can, however,
disincentivize models from using features that humans <i>definitely</i>
don’t use by imposing a <i>prior</i> during training. In our paper, we
explore a very simple prior: namely, that imperceptible changes in the
input should not cause large changes in the model’s prediction (i.e.,
models should not rely on brittle features):</p>



<p>Note that this stability is a necessary, but not sufficient property: all
features that humans use certainly obey this property (for reasonably small
), but not every feature obeying this property is one that we
want our models to rely on.</p>

<p>How should we enforce this prior? Well, observe that the condition
$\eqref{eq:robustcond}$ above is actually <i>precisely</i> $\ell_2$-<a href="https://gradientscience.org/intro_adversarial">adversarial
robustness</a>! Thus, a natural method to employ is robust optimization,
which, as we discussed in a <a href="https://gradientscience.org/robust_opt_pt1">previous post</a>, provides reasonable
robustness to adversarial perturbations. Concretely, instead of just
minimizing loss, we opt to minimize <i>adversarial loss</i>:</p>



<h3 id="inverting-representations">Inverting representations</h3>

<p>Now, given a network trained in this manner, what happens if we look for
images with the same representations? Concretely, fixing some image $x$,
what happens if we look for an image $x’$ that has a matching
representation:</p>



<p>(Note that we found the image pairs presented earlier for standard networks
by solving exactly the above problem.) It turns out that when our model is
<i>robust</i>, we end up with an image that is remarkably similar to the
original:</p>

<div class="widget">
    <div class="choices_one" id="left2">
	<span class="widgetheading">Choose an Image</span>
    </div>
    <span class="widgetheading">Reconstructed Image</span>
    <div class="beer-slider selected_one" id="inv_slider">
	<img id="selectedinv1" />
	<div style="border-right: 3px white solid;" class="beer-reveal">
	    <img class="slider_img" id="selectedinv2" />
	</div>
    </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Interactive demo</strong>: click on any of the images on the left to see its
reconstruction via the representation of a robust network. The top row
contains random images from the test set, and the bottom row has random
<i>out-of-distribution</i> inputs (images without a correct class).
</div>

<p>Indeed, instead of being able to manipulate feature representations
arbitrarily within a small radius, we now find that matching the
representation of an image leads to (approximately) matching the image
itself.</p>

<h2 id="what-can-we-do-with-these-representations">What can we do with these representations?</h2>
<p>We just saw that the learned representation of a robust deep classifier
suffices to reconstruct its input pretty accurately (at least in terms of
human perception). This highlights two crucial properties of these
representations: a) optimizing for closeness in representation space leads
to perceptually similar images, b) representations contain a large amount
of information about the high-level features of the inputs. These
properties are very desirable and prompt us to further explore the
structure and potential of these representations.  <i>What we find is that
the representations of robust networks can truly be thought of as
high-level feature representations, and thus (in stark contrast to standard
networks) are naturally amenable to various types of manipulation.</i></p>

<p>In the following sections, we explore these “robust representations” in
more depth. A crucial theme in our exploration is
<i>model-faithfulness</i>. Though significant work has been done in
manipulating and interpreting standard (non-robust) models, it seems as
though getting anything meaningful from standard networks requires
enforcing <i>priors</i> into the visualization process
(see this excerpt <a href="https://distill.pub/2017/feature-visualization/#enemy-of-feature-vis">“The Enemy of Feature Visualization”</a>
for a discussion and illustration of this). This comes at the cost of either hiding
vital signals the model utilizes or introducing information that was not
already present in the model—thus blurring the line between what
information the model actually has, versus what information we introduced
when interacting with it. In contrast, throughout our exploration we will
rely on only direct optimization over representation space, without
introducing any priors or extra information.</p>

<h3 id="feature-visualization">Feature visualization</h3>

<p>We begin our exploration of robust representations by trying to understand
the features captured by their individual components. We visualize these
components in the simplest possible way: we perform gradient descent to
find inputs that maximally activate individual components of the
representation. This is how a few <em>random</em> visualizations look like:</p>

<p><img src="http://gradientscience.org/assets/rf1_images/features.png" style="margin: 0;" /></p>
<div class="footnote">
   Inputs maximizing various coordinates (separated by column) of a robust network, found via gradient descent starting from the "seed" image on the far left.
</div>

<p>We see a surprising alignment with human concepts. For instance, the last
component above seems to correspond to “anemone” and the second-last component to
“flowers”. In fact, these names are consistent with the test images
maximally activating these neurons—here are the images corresponding to
each component:</p>

<div class="widget">
    <div class="choices_one" id="left_maxact">
	<span class="widgetheading">Choose a Coordinate (Feature)</span>
    </div>
    <span class="widgetheading">Top Images</span>
    <div class="selected_one" id="maxact_selected"></div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
    <strong>Interactive demo</strong>: On the left are components of the representation of a robust network
    (the thumbnails are a visualization of the components maximized from
    noise). On the right are the images from the test set that maximally activate the corresponding components.
</div>

<p>These visualizations might look familiar. Indeed, similar results have been
produced in prior work using non-robust models (e.g.
<a href="https://distill.pub/2017/feature-visualization/">here</a> or
<a href="https://distill.pub/2018/building-blocks/">here</a>). The difference is that
the images above are generated by directly maximizing representation
components with gradient descent in input space—we do not enforce any
priors or regularization. For standard networks, the same process is
unfruitful—to circumvent this, prior work imposes priors on the
optimization process.</p>

<h3 id="feature-manipulation">Feature Manipulation</h3>

<p>So far, we have seen that matching the representation of an image starting
from random noise, recovers the high-level features of the image itself. At
the same time, we saw that individual representation components correspond
to high-level human-meaningful concepts. These findings suggests an
intriguing possibility: perhaps we can directly modify high-level features
of an image by manipulating the corresponding representation over the input
space.</p>

<p>This turns out to yield remarkably plausible results! Here we visualize the
results of increasing a few select components via gradient descent over the
image space for a few <em>random</em> (not cherry-picked) inputs:</p>

<div class="widget">
    <div class="choices_right" id="right1">
	<span class="widgetheading">Choose a Coordinate</span>
    </div>
    <div class="selected_two">
	<span class="widgetheading">Feature Addition</span>
	<div class="beer-slider" id="manipulation_slider">
	    <img id="man_selected1" /> 
	    <div style="border-right: 3px white solid;" class="beer-reveal">
		<img class="slider_img" id="man_selected2" /> 
	    </div>
	</div>
    </div>
    <div class="choices_left" id="left1">
	<span class="widgetheading">Choose a Source Image</span>
    </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Interactive demo</strong>: On the left are randomly selected source images,
and on the right are components of the representation of a robust network
(the thumbnails are a visualization of the components maximized from
noise). In the middle is the feature from the selected component, "added"
to the selected image.
</div>

<p>These images end up actually exhibiting the relevant features in a way that
is plausible to humans (for example, stripes appear mostly on animals
instead of the background).</p>

<p>This opens up a wide range of fine-grained manipulations that one can
perform by leveraging the learned representations (in fact, stay tuned for
some applications in our next blog post).</p>

<h3 id="input-interpolation">Input interpolation</h3>
<p>In fact, this outlook can be pushed even further—robust models can be
leveraged as a tool for another kind of manipulation: input-to-input
interpolation. That is, if we think of robust representations as encoding
the high-level features of an input in a sensible manner, an intuitive way
to interpolate between any two inputs is to linearly interpolate their
representations. More precisely, given any two inputs, we can try to
construct an interpolation between them by linearly interpolating their
representations and then constructing inputs to match these
representations.</p>

<p>This rather intuitive way of dealing with representations turns out to work
reasonably well—we can interpolate between arbitrary images. Randomly
sampled interpolations are shown below:</p>

<div class="widget">
    <div class="choices_left" id="int_left1">
	<span class="widgetheading">Choose a Source Image</span>
    </div>
    <div class="selected_two">
	<span class="widgetheading">Interpolation</span>
	<video style="width: 100%;">
	    <source type="video/mp4" id="int_selected">
	</source></video>
    </div>
    <div class="choices_right" id="int_right1">
	<span class="widgetheading">Choose a Destination Image</span>
    </div>
</div>
<div style="clear: both;"></div>
<div class="footnote">
<strong>Interactive demo</strong>: On the left are randomly selected source images,
and on the right are randomly selected target images.
In the middle is the feature interpolation from the selected source image
to the selected target.
</div>

<p>As we can see, the interpolations appears perceptually plausible. Note
that, in contrast to approaches based on generative models
(e.g. <a href="https://arxiv.org/abs/1511.06434">here</a> or
<a href="https://arxiv.org/abs/1809.11096">here</a>), this approach can interpolate
between arbitrary inputs and not only between those produced by the
generative model.</p>

<h3 id="insight-into-model-predictions">Insight into model predictions</h3>
<p>Expanding on our view of deep classifiers as simple linear classifiers on
top of the learned representations, there is also a simple way to gain
insight into predictions of (robust) models. In particular, for incorrect
predictions, we can identify the component most heavily contributing to the
incorrect class (in the same way we would for a linear classifier) and then
directly manipulate the input to increase the value of that component (with
input-space gradient descent). Here we perform this visualization for a few
random misclassified inputs:</p>

<p><img src="http://gradientscience.org/assets/rf1_images/misclassification_IN.jpg" style="margin: 0;" /></p>

<p>The resulting images could provide insight into the model’s incorrect
decision. For instance, we see the bug becoming a dog eye or negative space
becoming the face of a dog. At a high level, these inputs demonstrate which
parts of the image the incorrect prediction was most sensitive to.</p>

<p>Still, as a word of caution, it is important to note that just as with all
saliency methods (e.g. heatmaps, occlusion studies, etc.), visualizing
features and studying misclassification only gives insights into a “local”
sense of model behaviour. Deep neural networks are complex, highly
non-linear models and it’s important to keep in mind that <i>local
sensitivity does not necessarily entail causality</i>.</p>

<h2 id="towards-better-learned-representations">Towards better learned representations</h2>
<p>As we discussed, robust feature representations possess properties that
make them desirable from a broader point of view. In particular, we found
these representations to be better aligned with a perceptual notion of
distance, while allowing us to perform direct input manipulations in a
model-faithful way. These are properties that are fundamental to any “truly
human-level” representation. One can thus view adversarial robustness as a
very potent prior for obtaining representations that are more aligned with
human perception beyond the standard goals of security and reliability.</p></div>







<p class="date">
<a href="http://gradientscience.org/robust_reps/"><span class="datestr">at June 03, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4199">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4199">NP-complete Problems and Physics: A 2019 View</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>If I want to get back to blogging on a regular basis, given the negative amount of time that I now have for such things, I’ll need to get better at dispensing with pun-filled titles, jokey opening statements, etc. etc., and resigning myself to a less witty, more workmanline blog.</p>



<p>So in that spirit: a few weeks ago I gave a talk at the Fields Institute in Toronto, at a <a href="http://www.fields.utoronto.ca/activities/18-19/NP50">symposium</a> to celebrate Stephen Cook and the 50th anniversary (or actually more like 48th anniversary) of the discovery of NP-completeness.  Thanks so much to the organizers for making this symposium happen.</p>



<p>You can <a href="http://www.fields.utoronto.ca/video-archive/static/2019/05/2774-20557/mergedvideo.ogv">watch the video of my talk here</a> (or <a href="https://www.scottaaronson.com/talks/npphys-toronto.ppt">read the PowerPoint slides here</a>).  The talk, on whether NP-complete problems can be efficiently solved in the physical universe, covers much the same ground as <a href="https://www.scottaaronson.com/papers/npcomplete.pdf">my 2005 survey article</a> on the same theme (not to mention dozens of earlier talks), but this is an updated version and I’m happier with it than I was with most past iterations.</p>



<p>As I explain at the beginning of the talk, I wasn’t going to fly to Toronto at all, due to severe teaching and family constraints—but my wife Dana uncharacteristically <em>urged me to go</em> (“don’t worry, I’ll watch the kids!”).  Why?  Because in her view, it was the risks that Steve Cook took 50 years ago, as an untenured assistant professor at Berkeley, that gave birth to the field of computational complexity that Dana and I both now work in.</p>



<p>Anyway, be sure to <a href="http://www.fields.utoronto.ca/video-archive//event/2774/2019">check out the other talks as well</a>—they’re by an assortment of random nobodies like Richard Karp, Avi Wigderson, Leslie Valiant, Michael Sipser, Alexander Razborov, Cynthia Dwork, and Jack Edmonds.  I found the talk by Edmonds particularly eye-opening: he explains how he thought about (the objects that we now call) P and NP∩coNP when he first defined them in the early 60s, and how it was similar to and different from the way we think about them today.</p>



<p>Another memorable moment came when Edmonds interrupted Sipser’s talk—about the history of P vs. NP—to deliver a booming diatribe about how what really matters is not mathematical proof, but just how quickly you can solve problems in the real world.  Edmonds added that, from a practical standpoint, P≠NP is “true today but might become false in the future.”  In response, Sipser asked “what does a mathematician like me care about the real world?,” to roars of approval from the audience.  I might’ve picked a different tack—about how for every practical person I meet for whom it’s blindingly obvious that “in real life, P≠NP,” I meet another for whom it’s equally obvious that “in real life, P=NP” (for all the usual reasons: because SAT solvers work so well in practice, because physical systems so easily relax as their ground states, etc).  No wonder it took 25+ years of smart people thinking about operations research and combinatorial optimization before the P vs. NP question was even explicitly posed.</p>



<hr />

<p><font color="red"><strong>Unrelated Announcement:</strong></font> The Texas Advanced Computing Center (TACC), a leading supercomputing facility in North Austin that’s part of the University of Texas, is seeking to hire a Research Scientist focused on quantum computing.  Such a person would be a full participant in our <a href="https://www.cs.utexas.edu/~qic/">Quantum Information Center</a> at UT Austin, with plenty of opportunities for collaboration.  <a href="https://utaustin.wd1.myworkdayjobs.com/UTstaff/job/PICKLE-RESEARCH-CAMPUS/Research-Scientist_R_00003442">Check out their posting!</a></p>



<p></p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4199"><span class="datestr">at June 02, 2019 01:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/082">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/082">TR19-082 |  Approximate degree, secret sharing, and concentration phenomena | 

	Andrej Bogdanov, 

	Nikhil Mande, 

	Justin Thaler, 

	Christopher Williamson</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The $\epsilon$-approximate degree $\widetilde{\text{deg}}_\epsilon(f)$ of a Boolean function $f$ is the least degree of a real-valued polynomial that approximates $f$ pointwise to error $\epsilon$.  The approximate degree of $f$ is at least $k$ iff there exists a pair of probability distributions, also known as a dual polynomial, that are perfectly $k$-wise indistinguishable, but are distinguishable by $f$ with advantage $1 - \epsilon$.  Our contributions are:

We give a simple new construction of a dual polynomial for the AND function, certifying that $\widetilde{\text{deg}}_\epsilon(f) \geq \Omega(\sqrt{n \log 1/\epsilon})$.  This construction is the first to extend to the notion of weighted degree, and yields the first explicit certificate that the $1/3$-approximate degree of any read-once DNF is $\Omega(\sqrt{n})$.

We show that any pair of symmetric distributions on $n$-bit strings that are perfectly $k$-wise indistinguishable are also statistically $K$-wise indistinguishable with error at most $K^{3/2} \cdot \exp(-\Omega(k^2/K))$ for all $k \leq K \leq n/64$.
This implies that any symmetric function $f$ is a reconstruction function with constant advantage for a ramp secret sharing scheme that is secure against size-$K$ coalitions with statistical error $K^{3/2} \exp(-\Omega(\widetilde{\text{deg}}_{1/3}(f)^2/K))$ for all values of $K$ up to $n/64$ simultaneously.
Previous secret sharing schemes required that $K$ be determined in advance, and only worked for $f=$ AND.  

Our analyses draw new connections between approximate degree and concentration phenomena.

As a corollary, we show that for any $d \leq n/64$, any degree $d$ polynomial approximating a symmetric function $f$ to error $1/3$
must have $\ell_1$-norm at least $K^{-3/2} \exp({\Omega(\widetilde{\text{deg}}_{1/3}(f)^2/d)})$, which we also show to be tight for any $d &gt; \widetilde{\text{deg}}_{1/3}(f)$.
These upper and lower bounds were also previously only known in the case $f=$ AND.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/082"><span class="datestr">at June 02, 2019 04:15 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/081">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/081">TR19-081 |  Channels of Small Log-Ratio Leakage and Characterization of Two-Party Differentially Private Computation | 

	Iftach Haitner, 

	Noam Mazor, 

	Ronen Shaltiel, 

	Jad Silbak</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Consider a PPT two-party protocol ?=(A,B) in which the parties get no private inputs and obtain outputs O^A,O^B?{0,1}, and let V^A and V^B denote the parties’ individual views. Protocol ? has ?-agreement if Pr[O^A=O^B]=1/2+?. The leakage of ? is the amount of information a party obtains about the event {O^A=O^B}; that is, the leakage ? is the maximum, over P?{A,B}, of the distance between V^P|OA=OB and V^P|OA!=OB. Typically, this distance is measured in statistical distance, or, in the computational setting, in computational indistinguishability. For this choice, Wullschleger [TCC ’09] showed that if ?&gt;&gt;? then the protocol can be transformed into an OT protocol.

We consider measuring the protocol leakage by the log-ratio distance (which was popularized by its use in the differential privacy framework). The log-ratio distance between X,Y over domain ? is the minimal ??0 for which, for every v??, log(Pr[X=v]/Pr[Y=v])? [??,?]. In the computational setting, we use computational indistinguishability from having log-ratio distance ?. We show that a protocol with (noticeable) accuracy ???(?^2) can be transformed into an OT protocol (note that this allows ?&gt;&gt;?). We complete the picture, in this respect, showing that a protocol with ??o(?^2) does not necessarily imply OT. Our results hold for both the information theoretic and the computational settings, and can be viewed as a “fine grained” approach to “weak OT amplification”.

We then use the above result to fully characterize the complexity of differentially private two-party computation for the XOR function, answering the open question put by Goyal, Khurana, Mironov, Pandey, and Sahai [ICALP ’16] and Haitner, Nissim, Omri, Shaltiel, and Silbak [FOCS ’18]. Specifically, we show that for any (noticeable) ???(?^2), a two-party protocol that computes the XOR function with ?-accuracy and ?-differential privacy can be transformed into an OT protocol. This improves upon Goyal et al. that only handle ???(?), and upon Haitner et al. who showed that such a protocol implies (infinitely-often) key agreement (and not OT). Our characterization is tight since OT does not follow from protocols in which ??o(?^2), and extends to functions (over many bits) that “contain” an “embedded copy” of the XOR function.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/081"><span class="datestr">at June 02, 2019 04:13 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/080">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/080">TR19-080 |  On List Recovery of High-Rate Tensor Codes | 

	Noga Ron-Zewi, 

	Swastik Kopparty, 

	Shubhangi Saraf, 

	Nicolas Resch, 

	Shashwat Silas</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We continue the study of list recovery properties of high-rate tensor codes, initiated by Hemenway, Ron-Zewi, and Wootters (FOCS'17). In that work it was shown that the tensor product of an efficient (poly-time) high-rate globally list recoverable code is {\em approximately}  locally list recoverable, as well as globally list recoverable in {\em probabilistic} near-linear time. This was used in turn to give the first capacity-achieving list decodable codes with (1) local list decoding algorithms, and with (2)  {\em probabilistic} near-linear time  global list decoding algorithms. This was also yielded constant-rate codes approaching the Gilbert-Varshamov bound  with  {\em probabilistic}  near-linear time global   unique decoding algorithms.

In the current work we obtain the following results:
1. The tensor product of an efficient (poly-time) high-rate globally list recoverable code is globally list recoverable in {\em deterministic} near-linear time. This yields in turn the first capacity-achieving list decodable codes with {\em deterministic} near-linear time global  list decoding algorithms. It also gives constant-rate codes approaching the Gilbert Varshamov bound with {\em deterministic} near-linear time global unique decoding algorithms.

2. If the base code is additionally locally correctable, then the tensor product is (genuinely) locally list recoverable. This yields in turn constant-rate codes approaching the Gilbert-Varshamov bound that are {\em locally correctable} with query complexity and running time $N^{o(1)}$. This improves over prior work by Gopi et. al. (SODA'17; IEEE Transactions on Information Theory'18) that only gave query complexity $N^{\epsilon}$ with rate that is exponentially small in $1/\epsilon$.

3. A nearly-tight combinatorial lower bound on output list size for list recovering high-rate tensor codes. This bound implies in turn a nearly-tight lower bound of $N^{\Omega(1/\log \log N)}$ on the product of  query complexity and output list size  for locally list recovering high-rate tensor codes.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/080"><span class="datestr">at June 01, 2019 04:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/079">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/079">TR19-079 |  Average Bias and Polynomial Sources | 

	Arnab Bhattacharyya, 

	Philips George John, 

	Suprovat Ghoshal, 

	Raghu Meka</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We identify a new notion of pseudorandomness for randomness sources, which we call the average bias. Given a distribution $Z$ over $\{0,1\}^n$, its average bias is: $b_{\text{av}}(Z) =2^{-n} \sum_{c \in \{0,1\}^n} |\mathbb{E}_{z \sim Z}(-1)^{\langle c, z\rangle}|$. A source with average bias at most $2^{-k}$ has min-entropy at least $k$, and so low average bias is a stronger condition than high min-entropy. We observe that the inner product function is an extractor for any source with average bias less than $2^{-n/2}$.

  The notion of average bias especially makes sense for polynomial sources, i.e., distributions sampled by low-degree $n$-variate polynomials over $\mathbb{F}_2$. For the well-studied case of affine sources, it is easy to see that min-entropy $k$ is exactly equivalent to average bias of $2^{-k}$. We show that for quadratic sources, min-entropy $k$ implies that the average bias is at most $2^{-\Omega(\sqrt{k})}$. We use this relation to design dispersers for separable quadratic sources with a min-entropy guarantee.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/079"><span class="datestr">at June 01, 2019 04:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/078">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/078">TR19-078 |  Pseudo-Mixing Time of Random Walks | 

	Itai Benjamini, 

	Oded Goldreich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We introduce the notion of pseudo-mixing time of a graph define as the number of steps in a random walk that suffices for generating a vertex that looks random to any polynomial-time observer, where, in addition to the tested vertex, the observer is also provided with oracle access to the incidence function of the graph. 

Assuming the existence of one-way functions,
we show that the pseudo-mixing time of a graph can be much smaller than its mixing time.
Specifically, we present bounded-degree $N$-vertex Cayley graphs that have pseudo-mixing time $t$ for any $t(N)=\omega(\log\log N)$. 
Furthermore, the vertices of these graphs can be represented by string of length $2\log_2N$, and the incidence function of these graphs can be computed by Boolean circuits of size $poly(\log N)$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/078"><span class="datestr">at June 01, 2019 07:34 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/05/31/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/05/31/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://plus.maths.org/content/democratic-dilemmas">No maths for Europe</a> (<a href="https://mathstodon.xyz/@11011110/102109693915830408"></a>). Sadly, the EU parliament has passed up a chance to find a nice (or even not-so-nice) <a href="https://en.wikipedia.org/wiki/Highest_averages_method">formula for its apportionment</a> of seats to countries, instead opting for back-room deals and numbers pulled out of a hat.</p>
  </li>
  <li>
    <p>Prominent cryptographers <a href="https://en.wikipedia.org/wiki/Adi_Shamir">Adi Shamir</a> and <a href="https://en.wikipedia.org/wiki/Ross_J._Anderson">Ross J. Anderson</a> were both <a href="https://www.schneier.com/blog/archives/2019/05/why_are_cryptog.html">denied visas to travel to the US</a> for a conference and a book awards ceremony respectively (<a href="https://mathstodon.xyz/@11011110/102112360619663485"></a>, <a href="https://boingboing.net/2019/05/17/denying-cryptographers-problem.html">see also</a>). Bruce Schneier mentions “two other prominent cryptographers who are in the same boat”. Odd and troubling.</p>
  </li>
  <li>
    <p><a href="https://mathlesstraveled.com/2019/05/09/computing-the-euler-totient-function-part-1/">Three</a> <a href="https://mathlesstraveled.com/2019/05/18/computing-the-euler-totient-function-part-2-seeing-phi-is-multiplicative/">new</a> <a href="https://mathlesstraveled.com/2019/05/27/computing-the-euler-totient-function-part-3-proving-phi-is-multiplicative/">blog posts</a> by Brent Yorgey concern the <a href="https://en.wikipedia.org/wiki/Euler%27s_totient_function">Euler totient function</a> (<a href="https://mathstodon.xyz/@11011110/102118180704402052"></a>). Computing it quickly would break RSA; Brent describes using factoring to do better than brute force. The problem is clearly in , and I think it may be a natural candidate for being -intermediate. Igor Pak (who asked me for -intermediate problems when I recently visited UCLA) <a href="https://cstheory.stackexchange.com/q/43954/95">thinks the prime-counting function may be another</a>, but neither function is very combinatorial. <a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html">In a recent blog post I found a couple of combinatorial candidates</a>, but others would be interesting.</p>
  </li>
  <li>
    <p>The image below (<a href="https://commons.wikimedia.org/wiki/File:Gr%C3%BCnbaum-Rigby_configuration,_vector_graphics.svg">as redrawn by Brammers</a>) is the 
<a href="https://en.wikipedia.org/wiki/Gr%C3%BCnbaum%E2%80%93Rigby_configuration">Grünbaum–Rigby configuration</a> (<a href="https://mathstodon.xyz/@11011110/102119858635464298"></a>) with 21 points and lines, 4 points per line, and 4 lines per point. Klein studied it in the complex projective plane in 1879, but it wasn’t known to have this nice real heptagonal realization until Grünbaum and Rigby (1990). The new Wikipedia article on it was started by “Tomo” (whose real-world identity Wikipedia’s arcane outing rules bar me from disclosing, but he just turned 70, so if you figure it out wish him a happy birthday).</p>

    <p style="text-align: center;"><img width="60%" alt="The Grünbaum–Rigby configuration" src="https://11011110.github.io/blog/assets/2019/grunrig.svg" /></p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Garden_of_Eden_(cellular_automaton)">Garden of Eden</a> (<a href="https://mathstodon.xyz/@11011110/102129199678568606"></a>). Now a Good Article on Wikipedia.</p>
  </li>
  <li>
    <p><a href="https://www.pnas.org/content/early/2019/05/20/1902572116. Via https://mathstodon.xyz/@helger/102138884170343694">Ono et al prove that almost all Jensen-Pólya polynomials have only real roots</a> (<a href="https://mathstodon.xyz/@11011110/102143915068349382"></a>, <a href="https://mathstodon.xyz/@helger/102138884170343694">via</a>). The Riemann Hypothesis is equivalent to the statement that they all do. The same thing works for similar families of polynomials associated with partition functions and proves a conjecture of Chen. See also <a href="https://phys.org/news/2019-05-mathematicians-revive-abandoned-approach-riemann.html">a popularized account</a> and <a href="http://people.oregonstate.edu/~petschec/ONTD/Talk1.pdf">Ono’s talk slides</a>.</p>
  </li>
  <li>
    <p><a href="https://scilogs.spektrum.de/hlf/imu-abacus-medal/">The International Mathematical Union is renaming</a> its <a href="https://en.wikipedia.org/wiki/Nevanlinna_Prize">Nevanlinna Prize</a> to be the IMU Abacus Medal (<a href="https://mathstodon.xyz/@11011110/102149453984232922"></a>). The prize is given every four years for major accomplishments in theoretical computer science. The article doesn’t say why rename but it’s because Nevanlinna was a Nazi sympathizer and collaborator. The prize was named after him in the early 1980s because its funding came from Finland, but Nevanlinna also never had much to do with TCS.</p>
  </li>
  <li>
    <p><a href="https://blog.computationalcomplexity.org/2019/05/notorious-lah-or-notorious-lah-or-you.html">Gasarch on proofreading</a> (<a href="https://mathstodon.xyz/@11011110/102154856271421940"></a>). Just as in programming, there’s always one more bug.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Ellen_Fetter">Ellen Fetter</a> and <a href="https://en.wikipedia.org/wiki/Margaret_Hamilton_(scientist)">Margaret Hamilton</a>: <a href="https://www.quantamagazine.org/hidden-heroines-of-chaos-ellen-fetter-and-margaret-hamilton-20190520/">Uncredited collaborators with Edward Lorenz at the birth of chaos theory</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/102163812229252010"></a>).</span></p>
  </li>
  <li>
    <p><a href="https://www.nytimes.com/2019/05/17/science/math-physics-knitting-matsumoto.html">Elisabetta Matsumoto is studying the mathematics of knitting</a> (<a href="https://mathstodon.xyz/@11011110/102177647389031957"></a>, <a href="https://twitter.com/Sabetta_">via</a>), with the hope that it can lead to new programmable metamaterials.</p>
  </li>
  <li>
    <p><a href="https://www.sciencemag.org/news/2019/05/ieee-major-science-publisher-bans-huawei-scientists-reviewing-papers">IEEE bans Huawei employees from reviewing submissions to its journals</a> (<a href="https://mathstodon.xyz/@11011110/102183137967208347"></a>, <a href="https://news.ycombinator.com/item?id=20046771">via</a>), saying it is forced to do so by US government sanctions.</p>
  </li>
  <li>
    <p>The UCI University Club (which in other places might be called a faculty club) is next door to the building I work in, and has a bustling side business hosting weddings. Here’s the view that greeted me as I left the office this evening, looking across their lawn towards the gazebo (<a href="https://mathstodon.xyz/@11011110/102188200347058355"></a>).</p>

    <p style="text-align: center;"><a href="https://www.ics.uci.edu/~eppstein/pix/uclub/index.html"><img src="https://www.ics.uci.edu/~eppstein/pix/uclub/uclub-m.jpg" alt="UCI University Club lawn" style="border-style: solid; border-color: black;" /></a></p>
  </li>
  <li>
    <p>Line arrangements in architecture (<a href="https://mathstodon.xyz/@11011110/102193430755771327"></a>): the beams of <a href="https://en.wikipedia.org/wiki/Mathematical_Bridge">Cambridge’s Mathematical Bridge</a> form tangent lines to its arch and then extend through and support its trusswork, while another set of radial lines tie the structure together. The bridge just looks like a wood truss bridge in real life but <a href="https://commons.wikimedia.org/wiki/File:Mathematical_Bridge_tangents.jpg">this artificially-colored image</a> makes the underlying structure clearer.</p>

    <p style="text-align: center;"><img width="80%" alt="Cambridge's Mathematical Bridge" style="border-style: solid; border-color: black;" src="https://11011110.github.io/blog/assets/2019/cambridgebridge.jpg" /></p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/05/31/linkage.html"><span class="datestr">at May 31, 2019 09:40 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/077">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/077">TR19-077 |  Consistency of circuit lower bounds with bounded theories | 

	Jan Bydzovsky, 

	Jan  Krajicek, 

	Igor Carboni Oliveira</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Proving that there are problems in $P^{NP}$ that require boolean circuits of super-linear size is a major frontier in complexity theory. While such lower bounds are known for larger complexity classes, existing results only show that the corresponding problems are hard on infinitely many input lengths. For instance, proving almost-everywhere circuit lower bounds is open even for problems in MAEXP. Giving the notorious difficulty of proving lower bounds that hold for all large input lengths, we ask the following question: 

Can we show that a large set of techniques cannot prove that NP is easy infinitely often? 

Motivated by this and related questions about the interaction between mathematical proofs and computations, we investigate circuit complexity from the perspective of logic.

  Among other results, we prove that for any parameter $k \geq 1$ it is consistent with theory $T$ that computational class $C$ is not contained infinitely often in SIZE$(n^k)$, where $(T, C)$ is one of the pairs:

  $T = T^1_2\;$ and $\;C = P^{NP}$, $\quad T = S^1_2\;$ and $\;C = NP$, $\quad T =~ $PV$\;$ and $C = P$.

  In other words, these theories cannot establish infinitely often circuit upper bounds for the corresponding problems. This is of interest because the weaker theory PV already formalizes sophisticated arguments, such as a proof of the PCP Theorem (Pich, 2015). These consistency statements are unconditional and improve on earlier theorems of Krajicek and Oliveira (2017) and Bydzovsky and Muller (2018) on the consistency of lower bounds with PV.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/077"><span class="datestr">at May 30, 2019 10:45 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://grigory.github.io/blog/theory-jobs-2019">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/yaroslavtsev.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="http://grigory.github.io/blog/theory-jobs-2019/">Theory Jobs 2019</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><img src="http://grigory.github.io/blog/pics/theory-jobs-2019.png" />
Apparently, it’s a busy life being an assistant prof so there were no posts here all year. However, while some of us are decompressing after the NeurIPS deadline, <a href="https://docs.google.com/spreadsheets/d/1Oegc0quwv2PqoR_pzZlUIrPw4rFsZ4FKoKkUvmLBTHM/edit?usp=sharing">here is a link</a> to a crowdsourced spreadsheet created to collect information about theory jobs this year. 
Congratulations to both job seekers and departments/labs who are done with their searches!</p>

<p>In the past my academic uncle Lance Fortnow set this spreadsheet up (check <a href="https://blog.computationalcomplexity.org/2017/06/theory-jobs-2016.html">this link</a> to his post from two years ago which also has links to all the previous years). This year the first entry is Lance himself who is moving back to Chicago to be the Dean of the College of Science at the Illinois Institute of Technology. Did Lance get the idea from his advisor <a href="https://en.wikipedia.org/wiki/Michael_Sipser">Michael Sipser</a> who is also a Dean of Science but at MIT? In any case, great to see theoretical computer scientists stepping up to be the deans of science, congratulations!</p>

<p>Rules about the spreadsheet have been copied from last years and all edits to the document are anonymized. Please, post a comment if you have any suggestions about the rules.</p>
<ul>
 <li>Separate sheets for faculty, industry and postdocs/visitors. </li>
 <li>People should be connected to theoretical computer science, broadly defined.</li>
 <li>Only add jobs that you are absolutely sure have been offered and accepted. This is not the place for speculation and rumors. </li>
 <li>You are welcome to add yourself, or people your department has hired. </li>
</ul>

<p>This document will continue to grow as more jobs settle.</p>




  <p><a href="http://grigory.github.io/blog/theory-jobs-2019/">Theory Jobs 2019</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on May 30, 2019.</p></div>







<p class="date">
by Grigory Yaroslavtsev (grigory@grigory.us) <a href="http://grigory.github.io/blog/theory-jobs-2019/"><span class="datestr">at May 30, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-1040381893569171546">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/05/nsf-panels.html">NSF Panels</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The government shut down in January led to delays at the National Science Foundation and only recently announcing decisions on grants submitted last fall. For those who successfully received awards, congratulations! For those who didn't, don't take it personally, buckle up and try again.<br />
<br />
For those who don't know how the process works, for each grant program, the program directors organize one or more panels which typically meets in person at NSF headquarters in Alexandria, Virginia. A typical panel has about a dozen panelists and twenty or so proposals. Before the panels, each proposal gets at least three reviews by the panelists. Discussions ensue over a day or two, proposals get sorted into categories: Highly Competitive, Competitive, Low Competitive and Not Competitive and then ranked ordered in the top categories.<br />
<br />
There are tight rules for Conflict-of-Interest and those who are conflicted have to leave the room during the discussions on those papers.<br />
<br />
If you do get asked to serve on a panel, you should definitely do so. You get to see how the process works and help influence funding and research directions in your field. You can't reveal when you serve on a particular panel but you can say "Served on NSF Panels" on your CV.<br />
<br />
Panels tend to take proposals that will likely make progress and not take ones less risky. Funding risky proposals is specifically mentioned to the panel but when push comes to shove and there is less funding than worthy proposals, panelists gravitate towards proposals that don't take chances.<br />
<br />
Panels are not unlike conference program committees. It didn't always work this way, it used to be more like journal publications. I remember when the program director would send out proposals for outside reviews and then make funding decisions. That gave the program director more discretion to fund a wider variety of proposals.<br />
<br />
The NSF budget for computing goes up slowly while the number of academic computer scientists grows at a much larger clip. Until this changes, we'll have more and more worthy proposals unfunded, particularly proposals of bold risky projects. That's the saddest part of all.</div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/05/nsf-panels.html"><span class="datestr">at May 29, 2019 08:09 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-25562705.post-831739446833439686">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/roth.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://aaronsadventures.blogspot.com/2019/05/individual-notions-of-fairness-you-can.html">Individual Notions of Fairness You Can Use</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div style="text-align: center;"><span style="font-size: x-large;"><u>Individual Notions of Fairness You Can Use</u></span></div><br />Our group at Penn has been thinking about when <i>individual </i>notions of fairness might be practically achievable for awhile, and we have <a href="https://arxiv.org/abs/1905.10660">two</a> <a href="https://arxiv.org/abs/1905.10607">new</a> approaches.<br /><br /><span style="font-size: large;"><u>Background</u>:</span><br /><u>Statistical Fairness</u><br />I've written about this before, <a href="http://aaronsadventures.blogspot.com/2017/11/between-statistical-and-individual.html">here</a>. But briefly: there are two families of definitions in the fairness in machine learning literature. The first group of definitions, which I call <i>statistical</i> fairness notions, is far and away the most popular. If you want to come up with your own statistical fairness notion, you can follow this recipe:<br /><ol><li>Partition the world into a small number of "protected sub-groups". You will probably be thinking along the lines of race or gender or something similar when you do this.</li><li>Pick your favorite error/accuracy metric for a classifier. This might literally be classification error, or false positive or false negative rate, or positive predictive value, or something else. Lots of options here. </li><li>Ask that this metric be approximately equalized across your protected groups.</li><li>Finally, enjoy your new statistical fairness measure! Congratulations!</li></ol><div>These definitions are far and away the most popular in this literature, in large part (I think) because they are so immediately actionable. Because they are defined as conditions on a small number of expectations, you can easily check whether your classifier is "fair" according to these metrics, and (although there are some interesting computational challenges) go and try and learn classifiers subject to these constraints. </div><div><br /></div><div>Their major problem is related to the reason for their success: they are defined as conditions on a small number of expectations or <i>averages</i> over people, and so they don't promise much to particular individuals. I'll borrow an example from our <a href="https://arxiv.org/abs/1711.05144">fairness gerrymandering</a> paper from a few years ago to put this in sharp relief. Imagine that we are building a system to decide who to incarcerate, and we want to be "fair" with respect to both gender (men and women) and race (green and blue people). We decide that in our scenario, it is the false positives who are harmed (innocent people sent to jail), and so to be fair, we decide should equalize the false positive rate: across men and women, and across greens and blues. But one way to do this is to jail all green men and blue women. This does indeed equalize the false positive rate (at 50%) across all four of the groups we specified, but is cold comfort if you happen to be a green man --- since then you will be jailed with certainty. The problem was our fairness constraint was never a promise to an individual to begin with, just a promise about the average behavior of our classifier over a large group. And although this is a toy example constructed to make a point, things like this happen in real data too. </div><br /><u>Individual Fairness</u><br />Individual notions of fairness, on the other hand, really do correspond to promises made to individuals. There are at least two kinds of individual fairness definitions that have been proposed: <a href="https://dl.acm.org/citation.cfm?id=2090255">metric fairness</a>, and <a href="http://papers.nips.cc/paper/6355-fairness-in-learning-classic-and-contextual-bandits">weakly meritocratic fairness</a>. Metric fairness proposes that the learner will be handed a <i>task specific similarity metric</i>, and requires that individuals who are close together in the metric should have a similar probability of being classified as positive. Weakly meritocratic fairness, on the other hand, takes the (unknown) labels of an individual as a measure of merit, and requires that individuals who have a higher probability of really having a positive label should have only a higher probability of being classified as positive. This in particular implies that false positive and false negative rates should be equalized <i>across individuals</i>, where now the word <i>rate</i> is averaging over only the randomness of the classifier, not over people. What makes both of these <i>individual</i> notions of fairness is that they impose constraints that bind on all pairs of individuals and not just over averages of people.<br /><br />Definitions like this have the advantage of strong individual-level semantics, which the statistical definitions don't have. But they also have big problems: for metric fairness, the obvious question is: <i>where does the metric come from</i>? Even granting that fairness should be some Lipschitz condition on a metric, it seems hard to pin down what the metric is, and different people will disagree: coming up with the metric seems to encapsulate a large part of the original problem of defining fairness. For weakly meritocratic fairness, the obvious problem is that we don't know what the labels are. Its possible to do non-trivial things if you make assumptions about the label generating process, but its not at all clear you can do any non-trivial learning subject to this constraint if you don't make strong assumptions.<br /><br /><span style="font-size: large;"><u>Two New Approaches:</u></span><br />We have two new approaches, building off of metric fairness and weakly meritocratic fairness respectively. Both have the advantages of statistical notions of fairness in that they can be put into practice without making unrealistic assumptions about the data, and without needing to wait on someone to hand us a metric. But they continue to make meaningful promises to individuals.<br /><br /><u>Subjective Individual Fairness</u><br />Lets start with our variant of metric fairness, which we call subjective individual fairness. (This is joint work with Michael Kearns, our PhD students Chris Jung and Seth Neel, our former PhD student Steven Wu, and Steven's student (our grand student!) Logan Stapleton). The paper is here: <a href="https://arxiv.org/abs/1905.10660">https://arxiv.org/abs/1905.10660</a>. We stick with the premise that "similar people should be treated similarly", and that whether or not it is correct/just/etc., it is at least fair to treat two people the same way, in the sense that we classify them as positive with the same probability. But we don't want to assume anything else.<br /><br />Suppose I were to create a machine learning fairness panel: I could recruit "AI Ethics" experts, moral philosophers, hyped up consultants, people off the street, toddlers, etc. I would expect that there would be as many different conceptions of fairness as there were people on the panel, and that none of them could precisely quantify what they meant by fairness --- certainly not in the form of a "fairness metric". But I could still ask these people, in particular cases, if they thought it was fair that two particular individuals be treated differently or not.<br /><br />Of course, I would have no reason to expect that the responses that I got from the different panelists would be consistent with one another --- or possibly even internally consistent (we won't assume, e.g. that the responses satisfy any kind of triangle inequality). Nevertheless, once we fix a data distribution and a group of people who have opinions about fairness, we have a well defined tradeoff we can hope to manage: any classifier we could choose will have both:<br /><ol><li>Some error rate, and</li><li>Some frequency with which it makes a pair of decisions that someone in the group finds unfair. </li></ol><div>We can hope to find classifiers that optimally trade off 1 and 2: note this is a coherent tradeoff even though we haven't forced the people to try and express their conceptions of fairness into some consistent metric. What we show is that you can do this. </div><div><br /></div><div>Specifically, given a set of pairs that we have determined should be treated similarly, there is an <i>oracle efficient </i>algorithm that can find the optimal classifier subject to the constraint that no pair of individuals that has been specified as a constraint should have a substantially different probability of positive classification. Oracle efficiency means that what we can do is reduce the "fair learning" problem to a regular old learning problem, without fairness constraints. If we can solve the regular learning problem, we can also solve the fair learning problem. This kind of fairness constraint also generalizes in the standard way: if you ask your fairness panel about a reasonably small number of pairs, and then solve the in-sample problem subject to these constraints, the classifier you learn will also satisfy the fairness constraints out of sample. And it works: we implement the algorithm and try it out on the COMPAS data set, with fairness constraints that we elicited from 43 human (undergrad) subjects. The interesting thing is that once you have an algorithm like this, it isn't only a tool to create "fair" machine learning models: its also a new instrument to investigate human conceptions of fairness. We already see quite a bit of variation among our 43 subjects in our preliminary experiments. We plan to pursue this direction more going forward.</div><div><br /></div><div><u>Average Individual Fairness</u></div><div>Next, our variant of weakly meritocratic fairness. This is joint work with Michael Kearns and our student Saeed Sharifi. The paper is here: <a href="https://arxiv.org/abs/1905.10607">https://arxiv.org/abs/1905.10607</a>. In certain scenarios, it really does seem tempting to think about fairness in terms of false positive rates. Criminal justice is a great example, in the sense that it is clear that everyone agrees on which outcome they <i>want</i> (they would like to be released from jail), and so the people we are being unfair to really do seem to be the false positives: the people who should have been released from jail, but who were mistakenly incarcerated for longer. So in our "fairness gerrymandering" example above, maybe the problem with thinking about false positive rates wasn't a problem with <i>false positives</i>, but with <i>rates</i>: i.e. the problem was that the word rate averaged over many people, and so it didn't promise <i>you</i> anything. Our idea is to redefine the word rate. </div><div><br /></div><div>In some (but certainly not all) settings, people are subject to not just one, but many classification tasks. For example, consider online advertising: you might be shown thousands of targeted ads each month. Or applying for schools (a process that is centralized in cities like New York): you apply not just to one school, but to many. In situations like this, we can model the fact that we have not just a distribution over people, but also a distribution over (or collection of) problems. </div><div><br /></div><div>Once we have a distribution over problems, we can define the error rate, or false positive rate, or any other rate you like <i>for individuals. </i>It is now sensible to talk about Alice's false positive rate, or Bob's error rate, because rate has been redefined as an average over problems, for a particular individual. So we can now ask for individual fairness notions in the spirit of the statistical notions of fairness we discussed above! We no longer need to define protected groups: we can now ask that the false positive rates, or error rates, be equalized across all pairs of people. </div><div><br /></div><div>It turns out that given a reasonably sized sample of people, and a reasonably sized sample of problems, it is tractable to find the optimal classifier subject to constraints like this in sample, and that these guarantees generalize out of sample. The in-sample algorithm is again an oracle-efficient algorithm, or in other words, a reduction to standard, unconstrained learning. The generalization guarantee here is a little interesting, because now we are talking about simultaneous generalization in two different directions: to people we haven't seen before, and also to problems we haven't seen before. This requires thinking a little bit about what kind of object we are even trying to output: a mapping from new problems to classifiers. The details are in the paper (spoiler --- the mapping is defined by the optimal dual variables for the empirical risk minimization problem): here, I'll just point out that again, the algorithm is practical to implement, and we perform some simple experiments with it. </div><div><br /></div><div><br /></div><br /><br /><br /></div>







<p class="date">
by Aaron (noreply@blogger.com) <a href="http://aaronsadventures.blogspot.com/2019/05/individual-notions-of-fairness-you-can.html"><span class="datestr">at May 28, 2019 10:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/05/28/one-or-more-phd-stipends-in-machine-learning-for-wireless-communications-8-19015-at-department-of-electronic-systems-aalborg-university-apply-by-june-10-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/05/28/one-or-more-phd-stipends-in-machine-learning-for-wireless-communications-8-19015-at-department-of-electronic-systems-aalborg-university-apply-by-june-10-2019/">One or more PhD Stipends in Machine Learning for Wireless Communications (8-19015) at Department of Electronic Systems, Aalborg University (apply by June 10, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>WINDMILL Early Stage Researcher 9: Optimizing URLLC metadata/data flows using machine learning</p>
<p>Aalborg University is seeking to hire an Early Stage Researcher (ESR) to join the Marie Skłodowska-Curie Innovative Training Network on “Integrating Wireless Communication ENgineering and MachIne Learning”.(WindMill). More details are included <a href="https://windmill-itn.eu/">https://windmill-itn.eu/</a></p>
<p>Website: <a href="https://www.stillinger.aau.dk/vis-stilling/?vacancy=1029705">https://www.stillinger.aau.dk/vis-stilling/?vacancy=1029705</a><br />
Email: edc@es.aau.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/05/28/one-or-more-phd-stipends-in-machine-learning-for-wireless-communications-8-19015-at-department-of-electronic-systems-aalborg-university-apply-by-june-10-2019/"><span class="datestr">at May 28, 2019 09:14 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/05/28/one-or-more-phd-stipends-in-machine-learning-for-wireless-communications-8-19014-at-department-of-electronic-systems-aalborg-university-apply-by-june-10-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/05/28/one-or-more-phd-stipends-in-machine-learning-for-wireless-communications-8-19014-at-department-of-electronic-systems-aalborg-university-apply-by-june-10-2019/">One or more PhD Stipends in Machine Learning for Wireless Communications (8-19014) at Department of Electronic Systems, Aalborg University (apply by June 10, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Aalborg University is seeking to hire an Early Stage Researcher (ESR) to join the Marie Skłodowska-Curie Innovative Training Network on “Integrating Wireless Communication ENgineering and MachIne Learning”.(WindMill). More details are included <a href="https://windmill-itn.eu/">https://windmill-itn.eu/</a></p>
<p>Website: <a href="https://www.stillinger.aau.dk/vis-stilling/?vacancy=1029703">https://www.stillinger.aau.dk/vis-stilling/?vacancy=1029703</a><br />
Email: edc@es.aau.dk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/05/28/one-or-more-phd-stipends-in-machine-learning-for-wireless-communications-8-19014-at-department-of-electronic-systems-aalborg-university-apply-by-june-10-2019/"><span class="datestr">at May 28, 2019 09:11 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html">Shattering and quasipolynomiality</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>An inadequately-explained phenomenon in computational complexity theory is that there are so few natural candidates for <a href="https://en.wikipedia.org/wiki/NP-intermediate">-intermediate problems</a>, problems in  but neither in  nor -complete. Of course, if  there are none, and the <a href="https://en.wikipedia.org/wiki/Schaefer%27s_dichotomy_theorem">dichotomy theorem</a> implies that there are no intermediate Boolean constraint satisfaction problems. But there are a lot of other types of problems in , and a theorem of Ladner<sup id="fnref:l"><a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fn:l" class="footnote">1</a></sup> shows that there should be an infinite hierarchy of degrees of hardness within . So where are all the members of this hierarchy, and why are they so shy?</p>

<p>The same thing happens not just for  but for other related complexity classes like <a href="https://en.wikipedia.org/wiki/%E2%99%AFP"></a>. There should be many -intermediate classes but we know even fewer than for . <a href="https://mathstodon.xyz/@11011110/102118180704402052">I recently posted</a> about a discussion I had with Igor Pak on this issue, in which we suggested to each other two number-theoretic candidates for being -intermediate, the <a href="https://en.wikipedia.org/wiki/Euler%27s_totient_function">Euler totient function</a> and the <a href="https://en.wikipedia.org/wiki/Prime-counting_function">prime-counting function</a> (see also <a href="https://cstheory.stackexchange.com/q/43954/95">Igor’s StackExchange question on this</a>). But although they’re in , neither of these functions is very combinatorial.</p>

<p>So anyway, the point of all this is to discuss more candidates for being -intermediate that are, I think, natural and combinatorial. They’re part of a family of problems that include a couple of related candidates for being -intermediate, and even a candidate for being -intermediate. These problems come from computational learning theory, or alternatively they can be seen as coming from mathematical logic, hereditary graph theory, and the theory of the <a href="https://en.wikipedia.org/wiki/Rado_graph">Rado graph</a>. And they’re all at what is in some sense the shallow end of the intermediate problems: they’re solvable in quasi-polynomial time, meaning , but not known to be solvable in polynomial time. So this is pretty strong evidence that they’re not complete for their respective complexity classes, but weaker evidence than usual that they’re not polynomial.</p>

<p>In learning theory, a family of sets  is said to <em>shatter</em> another set  (not necessarily belonging to ) if every subset of , including the empty set and  itself, can be obtained by intersecting  with some member of . The <a href="https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension">Vapnik–Chervonenkis dimension</a> of  is just the size of the largest set that is shattered by  . If we let  (the number of sets in the family) and  (the number of distinct elements in those sets), then the dimension is clearly at most , because sets of size larger than  have too many subsets for them all to be formed by intersection with a member of . Therefore, the following problem can be solved in quasipolynomial time, by a brute-force search of the  small-enough subsets of :<sup id="fnref:lmr"><a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fn:lmr" class="footnote">2</a></sup></p>

<dl>
  <dt>VC-dimension (largest shattered set)</dt>
  <dd>Input: family of sets , number 

    <p>Output: true if  shatters a set of size , false otherwise.</p>
  </dd>
</dl>

<p>The same quasipolynomial time bound applies to the following related problems,
the first of which is also in  and the second of which is in :</p>

<dl>
  <dt>Smallest non-shattered set</dt>
  <dd>Input: family of sets , number 

    <p>Output: True if there exists a subset  of 
of size  that is not shattered by , false otherwise.</p>
  </dd>
  <dt>Number of shattered sets</dt>
  <dd>Input: family of sets 

    <p>Output: the number of sets shattered by .</p>
  </dd>
</dl>

<p>For the first two problems, being non--complete hinges on the assumption that , but for the number of shattered sets, being non--complete (under <a href="https://en.wikipedia.org/wiki/Polynomial-time_counting_reduction">counting reductions</a>) is unconditional: the output doesn’t provide enough bits of information to encode the answers to all other  problems.
The VC-dimension is hard to approximate under a form of the <a href="https://en.wikipedia.org/wiki/Exponential_time_hypothesis">exponential time hypothesis</a>, strongly suggesting that it cannot be computed exactly in polynomial time.<sup id="fnref:mr"><a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fn:mr" class="footnote">3</a></sup></p>

<p>To see that the two existence problems can sometimes both have answers that are logarithmic, it’s helpful to turn to the theory of random graphs, and of <em>the</em> random graph, the <a href="https://en.wikipedia.org/wiki/Rado_graph">Rado graph</a>. This graph obeys a collection of <em>extension axioms</em> according to which, for every two disjoint finite subsets of vertices, there exists another vertex adjacent to everything in the first subset and to nothing in the second subset. Using these axioms, we can build up induced copies of any finite or countable subgraph, one vertex at a time, using a greedy algorithm. Based on this property, let’s define a subset  of the vertices in an undirected graph to be <em>extensible</em> if, for every partition of  into two disjoint subsets, there exists another vertex outside  that is adjacent to everything in the first subset and to nothing in the second subset. This is nothing more than being shattered by the neighborhoods of the vertices outside . So we have the following corresponding problems.</p>

<dl>
  <dt>Largest extensible set</dt>
  <dd>Input: Undirected graph , number 

    <p>Output: true if  has an extensible set of size , false otherwise.</p>
  </dd>
  <dt>Smallest non-extensible set</dt>
  <dd>Input: Undirected graph , number 

    <p>Output: true if  has a non-extensible set of size , false otherwise.</p>
  </dd>
  <dt>Smallest missing induced subgraph</dt>
  <dd>Input: Undirected graph , number 

    <p>Output: true if there is a graph  on at most  vertices that
is not an induced subgraph of , false otherwise.</p>
  </dd>
  <dt>Number of extensible sets</dt>
  <dd>Input: Undirected graph 

    <p>Output: The number of extensible sets of vertices of .</p>
  </dd>
</dl>

<p>The smallest missing induced subgraph size naturally falls into the complexity class  of problems for which you can guess a solution (the missing subgraph) but then verifying it involves solving a co- problem (is this subgraph missing).
It is greater than the size of the smallest non-extensible set, because if you try to build up a given induced subgraph by adding one vertex at a time greedily you can only get stuck at a non-extensible set. There must be a missing induced subgraph of size at most , because there are  isomorphism classes of -vertex labeled graphs and fewer than  ways of choosing which of the  labeled vertices correspond to vertices of , so for larger values of  than this bound there are more labeled graphs than placements of them as induced subgraphs. Another way of thinking about the smallest missing induced subgraph problem is that we are asking for the largest  for which  is <a href="https://en.wikipedia.org/wiki/Universal_graph">-universal</a>: it contains all graphs on at most  vertices as induced subgraphs.</p>

<p>The smallest non-extensible set and the smallest missing subgraph are both easy on any hereditary class of graphs, because these classes always have a missing subgraph of size . On the other hand, if  is chosen uniformly at random among the -vertex graphs, then any small subset of its vertices is extensible with high probability, so the smallest non-extensible set has expected size .</p>

<p>If these problems are not - and -complete, what are they? Papadimitriou and Yannakakis<sup id="fnref:py"><a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fn:py" class="footnote">4</a></sup> define a complexity class , and show that VC-dimension is -complete. Presumably, because it’s so similar, the same is true for the largest extensible set. Maybe it’s possible to prove completeness for the smallest missing induced subgraph in an analogue of  at the level of , and to prove completeness for the number of shattered sets and number of extensible sets in an analogue of  at this level.</p>

<div class="footnotes">
  <ol>
    <li id="fn:l">
      <p>Ladner, Richard (1975), “<a href="https://doi.org/10.1145/321864.321877">On the structure of polynomial time reducibility</a>”, <em>J. ACM</em> 22 (1): 155–171. <a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fnref:l" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:lmr">
      <p>Linial, Nathan, Mansour, Yishay, and Rivest, Ronald L. (1991), “<a href="https://doi.org/10.1016/0890-5401(91)90058-A">Results on learnability and the Vapnik–Chervonenkis dimension</a>”, <em>Inf. Comput.</em> 90 (1): 33–49. <a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fnref:lmr" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:mr">
      <p>Manurangsi, Pasin, and Rubinstein, Aviad (2017), “<a href="http://proceedings.mlr.press/v65/manurangsi17a.html">Inapproximability of VC dimension and Littlestone’s dimension</a>”, <em>Proc. 2017 Conf. Learning Theory (COLT 2017)</em>, Proceedings of Machine Learning Research 65, pp. 1432–1460. <a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fnref:mr" class="reversefootnote">↩</a></p>
    </li>
    <li id="fn:py">
      <p>Papadimitriou, Christos H., and Yannakakis, Mihalis (1996), “<a href="https://doi.org/10.1006/jcss.1996.0058">On limited nondeterminism and the complexity of the V–C dimension</a>”, <em>J. Comput. Syst. Sci.</em> 53 (2): 161–170. <a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fnref:py" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div>

<p>(<a href="https://mathstodon.xyz/@11011110/102170815471019923">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html"><span class="datestr">at May 27, 2019 05:07 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-3247584017741087776">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/05/separating-fact-from-fiction-with-56-of.html">separating fact from fiction with the 56% of Americans say Arabic Numerals should not be taught in school</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<br />
On the excellent TV show Veep there was a subplot about a political candidate (who himself had failed algebra in HS) objecting to Algebra since it was invented by the Muslims. I don't recall the exact line, but he said something like `Math teachers are terrorists'<br />
This was, of course, fiction.<br />
<br />
The same week I read that 56% of survey respondents say `<u><i>Arabic Numerals' shouldn't be taught in</i></u> <i><u>schools'</u></i> Obviously also a fiction. Perhaps a headline from <i>The Onion</i>.<br />
<br />
No. The story is true.<br />
<br />
See snopes entry on this: <a href="https://www.snopes.com/fact-check/teaching-arabic-numerals/">here</a><br />
<br />
but also see many FALSE but FUNNY websites:<br />
<br />
Sarah Palin wants Arabic Numerals out of the schools: <a href="http://nationalreport.net/sarah-palin-wants-arabic-numerals-banned-americas-schools/">here</a> Funny but false.<br />
<br />
Jerry Brown is forcing students in California to learn Arabic Numerals as part of multi-culturism False by funny:  <a href="https://me.me/i/sharia-law-must-be-stopped-under-gov-brown-students-in-20990368">here</a><br />
<br />
A website urging us to use Roman Numerals (which Jesus used!) False but funny:  <a href="http://freedomnumerals.com/">here</a><br />
<br />
OKAY, what to make of the truth that really, really, 56% of Americans are against Arab Numerals<br />
<br />
1) Bigotry combined with ignorance.<br />
<br />
2) Some of the articles I read about this say its a problem with polls and people. There may be some of that, but still worries me.<br />
<br />
3) In Nazi Germany (WOW- Goodwin's law popped up rather early!) they stopped teaching relativity because Albert Einstein was Jewish (the story is more complicated than that, see <a href="https://www.scientificamerican.com/article/how-2-pro-nazi-nobelists-attacked-einstein-s-jewish-science-excerpt1/">her</a>e). That could of course never happen in America now (or could it, see <a href="https://www.tabletmag.com/jewish-news-and-politics/50097/time-warp">here</a> and <a href="https://www.conservapedia.com/index.php?title=Counterexamples_to_Relativity">here</a>).<br />
<br />
4) There is no danger that we will dump Arabic Numerals. I wonder if we will change there name to Freedom Numerals.<br />
<br />
5) Ignorance of science is a more immediate problem with the anti-vax people. See <a href="https://www.thedailybeast.com/measles-outbreak-grows-with-60-new-cases-across-26-states?ref=home">here</a><br />
<br />
<br /></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/05/separating-fact-from-fiction-with-56-of.html"><span class="datestr">at May 27, 2019 03:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-955320605612447790">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2019/05/an-interview-with-jamie-gabbay-and.html">An interview with Jamie Gabbay and Andrew Pitts, 2019 Alonzo Church Award recipients</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The 2019 Alonzo Church Award committee consisting of Thomas Eiter, Javier Esparza, Radha Jagadeesan, Catuscia Palamidessi, and Natarajan Shankar, have selected <a href="http://www.gabbay.org.uk/">Murdoch J. Gabbay </a>and <a href="https://www.cl.cam.ac.uk/~amp12/">Andrew M. Pitts </a>for the <a href="http://eatcs.org/index.php/component/content/article/1-news/2812-the-2019-alonzo-church-award">2019 Alonzo Church Award</a>, for introducing the theory of nominal representations, a powerful and elegant mathematical model for computing with data involving atomic names. In particular, the nomination for the Alonzo Church Award singled out the following two papers:<br /><ul><li>“<a href="https://www.cl.cam.ac.uk/~amp12/papers/newaas/newaas-jv.pdf">A new approach to abstract syntax with variable binding</a>” by Murdoch J. Gabbay and Andrew M. Pitts, Formal Aspects of Computing 13(3):341– 363, 2002; and</li><li>“<a href="https://www.cl.cam.ac.uk/~amp12/papers/nomlfo/nomlfo-draft.pdf">Nominal logic, a first order theory of names and binding</a>” by Andrew M. Pitts, Information and Computation 186(2):165–193, 2003.</li></ul>For the conference version of the first article, Andy and Jamie will also be receiving the Test-of-Time Award from LICS 1999.<br /><br />The award recipients kindly agreed to answer some questions of mine via email. You can find the transcript of the interview below. My questions are labelled with <b>LA</b>, Andy's answers with <b>AP</b> and Jamie's with <b>JG</b>. I hope that you'll enjoy reading their insights and the story of their award-receiving work as much as I did myself. <br /><br /><div dir="ltr"><b>LA: </b>You are receiving the 2019 Alonzo Church Award  for  Outstanding Contributions to Logic and Computation as well as the  Test-of-Time Award from LICS 1999 for your invention of nominal  techniques to provide a semantic understanding of abstract syntax with  binding.   Could you briefly describe the history of the ideas that led  you to use the <a href="https://en.wikipedia.org/wiki/Permutation_model">permutation model of set theory with atoms</a> due to  Fraenkel and Mostowski to represent name abstraction and fresh name  generation? What were the  main inspirations and motivations for your work? In your opinion, how  did nominal techniques advance the state of the art at that time?</div><div dir="ltr"></div><br /><div dir="ltr"><b>AP: </b>I have had a long-standing interest in the mathematical semantics of programming language features that restrict resources to a specific scope, or hide information from a program's environment; think local mutable state in languages like <a href="http://ocaml.org/">OCaml</a>, or channel-name restriction in the <a href="https://en.wikipedia.org/wiki/%CE%A0-calculus">pi-calculus</a>. When <a href="http://homepages.inf.ed.ac.uk/stark/">Ian Stark</a> was doing his PhD with me in the 90s we tried to understand a simple instance: the observable properties of higher-order functions combined with dynamically generated atomic names that can be tested for equality, but don't have any other attribute -- we called this the "nu-calculus". Ian gave a denotational semantics for the nu-calculus using Moggi's monad for modelling dynamic allocation. That monad is defined on the category of pullback-preserving functors from the category of injective functions between finite ordinals to the category of sets. This functor category was well-known to me from topos theory, where it is called <a href="https://ncatlab.org/nlab/show/Schanuel+topos">Schanuel's topos </a>and hosts the generic model of a geometric theory of an infinite decidable set.  A few years later, when Jamie joined me as a PhD student in 1998, I suggested we look at the Schanuel topos as a setting for initial algebra semantics of syntax involving binding operations, modulo alpha-equivalence. I think Jamie prefers set theory over category theory, so he pushed us to use another known equivalent presentation of the Schanuel topos, in terms of continuous actions of the group of permutations of the set N of natural numbers (topologized as a subspace of the product of countably many copies of N). In this form there is an obvious connection with the cumulative hierarchy of sets (with atoms) that are hereditarily finitely supported with respect to the action of permuting atoms. This universe of sets was devised by Fraenkel and Mostowski in the first part of the twentieth century to model ZFA set theory without axioms of choice.  Whether one emphasises set theory or category theory, the move to making permutations of names, rather than injections between sets of names, the primary concept was very fruitful. For example, it certainly makes higher-order constructions (functions and powersets) in the topos/set-theory easier to describe and use. We ended up with a generic construction for name-abstraction modulo <a href="https://en.wikipedia.org/wiki/Lambda_calculus#Alpha_equivalence">alpha-equivalence</a> compatible with classical higher-order logic or set theory, so long as one abstains from unrestricted use of choice. </div><div dir="ltr"></div><div dir="ltr"><br /><b>JG:</b> At the time it wasn't an idea to consider names as elements of a distinctive datatype of names, with properties just like other datatypes such as the natural numbers Nat. If we want to add 1 to 1, we take 1:Nat and invoke the "plus" function, which is a specific thing associated to Nat; so why not abstract a in x by assuming a:Atm (where Atm is a distinct thing in our mathematical universe) and x:X and invoking a function "abstract", which is a thing associated to Atm?  We unfolded the implications of this idea in set theory and rediscovered FM sets.  I was inspired by the way I saw mathematics built up in ZF set theory as an undergraduate, starting from a simple basis and building up the cumulative hierarchy.  When I saw the chance to do this for a universe with names, I jumped at the chance.  It turns out FM sets are not required.  Nominal techniques can be built in ZFA set theory, which contains more functions and permits unrestricted choice. </div><div dir="ltr"><br /><b>LA: </b>Over  the last fifteen years, nominal techniques have become a fundamental  tool for modelling locality in computation, underlying research  presented in over a hundred papers, new programming languages and models  of computation. They have applications to the syntax and semantics of  programming languages, to logics for machine-assisted reasoning about  programming-language semantics and to the automatic verification of  specifications in process calculi. Variations on nominal sets are used  in automata theory over infinite alphabets, with applications to  querying XML and databases, and also feature in work on models of  Homotopy Type Theory. When did it dawn on you that you had succeeded in  finding a very good model for name abstraction and fresh name  generation, and one that would have a  lot of impact? Did you imagine that your model would generate such a  large amount of follow-up work, leading to a whole body of work on  nominal computation theory? <br /><br /><b>AP: </b>No, to begin with I was very focussed on getting better techniques for computing and reasoning about syntax with bound names. But that only represents a part of the current broad landscape of nominal techniques, the part that mainly depends on the mathematical notion of "finite support" (a way of expressing, via name-permutation, that an object only involves finitely many names). Independently of us, some people realised that a related notion of finiteness, "orbit-finiteness" (which expresses that an object is finite modulo symmetries) is crucial for many applications of nominal techniques. I am referring to the work of Montanari and Pistore on pi-calculus and <a href="https://core.ac.uk/download/pdf/82414059.pdf">HD automata </a>using named sets (yet another equivalent of the Schanuel topos) and the work on automata theory over infinite alphabets (and much else besides) using "sets with atoms" by the Warsaw group (Bojanczyk, Klin, Lasota, Torunczyk,...). The latter is particularly significant because it considers groups of symmetry for atoms other than the full permutation group (in which the only property of an atom preserved under symmetry is its identity). <br /><br /><b>JG:</b> Yes, I did.  Nobody could anticipate the specific applications but I knew we were on to something, which is why I stayed on to build the field after the PhD.  The amount of structure was just too striking.  This showed early: e.g. in the equivariance properties, and the commutation of nominal atoms-abstraction with function-spaces.  When I sent the proof of this property to Andrew, at first he didn't believe it!  I had a sense that there was something deep going on and I still do. <br /><br /><b>LA: </b>What is the result of  yours on nominal techniques you are most proud of? And what are your  favourite results amongst those achieved by others on nominal computation?<b></b><br /><b><br /></b><b>AP:</b> Not so much a specific result, but rather a logical concept, the freshness quantifier (which we wrote using an upside down "N" -- N stands for "New"). In informal practice when reasoning about syntax involving binders, one often chooses <i>some</i> fresh name for the bound variable, but then has to revise that choice in view of later ones; but fortunately <i>any </i>fresh name does as well as some particular one. This distinctive "some/any" property occurs all over the place when computing and reasoning about languages with binders and the freshness quantifier formalises it, in terms of the freshness ("not in the support of") relation and conventional quantifiers.  For the second part of your question I would choose two things. One is the work by Jamie with Fernandez and Mackie on <a href="https://www.sciencedirect.com/science/article/pii/S0890540106001635">nominalrewriting systems</a>, which won the PPDP Most Influential Paper 10-year Award in 2014. The second is the characterisation of orbit-finite sets with atoms in terms of "set-builder expressions"---see Klin et al, "<a href="https://www.mimuw.edu.pl/~szymtor/papers/locfin.pdf">Locally Finite Constraint Satisfaction Problems</a>", Proc. LICS 2015); it's a nice application of the classical model theory of homogeneous structures with interesting applications for languages that compute with finite structures. <br /><br /><b>JG:</b> Thanks for asking.  Aside from the initial papers, my work on nominal rewriting with Fernandez has probably had most impact.  However, I am rather fond of the thread of research going from Nominal Algebra, through the axiomatisation of substitution and first-order logic and the characterisation of quantification as a limit in nominal sets, and on to Stone duality.  It's a mathematical foundation built from a nominal perspective of naming and quantification and I hope that as the state of the art in nominal techniques advances and broadens, it might prove useful.  Andrew's book has been helpful in marking out nominal techniques as a field.  I also agree with Andrew that orbit-finiteness and the applications of this idea to transition systems and automata, is important.  I like the automata work for another concrete reason: nominal techniques were discovered in the context of names and binding in syntax, which has bequeathed a misconception that nominal techniques are <i>only</i> about this.  The Warsaw school of nominal techniques gives an independent illustration of the other applications of these ideas. <br /><b><br /></b><b><b>LA: </b></b>Twenty years have passed since your LICS 1999 paper and the  literature on variations on nominal techniques now contains over a hundred papers. Do you expect any further  development related to the theory and application of nominal techniques in the  coming years? What advice would you give to a PhD student who is  interested in working on topics related to nominal computation today?</div><div dir="ltr"><br /><b>AP: </b> For the purpose of answering your question, let's agree to divide LICS topics into Programming Languages and Semantics (PLS) versus Logic and Algorithms (LAS). (So long as we don't think of it as a dichotomy!) Then it seems to me that applications of nominal techniques to LAS are currently in the ascendant and show no sign of slowing down. My own interests are with PLS and there is still work to be done there. In particular, I would like better support for using nominal techniques within the mainstream interactive theorem proving systems: we have the Nominal Package of Urban and Berghofer for classical higher-order logic within Isabelle (which lead to Urban and Tasson winning the CADE Skolem Award in 2015), but nothing analogous for systems based on dependent type theory, such as Agda, Coq and Lean. Recent work of Swan (arXiv:1702.01556) gives us a better understanding of how to develop nominal sets within constructive logic; but I have yet to see a dependent type theory that both corresponds to some form of constructive nominal logic under Curry-Howard and is sufficiently simple that it appeals to users of systems lke Coq who want to mechanise programming language meta-theory in a nameful style. Really, I would like the utility of the FreshML programming language that Jamie, Mark Shinwell and I proposed in 2003 (and which Mark implemented as a patch of OCaml) restricted to total functional programming in the style of Agda; but I don't quite know how to achieve that. <br /><br /><b>JG:</b> Yes.  We are far from understanding nominal techniques and the field has a lot of life and will continue to surprise.  I've always believed that.  A key sticking-point right now is implementations.  I wrote a paper about this recently, on equivariance and the foundations of nominal techniques.  One point in the paper is a sketch for a next-generation nominal theorem-prover (based on ZFA + equivariance).  I'd like to see this carried out, so if anybody reading this is interested then please be in touch.  I'd also like to see nominal techniques implemented as a package in a language like Haskell, ML, or even Python!  If we can get this stuff into the working programmer's toolbox, in a way that just works and does not require special configuration, then that would be helpful.  I suspect that nominal techniques as currently presented in the maths papers, might not fit into a programming language at the moment.  The theory is too strong and may need weakened first.  We need a subset of nominal techniques weak enough to squeeze into an existing language, yet expressive enough for interesting applications.  Some general advice, specifically for the PhD student.  If you have an idea which most people around you don't understand, consider this may be a gap in the collective imagination.  There can be peer pressure when faced by incomprehension to blame yourself, back down, and think about something else.  By all means do this, but only if you yourself judge it right to do so. <br /><br /></div><b>LA: </b>Is there any general research-related lesson you have learnt in the process of working on nominal techniques?<br /><br /><b>AP: </b>On the one hand, don't lose sight of what application your theory is supposed to be good for; but on the other hand, let beauty and simplicity be your guide.<br /><br /><b>JG:</b> Yes:<br /><ul><li>Proving stuff is 30% of the work; convincing people is 70%. </li><li>It's the basic ideas that are hard, not the complicated theorems.</li><li>Competence and imagination are orthogonal. </li><li>It's doesn't have to be complex to be clever. </li><li>Elegant + applicable is a potent combination. </li><li>Seek out good listeners.  Give up quickly on bad ones.  Try to be a good listener. </li><li>Other people have a lot to teach you, but it might not be the things you expected. </li><li>Writing papers is fun.   </li></ul><b>LA: </b>Thanks to both of you for your willingness to answer my questions and congratulations for the awards you will be receiving this summer!</div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2019/05/an-interview-with-jamie-gabbay-and.html"><span class="datestr">at May 26, 2019 10:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/076">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/076">TR19-076 |  The Equivalences of Refutational QRAT | 

	Leroy Chew, 

	Judith Clymo</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The solving of Quantified Boolean Formulas (QBF) has been advanced considerably in the last two decades. In response to this, several proof systems have been put forward to universally verify QBF solvers. 
QRAT by Heule et al. is one such example of this and builds on technology from DRAT, a checking format used in propositional logic. 
Recent advances have shown conditional optimality results for QBF systems that use extension variables.
Since QRAT can simulate Extended Q-Resolution, we know it is strong, but we do not know if QRAT has the strategy extraction property as Extended Q-Resolution does. In this paper, we partially answer this question by showing that QRAT with a restricted reduction rule has strategy extraction (and consequentially is equivalent to Extended Q-Resolution modulo NP).
We also extend equivalence to another system, as we show an augmented version of QRAT known as QRAT+, developed by Lonsing and Egly, is in fact equivalent to the basic QRAT. We achieve this by constructing a line-wise simulation of QRAT+ using only steps valid in QRAT.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/076"><span class="datestr">at May 26, 2019 10:13 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/05/25/more-matching-mimicking">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/05/25/more-matching-mimicking.html">More matching-mimicking networks</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>My <a href="https://11011110.github.io/blog/2018/02/01/parallel-matching-in.html">paper with Vijay Vazirani on parallel matching</a> (soon to appear in <a href="http://spaa.acm.org/2019/">SPAA</a>) is based on the idea of a “matching-mimicking network”. If  is a graph with a designated set  of terminal vertices, then a matching-mimicking network for  is another graph  with the same terminals that has the same pattern of matchings. Here, by a <em>pattern of matchings</em>, I mean a family of subsets of , the subsets that can be covered by a matching that also covers all non-terminal vertices. We included a messy case analysis that, after some simplifications due to symmetry, had 21 cases for the matching mimicking networks on at most three terminals.</p>

<p>By now, I think I understand patterns of matchings a lot better, enough to do the three-terminal case in only four cases and to extend the analysis to four terminals in only seven more cases. The starting point is the observation that these patterns of matchings are <a href="https://en.wikipedia.org/wiki/Delta-matroid">even Δ-matroids</a>.</p>

<p>One way to think of a Δ-matroid is that it’s just a convex polyhedron or polytope in Euclidean space of some dimension , with the properties that all vertex coordinates are  or  and all edge lengths are  or . An even Δ-matroid has the stronger property that all edge lengths are , as is true for the three-dimensional regular tetrahedron with vertex coordinates (written in a more compact form as bitvectors) , , , and .</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/tet-in-cube.svg" alt="Regular tetrahedron formed from alternating vertices of a cube" /></p>

<p>Alternatively one can consider the same kind of structure to be a family of sets, drawn from a universe of  elements that correspond to the dimensions of the space. Each set in the family corresponds to a vertex of the polytope and includes the elements whose coordinates are one. So over the three-element set  the same regular tetrahedron can be written as the family of sets</p>



<p>When expressed in this way, the sets of a Δ-matroid obey an exchange axiom: if two sets  and  differ on whether they include some element , then there must exist an element  on which they also differ, so that the symmetric difference of sets  also belongs to the Δ-matroid. By repeatedly applying this axiom one can connect  to  by a geodesic path (in Hamming distance) of two-element moves. For the bases of a matroid, we have a stronger requirement that one of the two elements belongs to  and the other belongs to , or equivalently that all sets have the same size, but a Δ-matroid relaxes this requirement. It’s not even required that ! But in an even Δ-matroid  and  must be distinct, because otherwise the step would be along an edge of length one. Another way of expressing the extra requirements of an even Δ-matroid over an arbitrary Δ-matroid is that all sets must have the same parity (all have even size, or all have odd size).</p>

<p>So anyway, back to matching. Suppose that both  and  are sets drawn from a pattern of matchings. Choose arbitrarily a matching representing each set. Then the symmetric difference of these matchings is a collection of disjoint alternating paths and cycles, and we can get from  to  by 
a sequence of steps in which we take the symmetric difference of the current matching by one of the alternating paths. So this gives us not just one geodesic from  to  but a lot of different geodesics, one for each ordering of the alternating paths. Expressed as an exchange axiom, this means that when two sets  and  differ, the elements on which they differ can be partitioned into pairs, the symmetric differences with which can be performed independently. You can pick any subset of the pairs of differing elements, and change each of those pairs, leaving the rest alone. Because this is a strengthening of the even Δ-matroid axiom, every matching pattern is an even Δ-matroid.</p>

<p>Expressed in polyhedral terms, this stronger exchange axiom means that every two vertices at distance  from each other are connected by a -dimensional hypercube with side length . This is a little weird, because we started with a hypercube but then eliminated half of its vertices (by the parity condition) to get something else. Now we have hypercubes again, of lower dimension. They must be tilted with respect to the coordinate axes: each axis of one of these lower-dimensional hypercubes is tilted at a 45 degree angle with respect to the coordinate system of the overall polytope.</p>

<p>Not every even Δ-matroid obeys this sub-hypercube property. On the other hand, I was expecting the matching patterns that are matroids (all sets are the same size) to be transversal matroids (maximal subsets of vertices on one side of a bipartite graph that can be covered by a matching), and they aren’t. There is a six-element non-transversal matroid, whose six elements are the edges of a triangle with doubled edges and whose sets are pairs of edges from different sides of the triangle. But it is the pattern of matchings of a tree in which the (non-terminal) root has three children, each of which has two terminals as its children.</p>

<p>Conveniently, whether a 0-1 polyhedron can be represented by a pattern of matchings depends only on its shape and not on its orientation. You can obviously permute the coordinates of a polyhedron that represents a pattern of matchings, by relabeling which coordinate corresponds to which terminal vertex. But you can also reflect the polyhedron across any one of its coordinates by modifying the graph whose matchings represent it in the following way: turn the  terminal vertex for that coordinate into a non-terminal, and attach a new degree-one terminal vertex to it. These permutations and reflections generate all the symmetries of the hypercube in which the 0-1 polyhedron lives. So to find small matching-mimicking networks, we only need to look at one representative 0-1 polyhedron in each symmetry class. If we find a small network for this representative, we can modify it to create a different small matching-mimicking network for every other 0-1 polyhedron with the same shape.</p>

<p>So what are the possible shapes? Let’s define the dimension of a Δ-matroid to be the number of coordinates of the polytope that take both values,  and , at different vertices. Then a 0-dimensional even Δ-matroid must be a single point (), there are no 1-dimensional even Δ-matroids, and a two-dimensional even Δ-matroid must be a line segment (). There are two three-dimensional even Δ-matroids: a triangle  and the tetrahedron shown above, . The cube exchange axiom for patterns of matching starts to kick in for four-dimensional even Δ-matroids, whose vertices must be subsets of the four-dimensional hyperoctahedron . (This is the shape formed from a four-dimensional hypercube by keeping only vertices with the same parity as each other, just as we formed a regular tetrahedron by doing the same thing to a three-dimensional cube.) Here’s a drawing of  from <a href="https://11011110.github.io/blog/2010/09/26/in-response-to.html">an earlier post</a>:</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2010/k7/cocktail2.svg" alt="The hyperoctahedral graph K_{2,2,2,2}" /></p>

<p>If there are no two opposite vertices, we get  (a regular tetrahedron, again, but embedded in a four-dimensional way into the hypercube). Otherwise, we must take at least two pairs of opposite vertices to form a square, and the cases are  (only the square),  (a square pyramid),  (an octahedron), ,  (an octahedral pyramid), and  (the hyperoctahedron). All of these polyhedra can be represented as matching-mimicking networks with the additional property that all vertices are terminals:</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/4-terminal-mm.svg" alt="Matching-mimicking networks for up to four terminals" /></p>

<p>Based on these small examples, it’s tempting to guess that when a pattern of matchings includes the empty set, the whole pattern is just the set of matchings on a graph whose edges are the pairs of terminals in the pattern. But it isn’t true. The square pyramid , for instance, can represent the pattern of matchings</p>



<p>with the empty set at the apex of the pyramid. In this pattern, even though one can match terminal pairs <span style="white-space: nowrap;">— or —,</span> one can’t take the union of those two matchings and cover all four terminals. (This is what you get by reflecting the two middle terminals of the network shown above for ; its matching-mimicking network is a tree with two interior non-terminals and four terminal leaves.) My guess is that the number of patterns of matching should grow quickly relative to the number of graphs, so for large enough numbers of terminals it should not be possible to use graphs without non-terminals or their complements. But I haven’t taken the case analysis far enough to find an example of this.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/102160605632804102">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/05/25/more-matching-mimicking.html"><span class="datestr">at May 25, 2019 09:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15910">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/05/25/selected-papers-at-ccc-2019/">Selected Papers at CCC 2019</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><font color="#0044cc"><br />
<em>Some papers from the accepted list of this year’s Computational Complexity Conference</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/05/25/selected-papers-at-ccc-2019/unknown-122/" rel="attachment wp-att-15912"><img width="150" alt="" class="alignright  wp-image-15912" src="https://rjlipton.files.wordpress.com/2019/05/unknown-1.jpeg?w=150" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ UB CSE ]</font></td>
</tr>
</tbody>
</table>
<p>
Alan Selman is a long-time friend of Ken and I, and is a long-time researcher in complexity theory. Alan was the first president of the organizing <a href="https://www.computationalcomplexity.org/governance.php">body</a> for the Computational Complexity Conferences (CCC). </p>
<p>
Today we salute the <img src="https://s0.wp.com/latex.php?latex=%7B0b100010%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0b100010}" class="latex" title="{0b100010}" />th edition of the conference and discuss some of the accepted papers.<br />
<span id="more-15910"></span></p>
<p>
The conference, and the governing body, have changed names over the years; by any name it remains an important conference. Alan <a href="https://www.computationalcomplexity.org/documents/first-cfp.pdf">chaired</a> the first program committee with Steve Mahaney and <a href="https://dl.acm.org/citation.cfm?id=648296&amp;picked=prox">edited</a> the first proceedings, in 1986. </p>
<p>
Ken recently saw Alan two weeks ago at the banquet for the <a href="http://www.fields.utoronto.ca/activities/18-19/NP50">symposium</a> honoring Steve Cook at the University of Toronto. We will cover event, once <a href="https://rjlipton.wordpress.com/2019/05/21/making-up-tests/">exams</a> are done. Ken saw another of the CCC past presidents there—if you wish to guess who, a hint is it was one of Cook’s past students.</p>
<ul>
<li>
Dieter van Melkebeek, 2012-2018 <p></p>
</li><li>
Peter Bro Miltersen, 2009-2012 <p></p>
</li><li>
Pierre McKenzie, 2006-2009 <p></p>
</li><li>
Lance Fortnow, 2000-2006 <p></p>
</li><li>
Eric Allender, 1997-2000 <p></p>
</li><li>
Steven Homer, 1994-1997 <p></p>
</li><li>
Timothy Long, 1992-1994 <p></p>
</li><li>
Stephen Mahaney, 1988-1992 <p></p>
</li><li>
Alan Selman, 1985-1988
</li></ul>
<p>
Although we do not usually do announcements, we note from the conference <a href="https://computationalcomplexity.org">website</a>:</p>
<blockquote><p><b> </b> <em> Details of the local arrangements for CCC 2019 and the preceding events, including the DIMACS Day of Tutorials, are available. Early registration runs till June 26. </em>
</p></blockquote>
<p>
</p><p></p><h2> Six Papers with Some Comments </h2><p></p>
<p></p><p>
Here are some papers that I, Dick, found interesting from the list of accepted papers. All accepted papers are interesting, of course. I selected six that were on topics that were directly connected with my interests.</p>
<p>
<b>Criticality of Regular Formulas</b>—<a href="http://www.math.toronto.edu/rossman/criticality.pdf">paper</a><br />
Benjamin Rossman<br />
<i>I thought this was about regular expressions. Shows something about me.</i> Here “regular” means the in-degree of gates being the same at each level of the circuit. This condition seems likely to be removable as Rossman conjectures, but I doubt it will be easy. The term “criticality” is a parameter that measures how much a random restriction reduces the size of a formula. Think switching lemma.</p>
<p>
<b>Typically-Correct Derandomization for Small Time and Space</b>—<a href="https://arxiv.org/abs/1711.00565">paper</a><br />
William Hoza<br />
<i>I like the notion of typically-correct.</i> Their algorithms work by treating the input as a source of randomness. This idea was pioneered by Oded Goldreich and Avi Wigderson. The title of their 2002 <a href="http://www.wisdom.weizmann.ac.il/~oded/p_rnd02.html">article</a> “Derandomization that is rarely wrong from short advice that is typically good”, gives away how one can prove such results. </p>
<p>
<b>Optimal Short-Circuit Resilient Formulas</b>—<a href="https://arxiv.org/abs/1807.05014">paper</a><br />
Mark Braverman, Klim Efremenko, Ran Gelles, and Michael Yitayew <br />
<i>This is on a kind of fault-tolerance.</i> They consider fault-tolerant boolean formulas in which the output of a faulty gate is stuck at one of the gate’s inputs. This is an interesting model of errors, and they show roughly: any formula can be converted into a formula that is not too much bigger and survives even if about <img src="https://s0.wp.com/latex.php?latex=%7B1%2F5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1/5}" class="latex" title="{1/5}" /> of the gates are faulty. A surprise is that they use a method related to <i>blockchains</i>. Hmmmmm. Interesting.</p>
<p>
<b>Fourier and Circulant Matrices are Not Rigid</b>—<a href="https://arxiv.org/pdf/1902.07334.pdf">paper</a><br />
Allen Liu and Zeev Dvir <br />
<i>A matrix is rigid if its rank cannot be reduced significantly by changing a small number of entries.</i> As you probably know there are plenty of rigid matrices—take random ones—but no provable examples of explicit ones. Their beautiful results prove that specific families of matrices are not rigid. These families include ones that were long thought to be rigid. The highlight of this work could be that it suggests new families that may be rigid. </p>
<p>
<b>Average-Case Quantum Advantage with Shallow Circuits</b>—<a href="https://arxiv.org/pdf/1810.12792.pdf">paper</a><br />
François Le Gall <br />
<i>A quest, the quest that tops all others—is the search for evidence that quantum computers are better than classic ones.</i> Of course, this is nearly impossible, since P=PSPACE is an open problem. So one looks at special classes of computations. See <a href="https://arxiv.org/pdf/1612.05903.pdf">here</a> for how the quest for “quantum advantage” meets up with computational complexity.</p>
<p>
<b>Relations and Equivalences Between Circuit Lower Bounds and Karp-Lipton Theorems</b><br />
<a href="https://eccc.weizmann.ac.il/report/2019/075/">paper</a><br />
Lijie Chen, Dylan McKay, Cody Murray, and Ryan Williams <br />
<i>Of course I think this is an interesting paper.</i> There is the famous H-score. Perhaps there could be a T-score. This would be the number of times your name is in the title of a published paper. Thus Ron Rivest, for example, has a huge T-score.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
What are your selected papers? </p>
<p>
[Added link to Relations and Equivalences… paper]</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2019/05/25/selected-papers-at-ccc-2019/"><span class="datestr">at May 25, 2019 03:10 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
