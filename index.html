<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">"Greg Yang"</a>
<br>
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/?tag=tcs&amp;feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" class="message" title="403: forbidden">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at June 17, 2019 09:22 PM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2019/06/17/3rd-school-on-foundations-of-programming-and-software-systems/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2019/06/17/3rd-school-on-foundations-of-programming-and-software-systems/">3rd School on Foundations of Programming and Software Systems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
September 10-15, 2019 Warsaw, Poland https://www.mimuw.edu.pl/~fopss19/ The Summer School on Foundations of Programming and Software Systems (FoPSS) was jointly created by EATCS, ETAPS, ACM SIGLOG and ACM SIGPLAN. It was first organised in 2017. The goal is to introduce the participants to various aspects of computation theory and programming languages. The school, spread over a … <a href="https://cstheory-events.org/2019/06/17/3rd-school-on-foundations-of-programming-and-software-systems/" class="more-link">Continue reading <span class="screen-reader-text">3rd School on Foundations of Programming and Software Systems</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2019/06/17/3rd-school-on-foundations-of-programming-and-software-systems/"><span class="datestr">at June 17, 2019 12:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=16001">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/06/17/contraction-and-explosion/">Contraction and Explosion</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Different ways of recursing on graphs</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<img src="https://rjlipton.files.wordpress.com/2019/06/tuttebletchley.jpg?w=120&amp;h=200" alt="TutteBletchley" width="120" class="alignright wp-image-16003" height="200" />
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Bletchley Park 2017 <a href="https://bletchleypark.org.uk/news/codebreaker-bill-tutte-to-be-celebrated-in-centenary-exhibition">source</a></font></td>
</tr>
</tbody>
</table>
<p>
William Tutte was a British combinatorialist and codebreaker. He worked in a different group at Bletchley Park from that of Alan Turing. He supplied several key insights and algorithms for breaking the Lorenz cipher <a href="https://en.wikipedia.org/wiki/Lorenz_cipher">machine</a>. His algorithms were implemented alongside Turing’s on <a href="https://en.wikipedia.org/wiki/Colossus_computer">Colossus</a> code-breaking computers.</p>
<p>
Today we discuss graph recursions discovered by Tutte and Hassler Whitney.</p>
<p>
Tutte wrote a doctoral thesis after the war on graph theory and its generalization into <em>matroid theory</em>. We will follow the same arc in this and a followup post. He joined the faculty of the universities of Toronto and then Waterloo, where he was active long beyond his retirement. </p>
<p>
For more on Tutte and his work, see this <a href="https://theconversation.com/remembering-bill-tutte-another-brilliant-codebreaker-from-world-war-ii-77556">article</a> and <a href="http://thelaborastory.com/stories/william-thomas-tutte/">lecture</a> by Graham Farr, who is a professor at Monash University and a longtime friend of Ken’s from their Oxford days. We covered some of Tutte’s other work <a href="https://rjlipton.wordpress.com/2010/06/02/the-tensor-trick-and-tuttles-flow-conjectures/">here</a>.</p>
<p></p><h2> Deletion and Contraction </h2><p></p>
<p></p><p>
The two most basic recursion operations are <em>deleting</em> and <em>contracting</em> a chosen edge <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> in a given graph <img src="https://s0.wp.com/latex.php?latex=%7BG+%3D+%28V%2CE%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G = (V,E)}" class="latex" title="{G = (V,E)}" />:</p>
<p><img src="https://rjlipton.files.wordpress.com/2019/06/deletion_contraction.png?w=400&amp;h=118" alt="deletion_contraction" width="400" class="aligncenter wp-image-16004" height="118" /></p>
<p>
These operations produce graphs denoted by <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Csetminus+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G \setminus e}" class="latex" title="{G \setminus e}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BG%2Fe%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G/e}" class="latex" title="{G/e}" />, respectively. A motive for them harks back to Gustav Kirchhoff’s counting of spanning trees:</p>
<ul>
<li>
A spanning tree of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> avoids using edge <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> if and only if it is a spanning tree of the graph <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Csetminus+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G \setminus e}" class="latex" title="{G \setminus e}" /> with <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> deleted.<p></p>
</li><li>
A spanning tree of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> uses edge <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> if and only if the rest of it is a spanning tree of the graph <img src="https://s0.wp.com/latex.php?latex=%7BG%2Fe%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G/e}" class="latex" title="{G/e}" /> after contracting <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" />.
</li></ul>
<p>
Well, this is not how Kirchhoff counted trees. Counting via the recursion would take exponential time. Our whole object will be telling which cases of the recursions can be computed more directly.</p>
<p>Note that contracting one edge of the triangle graph <img src="https://s0.wp.com/latex.php?latex=%7BC_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_3}" class="latex" title="{C_3}" /> produces a <em>multi-</em>graph <img src="https://s0.wp.com/latex.php?latex=%7BC_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_2}" class="latex" title="{C_2}" /> with one double-edge. Then contracting one edge of <img src="https://s0.wp.com/latex.php?latex=%7BC_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_2}" class="latex" title="{C_2}" /> yields the loop graph <img src="https://s0.wp.com/latex.php?latex=%7BC_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_1}" class="latex" title="{C_1}" />.</p>
<p></p><p><br />
<img src="https://rjlipton.files.wordpress.com/2019/06/triangle.png?w=250&amp;h=73" alt="triangle" width="250" class="aligncenter wp-image-16005" height="73" /></p>
<p></p><p><br />
Thus contraction yields non-simple undirected graphs, but the logic of counting their spanning trees remains valid.</p>
<p>
The order of edges does not matter as long as one avoids disconnecting the graph, and the base case is a tree (ignoring any loops) which contributes <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />.</p>
<p></p><h2> Tutte’s Polynomial </h2><p></p>
<p></p><p>
A similar recursion counts colorings <img src="https://s0.wp.com/latex.php?latex=%7Bc%3A+V+%5Crightarrow+%5C%7B1%2C...%2Ck%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c: V \rightarrow \{1,...,k\}}" class="latex" title="{c: V \rightarrow \{1,...,k\}}" /> that are <em>proper</em>, meaning that for each edge <img src="https://s0.wp.com/latex.php?latex=%7Be+%3D+%28u%2Cv%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e = (u,v)}" class="latex" title="{e = (u,v)}" />, <img src="https://s0.wp.com/latex.php?latex=%7Bc%28u%29+%5Cneq+c%28v%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c(u) \neq c(v)}" class="latex" title="{c(u) \neq c(v)}" />.</p>
<ul>
<li>
A proper coloring <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c}" class="latex" title="{c}" /> of <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Csetminus+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G \setminus e}" class="latex" title="{G \setminus e}" /> makes <img src="https://s0.wp.com/latex.php?latex=%7Bc%28u%29+%5Cneq+c%28v%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c(u) \neq c(v)}" class="latex" title="{c(u) \neq c(v)}" /> iff it is a proper coloring of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />.<p></p>
</li><li>
A proper coloring <img src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c}" class="latex" title="{c}" /> of <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Csetminus+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G \setminus e}" class="latex" title="{G \setminus e}" /> makes <img src="https://s0.wp.com/latex.php?latex=%7Bc%28u%29+%3D+c%28v%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c(u) = c(v)}" class="latex" title="{c(u) = c(v)}" /> iff it induces a proper coloring of <img src="https://s0.wp.com/latex.php?latex=%7BG%2Fe%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G/e}" class="latex" title="{G/e}" />.
</li></ul>
<p>
This leads to the recursive definition of the <em>chromatic polynomial</em>:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++P_G%28x%29+%3D+P_%7BG%5Csetminus+e%7D%28x%29+-+P_%7BG%2Fe%7D%28x%29.+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle    P_G(x) = P_{G\setminus e}(x) - P_{G/e}(x).   " class="latex" title="\displaystyle    P_G(x) = P_{G\setminus e}(x) - P_{G/e}(x).   " /></p>
<p>
The base cases are that an isolated vertex contributes <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />, whereas an isolated loop contributes <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> since its single edge is never properly colored. The final rule is that <img src="https://s0.wp.com/latex.php?latex=%7BP_G%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P_G(x)}" class="latex" title="{P_G(x)}" /> is always the product of <img src="https://s0.wp.com/latex.php?latex=%7BP_H%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P_H(x)}" class="latex" title="{P_H(x)}" /> over all connected components <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />. Then <img src="https://s0.wp.com/latex.php?latex=%7BP_G%28k%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P_G(k)}" class="latex" title="{P_G(k)}" /> counts the number of proper <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" />-colorings.</p>
<p>
This is like the recursion for coutning spanning terees except for the minus sign. Tutte’s brilliant insight, which was anticipated by Whitney in less symbolic form, was that the features can be combined by using two variables <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> and <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" />. Call an edge a <a href="https://en.wikipedia.org/wiki/Bridge_(graph_theory)">bridge</a> if it is not part of any cycle. If <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> is not a bridge, the recursion is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++T_G%28x%2Cy%29+%3D+T_%7BG+%5Csetminus+e%7D%28x%2Cy%29+%2B+T_%7BG%2Fe%7D%28x%2Cy%29.+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle    T_G(x,y) = T_{G \setminus e}(x,y) + T_{G/e}(x,y).   " class="latex" title="\displaystyle    T_G(x,y) = T_{G \setminus e}(x,y) + T_{G/e}(x,y).   " /></p>
<p>
The base case is now a graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> with some number <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" /> of bridges and some number <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell}" class="latex" title="{\ell}" /> of loops, which gives <img src="https://s0.wp.com/latex.php?latex=%7BT_G%28x%2Cy%29+%3D+x%5Ek+y%5E%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_G(x,y) = x^k y^\ell}" class="latex" title="{T_G(x,y) = x^k y^\ell}" />. An important feature is that all <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-vertex trees have the same Tutte polynomial <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7Bn-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x^{n-1}}" class="latex" title="{x^{n-1}}" />, since there are <img src="https://s0.wp.com/latex.php?latex=%7Bn-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n-1}" class="latex" title="{n-1}" /> edges and they are all bridges. The following are just some of the beautiful <a href="https://en.wikipedia.org/wiki/Tutte_polynomial#Specialisations">rules</a> that <img src="https://s0.wp.com/latex.php?latex=%7BT_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_G}" class="latex" title="{T_G}" /> follows. Let <img src="https://s0.wp.com/latex.php?latex=%7Bc+%3D+c_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c = c_G}" class="latex" title="{c = c_G}" /> stand for the number of connected components of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />.</p>
<ul>
<li>
<img src="https://s0.wp.com/latex.php?latex=%7BT_G%281%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_G(1,1)}" class="latex" title="{T_G(1,1)}" /> counts the number of spanning trees forests. This counts the number of spanning trees if <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> is connected.<p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7BT_G%281-x%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_G(1-x,0)}" class="latex" title="{T_G(1-x,0)}" />, when multiplied by <img src="https://s0.wp.com/latex.php?latex=%7B%28-1%29%5E%7Bn-c%7D+x%5Ec%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(-1)^{n-c} x^c}" class="latex" title="{(-1)^{n-c} x^c}" />, yields the chromatic polynomial.<p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7BT_G%281%2C2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_G(1,2)}" class="latex" title="{T_G(1,2)}" /> counts the number of spanning subgraphs.<p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7BT_G%282%2C2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_G(2,2)}" class="latex" title="{T_G(2,2)}" /> is just <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B%7CE%7C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^{|E|}}" class="latex" title="{2^{|E|}}" />.<p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7BT_G%28x%2C%5Cfrac%7B1%7D%7Bx%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_G(x,\frac{1}{x})}" class="latex" title="{T_G(x,\frac{1}{x})}" /> gives the <a href="https://en.wikipedia.org/wiki/Jones_polynomial">Jones polynomial</a> of a knot related to <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />.
</li></ul>
<p>
There are many further relations. The Jones polynomial has many applications including in quantum physics.</p>
<p></p><h2> Contraction With a Twist </h2><p></p>
<p>
Recall our definition of the “amplitude” <img src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a(G)}" class="latex" title="{a(G)}" /> of an undirected <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-vertex graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> from the “Net-Zero Graphs” <a href="https://rjlipton.wordpress.com/2019/06/10/net-zero-graphs/">post</a>:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++a%28G%29+%3D+%5Cfrac%7Bc_0+-+c_1%7D%7B2%5En%7D%2C+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle    a(G) = \frac{c_0 - c_1}{2^n},   " class="latex" title="\displaystyle    a(G) = \frac{c_0 - c_1}{2^n},   " /></p>
<p>
where <img src="https://s0.wp.com/latex.php?latex=%7Bc_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c_0}" class="latex" title="{c_0}" /> is the number of black-and-white 2-colorings that make an even number of edges have both nodes colored black, and <img src="https://s0.wp.com/latex.php?latex=%7Bc_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c_1}" class="latex" title="{c_1}" /> for an odd number. </p>
<p>
There does not seem to be a simple recursion for <img src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a(G)}" class="latex" title="{a(G)}" /> from <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Csetminus+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G \setminus e}" class="latex" title="{G \setminus e}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BG%2Fe%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G/e}" class="latex" title="{G/e}" />. We can, however, obtain one by using another kind of contraction that adds a loop at the combined vertex:</p>
<p><img src="https://rjlipton.files.wordpress.com/2019/06/inchworm_contraction.png?w=250&amp;h=128" alt="inchworm_contraction" width="250" class="aligncenter wp-image-16006" height="128" /></p>
<p>
We denote this by <img src="https://s0.wp.com/latex.php?latex=%7BG%2F%5C%21%2Fe%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G/\!/e}" class="latex" title="{G/\!/e}" />. We have not found a simple reference for this. We obtain the following recursive formula:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++a%28G%29+%3D+a%28G%5Cbackslash+e%29+%2B+%5Cfrac%7B1%7D%7B2%7Da%28G%2F%5C%21%2Fe%29+-+%5Cfrac%7B1%7D%7B2%7Da%28G%2Fe%29.+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle    a(G) = a(G\backslash e) + \frac{1}{2}a(G/\!/e) - \frac{1}{2}a(G/e).   " class="latex" title="\displaystyle    a(G) = a(G\backslash e) + \frac{1}{2}a(G/\!/e) - \frac{1}{2}a(G/e).   " /></p>
<p>
This recursion allows <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> to be a bridge, so the base cases are <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> for an isolated vertex and <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> for a loop. More generally, the basis is <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> for a node with an even number of loops, <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> for odd. Here is an example for the ‘star graph’ <img src="https://s0.wp.com/latex.php?latex=%7BS_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S_4}" class="latex" title="{S_4}" /> on 4 vertices:</p>
<p></p><p><br />
<img src="https://rjlipton.files.wordpress.com/2019/06/stargraph.png?w=520&amp;h=294" alt="starGraph" width="520" class="aligncenter wp-image-16007" height="294" /></p>
<p></p><p><br />
The diagram would need another layer to get down to (products of) base cases, which we have shortcut by putting values of <img src="https://s0.wp.com/latex.php?latex=%7Ba%28H%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a(H)}" class="latex" title="{a(H)}" /> for each graph <img src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{H}" class="latex" title="{H}" /> at a leaf. Adding the products over all branches gives <img src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a(G)}" class="latex" title="{a(G)}" />. For the star graph,</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++a%28S_4%29+%3D+%5Cfrac%7B1%7D%7B2%7D+%2B+%5Cfrac%7B1%7D%7B4%7D+-+%5Cfrac%7B1%7D%7B4%7D+%2B+%5Cfrac%7B1%7D%7B4%7D+%2B+%5Cfrac%7B1%7D%7B8%7D+-+%5Cfrac%7B1%7D%7B8%7D+-+%5Cfrac%7B1%7D%7B4%7D+-+%5Cfrac%7B1%7D%7B8%7D+%2B+%5Cfrac%7B1%7D%7B8%7D+%3D+%5Cfrac%7B1%7D%7B2%7D.+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle    a(S_4) = \frac{1}{2} + \frac{1}{4} - \frac{1}{4} + \frac{1}{4} + \frac{1}{8} - \frac{1}{8} - \frac{1}{4} - \frac{1}{8} + \frac{1}{8} = \frac{1}{2}.   " class="latex" title="\displaystyle    a(S_4) = \frac{1}{2} + \frac{1}{4} - \frac{1}{4} + \frac{1}{4} + \frac{1}{8} - \frac{1}{8} - \frac{1}{4} - \frac{1}{8} + \frac{1}{8} = \frac{1}{2}.   " /></p>
<p></p><p><br />
Clearly this brute-force recursion grows as <img src="https://s0.wp.com/latex.php?latex=%7B3%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3^n}" class="latex" title="{3^n}" />. This is slower than the order-<img src="https://s0.wp.com/latex.php?latex=%7B2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^n}" class="latex" title="{2^n}" /> time of using the coloring definition directly, but what all this underscores is how singular it is to be able to compute <img src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a(G)}" class="latex" title="{a(G)}" /> in polynomial time, indeed <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" /> time. The search for a more-efficient recursion, one that might apply to <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{NP}}" class="latex" title="{\mathsf{NP}}" />-hard quantities, leads us to consider a more-drastic operation on edges.</p>
<p></p><h2> Exploding Edges </h2><p></p>
<p>
The new recursion operation is well illustrated by this figure:</p>
<p></p><p><br />
<img src="https://rjlipton.files.wordpress.com/2019/06/explosionsolo.png?w=300&amp;h=203" alt="explosionSolo" width="300" class="aligncenter wp-image-16008" height="203" /></p>
<p></p><p><br />
Two vertices disappear, not just one. Not only does the edge <img src="https://s0.wp.com/latex.php?latex=%7Be+%3D+%28u%2Cv%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e = (u,v)}" class="latex" title="{e = (u,v)}" /> disappear, but any other edge incident to <img src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u}" class="latex" title="{u}" /> or <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v}" class="latex" title="{v}" /> from a vertex <img src="https://s0.wp.com/latex.php?latex=%7Bw+%5Cneq+u%2Cv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w \neq u,v}" class="latex" title="{w \neq u,v}" /> gets “recoiled” into a loop at <img src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w}" class="latex" title="{w}" />. We denote this operation by <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Cbackslash%5C%21%5Cbackslash+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G \backslash\!\backslash e}" class="latex" title="{G \backslash\!\backslash e}" /> to connote that <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" /> is not just deleted but “exploded.” </p>
<p>
Properly speaking, we need to specify what happens if there are other edges between <img src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u}" class="latex" title="{u}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v}" class="latex" title="{v}" /> or loops at <img src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u}" class="latex" title="{u}" /> or <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v}" class="latex" title="{v}" />. In an upcoming post we will see that those become <em>circles</em> in a <em>graphical polymatroid</em> which generalizes the notion of a graph. For now, however, it suffices to let <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> be the total number of vaporized edges, including <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" />. Then we obtain a two-term recursive formula:</p>
<p><a name="expl"></a></p><a name="expl">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++a%28G%29+%3D+a%28G+%5Cbackslash+e%29+%2B+%5Cfrac%7B%28-1%29%5Er%7D%7B2%7D+a%28G%5Cbackslash+%5C%21%5Cbackslash+e%29.+++%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle    a(G) = a(G \backslash e) + \frac{(-1)^r}{2} a(G\backslash \!\backslash e).   \ \ \ \ \ (1)" class="latex" title="\displaystyle    a(G) = a(G \backslash e) + \frac{(-1)^r}{2} a(G\backslash \!\backslash e).   \ \ \ \ \ (1)" /></p>
</a><p><a name="expl"></a></p>
<p>
The base cases for isolated vertices are the same as before, but explosion also needs a base case for pure emptiness. This contributes <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />. In the following example diagram, for the path graph <img src="https://s0.wp.com/latex.php?latex=%7BP_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P_4}" class="latex" title="{P_4}" /> on four nodes, we denote such base cases by `w’ for “wisp”:</p>
<p><img src="https://rjlipton.files.wordpress.com/2019/06/p3explosion.png?w=450&amp;h=291" alt="P3explosion" width="450" class="aligncenter wp-image-16009" height="291" /></p>
<p>
Note again the rule that when the recursion disconnects the graph, the component values multiply together. Thus the value is</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++a%28P_4%29+%3D+%281+-+%5Cfrac%7B1%7D%7B2%7D%29%281+-+%5Cfrac%7B1%7D%7B2%7D%29+-+0+%3D+%5Cfrac%7B1%7D%7B4%7D.+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle    a(P_4) = (1 - \frac{1}{2})(1 - \frac{1}{2}) - 0 = \frac{1}{4}.   " class="latex" title="\displaystyle    a(P_4) = (1 - \frac{1}{2})(1 - \frac{1}{2}) - 0 = \frac{1}{4}.   " /></p>
<p>
This is different from the amplitude <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac{1}{2}}" class="latex" title="{\frac{1}{2}}" /> of the star graph. What this means is that <img src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a(G)}" class="latex" title="{a(G)}" /> does not obey the rules of the Tutte polynomial, which is the same for both of these 4-vertex trees. </p>
<p>
To prove the recursion equation (<a href="https://rjlipton.wordpress.com/feed/#expl">1</a>), for <img src="https://s0.wp.com/latex.php?latex=%7Be+%3D+%28u%2Cv%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e = (u,v)}" class="latex" title="{e = (u,v)}" />, note that every coloring has the same odd/even parity of black-black edges for <img src="https://s0.wp.com/latex.php?latex=%7BG%5Csetminus+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G\setminus e}" class="latex" title="{G\setminus e}" /> as for <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> except those that color both <img src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u}" class="latex" title="{u}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v}" class="latex" title="{v}" /> black. Let <img src="https://s0.wp.com/latex.php?latex=%7Bc_0%5E%7Buv%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c_0^{uv}}" class="latex" title="{c_0^{uv}}" /> denote the colorings among the latter that make an even number of black-black edges (including <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" />) overall, <img src="https://s0.wp.com/latex.php?latex=%7Bc_1%5E%7Buv%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c_1^{uv}}" class="latex" title="{c_1^{uv}}" /> for an odd number. Then</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++a%28G%29+%3D+a%28G+%5Csetminus+e%29+%2B+%5Cfrac%7B2%7D%7B2%5En%7D%28c_1%5E%7Buv%7D+-+c_0%5E%7Buv%7D%29.+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle    a(G) = a(G \setminus e) + \frac{2}{2^n}(c_1^{uv} - c_0^{uv}).   " class="latex" title="\displaystyle    a(G) = a(G \setminus e) + \frac{2}{2^n}(c_1^{uv} - c_0^{uv}).   " /></p>
<p>
Now if there are no other edges between or loops at <img src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u}" class="latex" title="{u}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v}" class="latex" title="{v}" />, then <img src="https://s0.wp.com/latex.php?latex=%7Bc_1%5E%7Buv%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c_1^{uv}}" class="latex" title="{c_1^{uv}}" /> is the same as the number of colorings of <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Cbackslash%5C%21%5Cbackslash+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G \backslash\!\backslash e}" class="latex" title="{G \backslash\!\backslash e}" /> that make an even number of black-black edges, and <img src="https://s0.wp.com/latex.php?latex=%7Bc_0%5E%7Buv%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c_0^{uv}}" class="latex" title="{c_0^{uv}}" /> becomes the odd case in <img src="https://s0.wp.com/latex.php?latex=%7BG+%5Cbackslash%5C%21%5Cbackslash+e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G \backslash\!\backslash e}" class="latex" title="{G \backslash\!\backslash e}" /> again because we subtracted <img src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e}" class="latex" title="{e}" />. Considering the sign change from other <img src="https://s0.wp.com/latex.php?latex=%7B%28u%2Cv%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(u,v)}" class="latex" title="{(u,v)}" /> edges or loops and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B2%7D%7B2%5En%7D+%3D+%5Cfrac%7B1%2F2%7D%7B2%5E%7Bn-2%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac{2}{2^n} = \frac{1/2}{2^{n-2}}}" class="latex" title="{\frac{2}{2^n} = \frac{1/2}{2^{n-2}}}" /> yields equation (<a href="https://rjlipton.wordpress.com/feed/#expl">1</a>).  It is also possible to “explode” a loop, and our readers may enjoy figuring out how to define it.</p>
<p></p><h2> The Amplitude Polynomial </h2><p></p>
<p></p><p>
We can expand on this by defining a polynomial <img src="https://s0.wp.com/latex.php?latex=%7BQ_G%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q_G(x)}" class="latex" title="{Q_G(x)}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29+%3D+Q_G%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a(G) = Q_G(1)}" class="latex" title="{a(G) = Q_G(1)}" />. The base cases are <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> for an isolated vertex but still <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" /> for a “wisp” and <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> for a loop. The basis extends to give <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> for an isolated node with an even number of loops and <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> for odd. Another way to put it is that two edges with the same endpoints, or two loops at the same node, can be removed. The above diagram shows that for the path graph <img src="https://s0.wp.com/latex.php?latex=%7BP_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P_4}" class="latex" title="{P_4}" />,</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++++Q_%7BP_4%7D%28x%29+%3D+%28x%5E2+-+%5Cfrac%7B1%7D%7B2%7D%29%5E2+%3D+x%5E4+-+x%5E2+%2B+%5Cfrac%7B1%7D%7B4%7D.+++&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle    Q_{P_4}(x) = (x^2 - \frac{1}{2})^2 = x^4 - x^2 + \frac{1}{4}.   " class="latex" title="\displaystyle    Q_{P_4}(x) = (x^2 - \frac{1}{2})^2 = x^4 - x^2 + \frac{1}{4}.   " /></p>
<p>
Whereas, the recursion for the star graph—noting that the “star” <img src="https://s0.wp.com/latex.php?latex=%7BS_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S_2}" class="latex" title="{S_2}" /> on two nodes is just a single edge—gives:</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++++Q_%7BS_4%7D%28x%29+%26%3D%26+xQ_%7BS_3%7D%28x%29+-+%5Cfrac%7B1%7D%7B2%7D0%5C%5C+++%26%3D%26+x%5E2+Q_%7BS_2%7D%28x%29+-+%5Cfrac%7B1%7D%7B2%7D0+-+%5Cfrac%7B1%7D%7B2%7D0%5C%5C+++%26%3D%26+x%5E4+-+x%5E2+%5Cfrac%7B1%7D%7B2%7D%281%5C%21%5Ccdot%5C%21+1%29+-+%5Cfrac%7B1%7D%7B2%7D0+-+%5Cfrac%7B1%7D%7B2%7D0+%3D+x%5E4+-+%5Cfrac%7B1%7D%7B2%7Dx%5E2.+++%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \begin{array}{rcl}    Q_{S_4}(x) &amp;=&amp; xQ_{S_3}(x) - \frac{1}{2}0\\   &amp;=&amp; x^2 Q_{S_2}(x) - \frac{1}{2}0 - \frac{1}{2}0\\   &amp;=&amp; x^4 - x^2 \frac{1}{2}(1\!\cdot\! 1) - \frac{1}{2}0 - \frac{1}{2}0 = x^4 - \frac{1}{2}x^2.   \end{array} " class="latex" title="\displaystyle  \begin{array}{rcl}    Q_{S_4}(x) &amp;=&amp; xQ_{S_3}(x) - \frac{1}{2}0\\   &amp;=&amp; x^2 Q_{S_2}(x) - \frac{1}{2}0 - \frac{1}{2}0\\   &amp;=&amp; x^4 - x^2 \frac{1}{2}(1\!\cdot\! 1) - \frac{1}{2}0 - \frac{1}{2}0 = x^4 - \frac{1}{2}x^2.   \end{array} " /></p>
<p>
This is not the same polynomial as <img src="https://s0.wp.com/latex.php?latex=%7BQ_%7BP_4%7D%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q_{P_4}(x)}" class="latex" title="{Q_{P_4}(x)}" />, again implying that <img src="https://s0.wp.com/latex.php?latex=%7BQ_G%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q_G(x)}" class="latex" title="{Q_G(x)}" /> is not a specialization of the Tutte polynomial. We will show in the last post in this series that <img src="https://s0.wp.com/latex.php?latex=%7BQ_G%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q_G(x)}" class="latex" title="{Q_G(x)}" /> does specialize the polynomial <img src="https://s0.wp.com/latex.php?latex=%7BS_G%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S_G(x,y)}" class="latex" title="{S_G(x,y)}" /> introduced in this 1993 <a href="http://homepages.mcs.vuw.ac.nz/~whittle/pubs/Tutte_invariants_of_2-polymatroids.pdf">paper</a> titled, “A Characterization of Tutte Invariants of 2-Polymatroids” and covered further in this 2006 <a href="https://www.researchgate.net/publication/49399603_Evaluating_the_Rank_Generating_Function_of_a_Graphic_2-Polymatroid">paper</a>.</p>
<p></p><h2> Open Problems </h2><p></p>
<p>
What other rules does our “amplitude polynomial” follow?  We will explore this in the mentioned upcoming post.  What other quantities can it be made to count?</p>
<p>
What we called “explosion” is in fact attested as the natural form of <em>contraction</em> for the <b>polymatroids</b> considered in these papers. What further uses might “explosion” have in graph theory apart from polymatroids?</p></font></font></div>







<p class="date">
by Chaowen Guan and K.W. Regan <a href="https://rjlipton.wordpress.com/2019/06/17/contraction-and-explosion/"><span class="datestr">at June 17, 2019 08:27 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.06226">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.06226">Correspondence-Free Region Localization for Partial Shape Similarity via Hamiltonian Spectrum Alignment</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rampini:Arianna.html">Arianna Rampini</a>, Irene Tallini, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Ovsjanikov:Maks.html">Maks Ovsjanikov</a>, Alex M. Bronstein, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rodol=agrave=:Emanuele.html">Emanuele Rodolà</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.06226">PDF</a><br /><b>Abstract: </b>We consider the problem of localizing relevant subsets of non-rigid geometric
shapes given only a partial 3D query as the input. Such problems arise in
several challenging tasks in 3D vision and graphics, including partial shape
similarity, retrieval, and non-rigid correspondence. We phrase the problem as
one of alignment between short sequences of eigenvalues of basic differential
operators, which are constructed upon a scalar function defined on the 3D
surfaces. Our method therefore seeks for a scalar function that entails this
alignment. Differently from existing approaches, we do not require solving for
a correspondence between the query and the target, therefore greatly
simplifying the optimization process; our core technique is also
descriptor-free, as it is driven by the geometry of the two objects as encoded
in their operator spectra. We further show that our spectral alignment
algorithm provides a remarkably simple alternative to the recent
shape-from-spectrum reconstruction approaches. For both applications, we
demonstrate improvement over the state-of-the-art either in terms of accuracy
or computational cost.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.06226"><span class="datestr">at June 17, 2019 01:33 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.06208">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.06208">Drawing Order Diagrams Through Two-Dimension Extension</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/D=uuml=rrschnabel:Dominik.html">Dominik Dürrschnabel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hanika:Tom.html">Tom Hanika</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stumme:Gerd.html">Gerd Stumme</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.06208">PDF</a><br /><b>Abstract: </b>Order diagrams are an important tool to visualize the complex structure of
ordered sets. Favorable drawings of order diagrams, i.e., easily readable for
humans, are hard to come by, even for small ordered sets. Many attempts were
made to transfer classical graph drawing approaches to order diagrams. Although
these methods produce satisfying results for some ordered sets, they
unfortunately perform poorly in general. In this work we present the novel
algorithm DimDraw to draw order diagrams. This algorithm is based on a relation
between the dimension of an ordered set and the bipartiteness of a
corresponding graph.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.06208"><span class="datestr">at June 17, 2019 01:22 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.06154">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.06154">Soft Subdivision Motion Planning for Complex Planar Robots</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Bo.html">Bo Zhou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chiang:Yi=Jen.html">Yi-Jen Chiang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yap:Chee.html">Chee Yap</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.06154">PDF</a><br /><b>Abstract: </b>The design and implementation of theoretically-sound robot motion planning
algorithms is challenging. Within the framework of resolution-exact algorithms,
it is possible to exploit soft predicates for collision detection. The design
of soft predicates is a balancing act between easily implementable predicates
and their accuracy/effectivity.
</p>
<p>In this paper, we focus on the class of planar polygonal rigid robots with
arbitrarily complex geometry. We exploit the remarkable decomposability
property of soft collision-detection predicates of such robots. We introduce a
general technique to produce such a decomposition. If the robot is an m-gon,
the complexity of this approach scales linearly in m. This contrasts with the
O(m^3) complexity known for exact planners. It follows that we can now
routinely produce soft predicates for any rigid polygonal robot. This results
in resolution-exact planners for such robots within the general Soft
Subdivision Search (SSS) framework. This is a significant advancement in the
theory of sound and complete planners for planar robots.
</p>
<p>We implemented such decomposed predicates in our open-source Core Library.
The experiments show that our algorithms are effective, perform in real time on
non-trivial environments, and can outperform many sampling-based methods.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.06154"><span class="datestr">at June 17, 2019 01:28 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.06122">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.06122">Topological Data Analysis with $\epsilon$-net Induced Lazy Witness Complex</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Arafat:Naheed_Anjum.html">Naheed Anjum Arafat</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Basu:Debabrota.html">Debabrota Basu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bressan:St=eacute=phane.html">Stéphane Bressan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.06122">PDF</a><br /><b>Abstract: </b>Topological data analysis computes and analyses topological features of the
point clouds by constructing and studying a simplicial representation of the
underlying topological structure. The enthusiasm that followed the initial
successes of topological data analysis was curbed by the computational cost of
constructing such simplicial representations. The lazy witness complex is a
computationally feasible approximation of the underlying topological structure
of a point cloud. It is built in reference to a subset of points, called
landmarks, rather than considering all the points as in the \v{C}ech and
Vietoris-Rips complexes. The choice and the number of landmarks dictate the
effectiveness and efficiency of the approximation. We adopt the notion of
$\epsilon$-cover to define $\epsilon$-net. We prove that $\epsilon$-net, as a
choice of landmarks, is an $\epsilon$-approximate representation of the point
cloud and the induced lazy witness complex is a $3$-approximation of the
induced Vietoris-Rips complex. Furthermore, we propose three algorithms to
construct $\epsilon$-net landmarks. We establish the relationship of these
algorithms with the existing landmark selection algorithms. We empirically
validate our theoretical claims. We empirically and comparatively evaluate the
effectiveness, efficiency, and stability of the proposed algorithms on
synthetic and real datasets.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.06122"><span class="datestr">at June 17, 2019 01:24 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.06015">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.06015">Dynamic Path-Decomposed Tries</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kanda:Shunsuke.html">Shunsuke Kanda</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/K=ouml=ppl:Dominik.html">Dominik Köppl</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tabei:Yasuo.html">Yasuo Tabei</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Morita:Kazuhiro.html">Kazuhiro Morita</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fuketa:Masao.html">Masao Fuketa</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.06015">PDF</a><br /><b>Abstract: </b>A keyword dictionary is an associative array whose keys are strings. Recent
applications handling massive keyword dictionaries in main memory have a need
for a space-efficient implementation. When limited to static applications,
there are a number of highly-compressed keyword dictionaries based on the
advancements of practical succinct data structures. However, as most succinct
data structures are only efficient in the static case, it is still difficult to
implement a keyword dictionary that is space efficient and dynamic. In this
article, we propose such a keyword dictionary. Our main idea is to embrace the
path decomposition technique, which was proposed for constructing
cache-friendly tries. To store the path-decomposed trie in small memory, we
design data structures based on recent compact hash trie representations.
Exhaustive experiments on real-world datasets reveal that our dynamic keyword
dictionary needs up to 68% less space than the existing smallest ones.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.06015"><span class="datestr">at June 17, 2019 01:21 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.06014">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.06014">Quantitative Comparison of Time-Dependent Treemaps</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Eduardo Vernier, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sondag:Max.html">Max Sondag</a>, Joao Comba, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Speckmann:Bettina.html">Bettina Speckmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Telea:Alexandru.html">Alexandru Telea</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Verbeek:Kevin.html">Kevin Verbeek</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.06014">PDF</a><br /><b>Abstract: </b>Rectangular treemaps are often the method of choice for visualizing large
hierarchical datasets. Nowadays such datasets are available over time, and
hence there is a need for (a) treemapping algorithms that can handle
time-dependent data, and (b) corresponding quality criteria and evaluations. In
recent years a wide variety of treemapping algorithms and corresponding quality
criteria have been proposed, each with their own advantages and limitations,
which are often hard to judge and compare. We aim to provide insights to allow
researchers and practitioners to make an informed choice when selecting a
treemapping algorithm for their specific application and their data. For this,
we perform an extensive quantitative evaluation of rectangular treemapping
algorithms for time-dependent data. As part of this evaluation we propose a
novel classification scheme for time-dependent tree datasets. We approximate
the infinite problem space of all such datasets by a low-dimensional feature
space spanned by measurable characteristics (features) of the problem
instances. We propose four such features to characterize time-dependent
hierarchical datasets and classify all datasets used in our experiments
accordingly. Moreover, we introduce a new method to measure the stability of
time-dependent treemaps which explicitly incorporates the change in the input
data. We analyze and visually summarize the results with respect to both visual
quality and stability. All datasets, metrics, and algorithms are openly
available to facilitate reuse and further comparative studies.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.06014"><span class="datestr">at June 17, 2019 01:33 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.05998">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.05998">Non-zero-sum Stackelberg Budget Allocation Game for Computational Advertising</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hatano:Daisuke.html">Daisuke Hatano</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuroki:Yuko.html">Yuko Kuroki</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kawase:Yasushi.html">Yasushi Kawase</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sumita:Hanna.html">Hanna Sumita</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kakimura:Naonori.html">Naonori Kakimura</a>, Ken-Ichi Kawarabayashi <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.05998">PDF</a><br /><b>Abstract: </b>Computational advertising has been studied to design efficient marketing
strategies that maximize the number of acquired customers. In an increased
competitive market, however, a market leader (a leader) requires the
acquisition of new customers as well as the retention of her loyal customers
because there often exists a competitor (a follower) who tries to attract
customers away from the market leader. In this paper, we formalize a new model
called the Stackelberg budget allocation game with a bipartite influence model
by extending a budget allocation problem over a bipartite graph to a
Stackelberg game. To find a strong Stackelberg equilibrium, a standard solution
concept of the Stackelberg game, we propose two algorithms: an approximation
algorithm with provable guarantees and an efficient heuristic algorithm. In
addition, for a special case where customers are disjoint, we propose an exact
algorithm based on linear programming. Our experiments using real-world
datasets demonstrate that our algorithms outperform a baseline algorithm even
when the follower is a powerful competitor.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.05998"><span class="datestr">at June 17, 2019 01:20 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.05996">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.05996">Multi-level tree based approach for interactive graph visualization with semantic zoom</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Luca:Felice_De.html">Felice De Luca</a>, Iqbal Hossain, Stephen Kobourov, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/B=ouml=rner:Katy.html">Katy Börner</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.05996">PDF</a><br /><b>Abstract: </b>A recent data visualization literacy study shows that most people cannot read
networks that use hierarchical cluster representations such as "super-noding"
and "edge bundling." Other studies that compare standard node-link
representations with map-like visualizations show that map-like visualizations
are superior in terms of task performance, memorization and engagement. With
this in mind, we propose the Zoomable Multilevel Tree (ZMLT) algorithm for
map-like visualization of large graphs that is representative, real,
persistent, overlap-free labeled, planar, and compact. These six desirable
properties are formalized with the following guarantees: (1) The abstract and
embedded trees represent the underlying graph appropriately at different level
of details (in terms of the structure of the graph as well as the embedding
thereof); (2) At every level of detail we show real vertices and real paths
from the underlying graph; (3) If any node or edge appears in a given level,
then they also appear in all deeper levels; (4) All nodes at the current level
and higher levels are labeled and there are no label overlaps; (5) There are no
crossings on any level; (6) The drawing area is proportional to the total area
of the labels. This algorithm is implemented and we have a functional prototype
for the interactive interface in a web browser.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.05996"><span class="datestr">at June 17, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.05921">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.05921">Symmetric Algorithmic Components for Shape Analysis with Diffeomorphisms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>N. Guigui, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jia:Shuman.html">Shuman Jia</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sermesant:Maxime.html">Maxime Sermesant</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pennec:Xavier.html">Xavier Pennec</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.05921">PDF</a><br /><b>Abstract: </b>In computational anatomy, the statistical analysis of temporal deformations
and inter-subject variability relies on shape registration. However, the
numerical integration and optimization required in diffeomorphic registration
often lead to important numerical errors. In many cases, it is well known that
the error can be drastically reduced in the presence of a symmetry. In this
work, the leading idea is to approximate the space of deformations and images
with a possibly non-metric symmetric space structure using an involution, with
the aim to perform parallel transport. Through basic properties of symmetries,
we investigate how the implementations of a midpoint and the involution compare
with the ones of the Riemannian exponential and logarithm on diffeomorphisms
and propose a modification of these maps using registration errors. This leads
us to identify transvections, the composition of two symmetries, as a mean to
measure how far from symmetric the underlying structure is. We test our method
on a set of 138 cardiac shapes and demonstrate improved numerical consistency
in the Pole Ladder scheme.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.05921"><span class="datestr">at June 17, 2019 01:26 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://offconvex.github.io/2019/06/16/modeconnectivity/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/convex.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://offconvex.github.io/2019/06/16/modeconnectivity/">Landscape Connectivity of Low Cost Solutions for Multilayer Nets</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>A big mystery about deep learning is how, in a highly nonconvex loss landscape, gradient descent often finds near-optimal solutions —those with training cost almost zero— even starting from a random initialization. This conjures an image of a landscape filled with deep pits.  Gradient descent started at a random point falls easily to the bottom of the nearest pit. In this mental image the pits are disconnected from each other, so there is no way to go from the bottom of one pit to bottom of another without going through regions of high cost.</p>

<p>The current post is about our <a href="https://arxiv.org/abs/1906.06247">new paper with Rohith Kuditipudi, Xiang Wang, Holden Lee, Yi Zhang, Wei Hu, Zhiyuan Li and Sanjeev Arora</a> which provides a mathematical explanation of  the following surprising phenomenon reported last year.</p>

<blockquote>
  <p><strong>Mode connectivity</strong> (<a href="https://papers.nips.cc/paper/8095-loss-surfaces-mode-connectivity-and-fast-ensembling-of-dnns.pdf">Garipov et al. 2018</a>, <a href="https://arxiv.org/abs/1803.00885">Draxler et al. 2018</a>) All pairs of low-cost solutions found via gradient descent  can actually be connected by simple paths in the parameter space, such that every point on the path is another solution of almost the same cost. In fact the low-cost path connecting two near-optima  can be <em>piecewise linear</em> with two line-segments, or a Bezier curve.</p>
</blockquote>

<p>See Figure 1 below from <a href="https://papers.nips.cc/paper/8095-loss-surfaces-mode-connectivity-and-fast-ensembling-of-dnns.pdf">Garipov et al. 2018</a>) for an illustration. Solutions A and B have low cost but the line connecting them goes through solutions with high cost. But we can find C of low cost such that paths AC and CB only pass through low-cost region.</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/modes.PNG" style="width: 300px;" />
<br />
<b>Figure 1</b> Mode Connectivity. Warm colors represent low loss. 
</div>

<p>Using a very simple example let us see that this phenomenon is highly counterintuitive. Suppose we’re talking about 2-layer nets with linear activations and a real-valued output. Let the two nets $\theta_A$ and 
$\theta_B$  with zero loss be
<br />
respectively where $x, U_1, U_2 \in \Re^n$
and matrices $W_1, W_2$ are $n\times n$. Then the straight line connecting them in parameter space corresponds to nets of the type $(\alpha U_1 + (1-\alpha)U_2)^\top(\alpha W_1 + (1-\alpha)W_2)$ which can be rewritten as</p>
<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/hybridnet.jpg" style="width: 650px;" /> 
</div>

<p>Note that the middle terms correspond to putting the top layer of one net on top of the bottom of the other, which in general is a nonsensical net (reminiscent of a <em>centaur</em>, a mythical half-man half-beast) that in general would be expected to have high loss.</p>

<p>Originally we figured mode connectivity would not be mathematically understood for a long time, because of the seeming difficulty of proving any mathematical theorems  about, say, $50$-layer nets trained on ImageNet data, and in particular dealing with such “centaur-like” nets in the interpolation.</p>

<p>Several authors (<a href="https://arxiv.org/abs/1611.01540">Freeman and Bruna, 2016</a>, <a href="https://arxiv.org/abs/1802.06384">Venturi et al. 2018</a>, <a href="https://arxiv.org/abs/1803.00909">Liang et al. 2018</a>, <a href="https://arxiv.org/abs/1809.10749">Nguyen et al. 2018</a>, <a href="https://arxiv.org/abs/1901.07417">Nguyen et al. 2019</a>) did try to explain the phenomenon of mode connectivity in simple two-layer settings (the first of these appeared even before the discovery of this phenomenon for deep neural networks). But these explanations only work for very unrealistic 2-layer nets (or multi-layer nets with special structure) which are highly redundant e.g., the number of neurons may have to be larger than the number of training samples.</p>

<p>Our paper starts by clarifying an important point: redundancy with respect to a ground truth neural network is  insufficient for mode connectivity, which we show via a simple counterexample sketched below.</p>

<p>Thus to explain mode connectivity for multilayer nets we  will need to leverage some stronger property of <em>typical</em> solutions discovered via gradient-based training, as we will see below.</p>

<h2 id="mode-connectivity-need-not-hold-for-2-layer-overparametrized-nets">Mode Connectivity need not hold for 2-layer overparametrized nets</h2>

<p>We show that the strongest version of mode connectivity (every two minimizers are connected) does not hold even for a simple two-layer setting, where $f(x) = W_2\sigma(W_1x)$, even where the net is vastly overparametrized than it needs to be for the dataset in question.</p>

<blockquote>
  <p><strong>Theorem</strong> For any $h&gt;1$ there exists a data set which is perfectly fitted by  a ground truth neural network with $2$ layers and only $2$ hidden neurons, but if we desire to train neural network with $h$ hidden units on this dataset then the set of global minimizers are not connected.</p>
</blockquote>

<h2 id="stability-properties-of-typical-nets">Stability properties of typical nets</h2>

<p>Since mode connectivity has been found to hold for a range of architectures and datasets, any explanation probably should only rely upon properties that <em>generically</em> seem to hold for deep net standard training. Our explanation relies upon properties that were discovered in recent years in the effort to understand the generalization properties of deep nets.  These properties say that the output of the final net is stable to various kinds of added noise.
The properties imply that the loss function does not change much when the net parameters are perturbed; this is informally described as the net being a <em>flat minimum</em> (<a href="https://www.cs.toronto.edu/~hinton/absps/colt93.html">Hinton and Van Camp 1993</a>).</p>

<p>Our explanation of mode connectivity will involve the following two properties.</p>

<h3 id="noise-stability-and-dropout-stability">Noise stability and Dropout Stability</h3>

<p><em>Dropout</em> was introduced by <a href="https://arxiv.org/abs/1207.0580">Hinton et al. 2012</a>: during gradient-based training, one  zeroes out the output of $50\%$ of the nodes, and doubles the output of the remaining nodes. The gradient used in the next update is computed for this net. While dropout may not be as popular these days, it can be added to any existing net training without loss of generality. We’ll say a net is “$\epsilon$-dropout stable” if applying dropout to $50\%$ of the nodes increases its loss by at most $\epsilon$. Note that unlike dropout training where nodes are <em>randomly</em> dropped out, in our definition a network is dropout stable as long as there <em>exists</em> a way of dropping out $50\%$ of the nodes that does not increase its loss by too much.</p>

<blockquote>
  <p><strong>Theorem 1:</strong> If two trained multilayer ReLU nets with the same architecture  are $\epsilon$-dropout stable, then they can be connected in the loss landscape via a piece-wise linear path in which the number of linear segments is linear in the number of layers, and the loss of every point on the path is at most $\epsilon$ higher than the loss of the two end points.</p>
</blockquote>

<p><em>Noise stability</em> was discovered by <a href="https://arxiv.org/abs/1802.05296">Arora et al. ICML18</a>; this   was described in a <a href="http://www.offconvex.org/2018/02/17/generalization2/">previous blog post</a>. They found that trained nets are very stable to noise injection: if one adds a fairly large Gaussian noise vector to the output of a layer, then this has only a small effect on the output of higher layers. In other words, the network <em>rejects</em> the injected noise. That paper showed that noise stability can be used to prove that the net is compressible. Thus noise stability is indeed a form of redundancy in the net.</p>

<p>In the new paper we show that a minor variant of the noise stability property (which we empirically find to still hold in trained nets) implies dropout stability. More importantly, solutions satisfying this property can be connected using a piecewise linear path with at most $10$ segments.</p>

<blockquote>
  <p><strong>Theorem 2:</strong> If two trained multilayer ReLU nets with the same architecture are $\epsilon$-noise stable, then they can be connected in the loss landscape via a piece-wise linear path with at most 10 segments, and the loss of every point on the path is at most $\epsilon$ higher than the loss of the two end points.</p>
</blockquote>

<h2 id="proving-mode-connectivity-for-dropout-stable-nets">Proving mode connectivity for dropout-stable nets</h2>
<p>We exhibit the main ideas by proving mode connectivity for  fully connected nets that are dropout-stable, meaning training loss is stable to dropping out $50\%$ of the nodes.</p>

<p>Let $W_1,W_2,…,W_p$ be the weight matrices of the neural network, so the function that is computed by the network is $f(x) = W_p\sigma(\cdots \sigma(W_2(\sigma(W_1x)))\cdots)$. Here $\sigma$ is the ReLU activation (our result in this section works for any activations). We use $\theta = (W_1,W_2,…,W_p)\in \Theta$ to denote the parameters for the neural network. Given a set of data points $(x_i,y_i)~i=1,2,…,n$, the empirical loss $L$ is just an average of the losses for the individual samples $L(\theta) = \frac{1}{n}\sum_{i=1}^n l(y_i, f_\theta(x_i))$. The function $l(y, \hat{y})$ is a loss function that is convex in the second parameter (popular loss functions such as cross-entropy or mean-squared-error are all in this category).</p>

<p>Using this notation, Theorem 1 can be restated as:</p>

<blockquote>
  <p><strong>Theorem 1 (restated)</strong> Let $\theta_A$ and $\theta_B$ be two solutions that are both $\epsilon$-dropout stable, then there exists a path $\pi:[0,1]\to \Theta$ such that $\pi(0) = \theta_A$, $\pi(1) = \theta_B$ and for any $t\in(0,1)$ the loss $L(\pi(t)) \le \max{L(\theta_A), L(\theta_B)} + \epsilon$.</p>
</blockquote>

<p>To prove this theorem, the major step is to connect a network with its dropout version where half of the neurons are not used (see next part). Then intuitively it is not too difficult to connect two dropout versions as they both have a large number of inactive neurons.</p>

<p>As we discussed before, directly interpolating between two networks may not work as it give rise to <em>centaur-like</em> networks.  A key idea in this simpler theorem is that each linear segment in the path involves varying the parameters of only one layer, which allows careful control of this issue. (Proof of Theorem 2 is more complicated because the number of layers in the net are allowed to exceed the number of path segments.)</p>

<p>As a simple example, we show how to connect a 3-layer neural network with its dropout version. (The same idea can be easily extended to more layers by a simple induction on number of layers.) Assume without loss of generality that we are going to dropout the second half of neurons for both hidden layers. For the weight matrices $W_3, W_2, W_1$, we will write them in block form: $W_3$ is a $1\times 2$ block matrix $W_3 = [L_3, R_3]$, $W_2$ is a $2\times 2$ block matrix $W_2 = \left[L_2, C_2; D_2, R_2 \right]$, and $W_1$ is a $2\times 1$ block matrix $W_1 = \left[L_1; B_1\right]$ (here ; represents the end of a row). The dropout stable property implies that the networks with weights $(W_3, W_2, W_1)$, $(2[L_3, 0], W_2, W_1)$, $([2L_3, 0], [2L_2, 0; 0, 0], W_1)$ all have low loss (these weights correspond to the cases of no dropout, dropout only applied to the top hidden layer and dropout applied to both hidden layers). Note that the final set of weights $([2L_3, 0], [2L_2, 0; 0, 0], W_1)$ is equivalent to $([2L_3, 0], [2L_2, 0; 0, 0], [L_1; 0])$ as the output from the $B_1$ part of $W_1$ has no connections. The path we construct is illustrated in Figure 2 below:
As a simple example, we show how to connect a 3-layer neural network with its dropout version. (The same idea can be easily extended to more layers by a simple induction on number of layers.) Assume without loss of generality that we are going to dropout the second half of neurons for both hidden layers. For the weight matrices $W_3, W_2, W_1$, we will write them in block form: $W_3$ is a $1\times 2$ block matrix $W_3 = [L_3, R_3]$, $W_2$ is a $2\times 2$ block matrix $W_2 = \left[L_2, C_2; D_2, R_2 \right]$, and $W_1$ is a $2\times 1$ block matrix $W_1 = \left[L_1; B_1\right]$ (here ; represents the end of a row). The dropout stable property implies that the networks with weights $(W_3, W_2, W_1)$, $(2[L_3, 0], W_2, W_1)$, $([2L_3, 0], [2L_2, 0; 0, 0], W_1)$ all have low loss (these weights correspond to the cases of no dropout, dropout only applied to the top hidden layer and dropout applied to both hidden layers). Note that the final set of weights $([2L_3, 0], [2L_2, 0; 0, 0], W_1)$ is equivalent to $([2L_3, 0], [2L_2, 0; 0, 0], [L_1; 0])$ as the output from the $B_1$ part of $W_1$ has no connections. The path we construct is illustrated in Figure 2 below:</p>

<div style="text-align: center;">
<img src="http://www.offconvex.org/assets/path.png" style="width: 400px;" /> <br />
<b>Figure 2</b> Path from a 3-layer neural network to its dropout version.
</div>

<p>We use two types of steps to construct the path: (a) Since the loss function is convex in the weight of the top layer, we can interpolate between two different networks that only differ in top layer weights; (b) if a set of neurons already has 0 output weights, then we can set its input weights arbitrarily.</p>

<p>Figure 2 shows how to alternate between these two types of steps to connect a 3-layer network to its dropout version. The red color highlights weights that have changed. In the case of type (a) steps, the red color only appears in the top layer weights; in the case of type (b) steps, the 0 matrices highlighted by the green color are the 0 output weights, where because of these 0 matrices setting the red blocks to any matrix will not change the output of the neural network.</p>

<p>The crux of this construction appears in steps (3) and (4). When we are going from (2) to (3), we changed the bottom rows of $W_2$ from $[D_2, R_2]$ to $[2L_2, 0]$. This is a type (b) step, and because currently the top-level weight is $[2L_3, 0]$, changing the bottom row of $W_2$ has no effect on the output of the neural network. However, making this change allows us to do the interpolation between (3) and (4), as now the two networks only differ in the top layer weights. The loss is bounded because the weights in (3) are equivalent to $(2[L_3, 0], W_2, W_1)$ (weights with dropout applied to top hidden layer), and the weights in (4) are equivalent to $([2L_3, 0], [2L_2, 0; 0, 0], W_1)$ (weights with dropout applied to both hidden layers). The same procedure can be repeated if the network has more layers.</p>

<p>The number of line segments in the path is linear in the number of layers. As mentioned, the paper also gives stronger results assuming noise stability, where we can actually consruct a path with constant number of line segments.</p>

<h2 id="conclusions">Conclusions</h2>

<p>Our results are a first-cut explanation for how mode connectivity can arise in realistic deep nets. Our methods do not answer all mysteries about mode connectivity. In particular, in many cases (especially when the number of parameters is not as large) the solutions found in practice are not as robust as we require in our theorems (either in terms of dropout stability or noise stability), yet empirically it is still possible to find simple paths connecting the solutions. Are there other properties satisfied by these solutions that allow them to be connected? Also, our results can be extended to convolutional neural networks via <em>channel-wise dropout</em>, where one randomly turn off half of the channels (this was considered before in <a href="https://arxiv.org/abs/1411.4280">Thompson et al. 2015</a>,<a href="https://arxiv.org/abs/1812.03965">Keshari et al.2018</a>). While it is possible to train networks that are robust to channel-wise dropout, standard networks or even the ones trained with standard dropout do not satisfy this property.</p>

<p>It would also be interesting to utilize the insights into the landscape given by our explanation to design better training algorithms.</p></div>







<p class="date">
<a href="http://offconvex.github.io/2019/06/16/modeconnectivity/"><span class="datestr">at June 16, 2019 10:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/088">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/088">TR19-088 |  On the Complexity of Estimating the Effective Support Size | 

	Oded Goldreich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Loosely speaking, the effective support size of a distribution is the size of the support of a distribution that is close to it (in totally variation distance). 
We study the complexity of estimating the effective support size of an unknown distribution when given samples of the distributions as well as an evaluation oracle (which returns the probability that the queried element appears in the distribution).
In this context, we present several algorithms that exhibit a trade-off between the quality of the approximation and the complexity of obtaining it, and leave open the question of their optimality.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/088"><span class="datestr">at June 16, 2019 03:33 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.05832">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.05832">The Communication Complexity of Optimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vempala:Santosh_S=.html">Santosh S. Vempala</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Ruosong.html">Ruosong Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woodruff:David_P=.html">David P. Woodruff</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.05832">PDF</a><br /><b>Abstract: </b>We consider the communication complexity of a number of distributed
optimization problems. We start with the problem of solving a linear system.
Suppose there is a coordinator together with $s$ servers $P_1, \ldots, P_s$,
the $i$-th of which holds a subset $A^{(i)} x = b^{(i)}$ of $n_i$ constraints
of a linear system in $d$ variables, and the coordinator would like to output
$x \in \mathbb{R}^d$ for which $A^{(i)} x = b^{(i)}$ for $i = 1, \ldots, s$. We
assume each coefficient of each constraint is specified using $L$ bits. We
first resolve the randomized and deterministic communication complexity in the
point-to-point model of communication, showing it is $\tilde{\Theta}(d^2L +
sd)$ and $\tilde{\Theta}(sd^2L)$, respectively. We obtain similar results for
the blackboard model.
</p>
<p>When there is no solution to the linear system, a natural alternative is to
find the solution minimizing the $\ell_p$ loss. While this problem has been
studied, we give improved upper or lower bounds for every value of $p \ge 1$.
One takeaway message is that sampling and sketching techniques, which are
commonly used in earlier work on distributed optimization, are neither optimal
in the dependence on $d$ nor on the dependence on the approximation $\epsilon$,
thus motivating new techniques from optimization to solve these problems.
</p>
<p>Towards this end, we consider the communication complexity of optimization
tasks which generalize linear systems. For linear programming, we first resolve
the communication complexity when $d$ is constant, showing it is
$\tilde{\Theta}(sL)$ in the point-to-point model. For general $d$ and in the
point-to-point model, we show an $\tilde{O}(sd^3 L)$ upper bound and an
$\tilde{\Omega}(d^2 L + sd)$ lower bound. We also show if one perturbs the
coefficients randomly by numbers as small as $2^{-\Theta(L)}$, then the upper
bound is $\tilde{O}(sd^2 L) + \textrm{poly}(dL)$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.05832"><span class="datestr">at June 16, 2019 11:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.05815">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.05815">Lower Bounds for Adversarially Robust PAC Learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Diochnos:Dimitrios_I=.html">Dimitrios I. Diochnos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mahloujifar:Saeed.html">Saeed Mahloujifar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mahmoody:Mohammad.html">Mohammad Mahmoody</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.05815">PDF</a><br /><b>Abstract: </b>In this work, we initiate a formal study of probably approximately correct
(PAC) learning under evasion attacks, where the adversary's goal is to
\emph{misclassify} the adversarially perturbed sample point $\widetilde{x}$,
i.e., $h(\widetilde{x})\neq c(\widetilde{x})$, where $c$ is the ground truth
concept and $h$ is the learned hypothesis. Previous work on PAC learning of
adversarial examples have all modeled adversarial examples as corrupted inputs
in which the goal of the adversary is to achieve $h(\widetilde{x}) \neq c(x)$,
where $x$ is the original untampered instance. These two definitions of
adversarial risk coincide for many natural distributions, such as images, but
are incomparable in general.
</p>
<p>We first prove that for many theoretically natural input spaces of high
dimension $n$ (e.g., isotropic Gaussian in dimension $n$ under $\ell_2$
perturbations), if the adversary is allowed to apply up to a sublinear
$o(||x||)$ amount of perturbations on the test instances, PAC learning requires
sample complexity that is exponential in $n$. This is in contrast with results
proved using the corrupted-input framework, in which the sample complexity of
robust learning is only polynomially more.
</p>
<p>We then formalize hybrid attacks in which the evasion attack is preceded by a
poisoning attack. This is perhaps reminiscent of "trapdoor attacks" in which a
poisoning phase is involved as well, but the evasion phase here uses the
error-region definition of risk that aims at misclassifying the perturbed
instances. In this case, we show PAC learning is sometimes impossible all
together, even when it is possible without the attack (e.g., due to the bounded
VC dimension).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.05815"><span class="datestr">at June 16, 2019 11:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.05736">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.05736">Querying a Matrix through Matrix-Vector Products</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Xiaoming.html">Xiaoming Sun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woodruff:David_P=.html">David P. Woodruff</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yang:Guang.html">Guang Yang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Jialin.html">Jialin Zhang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.05736">PDF</a><br /><b>Abstract: </b>We consider algorithms with access to an unknown matrix $M\in\mathbb{F}^{n
\times d}$ via matrix-vector products, namely, the algorithm chooses vectors
$\mathbf{v}^1, \ldots, \mathbf{v}^q$, and observes $M\mathbf{v}^1,\ldots,
M\mathbf{v}^q$. Here the $\mathbf{v}^i$ can be randomized as well as chosen
adaptively as a function of $ M\mathbf{v}^1,\ldots,M\mathbf{v}^{i-1}$.
Motivated by applications of sketching in distributed computation, linear
algebra, and streaming models, as well as connections to areas such as
communication complexity and property testing, we initiate the study of the
number $q$ of queries needed to solve various fundamental problems. We study
problems in three broad categories, including linear algebra, statistics
problems, and graph problems. For example, we consider the number of queries
required to approximate the rank, trace, maximum eigenvalue, and norms of a
matrix $M$; to compute the AND/OR/Parity of each column or row of $M$, to
decide whether there are identical columns or rows in $M$ or whether $M$ is
symmetric, diagonal, or unitary; or to compute whether a graph defined by $M$
is connected or triangle-free. We also show separations for algorithms that are
allowed to obtain matrix-vector products only by querying vectors on the right,
versus algorithms that can query vectors on both the left and the right. We
also show separations depending on the underlying field the matrix-vector
product occurs in. For graph problems, we show separations depending on the
form of the matrix (bipartite adjacency versus signed edge-vertex incidence
matrix) to represent the graph.
</p>
<p>Surprisingly, this fundamental model does not appear to have been studied on
its own, and we believe a thorough investigation of problems in this model
would be beneficial to a number of different application areas.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.05736"><span class="datestr">at June 16, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.05669">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.05669">Post-Processing of High-Dimensional Data</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Matthies:Hermann_G=.html">Hermann G. Matthies</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Espig:Mike.html">Mike Espig</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Litvinenko:Alexander.html">Alexander Litvinenko</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hackbusch:Wolfgang.html">Wolfgang Hackbusch</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zander:Elmar.html">Elmar Zander</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.05669">PDF</a><br /><b>Abstract: </b>Scientific computations or measurements may result in huge volumes of data.
Often these can be thought of representing a real-valued function on a
high-dimensional domain, and can be conceptually arranged in the format of a
tensor of high degree in some truncated or lossy compressed format. We look at
some common post-processing tasks which are not obvious in the compressed
format, as such huge data sets can not be stored in their entirety, and the
value of an element is not readily accessible through simple look-up. The tasks
we consider are finding the location of maximum or minimum, or minimum and
maximum of a function of the data, or finding the indices of all elements in
some interval --- i.e. level sets, the number of elements with a value in such
a level set, the probability of an element being in a particular level set, and
the mean and variance of the total collection.
</p>
<p>The algorithms to be described are fixed point iterations of particular
functions of the tensor, which will then exhibit the desired result. For this,
the data is considered as an element of a high degree tensor space, although in
an abstract sense, the algorithms are independent of the representation of the
data as a tensor. All that we require is that the data can be considered as an
element of an associative, commutative algebra with an inner product. Such an
algebra is isomorphic to a commutative sub-algebra of the usual matrix algebra,
allowing the use of matrix algorithms to accomplish the mentioned tasks. We
allow the actual computational representation to be a lossy compression, and we
allow the algebra operations to be performed in an approximate fashion, so as
to maintain a high compression level. One such example which we address
explicitly is the representation of data as a tensor with compression in the
form of a low-rank representation.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.05669"><span class="datestr">at June 16, 2019 11:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.05565">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.05565">A Turing Kernelization Dichotomy for Structural Parameterizations of $\mathcal{F}$-Minor-Free Deletion</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Huib Donkers, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jansen:Bart_M=_P=.html">Bart M. P. Jansen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.05565">PDF</a><br /><b>Abstract: </b>For a fixed finite family of graphs $\mathcal{F}$, the
$\mathcal{F}$-Minor-Free Deletion problem takes as input a graph $G$ and an
integer $\ell$ and asks whether there exists a set $X \subseteq V(G)$ of size
at most $\ell$ such that $G-X$ is $\mathcal{F}$-minor-free. For
$\mathcal{F}=\{K_2\}$ and $\mathcal{F}=\{K_3\}$ this encodes Vertex Cover and
Feedback Vertex Set respectively. When parameterized by the feedback vertex
number of $G$ these two problems are known to admit a polynomial kernelization.
Such a polynomial kernelization also exists for any $\mathcal{F}$ containing a
planar graph but no forests. In this paper we show that
$\mathcal{F}$-Minor-Free Deletion parameterized by the feedback vertex number
is MK[2]-hard for $\mathcal{F} = \{P_3\}$. This rules out the existence of a
polynomial kernel assuming $NP \subseteq coNP/poly$, and also gives evidence
that the problem does not admit a polynomial Turing kernel. Our hardness result
generalizes to any $\mathcal{F}$ not containing a $P_3$-subgraph-free graph,
using as parameter the vertex-deletion distance to treewidth
$mintw(\mathcal{F})$, where $mintw(\mathcal{F})$ denotes the minimum treewidth
of the graphs in $\mathcal{F}$. For the other case, where $\mathcal{F}$
contains a $P_3$-subgraph-free graph, we present a polynomial Turing
kernelization. Our results extend to $\mathcal{F}$-Subgraph-Free Deletion.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.05565"><span class="datestr">at June 16, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.05486">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.05486">On Longest Common Property Preserved Substring Queries</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kai:Kazuki.html">Kazuki Kai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nakashima:Yuto.html">Yuto Nakashima</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Inenaga:Shunsuke.html">Shunsuke Inenaga</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bannai:Hideo.html">Hideo Bannai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Takeda:Masayuki.html">Masayuki Takeda</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kociumaka:Tomasz.html">Tomasz Kociumaka</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.05486">PDF</a><br /><b>Abstract: </b>We revisit the problem of longest common property preserving substring
queries introduced by~Ayad et al. (SPIRE 2018, arXiv 2018). We consider a
generalized and unified on-line setting, where we are given a set $X$ of $k$
strings of total length $n$ that can be pre-processed so that, given a query
string $y$ and a positive integer $k'\leq k$, we can determine the longest
substring of $y$ that satisfies some specific property and is common to at
least $k'$ strings in $X$. Ayad et al. considered the longest square-free
substring in an on-line setting and the longest periodic and palindromic
substring in an off-line setting. In this paper, we give efficient solutions in
the on-line setting for finding the longest common square, periodic,
palindromic, and Lyndon substrings. More precisely, we show that $X$ can be
pre-processed in $O(n)$ time resulting in a data structure of $O(n)$ size that
answers queries in $O(|y|\log\sigma)$ time and $O(1)$ working space, where
$\sigma$ is the size of the alphabet, and the common substring must be a
square, a periodic substring, a palindrome, or a Lyndon word.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.05486"><span class="datestr">at June 16, 2019 11:26 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.05458">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.05458">Fixed-Parameter Tractability of Graph Deletion Problems over Data Streams</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bishnu:Arijit.html">Arijit Bishnu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghosh:Arijit.html">Arijit Ghosh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kolay:Sudeshna.html">Sudeshna Kolay</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mishra:Gopinath.html">Gopinath Mishra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saurabh:Saket.html">Saket Saurabh</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.05458">PDF</a><br /><b>Abstract: </b>In this work, we initiate a systematic study of parameterized streaming
complexity of graph deletion problems: ${\cal F}$-Subgraph Deletion, ${\cal
F}$-Minor Deletion and Cluster Vertex Deletion in the four most well-studied
streaming models: the EA (edge arrival), DEA (dynamic edge arrival), VA (vertex
arrival) and AL (adjacency list) models. We also consider the streaming
complexities of a collection of widely-studied problems that are special
variants of ${\cal F}$-Subgraph Deletion, namely Feedback Vertex Set, Even
Cycle Transversal, Odd Cycle Transversal, Triangle Deletion, and Cluster Vertex
Deletion.
</p>
<p>Except for the Triangle Deletion and Cluster Vertex Deletion problems, we
show that none of the other problems have space-efficient streaming algorithms
when the problems are parameterized by $k$, the solution size. In fact, we show
that these problems admit $\Omega(n\log n)$ lower bounds in all the four models
stated above. This improves the lower bounds given by Chitnis et al. (SODA'16)
for the EA model. For the Triangle Deletion and Cluster Vertex Deletion
problems, the question of lower bounds for the problems parameterized by $k$ is
open for the AL model. For all other models, we show an improved lower bound of
$\Omega(n\log n)$ for Triangle Deletion. With regards to Cluster Vertex
Deletion, we extend the results of Chitnis et al. (SODA'16) in the EA model to
the DEA and VA models.
</p>
<p>We exploit the power of parameterization - a usual approach taken in
parameterized algorithms - to study a problem with respect to parameters
greater than the solution size or consider some structural parameters.
Parameterized by vertex cover size $K$, some of these problems on some of the
graph streaming models do not admit space-efficient streaming algorithms, while
it does so for others
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.05458"><span class="datestr">at June 16, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.05422">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.05422">Lower Bounds for the Happy Coloring Problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bliznets:Ivan.html">Ivan Bliznets</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sagunov:Danil.html">Danil Sagunov</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.05422">PDF</a><br /><b>Abstract: </b>In this paper, we study the Maximum Happy Vertices and the Maximum Happy
Edges problems (MHV and MHE for short). Very recently, the problems attracted a
lot of attention and were studied in Agrawal '18, Aravind et al. '16, Choudhari
and Reddy '18, Misra and Reddy '18. Main focus of our work is lower bounds on
the computational complexity of these problems. Established lower bounds can be
divided into the following groups: NP-hardness of the above guarantee
parameterization, kernelization lower bounds (answering questions of Misra and
Reddy '18), exponential lower bounds under the Set Cover Conjecture and the
Exponential Time Hypothesis, and inapproximability results. Moreover, we
present an $\mathcal{O}^*(\ell^k)$ randomized algorithm for MHV and an
$\mathcal{O}^*(2^k)$ algorithm for MHE, where $\ell$ is the number of colors
used and $k$ is the number of required happy vertices or edges. These
algorithms cannot be improved to subexponential taking proved lower bounds into
account.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.05422"><span class="datestr">at June 16, 2019 11:38 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1906.05384">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1906.05384">Loop Programming Practices that Simplify Quicksort Implementations</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Shoupu Wan <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1906.05384">PDF</a><br /><b>Abstract: </b>Quicksort algorithm with Hoare's partition scheme is traditionally
implemented with nested loops. In this article, we present loop programming and
refactoring techniques that lead to simplified implementation for Hoare's
quicksort algorithm consisting of a single loop. We believe that the techniques
are beneficial for general programming and may be used for the discovery of
more novel algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1906.05384"><span class="datestr">at June 16, 2019 11:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:typepad.com,2003:post-6a00d83452383469e20240a466212d200c">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/erickson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://3dpancakes.typepad.com/ernie/2019/06/buy-my-free-book.html">Buy My Free Book!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p><a href="https://3dpancakes.typepad.com/.a/6a00d83452383469e20240a48f6b37200d-popup" class="asset-img-link"><img src="https://3dpancakes.typepad.com/.a/6a00d83452383469e20240a48f6b37200d-500wi" alt="Al-khwarizmi" style="display: block; margin-left: auto; margin-right: auto;" class="asset  asset-image at-xid-6a00d83452383469e20240a48f6b37200d img-responsive" title="Al-khwarizmi" /></a></p>
<p>I'm happy to finally announce the publication of an actual dead-tree paperback edition of my <em>Algorithms</em> textbook.  The book can be purchased from Amazon (<a href="https://www.amazon.com/dp/1792644833">US</a>, <a href="https://www.amazon.co.uk/dp/1792644833">UK</a>, <a href="https://www.amazon.de/dp/1792644833">DE</a>, <a href="https://www.amazon.es/dp/1792644833">ES</a>, <a href="https://www.amazon.fr/dp/1792644833">FR</a>, <a href="https://www.amazon.it/dp/1792644833">IT</a>, <a href="https://www.amazon.co.jp/dp/1792644833">JP</a>), for the <a href="https://www.youtube.com/watch?v=ygE01sOhzz0">ludicrous</a> price of $27.50 (or the equivalent in pounds, euros, and yen).</p>
<p>I've also updated the freely available, full-color electronic version at <a href="http://algorithms.wtf/">http://algorithms.wtf</a>; this electronic version will remain free indefinitely.  The same site includes several additional lecture notes and other course materials.</p>
<p>Thanks to everyone who reported dozens of errors in the 0th and ½th edition on the <a href="https://github.com/jeffgerickson/algorithms">Github issue-tracker</a>.  Please keep the bug reports and feature requests coming!</p>
<p>Enjoy!</p></div>







<p class="date">
by Jeff Erickson <a href="https://3dpancakes.typepad.com/ernie/2019/06/buy-my-free-book.html"><span class="datestr">at June 15, 2019 08:29 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/06/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/06/15/linkage.html">Linkage for the end of an academic year</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>The Spring term just ended at UCI (we’re on a quarter system, so we run later into June and then start up again later in September than most other US universities). I haven’t yet turned in my grades, but I can already feel summer setting in.</p>

<ul>
  <li>
    <p><a href="https://hackaday.com/2019/06/01/paper-strandbeest-is-strong-enough-to-walk/">Remote-controlled papercraft steerable Strandbeest</a> (<a href="https://mathstodon.xyz/@11011110/102197633382208469"></a>, <a href="https://news.ycombinator.com/item?id=20068166">via</a>).</p>
  </li>
  <li>
    <p><a href="https://www.instagram.com/p/BaSVpX_nMpU/">Voronoi origami</a> (<a href="https://mathstodon.xyz/@11011110/102206289543572358"></a>, <a href="http://orderinspace.blogspot.com/2015/07/voronoi.html">see also</a>).</p>
  </li>
  <li>
    <p><a href="https://archive.org/details/@mirtitles">Mir Books</a> (<a href="https://mathstodon.xyz/@11011110/102212071495683563"></a>, <a href="https://mathstodon.xyz/@jarban/102209797748141088">via</a>). A big collection of interesting-looking Soviet-era mathematics and science books and booklets, translated into English and free to read.</p>
  </li>
  <li>
    <p><a href="https://mathenchant.wordpress.com/2017/09/17/how-do-you-write-one-hundred-in-base-32/">Chip-firing games and sesquinary notation</a> (<a href="https://mathstodon.xyz/@11011110/102217258990536291"></a>). Jim Propp writes a monthly long-form math blog and somehow I hadn’t encountered it before; this is one of its many interesting entries. One of the oddities about base  is that you calculate it bottom-up (by starting from a ones digit that’s too big and then carrying things higher) instead of top-down (by greedily subtracting powers).</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@JordiGH/102226037720885699">A discussion on whether proofs in Wikipedia articles need references</a>, and what those references are for.</p>
  </li>
  <li>
    <p><a href="https://theinnerframe.wordpress.com/2018/07/16/death-of-proof-the-pleasures-of-failure-i/">The tale of Horgan’s surface</a> (<a href="https://mathstodon.xyz/@11011110/102228263549241096"></a>, <a href="https://mathenchant.wordpress.com/2019/06/06/carnival-of-mathematics-170/">via</a>, <a href="http://www.indiana.edu/~minimal/essays/horgan/index.html">see also</a>, <a href="https://www.scottaaronson.com/blog/?p=4133">see also</a>), a nonexistent minimal surface whose existence was incorrectly predicted by numerical experiments, named sarcastically after a journalist who incautiously suggested that proof was a dead concept.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1810.02231">Compact packings of the plane with three sizes of discs</a> (<a href="https://mathstodon.xyz/@11011110/102240130646118434"></a>), Thomas Fernique, Amir Hashemi, and Olga Sizova. Here, “compact packing” means interior-disjoint disks forming only 3-sided gaps. The circle packing theorem constructs these for any finite maximal planar graph, with little control over disk size. Instead this paper seeks packings of the whole plane by infinitely many disks, with few sizes. 9 pairs of sizes and 164 triples work.</p>
  </li>
  <li>
    <p>Luca Trevisan posts a series of tutorials on online convex optimization, where you want to approximately minimize a sequence of convex functions before discovering what the functions are (parts <a href="https://lucatrevisan.wordpress.com/2019/04/17/online-optimization-for-complexity-theorists/"></a>, <a href="https://lucatrevisan.wordpress.com/2019/04/22/online-optimization-post-0-definitions/"></a>, <a href="https://lucatrevisan.wordpress.com/2019/04/24/online-optimization-post-1-multiplicative-weights/"></a>, <a href="https://lucatrevisan.wordpress.com/2019/04/25/online-optimization-post-2-constructing-pseudorandom-sets/"></a>, <a href="https://lucatrevisan.wordpress.com/2019/05/06/online-optimization-post-3-follow-the-regularized-leader/"></a>, <a href="https://lucatrevisan.wordpress.com/2019/05/16/online-optimization-post-4-regularity-lemmas/"></a>, <a href="https://lucatrevisan.wordpress.com/2019/05/20/online-optimization-post-5-bregman-projections-and-mirror-descent/"></a>; <a href="https://mathstodon.xyz/@11011110/102244702633232612"></a>). It’s a hot topic in TCS with connections to regularity lemmas, fast SDP approximation, and spectral sparsifiers.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Cross_sea">Squared patterns of ocean waves</a> (<a href="https://mathstodon.xyz/@11011110/102251731269228854"></a>), and wave patterns in social media: search for “cross sea” and note its appearance on gizmodo in 2014, amusingplanet in 2015, azula in 2017, providr in 2018, sciencealert in 2019…all repeating the same somewhat garbled explanation of mathematical wave models and danger to shipping.</p>
  </li>
  <li>
    <p><a href="https://thmatters.wordpress.com/2019/06/11/wikipedia-edit-a-thon-at-stoc19/">The SIGACT Committee for the Advancement of Theoretical Computer Science is planning a Wikipedia edit-a-thon, in Phoenix on June 24 as part of STOC</a> (<a href="https://mathstodon.xyz/@11011110/102262460312659446"></a>). You can help, and you don’t even have to brave the desert heat to do so! There’s <a href="https://thmatters.wordpress.com/2017/05/02/tcs-wikipedia-project/">a shared spreadsheet</a> where CATCS is crowdsourcing TCS topics on Wikipedia that need help. Add your favorite missing algorithm, theorem, complexity class, etc, and it’s likely it’ll get some attention.</p>
  </li>
  <li>
    <p>While I’m publicizing activities associated with STOC and FCRC next week in Phoenix, here’s another: <a href="https://sigact.org/tcswomen/tcs-women-2019/">the TCS Women Spotlight Workshop</a> (<a href="https://mathstodon.xyz/@11011110/102266092255364896"></a>). It features an inspirational talk from Ronitt Rubinfeld (in my experience a great speaker), four “rising star” talks by Naama Ben-David, Debarati Das, Andrea Lincoln, and Oxana Poburinnaya, a panel/lunch for women at STOC, and a poster session of recent theoretical computer science research by women.</p>
  </li>
  <li>
    <p>Two colleagues from my department, Alex Nicolau and Alex Veidenbaum, are participating in <a href="http://artdaily.com/news/114313/University-of-California--Irvine-computer-scientists-breathe-life-into-Venice-Biennale-installations">a Venice Biennale project</a> (<a href="https://mathstodon.xyz/@11011110/102272037129033816"></a>) in which viewers converse with computerized simulations of poet <a href="https://en.wikipedia.org/wiki/Paul_Celan">Paul Celan</a> and politician <a href="https://en.wikipedia.org/wiki/Nicolae_Ceau%C8%99escu">Nicolae Ceaușescu</a>.
The Alexes usually work on the more technical side of CS (parallelizing compilers, computer architecture, and embedded systems) so it’s interesting to me to see this softer direction from them.</p>
  </li>
  <li>
    <p><a href="https://www.nature.com/articles/d41586-019-01796-1"><em>Nature</em> on how word processors and text editors have been shifting from roll-your-own equation editing to LaTeX</a> (<a href="https://mathstodon.xyz/@11011110/102277832022967684"></a>, <a href="https://news.ycombinator.com/item?id=20191348">via</a>).</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/06/15/linkage.html"><span class="datestr">at June 15, 2019 05:58 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2019/06/14/3rd-symposium-on-simplicity-in-algorithms/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2019/06/14/3rd-symposium-on-simplicity-in-algorithms/">3rd Symposium on Simplicity in Algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
January 6-7, 2020 Salt Lake City, UT https://www.siam.org/Conferences/CM/Conference/sosa20 Submission deadline: August 9, 2019 Symposium on Simplicity in Algorithms is a new conference in theoretical computer science dedicated to advancing algorithms research by promoting simplicity and elegance in the design and analysis of algorithms. The benefits of simplicity are manifold: simpler algorithms manifest a better understanding … <a href="https://cstheory-events.org/2019/06/14/3rd-symposium-on-simplicity-in-algorithms/" class="more-link">Continue reading <span class="screen-reader-text">3rd Symposium on Simplicity in Algorithms</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2019/06/14/3rd-symposium-on-simplicity-in-algorithms/"><span class="datestr">at June 14, 2019 04:09 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2019/06/13/5th-algorithmic-and-enumerative-combinatorics-summer-school-2019-2/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2019/06/13/5th-algorithmic-and-enumerative-combinatorics-summer-school-2019-2/">5th Algorithmic and Enumerative Combinatorics Summer School 2019</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
July 29 – August 2, 2019 Linz, Austria https://www3.risc.jku.at/conferences/aec2019/local.html Registration deadline: June 16, 2019 The goal of this summer school is to put forward the interplay between the fields of Enumerative Combinatorics, Analytic Combinatorics, and Algorithmics. This is a very active research area, which, aside from the three fields fueling each other mutually, receives as … <a href="https://cstheory-events.org/2019/06/13/5th-algorithmic-and-enumerative-combinatorics-summer-school-2019-2/" class="more-link">Continue reading <span class="screen-reader-text">5th Algorithmic and Enumerative Combinatorics Summer School 2019</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2019/06/13/5th-algorithmic-and-enumerative-combinatorics-summer-school-2019-2/"><span class="datestr">at June 13, 2019 10:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2019/06/13/adfocs-2019-games-brains-and-distributed-computing/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2019/06/13/adfocs-2019-games-brains-and-distributed-computing/">ADFOCS 2019 – Games, Brains, and Distributed Computing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
August 19-23, 2019 Saarbrücken, Germany http://resources.mpi-inf.mpg.de/conferences/adfocs/ Registration deadline: July 19, 2019 ADFOCS is an international summer school held annually at the Max Planck Institute for Informatics (MPII). It is organized as part of the activities of the MPII, in particular the International Max-Planck Research School (IMPRS), MPII’s graduate program. The purpose of this summer school … <a href="https://cstheory-events.org/2019/06/13/adfocs-2019-games-brains-and-distributed-computing/" class="more-link">Continue reading <span class="screen-reader-text">ADFOCS 2019 – Games, Brains, and Distributed Computing</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2019/06/13/adfocs-2019-games-brains-and-distributed-computing/"><span class="datestr">at June 13, 2019 10:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentdescent.org/smoothadv">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/yang.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentdescent.org/smoothadv.html">Provably Robust Deep Learning</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentdescent.org/" title="Decent Descent">"Greg Yang"</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<blockquote>
  <p>Recently, several works proposed the convolution of a neural network with a Gaussian as a smoothed classifier for provably robust classification. We show that adversarially training this <strong>smoothed</strong> classifier significantly increases its provable robustness through extensive experiments, achieving state-of-the-art <span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> <strong>provable robustness</strong> on CIFAR10 and Imagenet, as shown in the tables below.</p>
</blockquote>

<table>
  <thead>
    <tr>
      <th><span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> radius (Imagenet)</th>
      <th>0.5</th>
      <th>1</th>
      <th>1.5</th>
      <th>2</th>
      <th>2.5</th>
      <th>3</th>
      <th>3.5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> (%)</td>
      <td>49</td>
      <td>37</td>
      <td>29</td>
      <td>19</td>
      <td>15</td>
      <td>12</td>
      <td>9</td>
    </tr>
    <tr>
      <td><a href="https://arxiv.org/abs/1906.04584">Ours</a> (%)</td>
      <td>56</td>
      <td>43</td>
      <td>37</td>
      <td>27</td>
      <td>25</td>
      <td>20</td>
      <td>16</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th><span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> radius (CIFAR-10)</th>
      <th>0.25</th>
      <th>0.5</th>
      <th>0.75</th>
      <th>1.0</th>
      <th>1.25</th>
      <th>1.5</th>
      <th>1.75</th>
      <th>2.0</th>
      <th>2.25</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> (%)</td>
      <td>60</td>
      <td>43</td>
      <td>32</td>
      <td>23</td>
      <td>17</td>
      <td>14</td>
      <td>12</td>
      <td>10</td>
      <td>8</td>
    </tr>
    <tr>
      <td><a href="https://arxiv.org/abs/1906.04584">Ours</a> (%)</td>
      <td>74</td>
      <td>57</td>
      <td>48</td>
      <td>38</td>
      <td>33</td>
      <td>29</td>
      <td>24</td>
      <td>19</td>
      <td>17</td>
    </tr>
  </tbody>
</table>

<h1 id="introduction">Introduction</h1>

<p>It is now well-known that deep neural networks suffer from the brittleness problem: A small change in an input image imperceptible to humans can cause dramatic change in a neural network’s classification of the image. Such a perturbed input is known as an <em>adversarial example</em> and is by now immortalized in the famous picture below from <a href="https://arxiv.org/abs/1412.6572">Goodfellow et al.</a></p>

<p><img src="https://decentdescent.org/assets/smoothadv/panda_gibbon_adv.png" alt="A small carefully crafted noise can change a panda to a gibbon --- at least to a neural network!" /></p>

<p>As deep neural networks enter consumer and enterprise products of various forms, this brittleness can possibly have devastating consequences (<a href="https://arxiv.org/abs/1712.09665">Brown et al. 2018</a>,
<a href="https://arxiv.org/abs/1707.07397">Athalye et al. 2017</a>,
<a href="https://arxiv.org/abs/1707.089450">Evtimov &amp; Eykholt et al. 2018</a>,
<a href="https://arxiv.org/abs/1904.00759">Li et al. 2019</a>).
Most strikingly, Tencent Keen Security Lab recently <a href="https://keenlab.tencent.com/en/2019/03/29/Tencent-Keen-Security-Lab-Experimental-Security-Research-of-Tesla-Autopilot/">demonstrated</a> that the neural network underlying Tesla Autopilot can be fooled by an adversarially crafted marker on the ground into swerving into the opposite lane.</p>

<h2 id="adversarial-attack-and-defense">Adversarial Attack and Defense</h2>

<p>Given the importance of the problem, many researchers have formulated security models of adversarial attacks, along with ways to defend against adversaries in such models. In the most popular security model in the academic circle today, the adversary is allowed to perturb an input by a small noise bounded in <span class="katex"><span class="katex-mathml">ℓp\ell_p</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.15139200000000003em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.286108em;" class="vlist"><span></span></span></span></span></span></span></span></span></span>-norm, in order to cause the network to misclassify it. Thus, given a loss function <span class="katex"><span class="katex-mathml">LL</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">L</span></span></span></span>, a norm bound <span class="katex"><span class="katex-mathml">ϵ\epsilon</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">ϵ</span></span></span></span>, an input <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">x</span></span></span></span>, its label <span class="katex"><span class="katex-mathml">yy</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">y</span></span></span></span>, and a neural network <span class="katex"><span class="katex-mathml">FF</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">F</span></span></span></span>, the adversary tries to find an input <span class="katex"><span class="katex-mathml">x^\hat x</span><span class="katex-html"><span class="base"><span style="height: 0.69444em; vertical-align: 0em;" class="strut"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span></span></span></span>, within <span class="katex"><span class="katex-mathml">ℓp\ell_p</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.15139200000000003em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.286108em;" class="vlist"><span></span></span></span></span></span></span></span></span></span>-distance <span class="katex"><span class="katex-mathml">ϵ\epsilon</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">ϵ</span></span></span></span> of <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">x</span></span></span></span>, that maximizes the loss <span class="katex"><span class="katex-mathml">L(F(x),y)L(F(x), y)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span>, i.e. it solves the following optimization problem</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">x^=arg max⁡∥x′−x∥p≤ϵL(F(x′),y).\hat x = \argmax_{\|x' - x\|_p \le \epsilon} L(F(x'), y).</span><span class="katex-html"><span class="base"><span style="height: 0.69444em; vertical-align: 0em;" class="strut"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.43055999999999994em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∥</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.6828285714285715em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">x</span><span class="mord mtight"><span class="mord mtight">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.16454285714285716em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.2818857142857143em;" class="vlist"><span></span></span></span></span></span></span><span class="mrel mtight">≤</span><span class="mord mathdefault mtight">ϵ</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mop"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm">g</span><span class="mspace"></span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.18276em;" class="vlist"><span></span></span></span></span></span><span class="mspace"></span><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.801892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span><span class="mord">.</span></span></span></span></span>

<p>If <span class="katex"><span class="katex-mathml">FF</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">F</span></span></span></span> has trainable parameters <span class="katex"><span class="katex-mathml">θ\theta</span><span class="katex-html"><span class="base"><span style="height: 0.69444em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">θ</span></span></span></span>, then the defense needs to find the parameters that minimizes <span class="katex"><span class="katex-mathml">L(F(x^),y)L(F(\hat x), y)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span>, for <span class="katex"><span class="katex-mathml">(x,y)(x, y)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span> sampled from the data distribution <span class="katex"><span class="katex-mathml">DD</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">D</span></span></span></span>, i.e. it solves the following minimax problem</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">min⁡θE(x,y)∼DL(F(x^),y).\min_{\theta} \underset{(x, y) \sim D}{\mathbb{E}} L(F(\hat x), y).</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.66786em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">θ</span></span></span></span><span><span style="height: 2.7em;" class="pstrut"></span><span><span class="mop">min</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.7521079999999999em;" class="vlist"><span></span></span></span></span></span><span class="mspace"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.68889em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight">y</span><span class="mclose mtight">)</span><span class="mrel mtight">∼</span><span class="mord mathdefault mtight">D</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mord"><span class="mord mathbb">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.966em;" class="vlist"><span></span></span></span></span></span></span><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span><span class="mord">.</span></span></span></span></span>

<p>Empirically, during an attack, the adversarial input <span class="katex"><span class="katex-mathml">x^\hat x</span><span class="katex-html"><span class="base"><span style="height: 0.69444em; vertical-align: 0em;" class="strut"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span></span></span></span> can be obtained approximately by solving the max problem using gradient descent, making sure to project back to the <span class="katex"><span class="katex-mathml">ϵ\epsilon</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">ϵ</span></span></span></span>-ball after each step.
This is known as the PGD attack (<a href="https://arxiv.org/abs/1607.02533">Kurakin et al.</a>, <a href="https://arxiv.org/abs/1706.06083">Madry et al.</a>), short for “project gradient descent.”
During training by the defense, for every sample <span class="katex"><span class="katex-mathml">(x,y)∼D(x, y) \sim D</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span><span class="mspace"></span><span class="mrel">∼</span><span class="mspace"></span></span><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">D</span></span></span></span>, this estimate of <span class="katex"><span class="katex-mathml">x^\hat x</span><span class="katex-html"><span class="base"><span style="height: 0.69444em; vertical-align: 0em;" class="strut"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span></span></span></span> can be plugged into the min problem for gradient descent of <span class="katex"><span class="katex-mathml">θ\theta</span><span class="katex-html"><span class="base"><span style="height: 0.69444em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">θ</span></span></span></span>. 
This is known as <strong>Adversarial Training</strong>, or <strong>PGD training</strong> specifically when PGD is used for finding <span class="katex"><span class="katex-mathml">x^\hat x</span><span class="katex-html"><span class="base"><span style="height: 0.69444em; vertical-align: 0em;" class="strut"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span></span></span></span>.</p>

<h2 id="empirical-robust-accuracy">Empirical Robust Accuracy</h2>

<p>Currently, the standard benchmark for measuring the strength of a model’s adversarial defense is the model’s <em>(empirical) robust accuracy</em> on various standard datasets like CIFAR-10 and Imagenet.
This accuracy is calculated by attacking the model with a strong empirical attack (like PGD) for every sample of the test set.
The percentage of the test set that the model is still able to correctly classify is the empirical robust accuracy.</p>

<p>For example, consider an adversary allowed to perturb an input by <span class="katex"><span class="katex-mathml">ϵ=8255\epsilon = \frac{8}{255}</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">ϵ</span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.845108em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mtight">5</span><span class="mord mtight">5</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span style="border-bottom-width: 0.04em;" class="frac-line"></span></span><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">8</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.345em;" class="vlist"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> in <span class="katex"><span class="katex-mathml">ℓ∞\ell_\infty</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.151392em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">∞</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> norm.
On an image, this means that the adversary can change the color of each pixel by at most 8 units (out of 255 total) in each color channel — a rather imperceptible perturbation.
Currently, the state-of-the-art empirical robust accuracy against such an adversary on CIFAR-10 hovers around 55% (<a href="https://arxiv.org/abs/1901.08573">Zhang et al. 2019</a>, <a href="https://arxiv.org/abs/1901.09960">Hendrycks et al. 2019</a>), meaning that the best classifier can only withstand a strong attack on about 55% of the samples in CIFAR-10.
Contrast this with the <a href="https://paperswithcode.com/sota/image-classification-on-cifar-10">state-of-the-art nonrobust accuracy on CIFAR-10 of &gt;95%</a>.
Thus it’s clear that adversarial robustness research still has a long way to go.</p>

<h1 id="provable-robustness-via-randomized-smoothing">Provable Robustness via Randomized Smoothing</h1>

<p>Note that the empirical robust accuracy is only an upper bound on the <strong>true robust accuracy</strong>.
This is defined by hypothetically replacing the <em>strong empirical attack</em> used in empirical robust accuracy with the <em>ideal attack</em> able to find <span class="katex"><span class="katex-mathml">x^\hat x</span><span class="katex-html"><span class="base"><span style="height: 0.69444em; vertical-align: 0em;" class="strut"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span></span></span></span> exactly for every <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">x</span></span></span></span>.
Thus, nothing in principle prevents a stronger empirical attack from further lowering the empirical robust accuracy of a model.
Indeed, except a few notable cases like PGD (<a href="https://arxiv.org/abs/1706.06083">Madry et al.</a>), we have seen most claims of adversarial robustness broken down by systematic and thorough attacks (as examples, see <a href="https://arxiv.org/abs/1607.04311">Carlini &amp; Wagner 2016</a>,
<a href="https://arxiv.org/pdf/1705.07263">Carlini &amp; Wagner 2017</a>,
<a href="https://arxiv.org/abs/1707.07397">Athalye et al. 2017</a>,
<a href="https://arxiv.org/abs/1802.05666">Uesato et al. 2018</a>,
<a href="https://arxiv.org/abs/1802.00420">Athalye et al. 2018</a>,
<a href="https://arxiv.org/abs/1807.10272">Engstrom et al. 2018</a>,
<a href="https://arxiv.org/abs/1902.02322">Carlini 2019</a>).</p>

<p>This has motivated researchers into developing defenses that can <em>certify the absence of adversarial examples</em> (as prominent examples, see
<a href="https://arxiv.org/abs/1711.00851">Wong &amp; Kolter 2018</a>,
<a href="https://arxiv.org/abs/1702.01135">Katz et al. 2017</a>,
and see <a href="https://arxiv.org/pdf/1902.08722">Salman et al. 2019</a> for a thorough overview of these techniques). 
Such a defense is afforded a <strong>provable (or certified) robust accuracy</strong> on each dataset, defined as the percentage of the test set that can be proved to have no adversarial examples in its neighborhood.
In contrast with empirical robust accuracy, provable robust accuracy is a <em>lower bound</em> on the true robust accuracy, and therefore cannot be lowered further by more clever attacks.
The tables in the beginning of our blog post, for example, display provable robust accuracies on CIFAR-10 and Imagenet.</p>

<p>Until recently, most such certifiable defenses have not been able to scale to large networks and datasets (<a href="https://arxiv.org/pdf/1902.08722">Salman et al. 2019</a>), but a new technique called <em>randomized smoothing</em> (<a href="https://arxiv.org/abs/1802.03471">Lecuyer et al.</a>, <a href="https://arxiv.org/abs/1809.03113">Li et al.</a>, <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a>) was shown to bypass this limitation, obtaining highly-nontrival <span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> certified robust accuracy on Imagenet (<a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a>). We now briefly review randomized smoothing.</p>

<h2 id="definition">Definition</h2>

<p>Consider a classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span> from <span class="katex"><span class="katex-mathml">Rd\mathbb{R}^d</span><span class="katex-html"><span class="base"><span style="height: 0.849108em; vertical-align: 0em;" class="strut"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.849108em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span></span></span></span> to classes <span class="katex"><span class="katex-mathml">Y\mathcal{Y}</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathcal">Y</span></span></span></span></span>. Randomized smoothing is a method that constructs a new, <em>smoothed</em> classifier <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span> from the <em>base</em> classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span>. The smoothed classifier <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span> assigns to a query point <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">x</span></span></span></span> the class which is most likely to be returned by the base classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span> under isotropic Gaussian noise perturbation of <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">x</span></span></span></span>, i.e.,</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">g(x)=arg max⁡c∈Y  P(f(x+δ)=c)g(x) = \argmax_{c \in \mathcal{Y}} \; \mathbb{P}(f(x+\delta) = c)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.43055999999999983em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="mrel mtight">∈</span><span class="mord mtight"><span class="mord mathcal mtight">Y</span></span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mop"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm">g</span><span class="mspace"></span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.006825em;" class="vlist"><span></span></span></span></span></span><span class="mspace"></span><span class="mspace"></span><span class="mord"><span class="mord mathbb">P</span></span><span class="mopen">(</span><span class="mord mathdefault">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord mathdefault">δ</span><span class="mclose">)</span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord mathdefault">c</span><span class="mclose">)</span></span></span></span></span>

<p>where <span class="katex"><span class="katex-mathml">δ∼N(0,σ2I)\delta \sim \mathcal{N}(0, \sigma^2 I)</span><span class="katex-html"><span class="base"><span style="height: 0.69444em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">δ</span><span class="mspace"></span><span class="mrel">∼</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathcal">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.8141079999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault">I</span><span class="mclose">)</span></span></span></span>, and the variance <span class="katex"><span class="katex-mathml">σ2\sigma^2</span><span class="katex-html"><span class="base"><span style="height: 0.8141079999999999em; vertical-align: 0em;" class="strut"></span><span class="mord"><span class="mord mathdefault">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.8141079999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> is a hyperparameter of the smoothed classifier <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span> (it can be thought to control a robustness/accuracy tradeoff).
In <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a>, <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span> is a neural network.</p>

<h2 id="prediction">Prediction</h2>

<p>To estimate <span class="katex"><span class="katex-mathml">g(x)g(x)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>, one simply has to</p>

<ol>
  <li>Sample a collection of Gausian samples <span class="katex"><span class="katex-mathml">δi\delta_i</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.31166399999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span>.</li>
  <li>Predict the class <span class="katex"><span class="katex-mathml">yiy_i</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.31166399999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> of each <span class="katex"><span class="katex-mathml">x+δix + \delta_i</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">x</span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.31166399999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> using the base classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span>.</li>
  <li>Take the majority vote of the <span class="katex"><span class="katex-mathml">yiy_i</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.31166399999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span>’s as the final prediction of the smoothed classifier <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span> at <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">x</span></span></span></span>.</li>
</ol>

<h2 id="certification">Certification</h2>

<p>The robustness guarantee presented by <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> is as follows: suppose that when the base classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span> classifies  <span class="katex"><span class="katex-mathml">N(x,σ2I)\mathcal{N}(x, \sigma^2 I)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathcal">N</span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.8141079999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault">I</span><span class="mclose">)</span></span></span></span>, the (most popular) class <span class="katex"><span class="katex-mathml">cAc_A</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> is returned with probability  <span class="katex"><span class="katex-mathml">pA=Pδ(f(x+δ)=cA)p_A =  \mathbb{P}_\delta(f(x+\delta) = c_A)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mord"><span class="mord mathbb">P</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.33610799999999996em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">δ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord mathdefault">δ</span><span class="mclose">)</span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>, and the <em>runner-up</em> class <span class="katex"><span class="katex-mathml">cBc_B</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> is returned with probability <span class="katex"><span class="katex-mathml">pB=max⁡c≠cAPδ(f(x+δ)=c)p_B = \max_{c \neq c_A} \mathbb{P}_\delta(f(x+\delta) = c)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mop"><span class="mop">max</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.3361079999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="mrel mtight"><span class="mrel mtight"><span class="mord mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="rlap mtight"><span class="strut"></span><span class="inner"><span class="mrel mtight"></span></span><span class="fix"></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.19444em;" class="vlist"><span></span></span></span></span></span></span><span class="mrel mtight">=</span></span><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.3448em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.14329285714285717em;" class="vlist"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.286108em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="mord"><span class="mord"><span class="mord mathbb">P</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.33610799999999996em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">δ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord mathdefault">δ</span><span class="mclose">)</span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord mathdefault">c</span><span class="mclose">)</span></span></span></span>. We estimate <span class="katex"><span class="katex-mathml">pAp_A</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml">pBp_B</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> using Monte Carlo sampling and confidence intervals<sup id="fnref:1"><a href="https://decentdescent.org/smoothadv.html#fn:1" class="footnote">1</a></sup>. Then the smoothed classifier <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span> is robust around <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">x</span></span></span></span> within the radius</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">σ2(Φ−1(pA)−Φ−1(pB)),\frac{\sigma}{2} \left(\Phi^{-1}(p_A) - \Phi^{-1}(p_B)\right),</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 1.10756em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord">2</span></span></span><span><span style="height: 3em;" class="pstrut"></span><span style="border-bottom-width: 0.04em;" class="frac-line"></span></span><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord mathdefault">σ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.686em;" class="vlist"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord">Φ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.864108em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace"></span><span class="mbin">−</span><span class="mspace"></span><span class="mord"><span class="mord">Φ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.864108em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose delimcenter"><span class="delimsizing size1">)</span></span></span><span class="mspace"></span><span class="mpunct">,</span></span></span></span></span>

<p>where <span class="katex"><span class="katex-mathml">Φ−1\Phi^{-1}</span><span class="katex-html"><span class="base"><span style="height: 0.8141079999999999em; vertical-align: 0em;" class="strut"></span><span class="mord"><span class="mord">Φ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.8141079999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span> is the inverse of the standard Gaussian CDF. Thus, the bigger <span class="katex"><span class="katex-mathml">pAp_A</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> is and the smaller <span class="katex"><span class="katex-mathml">pBp_B</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> is, the more provably robust <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span> is.</p>

<h2 id="training">Training</h2>

<p><a href="https://arxiv.org/abs/1906.04584">Cohen et al.</a> simply trained the base classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span> under Gaussian noise data augmentation with cross entropy loss, i.e. for each data point <span class="katex"><span class="katex-mathml">(x,y)(x, y)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span>, sample <span class="katex"><span class="katex-mathml">δ∼N(0,σ2I)\delta \sim \mathcal{N}(0, \sigma^2 I)</span><span class="katex-html"><span class="base"><span style="height: 0.69444em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">δ</span><span class="mspace"></span><span class="mrel">∼</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathcal">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.8141079999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault">I</span><span class="mclose">)</span></span></span></span> and train <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span> on the example <span class="katex"><span class="katex-mathml">(x+δ,y)(x+\delta, y)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord mathdefault">δ</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span>.
With this simple training regime applied to a <a href="https://arxiv.org/abs/1512.03385">Resnet-110</a> base classifier, they were able to obtain significant certified robustness on CIFAR-10 and Imagenet, as shown in our tables.</p>

<h2 id="an-illustration">An Illustration</h2>

<p>The following figures modified from <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> illustrate randomized smoothing.
The base classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span> partitions the input space into different regions with different classifications, colored differently in the left figure.
The regions’ Gaussian measures (under the Gaussian <span class="katex"><span class="katex-mathml">N(x,σ2I)\mathcal{N}(x, \sigma^2 I)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathcal">N</span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.8141079999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault">I</span><span class="mclose">)</span></span></span></span> whose level curves are shown as dashed lines) are shown as a histogram on the right.
The class <span class="katex"><span class="katex-mathml">cAc_A</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> corresponding to the blue region is the output of the smoothed classifier <span class="katex"><span class="katex-mathml">g(x)g(x)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>; the class <span class="katex"><span class="katex-mathml">cBc_B</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> corresponding to the cyan region is the runner-up class.
If <span class="katex"><span class="katex-mathml">pAp_A</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> is large enough and <span class="katex"><span class="katex-mathml">pBp_B</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> is small enough, then we can prove that <span class="katex"><span class="katex-mathml">g(x′)=cAg(x') = c_A</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.751892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> for all <span class="katex"><span class="katex-mathml">∥x′−x∥2≤ϵ\|x' - x\|_2 \le \epsilon</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord">∥</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.751892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace"></span><span class="mbin">−</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord mathdefault">x</span><span class="mord"><span class="mord">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="mrel">≤</span><span class="mspace"></span></span><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">ϵ</span></span></span></span>, i.e. <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span> is robust at <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">x</span></span></span></span> for <span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> radius <span class="katex"><span class="katex-mathml">ϵ\epsilon</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">ϵ</span></span></span></span>.</p>

<p><img src="https://decentdescent.org/assets/smoothadv/randomized_smoothing_simple_light.png" class="limit-height" /></p>

<h1 id="adversarially-training-the-smoothed-classifier">Adversarially Training the Smoothed Classifier</h1>

<p>Intuitively, adversarial training attempts to make a classifier locally flat around input sampled from a data distribution.
Thus it would seem that adversarial training should make it easier to <em>certify</em> the lack of adversarial examples, despite having no provable guarantees itself.
Yet historically, it has been difficult to execute this idea (<a href="https://arxiv.org/pdf/1902.08722">Salman et al. 2019</a>, and folklore), with the closest being <a href="https://arxiv.org/abs/1809.03008">Xiao et al.</a></p>

<p>It is hence by no means a foregone conclusion that adversarial training should improve certified accuracy of randomized smoothing.
<em>A priori</em> there could also be many ways these two techniques can be combined, and it is not clear which one would work best:</p>

<ol>
  <li>Train the base classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span> to be adversarially robust, simultaneous with the Gaussian data augmentation training prescribed in <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a>.</li>
  <li>Find an adversarial example of the base classifier <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span>, then add Gaussian noise and train.</li>
  <li>Add Gaussian noise and find an adversarial example of <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span> in the neighborhood of this Gaussian perturbation. Train <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span> on this adversarial example.</li>
  <li>Find an adversarial example of the smoothed classifier <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span>, then train <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span> on this example.</li>
</ol>

<p>It turns out that certified accuracies of these methods follow the order (1) &lt; (2) &lt; (3) &lt; (4), with (4) achieving the highest certified accuracies (see <a href="https://arxiv.org/abs/1906.04584">our paper</a>).
Indeed, in hindsight, if <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span> is the classifer doing the prediction, then we should be adversarially training <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span>, and not <span class="katex"><span class="katex-mathml">ff</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span></span></span></span>.
In the rest of the blog post, we lay out the details of (4).</p>

<h2 id="randomized-smoothing-for--soft-classifiers">Randomized Smoothing for  <em>Soft</em> Classifiers</h2>
<p>Neural networks typically learn <em>soft</em> classifiers, namely, functions <span class="katex"><span class="katex-mathml">F:Rd→P(Y)F: \mathbb{R}^d \to P(\mathcal{Y})</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">F</span><span class="mspace"></span><span class="mrel">:</span><span class="mspace"></span></span><span class="base"><span style="height: 0.849108em; vertical-align: 0em;" class="strut"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.849108em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span></span></span></span></span><span class="mspace"></span><span class="mrel">→</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord mathdefault">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathcal">Y</span></span><span class="mclose">)</span></span></span></span>, where <span class="katex"><span class="katex-mathml">P(Y)P(\mathcal{Y})</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathcal">Y</span></span><span class="mclose">)</span></span></span></span> is the set of probability distributions over <span class="katex"><span class="katex-mathml">Y\mathcal{Y}</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathcal">Y</span></span></span></span></span>.
During prediction, the soft classifier is argmaxed to return the final hard classification.
We therefore consider a generalization of randomized smoothing to soft classifiers. Given a soft classifier <span class="katex"><span class="katex-mathml">FF</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">F</span></span></span></span>, its associated <em>smoothed</em> soft classifier  <span class="katex"><span class="katex-mathml">G:Rn→P(Y)G: \mathbb{R}^n \to P(\mathcal{Y})</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">G</span><span class="mspace"></span><span class="mrel">:</span><span class="mspace"></span></span><span class="base"><span style="height: 0.68889em; vertical-align: 0em;" class="strut"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.664392em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span><span class="mspace"></span><span class="mrel">→</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord mathdefault">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathcal">Y</span></span><span class="mclose">)</span></span></span></span> is defined as</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">G(x)=Eδ∼N(0,σ2I)F(x+δ).G (x) = \underset{\delta \sim \mathcal{N}(0, \sigma^2 I)}{\mathbb{E}} F(x + \delta).</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">G</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.68889em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">δ</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathcal mtight">N</span></span><span class="mopen mtight">(</span><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathdefault mtight">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.7463142857142857em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault mtight">I</span><span class="mclose mtight">)</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mord"><span class="mord mathbb">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.966em;" class="vlist"><span></span></span></span></span></span></span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord mathdefault">δ</span><span class="mclose">)</span><span class="mord">.</span></span></span></span></span>

<p>Let <span class="katex"><span class="katex-mathml">f(x)f(x)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">f</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span> and <span class="katex"><span class="katex-mathml">F(x)F (x)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span> denote the hard and soft classifiers learned by the neural network, respectively, and let <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span> and <span class="katex"><span class="katex-mathml">GG</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">G</span></span></span></span> denote the associated smoothed hard and smoothed soft classifiers. Directly finding adversarial examples for the smoothed <em>hard</em> classifier <span class="katex"><span class="katex-mathml">gg</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">g</span></span></span></span> is a somewhat ill-behaved problem because of the argmax, so we instead propose to <em>find adversarial examples for the smoothed soft classifier</em> <span class="katex"><span class="katex-mathml">GG</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">G</span></span></span></span>. Empirically we found that doing so will also find good adversarial examples for the smoothed hard classifier.</p>

<h2 id="finding-adverarial-examples-for-smoothed-soft-classifier">Finding Adverarial Examples for Smoothed Soft Classifier</h2>
<p>Given a labeled data point <span class="katex"><span class="katex-mathml">(x,y)(x, y)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span>, we wish to find a point <span class="katex"><span class="katex-mathml">x^\hat x</span><span class="katex-html"><span class="base"><span style="height: 0.69444em; vertical-align: 0em;" class="strut"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span></span></span></span> which maximizes the loss of <span class="katex"><span class="katex-mathml">GG</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">G</span></span></span></span> in an <span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> ball around <span class="katex"><span class="katex-mathml">xx</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">x</span></span></span></span> for some choice of loss function.
As is canonical in the literature, we focus on the cross entropy loss <span class="katex"><span class="katex-mathml">LCEL_{\mathrm{CE}}</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">C</span><span class="mord mathrm mtight">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span>.
Thus, given a labeled data point <span class="katex"><span class="katex-mathml">(x,y)(x, y)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span> our (ideal) adversarial perturbation is given by the formula:</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">x^=arg max⁡∥x′−x∥2≤ϵLCE(G(x′),y)=arg max⁡∥x′−x∥2≤ϵ(−log⁡Eδ∼N(0,σ2I)F(x′+δ)y).\begin{aligned}
    \hat x &amp;= \argmax_{\|x' - x\|_2 \leq \epsilon} L_{\mathrm{CE} } (G (x'), y)\\ 
    &amp;= \argmax_{\|x' - x\|_2 \leq \epsilon}  \left( - \log \underset{\delta \sim \mathcal{N} (0, \sigma^2 I)}{\mathbb{E}}  F (x' + \delta)_y \right).
\end{aligned}</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 2.8554399999999998em;" class="vlist"><span><span style="height: 3.45em;" class="pstrut"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span></span></span><span><span style="height: 3.45em;" class="pstrut"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 2.3554399999999998em;" class="vlist"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 2.8554399999999998em;" class="vlist"><span><span style="height: 3.45em;" class="pstrut"></span><span class="mord"><span class="mord"></span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.43056000000000016em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∥</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.6828285714285715em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">x</span><span class="mord mtight"><span class="mord mtight">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.31731428571428577em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.143em;" class="vlist"><span></span></span></span></span></span></span><span class="mrel mtight">≤</span><span class="mord mathdefault mtight">ϵ</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mop"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm">g</span><span class="mspace"></span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.16044em;" class="vlist"><span></span></span></span></span></span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">C</span><span class="mord mathrm mtight">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">G</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.801892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span><span><span style="height: 3.45em;" class="pstrut"></span><span class="mord"><span class="mord"></span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.43056000000000016em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∥</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.6828285714285715em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">x</span><span class="mord mtight"><span class="mord mtight">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.31731428571428577em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.143em;" class="vlist"><span></span></span></span></span></span></span><span class="mrel mtight">≤</span><span class="mord mathdefault mtight">ϵ</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mop"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm">g</span><span class="mspace"></span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.16044em;" class="vlist"><span></span></span></span></span></span><span class="mspace"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size3">(</span></span><span class="mord">−</span><span class="mspace"></span><span class="mop">lo<span>g</span></span><span class="mspace"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.68889em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">δ</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathcal mtight">N</span></span><span class="mopen mtight">(</span><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathdefault mtight">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.7463142857142857em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault mtight">I</span><span class="mclose mtight">)</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mord"><span class="mord mathbb">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.966em;" class="vlist"><span></span></span></span></span></span></span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.801892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span><span class="mord mathdefault">δ</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.15139200000000003em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.286108em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose delimcenter"><span class="delimsizing size3">)</span></span></span><span class="mspace"></span><span class="mord">.</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 2.3554399999999998em;" class="vlist"><span></span></span></span></span></span></span></span></span></span></span></span>

<p>We will refer to the above as the <strong>SmoothAdv</strong> objective. The <em>SmoothAdv</em> objective is highly non-convex, so as is common in the literature, we will optimize it via projected gradient descent (PGD), and variants thereof. It is hard to find exact gradients for <em>SmoothAdv</em>, so in practice we must use some estimator based on random Gaussian samples.</p>

<h2 id="estimating-the-gradient-of-smoothadv">Estimating the Gradient of <em>SmoothAdv</em></h2>

<p>If we let <span class="katex"><span class="katex-mathml">J(x′)=LCE(G(x′),y)J(x') = L_{\mathrm{CE} } (G (x'), y)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">J</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.751892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">C</span><span class="mord mathrm mtight">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">G</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.751892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span></span> denote the <em>SmoothAdv</em> objective, then</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">∇x′J(x′)=∇x′(−log⁡Eδ∼N(0,σ2I)F(x′+δ)y)  .\nabla_{x'} J(x') = \nabla_{x'} \left( - \log \underset{\delta \sim \mathcal{N}(0, \sigma^2 I)}{\mathbb{E}} F (x' + \delta)_y \right) \; .</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32797999999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.6828285714285715em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mord mathdefault">J</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.801892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32797999999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.6828285714285715em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size3">(</span></span><span class="mord">−</span><span class="mspace"></span><span class="mop">lo<span>g</span></span><span class="mspace"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.68889em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">δ</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathcal mtight">N</span></span><span class="mopen mtight">(</span><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathdefault mtight">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.7463142857142857em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault mtight">I</span><span class="mclose mtight">)</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mord"><span class="mord mathbb">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.966em;" class="vlist"><span></span></span></span></span></span></span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.801892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span><span class="mord mathdefault">δ</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.15139200000000003em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.286108em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose delimcenter"><span class="delimsizing size3">)</span></span></span><span class="mspace"></span><span class="mspace"></span><span class="mord">.</span></span></span></span></span>

<p>However, it is not clear how to evaluate the expectation inside the log exactly, as it takes the form of a complicated high dimensional integral.
Therefore, we will use Monte Carlo approximations.
We sample i.i.d. Gaussians <span class="katex"><span class="katex-mathml">δ1,…,δm∼N(0,σ2I)\delta_1, \ldots, \delta_m \sim \mathcal{N} (0, \sigma^2 I)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace"></span><span class="minner">…</span><span class="mspace"></span><span class="mpunct">,</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.151392em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="mrel">∼</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathcal">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.8141079999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault">I</span><span class="mclose">)</span></span></span></span>, and use the plug-in estimator for the expectation:</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">∇x′J(x′)≈∇x′(−log⁡(1m∑i=1mF(x′+δi)y))  .\nabla_{x'} J(x') \approx \nabla_{x'} \left( - \log \left( \frac{1}{m} \sum_{i = 1}^m  F (x' + \delta_i)_y \right) \right) \; .</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32797999999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.6828285714285715em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mord mathdefault">J</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.801892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace"></span><span class="mrel">≈</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32797999999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.6828285714285715em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size4">(</span></span><span class="mord">−</span><span class="mspace"></span><span class="mop">lo<span>g</span></span><span class="mspace"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size4">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 1.32144em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord mathdefault">m</span></span></span><span><span style="height: 3em;" class="pstrut"></span><span style="border-bottom-width: 0.04em;" class="frac-line"></span></span><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.686em;" class="vlist"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 1.6513970000000002em;" class="vlist"><span><span style="height: 3.05em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span><span style="height: 3.05em;" class="pstrut"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span><span style="height: 3.05em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.277669em;" class="vlist"><span></span></span></span></span></span><span class="mspace"></span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.801892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.31166399999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.15139200000000003em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.286108em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose delimcenter"><span class="delimsizing size4">)</span></span></span><span class="mclose delimcenter"><span class="delimsizing size4">)</span></span></span><span class="mspace"></span><span class="mspace"></span><span class="mord">.</span></span></span></span></span>

<p>It is not hard to see that if <span class="katex"><span class="katex-mathml">FF</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">F</span></span></span></span> is smooth, this estimator will converge to <span class="katex"><span class="katex-mathml">∇x′J(x′)\nabla_{x'} J(x')</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32797999999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.6828285714285715em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mord mathdefault">J</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.751892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> as we take more samples.</p>

<h2 id="smoothadv-is-not-the-naive-objective"><em>SmoothAdv</em> is not the <em>Naive</em> Objective</h2>

<p>We note that <em>SmoothAdv</em> should not be confused with the similar-looking objective</p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">=arg max⁡∥x′−x∥2≤ϵEδ∼N(0,σ2I)LCE(F(x′+δ),y)=arg max⁡∥x′−x∥2≤ϵ Eδ∼N(0,σ2I)[−log⁡F(x′+δ)y]  ,\begin{aligned}
&amp;\phantom{ {}={}} \argmax_{\|x' - x\|_2 \leq \epsilon}
    \underset{\delta \sim \mathcal{N} (0, \sigma^2 I)}{\mathbb{E}} 
        L_{\mathrm{CE} } (F (x' + \delta), y) \\
&amp;= \argmax_{\|x' - x\|_2 \leq \epsilon} 
    \ \underset{\delta \sim \mathcal{N} (0, \sigma^2 I)}{\mathbb{E}} 
        \left[-\log  F(x' + \delta)_y\right] \; ,
\end{aligned}</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 2.55044em;" class="vlist"><span><span style="height: 2.84em;" class="pstrut"></span><span class="mord"></span></span><span><span style="height: 2.84em;" class="pstrut"></span><span class="mord"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 2.05044em;" class="vlist"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 2.55044em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord"></span><span style="color: transparent;" class="mord"></span><span class="mspace"></span><span style="color: transparent;" class="mrel">=</span><span class="mspace"></span><span style="color: transparent;" class="mord"></span><span class="mspace"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.43056000000000016em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∥</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.6828285714285715em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">x</span><span class="mord mtight"><span class="mord mtight">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.31731428571428577em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.143em;" class="vlist"><span></span></span></span></span></span></span><span class="mrel mtight">≤</span><span class="mord mathdefault mtight">ϵ</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mop"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm">g</span><span class="mspace"></span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.16044em;" class="vlist"><span></span></span></span></span></span><span class="mspace"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.68889em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">δ</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathcal mtight">N</span></span><span class="mopen mtight">(</span><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathdefault mtight">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.7463142857142857em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault mtight">I</span><span class="mclose mtight">)</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mord"><span class="mord mathbb">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.966em;" class="vlist"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">C</span><span class="mord mathrm mtight">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.801892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span><span class="mord mathdefault">δ</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord mathdefault">y</span><span class="mclose">)</span></span></span><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord"></span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.43056000000000016em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∥</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.6828285714285715em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">x</span><span class="mord mtight"><span class="mord mtight">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.31731428571428577em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.143em;" class="vlist"><span></span></span></span></span></span></span><span class="mrel mtight">≤</span><span class="mord mathdefault mtight">ϵ</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mop"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm">g</span><span class="mspace"></span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.16044em;" class="vlist"><span></span></span></span></span></span><span class="mspace"></span><span class="mspace"> </span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.68889em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">δ</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathcal mtight">N</span></span><span class="mopen mtight">(</span><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathdefault mtight">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.7463142857142857em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault mtight">I</span><span class="mclose mtight">)</span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span><span class="mop"><span class="mord"><span class="mord mathbb">E</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.966em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="minner"><span class="mopen delimcenter">[</span><span class="mord">−</span><span class="mspace"></span><span class="mop">lo<span>g</span></span><span class="mspace"></span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.801892em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span><span class="mord mathdefault">δ</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.15139200000000003em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.286108em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose delimcenter">]</span></span><span class="mspace"></span><span class="mspace"></span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 2.05044em;" class="vlist"><span></span></span></span></span></span></span></span></span></span></span></span>

<p>where the <span class="katex"><span class="katex-mathml">log⁡\log</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mop">lo<span>g</span></span></span></span></span> and <span class="katex"><span class="katex-mathml">E\mathbb{E}</span><span class="katex-html"><span class="base"><span style="height: 0.68889em; vertical-align: 0em;" class="strut"></span><span class="mord"><span class="mord mathbb">E</span></span></span></span></span> have been swapped compared to <em>SmoothAdv</em>, as suggested in section G.3 of <a href="https://arxiv.org/abs/1902.02918">Cohen et al</a>.
This objective, which we shall call <strong>naive</strong>, is the one that corresponds to finding an adversarial example of <span class="katex"><span class="katex-mathml">FF</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">F</span></span></span></span> that is robust to Gaussian noise.
In contrast, <em>SmoothAdv</em> directly corresponds to finding an adversarial example of <span class="katex"><span class="katex-mathml">GG</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">G</span></span></span></span>.
From this point of view, <em>SmoothAdv</em> is the right optimization problem that should be used to find adversarial examples of <span class="katex"><span class="katex-mathml">GG</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">G</span></span></span></span>. 
This distinction turns out to be crucial in practice: empirically,  <a href="https://arxiv.org/abs/1902.02918">Cohen et al</a> found attacks based on the <em>naive</em> objective not to be effective.
In <a href="https://arxiv.org/abs/1906.04584">our paper</a>, we perform <em>SmoothAdv</em>-attack on <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a>’s smoothed model and find, indeed, that it works better than the <em>Naive</em> objective, and it performs better with more Gaussian noise samples used to estimate its gradient.</p>

<h2 id="adversarially-training-smoothed-classifiers">Adversarially Training Smoothed Classifiers</h2>

<p>We now wish to use our new <em>SmoothAdv</em> attack to boost the adversarial robustness of smoothed classifiers.
As described in the beginning of this blog post, in (ordinary) adversarial training, given a current set of model parameters <span class="katex"><span class="katex-mathml">θt\theta_t</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> and a labeled data point <span class="katex"><span class="katex-mathml">(xt,yt)(x_t, y_t)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>, one finds an adversarial perturbation <span class="katex"><span class="katex-mathml">x^t\hat x_t</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> of <span class="katex"><span class="katex-mathml">xtx_t</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> for the current model, and then takes a gradient step for the model parameters <span class="katex"><span class="katex-mathml">θt\theta_t</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span>, evaluated at the point <span class="katex"><span class="katex-mathml">(x^t,yt)(\hat x_t, y_t)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mopen">(</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>.
Intuitively, this encourages the network to learn to minimize the worst-case loss over a neighborhood around the input.</p>

<p>What is different in our proposed algorithm is that <em>we are finding the adversarial example <span class="katex"><span class="katex-mathml">x^t\hat x_t</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> with respect to the smoothed classifier <span class="katex"><span class="katex-mathml">GG</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">G</span></span></span></span> using the <strong>SmoothAdv</strong> objective</em>, and <em>we are training <span class="katex"><span class="katex-mathml">GG</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">G</span></span></span></span> at this adversarial example <span class="katex"><span class="katex-mathml">x^t\hat x_t</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> with respect to the <strong>SmoothAdv</strong> objective, estimated by the plug-in estimator.</em></p>

<span class="katex-display fleqn"><span class="katex"><span class="katex-mathml">θt+1=θt+η∇θlog⁡(1m′∑i=1m′F(x^t+δi)y),\begin{aligned}
\theta_{t+1} &amp;= \theta_t + \eta \nabla_\theta \log\left(\frac{1}{m'} \sum_{i=1}^{m'} F(\hat x_t + \delta_i)_y\right),
\end{aligned}</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 2.2000199999999994em;" class="vlist"><span><span style="height: 4.05002em;" class="pstrut"></span><span class="mord"><span class="mord"><span class="mord mathdefault">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.301108em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.208331em;" class="vlist"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.70002em;" class="vlist"><span></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 2.2000199999999994em;" class="vlist"><span><span style="height: 4.05002em;" class="pstrut"></span><span class="mord"><span class="mord"></span><span class="mspace"></span><span class="mrel">=</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span><span class="mord mathdefault">η</span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.33610799999999996em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="mop">lo<span>g</span></span><span class="mspace"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 2.05002em;" class="vlist"><span><span style="height: 3.1550000000000002em;" class="pstrut"></span><span class="delimsizinginner delim-size4"><span>⎝</span></span></span><span><span style="height: 3.1550000000000002em;" class="pstrut"></span><span class="delimsizinginner delim-size4"><span>⎛</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.55002em;" class="vlist"><span></span></span></span></span></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 1.32144em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord"><span class="mord mathdefault">m</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.6778919999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span style="border-bottom-width: 0.04em;" class="frac-line"></span></span><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.686em;" class="vlist"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 1.9294850000000003em;" class="vlist"><span><span style="height: 3.05em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span><span style="height: 3.05em;" class="pstrut"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span><span style="height: 3.05em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.8278285714285715em;" class="vlist"><span><span style="height: 2.5em;" class="pstrut"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.277669em;" class="vlist"><span></span></span></span></span></span><span class="mspace"></span><span class="mord mathdefault">F</span><span class="mopen">(</span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.69444em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord mathdefault">x</span></span><span><span style="height: 3em;" class="pstrut"></span><span class="accent-body">^</span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="mbin">+</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.31166399999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.15139200000000003em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.286108em;" class="vlist"><span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 2.05002em;" class="vlist"><span><span style="height: 3.1550000000000002em;" class="pstrut"></span><span class="delimsizinginner delim-size4"><span>⎠</span></span></span><span><span style="height: 3.1550000000000002em;" class="pstrut"></span><span class="delimsizinginner delim-size4"><span>⎞</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.55002em;" class="vlist"><span></span></span></span></span></span></span></span><span class="mspace"></span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 1.70002em;" class="vlist"><span></span></span></span></span></span></span></span></span></span></span></span>

<p>where <span class="katex"><span class="katex-mathml">θt\theta_t</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.2805559999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> are the parameters of <span class="katex"><span class="katex-mathml">FF</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">F</span></span></span></span> at time <span class="katex"><span class="katex-mathml">tt</span><span class="katex-html"><span class="base"><span style="height: 0.61508em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">t</span></span></span></span>,  <span class="katex"><span class="katex-mathml">δi∼N(0,σ2I)\delta_i \sim \mathcal{N}(0, \sigma^2 I)</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.31166399999999994em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span><span class="mspace"></span><span class="mrel">∼</span><span class="mspace"></span></span><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathcal">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace"></span><span class="mord"><span class="mord mathdefault">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span style="height: 0.8141079999999999em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathdefault">I</span><span class="mclose">)</span></span></span></span>, and <span class="katex"><span class="katex-mathml">η\eta</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord mathdefault">η</span></span></span></span> is a learning rate.</p>

<h2 id="results">Results</h2>

<p>Over the course of the blog post, we have introduced several hyperparameters, such as 1) <span class="katex"><span class="katex-mathml">ϵ\epsilon</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">ϵ</span></span></span></span>, the radius of perturbation used for adversarial training, 2) <span class="katex"><span class="katex-mathml">mm</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">m</span></span></span></span>, the number of Gaussian noise samples, 3) <span class="katex"><span class="katex-mathml">σ\sigma</span><span class="katex-html"><span class="base"><span style="height: 0.43056em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">σ</span></span></span></span>, the standard deviation of the Gaussian noise.
We also did not mention other hyperparameters like <span class="katex"><span class="katex-mathml">TT</span><span class="katex-html"><span class="base"><span style="height: 0.68333em; vertical-align: 0em;" class="strut"></span><span class="mord mathdefault">T</span></span></span></span>, the number of iterations used for PGD iterations, or the usage of DDN, an alternative attack to PGD that has been shown to be effective for <span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span>-perturbations (<a href="https://arxiv.org/abs/1811.09600">Rony et al.</a>).
In <a href="https://arxiv.org/abs/1906.04584">our paper</a> we do extensive analysis of the effects of these hyperparameters, to which we refer interested readers.</p>

<p>Taking the max over all such hyperparameter combinations for each <span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> perturbation radius, we obtain the <em>upper envelopes</em> of the certified accuracies of our method vs the <em>upper envelopes</em> of <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> in the tables in the beginning of this post, which we also replicate here for convenience.</p>

<table>
  <thead>
    <tr>
      <th><span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> radius (Imagenet)</th>
      <th>0.5</th>
      <th>1</th>
      <th>1.5</th>
      <th>2</th>
      <th>2.5</th>
      <th>3</th>
      <th>3.5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> (%)</td>
      <td>49</td>
      <td>37</td>
      <td>29</td>
      <td>19</td>
      <td>15</td>
      <td>12</td>
      <td>9</td>
    </tr>
    <tr>
      <td><a href="https://arxiv.org/abs/1906.04584">Ours</a> (%)</td>
      <td>56</td>
      <td>43</td>
      <td>37</td>
      <td>27</td>
      <td>25</td>
      <td>20</td>
      <td>16</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th><span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> radius (CIFAR-10)</th>
      <th>0.25</th>
      <th>0.5</th>
      <th>0.75</th>
      <th>1.0</th>
      <th>1.25</th>
      <th>1.5</th>
      <th>1.75</th>
      <th>2.0</th>
      <th>2.25</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> (%)</td>
      <td>60</td>
      <td>43</td>
      <td>32</td>
      <td>23</td>
      <td>17</td>
      <td>14</td>
      <td>12</td>
      <td>10</td>
      <td>8</td>
    </tr>
    <tr>
      <td><a href="https://arxiv.org/abs/1906.04584">Ours</a> (%)</td>
      <td>74</td>
      <td>57</td>
      <td>48</td>
      <td>38</td>
      <td>33</td>
      <td>29</td>
      <td>24</td>
      <td>19</td>
      <td>17</td>
    </tr>
  </tbody>
</table>

<h1 id="conclusion">Conclusion</h1>

<p>In this blog post, we reviewed adversarial training and randomized smoothing, a recently proposed provable defense.
By adversarially training the smoothed classifier — and carefully getting all the details right — we obtained the state-of-the-art <span class="katex"><span class="katex-mathml">ℓ2\ell_2</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.30110799999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> provable robustness on CIFAR-10 and Imagenet, demonstrating significant improvement over randomized smoothing alone.</p>

<h1 id="acknowledgements">Acknowledgements</h1>

<p>This blog post presented work done by Hadi Salman, Greg Yang, Jerry Li, Huan Zhang, Pengchuan Zhang, Ilya Razenshteyn, and Sebastien Bubeck.
We would like to thank Zico Kolter, Jeremy Cohen, Elan Rosenfeld, Aleksander Madry, Andrew Ilyas, Dimitris Tsipras, Shibani Santurkar, Jacob Steinhardt for comments and discussions during the making of this paper.</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>We actually estimate a lower bound <span class="katex"><span class="katex-mathml">pA‾\underline{p_A}</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord underline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.43056em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span style="border-bottom-width: 0.04em;" class="underline-line"></span></span><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.39443999999999996em;" class="vlist"><span></span></span></span></span></span></span></span></span> of <span class="katex"><span class="katex-mathml">pAp_A</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> and an upper bound <span class="katex"><span class="katex-mathml">pB‾\overline{p_B}</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord overline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.63056em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span style="border-bottom-width: 0.04em;" class="overline-line"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.19444em;" class="vlist"><span></span></span></span></span></span></span></span></span> of <span class="katex"><span class="katex-mathml">pBp_B</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> with high probability, and substitute <span class="katex"><span class="katex-mathml">pA‾\underline{p_A}</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord underline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.43056em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span style="border-bottom-width: 0.04em;" class="underline-line"></span></span><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.39443999999999996em;" class="vlist"><span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml">pB‾\overline{p_B}</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord overline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.63056em;" class="vlist"><span><span style="height: 3em;" class="pstrut"></span><span class="mord"><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span><span><span style="height: 3em;" class="pstrut"></span><span style="border-bottom-width: 0.04em;" class="overline-line"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.19444em;" class="vlist"><span></span></span></span></span></span></span></span></span> for <span class="katex"><span class="katex-mathml">pAp_A</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml">pBp_B</span><span class="katex-html"><span class="base"><span class="strut"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span style="height: 0.32833099999999993em;" class="vlist"><span><span style="height: 2.7em;" class="pstrut"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span style="height: 0.15em;" class="vlist"><span></span></span></span></span></span></span></span></span></span> everywhere. This is an overestimate, so our guarantee holds except for a small probability that the estimates are wrong. See <a href="https://arxiv.org/abs/1902.02918">Cohen et al.</a> or <a href="https://arxiv.org/abs/1906.04584">our paper</a> for more details. <a href="https://decentdescent.org/smoothadv.html#fnref:1" class="reversefootnote">↩</a></p>
    </li>
  </ol>
</div></div>







<p class="date">
<a href="https://decentdescent.org/smoothadv.html"><span class="datestr">at June 13, 2019 02:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15989">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/06/12/how-to-make-a-polynomial-map-nicer/">How To Make A Polynomial Map Nicer</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Stability theory and polynomials</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/06/12/how-to-make-a-polynomial-map-nicer/essen/" rel="attachment wp-att-15990"><img src="https://rjlipton.files.wordpress.com/2019/06/essen.jpeg?w=600" alt="" class="alignright size-full wp-image-15990" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ Essen ]</font></td>
</tr>
</tbody>
</table>
<p>
Arno van den Essen is the author of <b>the</b> book on the Jacobian Conjecture.</p>
<p>
Today I want to highlight one of the ideas he presents in his <a href="https://www.springer.com/gp/book/9783764363505">book</a>.<br />
<span id="more-15989"></span></p>
<p>
The theory is sometimes called stabilization methods. Or K-theory methods. It is often used in connection with the famous Jacobian conjecture (JC). I will not say any more about JC now—see <a href="https://rjlipton.wordpress.com/2014/01/29/progress-on-the-jacobian-conjecture/">this</a> for some comments we made a while ago. </p>
<p>
</p><p></p><h2> The Stability Philosophy </h2><p></p>
<p></p><p>
Essen states that the philosophy of stability theory is: </p>
<blockquote><p><b> </b> <em> <i>It is possible to change a map <img src="https://s0.wp.com/latex.php?latex=%7BF%3A+%5Cmathbb%7BK%7D%5E%7Bn%7D+%5Crightarrow+%5Cmathbb%7BK%7D%5E%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{F: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}" class="latex" title="{F: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}" /> and make it “nicer” provided we allow <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> to be increased</i>.<img src="https://s0.wp.com/latex.php?latex=%7B%5E%7B%5Cdagger%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{^{\dagger}}" class="latex" title="{^{\dagger}}" /> </em>
</p></blockquote>
<p></p><p>
<img src="https://s0.wp.com/latex.php?latex=%5Crule%7B0.4%5Ctextwidth%7D%7B0.4pt%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\rule{0.4\textwidth}{0.4pt}" class="latex" title="\rule{0.4\textwidth}{0.4pt}" /></p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%28%5Cdagger%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\dagger)}" class="latex" title="{(\dagger)}" /> Not a direct quote.</p>
<p>
That is provided we can change <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> to 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ctilde%7BF%7D+%3A+%5Cmathbb%7BK%7D%5E%7BN%7D+%5Crightarrow+%5Cmathbb%7BK%7D%5E%7BN%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \tilde{F} : \mathbb{K}^{N} \rightarrow \mathbb{K}^{N} " class="latex" title="\displaystyle  \tilde{F} : \mathbb{K}^{N} \rightarrow \mathbb{K}^{N} " /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N}" class="latex" title="{N}" /> is larger than <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />. The whole method is based on a simple observation. Suppose that <img src="https://s0.wp.com/latex.php?latex=%7BF%3A+%5Cmathbb%7BK%7D%5E%7Bn%7D+%5Crightarrow+%5Cmathbb%7BK%7D%5E%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}" class="latex" title="{F: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}" /> and define <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> by 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++G%28x_%7B1%7D%2C%5Cdots%2Cx_%7Bn%7D%2Cu_%7B1%7D%2C%5Cdots%2Cu_%7BN-n%7D%29+%3D+%28F%28x_%7B1%7D%2C%5Cdots%2Cx_%7Bn%7D%29%2C+u_%7B1%7D%2C%5Cdots%2Cu_%7BN-n%7D%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  G(x_{1},\dots,x_{n},u_{1},\dots,u_{N-n}) = (F(x_{1},\dots,x_{n}), u_{1},\dots,u_{N-n}). " class="latex" title="\displaystyle  G(x_{1},\dots,x_{n},u_{1},\dots,u_{N-n}) = (F(x_{1},\dots,x_{n}), u_{1},\dots,u_{N-n}). " /></p>
<p>Then <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> is injective if and only if <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> is injective. This is trivial—really trivial. From trivial observations sometimes important methods are created. </p>
<p>
I will now explain how why this is useful by presenting an example. </p>
<p>
</p><p></p><h2> The Method: By Example </h2><p></p>
<p></p><p>
Suppose that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%3A+%5Cmathbb%7BK%7D%5E%7B2%7D+%5Crightarrow+%5Cmathbb%7BK%7D%5E%7B2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  F: \mathbb{K}^{2} \rightarrow \mathbb{K}^{2} " class="latex" title="\displaystyle  F: \mathbb{K}^{2} \rightarrow \mathbb{K}^{2} " /></p>
<p>is a polynomial mapping where 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28x%2Cy%29+%3D+%28f%28x%2Cy%29%2Cg%28x%2Cy%29%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  F(x,y) = (f(x,y),g(x,y)). " class="latex" title="\displaystyle  F(x,y) = (f(x,y),g(x,y)). " /></p>
<p>We wish to show that we can replace <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> by another polynomial map <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\tilde F}" class="latex" title="{\tilde F}" /> that has degree at most <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3}" class="latex" title="{3}" />. Moreover, the new polynomial map is injective if and only if the original polynomial map is injective. The method can be used to preserve other properties of the polynomial mapping, but being injective is a important example. </p>
<p>
There seems to be no way to lower the degree of <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> without destroying its structure. But if we use the stability philosophy and allow extra dimensions we can succeed. That is we replace <img src="https://s0.wp.com/latex.php?latex=%7BF%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F(x,y)}" class="latex" title="{F(x,y)}" /> by the function 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ctilde%7BF%7D%28x%2Cy%2Cu%2Cv%29+%3D+%28f%28x%2Cy%29%2Cg%28x%2Cy%29%2Cu%2Cv%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \tilde{F}(x,y,u,v) = (f(x,y),g(x,y),u,v). " class="latex" title="\displaystyle  \tilde{F}(x,y,u,v) = (f(x,y),g(x,y),u,v). " /></p>
<p>Why does this work? The idea is that the extra two dimensions can be used as extra <i>registers</i>. These registers can be used to simplify the computation, and reduce the degree of <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" />. </p>
<p>
Let <img src="https://s0.wp.com/latex.php?latex=%7Bf%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f(x,y)}" class="latex" title="{f(x,y)}" /> have one term that we wish to remove. To be concrete, let’s assume the term is 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5E%7B3%7Dy.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x^{3}y. " class="latex" title="\displaystyle  x^{3}y. " /></p>
<p>Start with the input 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28x%2Cy%2Cu%2Cv%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  (x,y,u,v). " class="latex" title="\displaystyle  (x,y,u,v). " /></p>
<p>Now change this to 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28x%2Cy%2Cu%2BP%2Cv%2BQ%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  (x,y,u+P,v+Q), " class="latex" title="\displaystyle  (x,y,u+P,v+Q), " /></p>
<p>where 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++P%3Dx%5E%7B2%7D+%5Ctext%7B+and+%7D+Q%3Dxy.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  P=x^{2} \text{ and } Q=xy. " class="latex" title="\displaystyle  P=x^{2} \text{ and } Q=xy. " /></p>
<p>This is an invertible transformation. Note this is only possible because we have two extra dimensions or registers. Otherwise, we could not compute <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q}" class="latex" title="{Q}" /> without messing up the rest of the computation. Now map this to 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28f%28x%2Cy%29%2Cg%28x%2Cy%29%2Cu%2BP%2Cv%2BQ%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  (f(x,y),g(x,y),u+P,v+Q). " class="latex" title="\displaystyle  (f(x,y),g(x,y),u+P,v+Q). " /></p>
<p>This is nothing more than computing the original function and ignoring the new registers. </p>
<p>
The next step is to go to 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28f-%28u%2BP%29%28v%2BQ%29%2Cg%2Cu%2BPv%2BQ%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  (f-(u+P)(v+Q),g,u+Pv+Q). " class="latex" title="\displaystyle  (f-(u+P)(v+Q),g,u+Pv+Q). " /></p>
<p>The last point is that 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f-+%28u%2BP%29%28v%2BQ%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f- (u+P)(v+Q), " class="latex" title="\displaystyle  f- (u+P)(v+Q), " /></p>
<p>cancels the term <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7B3%7Dy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x^{3}y}" class="latex" title="{x^{3}y}" /> we wished to remove. 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f-+PQ+-uQ+-vP-uv+%3D+f-x%5E%7B2%7Dxy+-uxy+-+vx%5E%7B2%7D-uv.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f- PQ -uQ -vP-uv = f-x^{2}xy -uxy - vx^{2}-uv. " class="latex" title="\displaystyle  f- PQ -uQ -vP-uv = f-x^{2}xy -uxy - vx^{2}-uv. " /></p>
<p>The price we pay is that new terms have been added, but they have at most degree <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3}" class="latex" title="{3}" />. </p>
<p>
</p><p></p><h2> The Method: General Case </h2><p></p>
<p></p><p>
We can prove by induction the following general theorem: </p>
<blockquote><p><b>Theorem 1</b> <em> Suppose <img src="https://s0.wp.com/latex.php?latex=%7BF%3A+%5Cmathbb%7BK%7D%5E%7Bn%7D+%5Crightarrow+%5Cmathbb%7BK%7D%5E%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{F: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}" class="latex" title="{F: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}" /> is polynomial map where <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> is a field. Then we can construct a polynomial map of degree at most <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{3}" class="latex" title="{3}" /> denoted by <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde+F%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\tilde F}" class="latex" title="{\tilde F}" /> so that it is injective precisely when <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" /> is injective. </em>
</p></blockquote>
<p></p><p>
Even stronger theorems are possible. For example, the polynomial map <img src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\tilde F}" class="latex" title="{\tilde F}" /> can be required to be cubic linear: </p>
<blockquote><p><b>Definition 2</b> <em> Suppose that <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> is in <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n \times n}" class="latex" title="{n \times n}" /> matrix over the field <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BK%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\mathbb{K}}" class="latex" title="{\mathbb{K}}" />. The <img src="https://s0.wp.com/latex.php?latex=%7BF_%7BA%7D%28X%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{F_{A}(X)}" class="latex" title="{F_{A}(X)}" /> is the <b>cubic linear</b> map for the matrix <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> is defined to be the map <img src="https://s0.wp.com/latex.php?latex=%7BF_%7BA%7D%3A+%5Cmathbb%7BK%7D%5E%7Bn%7D+%5Crightarrow+%5Cmathbb%7BK%7D%5E%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{F_{A}: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}" class="latex" title="{F_{A}: \mathbb{K}^{n} \rightarrow \mathbb{K}^{n}}" /> 	</em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X+%5Crightarrow+X+%2B+%28AX%29%5E%7B%2A3%7D%2C+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="\displaystyle  X \rightarrow X + (AX)^{*3}, " class="latex" title="\displaystyle  X \rightarrow X + (AX)^{*3}, " /></p>
</em><p><em>where <img src="https://s0.wp.com/latex.php?latex=%7BY%5E%7B%2A3%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{Y^{*3}}" class="latex" title="{Y^{*3}}" /> is defined to be the vector <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" /> so that <img src="https://s0.wp.com/latex.php?latex=%7BZ_%7Bk%7D+%3D+Y_%7Bk%7D%5E%7B3%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{Z_{k} = Y_{k}^{3}}" class="latex" title="{Z_{k} = Y_{k}^{3}}" /> for all coordinates <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" />. </em>
</p></blockquote>
<p></p><p>
See Essen’s book for more details. Note a cubic linear map when <img src="https://s0.wp.com/latex.php?latex=%7Bn%3D2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n=2}" class="latex" title="{n=2}" /> is of the form: 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28+x+%2B+%28+ax+%2B+by%29%5E%7B3%7D%2C+y+%2B+%28cx+%2B+dy%29%5E%7B3%7D+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  ( x + ( ax + by)^{3}, y + (cx + dy)^{3} ) " class="latex" title="\displaystyle  ( x + ( ax + by)^{3}, y + (cx + dy)^{3} ) " /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2Cc%2Cd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a,b,c,d}" class="latex" title="{a,b,c,d}" /> are constants. This reduction to cubic linear maps is quite pretty, and requires a clever application of the stabilization method.</p>
<p>
</p><p></p><h2> A Limit of The Method </h2><p></p>
<p></p><p>
The reduction in degree is possible only to degree <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3}" class="latex" title="{3}" />. It cannot be reduced to degree <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> in general. Let’s look at the intuition why this is true. The last step is 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f-+%28u%2BP%29%28v%2BQ%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f- (u+P)(v+Q) " class="latex" title="\displaystyle  f- (u+P)(v+Q) " /></p>
<p>which is 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f-+PQ+-uQ+-vP.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f- PQ -uQ -vP. " class="latex" title="\displaystyle  f- PQ -uQ -vP. " /></p>
<p>Suppose <img src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f}" class="latex" title="{f}" /> has a leading term of degree <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" />. Also suppose that <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" /> has degree <img src="https://s0.wp.com/latex.php?latex=%7Bd_%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d_{P}}" class="latex" title="{d_{P}}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q}" class="latex" title="{Q}" /> has degree <img src="https://s0.wp.com/latex.php?latex=%7Bd_%7BQ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d_{Q}}" class="latex" title="{d_{Q}}" />. Then 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++d+%3D+d_%7BP%7D+%2B+d_%7BQ%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  d = d_{P} + d_{Q} " class="latex" title="\displaystyle  d = d_{P} + d_{Q} " /></p>
<p>since the leading term goes away. But <img src="https://s0.wp.com/latex.php?latex=%7BuQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{uQ}" class="latex" title="{uQ}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BvP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{vP}" class="latex" title="{vP}" /> have degrees <img src="https://s0.wp.com/latex.php?latex=%7Bd_%7BP%7D%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d_{P}+1}" class="latex" title="{d_{P}+1}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bd_%7BQ%7D%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d_{Q}+1}" class="latex" title="{d_{Q}+1}" /> respectively. So to keep <img src="https://s0.wp.com/latex.php?latex=%7Bd_%7BP%7D%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d_{P}+1}" class="latex" title="{d_{P}+1}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bd_%7BQ%7D%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d_{Q}+1}" class="latex" title="{d_{Q}+1}" /> both <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> or less, it follows that <img src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{d}" class="latex" title="{d}" /> can be at most <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />. However, in this case a term of degree <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> is removed and other terms of degree <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> are added. This is not a formal proof that the method cannot reduce the degree to <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />. I do believe that formalized properly it is a theorem that reduction to degree <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" /> is in general impossible. </p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
I like this technology. I wonder if it might be possible to use it on some of our favorite problems. I do like that it conserves invertibility. This seems like it could be related to quantum computing, because of the reversible nature of quantum computing. </p>
<p></p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2019/06/12/how-to-make-a-polynomial-map-nicer/"><span class="datestr">at June 12, 2019 08:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8019500166163846173">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/06/compressing-in-moscow.html">Compressing in Moscow</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div style="clear: both; text-align: center;" class="separator">
</div>
<div style="clear: both; text-align: center;" class="separator">
<a style="clear: right; float: right; margin-bottom: 1em; margin-left: 1em;" href="https://1.bp.blogspot.com/-Bisdr756vxM/XQDylk3YrYI/AAAAAAABozI/M1FgKQZyX08bscqjoy9wmF8BxtXG86RLACLcBGAs/s1600/Vereshchagin.jpg"><img src="https://1.bp.blogspot.com/-Bisdr756vxM/XQDylk3YrYI/AAAAAAABozI/M1FgKQZyX08bscqjoy9wmF8BxtXG86RLACLcBGAs/s1600/Vereshchagin.jpg" border="0" /></a><a style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;" href="https://1.bp.blogspot.com/-G6yjxH8R1pU/XQDylvxG2aI/AAAAAAABozE/P54lQPWWEa46pTgyDEi6QWQOsaWLxP5zwCLcBGAs/s1600/Shen.jpg"><img src="https://1.bp.blogspot.com/-G6yjxH8R1pU/XQDylvxG2aI/AAAAAAABozE/P54lQPWWEa46pTgyDEi6QWQOsaWLxP5zwCLcBGAs/s1600/Shen.jpg" border="0" /></a></div>
<br />
This week finds me in Moscow for a pair of workshops, the <a href="https://mipt.ru/education/chairs/dm/conferences/workshop-june-9-11-moscow-2019.php">Russian Workshop on Complexity and Model Theory</a> and a workshop on <a href="https://www.poncelet.ru/conference/ric">Randomness, Information and Complexity</a>. The latter celebrates the lives of Alexander Shen and Nikolay Vereshchagin on their 60th birthdays.<br />
<br />
Alexander Shen might be best known in computational complexity for his <a href="https://doi.org/10.1145/146585.146613">alternate proof</a> of IP = PSPACE. In 1989, Lund, Fortnow, Karloff and Nisan gave an interactive proof for the permanent, which got the entire polynomial-time hierarchy by Toda's theorem. But we didn't know how to push the protocol to PSPACE, we had a problem keeping degrees of polynomials low. Shamir had the first proof by looking at a specific protocol for PSPACE. Shen had the brilliant but simple idea to use a degree reducing operator, taking the polynomial modulo x<sup>2</sup>-x. The three papers appeared <a href="https://dl.acm.org/citation.cfm?id=146585#prox">back-to-back-to-back</a> in JACM.<br />
<br />
Shen and Vereshchagin though made their careers with their extensive work on Kolmogorov complexity and entropy, often together. Vereshchagin and I have co-authored some papers together during our mutual trips to Amsterdam, including on <a href="http://doi.org/10.1007/11672142_10">Kolmogorov Complexity with Errors</a> and how to <a href="http://doi.org/10.1007/b106485">increase Kolmogorov Complexity</a>. My <a href="https://doi.org/10.1006/jcss.1999.1677">favorite work</a> of Shen and Vereshchagin, which they did with Daniel Hammer and Andrei Romashchenko showed that every linear inequality that holds for entropy also holds for Kolmogorov complexity and vice-versa, the best argument that the two notions of information, one based on distributions, the other based on strings, share strong connections.<br />
<br />
Today is <a href="https://en.wikipedia.org/wiki/Russia_Day">Russia Day</a> that celebrates the reestablishment of Russia out of the Soviet Union in 1990. Just like how the British celebrate their succession from the US in 1776 I guess. But I'm celebrating Russia day by honoring these two great Russians. Congrats Sasha and Kolya!</div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/06/compressing-in-moscow.html"><span class="datestr">at June 12, 2019 04:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/06/11/dancing-arc-quadrilaterals">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/06/11/dancing-arc-quadrilaterals.html">Dancing arc-quadrilaterals</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>Several of my past papers concern Lombardi drawing, which I and my coauthors named after conspiracy-theory artist <a href="https://en.wikipedia.org/wiki/Mark_Lombardi">Mark Lombardi</a>. In this style of drawing, edges are drawn as circular arcs, and must meet at equal angles around every vertex. Not every graph has such a drawing, but many symmetrical graphs do (example below: the smallest <a href="https://en.wikipedia.org/wiki/Zero-symmetric_graph">zero-symmetric graph</a> with only two edge orbits).</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/Two-edge-orbit_GRR.svg" alt="The smallest zero-symmetric graph with only two edge orbits" /></p>

<p>All 2-degenerate graphs do as well; these are the graphs that can be reduced to nothing by removing vertices of degree at most two. And all 3-regular planar graphs have planar drawings in this style; I drew the one below to illustrate <a href="https://en.wikipedia.org/wiki/Grinberg%27s_theorem">Grinberg’s theorem</a>, a necessary condition for Hamiltonicity of planar graphs.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/Grinberg_5CEC_Nonhamiltonian_graph.svg" alt="Grinberg's non-Hamiltonian planar cubic graph with cyclic edge-connectivity five" /></p>

<p>So anyway, my newest arXiv preprint is “Bipartite and series-parallel graphs without planar Lombardi drawings” (<a href="https://arxiv.org/abs/1906.04401">arXiv:1906.04401</a>, to appear at <a href="https://sites.ualberta.ca/~cccg2019/">CCCG</a>). It is about some families of planar graphs that have Lombardi drawings (because they are 2-degenerate) but do not have planar Lombardi drawings. They include planar bipartite graphs like the one below (but with more edges and vertices):</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/nested-K2n.svg" alt="Construction for a family of planar bipartite graphs with no planar Lombardi drawing " /></p>

<p>and embedded series-parallel graphs like the one below (again, with more edges and vertices):</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/nonlom-serpar.svg" alt="Construction for a family of embedded series-parallel graphs with no planar Lombardi drawing" /></p>

<p>The key structures that makes both of these graphs hard to draw in Lombardi style are their yellow-blue quadrilateral faces.
The red parts of the graph are just filler to make these faces have the right angles. The yellow-blue quadrilaterals all share the same two yellow vertices,
and I like to think of them as forming a ring dancing hand-to-hand around these two yellow vertices like <a href="https://en.wikipedia.org/wiki/Dance_(Matisse)">Matisse’s dancers</a>.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/Matisse-Dance.jpg" alt="_La Danse_, Henri Matisse, 1910, from https://en.wikipedia.org/wiki/File:Matissedance.jpg" /></p>

<p>When I wrote about <a href="https://11011110.github.io/blog/2018/12/22/circles-crossing-equal.html">quadrilaterals with circular-arc edges meeting at equal angles at each vertex</a> last December, it was these rings of quadrilaterals I was thinking about. As I wrote in my previous post, each of these quadrilaterals has a circumscribing circle. It’s not hard to make one arc polygon with four equal angles, as sharp as you like. But when the angles get sharp and two of these polygons share two opposite vertices and are packed at a tight angle next to each other (as all the yellow-blue quadrilateral faces in these graphs do) each of the two adjacent quadrilaterals must have a deep pocket into which its neighbor reaches to touch its circumcircle, and this forces them to become quite distorted looking.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/reacharound.svg" alt="Two sharp circular-arc quadrilaterals reach into each others' pockets to touch their circumscribing circles" /></p>

<p>If we think of these two quadrilaterals as dancers, with their heads and toes at the shared points and with their hands free, then both of them have their right hands (from the perspective of the viewer) held high and their left hands held low. Extending the same picture to a complete ring of dancers, each dancer in the ring must be holding their right hand higher than their neighbor to the left. But this obviously can’t extend all the way around the ring, because when you came back to the dancer you started with their hand should be the same height as it was when you started.</p>

<p>This is all very metaphorical but fortunately this intuition can be turned into a mathematical proof that the drawing doesn’t exist. The correct tools for measuring the heights of the quadrilateral vertices turn out to be <a href="https://en.wikipedia.org/wiki/M%C3%B6bius_transformation">Möbius transformations</a> and <a href="https://en.wikipedia.org/wiki/Bipolar_coordinates">bipolar coordinates</a>, a system for assigning coordinates to points in the plane by the angle they make with two fixed points (the two yellow shared vertices of all the quadrilateral faces of our graph) and the ratio of their distances to the fixed points.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2019/apollo.svg" alt="Level sets for bipolar coordinates" /></p>

<p>Möbius transformations preserve the circular-arc shape of our quadrilateral sides, and the angles at which they meet. When we restrict them to the transformations that leave the two poles of the bipolar coordinate system fixed, they act very nicely on the two coordinates, essentially adding fixed amounts to each coordinate. We can use them to transform our quadrilaterals into a more canonical shape and prove that the radius-ratio coordinate increases from one quadrilateral to the next around our ring of quadrilaterals, getting the same contradiction described above and proving that no drawing exists.</p>

<p>The most obvious questions in the same direction that this paper leaves unsolved are: what about series-parallel graphs where you do not have a fixed planar embedding for the graph? And what about outerplanar graphs (either with the outerplanar embedding or without fixing an embedding)? We neither have a method for finding planar Lombardi drawings of all graphs of these types, nor a proof that these drawings do not exist.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/102256946045372078">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/06/11/dancing-arc-quadrilaterals.html"><span class="datestr">at June 11, 2019 09:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://adamdsmith.wordpress.com/?p=602">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/smith.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://adamdsmith.wordpress.com/2019/06/11/tpdp-2019/">TPDP 2019</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
The call for submissions for the latest edition of TPDP (Theory and Practice of Differential Privacy) is out! https://tpdp.cse.buffalo.edu/2019/  The workshop covers work on differential privacy (of course), and more generally on rigorous modeling of, and attacks on, statistical data privacy. The intention is to be inclusive. Submissions are due June 21, 2019. Advertisements</div>







<p class="date">
by adamdsmith <a href="https://adamdsmith.wordpress.com/2019/06/11/tpdp-2019/"><span class="datestr">at June 11, 2019 08:52 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/087">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/087">TR19-087 |  Coin Theorems and the Fourier Expansion | 

	Rohit Agrawal</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this note we compare two measures of the complexity of a class $\mathcal F$ of Boolean functions studied in (unconditional) pseudorandomness: $\mathcal F$'s ability to distinguish between biased and uniform coins (the coin problem), and the norms of the different levels of the Fourier expansion of functions in $\mathcal F$ (the Fourier growth). We show that for coins with low bias $\varepsilon = o(1/n)$, a function's distinguishing advantage in the coin problem is essentially equivalent to $\varepsilon$ times the sum of its level $1$ Fourier coefficients, which in particular shows that known level $1$ and total influence bounds for some classes of interest (such as constant-width read-once branching programs) in fact follow as a black-box from the corresponding coin theorems, thereby simplifying the proofs of some known results in the literature. For higher levels, it is well-known that Fourier growth bounds on all levels of the Fourier spectrum imply coin theorems, even for large $\varepsilon$, and we discuss here the possibility of a converse.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/087"><span class="datestr">at June 11, 2019 02:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://thmatters.wordpress.com/?p=1267">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sigact.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://thmatters.wordpress.com/2019/06/11/wikipedia-edit-a-thon-at-stoc19/">Wikipedia edit-a-thon at STOC’19</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>CATCS is organizing a Wikipedia edit-a-thon at <a href="http://acm-stoc.org/stoc2019/">STOC</a> in Phoenix this year. The goal is to create/edit Wikipedia articles on TCS topics that need improvement. (A crowdsourced list of such topics is maintained <a href="https://docs.google.com/spreadsheets/d/1zVUdxKk9nqR5Itwc37v26aRHMqgV9qI8XexssoFq_CE/edit">here</a>.) The event will be held on June 24th, 2019 in West 104A, starting right after the STOC business meeting around 9-9:30 pm.</p>
<p>We invite members of the community to participate. Prior experience with Wikipedia is a plus, but is not necessary. If you are interested in participating, please fill out <a href="https://forms.gle/ZnPkHN1Wc2edAWhB6">this form</a>. Participants are asked to bring their own laptop or other device. Power outlets will be available. Light refreshments will be provided.</p>
<p>If you are interested in helping improve TCS coverage on Wikipedia but are unable to attend this event, please see <a href="https://thmatters.wordpress.com/2017/05/02/tcs-wikipedia-project/">this post</a> for how you can help.</p>
<p> </p></div>







<p class="date">
by shuchic <a href="https://thmatters.wordpress.com/2019/06/11/wikipedia-edit-a-thon-at-stoc19/"><span class="datestr">at June 11, 2019 01:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15977">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/06/10/net-zero-graphs/">Net-Zero Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc">
<em>A new class of undirected graphs with quantum relevance</em>
<font color="#000000">





</font></font></p><table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/06/10/net-zero-graphs/345px-gustav_robert_kirchhoff/" rel="attachment wp-att-15978"><img src="https://rjlipton.files.wordpress.com/2019/06/345px-gustav_robert_kirchhoff.jpg?w=102&amp;h=156" alt="" width="102" class="alignright wp-image-15978" height="156" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from <a href="https://en.wikipedia.org/wiki/Gustav_Kirchhoff">source</a></font></td>
</tr>
</tbody>
</table><font color="#0044cc"><font color="#000000">



<p>
Gustav Kirchhoff was a German physicist active in the mid-1800s. He is known for many things, especially for his “Laws” governing voltage and current in electrical circuits. Today we ask whether anything akin to Kirchhoff’s laws can be formulated for quantum circuits.

</p><p>
What may be less known about Kirchhoff is that he was a pioneer in graph theory. He proved Kirchhoff’s <a href="https://en.wikipedia.org/wiki/Kirchhoff's_theorem">Theorem</a> that the number of spanning trees equals the determinant of an associated matrix. This shows that the trees can be counted in polynomial time—a cool result. Here we present a class of graphs arising from quantum circuits and associated operations more complex than determinants.

</p><p>
Our search for new graph-based laws was driven by our work on simulations of stabilizer circuits. We have discussed this recently <a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/">here</a> and <a href="https://rjlipton.wordpress.com/2019/06/07/a-rank-problem/">here</a>. The bottom line is this:

</p><p>

</p><blockquote><b> </b> <em> <i>A new class of graphs arises in a natural way and holds a key to improving certain quantum simulations.</i> </em>
</blockquote>


<p>



</p><p>
We call these <i>net-zero graphs</i>. We like to imagine that Kirchhoff would have been interested. We will say more about why after we present the graphs.<span id="more-15977"></span>



</p><p>


</p><p>



</p><p>

</p><h2> Net-Zero Graphs </h2>


<p>



</p><p>
The new class of graphs comes from a natural counting problem. Consider black/white two colorings (not necessarily proper) of the <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> vertices of a graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />, and count the number of edges whose two nodes are both colored black, being called B-B edges. Let <img src="https://s0.wp.com/latex.php?latex=%7Bc_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c_0}" class="latex" title="{c_0}" /> be the count of colorings that make an even number of B-B edges and <img src="https://s0.wp.com/latex.php?latex=%7Bc_1+%3D+2%5En+-+c_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{c_1 = 2^n - c_0}" class="latex" title="{c_1 = 2^n - c_0}" /> be the count of colorings that make an odd number of B-B edges. Then <img src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29+%3D+c_0+-+c_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a(G) = c_0 - c_1}" class="latex" title="{a(G) = c_0 - c_1}" /> divided by <img src="https://s0.wp.com/latex.php?latex=%7B2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^n}" class="latex" title="{2^n}" />. Now we are good to define “Net-Zero” graphs as follows:

</p><p>

</p><blockquote><b> </b> <em> An undirected graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> is net-zero if <img src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29+%3D+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{a(G) = 0}" class="latex" title="{a(G) = 0}" />. </em>
</blockquote>


<p>



</p><p>
Furthermore, we can call <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> <em>net-positive</em> if <img src="https://s0.wp.com/latex.php?latex=%7Ba+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a &gt; 0}" class="latex" title="{a &gt; 0}" /> and <em>net-negative</em> if <img src="https://s0.wp.com/latex.php?latex=%7Ba+%3C+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a &lt; 0}" class="latex" title="{a &lt; 0}" />. By simple trial and error, the smallest net-zero graph is the triangle graph and the graph made by two triangles sharing an edge is net-zero as well. Here are some connected net-zero graphs of small size:

</p><p>



</p><p>
You might ask why study such labelings of graphs? Why is net-zero an interesting property? An equivalent formulation of <img src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a(G)}" class="latex" title="{a(G)}" /> was given as “<img src="https://s0.wp.com/latex.php?latex=%7BZ_%7BH_2%7D%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z_{H_2}(G)}" class="latex" title="{Z_{H_2}(G)}" />” (divided by <img src="https://s0.wp.com/latex.php?latex=%7B2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^n}" class="latex" title="{2^n}" />) in a 2009 <a href="https://arxiv.org/abs/0804.1932">paper</a> by Leslie Goldberg, Martin Grohe, Mark Jerrum, and Marc Thurley, as part of a larger enumeration of polynomial-time cases. Their proof works by reduction to the problem of counting solutions to quadratic polynomials modulo 2, whose time we just <a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/">improved</a> from <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^3)}" class="latex" title="{O(n^3)}" /> to <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" />. Their paper does not mention quantum but does involve Hadamard-type matrices. Thus the short answer is that net-zero captures a type of balancing property that is related to understanding quantum circuits. 

</p><p>
<a href="https://rjlipton.wordpress.com/2019/06/10/net-zero-graphs/netzerographs/" rel="attachment wp-att-15981"><img src="https://rjlipton.files.wordpress.com/2019/06/netzerographs.png?w=550&amp;h=70" alt="" width="550" class="aligncenter wp-image-15981" height="70" /></a>


</p><p>

</p><h2> Some Examples and Facts </h2>


<p>



</p><p>
The following elementary facts show how the theory of our graphs takes shape.

</p><p>

</p><blockquote><b>Proposition 1</b> <em> Every cycle graph <img src="https://s0.wp.com/latex.php?latex=%7BC_n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{C_n}" class="latex" title="{C_n}" /> with <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> odd is net-zero. </em>
</blockquote>


<p>



</p><p>
This follows because every coloring of <img src="https://s0.wp.com/latex.php?latex=%7BC_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_n}" class="latex" title="{C_n}" /> has an even number of B-W edges. Hence the number of monochrome edges is odd, and so complementing the coloring flips the parity between B-B and W-W edges. However, having an odd cycle as an induced graph does not make a graph net-zero. An example of this would be a 4-clique graph. Any subset of vertices of size 3 gives an triangle which is net-zero, but the 4-clique itself is net-negative.

</p><p>

</p><blockquote><b>Proposition 2</b> <em> A graph is net-zero if and only if one of its connected components is net-zero. </em>
</blockquote>


<p>



</p><p>
This fact is intuitive when we look at the graph as a tensor product over the connected components, so the colorings to each component are independent. Every coloring <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> on nodes outside the net-zero connected component can be easily extended to one coloring <img src="https://s0.wp.com/latex.php?latex=%7BR%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R'}" class="latex" title="{R'}" /> for the entire graph by coloring nodes on the net-zero component, so the difference <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a}" class="latex" title="{a}" /> restricted by <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" /> is zero.

</p><p>
Now let’s deviate from net-zero graphs. What are some typical net-positive graphs?

</p><p>

</p><blockquote><b>Proposition 3</b> <em> Bipartite graphs are net-positive. </em>
</blockquote>


<p>



</p><p>
To prove this, let <img src="https://s0.wp.com/latex.php?latex=%7BV_1%2C+V_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V_1, V_2}" class="latex" title="{V_1, V_2}" /> be the two disjoint vertex sets such that each edge connects one node from <img src="https://s0.wp.com/latex.php?latex=%7BV_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V_1}" class="latex" title="{V_1}" /> and one from <img src="https://s0.wp.com/latex.php?latex=%7BV_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V_2}" class="latex" title="{V_2}" />. If all nodes in <img src="https://s0.wp.com/latex.php?latex=%7BV_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V_1}" class="latex" title="{V_1}" /> are set to be white, then there will be zero B-B edges regardless of how <img src="https://s0.wp.com/latex.php?latex=%7BV_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V_2}" class="latex" title="{V_2}" /> is acolored, and <img src="https://s0.wp.com/latex.php?latex=%7Ba+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a &gt; 0}" class="latex" title="{a &gt; 0}" /> in this case. Now if any of the nodes, say <img src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u}" class="latex" title="{u}" />, in <img src="https://s0.wp.com/latex.php?latex=%7BV_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{V_1}" class="latex" title="{V_1}" /> is colored black, then the number of B-B blacks equals the number of nodes connected to <img src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{u}" class="latex" title="{u}" /> that are colored black, and <img src="https://s0.wp.com/latex.php?latex=%7Ba+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a = 0}" class="latex" title="{a = 0}" /> in this situation by straightforward combination calculation. Hence bipartite graphs are net-positive. 

</p><p>
As a consequence, since all trees are bipartite, all trees are net-positive. So is <img src="https://s0.wp.com/latex.php?latex=%7BC_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_n}" class="latex" title="{C_n}" /> for <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> even. Net-negative graphs may seem to be rarer. We invite readers to work out from Pascal’s triangle when the <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" />-clique is net-negative, net-zero, and net-positive. Congruence modulo <img src="https://s0.wp.com/latex.php?latex=%7B8%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{8}" class="latex" title="{8}" /> is involved.

</p><p>
Other interesting examples come from allowing self-loops. The smallest net-zero graph of this kind is a single self-loop. But a 2-node graph with an edge connecting them and two self-loops is net-negative, and so is a graph of two triangles connected by one edge. Pictorially, these two graphs are: 



</p><p>
<a href="https://rjlipton.wordpress.com/2019/06/10/net-zero-graphs/localequiv/" rel="attachment wp-att-15982"><img src="https://rjlipton.files.wordpress.com/2019/06/localequiv.png?w=136&amp;h=120" alt="" width="136" class="aligncenter wp-image-15982" height="120" /></a>

</p><p>
There is a “local equivalence” between a single self-loop and a triangle: Any self-loop in a graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> can be replaced by a triangle using two new vertices, and the resulting graph <img src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G'}" class="latex" title="{G'}" /> will be net-zero if and only if <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> is.

</p><p>



</p><p>

</p><h2> The Quantum Connection </h2>


<p>

There is a special class of quantum circuits that relate closely to graphs. They use just two kinds of quantum gates: Hadamard gate and the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BCZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{CZ}}" class="latex" title="{\mathsf{CZ}}" /> gate. For more on quantum circuits see this elementary <a href="https://rjlipton.wordpress.com/2015/04/08/a-quantum-two-finger-exercise/">post</a> and this more involved <a href="https://rjlipton.wordpress.com/2012/07/08/grilling-quantum-circuits/">post</a>.

</p><p>

</p><blockquote><b>Definition 4</b> <em> Given a graph <img src="https://s0.wp.com/latex.php?latex=%7BG+%3D+%28V%2CE%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{G = (V,E)}" class="latex" title="{G = (V,E)}" />, the corresponding <em>graph state circuit</em> <img src="https://s0.wp.com/latex.php?latex=%7BC_G%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{C_G}" class="latex" title="{C_G}" /> involves <img src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+%7CV%7C%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n = |V|}" class="latex" title="{n = |V|}" /> qubits and consists of: 

<ul> 
<li>
An initial Hadamard gate on each qubit line <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" />. 
</li><li>
For every edge <img src="https://s0.wp.com/latex.php?latex=%7B%28i%2Cj%29+%5Cin+E%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{(i,j) \in E}" class="latex" title="{(i,j) \in E}" />, a <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BCZ%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\mathsf{CZ}}" class="latex" title="{\mathsf{CZ}}" /> gate connecting lines <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{j}" class="latex" title="{j}" />. The order of placing the <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BCZ%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\mathsf{CZ}}" class="latex" title="{\mathsf{CZ}}" /> gates does not matter. 
</li><li>
A closing Hadamard gate on each line <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" />. 
</li></ul>

</em>
</blockquote>


<p>



</p><p>
These circuits are a subset of <em>stabilizer circuits</em>, which we <a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/">have</a> been <a href="https://rjlipton.wordpress.com/2019/06/07/a-rank-problem/">discussing</a>. They become equivalent to stabilizer circuits if we also allow so-called phase gates <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{S}}" class="latex" title="{\mathsf{S}}" /> on single qubits, where they are analogous to a loop or “half-loop” at the corresponding vertex. We will stay with the simpler circuits here. The connection to graphs is expressed by:

</p><p>

</p><blockquote><b>Theorem 5</b> <em><a name="amplitude"></a> For any graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />, <img src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29+%3D+%5Clangle+0%5En+%7C+C_G+%7C+0%5En%5Crangle%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{a(G) = \langle 0^n | C_G | 0^n\rangle}" class="latex" title="{a(G) = \langle 0^n | C_G | 0^n\rangle}" />, that is, the amplitude of measuring an all-zero output given an all-zero input. In particular, <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> is net-zero if and only if <img src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7C+C_G+%7C+0%5En%5Crangle+%3D+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\langle 0^n | C_G | 0^n\rangle = 0}" class="latex" title="{\langle 0^n | C_G | 0^n\rangle = 0}" />. </em>
</blockquote>


<p>



</p><p>



</p><p>

</p><h2> Recognizing Net-Zero Graphs </h2>


<p>



</p><p>
Theorem <a href="https://rjlipton.wordpress.com/feed/#amplitude">5</a> implies that whether a graph is net-zero can be decided in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" /> time. The question is, can we improve the time to <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" />, which for dense graphs means linear in the number of edges? The reason why we want to do so is the following further theorem:

</p><p>


</p><p>

</p><blockquote><b>Theorem 6</b> <em> If net-zero graphs of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> nodes with self-loops allowed are recognizable in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> time, then computing the strong simulation probability <img src="https://s0.wp.com/latex.php?latex=%7B%7C%5Clangle+0%5En+%7C+C+%7C+0%5En%5Crangle%7C%5E2%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{|\langle 0^n | C | 0^n\rangle|^2}" class="latex" title="{|\langle 0^n | C | 0^n\rangle|^2}" /> for quantum stabilizer circuits <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> is <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" />-time equivalent to computing <img src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n \times n}" class="latex" title="{n \times n}" /> matrix rank over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_2%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\mathbb{Z}_2}" class="latex" title="{\mathbb{Z}_2}" />. </em>
</blockquote>


<p>



</p><p>
This is proved in section 5 of our <a href="https://arxiv.org/abs/1904.00101">paper</a>, which has a duality technique for eliminating the self-loops from phase gates that works for the probability but possibly not for the amplitude. Another way of stating our theorem is:

</p><p>

</p><blockquote><b>Theorem 7</b> <em> Given any <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-vertex graph <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" />, we can compute <img src="https://s0.wp.com/latex.php?latex=%7Bp%28G%29+%3D+a%28G%29%5E2%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{p(G) = a(G)^2}" class="latex" title="{p(G) = a(G)^2}" /> in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> time given only the rank of the adjacency matrix of <img src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{G}" class="latex" title="{G}" /> and the yes/no answer about whether <img src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29+%3D+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{a(G) = 0}" class="latex" title="{a(G) = 0}" />. </em>
</blockquote>


<p>



</p><p>
This result extends to computing the probability of any output of a stabilizer circuit given a standard-basis input. This is why the decision problem for recognizing net-zero graphs is important.

</p><p>
In upcoming posts we will connect net-zero graphs further to ideas of circuit “laws” by defining recursions for <img src="https://s0.wp.com/latex.php?latex=%7Ba%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a(G)}" class="latex" title="{a(G)}" />. These recursions do not give efficient algorithms by themselves, but they connect to a wide theory involving graph polynomials and matroids. That theory includes Kirchhoff’s counting of spanning trees as a special case, and we will be interested in which other cases are polynomial-time feasible. This may position quantum computing as a meeting point for closer connections between work such as this 1997 <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.51.2071">paper</a> by Andrei Broder and Ernst Mayr on counting minimum-weight spanning trees in <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^\omega)}" class="latex" title="{O(n^\omega)}" /> time and the paper by Goldberg et al. mentioned above.

</p><p>


</p><p>



</p><p>

</p><h2> Open Problems </h2>


<p>



</p><p>
What is the complexity of deciding whether a given <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-vertex graph is net-zero? We know it is at worst order-<img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^\omega}" class="latex" title="{n^\omega}" />. If it is <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" />, then we obtain a really tight connection between computing matrix rank and computing a quantum simulation probability.

</p><p>
Are there further applications of net-zero graphs?

</p><p>
[gave Kirchhoff his second “h”]



</p><ul class="wp-block-gallery columns-0 is-cropped"></ul></font></font></div>







<p class="date">
by Chaowen Guan and K.W. Regan <a href="https://rjlipton.wordpress.com/2019/06/10/net-zero-graphs/"><span class="datestr">at June 10, 2019 10:23 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7515">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2019/06/10/intro-tcs-rebooted/">Intro-TCS rebooted</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>This Spring and Summer I am doing some major editing to my text on <a href="https://introtcs.org">introduction to theoretical computer science</a>. I am adding figures (176 so far and counting..), examples, exercises, simplifying explanations, reducing footnotes, and mainly trying to make it more “user friendly” and less “idiosyncratic”. I am now adding in all chapters figures such as the following that outline the main results and how they are connected to one another.<br /></p>



<div class="wp-block-image"><figure class="aligncenter"><img src="https://windowsontheory.files.wordpress.com/2019/06/codedataoverview.png?w=600" alt="Overview of the results in &quot;Code and Data&quot; chapter presenting universal circuit and the counting lower bound" class="wp-image-7519" /><em>Overview of the results in <a href="https://introtcs.org/public/lec_04_code_and_data.html">“Code and Data” chapter</a> presenting universal circuit and the counting lower bound</em></figure></div>



<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2019/06/chaploopoverview.png?w=600" alt="Overview of the results in the &quot;loops and infinity&quot; chapter defining Turing Machines. A running theme in the book is the emphasis on distinguishing specification (the mathematical function being computed) from implementation (the algorithm, machine, or program doing the computation)." class="wp-image-7520" /><em>Overview of the results in the <a href="https://introtcs.org/public/lec_06_loops.html">“loops and infinity” chapter</a> defining Turing Machines. A running theme in the book is the emphasis on distinguishing <strong>specification</strong> (the mathematical function being computed) from <strong>implementation</strong> (the algorithm, machine, or program doing the computation).</em></figure>



<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2019/06/universalchapoverview.png?w=600" alt="Overview of the results in the chapter on universality and uncomputability. We use Sipser's metaphor on reductions as transforming a &quot;pig that can whistle&quot; (e.g., an algorithm for the deciding if a function halts on the zero input) into a &quot;horse that can fly&quot; (e.g., an algorithm for the general halting problem). " class="wp-image-7522" /><em>Overview of the results in the chapter on <a href="https://introtcs.org/public/lec_08_uncomputability.html">universality and uncomputability</a>. We use Sipser’s metaphor on reductions as transforming a “pig that can whistle” (e.g., an algorithm for the deciding if a function halts on the zero input) into a “horse that can fly” (e.g., an algorithm for the general halting problem). </em></figure>



<p><br /><br /><br />Specifically, in the previous version of the book I used <em>programming languages </em>as the main computational models. While I still think this is the right way if we were to “start from scratch”, these idiosyncratic models made it harder for students to use other resources such as textbooks and lecture notes. They also make it more difficult for instructors to use individual chapters in their courses without committing to using the full book.</p>



<p>Hence in the new revision the standard models of <strong>Turing Machines</strong> and <strong>Boolean Circuits</strong> are front and center. We do talk about the programming-language equivalents as well, since I think they are important for the connection to practice and some concepts such as the duality of code and data are better explained in these terms.  I also use the programming-language variants to <a href="https://github.com/boazbk/tcscode">demonstrate concepts to students in code</a> including compilers from circuits to straightline programs, various “syntactic sugar” transformations, and the Cook-Levin Theorem and NP reductions. </p>



<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2019/06/aoncircequiv.png?w=600" alt="" class="wp-image-7521" /><em>An equivalent description of a finite computation using straight-line programs and Boolean circuits. The <a href="https://nbviewer.jupyter.org/github/boazbk/tcscode/blob/master/Chap_03_Computation.ipynb">supplementary code</a> contains a Python implementation of the transformation between these two representations.</em></figure>



<p><br /><br />One thing did not change – we still start with <em>Boolean Circuits </em>rather than automata as the initial computational model. Boolean circuits are closer to actual implementations of computing, are a finite (and hence simpler) model, but one that is non-trivial enough to allow showing some important theorems early in the course including existence of a circuit for computing every finite function, the existence of a circuit to evaluate other circuits, and the counting lower bound as well as the counting lower bound. </p>



<p>Circuits are also crucial for later material in the course since they make the proof of the Cook-Levin Theorem much simpler and cleaner,  allow talking about results such as <img src="https://s0.wp.com/latex.php?latex=BPP+%5Csubseteq+P_%7B%2Fpoly%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="BPP \subseteq P_{/poly}" class="latex" title="BPP \subseteq P_{/poly}" /> and Sipser-Gacs, and are crucial to be even able to state results in advanced topics such as derandomization, cryptography, and quantum computing.</p>



<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2019/06/3sat2isreduction.png?w=600" alt="" class="wp-image-7523" /><em>Reduction of SAT to independent set from <a href="https://introtcs.org/public/lec_12_NP.html">Chapter 13</a> in the book. On the right is the Python code implementing the reduction, on the left is the resulting independent set.</em></figure>



<figure class="wp-block-image"><img src="https://windowsontheory.files.wordpress.com/2019/06/indsetfromnandsat.png?w=600" alt="" class="wp-image-7524" /><em>An instance of independent set obtained by chaining together the proof of the Cook-Levin Theorem together with the reduction of 3SAT to independent set. Figure taken from <a href="https://introtcs.org/public/lec_13_Cook_Levin.html">Chapter 14</a>.</em></figure>



<p><br /><br />We do cover automata as well, including the equivalence of regular expressions and deterministic finite automata. We also cover context-free grammars (though not pushdown automata) and the λ calculus, including its equivalence with Turing Machines and the Y combinator (see also <a href="https://nbviewer.jupyter.org/github/boazbk/tcscode/blob/master/lambda_calculus.ipynb">this notebook</a>)</p>



<p>I have also done some work on the technical side of producing the book. The book is written in markdown. Markdown has many advantages but it wasn’t designed for 600-page technical books full of equations and cross-references so I did need to use some extensions to it. I am using pandoc (and my own <a href="https://github.com/boazbk/tcs/blob/master/scripts/book-filter.py">filter</a>) to produce both the HTML and LaTeX/PDF versions of the book. </p>



<p>There is still more work to do. I plan to add a chapter on space complexity and on proofs and computation (including both interactive and zero knowledge proofs, as well as the “propositions as types” correspondence between proofs and programs). I need to add more examples and exercises. There are also still several chapters where the text is “rough around the edges”.</p>



<p>As usual, the latest version of the book is available on  <a href="https://introtcs.org/">https://introtcs.org</a>  . If you see any typo, problem, etc.., please post an issue on the <a href="https://github.com/boazbk/tcs/issues">GitHub repository</a>  (you can also make a <a href="https://github.com/boazbk/tcs/pulls">pull request</a> for small typo fixes if you prefer)</p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2019/06/10/intro-tcs-rebooted/"><span class="datestr">at June 10, 2019 05:47 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://blogs.princeton.edu/imabandit/?p=1359">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/bubeck.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://blogs.princeton.edu/imabandit/2019/06/10/amazing-progress-in-adversarially-robust-stochastic-multi-armed-bandits/">Amazing progress in adversarially robust stochastic multi-armed bandits</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>In this post I briefly discuss some recent stunning progress on robust bandits (for more background on bandits see these two posts, <a href="https://blogs.princeton.edu/imabandit/2016/05/11/bandit-theory-part-i/" class="liinternal">part 1</a> and <a href="https://blogs.princeton.edu/imabandit/2016/05/13/bandit-theory-part-ii/" class="liinternal">part 2</a>, in particular what is described below gives a solution to Open Problem 3 at the end of part 2).</p>
<p> </p>
<p><strong>Stochastic bandit and adversarial examples</strong></p>
<p>In multi-armed bandit problems the gold standard property, going back to <a href="https://core.ac.uk/download/pdf/82425825.pdf" class="lipdf">a seminal paper</a> of Lai and Robbins in 1985 is to have a regret upper bounded by:</p>
<p style="line-height: 50px;" class="ql-center-displayed-equation"><span class="ql-right-eqno"> (1) </span><span class="ql-left-eqno">   </span><img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-2ac69fee3f1e4890b09202da232941f6_l3.png?resize=91%2C50&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="50" width="91" alt="\begin{equation*} \sum_{i \neq i^*} \frac{\log(T)}{\Delta_i} \,. \end{equation*}" class="ql-img-displayed-equation " /></p>
<p>Let me unpack this a bit: this is for the scenario where the reward process for each action is simply an i.i.d. sequence from some fixed distribution, <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9af0b76a90462b68c1d83fca9cc6604d_l3.png?resize=12%2C13&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="13" width="12" alt="i^*" class="ql-img-inline-formula " /> is the index of the (unique) best action, and <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9ca6025283881ee4c29a1f9f236d72ba_l3.png?resize=20%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="15" width="20" alt="\Delta_i" class="ql-img-inline-formula " /> is the gap between the mean value of the best action and the one of <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-8511b1f6cf9db17d46ddabb67bac99f5_l3.png?resize=6%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="6" alt="i" class="ql-img-inline-formula " />. Such guarantee is extremely strong, as in particular it means that actions whose average performance is a constant away from the optimal arm are very rarely played (only of order <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b7ebb3614adc5ab9694e195d68bdbd06_l3.png?resize=49%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="49" alt="\log(T)" class="ql-img-inline-formula " />). On the other hand, the price to pay for such an aggressive behavior (by this I mean focusing on good actions very quickly) is that all the classical algorithms attaining the above bound are extremely sensitive to <em>adversarial examples</em>: that is if there is some deviation from the i.i.d. assumption (even very brief in time), the algorithms can suddenly suffer linear in <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-58f18d11e5ffdd11dd9095c427922c8b_l3.png?resize=13%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="13" alt="T" class="ql-img-inline-formula " /> regret.</p>
<p> </p>
<p><strong>Adversarial bandit</strong></p>
<p>Of course there is an entirely different line of works, on <em>adversarial multi-armed bandits</em>, where the whole point is to prove regret guarantee for <em>any</em> reward process. In this case the best one can hope for is a regret of order <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-13ba497e024059e3674272b6a0e11809_l3.png?resize=44%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="44" alt="\sqrt{K T}" class="ql-img-inline-formula " />. The classical algorithm in this model, Exp3, attains a regret of order <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-fc756065e4451f8bccc02a40b732c969_l3.png?resize=103%2C22&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="22" width="103" alt="\sqrt{K T \log(K)}" class="ql-img-inline-formula " />. In joint work with Jean-Yves Audibert we showed <a href="http://sbubeck.com/COLT09_AB.pdf" class="lipdf">back in 2009</a> that the following strategy, which we called PolyINF, attains the optimal <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3cf8321c07956bc2bd76714eabd30a0a_l3.png?resize=44%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="44" alt="\sqrt{KT}" class="ql-img-inline-formula " />: view Exp3 as Mirror Descent with the (negative) entropy as a regularizer, and now replace the entropy by a simple rational function namely <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ac7b6228954b4b6f9ceee99faf3d7776_l3.png?resize=77%2C24&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="24" width="77" alt="- \sum_{i=1}^K x_i^p" class="ql-img-inline-formula " /> with <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-de24a5fa6badcd9e8c80a7d72640dcc6_l3.png?resize=70%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="70" alt="p \in (0,1)" class="ql-img-inline-formula " /> (this mirror descent view was actually derived in <a href="https://pubsonline.informs.org/doi/10.1287/moor.2013.0598" class="liinternal">a later paper</a> with Gabor Lugosi). The proof becomes one line (given the appropriate knowledge of mirror descent and estimation in bandit games): the radius part of the bound is of the form <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-a3d6ffad60b5151ac3866f9fa9213b65_l3.png?resize=120%2C27&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="27" width="120" alt="\frac{1}{\eta} \sum_{i=1}^K x_i^p \leq \frac{K^{p}}{\eta}" class="ql-img-inline-formula " />, while the variance is of the form (since the inverse of the Hessian of the mirror map is a diagonal matrix with entries <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-0b3188b0bc4d641381b7ba4be936c230_l3.png?resize=35%2C23&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="23" width="35" alt="x_i^{2-p}" class="ql-img-inline-formula " />):</p>
<p style="line-height: 46px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5356f94049f314f34752afbac2277f16_l3.png?resize=177%2C46&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="46" width="177" alt="\[ \eta \sum_{i=1} x_i^{2-p} \frac{1}{x_i} \leq \eta K^{1-p} \,. \]" class="ql-img-displayed-equation " /></p>
<p>Thus optimizing over <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-86bde2afc6c0858d01ac505267801f02_l3.png?resize=9%2C12&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="12" width="9" alt="\eta" class="ql-img-inline-formula " /> yields <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-13ba497e024059e3674272b6a0e11809_l3.png?resize=44%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="44" alt="\sqrt{K T}" class="ql-img-inline-formula " /> for any <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-de24a5fa6badcd9e8c80a7d72640dcc6_l3.png?resize=70%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="70" alt="p \in (0,1)" class="ql-img-inline-formula " />. Interestingly, the best numerical constant in the bound is obtained for <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d9aaa949f328021d5d7cbd31573c7a0e_l3.png?resize=60%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="60" alt="p=1/2" class="ql-img-inline-formula " />.</p>
<p> </p>
<p><strong>Best of both worlds</strong></p>
<p>This was the state of affairs back in 2011, when with Alex Slivkins we started working on a <em>best of both worlds</em> type algorithm (which in today’s language is exactly a stochastic MAB robust to adversarial examples): namely one that gets the <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b7ebb3614adc5ab9694e195d68bdbd06_l3.png?resize=49%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="49" alt="\log(T)" class="ql-img-inline-formula " /> guarantee (in fact <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-00cc550d710d51cc64227da3b639018e_l3.png?resize=56%2C20&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="20" width="56" alt="\log^2(T)" class="ql-img-inline-formula " /> in <a href="http://sbubeck.com/COLT12_BS.pdf" class="lipdf">our original paper</a>) if the environment is the nice i.i.d. one, and also <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-13ba497e024059e3674272b6a0e11809_l3.png?resize=44%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="44" alt="\sqrt{K T}" class="ql-img-inline-formula " /> (in fact <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-4d85e820be8841c565042231480be3ac_l3.png?resize=107%2C33&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="33" width="107" alt="\sqrt{K T \log^3(T)}" class="ql-img-inline-formula " />) in the worst case. This original best of both worlds algorithm was of the following form: be aggressive as if it was a stochastic environment, but still sample sufficiently often the bad actions to make sure there isn’t an adversary trying to hide some non-stochastic behavior on these seemingly bad performing actions. Of course the whole difficulty was to show that it is possible to implement such a defense without hurting the stochastic performance too much (remember that bad actions can only be sampled of order <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b7ebb3614adc5ab9694e195d68bdbd06_l3.png?resize=49%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="49" alt="\log(T)" class="ql-img-inline-formula " /> times!). Since this COLT 2012 paper there has been many improvements to the original strategy, as well as many variants/refinements (one such variant worth mentioning are the works trying to do a smooth transition between the stochastic and adversarial models, see e.g. <a href="https://arxiv.org/abs/1803.09353" class="liinternal">here</a> and <a href="https://arxiv.org/abs/1902.08647" class="liinternal">here</a>).</p>
<p> </p>
<p><strong>A stunning twist</strong></p>
<p>The amazing development that I want to talk about in this post is the following: <a href="https://arxiv.org/abs/1807.07623" class="liinternal">about a year ago</a> Julian Zimmert and Yevgeny Seldin proved that the 2009 PolyINF (crucially with <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d9aaa949f328021d5d7cbd31573c7a0e_l3.png?resize=60%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="60" alt="p=1/2" class="ql-img-inline-formula " />) strategy actually gets the 2012 best of both worlds bound! This is truly surprising, as in principle mirror descent does not “know” anything about stochastic environments, it does not make any sophisticated concentration reasoning (say as in Lai and Robbins), yet it seems to automatically and optimally pick up on the regularity in the data. This is really amazing to me, and of course also a total surprise that the polynomial regularizer has such strong adaptivity property, while it was merely introduced to remove a benign log term.</p>
<p>The crucial observation of Zimmert and Seldin is that a a certain <em>self-bounding</em> property of the regret implies (in a one-line calculation) the best of both worlds result:</p>
<blockquote><p><strong>Lemma 1:</strong> Consider a strategy whose regret with respect to the optimal action <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9af0b76a90462b68c1d83fca9cc6604d_l3.png?resize=12%2C13&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="13" width="12" alt="i^*" class="ql-img-inline-formula " /> is upper bounded by</p>
<p style="line-height: 56px;" class="ql-center-displayed-equation"><span class="ql-right-eqno"> (2) </span><span class="ql-left-eqno">   </span><img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-58ac492ce88752df4dbc5bccc5f7c9ff_l3.png?resize=129%2C56&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="56" width="129" alt="\begin{equation*} C \sum_{t=1}^T \sum_{i \neq i^*} \sqrt{\frac{x_{i,t}}{t}} \,. \end{equation*}" class="ql-img-displayed-equation " /></p>
<p>(Recall that for multi-armed bandit one selects a probability distribution <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ae5b82123ba785d8c8153a037675bc56_l3.png?resize=15%2C11&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="11" width="15" alt="x_t" class="ql-img-inline-formula " /> over the actions, so <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-65be0e3045aefe9ef0dd0bd9531c0572_l3.png?resize=24%2C14&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="14" width="24" alt="x_{i,t}" class="ql-img-inline-formula " /> denote here the probability of playing action <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-8511b1f6cf9db17d46ddabb67bac99f5_l3.png?resize=6%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="6" alt="i" class="ql-img-inline-formula " /> at time <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-6b97bb0f65c75b6cc0fba1868749478d_l3.png?resize=6%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="6" alt="t" class="ql-img-inline-formula " />.) Then one has that the regret is in fact bounded by <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-6dc96815294ab91f00db7fc32adcc459_l3.png?resize=68%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="68" alt="2 C \sqrt{K T}" class="ql-img-inline-formula " /> (this follows trivially by Jensen on the <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-8511b1f6cf9db17d46ddabb67bac99f5_l3.png?resize=6%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="12" width="6" alt="i" class="ql-img-inline-formula " /> sum), and moreover if the environment is stochastic one has that the regret is in fact bounded by <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e4b609168e948e0de36c4f357b700408_l3.png?resize=21%2C15&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="15" width="21" alt="C^2" class="ql-img-inline-formula " /> times <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5048b7c449e02713d954071d1a80df0f_l3.png?resize=21%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="21" alt="(1)" class="ql-img-inline-formula " />.</p></blockquote>
<p><em>Proof:</em> Assuming that the environment is stochastic we can write the regret as <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-1e451f2e1cd6b24d6f4ceb19592378a8_l3.png?resize=80%2C21&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="21" width="80" alt="\sum_{i,t} \Delta_i x_{i,t}" class="ql-img-inline-formula " />, so by assumption and using that <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-37d8baadc3f1897c077c0f1bc6da832f_l3.png?resize=205%2C33&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="33" width="205" alt="C \sqrt{\frac{x_{i,t}}{t}} \leq \frac{1}{2} \left( \Delta_i x_{i,t} + \frac{C^2}{t \Delta_i} \right)" class="ql-img-inline-formula " /> one has:</p>
<p style="line-height: 52px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-19259c6dbd863aa77917fbddee306ca7_l3.png?resize=295%2C52&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="52" width="295" alt="\[ \sum_{i \neq i^*,t} \Delta_i x_{i,t} \leq \frac{1}{2} \sum_{i \neq i^*,t} \left(\Delta_i x_{i,t} + \frac{C^2}{t \Delta_i} \right) \,, \]" class="ql-img-displayed-equation " /></p>
<p>which means that the left hand side is smaller than <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3a170cbde4999b8a9aaab3ebc7698acb_l3.png?resize=82%2C26&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="26" width="82" alt="\sum_{i \neq i^*,t} \frac{C^2}{t \Delta_i}" class="ql-img-inline-formula " /> which is indeed smaller than <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e4b609168e948e0de36c4f357b700408_l3.png?resize=21%2C15&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="15" width="21" alt="C^2" class="ql-img-inline-formula " /> times <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5048b7c449e02713d954071d1a80df0f_l3.png?resize=21%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="21" alt="(1)" class="ql-img-inline-formula " />.</p>
<p> </p>
<p><strong>Yet another one-line proof (okay, maybe 5 lines)</strong></p>
<p>Zimmert and Seldin proved that PolyINF with <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d9aaa949f328021d5d7cbd31573c7a0e_l3.png?resize=60%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="60" alt="p=1/2" class="ql-img-inline-formula " /> actually satisfies the self-bounding property of Lemma 1 (and thus obtains the best of both worlds guarantee). In <a href="https://arxiv.org/abs/1901.08779" class="liinternal">another recent paper</a> by Zimmert, in joint work with Haipeng Luo and Chen-Yu Wei, they simplify the analysis by using a very mild variation of the PolyINF regularizer, namely <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-83b29164db23b237061cd205df4a585d_l3.png?resize=183%2C24&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="24" width="183" alt="- \sum_{i=1}^K (\sqrt{x_i} + \sqrt{1-x_i})" class="ql-img-inline-formula " />. In my view it’s the proof from the book for the best of both worlds result (or very close to it)! Here it is:</p>
<blockquote><p><strong>Lemma 2:</strong> Equation <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-8a996880dc323ee4b8cf323009c02635_l3.png?resize=21%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="21" alt="(2)" class="ql-img-inline-formula " /> with <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ae0daffd6d9302e8baf8d6432a69c5ca_l3.png?resize=56%2C13&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="13" width="56" alt="C=10" class="ql-img-inline-formula " /> is an upper bound on the regret of mirror descent with learning <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-27f9c61a5d83fe6d2fca62581c4d1b6f_l3.png?resize=57%2C27&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="27" width="57" alt="\eta_t = \frac{1}{\sqrt{t}}" class="ql-img-inline-formula " />, mirror map <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e28928d4c9de69492e5f786c52e80f3e_l3.png?resize=245%2C24&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="24" width="245" alt="\Phi(x) = - \sum_{i=1}^K (\sqrt{x_i} + \sqrt{1-x_i})" class="ql-img-inline-formula " />, and standard multi-armed bandit loss estimator.</p></blockquote>
<p><em>Proof:</em> The classical mirror descent analysis from any good book will tell you that the regret is controlled by (for <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-f1d27d479269ff49c66d302354f84569_l3.png?resize=144%2C24&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="24" width="144" alt="\Phi(x) = \sum_{i=1}^K \phi(x_i)" class="ql-img-inline-formula " /> and with the convention <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-db68debdd0cb300a9b79aed311c05d28_l3.png?resize=71%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="15" width="71" alt="\eta_0 = + \infty" class="ql-img-inline-formula " />):</p>
<p style="line-height: 54px;" class="ql-center-displayed-equation"><span class="ql-right-eqno"> (3) </span><span class="ql-left-eqno">   </span><img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d3de9d0c55a8d8aca491d928972530ed_l3.png?resize=437%2C54&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="54" width="437" alt="\begin{equation*} \sum_{t=1}^T \left(\frac{1}{\eta_t} - \frac{1}{\eta_{t-1}}\right) (\Phi(x^*) - \Phi(x_t)) + \sum_{t=1}^T \eta_t \sum_{i=1}^K \frac{1}{x_{i,t} \phi''(x_t)} \,. \end{equation*}" class="ql-img-displayed-equation " /></p>
<p>We now consider those terms for the specific <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-8581dbc45f448345dcc6bd9caed502e9_l3.png?resize=12%2C14&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="14" width="12" alt="\Phi" class="ql-img-inline-formula " /> and <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-f489909a6f16a3b5a6ee23901d48d9b6_l3.png?resize=14%2C12&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="12" width="14" alt="\eta_t" class="ql-img-inline-formula " /> suggested in the lemma. First notice that <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-7f8f2af48129af0fd204515d14d0f6f5_l3.png?resize=105%2C25&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="25" width="105" alt="\frac{1}{\eta_t} - \frac{1}{\eta_{t-1}} \leq \eta_t" class="ql-img-inline-formula " />. Moreover <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e59356dfe693c9414f1e407c034a4a4d_l3.png?resize=88%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="19" width="88" alt="\phi(x^*_i) = - 1" class="ql-img-inline-formula " /> (since <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-1b92fd5178a7e53e81d04a267f41c9d8_l3.png?resize=16%2C13&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" height="13" width="16" alt="x^*" class="ql-img-inline-formula " /> is integral) so that <img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-a13944e230b8362a3c996cdb9d42f634_l3.png?resize=298%2C22&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="22" width="298" alt="\phi(x^*_i) - \phi(x_{i,t}) \leq \min(\sqrt{x_{i,t}}, \sqrt{1-x_{i,t}})" class="ql-img-inline-formula " />. In other words the first term in <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c9264e4ae133cc333e4e98394c7e0656_l3.png?resize=21%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="21" alt="(3)" class="ql-img-inline-formula " /> is upper bounded by</p>
<p style="line-height: 57px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c048790fe35996074b87ea676060d91d_l3.png?resize=350%2C57&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="57" width="350" alt="\[ \sum_{t=1}^T \sum_{i=1}^K \sqrt{\frac{\min(x_{i,t}, 1-x_{i,t})}{t}} \leq 2 \sum_{t=1}^T \sum_{i \neq i^*} \sqrt{\frac{x_{i,t}}{t}} \, \]" class="ql-img-displayed-equation " /></p>
<p>where the inequality simply comes from <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-01c4dcfc661631d69e71024a0340222d_l3.png?resize=307%2C33&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="33" width="307" alt="\sqrt{1-x_{i^*,t}} = \sqrt{\sum_{i \neq i^*} x_{i,t}} \leq \sum_{i \neq i^*} \sqrt{x_{i,t}}" class="ql-img-inline-formula " />.</p>
<p>Next note that <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-83fb6f1ccbc0d202e6020dc0884a140f_l3.png?resize=358%2C28&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="28" width="358" alt="\phi''(s) = \frac{1}{4} (s^{-3/2} + (1-s)^{-3/2}) \geq \frac{1}{4 \min(s,1-s)^{3/2}}" class="ql-img-inline-formula " />, so that the second term in <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c9264e4ae133cc333e4e98394c7e0656_l3.png?resize=21%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="21" alt="(3)" class="ql-img-inline-formula " /> is upper bounded by</p>
<p style="line-height: 57px;" class="ql-center-displayed-equation"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-98a3e5f8a743148f5f6919b1dd53a8b9_l3.png?resize=442%2C57&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="57" width="442" alt="\[ 4 \sum_{t=1}^T \sum_{i=1}^K \sqrt{\frac{x_{i,t}}{t}} \min(1, (1-x_{i,t})/x_{i,t})^{3/2} \leq 8 \sum_{t=1}^T \sum_{i \neq i^*} \sqrt{\frac{x_{i,t}}{t}} \, \]" class="ql-img-displayed-equation " /></p>
<p>where the inequality follows trivially by considering the two cases whether <img src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-0bcf369611e215890249a1b1bb94089d_l3.png?resize=31%2C14&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="14" width="31" alt="x_{i^*,t}" class="ql-img-inline-formula " /> is smaller or larger than <img src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-1c652ece8cc629e4e659c41eeed4d410_l3.png?resize=25%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" height="18" width="25" alt="1/2" class="ql-img-inline-formula " />.</p></div>







<p class="date">
by Sebastien Bubeck <a href="https://blogs.princeton.edu/imabandit/2019/06/10/amazing-progress-in-adversarially-robust-stochastic-multi-armed-bandits/"><span class="datestr">at June 10, 2019 03:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://ptreview.sublinear.info/?p=1126">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://ptreview.sublinear.info/?p=1126">News for May 2019</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>We were able to find four new papers for May 2019 — as usual, please let us know if we missed any!<br />EDIT: We did, in fact, miss one paper, which is the bottom one listed below.</p>



<p><strong>On Local Testability in the Non-Signaling Setting</strong>, by Alessandro Chiesa, Peter Manohar, and Igor Shinkar (<a href="https://eccc.weizmann.ac.il/report/2019/070/">ECCC</a>).  This paper studies testability of a certain generalization of (distributions over) functions, known as \(k\)-non-signalling functions, objects which see use in hardness of approximation and delegation of computation. Prior work by the authors show the effectiveness of the linearity test in this setting, leading to the design of PCPs. On the other hand, in this work, the authors show that two types of bivariate tests are ineffective in revealing low-degree structure of these objects.</p>



<p><strong>Computing and Testing Small Vertex Connectivity in Near-Linear Time and Queries</strong>, by Danupon Nanongkai, Thatchaphol Saranurak, and Sorrachai Yingchareonthawornchai (<a href="https://arxiv.org/abs/1905.05329">arXiv</a>). This work, apparently simultaneous with the one by Forster and Yang that we covered <a href="https://ptreview.sublinear.info/?p=1116">last month</a>, also studies the problem of locally computing cuts in a graph. The authors also go further, and study approximation algorithms for the same problems. Inspired by the connections to property testing in the work of Forster and Yang, they apply these approximation algorithms to get even more query-efficient algorithms for the problems of testing \(k\)-edge- and \(k\)-vertex-connectivity.</p>



<p><strong>Testing Graphs against an Unknown Distribution</strong>, by Lior Gishboliner and Asaf Shapira (<a href="https://arxiv.org/abs/1905.09903">arXiv</a>). This paper studies graph property testing, under the vertex-distribution-free (VDF) model, as <a href="http://www.wisdom.weizmann.ac.il/~oded/p_vdf.html">recently introduced</a> by Goldreich. In the VDF model, rather than the ability to sample a random node, the algorithm has the ability to sample a node from some unknown distribution, and must be accurate with respect to the same distribution (reminiscent of the PAC learning model). In Goldreich’s work, it was shown that every property which is testable in the VDF model is semi-hereditary. This work strengthens this statement and proves a converse, thus providing a characterization: a property is testable in the VDF model if and only if it is both hereditary and extendable. These descriptors roughly mean that the property is closed under both removal and addition of nodes (with the choice of addition of edges in the latter case). This is a far simpler characterization than that of properties which are testable in the standard model, which is a special case of the VDF model.</p>



<p><strong>Private Identity Testing for High-Dimensional Distributions</strong>, by Clément L. Canonne, Gautam Kamath, Audra McMillan, Jonathan Ullman, and Lydia Zakynthinou (<a href="https://arxiv.org/abs/1905.11947">arXiv</a>). This work continues a recent line on distribution testing under the constraint of differential privacy. The settings of interest are multivariate distributions: namely, product distributions over the hypercube and Gaussians with identity covariance. An application of a statistic of <a href="https://arxiv.org/abs/1612.03156">CDKS</a>, combined with a Lipschitz extension from the set of datasets likely to be generated by such structured distributions, gives a sample-efficient algorithm. A time-efficient version of this extension is also provided, at the cost of some loss in the sample complexity. Some tools of independent interest include reductions between Gaussian mean and product uniformity testing, balanced product identity to product uniformity testing, and an equivalence between univariate and “extreme” product identity testing. </p>



<p><strong>Testing Bipartitness in an Augmented VDF Bounded-Degree Graph Model</strong>, by Oded Goldreich (<a href="https://arxiv.org/abs/1905.03070">arXiv</a>). Another work on the vertex-distribution-free (VDF) model, as described above. In this one, Goldreich considers an augmentation of the model, where the algorithm is further allowed to query the probability of each node. With this augmentation, he gives \(\tilde O(\sqrt{n})\)-time algorithms for testing bipartiteness and cycle-free-ness, where \(n\) is the “effective support” of the distribution. That is, \(n\) is the number of nodes in the graph after discarding the nodes with minimal probability until \(\varepsilon/5\) mass is removed.</p></div>







<p class="date">
by Gautam Kamath <a href="https://ptreview.sublinear.info/?p=1126"><span class="datestr">at June 10, 2019 07:38 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/086">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/086">TR19-086 |  Perfect zero knowledge for quantum multiprover interactive proofs | 

	Alex Bredariol Grilo, 

	William Slofstra, 

	Henry Yuen</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this work we consider the interplay between multiprover interactive proofs, quantum
entanglement, and zero knowledge proofs — notions that are central pillars of complexity theory,
quantum information and cryptography. In particular, we study the relationship between the
complexity class MIP$^*$ , the set of languages decidable by multiprover interactive proofs with
quantumly entangled provers, and the class PZK-MIP$^*$ , which is the set of languages decidable
by MIP$^*$ protocols that furthermore possess the perfect zero knowledge property.

Our main result is that the two classes are equal, i.e., MIP$^*$ = PZK-MIP$^*$ . This result provides
a quantum analogue of the celebrated result of Ben-Or, Goldwasser, Kilian, and Wigderson (STOC
1988) who show that MIP = PZK-MIP (in other words, all classical multiprover interactive
protocols can be made zero knowledge). We prove our result by showing that every MIP$^*$
protocol can be efficiently transformed into an equivalent zero knowledge MIP$^*$ protocol in a
manner that preserves the completeness-soundness gap. Combining our transformation with
previous results by Slofstra (Forum of Mathematics, Pi 2019) and Fitzsimons, Ji, Vidick and
Yuen (STOC 2019), we obtain the corollary that all co-recursively enumerable languages (which
include undecidable problems as well as all decidable problems) have zero knowledge MIP$^*$
protocols with vanishing promise gap.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/086"><span class="datestr">at June 09, 2019 10:46 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3399">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2019/06/09/max-plank-summer-school-games-brains-and-distributed-computing/">Max Plank Summer School: Games, Brains, and Distributed Computing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The 20th <a href="http://resources.mpi-inf.mpg.de/conferences/adfocs/">Max Planck Advanced Course on the Foundations of Computer Science focused on “Games, Brains, and Distributed Computing”</a> will take place on 19 – 23 August 2019, Saarbrücken, Germany.</p>
<p> </p>
<p>The instructors are Christos Papadimitriou, Eva Tardos, and Pierre Fraigniaud.</p></div>







<p class="date">
by algorithmicgametheory <a href="https://agtb.wordpress.com/2019/06/09/max-plank-summer-school-games-brains-and-distributed-computing/"><span class="datestr">at June 09, 2019 07:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
