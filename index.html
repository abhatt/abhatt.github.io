<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://www.blogger.com/feeds/25562705/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://benjamin-recht.github.io/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/?tag=tcs&amp;feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://www.blogger.com/feeds/21224994/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="http://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="http://learningwitherrors.org/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://learningwitherrors.org" title="Learning With Errors">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://www.blogger.com/feeds/27705661/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://www.blogger.com/feeds/32902056/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://kintali.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kintali.wordpress.com" title="My Brain is Open">Shiva Kintali</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at May 01, 2019 12:51 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.13369">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.13369">Constrained Orthogonal Segment Stabbing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bandyapadhyay:Sayan.html">Sayan Bandyapadhyay</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mehrabi:Saeed.html">Saeed Mehrabi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.13369">PDF</a><br /><b>Abstract: </b>Let $S$ and $D$ each be a set of orthogonal line segments in the plane. A
line segment $s\in S$ \emph{stabs} a line segment $s'\in D$ if $s\cap
s'\neq\emptyset$. It is known that the problem of stabbing the line segments in
$D$ with the minimum number of line segments of $S$ is NP-hard. However, no
better than $O(\log |S\cup D|)$-approximation is known for the problem. In this
paper, we introduce a constrained version of this problem in which every
horizontal line segment of $S\cup D$ intersects a vertical line. We study
several versions of the problem, depending on which line segments are used for
stabbing and which line segments must be stabbed. We obtain several NP-hardness
and constant approximation results for these versions. Our finding implies, the
problem remains NP-hard even under the extra assumption on input, but small
constant approximation algorithms can be designed.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.13369"><span class="datestr">at May 01, 2019 12:49 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.13210">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.13210">A Classification of Topological Discrepancies in Additive Manufacturing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Behandish:Morad.html">Morad Behandish</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mirzendehdel:Amir_M=.html">Amir M. Mirzendehdel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nelaturi:Saigopal.html">Saigopal Nelaturi</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.13210">PDF</a><br /><b>Abstract: </b>Additive manufacturing (AM) enables enormous freedom for design of complex
structures. However, the process-dependent limitations that result in
discrepancies between as-designed and as-manufactured shapes are not fully
understood. The tradeoffs between infinitely many different ways to approximate
a design by a manufacturable replica are even harder to characterize. To
support design for AM (DfAM), one has to quantify local discrepancies
introduced by AM processes, identify the detrimental deviations (if any) to the
original design intent, and prescribe modifications to the design and/or
process parameters to countervail their effects. Our focus in this work will be
on topological analysis. There is ample evidence in many applications that
preserving local topology (e.g., connectivity of beams in a lattice) is
important even when slight geometric deviations can be tolerated. We first
present a generic method to characterize local topological discrepancies due to
material under- and over-deposition in AM, and show how it captures various
types of defects in the as-manufactured structures. We use this information to
systematically modify the as-manufactured outcomes within the limitations of
available 3D printer resolution(s), which often comes at the expense of
introducing more geometric deviations (e.g., thickening a beam to avoid
disconnection). We validate the effectiveness of the method on 3D examples with
nontrivial topologies such as lattice structures and foams.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.13210"><span class="datestr">at May 01, 2019 12:49 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12804">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12804">The I/O complexity of hybrid algorithms for square matrix multiplication</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stefani:Lorenzo_De.html">Lorenzo De Stefani</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12804">PDF</a><br /><b>Abstract: </b>Asymptotically tight lower bounds are derived for the I/O complexity of a
general class of hybrid algorithms computing the product of $n \times n$ square
matrices combining ``\emph{Strassen-like}'' fast matrix multiplication approach
with computational complexity $\Theta{n^{\log_2 7}}$, and ``\emph{standard}''
matrix multiplication algorithms with computational complexity
$\Omega\left(n^3\right)$. We present a novel and tight
$\Omega\left(\left(\frac{n}{\max\{\sqrt{M},n_0\}}\right)^{\log_2
7}\left(\max\{1,\frac{n_0}{M}\}\right)^3M\right)$ lower bound for the I/O
complexity a class of ``\emph{uniform, non-stationary}'' hybrid algorithms when
executed in a two-level storage hierarchy with $M$ words of fast memory, where
$n_0$ denotes the threshold size of sub-problems which are computed using
standard algorithms with algebraic complexity $\Omega\left(n^3\right)$.
</p>
<p>The lower bound is actually derived for the more general class of
``\emph{non-uniform, non-stationary}'' hybrid algorithms which allow recursive
calls to have a different structure, even when they refer to the multiplication
of matrices of the same size and in the same recursive level, although the
quantitative expressions become more involved. Our results are the first I/O
lower bounds for these classes of hybrid algorithms. All presented lower bounds
apply even if the recomputation of partial results is allowed and are
asymptotically tight.
</p>
<p>The proof technique combines the analysis of the Grigoriev's flow of the
matrix multiplication function, combinatorial properties of the encoding
functions used by fast Strassen-like algorithms, and an application of the
Loomis-Whitney geometric theorem for the analysis of standard matrix
multiplication algorithms.
</p>
<p>Extensions of the lower bounds for a parallel model with $P$ processors are
also discussed.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12804"><span class="datestr">at May 01, 2019 12:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12777">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12777">Pushing Lines Helps: Efficient Universal Centralised Transformations for Programmable Matter</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Almethen:Abdullah.html">Abdullah Almethen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Michail:Othon.html">Othon Michail</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Potapov:Igor.html">Igor Potapov</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12777">PDF</a><br /><b>Abstract: </b>In this paper, we study a discrete system of entities residing on a
two-dimensional square grid. Each entity is modelled as a node occupying a
distinct cell of the grid. The set of all $n$ nodes forms initially a connected
shape $A$. Entities are equipped with a linear-strength pushing mechanism that
can push a whole line of entities, from 1 to $n$, in parallel in a single
time-step. A target connected shape $B$ is also provided and the goal is to
\emph{transform} $A$ into $B$ via a sequence of line movements. Existing models
based on local movement of individual nodes, such as rotating or sliding a
single node, can be shown to be special cases of the present model, therefore
their (inefficient, $\Theta(n^2)$) \emph{universal transformations} carry over.
Our main goal is to investigate whether the parallelism inherent in this new
type of movement can be exploited for efficient, i.e., sub-quadratic
worst-case, transformations. As a first step towards this, we restrict
attention solely to centralised transformations and leave the distributed case
as a direction for future research. Our results are positive. By focusing on
the apparently hard instance of transforming a diagonal $A$ into a straight
line $B$, we first obtain transformations of time $O(n\sqrt{n})$ without and
with preserving the connectivity of the shape throughout the transformation.
Then, we further improve by providing two $O(n\log n)$-time transformations for
this problem. By building upon these ideas, we first manage to develop an
$O(n\sqrt{n})$-time universal transformation. Our main result is then an $ O(n
\log n) $-time universal transformation. We leave as an interesting open
problem a suspected $\Omega(n\log n)$-time lower bound.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12777"><span class="datestr">at May 01, 2019 12:46 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12728">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12728">Accurate MapReduce Algorithms for $k$-median and $k$-means in General Metric Spaces</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Alessio Mazzetto, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pietracaprina:Andrea.html">Andrea Pietracaprina</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pucci:Geppino.html">Geppino Pucci</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12728">PDF</a><br /><b>Abstract: </b>Center-based clustering is a fundamental primitive for data analysis and
becomes very challenging for large datasets. In this paper, we focus on the
popular $k$-median and $k$-means variants which, given a set $P$ of points from
a metric space and a parameter $k&lt;|P|$, require to identify a set $S$ of $k$
centers minimizing respectively the sum of the distances and of the squared
distances of all points in $P$ from their closest centers. Our specific focus
is on general metric spaces for which it is reasonable to require that the
centers belong to the input set (i.e., $S \subseteq P$). We present
coreset-based 2-round distributed approximation algorithms for the above
problems using the MapReduce computational model. The algorithms are rather
simple and obliviously adapt to the intrinsic complexity of the dataset,
captured by the doubling dimension $D$ of the metric space. Remarkably, the
algorithms attain approximation ratios that can be made arbitrarily close to
those achievable by the best known polynomial-time sequential approximations,
and they are very space efficient for small $D$, requiring local memories sizes
substantially sublinear in the input size. To the best of our knowledge, no
previous distributed approaches were able to attain similar quality-performance
guarantees in general metric spaces.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12728"><span class="datestr">at April 30, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12682">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12682">An efficient branch-and-cut algorithm for approximately submodular function maximization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Uematsu:Naoya.html">Naoya Uematsu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Umetani:Shunji.html">Shunji Umetani</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kawahara:Yoshinobu.html">Yoshinobu Kawahara</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12682">PDF</a><br /><b>Abstract: </b>When approaching to problems in computer science, we often encounter
situations where a subset of a finite set maximizing some utility function
needs to be selected. Some of such utility functions are known to be
approximately submodular. For the problem of maximizing an approximately
submodular function (ASFM problem), a greedy algorithm quickly finds good
feasible solutions for many instances while guaranteeing
($1-e^{-\gamma}$)-approximation ratio for a given submodular ratio $\gamma$.
However, we still encounter its applications that ask more accurate or exactly
optimal solutions within a reasonable computation time. In this paper, we
present an efficient branch-and-cut algorithm for the non-decreasing ASFM
problem based on its binary integer programming (BIP) formulation with an
exponential number of constraints. To this end, we first derive a BIP
formulation of the ASFM problem and then, develop an improved constraint
generation algorithm that starts from a reduced BIP problem with a small subset
of constraints and repeats solving the reduced BIP problem while adding a
promising set of constraints at each iteration. Moreover, we incorporate it
into a branch-and-cut algorithm to attain good upper bounds while solving a
smaller number of nodes of a search tree. The computational results for three
types of well-known benchmark instances show that our algorithm performs better
than the conventional exact algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12682"><span class="datestr">at May 01, 2019 12:48 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12596">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12596">Graph Planarity Testing with Hierarchical Embedding Constraints</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liotta:Giuseppe.html">Giuseppe Liotta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rutter:Ignaz.html">Ignaz Rutter</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tappini:Alessandra.html">Alessandra Tappini</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12596">PDF</a><br /><b>Abstract: </b>Hierarchical embedding constraints define a set of allowed cyclic orders for
the edges incident to the vertices of a graph. These constraints are expressed
in terms of FPQ-trees. FPQ-trees are a variant of PQ-trees that includes
F-nodes in addition to P- and to Q-nodes. An F-node represents a permutation
that is fixed, i.e., it cannot be reversed. Let $G$ be a graph such that every
vertex of $G$ is equipped with a set of FPQ-trees encoding hierarchical
embedding constraints for its incident edges. We study the problem of testing
whether $G$ admits a planar embedding such that, for each vertex $v$ of $G$,
the cyclic order of the edges incident to $v$ is described by at least one of
the FPQ-trees associated with $v$. We prove that the problem is NP-complete
even when the number of FPQ-trees associated with each vertex is bounded by a
constant. If however the branchwidth of $G$ is bounded, the problem can be
solved in polynomial time. Besides being interesting on its own right, the
study of planarity testing with hierarchical embedding constraints can be used
to address other planarity testing problems which can be modeled by associating
a set of FPQ-trees to the vertices of the input graph. As a proof of concept,
we apply our techniques to the study of NodeTrix planarity testing of clustered
graphs. We show that NodeTrix planarity testing with fixed sides is
fixed-parameter tractable when parameterized by the size of the clusters and by
the tree-width of the multi-graph obtained by collapsing the clusters to single
vertices.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12596"><span class="datestr">at April 30, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12503">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12503">Solving Vertex Cover in Polynomial Time on Hyperbolic Random Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bl=auml=sius:Thomas.html">Thomas Bläsius</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fischbeck:Philipp.html">Philipp Fischbeck</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Friedrich:Tobias.html">Tobias Friedrich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Katzmann:Maximilian.html">Maximilian Katzmann</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12503">PDF</a><br /><b>Abstract: </b>The VertexCover problem is proven to be computationally hard in different
ways: It is NP-complete to find an optimal solution and even NP-hard to find an
approximation with reasonable factors. In contrast, recent experiments suggest
that on many real-world networks the run time to solve VertexCover is way
smaller than even the best known FPT-approaches can explain. Similarly, greedy
algorithms deliver very good approximations to the optimal solution in
practice, although VertexCover is NP-hard to approximate within reasonable
factors.
</p>
<p>We link these observations to two properties that are observed in many
real-world networks, namely a heterogeneous degree distribution and high
clustering. To formalize these properties and explain the observed behavior, we
analyze how a branch-and-reduce algorithm performs on hyperbolic random graphs,
which have become increasingly popular for modeling real-world networks. In
fact, we are able to show that the VertexCover problem on hyperbolic random
graphs can be solved in polynomial time, with high probability.
</p>
<p>The proof relies on interesting structural properties of hyperbolic random
graphs. Since these predictions of the model are interesting in their own
right, we conducted experiments on real-world networks showing that these
properties are also observed in practice. When utilizing the same structural
properties in an adaptive greedy algorithm, further experiments suggest that
this leads to even better approximations than the standard greedy approach on
real instances.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12503"><span class="datestr">at April 30, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12500">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12500">Composing dynamic programming tree-decomposition-based algorithms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Baste:Julien.html">Julien Baste</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12500">PDF</a><br /><b>Abstract: </b>Given two integers $\ell$ and $p$ as well as $\ell$ graph classes
$\mathcal{H}_1,\ldots,\mathcal{H}_\ell$, the problems
$\mathsf{GraphPart}(\mathcal{H}_1, \ldots, \mathcal{H}_\ell,p)$,
$\mathsf{VertPart}(\mathcal{H}_1, \ldots, \mathcal{H}_\ell)$, and
$\mathsf{EdgePart}(\mathcal{H}_1, \ldots, \mathcal{H}_\ell)$ ask, given graph
$G$ as input, whether $V(G)$, $V(G)$, $E(G)$ respectively can be partitioned
into $\ell$ sets $S_1, \ldots, S_\ell$ such that, for each $i$ between $1$ and
$\ell$, $G[V_i] \in \mathcal{H}_i$, $G[V_i] \in \mathcal{H}_i$, $(V(G),S_i) \in
\mathcal{H}_i$ respectively. Moreover in $\mathsf{GraphPart}(\mathcal{H}_1,
\ldots, \mathcal{H}_\ell,p)$, we request that the number of edges with
endpoints in different sets of the partition is bounded by $p$. We show that if
there exist dynamic programming tree-decomposition-based algorithms for
recognizing the graph classes $\mathcal{H}_i$, for each $i$, then we can
constructively create a dynamic programming tree-decomposition-based algorithms
for $\mathsf{GraphPart}(\mathcal{H}_1, \ldots, \mathcal{H}_\ell,p)$,
$\mathsf{VertPart}(\mathcal{H}_1, \ldots, \mathcal{H}_\ell)$, and
$\mathsf{EdgePart}(\mathcal{H}_1, \ldots, \mathcal{H}_\ell)$. We show that, in
some known cases, the obtained running times are comparable to those of the
best know algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12500"><span class="datestr">at May 01, 2019 12:35 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12467">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12467">A Fast Scalable Heuristic for Bin Packing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Divakaran:Srikrishnan.html">Srikrishnan Divakaran</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12467">PDF</a><br /><b>Abstract: </b>In this paper we present a fast scalable heuristic for bin packing that
partitions the given problem into identical sub-problems of constant size and
solves these constant size sub-problems by considering only a constant number
of bin configurations with bounded unused space. We present some empirical
evidence to support the scalability of our heuristic and its tighter empirical
analysis of hard instances due to improved lower bound on the necessary wastage
in an optimal solution.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12467"><span class="datestr">at May 01, 2019 12:46 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12427">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12427">Improved Dynamic Graph Coloring</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Solomon:Shay.html">Shay Solomon</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wein:Nicole.html">Nicole Wein</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12427">PDF</a><br /><b>Abstract: </b>This paper studies the fundamental problem of graph coloring in fully dynamic
graphs. Since the problem of computing an optimal coloring, or even
approximating it to within $n^{1-\epsilon}$ for any $\epsilon &gt; 0$, is NP-hard
in static graphs, there is no hope to achieve any meaningful computational
results for general graphs in the dynamic setting. It is therefore only natural
to consider the combinatorial aspects of dynamic coloring, or alternatively,
study restricted families of graphs.
</p>
<p>Towards understanding the combinatorial aspects of this problem, one may
assume a black-box access to a static algorithm for $C$-coloring any subgraph
of the dynamic graph, and investigate the trade-off between the number of
colors and the number of recolorings per update step. In WADS'17, Barba et al.
devised two complementary algorithms: For any $\beta &gt; 0$ the first
(respectively, second) maintains an $O(C \beta n^{1/\beta})$ (resp., $O(C
\beta)$)-coloring while recoloring $O(\beta)$ (resp., $O(\beta n^{1/\beta})$)
vertices per update. Our contribution is two-fold:
</p>
<p>- We devise a new algorithm for general graphs that improves significantly
upon the first trade-off in a wide range of parameters: For any $\beta &gt; 0$, we
get a $\tilde{O}(\frac{C}{\beta}\log^2 n)$-coloring with $O(\beta)$ recolorings
per update, where the $\tilde{O}$ notation supresses polyloglog$(n)$ factors.
In particular, for $\beta=O(1)$ we get constant recolorings with polylog$(n)$
colors; this is an exponential improvement over the previous bound.
</p>
<p>- For uniformly sparse graphs, we use low out-degree orientations to
strengthen the above result by bounding the update time of the algorithm rather
than the number of recolorings. Then, we further improve this result by
introducing a new data structure that refines bounded out-degree edge
orientations and is of independent interest.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12427"><span class="datestr">at May 01, 2019 12:35 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12370">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12370">Compact Fenwick trees for dynamic ranking and selection</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Stefano Marchini, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vigna:Sebastiano.html">Sebastiano Vigna</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12370">PDF</a><br /><b>Abstract: </b>The Fenwick tree is a classical implicit data structure that stores an array
in such a way that modifying an element, accessing an element, computing a
prefix sum and performing a predecessor search on prefix sums all take
logarithmic time. We introduce a number of variants which improve the classical
implementation of the tree: in particular, we can reduce its size when an upper
bound on the array element is known, and we can perform much faster predecessor
searches. Our aim is to use our variants to implement an efficient dynamic bit
vector: our structure is able to perform updates, ranking and selection in
logarithmic time, with a space overhead in the order of a few percents,
outperforming existing data structures with the same purpose. Along the way, we
highlight the pernicious interplay between the arithmetic behind the Fenwick
tree and the structure of current CPU caches, suggesting simple solutions that
improve performance significantly.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12370"><span class="datestr">at April 30, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12337">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12337">Efficient Black-Box Identity Testing over Free Group Algebra</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>V. Arvind, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chatterjee:Abhranil.html">Abhranil Chatterjee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Datta:Rajit.html">Rajit Datta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mukhopadhyay:Partha.html">Partha Mukhopadhyay</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12337">PDF</a><br /><b>Abstract: </b>Hrube\v{s} and Wigderson [HW14] initiated the study of noncommutative
arithmetic circuits with division computing a noncommutative rational function
in the free skew field, and raised the question of rational identity testing.
It is now known that the problem can be solved in deterministic polynomial time
in the white-box model for noncommutative formulas with inverses, and in
randomized polynomial time in the black-box model [GGOW16, IQS18, DM18], where
the running time is polynomial in the size of the formula. The complexity of
identity testing of noncommutative rational functions remains open in general
(when the formula size is not polynomially bounded). We solve the problem for a
natural special case. We consider polynomial expressions in the free group
algebra $\mathbb{F}\langle X, X^{-1}\rangle$ where $X=\{x_1, x_2, \ldots,
x_n\}$, a subclass of rational expressions of inversion height one. Our main
results are the following. 1. Given a degree $d$ expression $f$ in
$\mathbb{F}\langle X, X^{-1}\rangle$ as a black-box, we obtain a randomized
$\text{poly}(n,d)$ algorithm to check whether $f$ is an identically zero
expression or not. We obtain this by generalizing the Amitsur-Levitzki theorem
[AL50] to $\mathbb{F}\langle X, X^{-1}\rangle$. This also yields a
deterministic identity testing algorithm (and even an expression reconstruction
algorithm) that is polynomial time in the sparsity of the input expression. 2.
Given an expression $f$ in $\mathbb{F}\langle X, X^{-1}\rangle$ of degree at
most $D$, and sparsity $s$, as black-box, we can check whether $f$ is
identically zero or not in randomized $\text{poly}(n,\log s, \log D)$ time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12337"><span class="datestr">at April 30, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12334">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12334">Tight FPT Approximations for $k$-Median and $k$-Means</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cohen=Addad:Vincent.html">Vincent Cohen-Addad</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Anupam.html">Anupam Gupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Amit.html">Amit Kumar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lee:Euiwoong.html">Euiwoong Lee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jason.html">Jason Li</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12334">PDF</a><br /><b>Abstract: </b>We investigate the fine-grained complexity of approximating the classical
$k$-median / $k$-means clustering problems in general metric spaces. We show
how to improve the approximation factors to $(1+2/e+\varepsilon)$ and
$(1+8/e+\varepsilon)$ respectively, using algorithms that run in
fixed-parameter time. Moreover, we show that we cannot do better in FPT time,
modulo recent complexity-theoretic conjectures.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12334"><span class="datestr">at April 30, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12258">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12258">Generalizing the Covering Path Problem on a Grid</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zeng:Liwei.html">Liwei Zeng</a>, Karen Smilowitz, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chopra:Sunil.html">Sunil Chopra</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12258">PDF</a><br /><b>Abstract: </b>We study the covering path problem on a grid of R^{2}. We generalize earlier
results on a rectangular grid and prove that the covering path cost can be
bounded by the area and perimeter of the grid. We provide (2+\epsilon) and
(1+\epsilon)-approximations for the problem on a general grid and on a convex
grid, respectively.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12258"><span class="datestr">at May 01, 2019 12:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12217">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12217">A computational model for analytic column stores</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rozenberg:Eyal.html">Eyal Rozenberg</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12217">PDF</a><br /><b>Abstract: </b>This work presents an abstract model for the computations performed by
analytic column stores or columnar query processors. The model is based on
circuits whose wires carry columns rather than scalar values, and whose nodes
apply operators with column inputs and outputs. This model allows expression of
most of the architectural features of existing column-store DBMSes through
columnar execution plans, rather than such features being implemented
sui-generis, and without the column store maintaining significant out-of-plan
data. A strict adherence to columnarity allows for a relatively simple and
robust model; enabling extensive and intensive optimization of almost all
aspects of query processing; and also enabling massive uniform parallelization
of query process on modern hardware. Moreover, the computational model's
expressivity makes it useful also as an \emph{analytical} tool for considering
design aspects and features of existing column stores, individually and
comparatively.
</p>
<p>To achieve the model's wide expressiveness, much of this work develops
representation schemes of relevant data structures as combinations of plain
columns, with columnar circuits used as scheme encoders and decoders. A
particular focus is given to schemes which also compress the data, and their
use in query execution --- as an integral part of the computation: Subcircuits
of larger columnar circuits, not black boxes. Decoder and encoder circuits are
thus also composed to form more elaborate schemes. Such formulation allows both
for an alternative view of well-known compression schemes, and for the
development of new columnar compression schemes with useful features; these
should be of independent interest irrespective of column store systems.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12217"><span class="datestr">at April 30, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12068">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12068">Authenticated Key-Value Stores with Hardware Enclaves</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tang:Yuzhe.html">Yuzhe Tang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Ju.html">Ju Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Kai.html">Kai Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Jianliang.html">Jianliang Xu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Qi.html">Qi Zhang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12068">PDF</a><br /><b>Abstract: </b>Authenticated data storage on an untrusted platform is an important computing
paradigm for cloud applications ranging from big-data outsourcing, to
cryptocurrency and certificate transparency log. These modern applications
increasingly feature update-intensive workloads, whereas existing authenticated
data structures (ADSs) designed with in-place updates are inefficient to handle
such workloads. In this paper, we address this issue and propose a novel
authenticated log-structured merge tree (eLSM) based key-value store by
leveraging Intel SGX enclaves.
</p>
<p>We present a system design that runs the code of eLSM store inside enclave.
To circumvent the limited enclave memory (128 MB with the latest Intel CPUs),
we propose to place the memory buffer of the eLSM store outside the enclave and
protect the buffer using a new authenticated data structure by digesting
individual LSM-tree levels. We design protocols to support query authentication
in data integrity, completeness (under range queries), and freshness. The proof
in our protocol is made small by including only the Merkle proofs at selective
levels.
</p>
<p>We implement eLSM on top of Google LevelDB and Facebook RocksDB with minimal
code change and performance interference. We evaluate the performance of eLSM
under the YCSB workload benchmark and show a performance advantage of up to
4.5X speedup.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12068"><span class="datestr">at May 01, 2019 12:36 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12061">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12061">A Linear-Time Algorithm for Radius-Optimally Augmenting Paths in a Metric Space</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Johnson:Christopher.html">Christopher Johnson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Haitao.html">Haitao Wang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12061">PDF</a><br /><b>Abstract: </b>Let $P$ be a path graph of $n$ vertices embedded in a metric space. We
consider the problem of adding a new edge to $P$ to minimize the radius of the
resulting graph. Previously, a similar problem for minimizing the diameter of
the graph was solved in $O(n\log n)$ time. To the best of our knowledge, the
problem of minimizing the radius has not been studied before. In this paper, we
present an $O(n)$ time algorithm for the problem, which is optimal.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12061"><span class="datestr">at May 01, 2019 12:35 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12042">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12042">Truly Optimal Euclidean Spanners</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Le:Hung.html">Hung Le</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Solomon:Shay.html">Shay Solomon</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12042">PDF</a><br /><b>Abstract: </b>Euclidean spanners are important geometric structures, having found numerous
applications over the years. Cornerstone results in this area from the late 80s
and early 90s state that for any $d$-dimensional $n$-point Euclidean space,
there exists a $(1+\epsilon)$-spanner with $nO(\epsilon^{-d+1})$ edges and
lightness $O(\epsilon^{-2d})$. Surprisingly, the fundamental question of
whether or not these dependencies on $\epsilon$ and $d$ for small $d$ can be
improved has remained elusive, even for $d = 2$. This question naturally arises
in any application of Euclidean spanners where precision is a necessity.
</p>
<p>The state-of-the-art bounds $nO(\epsilon^{-d+1})$ and $O(\epsilon^{-2d})$ on
the size and lightness of spanners are realized by the {\em greedy} spanner. In
2016, Filtser and Solomon proved that, in low dimensional spaces, the greedy
spanner is near-optimal. The question of whether the greedy spanner is truly
optimal remained open to date.
</p>
<p>The contribution of this paper is two-fold. We resolve these longstanding
questions by nailing down the exact dependencies on $\epsilon$ and $d$ and
showing that the greedy spanner is truly optimal. Specifically, for any $d=
O(1), \epsilon = \Omega({n}^{-\frac{1}{d-1}})$:
</p>
<p>_ We show that any $(1+\epsilon)$-spanner must have $n
\Omega(\epsilon^{-d+1})$ edges, implying that the greedy (and other) spanners
achieve the optimal size.
</p>
<p>_ We show that any $(1+\epsilon)$-spanner must have lightness
$\Omega(\epsilon^{-d})$, and then improve the upper bound on the lightness of
the greedy spanner from $O(\epsilon^{-2d})$ to $O(\epsilon^{-d})$.
</p>
<p>We then complement our negative result for the size of spanners with a rather
counterintuitive positive result: Steiner points lead to a quadratic
improvement in the size of spanners! Our bound for the size of Steiner spanners
is tight as well (up to lower-order terms).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12042"><span class="datestr">at May 01, 2019 12:47 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12011">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12011">Parameterized algorithms for Partial vertex covers in bipartite graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mkrtchyan:Vahan.html">Vahan Mkrtchyan</a>, Garik Petrosyan, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Subramani:K=.html">K. Subramani</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12011">PDF</a><br /><b>Abstract: </b>In the weighted partial vertex cover problem (WPVC), we are given a graph
$G=(V,E)$, cost function $c:V\rightarrow N$, profit function $p:E\rightarrow
N$, and positive integers $R$ and $L$. The goal is to check whether there is a
subset $V'\subseteq V$ of cost at most $R$, such that the total profit of edges
covered by $V'$ is at least $L$. In this paper we study the fixed-parameter
tractability of WPVC in bipartite graphs (WPVCB). By extending the methods of
Amini et al., we show that WPVCB is FPT with respect to $R$ if $c\equiv 1$. On
the negative side, it is $W[1]$-hard for arbitrary $c$, even when $p\equiv 1$.
In particular, WPVCB is $W[1]$-hard parameterized by $R$. We complement this
negative result by proving that for bounded-degree graphs WPVC is FPT with
respect to $R$. The same result holds for the case of WPVCB when we allow to
take only one fractional vertex. Additionally, we show that WPVC is FPT with
respect to $L$. Finally, we discuss a variant of PVCB in which the edges
covered are constrained to include a matching of prescribed size and derive a
paramterized algorithm for the same.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12011"><span class="datestr">at April 30, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12000">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12000">On the fixed-parameter tractability of the maximum connectivity improvement problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Federico Corò, Gianlorenzo D'Angelo, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mkrtchyan:Vahan.html">Vahan Mkrtchyan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12000">PDF</a><br /><b>Abstract: </b>In the Maximum Connectivity Improvement (MCI) problem, we are given a
directed graph $G=(V,E)$ and an integer $B$ and we are asked to find $B$ new
edges to be added to $G$ in order to maximize the number of connected pairs of
vertices in the resulting graph. The MCI problem has been studied from the
approximation point of view. In this paper, we approach it from the
parameterized complexity perspective in the case of directed acyclic graphs. We
show several hardness and algorithmic results with respect to different natural
parameters. Our main result is that the problem is $W[2]$-hard for parameter
$B$ and it is FPT for parameters $|V| - B$ and $\nu$, the matching number of
$G$. We further characterize the MCI problem with respect to other
complementary parameters.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12000"><span class="datestr">at May 01, 2019 12:36 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.07358">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.07358">Deterministic Preparation of Dicke States</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/B=auml=rtschi:Andreas.html">Andreas Bärtschi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Eidenbenz:Stephan.html">Stephan Eidenbenz</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.07358">PDF</a><br /><b>Abstract: </b>The Dicke state $|D_k^n\rangle$ is an equal-weight superposition of all
$n$-qubit states with Hamming Weight $k$ (i.e. all strings of length $n$ with
exactly $k$ ones over a binary alphabet). Dicke states are an important class
of entangled quantum states that among other things serve as starting states
for combinatorial optimization quantum algorithms.
</p>
<p>We present a deterministic quantum algorithm for the preparation of Dicke
states. Implemented as a quantum circuit, our scheme uses $O(kn)$ gates, has
depth $O(n)$ and needs no ancilla qubits. The inductive nature of our approach
allows for linear-depth preparation of arbitrary symmetric pure states and --
used in reverse -- yields a quasilinear-depth circuit for efficient compression
of quantum information in the form of symmetric pure states, improving on
existing work requiring quadratic depth. All of these properties even hold for
Linear Nearest Neighbor architectures.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.07358"><span class="datestr">at May 01, 2019 12:48 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/064">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/064">TR19-064 |  Randomness and Intractability in Kolmogorov Complexity | 

	Igor Carboni Oliveira</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We introduce randomized time-bounded Kolmogorov complexity (rKt), a natural extension of Levin's notion of Kolmogorov complexity from 1984. A string w of low rKt complexity can be decompressed from a short representation via a time-bounded algorithm that outputs w with high probability. 

This complexity measure gives rise to a decision problem over strings: MrKtP (The Minimum rKt Problem). We explore ideas from pseudorandomness to prove that MrKtP and its variants cannot be solved in randomized quasi-polynomial time. This exhibits a natural string compression problem that is provably intractable, even for randomized computations. Our techniques also imply that there is no n^{1-eps}-approximate algorithm for MrKtP running in randomized quasi-polynomial time. 

Complementing this lower bound, we observe connections between rKt, the power of randomness in computing, and circuit complexity. In particular, we present the first hardness magnification theorem for a natural problem that is unconditionally hard against a strong model of computation.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/064"><span class="datestr">at April 30, 2019 06:41 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15814">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/04/30/network-coding-yields-lower-bounds/">Network Coding Yields Lower Bounds</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Practice leads theory</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<a href="https://rjlipton.files.wordpress.com/2019/04/akkl.jpg"><img src="https://rjlipton.files.wordpress.com/2019/04/akkl.jpg?w=123&amp;h=162" alt="" width="123" class="alignright wp-image-15816" height="162" /></a><p></p><p>
Peyman Afshani, Casper Freksen, Lior Kamma, and Kasper Larsen have a beautiful new <a href="https://arxiv.org/abs/1902.10935">paper</a> titled “Lower Bounds for Multiplication via Network Coding”. </p><p>
Today we will talk about how practical computing played a role in this theory research.</p><p>
The authors (AFKL) state this:</p><blockquote><p><b> </b> <em> In this work, we prove that if a central conjecture in the area of network coding is true, then any constant degree boolean circuit for multiplication must have size <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n+%5Clog+n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\Omega(n \log n)}" class="latex" title="{\Omega(n \log n)}" />, thus <s>almost</s> completely settling the complexity of multiplication circuits. </em>
</p></blockquote><p></p><p>
We added the strikeout because of the <img src="https://s0.wp.com/latex.php?latex=%7BO%28n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n \log n)}" class="latex" title="{O(n \log n)}" /> upper bound that we discussed recently <a href="https://rjlipton.wordpress.com/2019/03/29/integer-multiplication-in-nlogn-time/">here</a>.</p><p>
AFKL have conditionally solved a long standing open problem: “How hard is it to multiply two <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-bit numbers?” Their proof shows that a conjecture from practice implies a circuit lower bound. This is rare: using a conjecture from practice, to solve a complexity open problem. We have used conjectures from many parts of mathematics, and from some parts of physics, to make progress, but drawing on experience with practical networking is strikingly fresh. </p><p>
</p><p></p><h2> Integer Multiplication </h2><p></p><p></p><p>
The authors AFKL explain the history of the multiplication problem. We knew some of the story, but not all the delicious details.</p><blockquote><p><b> </b> <em> In 1960, Andrey Kolmogorov conjectured that the thousands of years old <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%7B2%7D%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{O(n^{2})}" class="latex" title="{O(n^{2})}" />-time algorithm is optimal and he arranged a seminar at Moscow State University with the goal of proving this conjecture. However only a week into the seminar, the student Anatoly Karatsuba came up with an <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%7B%5Clog_%7B2%7D3%7D%29+%5Capprox+O%28n%5E%7B1.585%7D%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{O(n^{\log_{2}3}) \approx O(n^{1.585})}" class="latex" title="{O(n^{\log_{2}3}) \approx O(n^{1.585})}" /> time algorithm. The algorithm was presented at the next seminar meeting and the seminar was terminated. </em>
</p></blockquote><p></p><p>
Ken and I wish we could have Kolmogorov’s luck, in one of our seminars. Partly because it would advance knowledge; partly because it would let us out of teaching. Sweet.</p><p>
The main result of AKKL is:</p><blockquote><p><b>Theorem 1</b> <em><a name="NC2mult"></a> Assuming the Network Conjecture, every general boolean circuit that computes the product of two <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-bit integers has size order at least <img src="https://s0.wp.com/latex.php?latex=%7Bn%5Clog+n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n\log n}" class="latex" title="{n\log n}" />. </em>
</p></blockquote><p></p><p>
This says that the boolean complexity of multiplication is super-linear. No restriction of a bounded depth, no restriction on the operations allowed, no restrictions at all. Given our non-existent lower bounds this is remarkable. If it was unconditional, it would be a terrific result. But it still is a strong one. </p><p>
We will next explain what the Network Coding Conjecture (NCC) is. </p><p>
</p><p></p><h2> Network Coding </h2><p></p><p></p><p>
One of the basic papers was authored by Rudolf Ahlswede, Ning Cai, Shuo-Yen Li, and Raymond Yeung <a href="http://www.inf.fu-berlin.de/lehre/WS11/Wireless/papers/CodAhlswede00.pdf">here</a>. The paper has close to ten thousand citations, which would be amazing for a theory paper.</p><p>
In basic networks each node can receive and send messages to and from other nodes. They can only move messages around—they are not allowed to peer into a message. The concept of <i>network coding</i> is to allow nodes also to decode and encode messages. Nodes can peer into messages and create new ones. The goal, of course, is to decrease the time required to transmit information through the network.</p><p>
The following example combines figures from a 2004 <a href="http://www.eecg.utoronto.ca/~bli/papers/allerton04.pdf">paper</a> by Zongpeng Li and Baochun Li which formulated the NCC. At left is a situation where two senders, <img src="https://s0.wp.com/latex.php?latex=%7BS_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S_1}" class="latex" title="{S_1}" /> with an <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-bit message <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a}" class="latex" title="{a}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BS_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S_2}" class="latex" title="{S_2}" /> with an <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" />-bit message <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" />, wish to transmit to respective receivers <img src="https://s0.wp.com/latex.php?latex=%7BT_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_1}" class="latex" title="{T_1}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BT_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T_2}" class="latex" title="{T_2}" />. The network’s links are one-way as shown, with two intermediate nodes <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> and <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" />, and each link can carry <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> bits at any one time.</p><p></p><p><br />
<a href="https://rjlipton.files.wordpress.com/2019/04/flowfigures.png"><img src="https://rjlipton.files.wordpress.com/2019/04/flowfigures.png?w=500&amp;h=115" alt="" width="500" class="aligncenter wp-image-15815" height="115" /></a></p><p></p><p><br />
If <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a}" class="latex" title="{a}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" /> are black-boxes that must be kept entire, there is no way to solve this in three time steps. But if the nodes can read messages and do lightweight computations, then the middle diagram gives a viable solution. Node <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> reads <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a}" class="latex" title="{a}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" /> and on-the-fly transmits their bitwise exclusive-or to node <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" />. Node <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> relays this to each receiver, who has also received the other party’s message directly. The receivers can each do a final exclusive-or to recover the messages intended for them. </p><p>
The ability to look inside messages seems powerful, and there are networks where it helps even more dramatically. Incidentally, as <a href="https://en.wikipedia.org/wiki/Linear_network_coding">noted</a> by Wikipedia, the exclusive-or trick was anticipated in a 1978 <a href="https://ieeexplore.ieee.org/document/1455117">paper</a> showing how the two senders can exchange their messages <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a}" class="latex" title="{a}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{b}" class="latex" title="{b}" /> by relaying them to a satellite which transmits <img src="https://s0.wp.com/latex.php?latex=%7Ba+%5Coplus+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a \oplus b}" class="latex" title="{a \oplus b}" /> back to each.</p><p>
</p><p></p><h2> The Conjecture </h2><p></p><p></p><p>
However, there is another solution if the links are bi-directional and messages can be broken in half. Sender <img src="https://s0.wp.com/latex.php?latex=%7BS_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S_1}" class="latex" title="{S_1}" /> simply routes half of <img src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{a}" class="latex" title="{a}" /> one way around the network and the other half the other way. Sender <img src="https://s0.wp.com/latex.php?latex=%7BS_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S_2}" class="latex" title="{S_2}" /> does similarly. This is shown at far right. Each link never has more than <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> bits of total load and the three-step elapsed time is the same. Moreover, the link from <img src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{A}" class="latex" title="{A}" /> to <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> is not needed. This is just an undirected network commodity flow with fractional units.</p><p>
In fact, <i>no</i> example is known in an undirected network where encoding beats fractional routing. That is, the known network encoding rate is just the flow rate of the network. The network coding (NCC) conjecture is informally:</p><blockquote><p><b> </b> <em> <i>The coding rate is never better than the flow rate in undirected graphs</i>. </em>
</p></blockquote><p></p><p>
The paper by Li and Li gave formal details and several equivalent statements. Quoting them:</p><blockquote><p><b> </b> <em> For undirected networks with integral routing, there still exist configurations that are feasible with network coding but infeasible with routing only. For undirected networks with fractional routing, we show that the potential of network coding to help increase throughput in a capacitied network is equivalent to the potential of network coding to increase bandwidth efficiency in an uncapacitied network. We conjecture that these benefits are non-existent. </em>
</p></blockquote><p>
</p><p></p><h2> Good and Bad News </h2><p></p><p></p><p>
What has become of the NCC in the fifteen years since? Here’s how Ken and I see it:</p><p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>The Good News</i>: The NCC helps solve long-standing open problems. Since this conjecture is widely believed this is impressive. Besides integer multiplication, NCC has been used to prove other lower bounds. For example, Larsen working with Alireza Farhadi, Mohammad Hajiaghayi, and Elaine Shi used it to <a href="https://arxiv.org/abs/1811.01313">prove</a> lower bounds on sorting with external memory. </p><p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>The Bad News</i>: The NCC helps solve long-standing open problems. This suggests that this conjecture could be deep and hard to resolve. The boolean complexity of integer multiplication is a long standing open question. Since the NCC leads to a non-linear lower bound, perhaps proving this conjecture could be hopeless.</p><p>
I have mixed feelings about these lower bound results. They are impressive and shed light on hard open problems. But I wonder if the NCC could be wrong. There is a long <a href="https://rjlipton.wordpress.com/2010/06/19/guessing-the-truth/">history</a> in complexity theory where guesses of the form: </p><blockquote><p><b> </b> <em> <i>The obvious algorithm is optimal</i> </em>
</p></blockquote><p>have failed. The situation strikes us as resembling that of the (Strong) Exponential Time Hypothesis, in ways we <a href="https://rjlipton.wordpress.com/2015/06/01/puzzling-evidence/">discussed</a> four years ago. </p><p>
</p><p></p><h2> How the New Paper Works </h2><p></p><p></p><p>
The authors AKKL did not know that an <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n\log n)}" class="latex" title="{O(n\log n)}" /> upper bound had been proved for integer multiplication when they posted their paper. They did, however, prove a stronger version of Theorem (<a href="https://rjlipton.wordpress.com/feed/#NC2mult">1</a>) for a problem with a known <img src="https://s0.wp.com/latex.php?latex=%7Bn%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n\log n}" class="latex" title="{n\log n}" /> upper bound. This is to create circuits with <img src="https://s0.wp.com/latex.php?latex=%7Bn+%2B+%5Clog_2+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n + \log_2 n}" class="latex" title="{n + \log_2 n}" /> input gates and <img src="https://s0.wp.com/latex.php?latex=%7B2n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2n}" class="latex" title="{2n}" /> output gates that given <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> and a binary number <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell+%5Cleq+n+%3D+%7Cx%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell \leq n = |x|}" class="latex" title="{\ell \leq n = |x|}" /> output the string <img src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{y}" class="latex" title="{y}" /> whose bits <img src="https://s0.wp.com/latex.php?latex=%7Bn-%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n-\ell}" class="latex" title="{n-\ell}" /> through <img src="https://s0.wp.com/latex.php?latex=%7B2n-%5Cell-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2n-\ell-1}" class="latex" title="{2n-\ell-1}" /> equal <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" />, with other bits <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" />. A conditional lower bound on this <i>shift</i> task implies the same for multiplication, since the shift is the same as multiplication by <img src="https://s0.wp.com/latex.php?latex=%7B2%5E%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2^\ell}" class="latex" title="{2^\ell}" />. </p><blockquote><p><b>Theorem 2</b> <em><a name="NC2shift"></a> Assuming the NCC, circuits for the shift task need size order <img src="https://s0.wp.com/latex.php?latex=%7Bn%5Clog+n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{n\log n}" class="latex" title="{n\log n}" />. </em>
</p></blockquote><p></p><p>
The proof is disarmingly elementary: The input <img src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x}" class="latex" title="{x}" /> gives <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> “senders” and each value of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell}" class="latex" title="{\ell}" /> creates a different set of <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> “receivers.” With a circuit <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> fixed, they show one can fix a shift <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_0}" class="latex" title="{\ell_0}" /> so that the average distance from sender to receiver in an undirected multi-commodity flow is <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Omega(\log n)}" class="latex" title="{\Omega(\log n)}" />, giving <img src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Omega(n\log n)}" class="latex" title="{\Omega(n\log n)}" /> total flow. If <img src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C}" class="latex" title="{C}" /> achieves smaller size, then it represents a counterexample to the NCC. </p><p>
Pretty neat—this is half a page in the paper. The paper proves more intricate results relating to conjectures by Les Valiant about Boolean circuits of bounded fan-in and <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(\log n)}" class="latex" title="{O(\log n)}" /> depth that compute permutations and their reduction to depth-3 circuits of unbounded fan-in. This also may extend to the sorting/shifting problem Ken wrote about long ago in a guest <a href="https://blog.computationalcomplexity.org/2007/07/concrete-open-problem.html">post</a> for Lance Fortnow and Bill Gasarch’s blog.</p><p>
</p><p></p><h2> Open Problems </h2><p></p><p></p><p>
Is the NCC true? Can it be proved for some interesting classes of graphs? I believe it is known for tiny size graphs of at most six nodes. What about, for example, planar graphs?</p><p>
[inserted “conditionally” before “solved” in intro]</p><table class="image alignright">










































</table></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2019/04/30/network-coding-yields-lower-bounds/"><span class="datestr">at April 30, 2019 01:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gradientscience.org/policy_gradients_pt3">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/madry.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://gradientscience.org/policy_gradients_pt3">A Closer Look at Deep Policy Gradients (Part 3&amp;#58; Landscapes and Trust Regions)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>This post is the last of a three part series about our recent paper: “<a href="https://arxiv.org/abs/1811.02553">Are Deep
Policy Gradient Algorithms Truly Policy Gradient
Algorithms?</a>” Today, we will analyze agents’
reward landscapes as well as try to understand to what extent, and by what mechanisms,
our agents enforce so-called <i>trust regions</i>.</p>

<p>First, a quick recap (it’s been a while!):</p>
<ul>
  <li>
    <p>In our <a href="https://gradientscience.org/policy_gradients_pt1">first post</a>, we outlined the
RL framework and introduced policy gradient algorithms. We saw that
auxiliary optimizations hidden in the implementation details of RL algorithms
drastically impact performance. These findings highlighted the need for a more
fine-grained analysis of how algorithms really operate.</p>
  </li>
  <li>
    <p>In our <a href="https://gradientscience.org/policy_gradients_pt2">second post</a>, we zoomed in on
three algorithms: trust region policy optimization (TRPO), proximal policy
optimization (PPO), and an algorithm we called “PPO-M.” PPO-M is the core PPO
algorithm exactly as described in the <a href="https://arxiv.org/abs/1707.06347">original
paper</a>, without any of the auxiliary
optimizations. Using these methods as a test-bed, we studied two core
primitives of the policy gradient framework: gradient estimation and value
prediction.</p>
  </li>
</ul>

<p>Our discussion today begins where we left off in our second post. Recall that
last time we studied the variance of the gradient estimates our algorithms use
to maximize rewards. We found (among other things) that, despite high variance, algorithm steps were
still (very slightly) correlated with the actual, “true” gradient of the reward.
However, how good was this true gradient to begin with? After all, a fundamental
assumption of the whole policy gradient framework is that our gradient steps actually point in a direction (in policy
parameter space) that increases the reward. Is this indeed so in practice?</p>

<h2 id="optimization-landscapes">Optimization Landscapes</h2>
<p>Recall from our <a href="https://gradientscience.org/policy_gradients_pt1">first post</a> that
policy gradient methods treat reward maximization as a zeroth-order optimization
problem. That is, they maximize the objective by applying first order methods
with finite sample gradient estimates of the form:</p>



<p>Here,  represents the cumulative reward of a trajectory , where
 is sampled from the distribution of trajectories induced by the current
policy . We let  represent an easily computable
function of  that is an unbiased estimator of the gradient of the reward
(seen on the left hand side)—for more details see <a href="https://gradientscience.org/policy_gradients_pt1/#rl-with-policy-gradients">our previous post</a>.
Finally, we denote by  the number of trajectories used to estimate the
gradient.</p>

<p>An important point is, however, that instead of following the gradients of the cumulative reward (as
suggested by the above equation), the algorithms we analyze actually use a
<i>surrogate reward</i> at each step. The surrogate reward is a function of the
collected trajectories that is meant to locally approximate the true reward,
while providing other benefits such as easier computation and a smoother
optimization landscape. Consequently, at each step these algorithms maximize the
surrogate rewards  instead of following the gradient of the
true reward .</p>

<p>A natural question to ask is: <i>do steps maximizing the surrogate reward
consistently increase policy returns?</i> To answer this, we will use
<i>landscape plots</i> as a tool for visualizing the landscape of returns
around a given policy :</p>

<p><img src="https://gradientscience.org/images/rl/labeled_landscape.jpg" alt="Labeled diagram of optimization landscape plot" /></p>

<p>Here, for each point  in the plot,  and  specify a policy
 parameterized by</p>



<p>where  is the step computed by the studied algorithm. (Note that
we include the random Gaussian  direction to visualize how
“important” the step direction is compared to a random baseline). The  axis
corresponds to the return attained by the policy , which we
denote by .</p>

<p>Now, when we make this plot for a random  (corresponding to a randomly
initialized policy network), everything looks as expected:</p>

<p><img src="https://gradientscience.org/images/rl/step_0_landscape.jpg" alt="Landscape for randomly initialized network" /></p>

<p>That is, the return increases significantly more quickly along the step direction than in
the random direction. However, repeating these landscape experiments at later
iterations, we find a more surprising picture: going in the step direction
actually <i>decreases</i> the average cumulative reward obtained by the resulting
agent!</p>

<p><img src="https://gradientscience.org/images/rl/landscape_step_150_300_450.jpg" alt="Landscape for trained networks" /></p>

<p>So what exactly is going on here? The steps computed by these algorithms are
estimates of the gradient of the reward, so it is unexpected that the reward
plateaus (or in some cases decreases!) along this direction.</p>

<p>We find that the
answer lies in a <i>misalignment</i> of the true reward and the surrogate. Specifically, while
the steps taken do correspond to an improvement in the surrogate problem, they
do <i>not</i> always correspond to a similar improvement in the true return.
Here are the same graphs as above shown again, this time with the corresponding
optimization landscapes of the <i>surrogate loss</i>
<sup><a href="https://gradientscience.org/feed.xml#footnote1">1</a></sup>:</p>
<div class="footnote">
<sup><a id="footnote1">1</a></sup>All of the landscapes we plot in this post are for PPO; we have (similar)
results for TRPO in <a href="https://arxiv.org/abs/1811.02553">our paper</a>.
</div>

<p><img src="https://gradientscience.org/images/rl/surrogate_vs_real_landscape.jpg" alt="Surrogate vs real landscape" /></p>






<p>To make matters worse, we find that in the low sample regime that policy
gradient methods actually operate in, it is hard to even <i>discern</i> directions of
improvement in the true reward landscape. (In all the plots above,
we use orders of magnitude more samples than an agent would ever see in practice
at a single step.) In the plot below, we visualize reward landscapes while
varying the number of samples used to estimate the expected return of a policy
:</p>

<p><img src="https://gradientscience.org/images/rl/landscape_conc.jpg" alt="Surrogate vs real landscape" /></p>

<p>In contrast to the smooth landscape we see on the right and in the plots above,
the reward landscape actually accessible to the model is jagged and poorly behaved.
This landscape makes it thus near-impossible for an agent to distinguish between good
and bad points in its relevant sample regime, even when the true underlying
landscape is fairly well-behaved!</p>

<p>Overall, our investigation into the optimization landscape of policy gradient algorithms
reveals that (a) the surrogate reward function is often misaligned with the
underlying true return of the policy, and (b) in the relevant sample regime, it
is hard to distinguish between “good” steps and “bad” steps, even when looking at the true reward
landscape. As always, however, <i>none of this stops the agents from training
and continually improving reward in the average sense</i>. This raises some
key questions about the landscape of policy optimization:</p>

<ul>
  <li>Given that the function we actually optimize is so often misaligned with the
underlying rewards, how is it that agents continually improve?</li>
  <li>Can we explain or link the local behaviour we observe in the landscape with a
more global view of policy optimization?</li>
  <li>How do we ensure that the reward landscape is navigable? And, more generally,
what is the best way to navigate it?</li>
</ul>

<h2 id="trust-regions">Trust Regions</h2>
<p>Let us now turn our attention to another important notion in the popular policy gradient algorithms: that of the <i>trust region</i>. 
Recall that a convenient way to think about our training process is to view it as a series of policy parameter iterates:</p>



<p>An important aspect of this process is ensuring that the steps we take don’t
lead us outside of the region (of parameter space) where the samples we
collected are informative. Intuitively, if we collect samples at a given set of
policy parameters , there is no reason to expect that these samples
should tell us about the performance of a new set of parameters that is far away
from .</p>

<p>Thus, in order to ensure that gradient steps are predictive, classical
algorithms like the 
<a href="http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf">conservative policy update</a> 
employ update schemes that constrain the probability distributions induced by 
successive policy parameters.  The <a href="https://arxiv.org/abs/1502.05477">TRPO paper</a> 
in particular showed 
that one can 
guarantee monotonic policy improvement with each step by solving a surrogate problem of the following form:</p>



<p>The second, “penalty” term in the above objective, referred to as the <i>trust region</i> penalty, is a
critical component of modern policy gradient algorithms. TRPO, one of the
algorithms we study, proposes a relaxation of \eqref{eq:klpen} that
instead imposes a hard constraint on the
<i>mean</i> KL divergence<sup><a href="https://gradientscience.org/feed.xml#footnote2">2</a></sup>
 (estimated using the empirical samples we obtain):</p>



<div class="footnote">
<sup><a id="footnote2">2</a></sup>It's worth noting that 
<a href="https://arxiv.org/abs/1705.10528">a
recent paper</a> showed that under some conditions, mean KL is actually
sufficient.
</div>

<p>In other words, we try to ensure that the <i>average</i> distance between 
conditional probability distributions is small. 
Finally, PPO approximates the mean KL bound of TRPO by attempting to
constrain a <i>ratio</i> between successive conditional probability
distributions, instead of the KL divergence. The exact mechanism for
enforcing this is shown in the 
box below. Intuitively, however, what PPO does is just throw away
(i.e. get no gradient signal from) the
rewards incurred from any state-action pair such that:</p>



<p>where  is a user-chosen hyperparameter.</p>

<section class="container">
<div>
<div class="checkboxdiv">
<input type="checkbox" id="ac-1" name="accordion-1" />
<label for="ac-1"><span class="fas fa-chevron-right" id="titlespan"></span> <strong>The PPO update step</strong> (Click to expand)</label>
<article class="small">
<i>Note that the following is only for interested readers and is unessential 
for reading the rest of the blog post.</i> <br /> <br />

The exact update used by PPO is as follows, where $\widehat{A}_\pi$ is
the <a href="https://arxiv.org/abs/1506.02438">generalized advantage 
estimate</a>:
$$
\begin{array}{c}{\max _{\theta} \mathbb{E}_{\left(s_{t}, a_{t}\right) \sim \pi}\left[\min \left(\operatorname{clip}\left(\rho_{t}, 1-\varepsilon, 1+\varepsilon\right) \widehat{A}_{\pi}\left(s_{t}, a_{t}\right), \rho_{t} \widehat{A}_{\pi}\left(s_{t}, a_{t}\right)\right)\right]} \\ {\text{where }\ \ \rho_{t}=\frac{\pi_{\theta}\left(a_{t} | s_{t}\right)}{\pi\left(a_{t} | s_{t}\right)}}\end{array}
$$
As described in the main text, this intuitively corresponds to throwing
away (i.e. getting no gradient signal) from state-action pairs where the
ratio of conditional probabilities between successive policies is too high.
</article>
</div>
</div>
</section>
<p><br /></p>

<p>To recap, there is a theoretically motivated algorithm \eqref{eq:klpen} which
constrains maximum KL. This motivates TRPO’s bound on
mean KL in \eqref{eq:trpotrust}, which in turn motivates the 
ratio-based bound of PPO (shown in the box). This chain of approximations might 
lead us to ask: <i>how well do these algorithms actually maintain trust regions</i>?</p>

<p>We first plot the mean KL divergence between successive policies for each
algorithm:
<img src="https://gradientscience.org/images/rl/meankl_trust.jpg" style="width: 50%;" />
<br /></p>

<p>TRPO seems to constrain this very well
<a href="https://gradientscience.org/feed.xml#footnote3"><sup>3</sup></a>
! On the other hand, our two
varieties of the PPO algorithm paint a drastically different picture. Recall that we decided to
separately study two versions of PPO: PPO (based on a state-of-the-art implementation), 
and PPO-M, which we defined to be the core PPO algorithm without auxiliary optimizations. 
PPO <i>with</i> optimizations does quite well at maintaining a KL trust region, 
but PPO-M does not. This is unexpected: PPO’s main mechanism for maintaining the
trust region (the ratio clipping) is present in both methods—the
differences are only in auxiliary optimizations such as Adam learning rate annealing or
orthogonal initialization. As such, it is unclear exactly which mechanisms 
in PPO are responsible for maintaining the mean KL constraint.</p>

<div class="footnote">
<a id="footnote3"><sup>3</sup></a>
Note that this is somewhat unsurprising, since TRPO constrains this
directly in its optimization.
</div>

<p>In fact, we find that PPO’s inability to maintain a KL-based trust region 
is not entirely due to the looseness of its relaxation; it turns out that 
PPO does not even successfully enforce its <i>own</i> ratio-based trust region.
Below, we plot the maximum ratio \eqref{eq:ratio} between successive
policies for the three algorithms in question:</p>

<p><img src="https://gradientscience.org/images/rl/maxratio_trust.jpg" style="width: 50%;" />
<br /></p>

<p>In the above, the dotted line represents , corresponding to the 
bound in \eqref{eq:ratio}—it looks like the max ratio is not kept at all! And once again, 
simply adding the auxiliary, code-level optimizations to PPO-M
yields <i>better</i> trust region
enforcement, despite the main clipping mechanism staying the same.
Indeed, it turns out that the
way that PPO enforces the ratio trust region does not actually keep the ratios
from becoming too large or too small. In fact, in our paper we show that there
are <i>infinite</i> optima of the optimization problem PPO solves to find each
step and only <i>one</i> of them enforces the intended trust region bound.</p>

<h2 id="wrapping-up">Wrapping Up</h2>

<p>Deep reinforcement learning algorithms are rooted in a well-grounded framework
of classical RL, and have shown great promise in practice. However, as we’ve
found in our three-part investigation, this framework often falls a little short
of explaining the behavior of these algorithms in practice.</p>

<p>Beyond just being disconcerting, this disconnect impedes our understanding of
why these algorithms succeed (or fail). It also poses a major barrier to
addressing key challenges facing deep RL, such as widespread brittleness and
poor reproducibility (as has been observed by our 
<a href="https://gradientscience.org/policy_gradients_pt1">study in part one</a> and
many others, e.g., [<a href="https://arxiv.org/abs/1709.06560">1</a>,
<a href="https://www.alexirpan.com/2018/02/14/rl-hard.html">2</a>,
<a href="http://amid.fish/reproducing-deep-rl">3</a>]).</p>

<p>To close this gap, we need to either develop methods that adhere more closely to
theory, or build theory that can capture what makes existing policy gradient
methods successful. In both cases, the first step is to precisely pinpoint where
theory and practice diverge. Even more broadly, our findings suggest that
developing a deep RL toolkit that is truly robust and reliable will require
moving beyond the current benchmark-driven evaluation model, to a more
fine-grained understanding of deep RL algorithms.</p></div>







<p class="date">
<a href="http://gradientscience.org/policy_gradients_pt3"><span class="datestr">at April 30, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12747">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12747">Testing tensor products</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dinur:Irit.html">Irit Dinur</a>, Konstantin Golubev <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12747">PDF</a><br /><b>Abstract: </b>A function $f:[n]^d\to\mathbb{F}_2$ is a direct sum if it is of the form $
f\left((a_1,\dots,a_d)\right) = f_1(a_1)+\dots + f_d (a_d),$ for some $d$
functions $f_1,\dots,f_d:[n]\to\mathbb{F}_2$. We present a $4$-query test which
distinguishes between direct sums and functions that are far from them. The
test relies on the BLR linearity test and on the direct product test
constructed by Dinur and Steurer.
</p>
<p>We also present a different test, which queries the function $(d+1)$ times,
but is easier to analyze.
</p>
<p>In multiplicative $\pm 1$ notation, the above reads as follows. A
$d$-dimensional tensor with $\pm 1$ entries is called a tensor product if it is
a tensor product of $d$ vectors with $\pm 1$ entries. In other words, it is a
tensor product if it is of rank $1$. The presented tests check whether a given
tensor is close to a tensor product.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12747"><span class="datestr">at April 30, 2019 11:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12424">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12424">Dichotomy for symmetric Boolean PCSPs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Miron Ficak, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kozik:Marcin.html">Marcin Kozik</a>, Miroslav Olsak, Szymon Stankiewicz <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12424">PDF</a><br /><b>Abstract: </b>A PCSP is a combination of two CSPs defined by two similar templates; the
computational question is to distinguish a YES instance of the first one from a
NO instance of the second. The computational complexity of many PCSPs remains
unknown. Even the case of Boolean templates (solved for CSP by Schaefer
[STOC'78]) remains wide open.
</p>
<p>The main result of Brakensiek and Guruswami [SODA'18] shows that Boolean
PCSPs exhibit a dichotomy (PTIME vs. NPC) when "all the clauses are symmetric
and allow for negation of variables''. In this paper we remove the "allow for
negation of variables'' assumption from the theorem. The "symmetric" assumption
means that changing the order of variables in a constraint does not change its
satisfiability. The "negation of variables" means that both of the templates
share a relation which can be used to effectively negate Boolean variables.
</p>
<p>The main result of this paper establishes dichotomy for all the symmetric
boolean templates. The tractability case of our theorem and the theorem of
Brakensiek and Guruswami are almost identical. The main difference, and the
main contribution of this work, is the new reason for hardness and the
reasoning proving the split.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12424"><span class="datestr">at April 30, 2019 11:22 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12335">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12335">Blended Matching Pursuit</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Cyrille W. Combettes, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pokutta:Sebastian.html">Sebastian Pokutta</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12335">PDF</a><br /><b>Abstract: </b>Matching pursuit algorithms are an important class of algorithms in signal
processing and machine learning. We present a blended matching pursuit
algorithm, combining coordinate descent-like steps with stronger gradient
descent steps, for minimizing a smooth convex function over a linear space
spanned by a set of atoms. We derive sublinear to linear convergence rates
according to the smoothness and sharpness orders of the function and
demonstrate computational superiority of our approach. In particular, we derive
linear rates for a wide class of non-strongly convex functions, and we
demonstrate in experiments that our algorithm enjoys very fast rates of
convergence and wall-clock speed while maintaining a sparsity of iterates very
comparable to that of the (much slower) orthogonal matching pursuit.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12335"><span class="datestr">at April 30, 2019 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12248">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12248">Fast Utility Mining on Complex Sequences</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gan:Wensheng.html">Wensheng Gan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lin:Jerry_Chun=Wei.html">Jerry Chun-Wei Lin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Jiexiong.html">Jiexiong Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fournier=Viger:Philippe.html">Philippe Fournier-Viger</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chao:Han=Chieh.html">Han-Chieh Chao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yu:Philip_S=.html">Philip S. Yu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12248">PDF</a><br /><b>Abstract: </b>High-utility sequential pattern mining is an emerging topic in the field of
Knowledge Discovery in Databases. It consists of discovering subsequences
having a high utility (importance) in sequences, referred to as high-utility
sequential patterns (HUSPs). HUSPs can be applied to many real-life
applications, such as market basket analysis, E-commerce recommendation,
click-stream analysis and scenic route planning. For example, in economics and
targeted marketing, understanding economic behavior of consumers is quite
challenging, such as finding credible and reliable information on product
profitability. Several algorithms have been proposed to address this problem by
efficiently mining utility-based useful sequential patterns. Nevertheless, the
performance of these algorithms can be unsatisfying in terms of runtime and
memory usage due to the combinatorial explosion of the search space for low
utility threshold and large databases. Hence, this paper proposes a more
efficient algorithm for the task of high-utility sequential pattern mining,
called HUSP-ULL. It utilizes a lexicographic sequence (LS)-tree and a
utility-linked (UL)-list structure to fast discover HUSPs. Furthermore, two
pruning strategies are introduced in HUSP-ULL to obtain tight upper-bounds on
the utility of candidate sequences, and reduce the search space by pruning
unpromising candidates early. Substantial experiments both on real-life and
synthetic datasets show that the proposed algorithm can effectively and
efficiently discover the complete set of HUSPs and outperforms the
state-of-the-art algorithms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12248"><span class="datestr">at April 30, 2019 11:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1904.12156">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1904.12156">Parameterised Counting Classes with Bounded Nondeterminism</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haak:Anselm.html">Anselm Haak</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meier:Arne.html">Arne Meier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Prakash:Om.html">Om Prakash</a>, Raghavendra Rao B. V <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1904.12156">PDF</a><br /><b>Abstract: </b>Stockhusen and Tantau (IPEC 2013) introduced the operators paraW and paraBeta
for parameterised space complexity classes by allowing bounded nondeterminism
with read-only and read-once access, respectively. Using these operators, they
could characterise the complexity of many parameterisations of natural problems
on graphs. In this article, we study the counting versions of the parameterised
space-bounded complexity classes introduced by Stockhusen and Tantau (IPEC
2013). We show that natural path counting problems in digraphs are complete for
the newly introduced classes #paraWL and #paraBetaL. Finally, we introduce
parameterised counting classes based on branching programs (BPs). We show that
parameterised counting classes based on branching programs coincide with the
corresponding parameterised space bounded counting classes.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1904.12156"><span class="datestr">at April 30, 2019 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-6025884855508893027">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/04/x-3-y-3-z-3-33-has-solution-in-z-and.html">x3   + y3 + z3 = 33  has a solution in Z. And its big!</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Consider the following problem:<br />
<br />
Given k, a natural number, determine if there exists x,y,z INTEGERS such that x<sup>3</sup>+y<sup>3</sup>+z<sup>3</sup>=k.<br />
<br />
It is not obvious that this problem is decidable (I think it is but have not been able to find an exact statement to that affect; however, if it was not solvable, I would know that, hence it is solvable. If you know a ref give it in the comments.)<br />
<br />
<br />
If k≡ 4,5 mod 9  then mod arguments easily show there is no solution. <a href="https://arxiv.org/pdf/1604.07746.pdf">Huisman</a> showed that if k≤ 1000, k≡1,2,3,6,7,8 mod 9 and max(|x|,|y|,|z|) ≤ 10<sup>15</sup> and k is NOT one of<br />
<br />
33, 42, 114, 165, 390, 579, 627, 633, 732, 795, 906, 921, 975<br />
<br />
then there was a solution. For those on the list it was unknown.<br />
<br />
Recently <a href="https://people.maths.bris.ac.uk/~maarb/papers/cubesv1.pdf">Booker</a> (not Cory Booker, the candidate for prez, but Andrew Booker who I assume is a math-computer science person and is not running for prez) showed that<br />
<br />
x<sup>3</sup> + y<sup>3</sup> + z<sup>3</sup> =33<br />
<br />
DOES have a solution in INTEGERS. It is<br />
<br />
x= 8,866,128,975,287,528<br />
<br />
y=-8,778,405,442,862,239<br />
<br />
z=-2,736,111,468,807,040<br />
<br />
<br />
does that make us more likely or less likely to think that<br />
<br />
x<sup>3</sup> + y<sup>3</sup> + z<sup>3</sup> =42<br />
<br />
has a solution? How about =114, etc, the others on the list?<br />
<br />
Rather than say what I think is true (I have no idea) here is what I HOPE is true: that the resolution of these problems leads to some mathematics of interest.<br />
<br />
<br />
<br />
<br />
<br /></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/04/x-3-y-3-z-3-33-has-solution-in-z-and.html"><span class="datestr">at April 29, 2019 02:26 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/063">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/063">TR19-063 |  Efficient Black-Box Identity Testing for Free Group Algebra | 

	Abhranil Chatterjee, 

	Vikraman Arvind, 

	Partha Mukhopadhyay, 

	Rajit Datta</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Hrubeš and Wigderson [HW14] initiated the study of
  noncommutative arithmetic circuits with division computing a
  noncommutative rational function in the free skew field, and
  raised the question of rational identity testing. It is now known
  that the problem can be solved in deterministic polynomial time in
  the white-box model for noncommutative formulas with
  inverses, and in randomized polynomial time in the black-box
  model [GGOW16, IQS18, DM18], where the running time is
  polynomial in the size of the formula. 

  The complexity of identity testing of noncommutative rational
  functions remains open in general (when the formula size
  is not polynomially bounded). We solve the problem for a natural
  special case. We consider polynomial expressions in the free group
  algebra $\mathbb{F}\langle X, X^{-1}\rangle$ where $X=\{x_1, x_2, \ldots, x_n\}$, a
  subclass of rational expressions of inversion height one. Our main
  results are the following.

1. Given a degree $d$ expression $f$ in $\mathbb{F}\langle X, X^{-1}\rangle$ as a black-box, we obtain a randomized $\text{poly}(n,d)$ algorithm to check
  whether $f$ is an identically zero expression or not. We obtain this
  by generalizing the Amitsur-Levitzki theorem [AL50] to
  $\mathbb{F}\langle X, X^{-1}\rangle$. This also yields a deterministic identity testing algorithm (and even
  an expression reconstruction algorithm) that is polynomial time in
  the sparsity of the input expression.

2. Given an expression $f$ in $\mathbb{F}\langle X, X^{-1}\rangle$ of degree at most
  $D$, and sparsity $s$, as black-box, we can check whether $f$ is
  identically zero or not in randomized $\text{poly}(n,\log s, \log D)$
  time.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/063"><span class="datestr">at April 28, 2019 03:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2019/04/26/new-york-area-theory-day-spring-2019/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2019/04/26/new-york-area-theory-day-spring-2019/">New York Area Theory Day (Spring 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
May 10, 2019 Columbia University http://www.cs.columbia.edu/theory/s19-tday.html The New York Area Theory Day, co-organized by Columbia, IBM, and NYU, is a semi-annual conference aiming to bring together researchers in the New York Metropolitan area. It usually features a few hour long talks on recent advances in theoretical computer science. The speakers this time are Sepehr Assadi, … <a href="https://cstheory-events.org/2019/04/26/new-york-area-theory-day-spring-2019/" class="more-link">Continue reading <span class="screen-reader-text">New York Area Theory Day (Spring 2019)</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2019/04/26/new-york-area-theory-day-spring-2019/"><span class="datestr">at April 26, 2019 10:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4238">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2019/04/25/online-optimization-post-2-constructing-pseudorandom-sets/">Online Optimization Post 2: Constructing Pseudorandom Sets</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
 Today we will see how to use the analysis of the multiplicative weights algorithm in order to construct pseudorandom sets. </p>
<p>
The method will yield constructions that are optimal in terms of the size of the pseudorandom set, but not very efficient, although there is at least one case (getting an “almost pairwise independent” pseudorandom generator) in which the method does something that I am not sure how to replicate with other techniques. </p>
<p>
Mostly, the point of this post is to illustrate a concept that will reoccur in more interesting contexts: that we can use an online optimization algorithm in order to construct a combinatorial object satisfying certain desired properties. The idea is to run a game between a “builder” against an “inspector,” in which the inspector runs the online optimization algorithm with the goal of finding a violated property in what the builder is building, and the builder plays the role of the adversary selecting the cost functions, with the advantage that it gets to build a piece of the construction after seeing what property the “inspector” is looking for. By the regret analysis of the online optimization problem, if the builder did well at each round against the inspector, then it will do well also against the “offline optimum” that looks for a violated property after seeing the whole construction. For example, the construction of graph sparsifiers by Allen-Zhu, Liao and Orecchia can be cast in this framework.</p>
<p>
(In some other applications, it will be the “builder” that runs the algorithm and the “inspector” who plays the role of the adversary. This will be the case of the Frieze-Kannan weak regularity lemma and of the Impagliazzo hard-core lemma. In those cases we capitalize on the fact that we know that there is a very good offline optimum, and we keep going for as long as the adversary is able to find violated properties in what the builder is constructing. After a sufficiently large number of rounds, the regret experienced by the algorithm would exceed the general regret bound, so the process must terminate in a small number of rounds. I have been told that this is just the “dual view” of what I described in the previous paragraph.)</p>
<p>
But, back the pseudorandom sets: if <img src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+C%7D+%3D+%5C%7B+C_1%2C%5Cldots%2CC_N+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{\cal C} = \{ C_1,\ldots,C_N \}}" class="latex" title="{{\cal C} = \{ C_1,\ldots,C_N \}}" /> is a collection of boolean functions <img src="https://s0.wp.com/latex.php?latex=%7BC_i+%3A+%5C%7B+0%2C1+%5C%7D%5En+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_i : \{ 0,1 \}^n \rightarrow \{ 0,1 \}}" class="latex" title="{C_i : \{ 0,1 \}^n \rightarrow \{ 0,1 \}}" />, for example the functions computed by circuits of a certain type and a certain size, then a multiset <img src="https://s0.wp.com/latex.php?latex=%7BS%5Csubseteq+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S\subseteq \{ 0,1 \}^n}" class="latex" title="{S\subseteq \{ 0,1 \}^n}" /> is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />-pseudorandom for <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\cal C}" class="latex" title="{\cal C}" /> if, for every <img src="https://s0.wp.com/latex.php?latex=%7BC_i+%5Cin+%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_i \in \cal C}" class="latex" title="{C_i \in \cal C}" />, we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D1%5D+-+%5Cmathop%7B%5Cmathbb+P%7D_%7Bs+%5Csim+S%7D+%5BC_i%28s%29+%3D+1+%5D+%7C+%5Cleq+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  | \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) =1] - \mathop{\mathbb P}_{s \sim S} [C_i(s) = 1 ] | \leq \epsilon " class="latex" title="\displaystyle  | \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) =1] - \mathop{\mathbb P}_{s \sim S} [C_i(s) = 1 ] | \leq \epsilon " /></p>
<p> That is, sampling uniformly from <img src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{S}" class="latex" title="{S}" />, which we can do with <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog_2+%7CS%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log_2 |S|}" class="latex" title="{\log_2 |S|}" /> random bits, is as good as sampling uniformly from <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ 0,1 \}^n}" class="latex" title="{\{ 0,1 \}^n}" />, which requires <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> bits, as far as the functions in <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\cal C}" class="latex" title="{\cal C}" /> are concerned.</p>
<p>
It is easy to use Chernoff bounds and union bounds to argue that there is such a set of size <img src="https://s0.wp.com/latex.php?latex=%7BO%28N%2F%5Cepsilon%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(N/\epsilon^2)}" class="latex" title="{O(N/\epsilon^2)}" />, so that we can sample from it using only <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog+N+%2B+2%5Clog+%5Cfrac+1+%5Cepsilon+%2B+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log N + 2\log \frac 1 \epsilon + O(1)}" class="latex" title="{\log N + 2\log \frac 1 \epsilon + O(1)}" /> random bits.</p>
<p>
We will prove this result (while also providing an “algorithm” for the construction) using multiplicative weights.</p>
<p>
<span id="more-4238"></span></p>
<p>
First of all, possibly by changing <img src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{N}" class="latex" title="{N}" /> to <img src="https://s0.wp.com/latex.php?latex=%7B2N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2N}" class="latex" title="{2N}" />, we may assume that for every function <img src="https://s0.wp.com/latex.php?latex=%7BC+%5Cin+%7B%5Ccal+C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C \in {\cal C}}" class="latex" title="{C \in {\cal C}}" /> the function <img src="https://s0.wp.com/latex.php?latex=%7B1-C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1-C}" class="latex" title="{1-C}" /> is also in <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\cal C}" class="latex" title="{\cal C}" />. This simplifies things a bit because then the pseudorandom condition is equivalent to just</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+C%5Cin+%7B%5Ccal+C%7D+%5C+%5C+%5C+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C+%28u%29+%3D1%5D+-+%5Cmathop%7B%5Cmathbb+P%7D_%7Bs+%5Csim+S%7D+%5BC%28s%29+%3D+1+%5D+%5Cgeq+-+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall C\in {\cal C} \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C (u) =1] - \mathop{\mathbb P}_{s \sim S} [C(s) = 1 ] \geq - \epsilon " class="latex" title="\displaystyle  \forall C\in {\cal C} \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C (u) =1] - \mathop{\mathbb P}_{s \sim S} [C(s) = 1 ] \geq - \epsilon " /></p>
<p>
We will make up an “experts” setup in which there is an expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> for each function <img src="https://s0.wp.com/latex.php?latex=%7BC_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{C_i}" class="latex" title="{C_i}" />. Thus, the algorithm, at each step, comes up with a probability distribution <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> over the functions, which we can think of as a “probabilistic function.” At time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />, the adversary chooses a string <img src="https://s0.wp.com/latex.php?latex=%7Bs_t+%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s_t \in \{ 0,1 \}^n}" class="latex" title="{s_t \in \{ 0,1 \}^n}" /> and defines the cost function </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t+%28x%29+%3A%3D+%5Csum_%7Bi%3D1%7D%5EN+x%28i%29+%5Ccdot+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s_t%29+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  f_t (x) := \sum_{i=1}^N x(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) " class="latex" title="\displaystyle  f_t (x) := \sum_{i=1}^N x(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) " /></p>
<p> where the adversary chooses an <img src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s_t}" class="latex" title="{s_t}" /> such that <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x_t%29+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(x_t) \geq 0}" class="latex" title="{f_t(x_t) \geq 0}" />. At this point, the reader should try, without reading ahead, to establish: </p>
<ol>
<li> That such a choice of <img src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s_t}" class="latex" title="{s_t}" /> is always possible;
</li><li> That the cost function is of the form <img src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x%29+%3D+%5Clangle+%5Cell_t+%2C+x%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t(x) = \langle \ell_t , x\rangle}" class="latex" title="{f_t(x) = \langle \ell_t , x\rangle}" />, where the loss vector <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_t}" class="latex" title="{\ell_t}" /> satisfies <img src="https://s0.wp.com/latex.php?latex=%7B%7C%5Cell_t+%28i%29+%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|\ell_t (i) | \leq 1}" class="latex" title="{|\ell_t (i) | \leq 1}" />, so that the regret after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cleq+2+%5Csqrt%7BT+%5Cln+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\leq 2 \sqrt{T \ln N}}" class="latex" title="{\leq 2 \sqrt{T \ln N}}" />;
</li><li> That the sequence <img src="https://s0.wp.com/latex.php?latex=%7Bs_1%2C%5Cldots%2Cs_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s_1,\ldots,s_T}" class="latex" title="{s_1,\ldots,s_T}" /> of choices by the adversary determines a <img src="https://s0.wp.com/latex.php?latex=%7B2%5Csqrt+%7B%5Cfrac+%7B%5Cln+N%7D%7BT%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2\sqrt {\frac {\ln N}{T}}}" class="latex" title="{2\sqrt {\frac {\ln N}{T}}}" />-pseudorandom multiset for <img src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\cal C}" class="latex" title="{\cal C}" />, and, in particular, we get an <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />-pseudorandom multiset of cardinality <img src="https://s0.wp.com/latex.php?latex=%7B4+%5Cfrac+%7B%5Cln+N%7D%7B%5Cepsilon%5E2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{4 \frac {\ln N}{\epsilon^2}}" class="latex" title="{4 \frac {\ln N}{\epsilon^2}}" />
</li></ol>
<p> For the first point, note that for a random <img src="https://s0.wp.com/latex.php?latex=%7Bs+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s \sim \{ 0,1 \}^n}" class="latex" title="{s \sim \{ 0,1 \}^n}" /> we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D_%7Bs%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5Csum_%7Bi%3D1%7D%5EN+x_t%28i%29+%5Ccdot+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s%29+%5Cright%29+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathop{\mathbb E}_{s\sim \{ 0,1 \}^n} \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s) \right) = 0 " class="latex" title="\displaystyle  \mathop{\mathbb E}_{s\sim \{ 0,1 \}^n} \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s) \right) = 0 " /></p>
<p> so there is an <img src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s_t}" class="latex" title="{s_t}" /> such that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5EN+x_t%28i%29+%5Ccdot+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s_t%29+%5Cright%29+%5Cgeq+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq 0 " class="latex" title="\displaystyle  \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq 0 " /></p>
<p> For the second point we just have to inspect the definition, and for the last point we have, by construction </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+%5Cgeq+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{t=1}^T f_t(x_t) \geq 0 " class="latex" title="\displaystyle  \sum_{t=1}^T f_t(x_t) \geq 0 " /></p>
<p> so the regret bound is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin_%7Bx%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x%29+%5Cgeq+-+%7B%5Crm+Regret%7D_T+%5Cgeq+-+2+%5Csqrt%7BT%5Cln+n%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \min_{x} \sum_{t=1}^T f_t(x) \geq - {\rm Regret}_T \geq - 2 \sqrt{T\ln n} " class="latex" title="\displaystyle  \min_{x} \sum_{t=1}^T f_t(x) \geq - {\rm Regret}_T \geq - 2 \sqrt{T\ln n} " /></p>
<p> which, after dividing by <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" />, is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+i+%3A+%5C+%5C+%5C+%5Cfrac+1T+%5Csum_%7Bt%3D1%7D%5ET+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s_t%29+%5Cright%29+%5Cgeq+-+2+%5Csqrt%7B%5Cfrac+%7B%5Cln+n%7D%7BT%7D%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall i : \ \ \ \frac 1T \sum_{t=1}^T \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq - 2 \sqrt{\frac {\ln n}{T}} " class="latex" title="\displaystyle  \forall i : \ \ \ \frac 1T \sum_{t=1}^T \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq - 2 \sqrt{\frac {\ln n}{T}} " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+i%3A+%5C+%5C+%5C+%5C+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+%5CPr_%7Bs%5Cin+%5C%7B+s_1%2C%5Cldots%2Cs_T+%5C%7D+%7D+%5BC_i+%28s%29+%3D+1+%5D+%5Cgeq+-+2+%5Csqrt%7B%5Cfrac+%7B%5Cln+n%7D%7BT%7D%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \forall i: \ \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - \Pr_{s\in \{ s_1,\ldots,s_T \} } [C_i (s) = 1 ] \geq - 2 \sqrt{\frac {\ln n}{T}} " class="latex" title="\displaystyle  \forall i: \ \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - \Pr_{s\in \{ s_1,\ldots,s_T \} } [C_i (s) = 1 ] \geq - 2 \sqrt{\frac {\ln n}{T}} " /></p>
<p> Consider now the application of constructing a small-support distribution over <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ 0,1 \}^n}" class="latex" title="{\{ 0,1 \}^n}" /> that is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />-almost-pairwise-independent, meaning that if <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s}" class="latex" title="{s}" /> is a random string sampled according to this distribution, then, for every <img src="https://s0.wp.com/latex.php?latex=%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i,j}" class="latex" title="{i,j}" />, the marginal <img src="https://s0.wp.com/latex.php?latex=%7B%28s_i%2Cs_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(s_i,s_j)}" class="latex" title="{(s_i,s_j)}" /> is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />-close to the uniform distribution over <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ 0,1 \}^2}" class="latex" title="{\{ 0,1 \}^2}" /> in total variation distance. This is the same thing as asking for a small-support distribution that is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />-pseudorandom for all functions <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ 0,1 \}^n \rightarrow \{ 0,1 \}}" class="latex" title="{\{ 0,1 \}^n \rightarrow \{ 0,1 \}}" /> that depend on only two input variables. There are only <img src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(n^2)}" class="latex" title="{O(n^2)}" /> such functions, so the above construction gives us a pseudorandom distribution that is uniform over a set of size <img src="https://s0.wp.com/latex.php?latex=%7BO%28%5Cepsilon%5E%7B-2%7D+%5Cln+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{O(\epsilon^{-2} \ln n)}" class="latex" title="{O(\epsilon^{-2} \ln n)}" />, meaning that the distribution can be sampled using <img src="https://s0.wp.com/latex.php?latex=%7B%5Clog%5Clog+n+%2B+2+%5Clog+%5Cfrac+1+%5Cepsilon+%2B+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log\log n + 2 \log \frac 1 \epsilon + O(1)}" class="latex" title="{\log\log n + 2 \log \frac 1 \epsilon + O(1)}" /> random bits. Furthermore the algorithm can be implemented to run in time <img src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7BO%281%29%7D+%2F+%5Cepsilon%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n^{O(1)} / \epsilon^2}" class="latex" title="{n^{O(1)} / \epsilon^2}" />. The only tricky step is how to find the string <img src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s_t}" class="latex" title="{s_t}" /> at each step. For a string <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s}" class="latex" title="{s}" />, the loss <img src="https://s0.wp.com/latex.php?latex=%7Bf+%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f (x_t)}" class="latex" title="{f (x_t)}" /> obtained by choosing <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s}" class="latex" title="{s}" /> as the “reference string” is a polynomial of degree 2 in the bits of <img src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s}" class="latex" title="{s}" />, so we can find a no-worse-than-average <img src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{s_t}" class="latex" title="{s_t}" /> using the method of conditional expectations. I am not sure if there is a more standard way of doing this construction, perhaps one in which the bit <img src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{j}" class="latex" title="{j}" /> of the <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" />-th string in the sample space can be generated in time <img src="https://s0.wp.com/latex.php?latex=%7B%28%5Clog+n%29%5E%7BO%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(\log n)^{O(1)}}" class="latex" title="{(\log n)^{O(1)}}" />. The standard approach is to combine a small-bias generator with a linear family of pairwise independent hash functions, but even using Ta-Shma’s construction of small-bias generators we would not get the correct dependency on <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon}" class="latex" title="{\epsilon}" />. This framework can “derandomize Chernoff bounds” in other settings as well, such as randomized rounding of packing and covering integer linear programs, and it is basically the same thing as the method of “pessimistic estimators” described in the Motwani-Raghavan book on randomized algorithms. </p>
<p></p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2019/04/25/online-optimization-post-2-constructing-pseudorandom-sets/"><span class="datestr">at April 26, 2019 01:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=351">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2019/04/25/tcs-talk-wednesday-may-1st-chris-peikert-university-of-michigan/">TCS+ talk: Wednesday, May 1st — Chris Peikert, University of Michigan</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk will take place this coming Wednesday, May 1st at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). <strong><a href="http://web.eecs.umich.edu/~cpeikert/">Chris Peikert</a></strong> from University of Michigan will speak about “<em>Noninteractive Zero Knowledge for NP from Learning With Errors</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: We finally close the long-standing problem of constructing a noninteractive zero-knowledge (NIZK) proof system for any NP language with security based on the Learning With Errors (LWE) problem, and thereby on worst-case lattice problems. Our proof system instantiates a framework developed in a series of recent works for soundly applying the Fiat—Shamir transform using a hash function family that is <em>correlation intractable</em> for a suitable class of relations. Previously, such hash families were based either on “exotic” assumptions (e.g., indistinguishability obfuscation or optimal hardness of ad-hoc LWE variants) or, more recently, on the existence of circularly secure fully homomorphic encryption. However, none of these assumptions are known to be implied by LWE or worst-case hardness.</p>
<p>Our main technical contribution is a hash family that is correlation intractable for arbitrary size-<img src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" alt="S" class="latex" title="S" /> circuits, for any polynomially bounded <img src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" alt="S" class="latex" title="S" />, based on LWE (with small polynomial approximation factors). Our construction can be instantiated in two possible “modes,” yielding a NIZK that is either computationally sound and statistically zero knowledge in the common random string model, or vice-versa in the common reference string model.</p>
<p>(This is joint work with Sina Shiehian. Paper: <a href="https://eprint.iacr.org/2019/158" target="_blank" rel="noopener">https://eprint.iacr.org/2019/158</a>)</p></blockquote></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2019/04/25/tcs-talk-wednesday-may-1st-chris-peikert-university-of-michigan/"><span class="datestr">at April 25, 2019 08:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://corner.mimuw.edu.pl/?p=1084">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/banach.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="http://corner.mimuw.edu.pl/?p=1084">Call for Participation: HALG 2019 (Highlights of Algorithms)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>-------------------------------------------------------------------<br />
4rd Highlights of Algorithms conference (HALG 2019)<br />
Copenhagen, June 14-16, 2019<br />
<a href="http://highlightsofalgorithms.org/" target="_blank" rel="noopener noreferrer">http://highlightsofalgorithms.org/</a></p>
<p>The Highlights of Algorithms conference is a forum for presenting the<br />
highlights of recent developments in algorithms and for discussing<br />
potential further advances in this area. The conference will provide a<br />
broad picture of the latest research in algorithms through a series of<br />
invited talks, as well as the possibility for all researchers and<br />
students to present their recent results through a series of short<br />
talks and poster presentations. Attending the Highlights of Algorithms<br />
conference will also be an opportunity for networking and meeting<br />
leading researchers in algorithms.<br />
-------------------------------------------------------------------</p>
<p>PROGRAM</p>
<p>The conference will begin on Friday, June 14, at 9:00 and end on<br />
Sunday, June 16, at 18:00. A detailed schedule and a list of all<br />
accepted short contributions can be found at:<br />
<a href="http://2018.highlightsofalgorithms.org/programme" target="_blank" rel="noopener noreferrer">2018.highlightsofalgorithms.org/programme</a>.</p>
<p>-------------------------------------------------------------------</p>
<p>REGISTRATION</p>
<p>Please register on our webpage<br />
     <a href="http://highlightsofalgorithms.org/registration" target="_blank" rel="noopener noreferrer">http://highlightsofalgorithms.org/registration</a><br />
We have done our best to keep registration fees at a minimum:</p>
<p>Early registration (by April 29, 2019)<br />
- academic rate (incl. postdocs): 160€<br />
- student rate: 115€</p>
<p>Regular registration will be 50€ more expensive.</p>
<p>The organizers strongly recommend that you book your hotel as soon as possible.</p>
<p>-------------------------------------------------------------------</p>
<p>CONFERENCE VENUE</p>
<p>The conference will take place at the H.C. Ørsted Institute of the<br />
University of Copenhagen.<br />
The address is: Universitetsparken 5, DK-2100 Copenhagen.</p>
<p>-------------------------------------------------------------------</p>
<p>INVITED SPEAKERS</p>
<p>Survey speakers:<br />
Monika Henzinger (University of Vienna)<br />
Thomas Vidick (California Institute of Technology)<br />
Laszlo Vegh (London School of Economics)<br />
James Lee (University of Washington)<br />
Timothy Chan (University of Illinois at Urbana-Champaign)<br />
Sergei Vassilvitskii (Google, New York)</p>
<p>Invited talks:<br />
Martin Grohe (RWTH Aachen University)<br />
Josh Alman (MIT)<br />
Nima Anari (Stanford University)<br />
Michal Koucký (Charles University)<br />
Naveen Garg (IIT Delhi)<br />
Vera Traub (University of Bonn)<br />
Rico Zenklusen (ETH Zurich)<br />
Shayan Oveis Gharan (University of Washington)<br />
Greg Bodwin (MIT)<br />
Cliff Stein (Columbia University)<br />
Sungjin Im (University of California at Merced)<br />
C. Seshadhriy (University of California, Santa Cruz)<br />
Shay Moran (Technion)<br />
Bundit Laekhanukit (Shanghai University of Finance and Economics)<br />
Sebastien Bubeck (Microsoft Research, Redmond)<br />
Sushant Sachdeva (University of Toronto)<br />
Kunal Talwar (Google Brain)<br />
Moses Charikar (Stanford University)<br />
Shuichi Hirahara (University of Tokyo)</p>
<p>------------------------------------------------------------------</p></div>







<p class="date">
by sank <a href="http://corner.mimuw.edu.pl/?p=1084"><span class="datestr">at April 25, 2019 12:36 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-8078353386966881451">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/04/geo-centric-complexity.html">Geo-Centric Complexity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
An interesting discussion during Dagstuhl last month about the US-centric view of theory. Bad enough that all talks and papers in an international venue are in English but we also have<br />
<ul>
<li><a href="https://en.wiktionary.org/wiki/Manhattan_distance">Manhattan Distance</a>. How are foreigners supposed to know about the structure of streets in New York? What's wrong with grid distance?</li>
<li><a href="https://en.wikipedia.org/wiki/Las_Vegas_algorithm">Las Vegas Algorithms</a>. I found this one a little unfair, after all Monte Carlo algorithms came first. Still today might not Macau algorithms make sense?</li>
<li><a href="https://en.wikipedia.org/wiki/Arthur%E2%80%93Merlin_protocol">Arthur-Merlin Games</a>. A British reference by a Hungarian living in the US (László Babai who also coined Las Vegas algorithms). Still the Chinese might not know the fables. Glad the Europeans don't remember the <a href="https://blog.computationalcomplexity.org/2017/04/alice-and-bob-and-pat-and-vanna.html">Pat and Vanna</a> terminology I used in my first STOC talk. </li>
<li>Alice and Bob. The famous pair of cryptographers but how generically American can you get. Why not Amare and Bhati?</li>
</ul>
<div>
I have two minds here. We shouldn't alienate or confuse those who didn't grow up in an Anglo-American culture. On the other hand, I hate to have to try and make all terminology culturally neutral, you'd just end up with technical and ugly names, like P and NP.</div></div>







<p class="date">
by Lance Fortnow (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/04/geo-centric-complexity.html"><span class="datestr">at April 25, 2019 12:12 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=17389">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2019/04/25/are-natural-mathematical-problems-bad-problems/">Are Natural Mathematical Problems Bad Problems?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>One unique aspect of the conference “Visions in Mathematics Towards 2000” (see the <a href="https://gilkalai.wordpress.com/2019/04/23/an-invitation-to-a-conference-visions-in-mathematics-towards-2000/">previous post</a>) was that there were several discussion sessions where speakers and other participants presented some thoughts about mathematics (or some specific areas), discussed and argued.  In the lectures themselves you could also see a large amount of audience participation and discussions which was very nice.</p>
<p>Let me draw your attention to  one question raised and discussed in one of the discussion sessions.</p>
<h3><a href="https://youtu.be/Fme_r-nE4CI?t=1400">3.4 Discussion on Geometry with introduction by M. Gromov</a></h3>
<p></p>
<p>Now, lets skip a lot of interesting staff and move <a href="https://youtu.be/Fme_r-nE4CI?t=1400">to minute 23:20</a> where Noga Alon asked Misha Gromov to elaborate a statement from his <a href="https://youtu.be/gd6EB2Zk6OE">opening lecture of the conference</a> that  the densest packing problem in <img src="https://s0.wp.com/latex.php?latex=R%5E3&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="R^3" class="latex" title="R^3" /> is not interesting.  In what follows Misha Gromov passionately argued that natural problems are bad problems (or are even stupid questions), and a lovely discussion emerged (in 25:00 Yuval Neeman commented about cosmology in response to Connes’s earlier remarks but then around 27:00 Vitali asked Misha to name some bad problems in geometry and the discussion resumed.) Misha made several lovely provocative further comments: he rejected the claim that this is a matter of taste, and argued that people make conjectures when they absolutely have no right to do so.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/misha-natural-bad.png"><img src="https://gilkalai.files.wordpress.com/2019/04/misha-natural-bad.png?w=640&amp;h=361" alt="" width="640" class="alignnone size-full wp-image-17390" height="361" /></a></p>
<p><strong><span style="color: #ff0000;"> Misha argues passionately that natural problems are stupid problems</span></strong></p>
<p>Actually one problem that Misha mentioned in his lecture as interesting (see also Gromov’s proceedings paper <a href="https://www.ihes.fr/~gromov/wp-content/uploads/2018/08/SpacesandQuestions.pdf">Spaces and questions),</a> and that was raised both by him and by me is to prove an exponential upper bound for the number of simplicial 3-spheres with n facets. I remember that we talked about it in the conference and Misha was certain that the problem could be solved for shellable spheres while I was confident that the case of shellable spheres would be as hard as the general case.  He was right! This goes back to works of physicists Durhuus and Jonsson see this paper <a href="https://arxiv.org/abs/0902.0436">On locally constructible spheres and balls</a> by Bruno Benedetti and  Günter M. Ziegler.</p>
<h5>(Disclaimer: I asked quite a few questions that were both unnatural and stupid and made several conjectures when I had no right to do so.)</h5>
<p><span id="more-17389"></span></p>
<p>encore</p>
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/v1.png"><img src="https://gilkalai.files.wordpress.com/2019/04/v1.png?w=300&amp;h=188" alt="" width="300" class="alignnone size-medium wp-image-17401" height="188" /></a>  <a href="https://gilkalai.files.wordpress.com/2019/04/v2.png"><img src="https://gilkalai.files.wordpress.com/2019/04/v2.png?w=300&amp;h=210" alt="" width="300" class="alignnone size-medium wp-image-17402" height="210" />  </a></p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/n1.png"><img src="https://gilkalai.files.wordpress.com/2019/04/n1.png?w=300&amp;h=227" alt="" width="300" class="alignnone size-medium wp-image-17403" height="227" /></a> <a href="https://gilkalai.files.wordpress.com/2019/04/n2.png"><img src="https://gilkalai.files.wordpress.com/2019/04/n2.png?w=214&amp;h=300" alt="" width="214" class="alignnone size-medium wp-image-17404" height="300" /></a></p>
<p><span style="color: #ff0000;">Vitali Milman attacked the solution of the 4CT as “bad”and Segei Novikov disagreed and referred to the proof as “great”.  </span></p>
<p> </p>
<p> </p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2019/04/25/are-natural-mathematical-problems-bad-problems/"><span class="datestr">at April 25, 2019 09:42 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://lucatrevisan.wordpress.com/?p=4236">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/trevisan.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://lucatrevisan.wordpress.com/2019/04/24/online-optimization-post-1-multiplicative-weights/">Online Optimization Post 1: Multiplicative Weights</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
 The <em>multiplicative weights</em> or <em>hedge</em> algorithm is the most well known and most frequently rediscovered algorithm in online optimization. </p>
<p>
The problem it solves is usually described in the following language: we want to design an algorithm that makes the best possible use of the advice coming from <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> self-described experts. At each time step <img src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%2C2%2C%5Cldots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t=1,2,\ldots}" class="latex" title="{t=1,2,\ldots}" />, the algorithm has to decide with what probability to follow the advice of each of the experts, that is, the algorithm has to come up with a probability distribution <img src="https://s0.wp.com/latex.php?latex=%7Bx_t+%3D+%28x_t%281%29%2C%5Cldots%2Cx_t%28n%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t = (x_t(1),\ldots,x_t(n))}" class="latex" title="{x_t = (x_t(1),\ldots,x_t(n))}" /> where <img src="https://s0.wp.com/latex.php?latex=%7Bx_t+%28i%29+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t (i) \geq 0}" class="latex" title="{x_t (i) \geq 0}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sum_{i=1}^n x_t(i)=1}" class="latex" title="{\sum_{i=1}^n x_t(i)=1}" />. After the algorithm makes this choice, it is revealed that following the advice of expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> leads to loss <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t+%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_t (i)}" class="latex" title="{\ell_t (i)}" />, so that the expected loss of the algorithm at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> is <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Cell_t+%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sum_{i=1}^n x_t(i) \ell_t (i)}" class="latex" title="{\sum_{i=1}^n x_t(i) \ell_t (i)}" />. A loss can be negative, in which case its absolute value can be interpreted as a profit.</p>
<p>
After <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps, the algorithm “regrets” that it did not just always follow the advice of the expert that, with hindsight, was the best one, so that the regret of the algorithm after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%3D+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Cell_t%28i%29+%5Cright%29+-+%5Cleft%28+%5Cmin_%7Bi%3D1%2C%5Cldots%2Cn%7D+%5C+%5C+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28i%29+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T = \left( \sum_{t=1}^T\sum_{i=1}^n x_t(i) \ell_t(i) \right) - \left( \min_{i=1,\ldots,n} \ \ \sum_{t=1}^T \ell_t(i) \right) " class="latex" title="\displaystyle  {\rm Regret}_T = \left( \sum_{t=1}^T\sum_{i=1}^n x_t(i) \ell_t(i) \right) - \left( \min_{i=1,\ldots,n} \ \ \sum_{t=1}^T \ell_t(i) \right) " /></p>
<p>
This corresponds to the instantiation of the framework we described in the previous post to the special case in which the set of feasible solutions <img src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{K}" class="latex" title="{K}" /> is the set <img src="https://s0.wp.com/latex.php?latex=%7B%5CDelta+%5Csubseteq+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Delta \subseteq {\mathbb R}^n}" class="latex" title="{\Delta \subseteq {\mathbb R}^n}" /> of probability distributions over the sample space <img src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+1%2C%5Cldots%2Cn%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\{ 1,\ldots,n\}}" class="latex" title="{\{ 1,\ldots,n\}}" /> and in which the loss functions <img src="https://s0.wp.com/latex.php?latex=%7Bf_t+%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t (x)}" class="latex" title="{f_t (x)}" /> are linear functions of the form <img src="https://s0.wp.com/latex.php?latex=%7Bf_t+%28x%29+%3D+%5Csum_i+x%28i%29+%5Cell_t+%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{f_t (x) = \sum_i x(i) \ell_t (i)}" class="latex" title="{f_t (x) = \sum_i x(i) \ell_t (i)}" />. In order to bound the regret, we also have to bound the “magnitude” of the loss functions, so in the following we will assume that for all <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> and all <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> we have <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Cell_t+%28i%29+%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \ell_t (i) | \leq 1}" class="latex" title="{| \ell_t (i) | \leq 1}" />, and otherwise we can scale everything by a known upper bound on <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmax_%7Bt%2Ci%7D+%7C%5Cell_t+%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\max_{t,i} |\ell_t |}" class="latex" title="{\max_{t,i} |\ell_t |}" />.</p>
<p>
We now describe the algorithm.</p>
<p>
The algorithm maintains at each step <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> a vector of <em>weights</em> <img src="https://s0.wp.com/latex.php?latex=%7Bw_t+%3D+%28w_t%281%29%2C%5Cldots%2Cw_t%28n%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w_t = (w_t(1),\ldots,w_t(n))}" class="latex" title="{w_t = (w_t(1),\ldots,w_t(n))}" /> which is initialized as <img src="https://s0.wp.com/latex.php?latex=%7Bw_1+%3A%3D+%281%2C%5Cldots%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w_1 := (1,\ldots,1)}" class="latex" title="{w_1 := (1,\ldots,1)}" />. The algorithm performs the following operations at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />: </p>
<ul>
<li> <img src="https://s0.wp.com/latex.php?latex=%7Bw_t+%28i%29+%3A%3D+w_%7Bt-1%7D+%28i%29+%5Ccdot+e%5E%7B-%5Cepsilon+%5Cell_%7Bt-1%7D+%28i%29+%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{w_t (i) := w_{t-1} (i) \cdot e^{-\epsilon \ell_{t-1} (i) }}" class="latex" title="{w_t (i) := w_{t-1} (i) \cdot e^{-\epsilon \ell_{t-1} (i) }}" />
</li><li> <img src="https://s0.wp.com/latex.php?latex=%7Bx_t+%28i%29+%3A%3D+%5Cdisplaystyle+%5Cfrac+%7Bw_t+%28i%29+%7D%7B%5Csum_%7Bj%3D1%7D%5En+w_t%28j%29+%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t (i) := \displaystyle \frac {w_t (i) }{\sum_{j=1}^n w_t(j) }}" class="latex" title="{x_t (i) := \displaystyle \frac {w_t (i) }{\sum_{j=1}^n w_t(j) }}" />
</li></ul>
<p>
That is, the weight of expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> is <img src="https://s0.wp.com/latex.php?latex=%7Be%5E%7B-%5Cepsilon+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+%28i%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e^{-\epsilon \sum_{k=1}^{t-1} \ell_k (i)}}" class="latex" title="{e^{-\epsilon \sum_{k=1}^{t-1} \ell_k (i)}}" />, and the probability <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t(i)}" class="latex" title="{x_t(i)}" /> of following the advice of expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> is proportional to the weight. The parameter <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon&gt;0}" class="latex" title="{\epsilon&gt;0}" /> is hardwired into the algorithm and we will optimize it later. Note that the algorithm gives higher weight to experts that produced small losses (or negative losses of large absolute value) in the past, and thus puts higher probability on such experts.</p>
<p>
We will prove the following bound.</p>
<blockquote><p><b>Theorem 1</b> <em> Assuming that for all <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> we have <img src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Cell_t%28i%29+%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{| \ell_t(i) | \leq 1}" class="latex" title="{| \ell_t(i) | \leq 1}" />, for every <img src="https://s0.wp.com/latex.php?latex=%7B0+%3C+%5Cepsilon+%3C+1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0 &lt; \epsilon &lt; 1/2}" class="latex" title="{0 &lt; \epsilon &lt; 1/2}" />, after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps the multiplicative weight algorithm experiences a regret that is always bounded as </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Cell%5E2+_t+%28i%29+%2B+%5Cfrac+%7B%5Cln+n%7D%7B%5Cepsilon%7D+%5Cleq+%5Cepsilon+T+%2B+%5Cfrac+%7B%5Cln+n%7D%7B%5Cepsilon%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n x_t(i) \ell^2 _t (i) + \frac {\ln n}{\epsilon} \leq \epsilon T + \frac {\ln n}{\epsilon} " class="latex" title="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n x_t(i) \ell^2 _t (i) + \frac {\ln n}{\epsilon} \leq \epsilon T + \frac {\ln n}{\epsilon} " /></p>
<p> In particular, if <img src="https://s0.wp.com/latex.php?latex=%7BT+%3E+4+%5Cln+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T &gt; 4 \ln n}" class="latex" title="{T &gt; 4 \ln n}" />, by setting <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3D+%5Csqrt%7B%5Cfrac%7B%5Cln+n%7D%7BT%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon = \sqrt{\frac{\ln n}{T}}}" class="latex" title="{\epsilon = \sqrt{\frac{\ln n}{T}}}" /> we achieve a regret bound </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+2+%5Csqrt%7BT+%5Cln+n%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  {\rm Regret}_T \leq 2 \sqrt{T \ln n} " class="latex" title="\displaystyle  {\rm Regret}_T \leq 2 \sqrt{T \ln n} " /></p>
</em><p><em> </em></p></blockquote>
<p></p><p>
<span id="more-4236"></span></p>
<p>
We will start by giving a short proof of the above theorem. </p>
<p>
For each time step <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />, define the quantity</p>
<p></p><p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_t+%3A%3D+%5Csum_%7Bi%3D1%7D%5En+w_t%28i%29+%5C+.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  W_t := \sum_{i=1}^n w_t(i) \ . " class="latex" title="\displaystyle  W_t := \sum_{i=1}^n w_t(i) \ . " /></p>
<p> We want to prove that, roughly speaking, the only way for an adversary to make the algorithm incur a large loss is to produce a sequence of loss functions such that <em>even the best expert incurs a large loss</em>. The proof will work by showing that if the algorithm incurs a large loss after <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> steps, then <img src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W_{T+1}}" class="latex" title="{W_{T+1}}" /> is small, and that if <img src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W_{T+1}}" class="latex" title="{W_{T+1}}" /> is small, then even the best expert incurs a large loss.</p>
<p>
Let us define </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++L%5E%2A+%3D+%5Cmin_%7Bi+%3D+1%2C%5Cldots%2C+n%7D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t+%28i%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  L^* = \min_{i = 1,\ldots, n} \sum_{t=1}^T \ell_t (i) " class="latex" title="\displaystyle  L^* = \min_{i = 1,\ldots, n} \sum_{t=1}^T \ell_t (i) " /></p>
<p> to be the loss of the best expert. Then we have</p>
<blockquote><p><b>Lemma 2 (If <img src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W_{T+1}}" class="latex" title="{W_{T+1}}" /> is small, then <img src="https://s0.wp.com/latex.php?latex=%7BL%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{L^*}" class="latex" title="{L^*}" /> is large)</b> <em> </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7BT%2B1%7D+%5Cgeq+e%5E%7B-%5Cepsilon+L%5E%2A%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  W_{T+1} \geq e^{-\epsilon L^*} " class="latex" title="\displaystyle  W_{T+1} \geq e^{-\epsilon L^*} " /></p>
</em><p><em> </em></p></blockquote>
<p></p><p>
<em>Proof:</em>  Let <img src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{j}" class="latex" title="{j}" /> be an index such that <img src="https://s0.wp.com/latex.php?latex=%7BL%5E%2A+%3D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t+%28j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{L^* = \sum_{t=1}^T \ell_t (j)}" class="latex" title="{L^* = \sum_{t=1}^T \ell_t (j)}" />. Then we have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7BT%2B1%7D+%3D+%5Csum_%7Bi%3D1%7D%5En+e%5E%7B-%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28i%29+%7D+%5Cgeq+e%5E%7B-%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28j%29%7D+%3D+e%5E%7B-%5Cepsilon+L%5E%2A%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  W_{T+1} = \sum_{i=1}^n e^{-\epsilon \sum_{t=1}^T \ell_t(i) } \geq e^{-\epsilon \sum_{t=1}^T \ell_t(j)} = e^{-\epsilon L^*} " class="latex" title="\displaystyle  W_{T+1} = \sum_{i=1}^n e^{-\epsilon \sum_{t=1}^T \ell_t(i) } \geq e^{-\epsilon \sum_{t=1}^T \ell_t(j)} = e^{-\epsilon L^*} " /></p>
<p> <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<blockquote><p><b>Lemma 3 (If the loss of the algorithm is large then <img src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W_{T+1}}" class="latex" title="{W_{T+1}}" /> is small)</b> <em> </em></p><em>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7BT%2B1%7D+%5Cleq+n+%5Cprod_%7Bt%3D1%7D%5En+%281+-+%5Cepsilon+%5Clangle+x_t+%2C+%5Cell_t+%5Crangle+%2B+%5Cepsilon%5E2+%5Clangle+x_t+%2C+%5Cell%5E2_t+%5Crangle%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  W_{T+1} \leq n \prod_{t=1}^n (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t , \ell^2_t \rangle) " class="latex" title="\displaystyle  W_{T+1} \leq n \prod_{t=1}^n (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t , \ell^2_t \rangle) " /></p>
</em><p><em> where <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell_t^2}" class="latex" title="{\ell_t^2}" /> is the vector whose <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" />-th coordinate is <img src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%28+%5Cell_t+%28i%29%5Cright%29%5E2+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\left( \ell_t (i)\right)^2 }" class="latex" title="{\left( \ell_t (i)\right)^2 }" /> </em></p></blockquote>
<p></p><p>
<em>Proof:</em>  Since we know that <img src="https://s0.wp.com/latex.php?latex=%7BW_1+%3D+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W_1 = n}" class="latex" title="{W_1 = n}" />, it is enough to prove that, for every <img src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%2C%5Cldots%2C+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t=1,\ldots, T}" class="latex" title="{t=1,\ldots, T}" />, we have <a name="eq.lemma.two"></a></p><a name="eq.lemma.two">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7Bt%2B1%7D+%5Cleq+%281+-+%5Cepsilon+%5Clangle+x_t+%2C+%5Cell_t+%5Crangle+%2B+%5Cepsilon%5E2+%5Clangle+x_t%2C+%5Cell_t%5E2+%5Crangle+%29+%5Ccdot+W_t++%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  W_{t+1} \leq (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t, \ell_t^2 \rangle ) \cdot W_t  \ \ \ \ \ (1)" class="latex" title="\displaystyle  W_{t+1} \leq (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t, \ell_t^2 \rangle ) \cdot W_t  \ \ \ \ \ (1)" /></p>
</a><p><a name="eq.lemma.two"></a> And we see that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7BW_%7Bt%2B1%7D%7D%7BW_t%7D+%3D+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac+%7Bw_%7Bt%2B1%7D%28i%29%7D%7BW_t%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \frac{W_{t+1}}{W_t} = \sum_{i=1}^n \frac {w_{t+1}(i)}{W_t} " class="latex" title="\displaystyle  \frac{W_{t+1}}{W_t} = \sum_{i=1}^n \frac {w_{t+1}(i)}{W_t} " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac+%7Bw_t%28i%29+%5Ccdot+e%5E%7B-%5Cepsilon+%5Cell_t+%28i%29+%7D+%7D%7BW_t%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \sum_{i=1}^n \frac {w_t(i) \cdot e^{-\epsilon \ell_t (i) } }{W_t} " class="latex" title="\displaystyle  = \sum_{i=1}^n \frac {w_t(i) \cdot e^{-\epsilon \ell_t (i) } }{W_t} " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Ccdot+e%5E%7B-%5Cepsilon+%5Cell_t%28i%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = \sum_{i=1}^n x_t(i) \cdot e^{-\epsilon \ell_t(i) } " class="latex" title="\displaystyle  = \sum_{i=1}^n x_t(i) \cdot e^{-\epsilon \ell_t(i) } " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Ccdot+%28+1+-+%5Cepsilon+%5Cell_t+%28i%29+%2B+%5Cepsilon%5E2+%5Cell_t%5E2%28i%29+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \leq \sum_{i=1}^n x_t(i) \cdot ( 1 - \epsilon \ell_t (i) + \epsilon^2 \ell_t^2(i) ) " class="latex" title="\displaystyle  \leq \sum_{i=1}^n x_t(i) \cdot ( 1 - \epsilon \ell_t (i) + \epsilon^2 \ell_t^2(i) ) " /></p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+1+-+%5Cepsilon+%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%2B+%5Cepsilon%5E2+%5Clangle+%5Cell_t%5E2+%2C+x_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  = 1 - \epsilon \langle x_t, \ell_t \rangle + \epsilon^2 \langle \ell_t^2 , x_t \rangle " class="latex" title="\displaystyle  = 1 - \epsilon \langle x_t, \ell_t \rangle + \epsilon^2 \langle \ell_t^2 , x_t \rangle " /></p>
<p> where we used the definitions of our quantities and the fact that <img src="https://s0.wp.com/latex.php?latex=%7Be%5E%7B-z%7D+%5Cleq+1-z%2Bz%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{e^{-z} \leq 1-z+z^2}" class="latex" title="{e^{-z} \leq 1-z+z^2}" /> for <img src="https://s0.wp.com/latex.php?latex=%7B%7Cz%7C+%5Cleq+1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|z| \leq 1/2}" class="latex" title="{|z| \leq 1/2}" />. <img src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\Box" class="latex" title="\Box" /></p>
<p>
Using the fact that <img src="https://s0.wp.com/latex.php?latex=%7B1-z+%5Cleq+e%5E%7B-z%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1-z \leq e^{-z}}" class="latex" title="{1-z \leq e^{-z}}" /> for all <img src="https://s0.wp.com/latex.php?latex=%7B%7Cz%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{|z| \leq 1}" class="latex" title="{|z| \leq 1}" />, the above lemmas can be restated as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cln+W_%7BT%2B1%7D+%5Cleq+%5Cln+n+-+%5Cleft%28%5Csum_%7Bt%3D1%7D%5ET+%5Cepsilon+%5Clangle+%5Cell_t+%2C+x_t+%5Crangle+%5Cright%29+%2B+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET%5Cepsilon%5E2+%5Clangle+%5Cell_t%5E2+x_t%5Crangle+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \ln W_{T+1} \leq \ln n - \left(\sum_{t=1}^T \epsilon \langle \ell_t , x_t \rangle \right) + \left( \sum_{t=1}^T\epsilon^2 \langle \ell_t^2 x_t\rangle \right) " class="latex" title="\displaystyle  \ln W_{T+1} \leq \ln n - \left(\sum_{t=1}^T \epsilon \langle \ell_t , x_t \rangle \right) + \left( \sum_{t=1}^T\epsilon^2 \langle \ell_t^2 x_t\rangle \right) " /></p>
<p> and </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cln+W_%7BT%2B1%7D+%5Cgeq+-+%5Cepsilon+L%5E%2A+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \ln W_{T+1} \geq - \epsilon L^* " class="latex" title="\displaystyle  \ln W_{T+1} \geq - \epsilon L^* " /></p>
<p> which together imply </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+%5Cell_t+%2C+x_t+%5Crangle+%5Cright%29+-+L%5E%2A+%5Cleq+%5Cfrac%7B%5Cln+n%7D%7B%5Cepsilon%7D+%2B+%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+%5Cell%5E2_t+%2C+x_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \left( \sum_{t=1}^T \langle \ell_t , x_t \rangle \right) - L^* \leq \frac{\ln n}{\epsilon} + \epsilon \sum_{t=1}^T \langle \ell^2_t , x_t \rangle " class="latex" title="\displaystyle  \left( \sum_{t=1}^T \langle \ell_t , x_t \rangle \right) - L^* \leq \frac{\ln n}{\epsilon} + \epsilon \sum_{t=1}^T \langle \ell^2_t , x_t \rangle " /></p>
<p> as desired.</p>
<p>
Personally, I find all of the above very unsatisfactory, because both the algorithm and the analysis, but especially the analysis, seem to come out of nowhere. In fact, I never felt that I actually understood this analysis until I saw it presented as a special case of the <em>Follow The Regularized Leader</em> framework that we will discuss in a future post. (We will actually prove a slightly weaker bound, but with a much more satisfying proof.)</p>
<p>
Here is, however, a story of how a statistical physicist might have invented the algorithm and might have come up with the analysis. Let’s call the loss caused by expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> after <img src="https://s0.wp.com/latex.php?latex=%7Bt-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t-1}" class="latex" title="{t-1}" /> steps the <em>energy</em> of expert <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E_t%28i%29+%3D+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k%28i%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  E_t(i) = \sum_{k=1}^{t-1} \ell_k(i) " class="latex" title="\displaystyle  E_t(i) = \sum_{k=1}^{t-1} \ell_k(i) " /></p>
<p> Note that we have defined it in such a way that the algorithm knows <img src="https://s0.wp.com/latex.php?latex=%7BE_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{E_t(i)}" class="latex" title="{E_t(i)}" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />. Our offline optimum is the energy of the lowest energy expert at time <img src="https://s0.wp.com/latex.php?latex=%7BT%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T+1}" class="latex" title="{T+1}" />, that, is, the energy of the <em>ground state</em> at time <img src="https://s0.wp.com/latex.php?latex=%7BT%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T+1}" class="latex" title="{T+1}" />. When we have a collection of numbers <img src="https://s0.wp.com/latex.php?latex=%7BE_t%281%29%2C%5Cldots%2C+E_t%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{E_t(1),\ldots, E_t(n)}" class="latex" title="{E_t(1),\ldots, E_t(n)}" />, a nice lower bound to their minimum is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin_i+E_t%28i%29+%5Cgeq+-+%5Cfrac+1+%5Cepsilon+%5Cln+%5Csum_%7Bi%3D1%7D%5En+e%5E%7B-%5Cepsilon+E_t%28i%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \min_i E_t(i) \geq - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } " class="latex" title="\displaystyle  \min_i E_t(i) \geq - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } " /></p>
<p> which is true for every <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon &gt;0}" class="latex" title="{\epsilon &gt;0}" />. The right-hand side above is the <em>free energy</em> at temperature <img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+1+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac 1 \epsilon}" class="latex" title="{\frac 1 \epsilon}" /> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />. This seems like the kind of expression that we could use to bound the offline optimum, so let’s give it a name </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPhi_t+%3A%3D+-+%5Cfrac+1+%5Cepsilon+%5Cln+%5Csum_%7Bi%3D1%7D%5En+e%5E%7B-%5Cepsilon+E_t%28i%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \Phi_t := - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } " class="latex" title="\displaystyle  \Phi_t := - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } " /></p>
<p> In terms of coming up with an algorithm, all that we have got to work with at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> are the losses of the experts at times <img src="https://s0.wp.com/latex.php?latex=%7B1%2C%5Cldots%2Ct-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1,\ldots,t-1}" class="latex" title="{1,\ldots,t-1}" />. If the adversary chooses to make one of the experts consistently much better than the others, it is clear that, in order to get any reasonable regret bound, the algorithm will have to put much of the probability mass in most of the steps on that expert. This suggests that the <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> should put higher probability on experts that have done well in the first <img src="https://s0.wp.com/latex.php?latex=%7Bt-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t-1}" class="latex" title="{t-1}" /> steps, that is <img src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x_t}" class="latex" title="{x_t}" /> should put higher probability on “lower-energy” experts. When we have a system in which, at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />, state <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> has energy <img src="https://s0.wp.com/latex.php?latex=%7BE_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{E_t(i)}" class="latex" title="{E_t(i)}" />, a standard distribution that puts higher probability on lower energy states is the <em>Gibbs distribution</em> at temperature <img src="https://s0.wp.com/latex.php?latex=%7B1%2F%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1/\epsilon}" class="latex" title="{1/\epsilon}" />, defined as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t%28i%29+%3D+%5Cfrac+%7B+e%5E%7B-%5Cepsilon+E_t+%28i%29%7D+%7D%7B%5Csum_j+e%5E%7B-%5Cepsilon+E_t%28j%29+%7D+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  x_t(i) = \frac { e^{-\epsilon E_t (i)} }{\sum_j e^{-\epsilon E_t(j) } } " class="latex" title="\displaystyle  x_t(i) = \frac { e^{-\epsilon E_t (i)} }{\sum_j e^{-\epsilon E_t(j) } } " /></p>
<p> where the denominator above is also called the <em>partition function</em> at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Z_t+%3A%3D+%5Csum_%7Bj%3D1%7D%5En+e%5E%7B-%5Cepsilon+E_t%28j%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  Z_t := \sum_{j=1}^n e^{-\epsilon E_t(j) } " class="latex" title="\displaystyle  Z_t := \sum_{j=1}^n e^{-\epsilon E_t(j) } " /></p>
<p> So far we have “rediscovered” our multiplicative weights algorithm, and the quantity <img src="https://s0.wp.com/latex.php?latex=%7BW_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{W_t}" class="latex" title="{W_t}" /> that we had in our analysis gets interpreted as the partition function <img src="https://s0.wp.com/latex.php?latex=%7BZ_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z_t}" class="latex" title="{Z_t}" />. The fact that <img src="https://s0.wp.com/latex.php?latex=%7B%5CPhi_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Phi_{T+1}}" class="latex" title="{\Phi_{T+1}}" /> bounds the offline optimum suggests that we should use <img src="https://s0.wp.com/latex.php?latex=%7B%5CPhi_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\Phi_t}" class="latex" title="{\Phi_t}" /> as a potential function, and aim for an analysis involving a telescoping sum. Indeed some manipulations (the same as in the short proof above, but which are now more mechanical) give that the loss of the algorithm at time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%5Cleq+%5CPhi_%7Bt%2B1%7D+-+%5CPhi_%7Bt%7D+%2B+%5Clangle+x_t+%2C+%5Cell%5E2+_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \langle x_t, \ell_t \rangle \leq \Phi_{t+1} - \Phi_{t} + \langle x_t , \ell^2 _t \rangle " class="latex" title="\displaystyle  \langle x_t, \ell_t \rangle \leq \Phi_{t+1} - \Phi_{t} + \langle x_t , \ell^2 _t \rangle " /></p>
<p> which telescopes to give </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%5Cleq+%5CPhi_%7BT%2B1%7D+-+%5CPhi_1+%2B+%5Csum_%7Bt%3D1%7D%5ET%5Clangle+x_t+%2C+%5Cell%5E2+_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{t=1}^T \langle x_t, \ell_t \rangle \leq \Phi_{T+1} - \Phi_1 + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle " class="latex" title="\displaystyle  \sum_{t=1}^T \langle x_t, \ell_t \rangle \leq \Phi_{T+1} - \Phi_1 + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle " /></p>
<p> Recalling that </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPhi_1+%3D+-+%5Cfrac+1+%7B%5Cepsilon%7D+%5Cln+n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \Phi_1 = - \frac 1 {\epsilon} \ln n " class="latex" title="\displaystyle  \Phi_1 = - \frac 1 {\epsilon} \ln n " /></p>
<p> and </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPhi_%7BT%2B1%7D+%5Cleq+%5Cmin_%7Bj%3D1%2C%5Cldots%2C+n%7D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28j%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \Phi_{T+1} \leq \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) " class="latex" title="\displaystyle  \Phi_{T+1} \leq \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) " /></p>
<p> we have again </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%5Cright%29+-+%5Cleft%28+%5Cmin_%7Bj%3D1%2C%5Cldots%2C+n%7D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28j%29+%5Cright%29+%5Cleq+%5Cfrac%7B%5Cln+n%7D%7B%5Cepsilon%7D+%2B+%5Csum_%7Bt%3D1%7D%5ET%5Clangle+x_t+%2C+%5Cell%5E2+_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \left( \sum_{t=1}^T \langle x_t, \ell_t \rangle \right) - \left( \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) \right) \leq \frac{\ln n}{\epsilon} + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle " class="latex" title="\displaystyle  \left( \sum_{t=1}^T \langle x_t, \ell_t \rangle \right) - \left( \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) \right) \leq \frac{\ln n}{\epsilon} + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle " /></p>
<p> As mentioned above, we will give a better story when we get to the <em>Follow The Regularized Leader</em> framework. In the next post, we will discuss complexity-theory consequences of the result we just proved. </p></div>







<p class="date">
by luca <a href="https://lucatrevisan.wordpress.com/2019/04/24/online-optimization-post-1-multiplicative-weights/"><span class="datestr">at April 25, 2019 06:44 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=15798">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/">Why Check A Proof?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>
<font color="#0044cc"><br />
<em>Why check another’s proof?</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/russellwhitehead1900s/" rel="attachment wp-att-15809"><img src="https://rjlipton.files.wordpress.com/2019/04/russellwhitehead1900s.png?w=300&amp;h=203" alt="" width="300" class="alignright size-medium wp-image-15809" height="203" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[Russell and Whitehead ]</font></td>
</tr>
</tbody>
</table>
<p>
Bertrand Russell and Alfred Whitehead were not primarily trying to mechanize mathematics in writing their famous book. They wanted to assure precision and certainty in proofs while minimizing the axioms and rules they rest on. They cared more about checking proofs than generating theorems. By the way: They are listed in the order Whitehead and Russell on the book. See <a href="https://thonyc.wordpress.com/2016/05/19/bertrand-russell-did-not-write-principia-mathematica/">this</a> for a discussion about the importance of the order.</p>
<p><a href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/unknown-120/" rel="attachment wp-att-15804"><img src="https://rjlipton.files.wordpress.com/2019/04/unknown-1.jpeg?w=600" alt="" class="aligncenter size-full wp-image-15804" /></a></p>
<p>
Today Ken and I thought we would add a few more thoughts on why proofs get checked.<br />
<span id="more-15798"></span></p>
<p>
We discussed those who <em>claim</em> proofs in our previous <a href="https://rjlipton.wordpress.com/2019/04/21/pnp-proofs/">post</a>. Once a proof is claimed, it needs people to check it. This is not as fraught as the <a href="https://en.wikipedia.org/wiki/Replication_crisis">replication crisis</a> in other sciences where “proof” is a statement of statistical significance whose most intensive check needs repeating the experiment. </p>
<p>
If you do a Google search on “why check proofs” you get lots of hits on using automated proof checkers. Coming on eleven decades after the publication of Russell and Whitehead’s three-volume <a href="https://en.wikipedia.org/wiki/Principia_Mathematica">opus</a> <em>Principia Mathematica</em>, these are still in their formative years. We <a href="https://rjlipton.wordpress.com/2013/07/14/surely-you-are-joking/">covered</a> a major system of this kind some years ago. </p>
<p>
We are personally more interested in what motivates us <em>humans</em> to check proofs. We believe that there are various factors that make it less or more likely to find a good human checker. So today we will try to list some of them. </p>
<p>
</p><p></p><h2> Why Check A Proof? </h2><p></p>
<p></p><p>
One of the questions that was raised by some commenters to our recent post is: <i>Why should I check your proof?</i></p>
<p>
This is a critical question. If their is no reason to check your proof, then your result will not get checked. It is almost a tautology. We like this question and thought we could suggest several ways to increase the likelihood that one will check another person’s proof. </p>
<p>
So lets assume that Alice is claiming some new theorem <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> and we ponder whether Bob will spend time checking it.</p>
<p>
</p><p></p><h3> Bob has to </h3><p></p>
<p>This happens when Bob is required to check her proof. This can happen if Bob is a referee of her paper. It could also be when Bob is hired to do this task. It usually is a weak reason for making someone do the checking. In real life we think that it is unlikely to be a strong motivator.</p>
<p>
</p><p></p><h3> Bob wants to </h3><p></p>
<p>This happens when Bob feels that he will benefit from checking. The main type of situation here is: Alice’s theorem <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> uses some new method or trick. If Bob believes that this method can be used in his work, in his research, in his future papers, then he is strongly motivated.</p>
<p>
We are all very self-centered in our research. If we think we could in the future use your method we are likely to spent time and energy on your proof. Thus if Bob is convinced that Alice has a some new ideas, he is much more likely to spent the time checking her theorem. This means that Alice should—if possible–explain that her proof of <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> uses something new. Proofs that are “just technical inductions” are very unlikely to get Bob to read them. In many areas some authors have stated things like: <i>The proof is a careful induction…</i> This is not a good idea. </p>
<p>
</p><p></p><h3> Bob needs to </h3><p></p>
<p>This happens when Bob has some “skin” in the game. A classic situation is when Bob has an earlier result that is affected by Alice’s new theorem. If <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> is stronger than Bob’s previous result, then he is motivated to check her theorem. Or if <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> shows that his earlier theorem is false, this is a very strong motivation. Or perhaps Alice has proven a lemma that enables Bob to push something through.</p>
<p>
</p><p></p><h2> Skin in the Game </h2><p></p>
<p></p><p>
Often we have situations where you do have skin in the game. An old <a href="https://rjlipton.wordpress.com/2009/09/27/surprises-in-mathematics-and-theory/">example</a> that comes to mind is from group theory. The problem is a natural question about a class of groups: Let <img src="https://s0.wp.com/latex.php?latex=%7BB%28m%2Cn%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B(m,n)}" class="latex" title="{B(m,n)}" /> be the class of groups that are generated by <img src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{m}" class="latex" title="{m}" /> elements and all elements in the group satisfy, <img src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7Bn%7D+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{x^{n} = 1}" class="latex" title="{x^{n} = 1}" />. Sergei Adian and Pyotr Novikov proved that <img src="https://s0.wp.com/latex.php?latex=%7BB%28m%2C+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B(m, n)}" class="latex" title="{B(m, n)}" /> is infinite for <img src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{n}" class="latex" title="{n}" /> odd, <img src="https://s0.wp.com/latex.php?latex=%7B%7Bn+%5Cge+4381%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{{n \ge 4381}}" class="latex" title="{{n \ge 4381}}" /> by a long complex combinatorial proof in 1968. This is a famous result. </p>
<p>
Shortly after another group theorist, John Britton, claimed an alternative proof in 1970. Unfortunately, Adian later discovered that Britton’s proof was wrong. I do not have first-hand information, but I was told that Adian was motivated by wanting to have <i>the</i> proof. He worked hard until he discovered an unrepairable bug in Britton’s 300-page monograph. The proof was unsalvageable.</p>
<p>
<a href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/yau/" rel="attachment wp-att-15802"><img src="https://rjlipton.files.wordpress.com/2019/04/yau.jpg?w=199&amp;h=300" alt="" width="199" class="aligncenter size-medium wp-image-15802" height="300" /></a></p>
<p>
A much newer example is from a recent book by Shing-Tung Yau, <a href="https://yalebooks.yale.edu/book/9780300235906/shape-life">The Shape of a Life</a>. He is a famous geometry expert and has made many important contributions to many areas of mathematics. We will probably discuss his book in detail in the future, but for today it has a neat example of “skin in the game”. He writes about an enumeration problem of counting how many curves lie on a certain manifold—a century old problem. One group used a clever trick to get the number 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++317%2C206%2C375.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  317,206,375. " class="latex" title="\displaystyle  317,206,375. " /></p>
<p>However another group discovered via a different method that the count was 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%2C682%2C549%2C425.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  2,682,549,425. " class="latex" title="\displaystyle  2,682,549,425. " /></p>
<p>Somewhat a different count—not even close. Clearly, both sets of authors were heavily motivated to check their work. And within a month the larger count was found to be wrong and the first was correct.</p>
<p>
</p><p></p><h2> A<del datetime="2019-04-24T13:55:30-04:00">n</del> <del datetime="2019-04-24T13:55:11-04:00">Un</del>resolved Claim </h2><p></p>
<p></p><p>
This is from the wonderful P vs NP <a href="https://www.win.tue.nl/~gwoegi/P-versus-NP.htm">pages</a> of Gerhard Woeginger. It was pointed out to us by the commenter <i>gentzen</i>. Quoting Woeginger’s page, including its use of “showed”:</p>
<blockquote><p><b> </b> <em> In February 2016, Mathias Hauptmann showed that P is not equal to NP. Hauptmann starts from the assumption that P equals <img src="https://s0.wp.com/latex.php?latex=%7B%5CSigma_%7B2%7D%5E%7Bp%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\Sigma_{2}^{p}}" class="latex" title="{\Sigma_{2}^{p}}" />, proves a new variant of the Union Theorem of McCreight and Meyer for <img src="https://s0.wp.com/latex.php?latex=%7B%5CSigma_%7B2%7D%5E%7Bp%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{\Sigma_{2}^{p}}" class="latex" title="{\Sigma_{2}^{p}}" />, and eventually derives a contradiction. This implies P not equal to NP. </em>
</p></blockquote>
<p></p><p>
Woeginger gives a link to Hauptmann’s <a href="http://arxiv.org/abs/1602.04781">paper</a>, “On Alternation and the Union Theorem,” and thanks two people who communicated this to him. </p>
<p>
The union <a href="https://people.csail.mit.edu/meyer/meyer-mccreight.pdf">theorem</a> of Albert Meyer and Edward McCreight is the classic theorem that shows how to encode many complexity classes into one. Hauptmann’s idea is not unreasonable. He makes an assumption that P=NP and tries to use it to improve the union theorem. This is a nice idea: Make a strong assumption and then try to improve a deep result. The hope is that this will lead to a contradiction. His abstract ends by saying, “Hence the assumption <img src="https://s0.wp.com/latex.php?latex=%7BP+%3D+%5CSigma_%7B2%7D%5E%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P = \Sigma_{2}^{p}}" class="latex" title="{P = \Sigma_{2}^{p}}" /> cannot hold.” We do not know if this paper has received a thorough reading. <b>Update:</b> We have learned that a pair of experts reviewed the argument and found that part of it implied a contradiction to the deterministic time hierarchy theorem, while another part relativizes in a way that would yield a false statement under certain oracles.</p>
<p>
</p><p></p><h2> A Resolved Claim </h2><p></p>
<p></p><p>
Hauptmann is a colleague of Norbert Blum at the University of Bonn. Two years ago, Blum claimed to prove P <img src="https://s0.wp.com/latex.php?latex=%7B%5Cneq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\neq}" class="latex" title="{\neq}" /> NP by making technical improvements on a well-known circuit-based attack from the 1980s and 1990s. He has had a long track record of expertise and reliability in this area and his <a href="http://arxiv.org/trackback/1708.03486">paper</a> was read right away. </p>
<p>
The reading was helped by his paper being well-organized, straightforward, and relatively short—the crucial segment was under ten pages. The news broke while we were preparing a post on the August 2017 total solar eclipse in the US. In the 24–48 hours it took us to modify our <a href="https://rjlipton.wordpress.com/2017/08/17/on-the-edge-of-eclipses-and-pnp/">post</a>, we were already able to draw on several accounts by first-responder readers and check those accounts ourselves against the paper. </p>
<p>
The error was triangulated in an interesting way. It was first observed that if Blum’s attack could succeed by the means and premises stated, then it would extend to prove something else that is known not to be true. Once this was ascertained, a closer reading was able to zero in on the exact technical point of error. Blum soon acknowledged this and that the breach was unfixable. The attempt still combines circuit theory and graph theory in ways a student can benefit from learning about, and this furnished its own incentive to read it.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
We appreciate the comments on the previous post and hope this adds some additional insights.</p>
<p>
[added update about Hauptmann’s paper]</p></font></font></div>







<p class="date">
by rjlipton <a href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/"><span class="datestr">at April 24, 2019 02:32 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
