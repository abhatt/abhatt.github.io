<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<br>
<a class="feedlink" href="https://differentialprivacy.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://differentialprivacy.org" title="Differential Privacy">DifferentialPrivacy.org</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://dstheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://dstheory.wordpress.com" title="Foundation of Data Science – Virtual Talk Series">Foundation of Data Science - Virtual Talk Series</a>
<br>
<a class="feedlink" href="https://francisbach.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at July 17, 2020 12:22 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2020-07-17-the-marvels-of-polynomials-over-a-field/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2020-07-17-the-marvels-of-polynomials-over-a-field/">The Marvels of Polynomials over a Field</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
In this series of posts, we explore the mathematical foundations of polynomials over a field. These objects are at the heart of several results in computer science: secret sharing, Multi Party Computation, Complexity, and Zero Knowledge protocols. All this wonder and more can be traced back to a very useful...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2020-07-17-the-marvels-of-polynomials-over-a-field/"><span class="datestr">at July 17, 2020 05:55 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07880">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07880">Coloring and Maximum Weight Independent Set of Rectangles</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chalermsook:Parinya.html">Parinya Chalermsook</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Walczak:Bartosz.html">Bartosz Walczak</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07880">PDF</a><br /><b>Abstract: </b>In 1960, Asplund and Gr\"unbaum proved that every intersection graph of
axis-parallel rectangles in the plane admits an $O(\omega^2)$-coloring, where
$\omega$ is the maximum size of a clique. We present the first asymptotic
improvement over this six-decade-old bound, proving that every such graph is
$O(\omega\log\omega)$-colorable and presenting a polynomial-time algorithm that
finds such a coloring. This improvement leads to a polynomial-time $O(\log\log
n)$-approximation algorithm for the maximum weight independent set problem in
axis-parallel rectangles, which improves on the previous approximation ratio of
$O(\frac{\log n}{\log\log n})$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07880"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07863">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07863">Empty Rainbow Triangles in $k$-colored Point Sets</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Ruy Fabila-Monroy, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Perz:Daniel.html">Daniel Perz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Trujillo=Negrete:Ana_Laura.html">Ana Laura Trujillo-Negrete</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07863">PDF</a><br /><b>Abstract: </b>Let $S$ be a set of $n$ points in general position in the plane. Suppose that
each point of $S$ has been assigned one of $k \ge 3$ possible colors and that
there is the same number, $m$, of points of each color class. A polygon with
vertices on $S$ is empty if it does not contain points of $S$ in its interior;
and it is rainbow if all its vertices have different colors. Let $f(k,m)$ be
the minimum number of empty rainbow triangles determined by $S$. In this paper
we give tight asymptotic bounds for this function. Furthermore, we show that
$S$ may not determine an empty rainbow quadrilateral for some arbitrarily large
values of $k$ and $m$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07863"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07862">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07862">Vertex Sparsification for Edge Connectivity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chalermsook:Parinya.html">Parinya Chalermsook</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Das:Syamantak.html">Syamantak Das</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Laekhanukit:Bundit.html">Bundit Laekhanukit</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kook:Yunbum.html">Yunbum Kook</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Yang_P=.html">Yang P. Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peng:Richard.html">Richard Peng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sellke:Mark.html">Mark Sellke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vaz:Daniel.html">Daniel Vaz</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07862">PDF</a><br /><b>Abstract: </b>Graph compression or sparsification is a basic information-theoretic and
computational question. A major open problem in this research area is whether
$(1+\epsilon)$-approximate cut-preserving vertex sparsifiers with size close to
the number of terminals exist. As a step towards this goal, we study a
thresholded version of the problem: for a given parameter $c$, find a smaller
graph, which we call connectivity-$c$ mimicking network, which preserves
connectivity among $k$ terminals exactly up to the value of $c$. We show that
connectivity-$c$ mimicking networks with $O(kc^4)$ edges exist and can be found
in time $m(c\log n)^{O(c)}$. We also give a separate algorithm that constructs
such graphs with $k \cdot O(c)^{2c}$ edges in time $mc^{O(c)}\log^{O(1)}n$.
These results lead to the first data structures for answering fully dynamic
offline $c$-edge-connectivity queries for $c \ge 4$ in polylogarithmic time per
query, as well as more efficient algorithms for survivable network design on
bounded treewidth graphs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07862"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07808">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07808">A Finite Time Combinatorial Algorithm for Instantaneous Dynamic Equilibrium Flows</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Lukas Graf und Tobias Harks <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07808">PDF</a><br /><b>Abstract: </b>Instantaneous dynamic equilibrium (IDE) is a standard game-theoretic concept
in dynamic traffic assignment in which individual flow particles myopically
select en route currently shortest paths towards their destination. We analyze
IDE within the Vickrey bottleneck model, where current travel times along a
path consist of the physical travel times plus the sum of waiting times in all
the queues along a path. Although IDE have been studied for decades, several
fundamental questions regarding equilibrium computation and complexity are not
well understood. In particular, all existence results and computational methods
are based on fixed-point theorems and numerical discretization schemes and no
exact finite time algorithm for equilibrium computation is known to date. As
our main result we show that a natural extension algorithm needs only finitely
many phases to converge leading to the first finite time combinatorial
algorithm computing an IDE. We complement this result by several hardness
results showing that computing IDE with natural properties is NP-hard.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07808"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07806">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07806">Plattenbauten: Touching Rectangles in Space</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Felsner:Stefan.html">Stefan Felsner</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Knauer:Kolja.html">Kolja Knauer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Ueckerdt:Torsten.html">Torsten Ueckerdt</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07806">PDF</a><br /><b>Abstract: </b>Planar bipartite graphs can be represented as touching graphs of horizontal
and vertical segments in $\mathbb{R}^2$. We study a generalization in space,
namely, touching graphs of axis-aligned rectangles in $\mathbb{R}^3$. We prove
that planar $3$-colorable graphs can be represented as touching graphs of
axis-aligned rectangles in $\mathbb{R}^3$. The result implies a
characterization of corner polytopes previously obtained by Eppstein and
Mumford. A by-product of our proof is a distributive lattice structure on the
set of orthogonal surfaces with given skeleton.
</p>
<p>Moreover, we study the subclass of strong representations, i.e., families of
axis-aligned rectangles in $\mathbb{R}^3$ in general position such that all
regions bounded by the rectangles are boxes. We show that the resulting graphs
correspond to octahedrations of an octahedron. This generalizes the
correspondence between planar quadrangulations and families of horizontal and
vertical segments in $\mathbb{R}^2$ with the property that all regions are
rectangles.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07806"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07802">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07802">Permutree sorting</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pilaud:Vincent.html">Vincent Pilaud</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pons:Viviane.html">Viviane Pons</a>, Daniel Tamayo Jiménez <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07802">PDF</a><br /><b>Abstract: </b>Generalizing stack sorting and $c$-sorting for permutations, we define the
permutree sorting algorithm. Given two disjoint subsets $U$ and $D$ of $\{2,
\dots, n-1\}$, the $(U,D)$-permutree sorting tries to sort the permutation $\pi
\in \mathfrak{S}_n$ and fails if and only if there are $1 \le i &lt; j &lt; k \le n$
such that $\pi$ contains the subword $jki$ if $j \in U$ and $kij$ if $j \in D$.
This algorithm is seen as a way to explore an automaton which either rejects
all reduced expressions of $\pi$, or accepts those reduced expressions for
$\pi$ whose prefixes are all $(U,D)$-permutree sortable.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07802"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07795">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07795">A family of metrics from the truncated smoothing of Reeb graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chambers:Erin_Wolf.html">Erin Wolf Chambers</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Munch:Elizabeth.html">Elizabeth Munch</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Ophelders:Tim.html">Tim Ophelders</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07795">PDF</a><br /><b>Abstract: </b>In this paper, we introduce an extension of smoothing on Reeb graphs, which
we call truncated smoothing; this in turn allows us to define a new family of
metrics which generalize the interleaving distance for Reeb graphs.
Intuitively, we "chop off" parts near local minima and maxima during the course
of smoothing, where the amount cut is controlled by a parameter $\tau$. After
formalizing truncation as a functor, we show that when applied after the
smoothing functor, this prevents extensive expansion of the range of the
function, and yields particularly nice properties (such as maintaining
connectivity) when combined with smoothing for $0 \leq \tau \leq 2\varepsilon$,
where $\varepsilon$ is the smoothing parameter. Then, for the restriction of
$\tau \in [0,\varepsilon]$, we have additional structure which we can take
advantage of to construct a categorical flow for any choice of slope $m \in
[0,1]$. Using the infrastructure built for a category with a flow, this then
gives an interleaving distance for every $m \in [0,1]$, which is a
generalization of the original interleaving distance, which is the case $m=0$.
While the resulting metrics are not stable, we show that any pair of these for
$m,m' \in [0,1)$ are strongly equivalent metrics, which in turn gives stability
of each metric up to a multiplicative constant. We conclude by discussing
implications of this metric within the broader family of metrics for Reeb
graphs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07795"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07772">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07772">Explicit Extremal Designs and Applications to Extractors</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chattopadhyay:Eshan.html">Eshan Chattopadhyay</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goodman:Jesse.html">Jesse Goodman</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07772">PDF</a><br /><b>Abstract: </b>An $(n,r,s)$-design, or $(n,r,s)$-partial Steiner system, is an $r$-uniform
hypergraph over $n$ vertices with pairwise hyperedge intersections of size
$&lt;s$. An independent set in a hypergraph $G$ is a subset of vertices covering
no hyperedge, and its independence number $\alpha(G)$ is the size of its
largest independent set. For all constants $r\geq s\in\mathbb{N}$ with $r$
even, we explicitly construct $(n,r,s)$-designs $(G_n)_{n\in\mathbb{N}}$ with
independence number $\alpha(G_n)\leq O(n^{\frac{2(r-s)}{r}})$. This gives the
first derandomization of a result by R\"odl and \v{S}inajov\'a (Random
Structures &amp; Algorithms, 1994).
</p>
<p>By combining our designs with a recent explicit construction of a
leakage-resilient extractor that works for low-entropy (Chattopadhyay et al.,
FOCS 2020), we obtain simple and significantly improved low-error explicit
extractors for adversarial and small-space sources. In particular, for any
constant $\delta&gt;0$, we extract from $(N,K,n,k)$-adversarial sources of
locality $0$, where $K\geq N^\delta$ and $k\geq\text{polylog }n$. The previous
best result (Chattopadhyay et al., STOC 2020) required $K\geq N^{1/2+o(1)}$. As
a result, we get extractors for small-space sources over $n$ bits with entropy
requirement $k\geq n^{1/2+\delta}$, whereas the previous best result
(Chattopadhyay et al., STOC 2020) required $k\geq n^{2/3+\delta}$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07772"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07738">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07738">Adapting the Directed Grid Theorem into an FPT Algorithm</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Campos:Victor.html">Victor Campos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lopes:Raul.html">Raul Lopes</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maia:Ana_Karolinna.html">Ana Karolinna Maia</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sau:Ignasi.html">Ignasi Sau</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07738">PDF</a><br /><b>Abstract: </b>The Grid Theorem of Robertson and Seymour [JCTB, 1986], is one of the most
important tools in the field of structural graph theory, finding numerous
applications in the design of algorithms for undirected graphs. An analogous
version of the Grid Theorem in digraphs was conjectured by Johnson et al.
[JCTB, 2001], and proved by Kawarabayashi and Kreutzer [STOC, 2015]. Namely,
they showed that there is a function $f(k)$ such that every digraph of directed
tree-width at least $f(k)$ contains a cylindrical grid of size $k$ as a
butterfly minor and stated that their proof can be turned into an XP algorithm,
with parameter $k$, that either constructs a decomposition of the appropriate
width, or finds the claimed large cylindrical grid as a butterfly minor. In
this paper, we adapt some of the steps of the proof of Kawarabayashi and
Kreutzer to improve this XP algorithm into an FPT algorithm. Towards this, our
main technical contributions are two FPT algorithms with parameter $k$. The
first one either produces an arboreal decomposition of width $3k-2$ or finds a
haven of order $k$ in a digraph $D$, improving on the original result for
arboreal decompositions by Johnson et al. The second algorithm finds a
well-linked set of order $k$ in a digraph $D$ of large directed tree-width. As
tools to prove these results, we show how to solve a generalized version of the
problem of finding balanced separators for a given set of vertices $T$ in FPT
time with parameter $|T|$, a result that we consider to be of its own interest.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07738"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07721">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07721">Online Generalized Network Design Under (Dis)Economies of Scale</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nagarajan:Viswanath.html">Viswanath Nagarajan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Lily.html">Lily Wang</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07721">PDF</a><br /><b>Abstract: </b>We consider a general online network design problem where a sequence of N
requests arrive over time, each of which needs to use some subset of the
available resources E. The cost incurred by any resource e is some function
$f_e$ of the total load $L_e$ on that resource. The objective is to minimize
the total cost $\sum_{e\in E} f_e(L_e)$. We focus on cost functions that
exhibit (dis)economies of scale, that are of the form $f_e(x) = \sigma_e +
\xi_e\cdot x^{\alpha_e}$ if $x&gt;0$ (and zero if $x=0$), where the exponent
$\alpha_e\ge 1$. Optimization problems under these functions have received
significant recent attention due to applications in energy-efficient computing.
Our main result is a deterministic online algorithm with tight competitive
ratio $\Theta\left(\max_{e\in E}
\left(\frac{\sigma_e}{\xi_e}\right)^{1/\alpha_e}\right)$ when $\alpha_e$ is
constant for all $e\in E$. This framework is applicable to a variety of network
design problems in undirected and directed graphs, including multicommodity
routing, Steiner tree/forest connectivity and set-connectivity. In fact, our
online competitive ratio even matches the previous-best (offline) approximation
ratio for generalized network design.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07721"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07720">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07720">An $\tilde{O}(n^{5/4})$ Time $\varepsilon$-Approximation Algorithm for RMS Matching in a Plane</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lahn:Nathaniel.html">Nathaniel Lahn</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Raghvendra:Sharath.html">Sharath Raghvendra</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07720">PDF</a><br /><b>Abstract: </b>The 2-Wasserstein distance (or RMS distance) is a useful measure of
similarity between probability distributions that has exciting applications in
machine learning. For discrete distributions, the problem of computing this
distance can be expressed in terms of finding a minimum-cost perfect matching
on a complete bipartite graph given by two multisets of points $A,B \subset
\mathbb{R}^2$, with $|A|=|B|=n$, where the ground distance between any two
points is the squared Euclidean distance between them. Although there is a
near-linear time relative $\varepsilon$-approximation algorithm for the case
where the ground distance is Euclidean (Sharathkumar and Agarwal, JACM 2020),
all existing relative $\varepsilon$-approximation algorithms for the RMS
distance take $\Omega(n^{3/2})$ time. This is primarily because, unlike
Euclidean distance, squared Euclidean distance is not a metric. In this paper,
for the RMS distance, we present a new $\varepsilon$-approximation algorithm
that runs in $O(n^{5/4}\mathrm{poly}\{\log n,1/\varepsilon\})$ time.
</p>
<p>Our algorithm is inspired by a recent approach for finding a minimum-cost
perfect matching in bipartite planar graphs (Asathulla et al., TALG 2020).
Their algorithm depends heavily on the existence of sub-linear sized vertex
separators as well as shortest path data structures that require planarity.
Surprisingly, we are able to design a similar algorithm for a complete
geometric graph that is far from planar and does not have any vertex
separators. Central components of our algorithm include a quadtree-based
distance that approximates the squared Euclidean distance and a data structure
that supports both Hungarian search and augmentation in sub-linear time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07720"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07718">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07718">On Indexing and Compressing Finite Automata</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Nicola Cotumaccio, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Prezza:Nicola.html">Nicola Prezza</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07718">PDF</a><br /><b>Abstract: </b>An index for a finite automaton is a powerful data structure that supports
locating paths labeled with a query pattern, thus solving pattern matching on
the underlying regular language. In this paper, we solve the long-standing
problem of indexing arbitrary finite automata. Our solution consists in finding
a partial co-lexicographic order of the states and proving, as in the total
order case, that states reached by a given string form one interval on the
partial order, thus enabling indexing. We provide a lower bound stating that
such an interval requires $O(p)$ words to be represented, $p$ being the order's
width (i.e. the size of its largest antichain). Indeed, we show that $p$
determines the complexity of several fundamental problems on finite automata:
(i) Letting $\sigma$ be the alphabet size, we provide an encoding for NFAs
using $\lceil\log \sigma\rceil + 2\lceil\log p\rceil + 2$ bits per transition
and a smaller encoding for DFAs using $\lceil\log \sigma\rceil + \lceil\log
p\rceil + 2$ bits per transition. This is achieved by generalizing the
Burrows-Wheeler transform to arbitrary automata. (ii) We show that indexed
pattern matching can be solved in $\tilde O(m\cdot p^2)$ query time on NFAs.
(iii) We provide a polynomial-time algorithm to index DFAs, while matching the
optimal value for $ p $. On the other hand, we prove that the problem is
NP-hard on NFAs. (iv) We show that, in the worst case, the classic powerset
construction algorithm for NFA determinization generates an equivalent DFA of
size $2^p(n-p+1)-1$, where $n$ is the number of NFA's states.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07718"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07660">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07660">Leafy Spanning Arborescences in DAGs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fernandes:Cristina_G=.html">Cristina G. Fernandes</a>, Carla N. Lintzmayer <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07660">PDF</a><br /><b>Abstract: </b>Broadcasting in a computer network is a method of transferring a message to
all recipients simultaneously. It is common in this situation to use a tree
with many leaves to perform the broadcast, as internal nodes have to forward
the messages received, while leaves are only receptors. We consider the
subjacent problem of, given a directed graph~$D$, finding a spanning
arborescence of D, if one exists, with the maximum number of leaves. In this
paper, we concentrate on the class of rooted directed acyclic graphs, for which
the problem is known to be MaxSNP-hard. A 2-approximation was previously known
for this problem on this class of directed graphs. We improve on this result,
presenting a (3/2)-approximation. We also adapt a result for the undirected
case and derive an inapproximability result for the vertex-weighted version of
Maximum Leaf Spanning Arborescence on rooted directed acyclic graphs.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07660"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07575">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07575">On the parameterized complexity of the Minimum Path Cover problem in DAGs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/C=aacute=ceres:Manuel.html">Manuel Cáceres</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cairo:Massimo.html">Massimo Cairo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mumey:Brendan.html">Brendan Mumey</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rizzi:Romeo.html">Romeo Rizzi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tomescu:Alexandru_I=.html">Alexandru I. Tomescu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07575">PDF</a><br /><b>Abstract: </b>A minimum path cover (MPC) of a directed acyclic graph (DAG) G = (V,E) is a
minimum-size set of paths that together cover all the vertices of the DAG. The
size k of a MPC is also called the width of G. Computing a MPC is a basic
problem, dating back to Dilworth's and Fulkerson's results in the 1950s, and is
solvable in quadratic time in the worst case. Since the width of the DAG can be
small in practical applications (e.g., from bioinformatics), research has also
studied algorithms whose complexity is parameterized on k. Despite these
efforts, it is a major open problem whether there exists a linear-time
$O(f(k)(|E| + |V|))$ parameterized algorithm. We present here two significant
results in this direction.
</p>
<p>First, we obtain an $O(|E| + k^2|V|\log{|V|})$-time algorithm, which in
particular is faster than all existing MPC algorithms when $k =
o(\sqrt{|V|}/\log{|V|})$ and $|E| = \omega(k|V|)$ but $|E| = o(|V|^2)$. We
obtain this by a new combination of three techniques: transitive edge
sparsification, divide-and-conquer, and shrinking. This algorithm is also
simple and can be parallelized, making it ideal for practical use. We also show
that some basic problems on DAGs (reachability queries, longest increasing /
common subsequence, co-linear chaining) get faster algorithms as immediate
corollaries of this result.
</p>
<p>Second, we obtain an $O(poly(k)(2^k|E| + 4^k|V|))$-time algorithm for the
dual problem of computing the width of the DAG. This is based on the notion of
frontier antichains, generalizing the standard notion of right-most maximum
antichain. As we process the vertices in a topological order, these at most
$2^k$ frontier antichains can be maintained with the help of several
combinatorial properties. As such, it is enough to sweep the graph once from
left to right, paying only f(k) along the way, which is a new surprising
insight into the classical MPC problem.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07575"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07563">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07563">Learning Part Boundaries from 3D Point Clouds</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Marios Loizou, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Averkiou:Melinos.html">Melinos Averkiou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kalogerakis:Evangelos.html">Evangelos Kalogerakis</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07563">PDF</a><br /><b>Abstract: </b>We present a method that detects boundaries of parts in 3D shapes represented
as point clouds. Our method is based on a graph convolutional network
architecture that outputs a probability for a point to lie in an area that
separates two or more parts in a 3D shape. Our boundary detector is quite
generic: it can be trained to localize boundaries of semantic parts or
geometric primitives commonly used in 3D modeling. Our experiments demonstrate
that our method can extract more accurate boundaries that are closer to
ground-truth ones compared to alternatives. We also demonstrate an application
of our network to fine-grained semantic shape segmentation, where we also show
improvements in terms of part labeling performance.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07563"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07554">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07554">Minimum Weight Pairwise Distance Preservers</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Abdolmaleki:Mojtaba.html">Mojtaba Abdolmaleki</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yin:Yafeng.html">Yafeng Yin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Masoud:Neda.html">Neda Masoud</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07554">PDF</a><br /><b>Abstract: </b>In this paper, we study the Minimum Weight Pairwise Distance Preservers
(MWPDP) problem. Consider a positively weighted undirected/directed connected
graph $G = (V, E, c)$ and a subset $P$ of pairs of vertices, also called demand
pairs. A subgraph $G'$ is a distance preserver with respect to $P$ if and only
if every pair $(u, w) \in P$ satisfies $dist_{G'} (u, w) = dist_{G}(u, w)$. In
MWPDP problem, we aim to find the minimum-weight subgraph $G^*$ that is a
distance preserver with respect to $P$. Taking a shortest path between each
pair in $P$ gives us a trivial solution with the weight of at most
$U=\sum_{(u,v) \in P} dist_{G} (u, w)$. Subsequently, we ask how much
improvement we can make upon $U$. In other words, we opt to find a distance
preserver $G^*$ that maximizes $U-c(G^*)$. Denote this problem as Cost Sharing
Pairwise Distance Preservers (CSPDP), which has several applications in the
planning and operations of transportation systems.
</p>
<p>The only known work that can provide a nontrivial solution for CSPDP is that
of Chlamt\'a\v{c} et al. (SODA, 2017). This algorithm works for unweighted
graphs and guarantees a non-zero objective only if the optimal solution is
extremely sparse with respect to the trivial solution. We address this issue by
proposing an $O(|E|^{1/2+\epsilon})$-approximation algorithm for CSPDP in
weighted graphs that runs in $O((|P||E|)^{2.38} (1/\epsilon))$ time. Moreover,
we prove CSPDP is at least as hard as $\text{LABEL-COVER}_{\max}$. This implies
that CSPDP cannot be approximated within $O(|E|^{1/6-\epsilon})$ factor in
polynomial time, unless there is an improvement in the notoriously difficult
$\text{LABEL-COVER}_{\max}$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07554"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07553">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07553">A Faster Exact Algorithm to Count X3SAT Solutions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoi:Gordon.html">Gordon Hoi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jain:Sanjay.html">Sanjay Jain</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stephan:Frank.html">Frank Stephan</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07553">PDF</a><br /><b>Abstract: </b>The Exact Satisfiability problem, XSAT, is defined as the problem of finding
a satisfying assignment to a formula in CNF such that there is exactly one
literal in each clause assigned to be 1 and the other literals in the same
clause are set to 0. If we restrict the length of each clause to be at most 3
literals, then it is known as the X3SAT problem. In this paper, we consider the
problem of counting the number of satisfying assignments to the X3SAT problem,
which is also known as #X3SAT.
</p>
<p>The current state of the art exact algorithm to solve #X3SAT is given by
Dahll\"of, Jonsson and Beigel and runs in $O(1.1487^n)$, where $n$ is the
number of variables in the formula. In this paper, we propose an exact
algorithm for the #X3SAT problem that runs in $O(1.1120^n)$ with very few
branching cases to consider, by using a result from Monien and Preis to give us
a bisection width for graphs with at most degree 3.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07553"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07515">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07515">Improved algorithms for online load balancing</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Yaxiong.html">Yaxiong Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hatano:Kohei.html">Kohei Hatano</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Takimoto:Eiji.html">Eiji Takimoto</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07515">PDF</a><br /><b>Abstract: </b>We consider an online load balancing problem and its extensions in the
framework of repeated games. On each round, the player chooses a distribution
(task allocation) over $K$ servers, and then the environment reveals the load
of each server, which determines the computation time of each server for
processing the task assigned. After all rounds, the cost of the player is
measured by some norm of the cumulative computation-time vector. The cost is
the makespan if the norm is $L_\infty$-norm. The goal is to minimize the
regret, i.e., minimizing the player's cost relative to the cost of the best
fixed distribution in hindsight. We propose algorithms for general norms and
prove their regret bounds. In particular, for $L_\infty$-norm, our regret bound
matches the best known bound and the proposed algorithm runs in polynomial time
per trial involving linear programming and second order programming, whereas no
polynomial time algorithm was previously known to achieve the bound.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07515"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07496">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07496">Observations on Symmetric Circuits</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Engels:Christian.html">Christian Engels</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07496">PDF</a><br /><b>Abstract: </b>We study symmetric arithmetic circuits and improve on lower bounds given by
Dawar and Wilsenach (ArXiv 2020). Their result showed an exponential lower
bound of the permanent computed by symmetric circuits. We extend this result to
show a simpler proof of the permanent lower bound and show that a large class
of polynomials have exponential lower bounds in this model. In fact, we prove
that all polynomials that contain at least one monomial of the permanent have
exponential size lower bounds in the symmetric computation model. We also show
super-polynomial lower bounds for smaller groups. We support our conclusion
that the group is much more important than the polynomial by showing that on a
random process of choosing polynomials, the probability of not encountering a
super-polynomial lower bound is exponentially low.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07496"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07488">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07488">An algorithm for integrating peer-to-peer ridesharing and schedule-based transit system for first mile/last mile access</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Pramesh.html">Pramesh Kumar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khani:Alireza.html">Alireza Khani</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07488">PDF</a><br /><b>Abstract: </b>Due to limited transit network coverage and infrequent service, suburban
commuters often face the transit first mile/last mile (FMLM) problem. To deal
with this, they either drive to a park-and-ride location to take transit, use
carpooling, or drive directly to their destination to avoid inconvenience.
Ridesharing, an emerging mode of transportation, can solve the transit first
mile/last mile problem. In this setup, a driver can drive a ride-seeker to a
transit station, from where the rider can take transit to her respective
destination. The problem requires solving a ridesharing matching problem with
the routing of riders in a multimodal transportation network. We develop a
transit-based ridesharing matching algorithm to solve this problem. The method
leverages the schedule-based transit shortest path to generate feasible matches
and then solves a matching optimization program to find an optimal match
between riders and drivers. The proposed method not only assigns an optimal
driver to the rider but also assigns an optimal transit stop and a transit
vehicle trip departing from that stop for the rest of the rider's itinerary. We
also introduce the application of space-time prism (STP) (the geographical area
which can be reached by a traveler given the time constraints) in the context
of ridesharing to reduce the computational time by reducing the network search.
An algorithm to solve this problem dynamically using a rolling horizon approach
is also presented. We use simulated data obtained from the activity-based
travel demand model of Twin Cities, MN to show that the transit-based
ridesharing can solve the FMLM problem and save a significant number of
vehicle-hours spent in the system.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07488"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07449">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07449">Downsampling for Testing and Learning in Product Distributions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Harms:Nathaniel.html">Nathaniel Harms</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yoshida:Yuichi.html">Yuichi Yoshida</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07449">PDF</a><br /><b>Abstract: </b>We study the domain reduction problem of eliminating dependence on $n$ from
the complexity of property testing and learning algorithms on domain $[n]^d$,
and the related problem of establishing testing and learning results for
product distributions over $\mathbb{R}^d$. Our method, which we call
downsampling, gives conceptually simple proofs for several results:
</p>
<p>1. A 1-page proof of the recent $o(d)$-query monotonicity tester for the
hypergrid (Black, Chakrabarty &amp; Seshadhri, SODA 2020), and an improvement from
$O(d^7)$ to $\widetilde O(d^4)$ in the sample complexity of their
distribution-free monotonicity tester for product distributions over
$\mathbb{R}^d$;
</p>
<p>2. An $\exp(\widetilde O(kd))$-time agnostic learning algorithm for functions
of $k$ convex sets in product distributions;
</p>
<p>3. A polynomial-time agnostic learning algorithm for functions of a constant
number of halfspaces in product distributions;
</p>
<p>4. A polynomial-time agnostic learning algorithm for constant-degree
polynomial threshold functions in product distributions;
</p>
<p>5. An $\exp(\widetilde O(k \sqrt d))$-time agnostic learning algorithm for
$k$-alternating functions in product distributions.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07449"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07405">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07405">On the hop-constrained Steiner tree problems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jabrayilov:Adalat.html">Adalat Jabrayilov</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07405">PDF</a><br /><b>Abstract: </b>The hop-constrained Steiner tree problem is a generalization of the classical
Steiner tree problem, and asks for minimum cost subtree that spans some
specified nodes of a given graph, such that the number of edges between each
node of the tree and its root respects a given hop limit. This NP-hard problem
has many variants, which are often modeled as integer linear programs. Two of
the models are so called assignment and partial-ordering based models, which
yield (up to our knowledge) the best two state-of-the-art formulations for the
problem variant Steiner tree problem with revenues, budgets and hop
constraints. We show that the linear programming relaxation of the
partial-ordering based model is stronger than that of the assignment model for
the hop-constrained Steiner tree problem; this remains true also for a class of
hop-constrained problems, including the hop-constrained minimum spanning tree
problem, the Steiner tree problem with revenues, budgets and hop constraints.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07405"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07384">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07384">A Pairwise Fair and Community-preserving Approach to k-Center Clustering</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brubach:Brian.html">Brian Brubach</a>, Darshan Chakrabarti, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dickerson:John_P=.html">John P. Dickerson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khuller:Samir.html">Samir Khuller</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Srinivasan:Aravind.html">Aravind Srinivasan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tsepenekas:Leonidas.html">Leonidas Tsepenekas</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07384">PDF</a><br /><b>Abstract: </b>Clustering is a foundational problem in machine learning with numerous
applications. As machine learning increases in ubiquity as a backend for
automated systems, concerns about fairness arise. Much of the current
literature on fairness deals with discrimination against protected classes in
supervised learning (group fairness). We define a different notion of fair
clustering wherein the probability that two points (or a community of points)
become separated is bounded by an increasing function of their pairwise
distance (or community diameter). We capture the situation where data points
represent people who gain some benefit from being clustered together.
Unfairness arises when certain points are deterministically separated, either
arbitrarily or by someone who intends to harm them as in the case of
gerrymandering election districts. In response, we formally define two new
types of fairness in the clustering setting, pairwise fairness and community
preservation. To explore the practicality of our fairness goals, we devise an
approach for extending existing $k$-center algorithms to satisfy these fairness
constraints. Analysis of this approach proves that reasonable approximations
can be achieved while maintaining fairness. In experiments, we compare the
effectiveness of our approach to classical $k$-center algorithms/heuristics and
explore the tradeoff between optimal clustering and fairness.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07384"><span class="datestr">at July 16, 2020 11:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07334">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07334">Quadrilateral Mesh Generation III: Optimizing Singularity Configuration Based on Abel-Jacobi Theory</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zheng:Xiaopeng.html">Xiaopeng Zheng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhu:Yiming.html">Yiming Zhu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lei:Na.html">Na Lei</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Luo:Zhongxuan.html">Zhongxuan Luo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gu:Xianfeng.html">Xianfeng Gu</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07334">PDF</a><br /><b>Abstract: </b>This work proposes a rigorous and practical algorithm for generating
meromorphic quartic differentials for the purpose of quad-mesh generation. The
work is based on the Abel-Jacobi theory of algebraic curve. The algorithm
pipeline can be summarized as follows: calculate the homology group; compute
the holomorphic differential group; construct the period matrix of the surface
and Jacobi variety; calculate the Abel-Jacobi map for a given divisor; optimize
the divisor to satisfy the Abel-Jacobi condition by an integer programming;
compute the flat Riemannian metric with cone singularities at the divisor by
Ricci flow; isometric immerse the surface punctured at the divisor onto the
complex plane and pull back the canonical holomorphic differential to the
surface to obtain the meromorphic quartic differential; construct the
motor-graph to generate the resulting T-Mesh. The proposed method is rigorous
and practical. The T-mesh results can be applied for constructing T-Spline
directly. The efficiency and efficacy of the proposed algorithm are
demonstrated by experimental results.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07334"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07329">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07329">On approximations to minimum link visibility paths in simple polygons</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zarrabi:Mohammad_Reza.html">Mohammad Reza Zarrabi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Charkari:Nasrollah_Moghaddam.html">Nasrollah Moghaddam Charkari</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07329">PDF</a><br /><b>Abstract: </b>We investigate a practical variant of the well-known polygonal visibility
path (watchman) problem. For a polygon $P$, a minimum link visibility path is a
polygonal visibility path in $P$ that has the minimum number of links. The
problem of finding a minimum link visibility path is NP-hard for simple
polygons. If the link-length (number of links) of a minimum link visibility
path (tour) is $OPT$ for a simple polygon $P$ with $n$ vertices and $k$
nonredundant cuts, we provide an algorithm with $O(kn^2)$ runtime that produces
polygonal visibility paths (or tours) of link-length at most
$(\gamma+a_l/(k-1))OPT$ (or $(\gamma+a_l/k)OPT$), where $a_l$ is an output
sensitive parameter and $\gamma$ is the approximation factor of an $O(k^3)$
time approximation algorithm for the GTSP (path or tour version).
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07329"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/2007.07294">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/2007.07294">Competitively Pricing Parking in a Tree</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Max Bender, Jacob Gilbert, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krishnan:Aditya.html">Aditya Krishnan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pruhs:Kirk.html">Kirk Pruhs</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/2007.07294">PDF</a><br /><b>Abstract: </b>Motivated by demand-responsive parking pricing systems we consider
posted-price algorithms for the online metrical matching problem and the online
metrical searching problem in a tree metric. Our main result is a poly-log
competitive posted-price algorithm for online metrical searching.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/2007.07294"><span class="datestr">at July 16, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/07/15/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/07/15/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p>A <a href="https://en.wikipedia.org/wiki/Sir_Ronald_Fisher_window">stained glass window of a Latin square</a> (<a href="https://mathstodon.xyz/@11011110/104440661362867680"></a>) will be <a href="https://www.theguardian.com/education/2020/jun/27/cambridge-gonville-caius-college-eugenicist-window-ronald-fisher">removed from Cambridge University</a> because it honors <a href="https://en.wikipedia.org/wiki/Ronald_Fisher">prominent eugenicist R. A. Fisher</a>. The window visualizes a nice piece of mathematics, with a long history that surprisingly originates in Korea (predating Euler) but in context among windows celebrating Cambridge luminaries it could not be separated from Fisher’s racist history, so it’s sad but I think it’s the right decision.</p>
  </li>
  <li>
    <p>What is it about <em>Quanta</em>’s oversimplifications (<a href="https://mathstodon.xyz/@11011110/104448448945740867"></a>)?  A <a href="https://www.quantamagazine.org/new-geometric-perspective-cracks-old-problem-about-rectangles-20200625/">recent article</a> is on a variation of the problem of <a href="https://en.wikipedia.org/wiki/Inscribed_square_problem">squares in Jordan curves</a>, known to exist in smooth curves but unknown for some nastier ones. The new result described by <em>Quanta</em> concerns <a href="https://arxiv.org/abs/2005.09193">rectangles of given aspect ratio in smooth Jordan curves</a>. Wikipedia editors have had to fend off repeated edits by Quanta readers who came away thinking the new paper solved the original problem. It doesn’t.</p>
  </li>
  <li>
    <p><a href="http://www.mseymour.ca/hex_puzzle/hexpuzzle.html">Hex puzzles by Matthew Seymour</a> (<a href="https://mathstodon.xyz/@jsiehler/104412077950183358"></a>). 500 of them, designed to guide you to greater Hex mastery, in an online applet. In the Mastodon post, Jacob Siehler explains that he can justify playing with these as work, because it relates to an upcoming course he’s teaching.</p>
  </li>
  <li>
    <p><a href="http://www.dam.brown.edu/people/mumford/blog/2020/Ridiculous.html">David Mumford on the long history of ridiculous word problems in mathematics</a> (<a href="https://mathstodon.xyz/@11011110/104462285880982461"></a>, <a href="https://news.ycombinator.com/item?id=23739243">via</a>).</p>
  </li>
  <li>
    <p>A tiny improvement sometimes makes for a big result (<a href="https://mathstodon.xyz/@11011110/104465689831962167"></a>): In a new preprint “<a href="https://arxiv.org/abs/2007.01409">A (Slightly) Improved Approximation Algorithm for Metric TSP</a>”, Anna Karlin, Nathan Klein, and Shayan Oveis Gharan claim a reduction in the approximation ratio for traveling salesperson in arbitrary metric spaces from  to . But it’s the first such improvement since Christofides and Serdyukov in 1976, on a central problem in approximation algorithms.</p>
  </li>
  <li>
    <p><a href="https://www.ice.gov/news/releases/sevp-modifies-temporary-exemptions-nonimmigrant-students-taking-online-courses-during">US Department of Homeland Security tried to require foreign students in the US to either attend in-person classes or leave the country</a> (<a href="https://mathstodon.xyz/@11011110/104468994612372729"></a>, <a href="https://news.ycombinator.com/item?id=23751931">via</a>, <a href="https://www.nbcnews.com/politics/immigration/ice-tells-foreign-students-leave-u-s-if-their-school-n1233026">see also</a>). After facing pushback in the courts, they gave up. But while it was happening, it had the appearance of pressuring US universities into opening up in-person classes despite the ongoing pandemic, using the threat of taking away all of their foreign students.</p>
  </li>
  <li>
    <p><a href="https://www.ams.org/news?news_id=6244">Sad news from the AMS: Ron Graham has died</a> (<a href="https://mathstodon.xyz/@11011110/104477077422403456"></a>). See also the blog posts about him by <a href="https://www.solipsys.co.uk/new/MeetingRonGraham.html?tg08mn">Lipton and Regan</a>, <a href="https://blog.computationalcomplexity.org/2020/07/ronald-graham-summary-of-blog-posts-we.html">Gasarch</a>, <a href="https://blog.plover.com/math/graham.html">Dominus</a>, <a href="https://www.bradyharanblog.com/blog/the-day-i-met-ron-graham">Haran</a>, and (mostly from earlier) <a href="https://www.solipsys.co.uk/new/MeetingRonGraham.html?tg08mn">Wright</a>.</p>
  </li>
  <li>
    <p><a href="http://paulbourke.net/miscellaneous/reverseperspective/">Experiments on reverse perspective</a> (<a href="https://mathstodon.xyz/@11011110/104486648345638828"></a>), recent post by Paul Bourke with a link to a recent video, “<a href="https://www.youtube.com/watch?v=iJ4yL6kaV1A">Hypercentric optics</a>” by Ben Krasnow, showing how to achieve reverse perspective physically using a giant Fresnel lens.</p>
  </li>
  <li>
    <p><a href="https://eccc.weizmann.ac.il/report/2020/096/">On the asymptotic complexity of sorting</a> (<a href="https://mathstodon.xyz/@11011110/104493642072558461"></a>), Igor Sergeev. We still study the number of comparisons for sorting in introductory CS, although other factors like locality of reference may be more important in practice. Common topics are the  comparison-tree lower bound and the nearly-matching  merge sort upper bound. Better sorts were known but still with an  error term. Now Sergeev has reduced the error term <span style="white-space: nowrap;">to .</span></p>
  </li>
  <li>
    <p>A messy story of unethical doings at ISCA, a major computer architecture conference (<a href="https://mathstodon.xyz/@11011110/104496752991792669"></a>, <a href="https://retractionwatch.com/2020/07/11/weekend-reads-a-paper-mill-science-needs-to-clean-its-own-house-is-the-covid-19-retraction-rate-exceptionally-high/">via</a>):</p>

    <ul>
      <li>
        <p>Junk research accepted to ISCA <a href="https://medium.com/@huixiangvoice/the-hidden-story-behind-the-suicide-phd-candidate-huixiang-chen-236cd39f79d3">causes its student coauthor to kill himself</a>.</p>
      </li>
      <li>
        <p>The ensuing investigation brings to light apparent serious breaches of ISCA’s double-blind reviewing process, but <a href="https://medium.com/@huixiangvoice/evidence-put-doubts-on-the-ieee-acms-investigation-991a6d50802a">ACM and IEEE find no wrongdoing</a>.</p>
      </li>
      <li>
        <p>A <a href="https://www.natureindex.com/news-blog/probe-into-leaked-papers-submitted-to-leading-engineering-conference">somewhat-confused description of the affair hits <em>Nature</em></a>.</p>
      </li>
    </ul>
  </li>
  <li>
    <p><a href="https://www.latimes.com/world-nation/story/2020-06-27/in-chinas-universities-targeted-attacks-on-intellectuals-raise-memories-of-the-cultural-revolution">“Spied on. Fired. Publicly shamed. China’s crackdown on professors reminds many of Mao era.”</a> (<a href="https://mathstodon.xyz/@11011110/104513232965232968"></a>). (If the paywalled <em>LA Times</em> link is a problem, <a href="https://beta.trimread.com/">trimread</a> might help.)</p>
  </li>
  <li>
    <p><a href="https://www.flyingcoloursmaths.co.uk/ask-uncle-colin-a-fraction-of-a-square/">A cute dissection proof of an area calculation of a tilted square within a square</a> (<a href="https://mathstodon.xyz/@11011110/104516441062427682"></a>). But to generalize from there to: tick marks that split the sides in the ratio  (in this example, 1:2) give a ratio of areas of the inner tilted square to the outer square that is  (in this case 4:10, simplifying to 2:5) it seems easier to apply similar triangles and then use Pythagoras in the tilted grid.</p>
  </li>
  <li>
    <p><a href="https://www.efavdb.com/quinoa%20packing">Quinoa packing 2 + 1 = 4</a> (<a href="https://mathstodon.xyz/@11011110/104520918846919905"></a>, <a href="https://news.ycombinator.com/item?id=23727749">via</a>). This blog post theorizes that the combination of 2 cups of water and 1 cup of quinoa to form 4 cups of cooked quinoa might happen because the water fills the spaces between the grains before cooking, but merges into the grains causing them to pack like spheres with air pockets between them afterwards. On the other hand, maybe they just expand into a less-dense combination of materials that takes more room.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/07/15/linkage.html"><span class="datestr">at July 15, 2020 10:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/106">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/106">TR20-106 |  Explicit Extremal Designs and Applications to Extractors | 

	Eshan Chattopadhyay, 

	Jesse Goodman</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
An $(n,r,s)$-design, or $(n,r,s)$-partial Steiner system, is an $r$-uniform hypergraph over $n$ vertices with pairwise hyperedge intersections of size $0$, we extract from $(N,K,n,k)$-adversarial sources of locality $0$, where $K\geq N^\delta$ and $k\geq\text{polylog }n$. The previous best result (Chattopadhyay et al., STOC 2020) required $K\geq N^{1/2+o(1)}$. As a result, we get extractors for small-space sources over $n$ bits with entropy requirement $k\geq n^{1/2+\delta}$, whereas the previous best result (Chattopadhyay et al., STOC 2020) required $k\geq n^{2/3+\delta}$.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/106"><span class="datestr">at July 15, 2020 04:25 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2020-07-15-asynchronous-fault-tolerant-computation-with-optimal-resilience/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2020-07-15-asynchronous-fault-tolerant-computation-with-optimal-resilience/">Asynchronous Fault Tolerant Computation with Optimal Resilience</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Decentralized Thoughts</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A basic question of distributed computing: Is there a fundamental limit to fault tolerant computation in the Asynchronous model? The celebrated FLP theorem says that any protocol that solves Agreement in the asynchronous model that is resilient to at least one crash failure must have a non-terminating execution. This means...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2020-07-15-asynchronous-fault-tolerant-computation-with-optimal-resilience/"><span class="datestr">at July 15, 2020 08:39 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/105">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/105">TR20-105 |  Automating Regular or Ordered Resolution is NP-Hard | 

	Zoë Bell</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that is hard to find regular or even ordered (also known as Davis-Putnam) Resolution proofs, extending the breakthrough result for general Resolution from Atserias and Müller to these restricted forms. Namely, regular and ordered Resolution are automatable if and only if P = NP. Specifically, for a CNF formula $F$ the problem of distinguishing between the existence of a polynomial-size ordered Resolution refutation of $F$ and an at least exponential-size general Resolution proof being required to refute $F$ is NP-complete.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/105"><span class="datestr">at July 14, 2020 01:43 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://benjamin-recht.github.io/2020/07/14/there-are-none/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/recht.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://benjamin-recht.github.io/2020/07/14/there-are-none/">There are none</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>In the <a href="http://www.argmin.net/2020/07/08/gain-margin/">last post</a>, we showed that continuous-time LQR has “natural robustness” insofar as the optimal solution is robust to a variety of model-mismatch conditions. LQR makes the assumption that the state of the system is fully, perfectly observed. In many situations, we don’t have access to such perfect state information. What changes?</p>

<p>The generalization of LQR to the case with imperfect state observation is called “Linear Quadratic Gaussian” control (LQG). This is the simplest, special case of a Partially Observed Markov Decision Process (POMDP). We again assume linear dynamics:</p>



<p>where the state is now corrupted by zero-mean Gaussian noise, $w_t$. Instead of measuring the state $x_t$ directly, we instead  measure a signal $y_t$ of the form</p>



<p>Here, $v_t$ is also zero-mean Gaussian noise. Suppose we’d still like to minimize a quadratic cost function</p>



<p>This problem is very similar to our LQR problem except for the fact that we get an indirect measurement of the state and need to apply some sort of <em>filtering</em> of the $y_t$ signal to estimate $x_t$.</p>

<p>The optimal solution for LQG is strikingly elegant. Since the observation of $x_t$ is through a Gaussian process, the maximum likelihood estimation algorithm has a clean, closed form solution, even in continuous time. Our best estimate for $x_t$, denoted $\hat{x}_t$, given all of the data observed up to time $t$ obeys a differential equation</p>



<p>The matrix $L$ that can be found by solving an algebraic Riccati equation that depends on the variance of $v_t$ and $w_t$ and on the matrices $A$ and $C$. In particular, it’s the CARE with data $(A^\top,C^\top,\Sigma_w,\Sigma_v)$. This solution is called a <em>Kalman Filter</em> and is a continuous limit of the discrete time Kalman Filter one might see in a course on graphical models.</p>

<p>The optimal LQG solution takes the estimate of the Kalman Filter, $\hat{x}_t$, and sets the control signal to be</p>



<p>Here, $K$ is gain matrix that would be used to solve the LQR problem with data $(A,B,Q,R)$. That is, LQG performs optimal filtering to compute the best state estimate, and then computes a feedback policy as if this estimate was a noiseless measurement of the state. That this turns out to be optimal is one of the more amazing results in control theory. It decouples the process of designing an optimal filter from designing an optimal controller, enabling simplicity and modularity in control design. This decoupling where we treat the output of our state estimator as the true state is an example of <em>certainty equivalence</em>, the umbrella term for using point estimates of stochastic quantities as if they were the correct value. Though certainty equivalent control may be suboptimal in general, it remains ubiquitous for all of the benefits it brings as a design paradigm. Unfortunately, not only is this decoupled design of filters and controllers often suboptimal, it has many hidden fragilities. LQG highlights a particular scenario where certainty equivalent control leads to misplaced optimism about robustness.</p>

<p>We saw in the previous post that LQR had this amazing robustness property: even if you optimize with the wrong model, you’ll still probably be OK. Is the same true about LQG? What are the guaranteed stability margins for LQG regulators? The answer was succinctly summed up in the <a href="https://ieeexplore.ieee.org/document/1101812">abstract of a 1978 paper by John Doyle</a>: “There are none.”</p>

<p class="center"><img width="400px" alt="There Are None" src="http://www.argmin.net/assets/there_are_none.png" /></p>

<p>What goes wrong? Doyle came up with a simple counterexample, that I’m going to simplify even further for the purpose of contextualizing in our modern discussion. Before presenting the example, let’s first dive into <em>why</em> LQG is likely less robust than LQR. Let’s assume that the true dynamics obeys the ODE:</p>



<p>though we computed the optimal controller with the matrix $B$. Define an error signal, $e_t = x_t - \hat{x}_t$, that measures the current deviation between the actual state and the estimate. Then, using the fact that $u_t = -K \hat{x}_t$, we get the closed loop dynamics</p>



<p>When $B=B_\star$, the bottom left block is equal to zero. The system is then stable provided $A-BK$ and $A-LC$ are both stable matrices (i.e., have eigenvalues in the left half plane). However, small perturbations in the off-diagonal block can make the matrix unstable. For intuition, consider the matrix</p>



<p>The eigenvalues of this matrix are $-1$ and $-2$, so the matrix is clearly stable. But the matrix</p>



<p>has an eigenvalue greater than zero if $t&gt;0.01$. So a tiny perturbation significantly shifts the eigenvalues and makes the matrix unstable.</p>

<p>Similar things happen in LQG. In Doyle’s example he uses the problem instance:</p>







<p>The open loop system here is unstable, having two eigenvalues at $1$. We can stabilize the system only by modifying the second state. The state disturbance is aligned along the $[1;1]$ direction, and the state cost only penalizes states aligned with this disturbance. So the goal is simply to remove as much signal as possible in the $[1;1]$ direction without using too much control authority. We only are able to measure the first component of the state, and this measurement is corrupted by Gaussian noise.</p>

<p>What does the optimal policy look like? Perhaps unsurprisingly, it focuses all of its energy on ensuring that there is little state signal along the disturbance direction. The optimal $K$ and $L$ matrices are</p>



<p>Now what happens when we have model mismatch? If we set $B_\star=tB$ and use the formula for the closed loop above, we see that closed loop state transition matrix is</p>



<p>It’s straight forward to check that when $t=1$ (i.e., no model mismatch), the eigenvalues of  $A-BK$ and $A-LC$ all have negative real parts. For the full closed loop matrix, analytically computing the eigenvalues themselves is a pain, but we can prove instability by looking at the characteristic polynomial. For a matrix to have all of its eigenvalues in the left half plane, its characteristic polynomial necessarily must have all positive coefficients. If we look at the linear term in the polynomial, we see that we must have</p>



<p>if we’d like any hope of having a stable system. Hence, we can guarantee that this closed loop system is unstable if $t\geq 1+\sigma$. This is a very conservative condition, and we could get a tighter bound if we’d like, but it’s good enough to reveal some paradoxical properties of LQG. The most striking is that if we build a sensor that gives us a better and better measurement, our system becomes more and more fragile to perturbation and model mismatch. For machine learning scientists, this seems to go against all of our training. How can a system become <em>less</em> robust if we improve our sensing and estimation?</p>

<p>Let’s look at the example in more detail to get some intuition for what’s happening. When the sensor noise gets small, the optimal Kalman Filter is more aggressive. If the model is true, then the disturbance has equal value in both states, so, when $\sigma$ is small, the filter can effectively just set the value of the second state to be equal to whatever is in the first state. The filter is effectively deciding that the first state should equal the observation $y_t$, and the second state should be equal to the first state. In other words, it rapidly damps any errors in the disturbance direction $[1;1]$ and, as $d$ increases, it damps the $[0;1]$ direction less. When $t \neq 1$, we are effectively introducing a disturbance that makes the two states unequal. That is, $B-B_\star$ is aligned in the $[0;1]$ and can be treated as a disturbance signal. This undamped component of the error is fed errors from the state estimate $\hat{x}$, and these errors compound each other. Since we spend so much time focusing on our control along the direction of the injected state noise, we become highly susceptible to errors in a different direction and these are the exact errors that occur when there is a gain mismatch between the model and reality.</p>

<p>The fragility of LQG has many takeaways. It highlights that noiseless state measurement can be a dangerous modeling assumption, because it is then optimal to trust our model too much. Though we apparently got a freebie with LQR, for LQG, model mismatch must be explicitly accounted for when designing the controller.</p>

<p>This should be a cautionary tale for modern AI systems. Most of the papers I read in reinforcement learning consider MDPs where we get perfect state measurement. Building an entire field around optimal actions with perfect state observation builds too much optimism. Any realistic scenario is going to have partial state observation, and such problems are much thornier.</p>

<p>A second lesson is that it is not enough to just improve the prediction components in feedback systems that are powered by machine learning. I have spoken with many applied machine learning engineers who have told me that they have seen performance degrade in production systems when they improve their prediction model. They might spend months building some state of the art LSTM mumbo jumbo that is orders of magnitude more accurate in prediction, but in production yields worse performance than the legacy system with a boring ARMA model. It is quite possible that these performance drops are due to the Doyle effect: the improved prediction system is increasing sensitivity to a modeling flaw in some other part of the engineering pipeline.</p>

<p>The story turns out to be even worse than what I have described thus far. The supposed robustness guarantees we derived for LQR assume not just full noiseless state measurement, but that the sensors and actuators have infinite bandwidth. That is, they assume you can build controllers $K$ with arbitrarily large entries and that react instantaneously, without delay, to changes in the state. In the next post, I’ll show how realistic sampled data controllers for LQR, even with noiseless state measurement, also have no guarantees.</p></div>







<p class="date">
<a href="http://benjamin-recht.github.io/2020/07/14/there-are-none/"><span class="datestr">at July 14, 2020 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://francisbach.com/?p=3843">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://francisbach.com/gradient-descent-for-wide-two-layer-neural-networks-implicit-bias/">Gradient descent for wide two-layer neural networks – II: Generalization and implicit bias</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://francisbach.com" title="Machine Learning Research Blog">Francis Bach</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p class="justify-text">In this blog post, we continue our investigation of gradient flows for wide two-layer “relu” neural networks. In the <a href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/">previous post</a>, Francis explained that under suitable assumptions these dynamics converge to global minimizers of the training objective. Today, we build on this to understand qualitative aspects of the predictor learnt by such neural networks. The content is mostly based on our recent joint work [<a href="https://arxiv.org/pdf/2002.04486.pdf">1</a>].</p>



<h2>1. Generalization with weight decay regularization</h2>



<p class="justify-text">Let us start our journey with the comfortable case where the training objective includes an explicit <em>weight decay</em> regularization (i.e. \(\ell_2\)-regularization on the parameters). Using the notations of the previous post, this consists in the following objective function on the space of probability measures on \(\mathbb{R}^{d+1}\):  $$ \underbrace{R\Big(\int_{\mathbb{R}^{d+1}} \Phi(w)d\mu(w)\Big)}_{\text{Data fitting term}} + \underbrace{\frac{\lambda}{2} \int_{\mathbb{R}^{d+1}} \Vert w \Vert^2_2d\mu(w)}_{\text{Regularization}} \tag{1}$$ where \(R\) is the loss and \(\lambda&gt;0\) is the regularization strength. Remember that a  neural network of finite width with \(m\) neurons is recovered with an empirical measure \(\mu = \frac1m \sum_{j=1}^m\delta_{w_j}\), in which case this regularization is proportional to the sum of the squares of all the parameters \(\frac{\lambda}{2m}\sum_{j=1}^m \Vert w_j\Vert^2_2\).</p>



<p class="justify-text"><strong>Variation norm.</strong> In the previous post, we have seen that the Wasserstein gradient flow of this objective function — an idealization of the gradient descent training dynamics in the large width limit — converges to a global minimizer \(\mu^*\) when initialized properly. An example of an admissible initialization is the hidden weights \(b_j\) distributed according to the uniform distribution \(\tau\) on the unit sphere \(\mathbb{S}^{d-1}\subset \mathbb{R}^d\) and the output weights \(a_j\) uniform in \(\{-1,1\}\). What does this minimizer look like in predictor space when the objective function is as in Eq. (1) ? </p>



<p class="justify-text">To answer this question, we define for a predictor \(h:\mathbb{R}^d\to \mathbb{R}\), the quantity $$ \Vert h \Vert_{\mathcal{F}_1} := \min_{\mu \in \mathcal{P}(\mathbb{R}^{d+1})} \frac{1}{2} \int_{\mathbb{R}^{d+1}} \Vert w\Vert^2_2 d\mu(w) \quad \text{s.t.}\quad h = \int_{\mathbb{R}^{d+1}} \Phi(w)d\mu(w).\tag{2} $$ As the notation suggests, \(\Vert \cdot \Vert_{\mathcal{F}_1}\) is a norm in the space of predictors. It is known as the <em>variation norm</em> [<a href="http://jmlr.org/papers/volume18/14-546/14-546.pdf">2</a>, <a href="https://www.cs.cas.cz/~vera/publications/journals/I3Edin.pdf">3</a>]. We call \(\mathcal{F}_1\) the space of functions with finite norm, which is a Banach space. By construction, the learnt predictor \(h^* = \int \Phi(w)d\mu^*(w)\) is a minimizer of the \(\mathcal{F}_1\)-regularized regression: $$ \min_{h:\mathbb{R}^d\to \mathbb{R}} R(h) + \lambda \Vert h \Vert_{\mathcal{F}_1} \tag{3}.$$ This \(\mathcal{F}_1\)-norm regularization shares similarity with \(\ell_1\) regularization [<a href="https://arxiv.org/pdf/1412.6614.pdf">4</a>]. To see this, observe that the “magnitude” \(\vert a\vert \Vert b\Vert_2\) of a relu function \(x\mapsto a(b^\top x)_+\) with parameter \(w=(a,b)\) equals \(\Vert w\Vert^2_2/2\) if \(\vert a\vert = \Vert b\Vert_2\) and is smaller otherwise. Thus parameterizing the relus by their direction \(\theta = b/\Vert b\Vert_2\) and optimizing over their signed magnitude \(r(\theta) = a\Vert b\Vert_2\)  we have $$ \Vert h \Vert_{\mathcal{F}_1} = \inf_{r:\mathbb{S}^{d-1}\to \mathbb{R}} \int_{\mathbb{S}^{d-1}} \vert r(\theta)\vert d\tau(\theta) \quad \text{s.t.}\quad h(x) = \int _{\mathbb{S}^{d-1}} r(\theta) (\theta^\top x)_+ d\tau(\theta).\tag{4}$$</p>



<p class="justify-text"><strong>Conjugate RKHS norm.</strong> The regression in the space \(\mathcal{F}_1\) is best understood when compared with the regression obtained by only training the output weights. We consider the same training dynamics with weight decay except that we fix the hidden weights to their initial value, where they are distributed according to the uniform distribution \(\tau\) on the sphere. In that case, the Wasserstein gradient flow also converges to the solution of a regularized regression as in Eq. (3) — this is in fact a convex problem —  but the regularizing norm is different and now defined as $$ \Vert h \Vert_{\mathcal{F}_2}^2 := \min_{r:\mathbb{S}^{d-1}\to \mathbb{R}} \int_{\mathbb{S}^{d-1}} \vert r(\theta)\vert^2 d\tau(\theta) \quad \text{s.t.}\quad h(x) = \int _{\mathbb{S}^{d-1}} r(\theta) (\theta^\top x)_+ d\tau(\theta).$$ We call \(\mathcal{F}_2\) the set of functions with finite norm. It can be shown to be a <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">Reproducing Kernel Hilbert Space</a> (RKHS), with kernel  $$ K(x,x’) = \int_{\mathbb{S}^{d-1}} (\theta^\top x)_+ (\theta^\top x’)_+ d\tau(\theta),$$ which has a closed form expression [<a href="https://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf">5</a>]. In this context, taking a finite width neural network corresponds to a random feature approximation of the kernel [<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&amp;rep=rep1&amp;type=pdf">6</a>, <a href="https://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines">7</a>].</p>



<p class="justify-text">Let us informally compare the properties of these spaces \(\mathcal{F}_1\) and \(\mathcal{F}_2\) (see [<a href="https://arxiv.org/abs/1412.8690">2</a>] for details):</p>



<ul class="justify-text"><li><strong>Approximation power.</strong> In high dimension, only very smooth functions have small \(\mathcal{F}_2\)-norm (in rough terms, the \(\lceil (d+3)/2\rceil\) first derivatives should be small). In contrast, there exists non-smooth functions with small \(\mathcal{F}_1\)-norm, an example being the relu function \(x\mapsto (\theta^\top x)_+\). Remarkably, if we define \(f(x)=g(Ux)\) where \(U\) is an orthogonal projection then \(\Vert f\Vert_{\mathcal{F}_1} \leq  \Vert g\Vert_{\mathcal{F}_2}\). This shows in particular that \(\mathcal{F}_1\) contains \(\mathcal{F}_2\) and that \(\mathcal{F}_1\) is <em>adaptive</em> to lower dimensional structures.</li><li><strong>Statistical complexity.</strong> It could be feared that the good approximation properties of \(\mathcal{F}_1\) come at the price of being “too large” as a hypothesis space, making it difficult to estimate a predictor in \(\mathcal{F}_1\) from few samples. But, as measured by their Rademacher complexities, the unit ball of \(\mathcal{F}_1\) is only \(O(\sqrt{d})\) larger than that of \(\mathcal{F}_2\). By going from \(\mathcal{F}_2\) to \(\mathcal{F}_1\), we thus add some nicely structured predictors to our hypothesis space, but not too much garbage that could fit unstructured noise.</li><li><strong>Generalization guarantees.</strong> By combining the two previous points, it is possible to prove that supervised learning in \(\mathcal{F}_1\) breaks the curse of dimensionality when the output depends on a lower dimensional projection of the input: the required number of training samples only depends mildly on the dimension \(d\).</li><li><strong>Optimization guarantees.</strong> However \(\mathcal{F}_1\) has a strong drawback : there is no known algorithm that solves the problem of Eq. (3) in polynomial time. On practical problems, gradient descent seems to behave well, but in general only qualitative results such as presented in the previous post are known. In contrast, various provably efficient algorithms can solve regression in \(\mathcal{F}_2\), which is a classical kernel ridge regression problem [Chap. 14.4.3, <a href="https://doc.lagout.org/science/Artificial%20Intelligence/Machine%20learning/Machine%20Learning_%20A%20Probabilistic%20Perspective%20%5BMurphy%202012-08-24%5D.pdf">8</a>].</li></ul>



<p class="justify-text">In the plot below, we compare the predictor learnt by gradient descent for a 2-D regression with the square loss and weight decay, after training (a) both layers — which is regression in \(\mathcal{F}_1\) — or (b) just the output layer — which is regression in \(\mathcal{F}_2\). This already illustrates some distinctive features of both spaces, although the differences become more stringent in higher dimensions. In particular, observe that in (a) the predictor is the combination of few relu functions, which illustrates  the sparsifying effect of the \(L^1\)-norm in Eq. (4). To simplify notations, we do not include a bias/intercept in the formulas but our numerical experiments include it, so in this plot the input is of the form \(x=(x_1,x_2,1)\) and \(d=3\).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="564" alt="" src="https://francisbach.com/wp-content/uploads/2020/07/regularized-2.png" class="wp-image-4231" height="293" />Predictor learnt by the gradient flow on the square loss with weight decay, when training (a) both layers (b) only the output layer. The markers indicate the location of the training samples  \((x_i)_{i=1}^n\). <a href="https://github.com/lchizat/2020_implicitbias_blog/blob/master/exp_weightdecay.jl">[code]</a></figure></div>



<p class="justify-text">The qualitative picture is quite clear so far, but something is a bit unsettling: weight decay is often not needed to obtain a good performance in practice. Our line of reasoning however completely falls apart without such a regularization: if the objective function depends on the predictor only via its values on the training set, being a minimizer does not guarantee anything about generalization outside of the training set (remember that wide relu neural networks are <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">universal approximators</a>). Why does it still work in the unregularized case? There must be something in the algorithm…</p>



<h2>2. Implicit bias: linear classification</h2>



<p class="justify-text">This something is called the <em>implicit bias</em> : when there are several minimizers, the optimization algorithm makes a specific choice. In the unregularized case, the “quality” of this choice is a crucial property of an algorithm; much more crucial than, say, its convergence speed on the training objective. To gradually build our intuition of the implicit bias of gradient flows, let us put neural networks aside for a moment and consider, following Soudry, Hoffer, Nacson, Gunasekar and Srebro [<a href="http://www.jmlr.org/papers/volume19/18-188/18-188.pdf">9</a>], a linear classification task.</p>



<p class="justify-text"><strong>Gradient flow of the smooth-margin.</strong> Let \((x_i,y_i)_{i=1}^n\) be a training set of \(n\) pairs of inputs \(x_i\in \mathbb{R}^d\) and outputs \(y_i\in \{-1,1\}\) and let us choose the exponential loss. The analysis that follows also apply to the logistic loss (which is the same as the cross-entropy loss after a sigmoid non-linearity) because only the “tail” of the loss matters, but it is more straightforward with the exponential loss. In order to give a natural “scale” to the problem, we  renormalize the empirical risk by taking minus its logarithm and consider the concave objective $$ F_\beta(a) = -\frac{1}{\beta}\log\Big( \frac1n \sum_{i=1}^n \exp(-\beta y_i \ x_i^\top a) \Big).\tag{5}$$ </p>



<p class="justify-text">Here \(\beta&gt;0\) is a parameter that will be useful in a moment. For now, we take \(\beta=1\) and we note \(F(a)=F_1(a)\).  In this context, the <em>margin</em> of a vector \(a\in \mathbb{R}^d\) is the quantity \(\min_{i} y_i\ x_i^\top a\) which quantifies how far this linear predictor is from making a wrong prediction on the training set.  </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="453" alt="" src="https://francisbach.com/wp-content/uploads/2020/07/max_margin-4.png" class="wp-image-4274" height="386" />The margin of the linear predictor \(x \mapsto a^\top x\) with parameters \(a \in \mathbb{S}^{d-1}\) is the smallest distance of a training point to the decision boundary. We show here the max-margin predictor.</figure></div>



<p class="justify-text">Obtained via simple manipulations, the inequalities  $$ \min_i y_i\ x_i^\top a \leq F_\beta(a) \leq \min_i y_i\ x_i^\top a +\frac{\log(n)}{\beta}, \tag{6}$$ suggest to call \(F_\beta\) the <em>smooth-margin</em> because, well, it is smooth and converges to the margin \(F_\infty(a) := \min_i y_i x_i^\top a\) as \(\beta\to \infty\). Let us look at the gradient flow in the ascent direction that maximizes the smooth-margin: $$ a'(t) = \nabla F(a(t))$$ initialized with \(a(0)=0\) (here the initialization does not matter so much). The path followed by this gradient flow is exactly the same as the gradient flow on the empirical risk: taking the logarithm only changes the time parameterization or, in practice, the step-size.</p>



<p class="justify-text"><strong>Convergence to the max-margin.</strong> Assume that the data set is linearly separable, which means that the \(\ell_2\)-max-margin $$ \gamma := \max_{\Vert a\Vert_2 \leq 1} \min_i y_i x_i^\top a$$ is positive. In this case \(F\) is unbounded (indeed \(\lim_{\alpha \to \infty} F(\alpha a) =\infty\) whenever \(a\)  has a positive margin) and thus \(a(t)\) diverges. This is not an issue as such, since for classification, only the sign of the prediction matters.  This just means that the relevant question is not “where does \(a(t)\) converge?” but rather “towards which direction does it diverge?”. In other words, we are interested in the limit of \(\bar a(t):= a(t)/\Vert a(t)\Vert_2\) (in convex analysis, this is called the <em>cosmic limit</em> of \(a(t)\) [Chap. 3, <a href="https://www.springer.com/gp/book/9783540627722">10</a>], isn’t it beautiful ?).</p>



<p class="justify-text">The argument that follows is adapted from [<a href="https://arxiv.org/pdf/1802.08246.pdf">11</a>, <a href="https://arxiv.org/pdf/1803.07300.pdf">12</a>] and can be traced back to [<a href="http://proceedings.mlr.press/v28/telgarsky13-supp.pdf">13</a>] for coordinate ascent. It can be shown by looking at the structure of the gradient (see the end of the blog post) that \(\Vert \nabla F(a)\Vert_2\geq \gamma\) for all \(a\in \mathbb{R}^d\). By the inequality of Eq. (6) and the gradient flow property \(\frac{d}{dt}F(a(t))=\Vert \nabla F(a(t))\Vert_2^2\), it follows $$\begin{aligned}\min_i y_i x_i^\top a(t) \geq F(a(t)) \  – \log(n) \geq \gamma \int_0^t \Vert \nabla F(a(s))\Vert_2ds -\log (n).\end{aligned}$$  For \(t&gt; \log(n)/\gamma^2\), this lower bound is positive. We can then divide the left-hand side by \(\Vert a(t)\Vert_2\) and the right-hand side by the larger quantity \(\int_0^t \Vert\nabla F(a(s))\Vert_2ds\), and we get $$\min_i y_i x_i^\top \bar a(t) \geq \gamma -\frac{\log(n)}{\int_0^t \Vert\nabla F(a(s))\Vert_2ds} \geq \gamma -\frac{\log(n)}{\gamma t}.$$ This shows that the margin of \(\bar a(t) := a(t)/\Vert a(t)\Vert_2\) converges to the \(\ell_2\)-max-margin at a rate \(\log(n)/\gamma t\). That’s it, the implicit bias of this gradient flow is exposed!</p>



<p class="justify-text"><strong>Stability to step-size choice.</strong> To translate this argument to discrete time, we need decreasing step-sizes of order \(1/\sqrt{t}\) which deteriorates the convergence rate to \(\tilde O(1/\sqrt{t})\), see [<a href="https://arxiv.org/pdf/1802.08246.pdf">11</a>, <a href="https://arxiv.org/pdf/1803.07300.pdf">12</a>]. In [<a href="https://arxiv.org/pdf/2002.04486.pdf">1</a>], we proposed a different proof strategy (based on an online optimization interpretation of \(\bar a(t)\), as below) which recovers the same convergence rate \(O(1/\sqrt{t})\) with <em>exponentially larger</em> step-sizes. This suggests that these diverging trajectories are extremely robust to the choice of step-size.</p>



<p class="justify-text"><strong>Illustration. </strong>In the figure below, we plot on the left the evolution of the parameter \(a(t)\) and on the right the predictor \(x\mapsto (x,1)^\top a(t)\) with \(x\in \mathbb{R}^2\). In parameter space, we apply the hyperbolic tangent to the radial component which allows to easily visualize diverging trajectories. This way, the unit sphere represents the <em>horizon</em> of \(\mathbb{R}^d\), i.e., the set of directions at infinity [Chap. 3 in <a href="https://www.springer.com/gp/book/9783540627722">9</a>]. We will use the same convention in the other plots below.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img width="586" alt="" src="https://francisbach.com/wp-content/uploads/2020/07/linear.gif" class="wp-image-4106" height="288" />Implicit bias of gradient descent for a linear classification task with the exponential loss: (left) parameter space, (right) predictor space.</figure></div>



<h2>3. Implicit bias:  training only the output layer</h2>



<p class="justify-text">Despite its apparently restrictive setting, the previous result already tells us something about wide neural networks. Consider the situation touched upon earlier where we only train the output weights \(a_j\) and the hidden weights \(b_j\) are picked uniformly at random on the sphere. This corresponds to learning a linear classifier on top of the random feature \([(b_j^\top x)_+]_{j=1}^m\). </p>



<p class="justify-text">As we have just shown, if the training set is separable, the normalized gradient flow of the unregularized exponential loss (or logistic loss) converges to a solution to  $$ \max_{\Vert a\Vert_2 \leq 1}\min_i y_i \sum_{j=1}^m  a_j (b_j^\top x_i)_+.$$ </p>



<p class="justify-text">This is a random feature approximation for the unregularized kernel support vector machine problem in the RKHS \(\mathcal{F}_2\), which is recovered in the large width limit \(m\to \infty\):  $$\max_{\Vert h\Vert_{\mathcal{F}_2}\leq 1} \min_i y_i h(x_i).$$ Notice that if \(m\) is large enough, the linear separability assumption is not even needed anymore, because any training set is separable in \(\mathcal{F}_2\) (at least if all \(x_i\)s are distinct and if we do not forget to include the bias/intercept).</p>



<p class="justify-text"><strong>Illustration.</strong> In the animation below, we plot on the left the evolution of the parameters and on the right the predictor for a 2-D classification task. In parameter space, each particle represents a neuron: their direction is fixed, their distance to \(0\) is their absolute weight and the color is red (+) or blue (-) depending on the sign of the weight. As above, the unit sphere is at infinity and the particles diverge. In predictor space, the markers represent the training samples of both classes, the color shows the predictor and the black line is the decision boundary. The fact that the predictor has a smooth decision boundary is in accordance with the properties of \(\mathcal{F}_2\) given above. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img src="https://francisbach.com/wp-content/uploads/2020/07/film_output_comp-1.gif" alt="" class="wp-image-4275" />Gradient descent on the output layer of a two-layer relu neural network with the exponential loss: (left) parameter space, (right) predictor space. <a href="https://github.com/lchizat/2020_implicitbias_blog/blob/master/exp_output.jl">[code]</a></figure></div>



<h2>4. Implicit bias: 2-homogeneous linear classifiers</h2>



<p class="justify-text">Although the analyses where neural networks behave like kernel methods are pleasant for us theoreticians because we are in conquered territory, they miss essential aspects of neural networks such as their adaptivity and their ability to learn a representation. Let us see if we can characterize the implicit bias of the gradient flow of the unregularized exponential loss when training <em>both</em> layers of the neural network.</p>



<p class="justify-text"><strong>A 2-homogeneous linear model.</strong> From an optimization point of view, an important property of two layer relu neural networks is that \(\Phi(\alpha w)= \alpha^2 \Phi(w)\) for all \(\alpha&gt;0\), i.e., they are positively 2-homogeneous in the training parameters. In contrast, a linear model is 1-homogeneous in the parameters. This seemingly little difference leads to drastic changes in the gradient flow dynamics. </p>



<p class="justify-text">Let us again build our intuition with a simplified model that captures key aspects of the dynamics, namely the linear classification setting of above. This time, we take any initialization \(r(0)\in \mathbb{R}^d\) with positive entries and the gradient flow in the ascent direction of the function \( F(r\odot r)\) where \(\odot\) is the pointwise product between two vectors and \(F\) is defined in Eq. (5). This is just a trick to obtain a 2-homogeneous parameterization of a linear model. This gradient flow satisfies $$ r'(t) = 2 r(t)\odot \nabla F(r(t)\odot r(t)).$$ </p>



<p class="justify-text"><strong>Normalized dynamics.</strong> Let us define \(\bar a(t):=(r(t)\odot r(t))/\Vert r(t)\Vert_2^2\) the normalized predictor associated to our dynamics which, by definition, belongs to the simplex \(\Delta_d\), i.e., the set of nonnegative vectors in \(\mathbb{R}^d\) that sum to one. Using the fact that \(\nabla F(\beta a) = \nabla F_\beta (a)\) for all \(\beta&gt;0\), we obtain $$\begin{aligned} \bar a'(t) &amp;= 2\frac{r(t)\odot r'(t)}{\Vert r(t)\Vert_2^2} -2 (r(t)^\top r'(t))\frac{r(t)\odot r(t)}{\Vert r(t)\Vert_2^4}\\ &amp;=4\bar a(t) \odot \nabla F_{\Vert r(t)\Vert_2^2}(\bar a(t))\ – \alpha(t) \bar a(t)\end{aligned}$$ where \(\alpha(t)\) is the scalar such that \(\sum_{i=1}^d a’_i(t) =0\). Online optimization experts might have recognized that this is (continuous time) <em>online mirror ascent in the simplex</em> for the sequence of smooth-margin functions \(F_{\Vert r(t)\Vert_2^2}\). Notice in particular the multiplicative updates: they correspond to the entropy mirror function, and they are particularly well suited for optimization in the high dimensional simplex [Chap.4, <a href="https://arxiv.org/pdf/1405.4980.pdf">14</a>].</p>



<p>What do we learn from this reformulation? </p>



<ul class="justify-text"><li>We can prove (by similar means) that if the data set is linearly separable then \(\Vert r(t)\Vert_2^2\) diverges. So the sequence of functions \(F_{\Vert r\Vert_2^2}\) converges to the margin \(F_\infty\) which means that \(\bar a(t)\) just ends up optimizing the function \(F_\infty\). As a consequence, we have $$\lim_{t\to \infty} y_i x_i^\top \bar a(t) = \max_{a\in \Delta_d} \min_{i} y_i x_i^\top a.$$ This exposes another implicit bias of gradient flow. Notice the key difference with the implicit bias obtained with a linear parameterization: we obtain here the \(\ell_1\)-max-margin (over classifiers with non-negative entries) instead of the \(\ell_2\)-max-margin.  </li><li>Beyond exposing the implicit bias, this reformulation shows that \(\bar a(t)\) implicitly optimizes a sequence of smooth objectives which converge to the margin \(F_\infty\). Unknowingly, we have recovered the well-principled optimization method that consists in approximating a non-smooth objective with smooth functions [<a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">15</a>].</li><li>While the conclusion above was only formal, this point of view leads to rigorous proofs of convergence and convergence rates in discrete time in \(\tilde O(1/\sqrt{t})\) with a step-size in \(O(1/\sqrt{t})\), by  exploiting tools from online optimization, see [<a href="https://arxiv.org/pdf/2002.04486.pdf">1</a>].</li></ul>



<h2>5. Implicit bias: fully trained 2-layer neural networks</h2>



<p class="justify-text">Once again this argument about linear predictors applies to neural networks: if we train both layers but only the magnitude of the hidden weights and not their direction, then this is equivalent to learning a 2-homogeneous linear model on top of the random feature \([  a_j(0) (x_i^\top b_j(0))_+]_{j=1}^m\). If each feature appears twice with opposite signs — which is essentially the case in the large width limit — then the simplex constraint can be equivalently replaced by an \(\ell_1\)-norm constraint on the weights. Recalling the definition of the \(\mathcal{F}_1\)-norm from Eq. (4), we thus obtain that, in the infinite-width limit, the normalized predictor converges to a solution to $$ \max_{\Vert h\Vert_{\mathcal{F}_1} \leq 1} \min_i y_i h(x_i).$$</p>



<p class="justify-text">This result is correct, but it is not relevant. In contrast to functions in \(\mathcal{F}_2\), functions in \(\mathcal{F}_1\) <em>can not</em> in general be approximated with few <em>random</em> features in high dimension. In fact, lower bounds that are exponential in the dimension exist in certain settings [Sec. X, <a href="http://www.stat.yale.edu/~arb4/publications_files/UniversalApproximationBoundsForSuperpositionsOfASigmoidalFunction.pdf">16</a>]. They can be approximated with a small number of features but those need to be data-dependent: in that sense, it is necessary to learn a representation – here,  a distribution over the hidden weights — in order to learn in \(\mathcal{F}_1\). </p>



<p class="justify-text">This raises the following question: do we obtain the same implicit bias when training both layers of the neural network, without fixing the direction of the input weights? In the following result, which is the main theorem of our paper [<a href="https://arxiv.org/abs/2002.04486">1</a>], we answer by the affirmative.</p>



<p class="justify-text"><strong>Theorem</strong> (C. and Bach [<a href="https://arxiv.org/abs/2002.04486">1</a>], informal). Assume that for some \(\sigma&gt;0\), the hidden weights \(b_j\) are initialized uniformly on the sphere of radius \(\sigma\) and the output weights \(a_j\) are uniform in \(\{-\sigma,\sigma\}\). Let \(\mu_t\) be the Wasserstein gradient flow for the unregularized exponential loss and \(h_t = \int \Phi(w)d\mu_t(w)\) be the corresponding dynamics in predictor space. Under some technical assumptions, the normalized predictor \(h_t/\Vert h_t\Vert_{\mathcal{F}_1}\) converges to a solution to the \(\mathcal{F}_1\)-max-margin problem: $$\max_{\Vert h\Vert_{\mathcal{F}_1} \leq 1} \min_i y_i h(x_i).$$</p>



<p class="justify-text">Giving an idea of proof would be a bit too technical for this blog post, but let us make some remarks:</p>



<ul class="justify-text"><li>The strength of this result is that although this dynamics could get trapped towards limit directions which are not optimal, this choice of initialization allows to avoid them all and to only converge to <em>global</em> minimizers of this max-margin problem. The principle behind this is similar to the global convergence result in the previous blog post. </li><li>The fact that optimizing on the direction of the hidden weights is compatible with the global optimality conditions of the \(\mathcal{F}_1\)-max-margin problem is very specific to the structure of positively 2-homogeneous problems, and should not be taken for granted for other architectures of neural networks.</li><li>Although at a formal level this result works for any initialization that is diverse enough (such as the standard Gaussian initialization), the initialization proposed here yields dynamics with a better behavior for relu networks: by initializing the hidden and output weights with equal norms – a property preserved by the dynamics – we avoid some instabilities in the gradient. Also notice that this result applies to any scale \(\sigma&gt;0\) of the initialization (we’ll see an intriguing consequence of this in the next section).</li></ul>



<p class="justify-text"><strong>Illustration.</strong> In the figure below, we plot the training dynamics when both layers are trained. In parameter space (left), each particle represents a neuron: its position is \(\vert a_j\vert b_j\) and its color depends on the sign of \(a_j\).  Here again the unit sphere is at infinity. The inactive neurons at the bottom correspond to those with a bias that is “too negative” at initialization. We observe that all the other neurons gather into few clusters: this is the sparsifying effect of the \(L^1\)-norm in Eq. (4). In predictor space, we obtain a polygonal classifier, as expected for a \(\mathcal{F}_1\)-max-margin classifier. See the paper [<a href="https://arxiv.org/pdf/2002.04486.pdf">1</a>] for experiments that illustrate the strengths of this classifier in terms of generalization.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img src="https://francisbach.com/wp-content/uploads/2020/07/film_both_comp.gif" alt="" class="wp-image-4194" />Training both layers of a wide relu neural network with the exponential loss: (left) space of parameters, (right) space of predictors. <a href="https://github.com/lchizat/2020_implicitbias_blog/blob/master/exp_bothlayers.jl">[code]</a></figure></div>



<h2>6. Lazy regime and the neural tangent kernel</h2>



<p class="justify-text">This blog post would not be complete without mentioning the <em>lazy regime</em>. This is yet another kind of implicit bias which, in our context, takes place when at initialization the weights have a large magnitude and the step-size is small. It was first exhibited in [<a href="https://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf">17</a>] for deep neural networks.</p>



<p class="justify-text"><strong>Lazy training via scaling.</strong> This phenomenon is in fact very general so let us present it with a generic parametric predictor \(h(W)\) with differential \(Dh(W)\). We introduce a scaling factor \(\alpha&gt;0\) and look at the gradient flow of \(F(W) := R(\alpha h(W))\) with a step-size \(1/\alpha^2\), that is $$ W'(t) = \ – \frac{1}{\alpha}Dh(W(t))^\top \nabla R(\alpha h(W(t))),$$ with initialization \(W(0)\). In terms of the predictor \(\alpha h(W)\), this yields the dynamics $$\frac{d}{dt} \alpha h(W(t)) = \ – Dh(W(t))Dh(W(t))^\top \nabla R(\alpha h(W(t)).$$ </p>



<p class="justify-text">Lazy training [<a href="https://arxiv.org/pdf/1812.07956.pdf">18</a>] happens when we take \(\alpha\) large while making sure that \(\alpha h(W(0))\) stays bounded. In this case, we see that the parameters change at a rate \(O(1/\alpha)\), while the predictor changes at a rate independent of \(\alpha\). On any bounded time interval, in the limit of  a large \(\alpha\), the parameters only move infinitesimally, while the predictor still makes significant progress, hence the name <em>lazy training</em>.</p>



<p class="justify-text"><strong>Equivalent linear model.</strong> Since the parameters hardly move, if we assume that \(Dh(W(0))\neq 0\) then we can replace the map \(h\) by its linearization \(W \mapsto h(W(0))+Dh(W(0))(W-W(0))\). This means that the training dynamics essentially follows the gradient flow of the  objective $$ R\big ( \alpha h(W(0)) + \alpha Dh(W(0))(W-W(0)) \big)$$ which is a convex function of \(W\) as soon as \(R\) is convex.</p>



<p class="justify-text">If this objective admits a minimizer that is not too far away from \(W(0)\), then \(W(t)\) converges to this minimizer. If in contrast all  the minimizers are too far away (think of the exponential loss where they are at infinity), then the parameters will eventually move significantly and the lazy regime is just a transient regime in the early phase of training.  Of course, all these behaviors can be quantified and made more precise, because this phenomenon brings us back to the realm of linear models. </p>



<p class="justify-text">What all of this has to do with two-layer neural networks? As it happens, this scale factor appears implicit in various situations for these models; let us detail two of them. </p>



<p class="justify-text"><strong>Neural networks with \(1/\sqrt{m}\) scaling.</strong> For two-layer neural networks, lazy training occurs if we define \(h = \frac{1}{\sqrt{m}} \sum_{j=1}^m \Phi(w_j)\) instead of \(h=\frac{1}{m} \sum_{j=1}^m \Phi(w_j)\) before taking the infinite width limit. Indeed:</p>



<ul class="justify-text"><li>This induces a scaling factor \(\alpha = \sqrt{m} \to \infty\) compared to \(1/m\) which, as we have already seen, is the “correct” scaling that leads to a non-degenerate dynamics in parameter space as \(m\) increases. </li><li>Moreover, by the central limit theorem,  \(\frac{1}{\sqrt{m}} \sum_{j=1}^m \Phi(w_j(0)) = O(1)\) for typical random initializations of the parameters. So the initial predictor stays bounded.</li></ul>



<p class="justify-text">To take the Wasserstein gradient flow limit, the step-size has to be of order \(m\) (see previous blog post). So here we should take a step-size of order \(m/\alpha^2 = 1\). With such a step-size, all the conditions for lazy training are gathered when \(m\) is large. Intuitively, each neuron only moves infinitesimally, but they collectively produce a significant movement in predictor space.</p>



<p class="justify-text"><strong>Neural networks with large initialization.</strong> Coming back to our scaling in \(1/m\) and our Wasserstein gradient flow that is obtained in the large width limit, there is another way to enter the lazy regime: by increasing the variance of the initialization. </p>



<p class="justify-text">To see this, assume that \(h\) is a positively \(p\)-homogeneous parametric predictor, which means that \(h(\sigma W)=\sigma^p h(W)\) for all \(\sigma&gt;0\) and some \(p&gt;1\) (remember that this is true with \(p=2\) for our two-layer relu neural network). Take an initialization of the form \(W(0) = \sigma \bar W_0\) where \(\sigma&gt;0\) and \(h(\bar W_0)=0\) (which is also satisfied for our infinite width neural networks with the initialization considered previously). Consider the gradient flow of \(R(h(W))\) with step-size \(\sigma^{2-2p}\).   By defining \(\bar W(t) = W(t)/\sigma\) and using the fact that the differential of a p-homogeneous function <a href="https://en.wikipedia.org/wiki/Homogeneous_function#Positive_homogeneity">is (p-1)-homogeneous</a>, we have, on the one hand $$ \bar W'(t) = -\sigma^{-p} Dh(\bar W(t))^\top \nabla R(\sigma^p h(\bar W(t))), $$ and on the other hand $$\frac{d}{dt} \sigma^p h(\bar W(t)) =\  – Dh(\bar W(t))Dh(\bar W(t))^\top \nabla R(\sigma^p h(\bar W(t))).$$ So in terms of the dynamics \(\bar W(t)\), the situation is exactly equivalent to having a scaling factor \(\alpha=\sigma^p\). This implies that as the magnitude \(\sigma\) of the initialization increases, we enter the lazy regime, provided the step-size is of order \(\sigma^{2-2p}\).</p>



<p class="justify-text"><strong>Neural tangent kernel. </strong>What does the lazy regime tell us about the learnt predictor for two-layer neural networks? Assuming for simplicity that the predictor at initialization is \(0\), this regime amounts to learning a linear model on top of the feature \([(b_j^\top x)_+]_{j=1}^m\) — the derivative with respect to the output weights — concatenated with the feature \([x a_j 1_{b_j^\top x &gt; 0} ]_{j=1}^m\)  — the derivative with respect to the input weights. Compared to training only the output layer, this thus simply adds some features. </p>



<p class="justify-text">Assume for concreteness, that at initialization the hidden weights \(b_j\) are uniform on a sphere of large radius \(\sigma&gt;0\) and the output weights are uniform on \(\{-\kappa\sigma, \kappa\sigma\}\) where \(\kappa\geq 0\). For a large width and a large \(\sigma\), we enter the lazy regime which amounts to learning in a RKHS — let us call it \(\mathcal{F}_{2,\kappa}\) — that is slightly different from \(\mathcal{F}_2 = \mathcal{F}_{2,0}\), since its kernel \(K_\kappa\) contains another term: $$ K_\kappa(x,x’) = \int_{\mathbb{S}^{d-1}} (\theta^\top x)_+ (\theta^\top x’)_+d\tau(\theta) + \kappa^2 \int_{\mathbb{S}^{d-1}} (x^\top x’) 1_{\theta^\top x &gt; 0}1_{\theta^\top x’ &gt; 0}d\tau(\theta). $$</p>



<p class="justify-text">This kernel is called the Neural Tangent Kernel [<a href="https://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf">17</a>] and the properties of the associated RKHS have been studied in [<a href="https://arxiv.org/pdf/1904.12191.pdf">19</a>, <a href="http://papers.nips.cc/paper/9449-on-the-inductive-bias-of-neural-tangent-kernels.pdf">20</a>], where it is shown to include functions that are slightly less smooth than those of \(\mathcal{F}_2\) when \(\kappa\) increases. This is illustrated in the plot below, obtained by training a wide neural network with \(\sigma\) large (to reach the lazy regime) on the square loss, and various values of \(\kappa\).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img width="574" alt="" src="https://francisbach.com/wp-content/uploads/2020/07/interp-4.png" class="wp-image-4213" height="287" />1-D regression with a wide two-layer relu neural network (gradient descent on square loss, in the lazy regime) with 4 training samples (black dots). At initialization, output weights have \(\kappa\) times the (large) magnitude of the hidden weights. This implicitly solves kernel ridgeless regression for a kernel that depends on \(\kappa\). <a href="https://github.com/lchizat/2020_implicitbias_blog/blob/master/exp_NTK.jl">[code]</a></figure></div>



<p class="justify-text"><strong>Two implicit biases in one shot.</strong> The attentive reader might have noticed that for large initialization scale \(\sigma\gg 1\), when training both layers on the unregularized exponential loss, two of our analyses apply:  lazy training — that leads to a max-margin predictor in \(\mathcal{F}_{2,\kappa}\) — and the asymptotic implicit bias — that leads to a max-margin predictor in \(\mathcal{F}_{1}\).  So, where is the catch? </p>



<p class="justify-text">There is none! Since the minimizers of this loss are at infinity, the lazy regime is just a transient phase and we will observe both implicit biases along the training dynamics! Take a look at the video below: we observe that in early phases of training, the neurons do not move while learning a smooth classifier — this is the lazy regime and the classifier approaches the \(\mathcal{F}_{2,\kappa}\)-max-margin classifier. In later stages of training, the neurons start moving and the predictor converges to a \(\mathcal{F}_1\)-max-margin classifier as stated by the main theorem. The predictor jitters a little bit during training because I have chosen rather aggressive step-sizes.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large"><img src="https://francisbach.com/wp-content/uploads/2020/07/film_lazy2sparse_ns_comp-1.gif" alt="" class="wp-image-4219" />Training both layers with gradient descent for the unregularized exponential loss. The only difference with the previous video is that at initialization the variance \(\sigma^2\) is larger and the step-size smaller \(\approx \sigma^{-2}\). First the network learns a classifier in the lazy regime (a kernel max-margin classifier) and eventually converges to the \(\mathcal{F}_1\)-max-margin classifier.</figure></div>



<h2>Discussion</h2>



<p class="justify-text">In this blog post, I described how analyses of the training dynamics can help us understand the properties of the predictor learnt by neural networks even in the absence of an explicit regularization. Already for the simplest algorithm one can think of — gradient descent — we have found a variety of behaviors depending on the loss, the initialization or the step-size. </p>



<p class="justify-text">To achieve this description, the infinite width limit is of great help. It allows to obtain synthetic and precise characterizations of the learnt predictor, that can be used to derive generalization bounds. Yet, there are many interesting non-asymptotic effects caused by having a finite width.  In that sense, we were only concerned with the end of the curve of double descent [<a href="https://www.pnas.org/content/pnas/116/32/15849.full.pdf">21</a>].</p>



<h2>References</h2>



<p class="justify-text">[1] Lénaïc Chizat, Francis Bach. <a href="https://arxiv.org/pdf/2002.04486.pdf">Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss.</a> <em>To appear in Conference On Learning Theory</em>, 2020.<br />[2] Francis Bach. <a href="http://jmlr.org/papers/volume18/14-546/14-546.pdf">Breaking the curse of dimensionality with convex neural networks.</a> <em>The Journal of Machine Learning Research</em>, <em>18</em>(1), 629-681, 2017.<br />[3]  Vera Kurková, Marcello Sanguineti. <a href="https://www.cs.cas.cz/~vera/publications/journals/I3Edin.pdf">Bounds on rates of variable-basis and neural-network approximation.</a> <em>IEEE Transactions on Information Theory</em>, 47(6):2659-2665, 2001.  <br />[4] Behnam Neyshabur, Ryota Tomioka, Nathan Srebro. <a href="https://arxiv.org/pdf/1412.6614.pdf">In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning.</a> <em>ICLR (Workshop)</em>. 2015.<br />[5] Youngmin Cho, Lawrence K. SAUL.  <a href="https://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf">Kernel methods for deep learning.</a> <em>Advances in neural information processing systems</em>. 342-350, 2009.<br />[6] Radford M. Neal. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&amp;rep=rep1&amp;type=pdf"><em>Bayesian learning for neural networks</em>.</a> Springer Science &amp; Business Media, 2012.<br />[7] Ali Rahimi, Benjamin Recht. <a href="https://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf">Random features for large-scale kernel machines.</a> <em>Advances in neural information processing systems</em>. 1177-1184, 2008.<br />[8] Kevin P. Murphy. Machine Learning: A Probabilistic Perspective. <em>The MIT Press</em>, 2012<br />[9] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, Nathan Srebro. <a href="http://www.jmlr.org/papers/volume19/18-188/18-188.pdf">The Implicit Bias of Gradient Descent on Separable Data.</a><em> The Journal of Machine Learning Research</em>, <em>19</em>(1), 2822-2878, 2018.<br />[10] R. Tyrrell Rockafellar, Roger J-B. Wets. <a href="https://www.springer.com/gp/book/9783540627722"><em>Variational analysis</em>.</a> Springer Science &amp; Business Media, 2009.<br />[11] Suriya Gunasekar,  Jason D. Lee, Daniel Soudry, Nathan Srebro.  <a href="https://par.nsf.gov/servlets/purl/10107856">Characterizing implicit bias in terms of optimization geometry.</a> <em>International Conference on Machine Learning</em>, 2018.<br />[12] Ziwei Ji, Matus Telgarsky. <a href="https://arxiv.org/pdf/1803.07300.pdf">Risk and parameter convergence of logistic regression.</a> 2018.<br />[13] Matus Telgarsky. <a href="https://arxiv.org/abs/1303.4172">Margins, Shrinkage, and Boosting.</a> <em>International Conference on Machine Learning</em>, 307-315, 2013.<br />[14] Sébastien Bubeck. <a href="https://arxiv.org/pdf/1405.4980.pdf">Convex Optimization: Algorithms and Complexity.</a> Foundations and Trends in Machine Learning, 8(3-4):231-357, 2015.<br />[15] Yuri Nesterov. <a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">Smooth minimization of non-smooth functions.</a> <em>Mathematical programming</em>, 103(1):127-152, 2005.<br />[16] Anrew R. Barron. <a href="http://www.stat.yale.edu/~arb4/publications_files/UniversalApproximationBoundsForSuperpositionsOfASigmoidalFunction.pdf">Universal approximation bounds for superpositions of a sigmoidal function.</a> <em>IEEE Transactions on Information theory. </em>39(3), 930-945, 1993.<br />[17] Jacot, Arthur, Franck Gabriel, Clément Hongler. <a href="https://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf">Neural tangent kernel: Convergence and generalization in neural networks.</a> <em>Advances in neural information processing systems.</em> 8571-8580, 2018.<br />[18] Lénaïc Chizat, Édouard Oyallon, Francis Bach. <a href="https://papers.nips.cc/paper/8559-on-lazy-training-in-differentiable-programming.pdf">On lazy training in differentiable programming.</a> <em>Advances in Neural Information Processing Systems.</em> 2937-2947, 2019.<br />[19] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, Andrea Montanari. <a href="https://arxiv.org/pdf/1904.12191.pdf">Linearized two-layers neural networks in high dimension.</a> To appear in <em>Annals of Statistics</em>. 2019.<br />[20] Alberto Bietti, Julien Mairal. <a href="http://papers.nips.cc/paper/9449-on-the-inductive-bias-of-neural-tangent-kernels">On the Inductive Bias of Neural Tangent Kernels</a>. <em>Advances in Neural Information Processing Systems.</em> p. 12893-12904, 2019.<br />[21] Mikhail Belkin, Daniel Hsu, Siyuan Ma, Soumik Mandal. <a href="https://www.pnas.org/content/pnas/116/32/15849.full.pdf">Reconciling modern machine-learning practice and the classical bias–variance trade-off.</a> <em>Proceedings of the National Academy of Sciences.</em> <em>116</em>(32), 15849-15854, 2019.</p>



<h3>Lower bound on the gradient norm for linear classification with the exponential loss</h3>



<p class="justify-text">In the context of Section 2, we want to prove that \(\Vert \nabla F(a)\Vert_2\geq \gamma\). For this, let \(Z\in \mathbb{R}^{n\times d}\) be the matrix with rows \(y_i x_i\) and let \(\Delta_n\) be the simplex in \(\mathbb{R}^n\). We have by duality $$ \gamma = \max_{\Vert a\Vert_2\leq 1}\min_{p\in \Delta_n} p^\top Z a =   \min_{p\in \Delta_n} \max_{\Vert a\Vert_2\leq 1} a^\top Z^\top p = \min_{p\in \Delta_n} \Vert Z^\top p\Vert_2 .$$  Also, notice that \(\nabla F(a) = Z^\top p\) with \(p_i = \frac{e^{-y_ix_i^\top a}}{\sum_{j=1}^n e^{-y_{j}x_{j}^\top a}}\). Since \(p \in \Delta_n\), we conclude that \(\Vert \nabla F(a)\Vert_2\geq \min_{p\in \Delta_n} \Vert Z^\top p\Vert_2 = \gamma\).</p></div>







<p class="date">
by Lénaïc Chizat <a href="https://francisbach.com/gradient-descent-for-wide-two-layer-neural-networks-implicit-bias/"><span class="datestr">at July 13, 2020 07:39 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7770">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2020/07/13/simons-institute-lectures-on-analysis-of-boolean-functions/">Simons institute lectures on analysis of Boolean functions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>(Does it still make sense to blog such announcements or is these days <a href="https://twitter.com/boazbaraktcs/status/1282443765224017920">Twitter</a> the only way to go about this? Asking for a friend <img src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;" class="wp-smiley" alt="🙂" /> )</em></p>



<p>Prasad Raghavendra and Avishay Tal have organized a sequence of 6 lectures on some of the exciting recent advances in analysis of Boolean functions. </p>



<p><a href="https://simons.berkeley.edu/events/boolean" target="_blank" rel="noreferrer noopener">Lecture Series: Advances in Boolean Function Analysis</a><br /></p>



<p>The Simons Institute is organizing a series of lectures on Advances in Boolean Function Analysis, that will highlight a few major developments in the area. The series will feature weekly two-hour lectures from July 15th to Aug 18th.  The lectures aim to address both the broad context of the results and their technical details. Though closely related in theme, each lecture will be self-contained.  The schedule is attached below (more info at <a href="https://simons.berkeley.edu/events/boolean" target="_blank" rel="noreferrer noopener">link</a>). </p>



<p>Talks take place on Wednesdays at 10am Pacific time (1pm Eastern). If you can’t catch them live, they will be redcorded.</p>



<p><strong>Zoom Link: </strong><a href="https://berkeley.zoom.us/j/93086371156" target="_blank" rel="noreferrer noopener">https://berkeley.zoom.us/j/93086371156</a><br /></p>



<p><strong><u>Talk Schedule:</u></strong></p>



<p>July 15, Wednesday  10:00am PDT (1pm EDT) <em>Dor Minzer (Institute of Advanced Study)<a href="https://simons.berkeley.edu/events/boolean-1" target="_blank" rel="noreferrer noopener">On the Fourier-Entropy Influence Conjecture</a></em></p>



<p><br />July 22, Wednesday, 10:00am PDT (1pm EDT) <em>Hao Huang (Emory University) &amp; Avishay Tal (UC Berkeley)<a href="https://simons.berkeley.edu/events/boolean-3" target="_blank" rel="noreferrer noopener">Sensitivity Conjecture and Its Applications</a></em></p>



<p><br />August 3rd, Monday, 10:00am PDT (1pm EDT)<em>  Shachar Lovett (UC San Diego)<a href="https://simons.berkeley.edu/events/boolean-2" target="_blank" rel="noreferrer noopener">Improved Bounds for the Sunflower Lemma</a></em></p>



<p><br />August 5, Wednesday, 10:00am PDT (1pm EDT) <em>Ronen Eldan (Weizmann Institute)</em><a href="https://simons.berkeley.edu/events/boolean-4" target="_blank" rel="noreferrer noopener"><em>Concentration on the Boolean Hypercube via Pathwise Stochastic Analysis</em></a></p>



<p><br />August 12, Wednesday, 10:00am <em>Esty Kelman (Tel Aviv University) <a href="https://simons.berkeley.edu/events/boolean-5" target="_blank" rel="noreferrer noopener">KKL via Random Restrictions</a></em></p>



<p><br />August 18, Tuesday, 10:00am <em>Pooya Hatami (Ohio State University)<a href="https://simons.berkeley.edu/events/boolean-6" target="_blank" rel="noreferrer noopener">Pseudorandom Generators from Polarizing Random Walks</a></em></p></div>







<p class="date">
by Boaz Barak <a href="https://windowsontheory.org/2020/07/13/simons-institute-lectures-on-analysis-of-boolean-functions/"><span class="datestr">at July 13, 2020 04:18 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://agtb.wordpress.com/?p=3507">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/agtb.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://agtb.wordpress.com/2020/07/13/adfocs-2020-market-design-and-computational-fair-division/">ADFOCS 2020 (Market Design and Computational Fair Division)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Via Pieter Kleer:</p>
<hr />
<p>21st Max Planck Summer School:<br />
Advanced Course on the Foundations of Computer Science (ADFOCS 2020)</p>
<p>August 24 – 28, 2020</p>
<p>Saarbruecken, Germany</p>
<p>THIS IS A VIRTUAL EVENT<a href="http://www.mpi-inf.mpg.de/conference/adfocs" target="_blank" rel="noopener noreferrer"></a></p><a href="http://www.mpi-inf.mpg.de/conference/adfocs" target="_blank" rel="noopener noreferrer">
</a><p><a href="http://www.mpi-inf.mpg.de/conference/adfocs" target="_blank" rel="noopener noreferrer">http://www.mpi-inf.mpg.de/conference/adfocs</a><br />
—————————————————————————————————</p>
<p><b>About ADFOCS</b><br />
ADFOCS is an international summer school that has been held annually for the last twenty years at the Max Planck Institute for Informatics (MPII) in Saarbruecken, Germany. It is organized as part of the activities of the MPII, in particular the International Max Planck Research School (IMPRS), MPII’s graduate program. The purpose of this summer school is to introduce young researchers to topics which are the focus of current research in theoretical computer science. We bring together leading researchers in the field and international participants at the graduate level and above. This year’s focus is on:</p>
<p><b>*** Market Design and Computational Fair Division ***</b><br />
<b>Program</b><br />
Our invited speakers give five 60-min lectures with subsequent exercise and discussion sessions. These sessions will take place daily from 14:30 to 18:30 UTC+2 (CEST) in the week of August 24-28. On some days there will be a social event after the regular schedule. This year’s speakers are:</p>
<p>* Nicole Immorlica, Microsoft Research Lab, New York City, USA<br />
* Jugal Garg and Ruta Mehta, University of Illinois at Urbana-Champaign, USA<br />
<b>Registration</b><br />
This year registration is free as the event takes place virtually. Nevertheless, registration is MANDATORY and can be done through the website (at the latest August 10)</p>
<p><b>Contact</b><br />
The homepage of ADFOCS, including forms for registration, can be found at <a href="http://www.mpi-inf.mpg.de/conference/adfocs" target="_blank" rel="noopener noreferrer">http://www.mpi-inf.mpg.de/conference/adfocs</a></p>
<p>If you have further questions, please do not hesitate to contact the ADFOCS team by sending an email to <a href="mailto:adfocs@mpi-inf.mpg.de" target="_blank" rel="noopener">adfocs@mpi-inf.mpg.de</a></p>
<p>Organizers: Cosmina Croitoru, Sandor Kisfaludi-Bak and Pieter Kleer</p></div>







<p class="date">
by timroughgarden <a href="https://agtb.wordpress.com/2020/07/13/adfocs-2020-market-design-and-computational-fair-division/"><span class="datestr">at July 13, 2020 01:25 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-127536327837647663">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2020/07/ronald-graham-summary-of-blog-posts-we.html">Ronald Graham: A summary of blog Posts We had about his work</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
To Honor Ronald Graham I summarize the blog posts we had about his work.<br />
<br />
1) Blog post <a href="https://blog.computationalcomplexity.org/2016/05/new-ramsey-result-that-will-be-hard-to.html">New Ramsey Result that will be hard to verify but Ronald Graham thinks its right which is good enough for me</a>.<br />
<br />
Wikipedia (see <a href="https://en.wikipedia.org/wiki/Boolean_Pythagorean_triples_problem">here</a>) says that in the early 1980's (can't Wikipedia be more precise than that?) Ronald Graham conjectured the following:<br />
<br />
For all 2-colorings of N, there exists x,y,z all the same color such that (x,y,z) form a Pythagorean triple.<br />
<br />
I cannot imagine he did not also conjecture this to be true for all finite colorings.<br />
<br />
I suspect that when he conjectured it, the outcomes thought to be likely were:<br />
<br />
a) A purely combinatorial (my spell check says that combinatorial  is not a word. Really? It gets 14,000,000 hits) proof. Perhaps a difficult one. (I think Szemeredi's proof of his density theorem is a rather difficult but purely combinatorial proof).<br />
<br />
b) A proof that uses advanced mathematics, like Roth's proof of the k=3 case of Sz-density, or Furstenberg's proof of Sz theorem.<br />
<br />
c) The question stays open though with some progress over the years, like R(5).<br />
<br />
What actually happened was<br />
<br />
d) A SAT Solver solves it AND gets exact bounds:<br />
<br />
For all 2-colorings of {1,...,7285} there is a mono Pythag triple.<br />
<br />
There exists a 2-coloring of {1,...,7284} with no mono Pythag triple.<br />
<br />
I wonder if this would have been guessed as the outcome back in the early 1980's.<br />
<br />
-------------------------------------------------------------------------------<br />
2) Blog Post <a href="https://blog.computationalcomplexity.org/2019/05/ronald-grahams-other-large-number-well.html">Ronald Graham's Other Large Number- well it was large in 1964 anyway</a><br />
<br />
Let<br />
<br />
a(n) = a(n-1) + a(n-2)<br />
<br />
I have not given a(0) and a(1). Does there exists rel prime values of a(0) and a(1) such that for all n, a(n) is composite.<br />
<br />
In 1964 Ronald Graham showed yes, though the numbers he found (with the help of 1964-style computing) were<br />
<br />
a(0) = 1786772701928802632268715130455793<br />
<br />
a(1) = 2059683225053915111058164141686995<br />
<br />
I suspect it is open to get smaller numbers, though I do not know.<br />
<br />
<br />
------------------------------------------------------------------------------<br />
3) Blog Post <a href="https://blog.computationalcomplexity.org/2011/12/solution-to-reciprocals-problem.html">Solution to the reciprocals problem</a><br />
<br />
Prove or disprove that there exists 10 natural numbers a,...,j such that<br />
<br />
2011= a+ ... + j<br />
1 = 1/a + ... + 1/j<br />
<br />
I had pondered putting this on a HS math competition in 2011; however, the committee thought it was too hard. I blogged on the problem asking for solutions, seeing if there was one that a HS student could have gotten. The following post (this one) gave those solutions. My conclusion is that it could have been put on the competition, but its a close call.<br />
<br />
All of the answers submitted had some number repeated.<br />
<br />
So I wondered if there was a way to do this with distinct a,...,j.<br />
<br />
 I was told about Ronald Grahams result:<br />
<br />
For all n at least 78, n can be written as the sum of DISTINCT naturals, where the sum of<br />
the reciprocals is 1.<br />
<br />
This is tight: 77 cannot be so written.<br />
<br />
Comment on that blog DID include solutions  to my original problem with all distinct numbers<br />
<br />
----------------------------------------------------------------------<br />
4) Blog Post <a href="https://blog.computationalcomplexity.org/2013/04/a-nice-case-of-interdisciplinary.html">A nice case of interdisplanary research</a> tells the story of how the study of history lead to R(5) being determined (see <a href="http://www.cs.umd.edu/~gasarch/BLOGPAPERS/ramseykings.pdf">here</a> for the actual paper on the subject). One of the main players in the story is the mathematician<br />
<br />
Alma Grand-Rho.<br />
<br />
Note that this is an anagram of<br />
<br />
Ronald Graham.<br />
<br />
What is the probability that two mathematicians have names that are anagrams. I suspect very small. However, see <a href="https://blog.computationalcomplexity.org/2013/04/post-mortem-on-april-fools-day-joke.html">this</a> blog post to see why the probability is not as small as it might be.<br />
<br />
-----------------------------------------------------------------------<br />
5) Blog Post <a href="https://blog.computationalcomplexity.org/search?q=Meme">Winner of Ramsey Meme Contest</a> This post didn't mention Ronald Graham; however I think he would have liked it.<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br />
<br /></div>







<p class="date">
by gasarch (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2020/07/ronald-graham-summary-of-blog-posts-we.html"><span class="datestr">at July 12, 2020 08:18 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2020/07/12/graham-pollak-partitions">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2020/07/12/graham-pollak-partitions.html">Graham–Pollak partitions</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p>As you’ve probably already seen, Ron Graham recently died. I first met him many years ago at Xerox PARC; what I remember from that meeting is this old guy easily beating me at ping-pong, and I was startled to learn (while working to beef up <a href="https://en.wikipedia.org/wiki/Ronald_Graham">his Wikipedia article</a> after his death) that that was exactly Graham’s first impression of Paul Erdős. We’ve chatted about research, most recently in <a href="https://www.ics.uci.edu/~eppstein/pix/bellairs18/index.html">2018 in Barbados</a>, but somehow never published anything together; on the other hand, Graham’s work in computational geometry, Ramsey theory, and approximation algorithms has certainly had a strong influence on me. Anyway, as part of the project of improving his Wikipedia article, I put together a separate new article on <a href="https://en.wikipedia.org/wiki/Graham%E2%80%93Pollak_theorem">the Graham–Pollak theorem</a>, the theorem that partitioning the edges of an -vertex complete graph into complete bipartite subgraphs requires at least  subgraphs. And while doing that, I started to wonder about what the optimal partitions look like, and how many there are.</p>

<p>In <em>Proofs from THE BOOK</em>, Aigner and Ziegler describe a simple construction for an -subgraph partition: just order the vertices of the complete graph, and make a star connecting each vertex (except the last) to its later neighbors.
But there are a lot more partitions than that. For instance, you can take any rooted binary tree whose leaves are the vertices of the complete graph, and form a partition in which each complete bipartite subgraph connects the left and right descendants of one of the interior nodes of the tree. The ordered star partition is the special case of this where each internal node has one leaf child.</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/graham-pollak-hierarchy.svg" alt="Graham–Pollak partitions from binary trees" /></p>

<p>Even these are not the only possibilities. For instance, a four-vertex complete graph can be partitioned into  subgraphs in this triskelion pattern:</p>

<p style="text-align: center;"><img src="https://11011110.github.io/blog/assets/2020/graham-pollak-triskelion.svg" alt="Graham–Pollak partitions from binary trees" /></p>

<p>More generally, whenever one has a partition of , one can form a partition of a larger complete graph by partitioning its vertices into  subsets, applying the partition of  to the edges that go from one subset to another, and then recursively partitioning the edges within each subset. This is already enough to show that there is a rapidly growing number of these partitions, but not enough to count them more precisely.</p>

<p>This still leaves many questions. How many Graham–Pollak partitions does  have, as a function of ? How complicated can they be? If we define a state space whose states are Graham–Pollak partitions, and whose state transitions correspond to re-partitioning the subgraph formed by two of the complete bipartite graphs, is it connected? Can a graph traversal of this state space list all the Graham–Pollak partitions faster than a brute force search? What does a random partition look like?</p>

<p>It’s too bad Ron’s no longer around to help answer some of them.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/104503441875881282">Discuss on Mastodon</a>)</p></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2020/07/12/graham-pollak-partitions.html"><span class="datestr">at July 12, 2020 03:46 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/104">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/104">TR20-104 |  On Counting $t$-Cliques Mod 2 | 

	Oded Goldreich</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
For a constant integer $t$, we consider the problem of counting the number of $t$-cliques $\bmod 2$ in a given graph. 
We show that this problem is not easier than determining whether a given graph contains a $t$-clique, and present a simple worst-case to average-case reduction for it. The reduction runs in linear time when graphs are presented by their adjacency matrices, and average-case is with respect to the uniform distribution over graphs with a given number of vertices.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/104"><span class="datestr">at July 12, 2020 03:14 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2020/103">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2020/103">TR20-103 |  One-Tape Turing Machine and Branching Program Lower Bounds for MCSP | 

	Mahdi Cheraghchi, 

	Shuichi Hirahara, 

	Dimitrios Myrisiotis, 

	Yuichi Yoshida</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
For a size parameter $s\colon\mathbb{N}\to\mathbb{N}$, the Minimum Circuit Size Problem (denoted by ${\rm MCSP}[s(n)]$) is the problem of deciding whether the minimum circuit size of a given function $f \colon \{0,1\}^n \to \{0,1\}$ (represented by a string of length $N := 2^n$) is at most a threshold $s(n)$. A recent line of work exhibited ``hardness magnification'' phenomena for MCSP: A very weak lower bound for MCSP implies a breakthrough result in complexity theory. For example, McKay, Murray, and Williams (STOC 2019) implicitly showed that, for some constant $\mu_1 &gt; 0$, if ${\rm MCSP}[2^{\mu_1\cdot n}]$ cannot be computed by a one-tape Turing machine (with an additional one-way read-only input tape) running in time $N^{1.01}$, then ${\rm P}\neq{\rm NP}$.
    
    In this paper, we present the following new lower bounds against one-tape Turing machines and branching programs:
    \begin{enumerate}
        \item  A randomized two-sided error one-tape Turing machine (with an additional one-way read-only input tape) cannot compute ${\rm MCSP}[2^{\mu_2\cdot n}]$ in time $N^{1.99}$, for some constant $\mu_2 &gt; \mu_1$.  
        \item A non-deterministic (or parity) branching program of size $o(N^{1.5}/\log N)$ cannot compute MKTP, which is a time-bounded Kolmogorov complexity analogue of MCSP. This is shown by directly applying the Nechiporuk method to MKTP, which previously appeared to be difficult.
    \end{enumerate}
    These results are the first non-trivial lower bounds for MCSP and MKTP against one-tape Turing machines and non-deterministic branching programs, and essentially match the best-known lower bounds for any explicit functions against these computational models.
    
    The first result is based on recent constructions of pseudorandom generators for read-once oblivious branching programs (ROBPs) and combinatorial rectangles (Forbes and Kelley, FOCS 2018; Viola 2019). En route, we obtain several related results:
    \begin{enumerate}
        \item There exists a (local) hitting set generator with seed length $\widetilde{O}(\sqrt{N})$ secure against read-once polynomial-size non-deterministic branching programs on $N$-bit inputs.
        \item Any read-once co-non-deterministic branching program computing MCSP must have size at least $2^{\widetilde{\Omega}(N)}$.
    \end{enumerate}</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2020/103"><span class="datestr">at July 11, 2020 05:31 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2020/07/11/2020-virtual-telluride-neuromorphic-cognition-engineering-workshop/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2020/07/11/2020-virtual-telluride-neuromorphic-cognition-engineering-workshop/">2020 Virtual Telluride Neuromorphic Cognition Engineering Workshop</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
July 27-31, 2020 Telluride CO (virtual) https://sites.google.com/view/telluride2020/home We are happy to announce a Virtual Telluride Neuromorphic Cognition Engineering Workshop 2020 (https://tellurideneuromorphic.org/) this year in replacement of our usual Workshop in Telluride. The workshop will take place from July 27 to July 31 (8am to 10am PDT, or 17:00 to 19:00 CET). The format will be … <a href="https://cstheory-events.org/2020/07/11/2020-virtual-telluride-neuromorphic-cognition-engineering-workshop/" class="more-link">Continue reading <span class="screen-reader-text">2020 Virtual Telluride Neuromorphic Cognition Engineering Workshop</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2020/07/11/2020-virtual-telluride-neuromorphic-cognition-engineering-workshop/"><span class="datestr">at July 11, 2020 05:05 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
