<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>











<head>
<title>Theory of Computing Blog Aggregator</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="generator" content="http://intertwingly.net/code/venus/">
<link rel="stylesheet" href="css/twocolumn.css" type="text/css" media="screen">
<link rel="stylesheet" href="css/singlecolumn.css" type="text/css" media="handheld, print">
<link rel="stylesheet" href="css/main.css" type="text/css" media="@all">
<link rel="icon" href="images/feed-icon.png">
<script type="text/javascript" src="library/MochiKit.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/javascript" src="js/main.js"></script> 
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": { availableFonts: [],
      webFont: 'TeX' }
});
</script>
 <script type="text/javascript" 
	 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML-full">
 </script>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-2793256-2");
pageTracker._trackPageview();
</script>
<link rel="alternate" href="http://www.cstheory-feed.org/atom.xml" title="" type="application/atom+xml">
</head>

<body onload="onLoadCb()">
<div class="sidebar">
<div style="background-color:#feb; border:1px solid #dc9; padding:10px; margin-top:23px; margin-bottom: 20px">
Stay up to date
</ul>
<li>Subscribe to the <A href="atom.xml">RSS feed</a></li>
<li>Follow <a href="http://twitter.com/cstheory">@cstheory</a> on Twitter</li>
</ul>
</div>

<h3>Blogs/feeds</h3>
<div class="subscriptionlist">
<a class="feedlink" href="http://aaronsadventures.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://aaronsadventures.blogspot.com/" title="Adventures in Computation">Aaron Roth</a>
<br>
<a class="feedlink" href="https://adamsheffer.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<br>
<a class="feedlink" href="https://adamdsmith.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://adamdsmith.wordpress.com" title="Oddly Shaped Pegs">Adam Smith</a>
<br>
<a class="feedlink" href="https://polylogblog.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://polylogblog.wordpress.com" title="the polylogblog">Andrew McGregor</a>
<br>
<a class="feedlink" href="http://corner.mimuw.edu.pl/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<br>
<a class="feedlink" href="http://www.argmin.net/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://benjamin-recht.github.io/" title="arg min blog">Ben Recht</a>
<br>
<a class="feedlink" href="https://cstheory-jobs.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<br>
<a class="feedlink" href="https://cstheory-events.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">CS Theory StackExchange (Q&A)</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="404: not found">Center for Computational Intractability</a>
<br>
<a class="feedlink" href="https://blog.computationalcomplexity.org/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<br>
<a class="feedlink" href="https://11011110.github.io/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<br>
<a class="feedlink" href="https://daveagp.wordpress.com/category/toc/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://daveagp.wordpress.com" title="toc – QED and NOM">David Pritchard</a>
<br>
<a class="feedlink" href="https://decentdescent.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentdescent.org/" title="Decent Descent">Decent Descent</a>
<br>
<a class="feedlink" href="https://eccc.weizmann.ac.il//feeds/reports/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<br>
<a class="feedlink" href="https://minimizingregret.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://minimizingregret.wordpress.com" title="Minimizing Regret">Elad Hazan</a>
<br>
<a class="feedlink" href="https://emanueleviola.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://emanueleviola.wordpress.com" title="Thoughts">Emanuele Viola</a>
<br>
<a class="feedlink" href="https://3dpancakes.typepad.com/ernie/atom.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://3dpancakes.typepad.com/ernie/" title="Ernie's 3D Pancakes">Ernie's 3D Pancakes</a>
<br>
<a class="feedlink" href="https://gilkalai.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<br>
<a class="feedlink" href="http://blogs.oregonstate.edu/glencora/tag/tcs/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blogs.oregonstate.edu/glencora" title="tcs – Glencora Borradaile">Glencora Borradaile</a>
<br>
<a class="feedlink" href="https://research.googleblog.com/feeds/posts/default/-/Algorithms" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://research.googleblog.com/search/label/Algorithms" title="Research Blog">Google Research Blog: Algorithms</a>
<br>
<a class="feedlink" href="https://gradientscience.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://gradientscience.org/" title="gradient science">Gradient Science</a>
<br>
<a class="feedlink" href="http://grigory.us/blog/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://grigory.github.io/blog" title="The Big Data Theory">Grigory Yaroslavtsev</a>
<br>
<a class="feedlink" href="https://blog.ilyaraz.org/rss/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.ilyaraz.org/" title="Lullaby of Cape Cod">Ilya Razenshteyn</a>
<br>
<a class="feedlink" href="https://decentralizedthoughts.github.io/feed" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Ittai Abraham</a>
<br>
<a class="feedlink" href="https://tcsmath.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsmath.wordpress.com" title="tcs math">James R. Lee</a>
<br>
<a class="feedlink" href="https://kamathematics.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://kamathematics.wordpress.com" title="Kamathematics">Kamathematics</a>
<br>
<a class="feedlink" href="" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a class="message" title="internal server error">Learning with Errors: Student Theory Blog</a>
<br>
<a class="feedlink" href="http://processalgebra.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<br>
<a class="feedlink" href="https://lucatrevisan.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://lucatrevisan.wordpress.com" title="in   theory">Luca Trevisan</a>
<br>
<a class="feedlink" href="https://mittheory.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mittheory.wordpress.com" title="Not so Great Ideas in Theoretical Computer Science">MIT CSAIL student blog</a>
<br>
<a class="feedlink" href="http://mybiasedcoin.blogspot.com/feeds/posts/default" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mybiasedcoin.blogspot.com/" title="My Biased Coin">Michael Mitzenmacher</a>
<br>
<a class="feedlink" href="http://blog.mrtz.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.mrtz.org/" title="Moody Rd">Moritz Hardt</a>
<br>
<a class="feedlink" href="http://mysliceofpizza.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://mysliceofpizza.blogspot.com/search/label/aggregator" title="my slice of pizza">Muthu Muthukrishnan</a>
<br>
<a class="feedlink" href="https://nisheethvishnoi.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://nisheethvishnoi.wordpress.com" title="Algorithms, Nature, and Society">Nisheeth Vishnoi</a>
<br>
<a class="feedlink" href="http://www.solipsistslog.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.solipsistslog.com" title="Solipsist's Log">Noah Stephens-Davidowitz</a>
<br>
<a class="feedlink" href="http://www.offconvex.org/feed.xml" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://offconvex.github.io/" title="Off the convex path">Off the Convex Path</a>
<br>
<a class="feedlink" href="http://paulwgoldberg.blogspot.com/feeds/posts/default/-/aggregator" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://paulwgoldberg.blogspot.com/search/label/aggregator" title="Paul Goldberg">Paul Goldberg</a>
<br>
<a class="feedlink" href="https://ptreview.sublinear.info/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://ptreview.sublinear.info" title="Property Testing Review">Property Testing Review</a>
<br>
<a class="feedlink" href="https://rjlipton.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<br>
<a class="feedlink" href="http://www.contrib.andrew.cmu.edu/~ryanod/index.php?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://www.contrib.andrew.cmu.edu/~ryanod" title="Analysis of Boolean Functions">Ryan O'Donnell</a>
<br>
<a class="feedlink" href="https://blogs.princeton.edu/imabandit/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blogs.princeton.edu/imabandit" title="I’m a bandit">S&eacute;bastien Bubeck</a>
<br>
<a class="feedlink" href="https://sarielhp.org/blog/?feed=rss2" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://sarielhp.org/blog" title="Vanity of Vanities, all is Vanity">Sariel Har-Peled</a>
<br>
<a class="feedlink" href="https://www.scottaaronson.com/blog/?feed=atom" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<br>
<a class="feedlink" href="https://blog.simons.berkeley.edu/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://blog.simons.berkeley.edu" title="Calvin Café: The Simons Institute Blog">Simons Institute blog</a>
<br>
<a class="feedlink" href="https://tcsplus.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<br>
<a class="feedlink" href="http://feeds.feedburner.com/TheGeomblog" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://blog.geomblog.org/" title="The Geomblog">The Geomblog</a>
<br>
<a class="feedlink" href="https://theorydish.blog/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://theorydish.blog" title="Theory Dish">Theory Dish: Stanford blog</a>
<br>
<a class="feedlink" href="https://thmatters.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://thmatters.wordpress.com" title="Theory Matters">Theory Matters</a>
<br>
<a class="feedlink" href="https://mycqstate.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://mycqstate.wordpress.com" title="MyCQstate">Thomas Vidick</a>
<br>
<a class="feedlink" href="https://agtb.wordpress.com/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://agtb.wordpress.com" title="Turing's Invisible Hand">Turing's invisible hand</a>
<br>
<a class="feedlink" href="https://windowsontheory.org/feed/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CC" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.CG" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.DS">arXiv.org: Computational geometry</a>
<br>
<a class="feedlink" href="http://export.arxiv.org/rss/cs.DS" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://arxiv.org/" class="message" title="duplicate subscription: http://export.arxiv.org/rss/cs.CC">arXiv.org: Data structures and Algorithms</a>
<br>
<a class="feedlink" href="http://bit-player.org/feed/atom/" title="subscribe"> <img src="images/feed-icon.png" width=20 height=20 alt="(feed)" class="feedicon" border=0></a> 
<a href="http://bit-player.org" title="bit-player">bit-player</a>
<br>
</div>

<p>
Maintained by <A href="https://www.comp.nus.edu.sg/~arnab/">Arnab Bhattacharyya</A>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, &amp; <A href="http://www.cs.utah.edu/~suresh/">Suresh Venkatasubramanian</a> (<a href="mailto:arbhat+cstheoryfeed@gmail.com">email</a>).
</p>

<p>
Last updated <span class="datestr">at December 06, 2019 12:21 AM UTC</span>.
<p>
Powered by<br>
<a href="http://www.intertwingly.net/code/venus/planet/"><img src="images/planet.png" alt="Planet Venus" border="0"></a>
<p/><br/><br/>
</div>
<div class="maincontent">
<h1 align=center>Theory of Computing Blog Aggregator</h1>








<div class="channelgroup">
<div class="entrygroup" 
	id="http://windowsontheory.org/?p=7588">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/wot.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://windowsontheory.org/2019/12/05/deep-double-descent/">Deep Double Descent (cross-posted on OpenAI blog)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://windowsontheory.org" title="Windows On Theory">Windows on Theory</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><em>By Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever</em></p>



<p><em>This is a lightly edited and expanded version of the following post on the <a href="https://openai.com/blog/deep-double-descent/">OpenAI blog</a> about the following <a href="http://mltheory.org/deep.pdf">paper</a>. While I usually don’t advertise my own papers on this blog, I thought this might be of interest to theorists, and a good follow up to <a href="https://windowsontheory.org/2019/11/15/puzzles-of-modern-machine-learning/">my prior post</a>. I promise not to make a habit out of it. –Boaz</em></p>



<p><strong>TL;DR:</strong>   <a href="http://mltheory.org/deep.pdf">Our paper</a> shows that <a href="https://arxiv.org/abs/1812.11118" target="_blank" rel="noreferrer noopener">double descent</a> occurs in conventional modern deep learning settings: visual classification in the presence of label noise (CIFAR 10, CIFAR 100) and machine translation (IWSLT’14 and WMT’14). As we increase the number of parameters in a neural network, initially the test error decreases, then increases, and then, just as the model is able to fit the train set, it undergoes a second descent, again decreasing as the number of parameters increases. This behavior also extends over train epochs, where a single model undergoes double-descent in test error over the course of training. Surprisingly (at least to us!), we show these phenomenon can lead to a regime where “<em><strong>more data hurts</strong></em>”—training a deep network on a larger train set actually performs worse.</p>



<h2>Introduction</h2>



<p>Open a statistics textbook and you are likely to see warnings against the danger of “overfitting”: If you are trying to find a good classifier or regressor for a given set of labeled examples, you would be well-advised to steer clear of having so many parameters in your model that you are able to completely fit the training data, because you risk not generalizing to new data.</p>



<p>The canonical example for this is polynomial regression. Suppose that we get <em>n</em> samples of the form <em>(x, p(x)+noise)</em> where <em>x</em> is a real number and <em>p(x)</em> is a cubic (i.e. degree 3) polynomial. If we try to fit the samples with a degree 1 polynomial—-a linear function, then we would get many points wrong. If we try to fit it with just the right degree, we would get a very good predictor. However, as the degree grows, we get worse till the degree is large enough to fit all the noisy training points, at which point the regressor is terrible, as shown in this figure:</p>



<figure class="wp-block-image"><img src="https://lh4.googleusercontent.com/H4f4ST5B9RnLX1ski6HEI7RBV5gqvk7WGiFR0qf6Savafmep6i08RYlpF5sgtq9oVqQ6ZkuglvCn0PTMQ_uaK3XStJlSskTSM6I52SyCZ91FeAcphq11MKa56wsfnDAG6GuruTT3" alt="" /></figure>



<p>It seems that the higher the degree, the worse things are, but what happens if we go <em>even higher</em>? It seems like a crazy idea—-why would we increase the degree beyond the number of samples? But it corresponds to the practice of having many more  parameters than training samples in modern deep learning. Just like in deep learning, when the degree is larger than the number of samples, there is more than one polynomial that fits the data– but we choose a specific one: the one found running gradient descent.</p>



<p>Here is what happens if we do this for degree 1000, fitting a polynomial using gradient descent (see <a href="https://colab.research.google.com/drive/1oMuUz3_BOENSoaOVOymLoB2mHeYBex8S">this notebook</a>):</p>



<figure class="wp-block-image"><img src="https://lh6.googleusercontent.com/DjQoPWG5gCueu_sAORKtrhNrrWY35OU3y-RXKJx0AZxIofVzZo-psP-JFsr_a4v20pWhH7EFfFPozckotWzvKypEebkPRJmdSa3vuy39ABdp2PqtqMr7dPX1PTDfMT362n4dGzVG" alt="" /></figure>



<p>We still fit all the training points, but now we do so in a more controlled way which actually tracks quite closely the ground truth. We see that despite what we learn in statistics textbooks, sometimes overfitting is not that bad, as long as you go “all in” rather than “barely overfitting” the data. That is, overfitting doesn’t hurt us if we take the number of parameters to be much larger than what is needed to just fit the training set — and in fact, as we see in deep learning, larger models are often better.</p>



<p>The above is not a novel observation. <a href="https://arxiv.org/abs/1812.11118">Belkin et al </a>called this phenomenon <strong><em>“double descent”</em></strong> and this goes back to <a href="http://www.ki.tu-berlin.de/fileadmin/fg135/publikationen/opper/Op01.pdf">even</a> <a href="https://arxiv.org/abs/1710.03667">earlier</a> <a href="https://arxiv.org/abs/1809.09349">works</a><a href="http://www.ki.tu-berlin.de/fileadmin/fg135/publikationen/opper/Op03b.pdf"> </a>. In this <a href="http://mltheory.org/deep.pdf">new paper</a> we (Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever) extend the prior works and report on a variety of experiments showing that “double descent” is widely prevalent across several modern deep neural networks and for several natural tasks such as image recognition (for the CIFAR 10 and CIFAR 100 datasets) and language translation (for IWSLT’14 and WMT’14 datasets).  As we increase the number of parameters in a neural network, initially the test error decreases, then increases, and then, just as the model is able to fit the train set, it undergoes a <em>second descent,</em> again decreasing as the number of parameters increases.  Moreover, double descent also extends beyond number of parameters to other measures of “complexity” such as the number of training epochs of the algorithm. </p>



<p>The take-away from our work (and the prior works it builds on) is that neither the classical statisticians’ conventional wisdom that <strong><em>“too large models are worse”</em></strong> nor the modern ML paradigm that <strong><em>“bigger models are always better”</em></strong><em> </em>always hold. Rather it all depends on whether you are on the first or second descent.  Further more, these insights also allow us to generate natural settings in which even the age-old adage of <strong><em>“more data is always better”</em></strong> is violated!</p>



<p>In the rest of this blog post we present a few sample results from this recent paper.</p>



<h3 id="modelwisedoubledescent">Model-wise Double Descent</h3>



<p>We observed many cases in which, just like in the polynomial interpolation example above, the test error undergoes a “double descent” as we increase the complexity of the model. The figure below demonstrates one such example: we plot the test error as a function of the complexity of the model for ResNet18 networks. The complexity of the model is the width of the layers, and the dataset is CIFAR10 with 15% label noise. Notice that the peak in test error occurs around the “interpolation threshold”: when the models are just barely large enough to fit the train set. In all cases we’ve observed, changes which affect the interpolation threshold (such as changing the optimization algorithm, changing the number of train samples, or varying the amount of label noise) also affect the location of the test error peak correspondingly. </p>



<p>We found the double descent phenomena is most prominent in settings with added label noise— without it, the peak is much smaller and easy to miss. But adding label noise amplifies this general behavior and allows us to investigate it easily.</p>



<figure class="wp-block-image"><img src="https://lh4.googleusercontent.com/tFWTvNkFwG8Ljx8zp9X3Ul6aUZT9YmkwcNk07g6_EFUklZoBUd9MqApBEojLzOp9N7yndveLwg5A7uj_vxpGVofQ2QCe-bbMAguQB38cK4NhRfXBp-SWMDQUt9x44r6d_fMur7NO" alt="" /></figure>



<p></p>



<h3 id="samplewisenonmonotonicity">Sample-Wise Nonmonotonicity</h3>



<p>Using the model-wise double descent phenomenon we can obtain examples where training on <strong>more data actually hurts</strong>. To see this, let’s look at the effect of increasing the number of train samples on the test error vs. model size graph. The below plot shows Transformers trained on a language-translation task (with no added label noise):</p>



<figure class="wp-block-image"><img src="https://lh5.googleusercontent.com/YkuZGjuWGKoCcl1gNbQfaKO-96hX9ShcVZ_Dj0KlKddRZhgpMGtzs427zPuC9QwHOOKgmuV5E0aEKOU3zwhwpGmdiWwDDDBIk7hroJZfRLa7kXSThzwTDZoNSM8I2M9OfWxIW5X_" alt="" /></figure>



<p>On the one hand, (as expected) increasing the number of samples generally shifts the curve downwards towards lower test error. On the other hand, it also shifts the curve to the right: since more samples require larger models to fit, the interpolation threshold (and hence, the peak in test error) shifts to the right. For intermediate model sizes, these two effects combine, and we see that <strong>training on 4.5x more samples actually hurts test performance.</strong></p>



<h3 id="epochwisedoubledescent">Epoch-Wise Double Descent</h3>



<p><strong>There is a regime where training longer reverses overfitting.</strong> Let’s look closer at the experiment of Figure 1, and plot Test Error as a function of both model-size and number of optimization steps. In the plot below to the right, each column tracks the Test Error of a given model over the course of training. The top horizontal dotted-line corresponds to the double-descent of the first figure. But we can also see that for a fixed large model, as training proceeds test error goes down, then up and down again—we call this phenomenon “epoch-wise double-descent.”</p>



<figure class="wp-block-image"><img src="https://lh3.googleusercontent.com/SCP6M-txj9ax5g9cR9Ry27X2nF1sEJbpsfC9FiME5umNm-BxcHHBmhseuuWEm3mHMzo_0o5q-92ETcaU0OEXnhTmv_7MpOREaaeIMs7zbL9P5aFeqkXMYh8O8VN4FdASnAu0HOPI" alt="" /></figure>



<p>Moreover, if we plot the Train error of the same models and the corresponding interpolation contour (dotted line) we see that it exactly matches the ridge of high test error (on the right).</p>



<p><strong>In general, the peak of test error appears systematically when models are just barely able to fit the train set.</strong></p>



<p>Our intuition is that for models at the interpolation threshold, there is effectively only one model that fits the train data, and forcing it to fit even slightly-noisy or mis-specified labels will destroy its global structure. That is, there are no “good models”, which both interpolate the train set, and perform well on the test set. However in the over-parameterized regime, there are many models that fit the train set, and there exist “good models” which both interpolate the train set and perform well on the distribution. Moreover, the implicit bias of SGD leads it to such “good” models, for reasons we don’t yet understand.</p>



<p>The above intuition is theoretically justified for linear models, via a series of recent works including [<a href="https://arxiv.org/abs/1903.08560">Hastie et al.</a>] and [<a href="https://arxiv.org/abs/1908.05355">Mei-Montanari</a>]. We leave fully understanding the mechanisms behind double descent in deep neural networks as an important open question.</p>



<p></p>



<hr class="wp-block-separator" />



<h2>Commentary: Experiments for Theory</h2>



<p>The experiments above are especially interesting (in our opinion) because of how they can inform ML theory: any theory of ML must be consistent with “double descent.” In particular, one ambitious hope for what it means to “theoretically explain ML” is to prove a theorem of the form:</p>



<p class="has-text-align-center">“If the distribution satisfies property X and architecture/initialization satisfies property Y, then SGD trained on ‘n’ samples, for T steps, will have small test error with high probability”</p>



<p>For values of X, Y, n, T, “small” and “high” that are used in practice.</p>



<p>However, these experiments show that these properties are likely more subtle than we may have hoped for, and must be non-monotonic in certain natural parameters.</p>



<p>This rules out even certain natural “conditional conjectures” that we may have hoped for, for example the conjecture that</p>



<p class="has-text-align-center">“If SGD on a width W network works for learning from ‘n’ samples from distribution D, then SGD on a width W+1 network will work at least as well”</p>



<p>Or the conjecture</p>



<p class="has-text-align-center">“If SGD on a certain network and distribution works for learning with ‘n’ samples, then it will work at least as well with n+1 samples”</p>



<p>It also appears to conflict with a “2-phase” view of the trajectory of SGD, as an initial “learning phase” and then an “overfitting phase” — in particular, because the overfitting is sometimes reversed (at least, as measured by test error) by further training.</p>



<p>Finally, the fact that these phenomena are not specific to neural networks, but appear to hold fairly universally for natural learning methods (linear/kernel regression, decision trees, random features) gives us hope that there is a deeper phenomenon at work, and we are yet to find the right abstraction.</p>



<p></p>



<p></p>



<p></p>



<p><em>We especially thank Mikhail Belkin and Christopher Olah for helpful discussions throughout this work.</em> <em>The polynomial example is inspired in part by experiments in [<a href="https://arxiv.org/abs/1903.09139">Muthukumar et al.</a>]</em>.</p>



<p></p></div>







<p class="date">
by preetum <a href="https://windowsontheory.org/2019/12/05/deep-double-descent/"><span class="datestr">at December 05, 2019 05:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/12/05/postdoctoral-fellow-at-the-university-of-texas-at-austin-apply-by-january-15-2020-3/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/12/05/postdoctoral-fellow-at-the-university-of-texas-at-austin-apply-by-january-15-2020-3/">Postdoctoral Fellow at The University of Texas at Austin (apply by January 15, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Computer Science Department at UT Austin invites applications for a Postdoctoral Fellow in theoretical computer science for the 2020-21 academic year. The Fellow will work with Dana Moshkovitz and David Zuckerman on pseudorandomness and computational complexity. Review of applicants will begin on January 15, but applications will be accepted until the position is filled.</p>
<p>Website: <a href="https://utaustin.wd1.myworkdayjobs.com/UTstaff/job/UT-MAIN-CAMPUS/Postdoctoral-Fellow_R_00006957">https://utaustin.wd1.myworkdayjobs.com/UTstaff/job/UT-MAIN-CAMPUS/Postdoctoral-Fellow_R_00006957</a><br />
Email: maguilar@cs.utexas.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/12/05/postdoctoral-fellow-at-the-university-of-texas-at-austin-apply-by-january-15-2020-3/"><span class="datestr">at December 05, 2019 02:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/12/05/postdoc-at-university-of-edinburgh-apply-by-december-9-2019-2/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/12/05/postdoc-at-university-of-edinburgh-apply-by-december-9-2019-2/">postdoc at University of Edinburgh (apply by December 9, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Applications are invited for a postdoctoral position in algorithms in the School of Informatics at the University of Edinburgh. The position is for a period of up to 3 years, and can start at any time in 2020.</p>
<p>Website: <a href="https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.jobspec?p_id=050307">https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.jobspec?p_id=050307</a><br />
Email: h.sun@ed.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/12/05/postdoc-at-university-of-edinburgh-apply-by-december-9-2019-2/"><span class="datestr">at December 05, 2019 10:53 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/12/05/postdoc-at-university-of-edinburgh-apply-by-december-9-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/12/05/postdoc-at-university-of-edinburgh-apply-by-december-9-2019/">postdoc at University of Edinburgh (apply by December 9, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Applications are invited for a postdoctoral position in algorithms in the School of Informatics at the University of Edinburgh. The position is for a period of up to 3 years, and can start at any time in 2020.</p>
<p>Website: <a href="https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.display_form">https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.display_form</a><br />
Email: h.sun@ed.ac.uk</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/12/05/postdoc-at-university-of-edinburgh-apply-by-december-9-2019/"><span class="datestr">at December 05, 2019 10:49 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/176">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/176">TR19-176 |  On Prover-Efficient Public-Coin Emulation of Interactive Proofs | 

	Gal Arnon, 

	Guy Rothblum</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
A central question in the study of interactive proofs is the relationship between private-coin proofs, where the verifier is allowed to hide its randomness from the prover, and public-coin proofs, where the verifier's random coins are sent to the prover. 
	
	In this work, we study transformations from private-coin proofs to public-coin proofs, which preserve (up to polynomial factors) the running time of both the prover and the verifier. We re-consider this question in light of the emergence of doubly-efficient interactive proofs, where the honest prover is required to run in polynomial time. Can every private-coin doubly-efficient interactive proof be transformed into a public-coin doubly-efficient proof? We show that, assuming one-way functions exist, there is no black-box private-coin to public-coin transformation for doubly-efficient interactive proofs.
	
	Our main result is a (loose) converse: every doubly-efficient proof has a function such that if this function is invertible, then the proof can be efficiently transformed. 
	
	Turning our attention back to classic interactive proofs, where the honest prover need not run in polynomial time, we show a similar result for constant-round general interactive proofs. Finally, we demonstrate a condition that suffices for efficiency preserving emulation of any protocol (even if the number of rounds is polynomial and the honest prover is unbounded). For a given interactive proof, if for every transcript prefix it is possible to efficiently approximate the number of consistent verifier random coins, then there is an efficiency-preserving transformation.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/176"><span class="datestr">at December 05, 2019 01:56 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1912.02176">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1912.02176">Quantum Query Complexity of Dyck Languages with Bounded Height</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khadiev:Kamil.html">Kamil Khadiev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shen:Yixin.html">Yixin Shen</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1912.02176">PDF</a><br /><b>Abstract: </b>We consider the problem of determining if a sequence of parentheses is well
parenthesized, with a depth of at most h. We denote this language as $Dyck_h$.
We study the quantum query complexity of this problem for different h as
function of the length n of the word. It has been known from a recent paper by
Aaronson et al. that, for any constant h, since $Dyck_h$ is star-free, it has
quantum query complexity $\tilde{\Theta}(\sqrt{n})$, where the hidden logarithm
factors in $\tilde{\Theta}$ depend on h. Their proof does not give rise to an
algorithm. When h is not a constant, $Dyck_h$ is not even context-free. We give
an algorithm with $O\left(\sqrt{n}\log(n)^{h-1}\right)$ quantum queries for
$Dyck_h$ for all h. This is better than the trival upper bound $n$ when
$h=o(\frac{\log(n)}{\log\log n})$. We also obtain lower bounds: we show that
for every $0&lt;\epsilon\leq 0.37$, there exists $c&gt;0$ such that
$Q(\text{Dyck}_{c\log(n)}(n))=\Omega(n^{1-\epsilon})$. When
$h=\omega(\log(n))$, the quantum query complexity is close to $n$, i.e.
$Q(\text{Dyck}_h(n))=\omega(n^{1-\epsilon})$ for all $\epsilon&gt;0$. Furthermore
when $h=\Omega(n^\epsilon)$ for some $\epsilon&gt;0$,
$Q(\text{Dyck}_{h}(n))=\Theta(n)$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1912.02176"><span class="datestr">at December 05, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1912.02021">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1912.02021">Derandomization and absolute reconstruction for sums of powers of linear forms</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CC updates on arXiv.org">arXiv.org: Computational complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Koiran:Pascal.html">Pascal Koiran</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Skomra:Mateusz.html">Mateusz Skomra</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1912.02021">PDF</a><br /><b>Abstract: </b>We study the decomposition of multivariate polynomials as sums of powers of
linear forms.
</p>
<p>Our main result is an algorithm for the following problem: given a
homogeneous polynomial of degree 3, decide whether it can be written as a sum
of cubes of linearly independent linear forms with complex coefficients.
Compared to previous algorithms for the same problem, the two main novel
features of this algorithm are:
</p>
<p>(i) It is an algebraic algorithm, i.e., it performs only arithmetic
operations and equality tests on the coefficients of the input polynomial. In
particular, it does not make any appeal to polynomial factorization.
</p>
<p>(ii) For an input polynomial with rational coefficients, the algorithm runs
in polynomial time when implemented in the bit model of computation.
</p>
<p>The algorithm relies on methods from linear and multilinear algebra
(symmetric tensor decomposition by simultaneous diagonalization).
</p>
<p>We also give a version of our algorithm for decomposition over the field of
real numbers. In this case, the algorithm performs arithmetic operations and
comparisons on the input coefficients.
</p>
<p>Finally we give several related derandomization results on black box
polynomial identity testing, the minimization of the number of variables in a
polynomial, the computation of Lie algebras and factorization into products of
linear forms.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1912.02021"><span class="datestr">at December 05, 2019 11:20 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1912.01990">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1912.01990">On Computing the Hamiltonian Index of Graphs</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Philip:Geevarghese.html">Geevarghese Philip</a>, Rani M. R., Subashini R <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1912.01990">PDF</a><br /><b>Abstract: </b>The $r$-th iterated line graph $L^{r}(G)$ of a graph $G$ is defined by: (i)
$L^{0}(G) = G$ and (ii) $L^{r}(G) = L(L^{(r- 1)}(G))$ for $r &gt; 0$, where $L(G)$
denotes the line graph of $G$. The Hamiltonian Index $h(G)$ of $G$ is the
smallest $r$ such that $L^{r}(G)$ has a Hamiltonian cycle. Checking if $h(G) =
k$ is NP-hard for any fixed integer $k \geq 0$ even for subcubic graphs $G$. We
study the parameterized complexity of this problem with the parameter
treewidth, $tw(G)$, and show that we can find $h(G)$ in time $O*((1 +
2^{(\omega + 3)})^{tw(G)})$ where $\omega$ is the matrix multiplication
exponent and the $O*$ notation hides polynomial factors in input size.
</p>
<p>The NP-hard Eulerian Steiner Subgraph problem takes as input a graph $G$ and
a specified subset $K$ of terminal vertices of $G$ and asks if $G$ has an
Eulerian (that is: connected, and with all vertices of even degree.) subgraph
$H$ containing all the terminals. A second result (and a key ingredient of our
algorithm for finding $h(G)$) in this work is an algorithm which solves
Eulerian Steiner Subgraph in $O*((1 + 2^{(\omega + 3)})^{tw(G)})$ time.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1912.01990"><span class="datestr">at December 05, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1912.01977">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1912.01977">Proof of Dudley's Convex Approximation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Har=Peled:Sariel.html">Sariel Har-Peled</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jones:Mitchell.html">Mitchell Jones</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1912.01977">PDF</a><br /><b>Abstract: </b>We provide a self contained proof of a result of Dudley [Dud64]} which shows
that a bounded convex-body in $\Re^d$ can be $\varepsilon$-approximated, by the
intersection of $O_d\bigl(\varepsilon^{-(d-1)/2} \bigr)$ halfspaces, where
$O_d$ hides constants that depends on $d$.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1912.01977"><span class="datestr">at December 05, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1912.01934">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1912.01934">A New Paradigm for Identifying Reconciliation-Scenario Altering Mutations Conferring Environmental Adaptation</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zoller:Roni.html">Roni Zoller</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zehavi:Meirav.html">Meirav Zehavi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Ziv=Ukelson:Michal.html">Michal Ziv-Ukelson</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1912.01934">PDF</a><br /><b>Abstract: </b>An important goal in microbial computational genomics is to identify crucial
events in the evolution of a gene that severely alter the duplication, loss and
mobilization patterns of the gene within the genomes in which it disseminates.
In this paper, we formalize this microbiological goal as a new pattern-matching
problem in the domain of Gene tree and Species tree reconciliation, denoted
"Reconciliation-Scenario Altering Mutation (RSAM) Discovery". We propose an
$O(m\cdot n\cdot k)$ time algorithm to solve this new problem, where $m$ and
$n$ are the number of vertices of the input Gene tree and Species tree,
respectively, and $k$ is a user-specified parameter that bounds from above the
number of optimal solutions of interest. The algorithm first constructs a
hypergraph representing the $k$ highest scoring reconciliation scenarios
between the given Gene tree and Species tree, and then interrogates this
hypergraph for subtrees matching a pre-specified RSAM Pattern. Our algorithm is
optimal in the sense that the number of hypernodes in the hypergraph can be
lower bounded by $\Omega(m\cdot n\cdot k)$. We implement the new algorithm as a
tool, called RSAM-finder, and demonstrate its application to -the
identification of RSAMs in toxins and drug resistance elements across a dataset
spanning hundreds of species.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1912.01934"><span class="datestr">at December 05, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1912.01870">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1912.01870">A Fully-Integrated Sensing and Control System for High-Accuracy Mobile Robotic Building Construction</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gawel:Abel.html">Abel Gawel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blum:Hermann.html">Hermann Blum</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pankert:Johannes.html">Johannes Pankert</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kr=auml=mer:Koen.html">Koen Krämer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bartolomei:Luca.html">Luca Bartolomei</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Ercan:Selen.html">Selen Ercan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Farshidian:Farbod.html">Farbod Farshidian</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chli:Margarita.html">Margarita Chli</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gramazio:Fabio.html">Fabio Gramazio</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Siegwart:Roland.html">Roland Siegwart</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hutter:Marco.html">Marco Hutter</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sandy:Timothy.html">Timothy Sandy</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1912.01870">PDF</a><br /><b>Abstract: </b>We present a fully-integrated sensing and control system which enables mobile
manipulator robots to execute building tasks with millimeter-scale accuracy on
building construction sites. The approach leverages multi-modal sensing
capabilities for state estimation, tight integration with digital building
models, and integrated trajectory planning and whole-body motion control. A
novel method for high-accuracy localization updates relative to the known
building structure is proposed. The approach is implemented on a real platform
and tested under realistic construction conditions. We show that the system can
achieve sub-cm end-effector positioning accuracy during fully autonomous
operation using solely on-board sensing.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1912.01870"><span class="datestr">at December 05, 2019 11:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1912.01861">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1912.01861">Mining Top-k Trajectory-Patterns from Anonymized Data</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b>Anuj S. Saxena, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dawar:Siddharth.html">Siddharth Dawar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goyal:Vikram.html">Vikram Goyal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bera:Debajyoti.html">Debajyoti Bera</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1912.01861">PDF</a><br /><b>Abstract: </b>The ubiquity of GPS enabled devices result into the generation of an enormous
amount of user movement data consisting of a sequence of locations together
with activities performed at those locations. Such data, commonly known as {\it
activity-trajectory data}, can be analysed to find common user movements and
preferred activities, which will have tremendous business value. However,
various countries have now introduced data privacy regulations that make it
mandatory to anonymize any data before releasing it. This makes it difficult to
look for patterns as the existing mining techniques may not be directly
applicable over anonymized data. User locations in an activity-trajectory
dataset are anonymized to regions of different shapes and sizes making them
uncomparable; therefore, it is unclear how to define suitable patterns over
those regions. In this paper, we propose a top-k pattern mining technique
called TopKMintra that employs a pre-processing step to transform anonymized
activity-trajectory into an intermediate representation that address the above
problem. Unlike usual sequential data, activity-trajectory data is
2-dimensional that can lead to generation of duplicate patterns. To handle
this, TopKMintra restricts arbitrary extensions in the two dimensions by
imposing an order over the search space; this also helps in addressing the
common problem of updating the threshold in top-k pattern mining algorithms
during various stages. We perform extensive experiments on real datasets to
demonstrate the efficiency and the effectiveness of TopKMintra. Our results
show that even after anonymization, certain patterns may remain in a dataset
and those could be mined efficiently.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1912.01861"><span class="datestr">at December 05, 2019 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1912.01854">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1912.01854">Popular Branchings and Their Dual Certificates</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kavitha:Telikepalli.html">Telikepalli Kavitha</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kir=aacute=ly:Tam=aacute=s.html">Tamás Király</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Matuschke:Jannik.html">Jannik Matuschke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schlotter:Ildik=oacute=.html">Ildikó Schlotter</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schmidt=Kraepelin:Ulrike.html">Ulrike Schmidt-Kraepelin</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1912.01854">PDF</a><br /><b>Abstract: </b>Let $G$ be a digraph where every node has preferences over its incoming
edges. The preferences of a node extend naturally to preferences over
branchings, i.e., directed forests; a branching $B$ is popular if $B$ does not
lose a head-to-head election (where nodes cast votes) against any branching.
Such popular branchings have a natural application in liquid democracy. The
popular branching problem is to decide if $G$ admits a popular branching or
not. We give a characterization of popular branchings in terms of dual
certificates and use this characterization to design an efficient combinatorial
algorithm for the popular branching problem. When preferences are weak
rankings, we use our characterization to formulate the popular branching
polytope in the original space and also show that our algorithm can be modified
to compute a branching with least unpopularity margin. When preferences are
strict rankings, we show that "approximately popular" branchings always exist.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1912.01854"><span class="datestr">at December 05, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1912.01781">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1912.01781">Faster Lattice Enumeration</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Mithilesh.html">Mithilesh Kumar</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1912.01781">PDF</a><br /><b>Abstract: </b>A lattice reduction is an algorithm that transforms the given basis of the
lattice to another lattice basis such that problems like finding a shortest
vector and closest vector become easier to solve. Some of the famous lattice
reduction algorithms are LLL and BKZ reductions. We define a class of bases
called \emph{obtuse bases} and show that any lattice basis can be transformed
to an obtuse basis in $\mathcal{O}(n^4)$ time. A shortest vector s can be
written as $v_1b_1+\cdots+v_nb_n$ where $b_1,\dots,b_n$ are the input basis
vectors and $v_1,\dots,v_n$ are integers. When the input basis is obtuse, all
these integers can be chosen to be positive for a shortest vector. This
property of the obtuse basis makes lattice enumeration algorithm for finding a
shortest vector exponentially faster. Moreover, extreme pruning, the current
fastest algorithm for lattice enumeration, can be run on an obtuse basis.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1912.01781"><span class="datestr">at December 05, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1912.01698">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1912.01698">Online and Bandit Algorithms for Nonstationary Stochastic Saddle-Point Optimization</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roy:Abhishek.html">Abhishek Roy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Yifang.html">Yifang Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Balasubramanian:Krishnakumar.html">Krishnakumar Balasubramanian</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mohapatra:Prasant.html">Prasant Mohapatra</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1912.01698">PDF</a><br /><b>Abstract: </b>Saddle-point optimization problems are an important class of optimization
problems with applications to game theory, multi-agent reinforcement learning
and machine learning. A majority of the rich literature available for
saddle-point optimization has focused on the offline setting. In this paper, we
study nonstationary versions of stochastic, smooth, strongly-convex and
strongly-concave saddle-point optimization problem, in both online (or
first-order) and multi-point bandit (or zeroth-order) settings. We first
propose natural notions of regret for such nonstationary saddle-point
optimization problems. We then analyze extragradient and Frank-Wolfe
algorithms, for the unconstrained and constrained settings respectively, for
the above class of nonstationary saddle-point optimization problems. We
establish sub-linear regret bounds on the proposed notions of regret in both
the online and bandit setting.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1912.01698"><span class="datestr">at December 05, 2019 11:21 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1912.01668">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1912.01668">Learning Multi-dimensional Indexes</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.DS updates on arXiv.org">arXiv.org: Data structures and Algorithms</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nathan:Vikram.html">Vikram Nathan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Ding:Jialin.html">Jialin Ding</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alizadeh:Mohammad.html">Mohammad Alizadeh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kraska:Tim.html">Tim Kraska</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1912.01668">PDF</a><br /><b>Abstract: </b>Scanning and filtering over multi-dimensional tables are key operations in
modern analytical database engines. To optimize the performance of these
operations, databases often create clustered indexes over a single dimension or
multi-dimensional indexes such as R-trees, or use complex sort orders (e.g.,
Z-ordering). However, these schemes are often hard to tune and their
performance is inconsistent across different datasets and queries. In this
paper, we introduce Flood, a multi-dimensional in-memory index that
automatically adapts itself to a particular dataset and workload by jointly
optimizing the index structure and data storage. Flood achieves up to three
orders of magnitude faster performance for range scans with predicates than
state-of-the-art multi-dimensional indexes or sort orders on real-world
datasets and workloads. Our work serves as a building block towards an
end-to-end learned database system.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1912.01668"><span class="datestr">at December 05, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://arxiv.org/abs/1912.01639">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/corr.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="http://arxiv.org/abs/1912.01639">Fast Algorithms for Geometric Consensuses</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://arxiv.org/" title="cs.CG updates on arXiv.org">arXiv.org: Computational geometry</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Har=Peled:Sariel.html">Sariel Har-Peled</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jones:Mitchell.html">Mitchell Jones</a> <br /><b>Download:</b> <a href="http://arxiv.org/pdf/1912.01639">PDF</a><br /><b>Abstract: </b>Let $P$ be a set of $n$ points in $\Re^d$ in general position. A median
hyperplane (roughly) splits the point set $P$ in half. The yolk of $P$ is the
ball of smallest radius intersecting all median hyperplanes of $P$. The egg of
$P$ is the ball of smallest radius intersecting all hyperplanes which contain
exactly $d$ points of $P$.
</p>
<p>We present exact algorithms for computing the yolk and the egg of a point
set, both running in expected time $O(n^{d-1} \log n)$. The running time of the
new algorithm is a polynomial time improvement over existing algorithms. We
also present algorithms for several related problems, such as computing the
Tukey and center balls of a point set, among others.
</p></div>







<p class="date">
<a href="http://arxiv.org/abs/1912.01639"><span class="datestr">at December 05, 2019 12:00 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/175">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/175">TR19-175 |  Matching Smolensky&amp;#39;s correlation bound with majority | 

	Emanuele Viola</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show that there are degree-$d$ polynomials over $\mathbb{F}_{2}$ with
correlation $\Omega(d/\sqrt{n})$ with the majority function on $n$
bits. This matches the $O(d/\sqrt{n})$ bound by Smolensky.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/175"><span class="datestr">at December 04, 2019 04:53 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/12/04/arc-postdoctoral-fellow-at-georgia-institute-of-technology-apply-by-january-6-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/12/04/arc-postdoctoral-fellow-at-georgia-institute-of-technology-apply-by-january-6-2020/">ARC Postdoctoral Fellow at Georgia Institute of Technology (apply by January 6, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Algorithms and Randomness Center (ARC) at Georgia Institute of Technology is seeking multiple postdoctoral fellows starting Fall 2020. ARC has faculty associated with multiple departments including CS, Math, ISyE and EE. The candidate will work on any aspect of algorithms, optimization, broadly interpreted, and collaborate with ARC faculty.</p>
<p>Website: <a href="https://www.isye.gatech.edu/about/employment-opportunities/postdoctoral-fellow-arc">https://www.isye.gatech.edu/about/employment-opportunities/postdoctoral-fellow-arc</a><br />
Email: arc-postdoc@cc.gatech.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/12/04/arc-postdoctoral-fellow-at-georgia-institute-of-technology-apply-by-january-6-2020/"><span class="datestr">at December 04, 2019 02:57 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=16447">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/12/03/end-to-end-encryption-a-problem/">End-to-End Encryption: A Problem</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><font color="#0044cc"><br />
<em>Bopuifs fodszqujpo qspcmfn.</em><br />
<font color="#000000"></font></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2019/12/ochs.jpg"><img src="https://rjlipton.files.wordpress.com/2019/12/ochs.jpg?w=142&amp;h=180" alt="" width="142" class="alignright wp-image-16449" height="180" /></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">“Unsung Entrepreneur” <a href="https://www.archbridgeinstitute.org/2019/01/02/adolph-ochs-the-unsung-entrepreneur-who-transformed-journalism/">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Adolph Ochs was the owner of the New York Times. In 1897 he created the paper’s slogan, “All the News That’s Fit to Print.” We at GLL would like some suggestions on our own slogan. Send us your ideas. Please no suggestion of “All the news that fits we print,” as that is already out there.</p>
<p>
Today Ken and I wish to comment on a recent article in the NYT that was on end-to-end encryption. </p>
<p>
The <a href="https://www.nytimes.com/2019/11/19/technology/end-to-end-encryption.html">article</a> leads by saying: </p>
<blockquote><p><b> </b> <em> A Justice Department official hinted on Monday that a yearslong fight over encrypted communications could become part of a sweeping investigation of big tech companies. </em>
</p></blockquote>
<p></p><p>
Of course, end-to-end encryption scrambles messages so that <em>only</em> the sender and receiver can decode the message. Other methods are weaker: some only encrypt messages as they enter part of the network. This means that one must trust the network to keep your message secret. Thus the end-to-end method reduces the number of parties that one must trust. </p>
<p>
In 1912, Ochs was a party to encryption that was literally end-to-end on the globe. The New York Times had bought exclusive American rights to report Roald Amundsen’s expedition to the South Pole. When Amundsen returned to Hobart, Tasmania, he sent a coded cable to his brother Leon who was acting as conduit to the Times and the London Daily Chronicle. The brother pronounced the coast clear for Amundsen to communicate directly to the papers. The stories were still quickly plagiarized once the first one appeared in the Times, and Ochs had to defend his rights with lawsuits. </p>
<p>
</p><p></p><h2> The Usual Problem </h2><p></p>
<p></p><p>
There is an ongoing interest in using end-to-end encryption to protect more and more of our messages. And this interest leads to several hard problems. </p>
<p>
The main one addressed by the NYT article is: Does this type of encryption protect bad actors? Many believe that encryption makes it impossible to track criminals. Many in law enforcement, for example, wish to have the ability to access any messages, at least on a court order. Some countries are likely to make this the law—that is, they will insist that they always can access any message. A followup NYT <a href="https://www.nytimes.com/reuters/2019/11/27/technology/27reuters-interpol-encryption.html">article</a> described debates within Interpol about these matters.</p>
<p>
</p><p></p><h2> Another Problem </h2><p></p>
<p></p><p>
The above problem is <b>not</b> what we wish to talk about today. We want to raise another problem. </p>
<blockquote><p><b> </b> <em> <i>How do we know that our messages are being properly encrypted?</i> </em>
</p></blockquote>
<p></p><p>
We could check that our app is in end-to-end mode. The app will say “yes”. The problem is that this does not prove anything. The deeper question is how do we know that messages are correctly encrypted. Indeed. </p>
<p>
Suppose that we are told that the message <img src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{M}" class="latex" title="{M}" /> has been sent to another person as the encrypted message <img src="https://s0.wp.com/latex.php?latex=%7BE%28M%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{E(M)}" class="latex" title="{E(M)}" />. How do we know that this has been done? Several issues come to mind:</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>The app could lie.</i> The app could for example say it is encrypting your message and it did not.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>The app could mislead.</i> The app could send an encrypted message and also send the clear message to who ever it wishes.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>The app could be wrong.</i> The app could think that the message was properly encrypted. The key, for example, could be a weak key.</p>
<p>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\bullet }" class="latex" title="{\bullet }" /> <i>The app method could be flawed.</i> The app’s method could be incorrect. The method used might be vulnerable to non or unknown attacks.</p>
<p>
<a href="https://en.wikipedia.org/wiki/Authenticated_encryption">Authenticated</a> encryption seems to cover only part of the need. It can confirm the identity of the sender and that the ciphertext has not been tampered with. This is, however, a far cry from verifying that the encryption itself is proper and free of holes that could make it easy to figure out. Our point is also aside from problems with particular end-to-end implementations such as those considered in this 2017 <a href="https://arxiv.org/pdf/1707.05285.pdf">paper</a>.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Bopuifs fodszqujpo qspcmfn was encrypted with the simple key 	</p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++a+%5Crightarrow+b%2C+b+%5Crightarrow+c%2C+c+%5Crightarrow+d%2C+%5Cdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  a \rightarrow b, b \rightarrow c, c \rightarrow d, \dots " class="latex" title="\displaystyle  a \rightarrow b, b \rightarrow c, c \rightarrow d, \dots " /></p>
<p>The point of this silly example is that it might have been encrypted by a harder method, but it was only encrypted by a trivial substitution method. Nevertheless, Google could not figure it out:</p>
<p></p><p><br />
<a href="https://rjlipton.files.wordpress.com/2019/12/googleanotherencryptionproblem.png"><img src="https://rjlipton.files.wordpress.com/2019/12/googleanotherencryptionproblem.png?w=300&amp;h=175" alt="" class="aligncenter size-medium wp-image-16450" /></a></p>
<p></p></font></font></div>







<p class="date">
by RJLipton+KWRegan <a href="https://rjlipton.wordpress.com/2019/12/03/end-to-end-encryption-a-problem/"><span class="datestr">at December 04, 2019 03:03 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-3722233.post-1485483742428466612">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/gasarch.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://blog.computationalcomplexity.org/2019/12/julia-robinsons-100th-birthday.html">Julia Robinson's 100th birthday</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://blog.computationalcomplexity.org/" title="Computational Complexity">Computational Complexity</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
On Dec 8, 1919 Julia Robinson was born, so today is close to her 100th birthday (she passed away at<br />
<div>
the age of 65 on July 30, 1985).</div>
<div>
<br /></div>
<div>
So time for some facts about her</div>
<div>
<br /></div>
<div>
1) She got her PhD from Tarski where she proved the undecidability of the theory of the rationals.</div>
<div>
<br /></div>
<div>
2) She is probably best known for her work on Hilbert's tenth problem (which we call H10)</div>
<div>
<br /></div>
<div>
In todays' terminology H10 would be stated as:</div>
<div>
<br />
Find an algorithm that will, given p in Z[x_1,...,x_n] determine if it has an integer solution.</div>
<div>
<br /></div>
<div>
Hilbert posed it to inspire deep research in Number Theory. There are some cases that are</div>
<div>
solvable (the topic of a later blog post) but the general problem is undecidable. This is not what Hilbert was aiming for. I wonder if he would be happy with the resolution.</div>
<div>
<br /></div>
<div>
The Davis-Putnam-Robinson paper showed that the decision problem for exponential diophantine equations was undecidable. It was published in 1961. The paper is <a href="https://www.jstor.org/stable/1970289?seq=1#metadata_info_tab_contents">here</a>.  Martin Davis predicted that the proof that H10 was undecidable would be by a young Russian mathematician. He was proven correct when Yuri Matiyasevich supplied the missing piece needed to complete the proof.  </div>
<div>
<br /></div>
<div>
I often read `H10 was resolved by Davis-Putnam-Robinson and Matiyasevich' or sometimes they put all four names in alphabetical order. I like that--- it really was a joint effort.</div>
<div>
<br /></div>
<div>
3) She was inducted (a proof by induction?) into the National Academy of Sciences in 1975.</div>
<div>
<br /></div>
<div>
4) She was elected to be president of the American Math Society in 1982.  </div>
<div>
<br /></div>
<div>
5) She got a MacAuthor Fellowship prize in 1985 (Often called the MacAuthor Genius award.)</div>
<div>
At the time it was worth $60,000.  Its now $625,000.</div>
<div>
<br /></div>
<div>
6) She also did work in Game Theory. Her paper An Iterative Method of Solving a Game, which is<br />
<a href="https://www.jstor.org/stable/1969530?seq=1#metadata_info_tab_contents">here</a>, is a proof from the book according to Paul Goldberg's comment on this post.</div>
<div>
<br /></div>
<div>
7) The Julia Robinson Math Festival is named in her honor (hmmm- is that a tautology?) Its purpose is to inspire K-12 students to get involved in math. For more on it see <a href="https://en.wikipedia.org/wiki/Julia_Robinson_Mathematics_Festival">here</a>.<br />
<br />
8) (ADDED LATER) Commenter David Williamson pointed out that Julia Robinson did work on the transportation problem. See his comment and his pointer to the paper.<br />
<br />
(ADDED LATER) When I hear <i>Julia Robinson</i> I think <i>Hilbert's 10th problem. </i> I suspect many of you do the same. However, looking at items 6 and 8 above, one realizes that she did research in non-logic branches of math as well.</div>
<div>
<br /></div>
<div>
<br /></div>
<div>
<br /></div></div>







<p class="date">
by GASARCH (noreply@blogger.com) <a href="https://blog.computationalcomplexity.org/2019/12/julia-robinsons-100th-birthday.html"><span class="datestr">at December 02, 2019 11:01 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/12/02/postdoc-at-boston-college-apply-by-january-15-2020-2/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/12/02/postdoc-at-boston-college-apply-by-january-15-2020-2/">Postdoc at Boston College (apply by January 15, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Applications are invited for a postdoc position hosted by Hsin-Hao Su at Boston College. Areas of specific interests include but not limited to distributed graph algorithms, local algorithms, dynamic graph algorithms, gossip algorithms, and MPC algorithms. The position can start at any time in 2020 after February. The length of the position is for a period of up to two years.</p>
<p>Website: <a href="https://sites.google.com/site/distributedhsinhao/postdoc">https://sites.google.com/site/distributedhsinhao/postdoc</a><br />
Email: suhx@bc.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/12/02/postdoc-at-boston-college-apply-by-january-15-2020-2/"><span class="datestr">at December 02, 2019 10:37 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="tag:blogger.com,1999:blog-27705661.post-857009480691482743">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aceto.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="http://processalgebra.blogspot.com/2019/12/faculty-positions-at-department-of.html">Faculty positions at the Department of Computer Science, Reykjavik University</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://processalgebra.blogspot.com/" title="Process Algebra Diary">Luca Aceto</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<div><i>This job ad could be of interest to you or to someone you know. Feel free to spread it as you see fit. Thanks! </i></div><div> </div><div>The Department of Computer Science at Reykjavik University invites applications for several full-time faculty positions.</div><br />We  are looking for energetic, highly qualified academics who are eager to  develop their own research programs, strengthen existing research within  the department and build bridges with industry. Of particular interest  are candidates in the areas of machine learning, data science, computer  security and software systems, broadly construed, but exceptionally  qualified candidates from all areas of computer science are encouraged  to apply.<br /><br />Candidates should have a proven international research  record that is commensurate with the level of the position for which  they apply. The successful applicants will play a full part in the  teaching and administrative activities of the department; in particular,  they will teach courses and supervise students at both graduate and  undergraduate level. Applicants having a demonstrated history of  excellence in teaching are preferred. A PhD in computer science or a  related field is required.<br /><br />Salary and rank are commensurate with  experience. An appointment at assistant-professor level is permanent  track, with the expectation that a successful candidate will qualify for  promotion to associate professor within six years.<br /><br />The positions  are open until filled, with the earliest available starting date in  August 2020. Later starting dates can be negotiated.<br /><br />The review of the applications will begin on Monday, 17 February 2020, and will continue until the positions are filled.<br /><div><br /></div><div>See <a href="http://radningar.hr.is/storf/viewjobonweb.aspx?jobid=3558" target="_blank">http://radningar.hr.is/storf/viewjobonweb.aspx?jobid=3558</a> for further information on how to apply for the position and on the required documents. </div><br /><div><b>About the department, Reykjavik University and Iceland</b></div><div><br /></div>The  Department of Computer Science at Reykjavik University has about 650  full-time-equivalent students and 22 faculty members. It provides an  excellent working environment that encourages and supports independence  in academic endeavours, and in which a motivated academic can have  impact at all levels. The department offers undergraduate and graduate  programs in computer science and software engineering, as well as a  combined-degree undergraduate program in discrete mathematics and  computer science. The doctoral program within the department received  its ministerial accreditation in 2009 and has been active ever since.<br /><br />The  department is home to several research centres producing high-quality  collaborative research in areas such as artificial intelligence,  financial technology, language technology, software systems, and  theoretical computer science, among others; for more information on  those research centres, see <a href="https://en.ru.is/research/units/" target="_blank">https://en.ru.is/research/units/</a>.<br /><br />On  the Times Higher Education rankings for 2020, Reykjavík University is  ranked in first place along with six other universities for the average  number of citations per faculty. Overall, according to that list, RU is  ranked among the 300 best universities world-wide, 52nd out of all  universities established fewer than 50 years ago, 14th among  universities with fewer than 5000 students, and first among Icelandic  universities.<br /><br />Iceland is well known for its breathtaking natural  beauty, with volcanoes, geysers, hot springs, lava fields and glaciers  offering a dramatic landscape. It is consistently ranked as one of the  best places in the world to live. It offers a high quality of life, is  one of the safest places in the world, has high gender equality, and  strong health-care and social-support systems. It is in fourth position  in the 2019 UN World Happiness Report, which ranks the world's countries  by their happiness level.<br /><br />For further information about the Department of Computer Science at Reykjavik University and its activities, see <a href="http://en.ru.is/scs/" target="_blank">http://en.ru.is/scs/</a>.</div>







<p class="date">
by Luca Aceto (noreply@blogger.com) <a href="http://processalgebra.blogspot.com/2019/12/faculty-positions-at-department-of.html"><span class="datestr">at December 02, 2019 10:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/12/02/postdoc-at-epfl-apply-by-january-15-2020/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/12/02/postdoc-at-epfl-apply-by-january-15-2020/">Postdoc at EPFL (apply by January 15, 2020)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>Applications are invited for a postdoctoral position in theoretical computer science in the School of Computer and Communication Sciences at EPFL. The position is for a period of up to two years, and comes with a competitive salary as well as a generous travel allowance.</p>
<p>Website: <a href="https://theory.epfl.ch/kapralov/postdoc.html">https://theory.epfl.ch/kapralov/postdoc.html</a><br />
Email: michael.kapralov@epfl.ch</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/12/02/postdoc-at-epfl-apply-by-january-15-2020/"><span class="datestr">at December 02, 2019 08:34 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/12/02/assistant-or-associate-professor-at-university-of-massachusetts-amherst-apply-by-december-31-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/12/02/assistant-or-associate-professor-at-university-of-massachusetts-amherst-apply-by-december-31-2019/">assistant or associate professor at University of Massachusetts, Amherst (apply by December 31, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The College of Information and Computer Sciences at the University of Massachusetts<br />
Amherst invites applications for tenure-track faculty in Theoretical Computer Science at the<br />
Associate and Assistant Professor levels. Exceptional candidates at other ranks may be<br />
considered.</p>
<p>Website: <a href="http://careers.umass.edu/amherst/en-us/job/502778/assistantassociate-professortheory">http://careers.umass.edu/amherst/en-us/job/502778/assistantassociate-professortheory</a><br />
Email: facrec@cs.umass.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/12/02/assistant-or-associate-professor-at-university-of-massachusetts-amherst-apply-by-december-31-2019/"><span class="datestr">at December 02, 2019 06:13 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://www.scottaaronson.com/blog/?p=4439">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/aaronson.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="https://www.scottaaronson.com/blog/?p=4439">Two updates</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://www.scottaaronson.com/blog" title="Shtetl-Optimized">Scott Aaronson</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<ol><li>Two weeks ago, I <a href="https://www.scottaaronson.com/blog/?p=4414">blogged about</a> the <a href="https://arxiv.org/abs/1911.03748">claim</a> of Nathan Keller and Ohad Klein to have proven the Aaronson-Ambainis Conjecture.  Alas, Keller and Klein tell me that they’ve now withdrawn their preprint (though it may take another day for that to show up on the arXiv), because of what looks for now like a fatal flaw, in Lemma 5.3, discovered by Paata Ivanishvili.  (My own embarrassment over having missed this flaw is <em>slightly</em> mitigated by most of the experts in discrete Fourier analysis having missed it as well!)  Keller and Klein are now working to fix the flaw, and I wholeheartedly wish them success.</li><li>In unrelated news, I was saddened to read that <a href="https://en.wikipedia.org/wiki/Virgil_Griffith">Virgil Griffith</a>—cryptocurrency researcher, former Integrated Information Theory researcher, and <a href="https://www.scottaaronson.com/blog/?p=1893">onetime contributor to <em>Shtetl-Optimized</em></a>—was <a href="https://www.forbes.com/sites/jasonbrett/2019/11/29/us-authorities-arrest-virgil-griffith-for-teaching-cryptocurrency-and-blockchain/#1a19647142cb">arrested at LAX</a> for having traveled to North Korea to teach the DPRK about cryptocurrency, against the admonitions of the US State Department.  I didn’t know Virgil well, but I did meet him in person at least once, and I liked <a href="http://www.scottaaronson.com/response-p1.pdf">his</a> <a href="http://www.scottaaronson.com/response-p2.pdf">essays</a> for this blog about how, after spending years studying IIT under Giulio Tononi himself, he became disillusioned with many aspects of it and evolved to a position not far from mine (though not identical either).<br />Personally, I despise the North Korean regime for the obvious reasons—I regard it as not merely evil, but <em>cartoonishly</em> so—and I’m mystified by Virgil’s <a href="https://twitter.com/nicksdjohnson/status/1201212127945605122">apparently sincere belief</a> that he could bring peace between the North and South by traveling to North Korea to give a lecture about blockchain.  Yet, however world-historically naïve he may have been, his intentions appear to have been good.  More pointedly—and here I’m asking not in a legal sense but in a human one—if giving aid and comfort to the DPRK is treasonous, then isn’t the current occupant of the Oval Office a million times guiltier of that particular treason (to say nothing of others)?  It’s like, what does “treason” even mean anymore?  In any case, I hope some plea deal or other arrangement can be worked out that won’t end Virgil’s productive career.</li></ol>



<p></p></div>







<p class="date">
by Scott <a href="https://www.scottaaronson.com/blog/?p=4439"><span class="datestr">at December 02, 2019 03:31 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://gilkalai.wordpress.com/?p=18705">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/kalai.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://gilkalai.wordpress.com/2019/12/02/tyi-41-how-many-steps-does-it-take-for-a-simple-random-walk-on-the-discrete-cube-to-reach-the-uniform-distribution/">TYI 41: How many steps does it take for a simple random walk on the discrete cube to reach the uniform distribution?</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://gilkalai.wordpress.com" title="Combinatorics and more">Gil Kalai</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><a href="https://www.math.bgu.ac.il/~yadina/"><img src="https://gilkalai.files.wordpress.com/2019/11/ayadin.png?w=244&amp;h=300" alt="" width="244" class="alignnone size-medium wp-image-18724" height="300" /></a></p>
<p><span style="color: #ff0000;">Aeiel Yadin’s <a href="https://www.math.bgu.ac.il/~yadina/">homepage</a> contains great lecture notes on harmonic functions on groups and on various other topics.</span></p>
<p>I have a lot of things to discuss and to report; exciting developments in the analysis of Boolean functions; much to report on algebraic, geometric and probabilistic combinatorics following our Oberwolfach summer meeting; much to tell about our Kazhdan seminar on quantum computation and symplectic geometry; a lot of exciting math and TCS activities in Israel; exciting things that Avi Wigderson’s told me on non commutative optimization and moment maps; and, of course, last but not least, the exciting <strong>Google supremacy demonstration</strong> that I most recently wrote about in my post <a href="https://gilkalai.wordpress.com/2019/11/13/gils-collegial-quantum-supremacy-skepticism-faq/" rel="bookmark"><strong>Gil’s Collegial Quantum Supremacy Skepticism FAQ</strong>.</a>  In particular, the unbelievable local-to-global fidelity <strong><span style="color: #ff0000;">Formula (77), </span></strong>and<span style="color: #0000ff;"><strong> (NEW) </strong></span><strong>a poem by Peter Shor for quantum computer skeptics</strong>. More poems are most welcome!</p>
<p>With all these excitements, plans, and blog duties it looks that this is the right time to take a pause for a Test Your Intuition post. (Based on chats with <a href="http://www.math.tau.ac.il/~asafnach/">Asaf Nachmias</a>, <a href="https://www.dpmms.cam.ac.uk/~jh2129/">Jonathan Hermon</a> and <a href="http://www.wisdom.weizmann.ac.il/~itai/">Itai Benjamini</a>.)</p>
<h2>Approaching the uniform distribution on the discrete cube with a lazy simple random walk.</h2>
<p>Consider the discrete cube as a graph: the vertices are all the 0-1 vertices of length <img src="https://s0.wp.com/latex.php?latex=n+&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n " class="latex" title="n " />, two vertices are adjacent if they differ in one coordinate.</p>
<p>A (lazy) simple random walk is described as follows: You start at the all 0 vertex. At each step when you are at vertex <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v" class="latex" title="v" /> you stay where you are with probability 1/2 and, with probability 1/2,  you move to a neighbor  of <img src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="v" class="latex" title="v" /> chosen uniformly at random.</p>
<p>Your position after <img src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="t" class="latex" title="t" /> steps is a random variable describing a probability distribution <img src="https://s0.wp.com/latex.php?latex=D_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D_t" class="latex" title="D_t" /> on the vertices of the discrete cube. Now, lets fix once and for all the value of <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" /> to be 0.1.</p>
<p><strong>Test your intuition:</strong> How many steps <em><strong>T(n)</strong></em> does it take until <img src="https://s0.wp.com/latex.php?latex=D_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D_t" class="latex" title="D_t" /> is <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" />-close to the uniform distribution <img src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="U" class="latex" title="U" /> in <a href="https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures">total variation distance</a>.  (The total variation distance is 1/2 the <img src="https://s0.wp.com/latex.php?latex=L%5E1&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="L^1" class="latex" title="L^1" /> distance).</p>
<h3>Others measure of proximity</h3>
<p>We can also ask: How many steps <em><strong>M(n)</strong></em> does it take until <img src="https://s0.wp.com/latex.php?latex=D_t%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D_t(x)" class="latex" title="D_t(x)" /> is close to the uniform distribution <em>for every</em> <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" />? Namely, for every <img src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="x" class="latex" title="x" />,</p>
<p><img src="https://s0.wp.com/latex.php?latex=%281-%5Cepsilon%29%2F2%5En+%5Cle+D_t%28x%29+%5Cle+%281%2B%5Cepsilon%29%2F2%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="(1-\epsilon)/2^n \le D_t(x) \le (1+\epsilon)/2^n" class="latex" title="(1-\epsilon)/2^n \le D_t(x) \le (1+\epsilon)/2^n" /></p>
<p>For this question there is a simple analysis based on the <a href="https://en.wikipedia.org/wiki/Coupon_collector%27s_problem">coupon collector problem</a>.</p>
<p>We can also consider intermediate measures of proximity, like the entropy:</p>
<p>How many steps <em><strong>H(n)</strong></em> it takes until the entropy of <img src="https://s0.wp.com/latex.php?latex=D_t%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="D_t(x)" class="latex" title="D_t(x)" /> is <img src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="\epsilon" class="latex" title="\epsilon" />-close to the <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> the entropy of the uniform distribution?</p>
<p>Let me try now to test your more detailed intuition: For the public opinion poll below we say that <span style="color: #0000ff;"><em>X behaves like Y</em></span> if their ratio tends to one as <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" /> tends to infinity, and that <span style="color: #0000ff;"><em>X is really smaller than Y</em></span> if their ratio X/Y tends to a limit smaller than 1.</p>
<a name="pd_a_10464820"></a><div style="display: inline-block;" class="CSS_Poll PDS_Poll" id="PDI_container10464820"></div><div id="PD_superContainer"></div><noscript>&lt;a href="https://polldaddy.com/p/10464820" target="_blank"&gt;Take Our Poll&lt;/a&gt;</noscript>
<h3><span style="color: #0000ff;">NEW: Share your knowledge</span></h3>
<p>Let’s try something new: “Share your knowledge (SYK):” What other distances between probability distributions do you recommend? Tell us about them!</p>
<h2>A theorem by Ajtai-Komlos-Szemeredi and a problem by Itai Benjamini</h2>
<p>Ajtai, Komlos and Szemeredi proved that when you choose every edge of the discrete <img src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n" class="latex" title="n" />-cube with probability greater than <img src="https://s0.wp.com/latex.php?latex=1%2Fn&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="1/n" class="latex" title="1/n" /> a giant component emerges! Now, choose every edge with probability <img src="https://s0.wp.com/latex.php?latex=2%2Fn&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="2/n" class="latex" title="2/n" />  and start a simple random walk from a vertex of the giant component. Itai Benjamini conjectured that it will take roughly <img src="https://s0.wp.com/latex.php?latex=n%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" alt="n^2" class="latex" title="n^2" /> steps to approximately reach the stationary distribution. This seems very difficult.</p>
<p> </p></div>







<p class="date">
by Gil Kalai <a href="https://gilkalai.wordpress.com/2019/12/02/tyi-41-how-many-steps-does-it-take-for-a-simple-random-walk-on-the-discrete-cube-to-reach-the-uniform-distribution/"><span class="datestr">at December 02, 2019 06:15 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/174">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/174">TR19-174 |  Exponential Resolution Lower Bounds for Weak Pigeonhole Principle and Perfect Matching Formulas over Sparse Graphs | 

	Susanna de Rezende, 

	Jakob Nordström, 

	Kilian Risse, 

	Dmitry Sokolov</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We show exponential lower bounds on resolution proof length for pigeonhole principle (PHP) formulas and perfect matching formulas over highly unbalanced, sparse expander graphs, thus answering the challenge to establish strong lower bounds in the regime between balanced constant-degree expanders as in [Ben-Sasson and Wigderson '01] and highly unbalanced, dense graphs as in [Raz '04] and [Razborov '03, '04]. We obtain our results by revisiting Razborov's pseudo-width method for PHP formulas over dense graphs and extending it to sparse graphs. This further demonstrates the power of the pseudo-width method, and we believe it could potentially be useful for attacking also other longstanding open problems for resolution and other proof systems.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/174"><span class="datestr">at December 02, 2019 01:04 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-events.org/2019/12/01/tropical-geometry-berkovich-spaces-arithmetic-d-modules-and-p-adic-local-systems/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/events.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-events.org/2019/12/01/tropical-geometry-berkovich-spaces-arithmetic-d-modules-and-p-adic-local-systems/">Tropical Geometry, Berkovich Spaces, Arithmetic D-Modules and p-adic Local Systems</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-events.org" title="CS Theory Events">CS Theory Events</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
June 15-19, 2020 Imperial College of London (UK) https://www-fourier.ujf-grenoble.fr/~pulitaa/Imperial-Conference/Imperial-Conference.html Registration deadline: March 31, 2020 With this workshop we would like to promote the interaction between the following five fields: Berkovich spaces Tropical geometry p-adic differential equations Arithmetic D-modules and representations of p-adic Lie groups Arithmetic applications of p-adic local systems While the first two are … <a href="https://cstheory-events.org/2019/12/01/tropical-geometry-berkovich-spaces-arithmetic-d-modules-and-p-adic-local-systems/" class="more-link">Continue reading <span class="screen-reader-text">Tropical Geometry, Berkovich Spaces, Arithmetic D-Modules and p-adic Local Systems</span></a></div>







<p class="date">
by shacharlovett <a href="https://cstheory-events.org/2019/12/01/tropical-geometry-berkovich-spaces-arithmetic-d-modules-and-p-adic-local-systems/"><span class="datestr">at December 01, 2019 04:59 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://adamsheffer.wordpress.com/?p=5474">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/sheffer.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://adamsheffer.wordpress.com/2019/12/01/teenagers-doing-mathematical-research/">Teenagers doing Mathematical Research</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://adamsheffer.wordpress.com" title="Some Plane Truths">Adam Sheffer</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
I’d like to ramble about another program that our group is running. We are searching for unusually promising high-school-age students and mentor each in a serious research project. We started doing this already before our REU program. However, I never advertised this program before, because I felt that we were still learning how to do […]</div>







<p class="date">
by Adam Sheffer <a href="https://adamsheffer.wordpress.com/2019/12/01/teenagers-doing-mathematical-research/"><span class="datestr">at December 01, 2019 03:27 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/173">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/173">TR19-173 |  Extractor Lower Bounds, Revisited | 

	Divesh Aggarwal, 

	Siyao  Guo, 

	Maciej Obremski, 

	Joao Ribeiro, 

	Noah Stephens-Davidowitz</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
We revisit the fundamental problem of determining seed length lower bounds for strong extractors and natural variants thereof. These variants stem from a ``change in quantifiers'' over the seeds of the extractor: While a strong extractor requires that the average output bias (over all seeds) is small for all input sources with sufficient min-entropy, a somewhere extractor only requires that there exists a seed whose output bias is small. More generally, we study what we call probable extractors, which on input a source with sufficient min-entropy guarantee that a large enough fraction of seeds have small enough associated output bias. Such extractors have played a key role in many constructions of pseudorandom objects, though they are often defined implicitly and have not been studied extensively.

Prior known techniques fail to yield good seed length lower bounds when applied to the variants above. Our novel approach yields significantly improved lower bounds for somewhere and probable extractors. To complement this, we construct a somewhere extractor that implies our lower bound for such functions is tight in the high min-entropy regime. Surprisingly, this means that a random function is far from an optimal somewhere extractor in this regime. The techniques that we develop also yield an alternative, simpler proof of the celebrated optimal lower bound for strong extractors originally due to Radhakrishnan and Ta-Shma (SIAM J. Discrete Math., 2000).</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/173"><span class="datestr">at December 01, 2019 02:42 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://11011110.github.io/blog/2019/11/30/linkage">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eppstein.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://11011110.github.io/blog/2019/11/30/linkage.html">Linkage</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://11011110.github.io/blog/" title="11011110">David Eppstein</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
<ul>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Hoffman%27s_packing_puzzle">Hoffman’s packing puzzle, and its connection to the inequality of arithmetic and geometric means</a> (<a href="https://mathstodon.xyz/@11011110/103151390413241726"></a>). The one I have is not quite so colorful as the illustration for this new Wikipedia article. My father-in-law made it for me some 30 years ago; you can see it in a corner of the photo at <a href="https://11011110.github.io/blog/2018/05/17/book-arrival.html">this post</a>. I don’t unpack it very often, though, because I lost track of the handwritten table of solutions that I made when I first got it and it’s quite difficult to re-pack.</p>
  </li>
  <li>
    <p><a href="https://www.wired.com/story/iran-internet-shutoff/">How the Iranian government shut off the internet</a> (<a href="https://mathstodon.xyz/@11011110/103155401533962129"></a>). According to this story, they have effected “a near-total internet and mobile data blackout” in an attempt to quell gasoline-price protests.</p>
  </li>
  <li>
    <p><a href="https://agtb.wordpress.com/2019/11/19/a-market-for-tcs-papers/">A Market for TCS Papers??</a> (<a href="https://mathstodon.xyz/@11011110/103162041586643628"></a>, with Vijay Vazirani, on the “Turing’s Invisible Hand” blog.) The current situation with theoretical computer science conference reviewing is a mess of long publication delays and reviewer overload caused by repeated submissions and rejections. Vijay and I argue that it should instead be treated as a matching market with pooled submissions and stable matching, getting better results for less time and effort.</p>
  </li>
  <li>
    <p><a href="https://www.siam.org/conferences/cm/program/accepted-papers/soda20-accepted-papers">SODA 2020 accepted papers</a> (<a href="https://mathstodon.xyz/@11011110/103167433760168313"></a>). It only lists titles and authors, but if you notice a title you find intriguing you can find find more detail elsewhere. However, this depends on avoiding obscure titles; if, say, you found a breakthrough on clustered planarity showing that it’s in polynomial time, but you titled your paper “Atomic Embeddability, Clustered Planarity, and Thickenability”, others might not notice.</p>
  </li>
  <li>
    <p>Did you know that <a href="https://en.wikipedia.org/wiki/William_Chapple_(surveyor)">William Chapple</a> (<a href="https://mathstodon.xyz/@11011110/103174753258552519"></a>) discovered Euler’s formula for circumcenter-incenter distance before Euler, Poncelet’s porism on families of triangles inscribed and circumscribed by the same two circles before Poncelet, and was the first to publish a proof that Euclid missed, on the existence of orthocenters of triangles? Did you know that a street in Witheridge is named for him? Have you even heard of William Chapple before? Or Witheridge? Now you have.</p>
  </li>
  <li>
    <p><a href="https://boingboing.net/2019/11/21/portal-icosahedron-sculpture-l.html">Portal Icosahedron by Anthony James</a> (<a href="https://mathstodon.xyz/@11011110/103177399066941274"></a>). An icosahedral frame, infinity mirrors, and LED lighting create a view into an infinite icosahedral grid, creating an effect that, in the jargon of the art world, “is both esoteric and industrial, orphic and distinctly concrete”. Whatever that’s supposed to mean.</p>
  </li>
  <li>
    <p><a href="https://www.theregister.co.uk/2019/11/20/org_registry_sale_shambles/">Get ready to change all of your bookmarks for non-profit organizations</a> (<a href="https://mathstodon.xyz/@11011110/103186257585749674"></a>, <a href="https://www.metafilter.com/184269/Seems-bad">via</a>) as the top-level .org domain name registry is sold to profiteers, drops its own non-profit status, and eliminates price caps on domain name renewals.</p>
  </li>
  <li>
    <p><a href="ttp://thelaborastory.com/stories/professor-ian-wanless-eliyahu-rips/">Ian Wanless on mathematician Eliyahu Rips and his Ig Nobel Prize for Literature</a> (<a href="https://mathstodon.xyz/@11011110/103189764743270099"></a>). An entertaining general-audience talk; audio only.</p>
  </li>
  <li>
    <p><a href="https://math.indiana.edu/research/gallery/tree.html">Charles Darwin’s first drawing of an evolutionary tree</a> (<a href="https://mathstodon.xyz/@11011110/103196622232243519"></a>).</p>
  </li>
  <li>
    <p><a href="https://boingboing.net/2019/11/24/bechdelgrams-are-beautiful.html">Bechdelgrams illustrate of whether a movie passes the Bechdel test</a> (<a href="https://mathstodon.xyz/@11011110/103208740086306490"></a>). A nice use of color to highlight the information you’re looking for in a social network: Here, the network consists of interactions between characters in a film, and the women and conversations not about men are given distinctive colors to show the test criteria: does the film have at least two named female characters, who speak to each other, about something other than men?</p>
  </li>
  <li>
    <p><a href="https://boingboing.net/2019/11/27/paper-sculptures-of-microorgan.html">Rogan Brown creates intricate paper sculptures inspired by microorganisms</a> (<a href="https://mathstodon.xyz/@11011110/103211347688747567"></a>).</p>
  </li>
  <li>
    <p>Some recent open-access conference proceedings (<a href="https://mathstodon.xyz/@11011110/103222522284012005"></a>): <a href="http://drops.dagstuhl.de/opus/portals/lipics/index.php?semnr=16123">27th European Symp. on Algorithms (ESA)</a>; <a href="http://drops.dagstuhl.de/opus/portals/lipics/index.php?semnr=16131">30th Int. Symp. on Algorithms and Computation (ISAAC)</a>; <a href="http://www.jcdcgg.u-tokai.ac.jp/JCDCG3_2019_abstracts_v1.pdf">22nd Japan Conf. on Discrete and Computational Geometry, Graphs, and Games (JCDCGGG)</a>. JCDCGGG is not very selective (think CCCG but more so), but I have <a href="https://erikdemaine.org/papers/MinimalUnunfoldable_JCDCGGG2019/">a paper there with several co-authors on ununfoldable polyhedra with few vertices</a>.</p>
  </li>
  <li>
    <p><a href="https://slate.com/technology/2019/11/nefertiti-bust-neues-museum-3d-printing.html">The Nefertiti bust meets the 21st century</a> (<a href="https://mathstodon.xyz/@11011110/103228129647767519"></a>, <a href="https://news.ycombinator.com/item?id=21670786">via</a>). Interesting essay on claims of intellectual property on ancient artifacts (in this case a high-resolution 3d scan of a bust of Nefertiti), clearly invalid under both US law and still-being-implemented EU law and “dangerously close to committing copy fraud”.</p>
  </li>
</ul></div>







<p class="date">
by David Eppstein <a href="https://11011110.github.io/blog/2019/11/30/linkage.html"><span class="datestr">at November 30, 2019 10:10 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://decentralizedthoughts.github.io/2019-11-29-Analysis-Nakamoto/">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/ittai.jpg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3><a class="entryheader" href="https://decentralizedthoughts.github.io/2019-11-29-Analysis-Nakamoto/">Security proof for Nakamoto Consensus</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://decentralizedthoughts.github.io" title="Decentralized Thoughts">Ittai Abraham</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Bitcoin’s underlying consensus protocol, now known as Nakamoto consensus, is an extremely simple and elegant solution to the Byzantine consensus problem. One may expect this simple protocol to come with a simple security proof. But that turns out not to be the case. The Bitcoin white paper did not provide...</div>







<p class="date">
<a href="https://decentralizedthoughts.github.io/2019-11-29-Analysis-Nakamoto/"><span class="datestr">at November 29, 2019 09:05 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/29/postdoc-at-university-of-waterloo-apply-by-december-31-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/29/postdoc-at-university-of-waterloo-apply-by-december-31-2019/">Postdoc at University of Waterloo (apply by December 31, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Algorithms &amp; Complexity group at the University of Waterloo is offering one postdoctoral position starting in the Fall of 2020. We seek candidates from all areas of TCS.<br />
Interested applicants should have their CV, research statement, and three recommendation letters emailed to theory.waterloo@gmail.com. Applications are due December 31st.<br />
Questions should be sent to the email above.</p>
<p>Website: <a href="https://algcomp.uwaterloo.ca/#MoreInfo">https://algcomp.uwaterloo.ca/#MoreInfo</a><br />
Email: theory.waterloo@gmail.com</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/29/postdoc-at-university-of-waterloo-apply-by-december-31-2019/"><span class="datestr">at November 29, 2019 08:24 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://rjlipton.wordpress.com/?p=16420">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/lipton.jpeg">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://rjlipton.wordpress.com/2019/11/29/predicating-predictivity/">Predicating Predictivity</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://rjlipton.wordpress.com" title="Gödel’s Lost Letter and P=NP">Richard Lipton</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p><span style="color: #0044cc;"><br />
<em>Plus predicaments of error modeling</em><br />
</span></p>
<table class="image alignright">
<tbody>
<tr>
<td><a href="https://rjlipton.wordpress.com/2019/11/29/predicating-predictivity/spiegelhalterbacon/" rel="attachment wp-att-16422"><img src="https://rjlipton.files.wordpress.com/2019/11/spiegelhalterbacon.jpg?w=200&amp;h=168" alt="" width="200" class="alignright wp-image-16422" height="168" /></a></td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from Bacon Sandwich <a href="https://www.youtube.com/watch?v=4szyEbU94ig">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Sir David Spiegelhalter is a British statistician. He is a strong voice for the public understanding of statistics. His work extends to all walks of life, including <a href="https://www.regulation.org.uk/library/2017-Spiegelhalter-Risk_and_Uncertainty_Communication.pdf">risk</a>, <a href="https://understandinguncertainty.org/coincidences">coincidences</a>, <a href="https://www.spectator.co.uk/2019/04/i-could-have-stopped-harold-shipmans-killing-spree-and-saved-175-lives/">murder</a>, and <a href="https://www.ft.com/content/f8793aaa-dfa1-11e4-a06a-00144feab7de">sex</a>.</p>
<p>
Today we talk about extending one of his inventions.<br />
<span id="more-16420"></span></p>
<p>
His invention has to do with grading the performance of people and models that make predictions. A <b>scoring rule</b> grades how often predictions are right. But it may not tell how difficult the situations are. It is easy to look good with predictions when they start with a high chance of success. A weather forecaster predicting sunny-versus-rainy will be right more often in Las Vegas than in Boston. Quoting this FiveThirtyEight <a href="https://fivethirtyeight.com/features/which-city-has-the-most-unpredictable-weather/">item</a>:</p>
<blockquote><p><b> </b> <em> If you want to have an easy life as a weather forecaster, you should get a job in Las Vegas, Phoenix or Los Angeles. Predict that it won’t rain in one of those cities, and you’ll be right about 90 percent of the time. </em>
</p></blockquote>
<p></p><p>
In a 1986 <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780050506">paper</a>, for a particular scoring rule <a href="https://www.semanticscholar.org/paper/VERIFICATION-OF-FORECASTS-EXPRESSED-IN-TERMS-OF-Brier/feee6551179612b9691f021b583d8a99b81b9b86">defined</a> by Glenn Brier in 1950, Spiegelhalter worked out how to equalize the forecaster grading. He applied his <b>Z-test</b> not to weather as Brier was concerned with but to medical prognoses and clinical trials. </p>
<p>
What I am doing with a small group of graduate students in Buffalo is trying to turn Spiegelhalter’s kind of Z-test around once more. If a forecaster fares poorly, we will try to flag not the model but the behavior of the subjects being modeled. In weather we would want to tell when Mother Nature, not the models, has gone off the rails. Well, we are actually looking for ways to tell when a human being has left the bounds of human predictability for reasons that are inhuman—such as cheating with a computer at chess. And maybe it can shed more light on whether our computers can possibly “cheat” with quantum mechanics.</p>
<p>
</p><p></p><h2> Prediction Scores </h2><p></p>
<p></p><p>
Let’s consider situations <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> in which the number <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell+%3D+%5Cell_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell = \ell_t}" class="latex" title="{\ell = \ell_t}" /> is usually more than <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />, that is, usually more than “rain” or “no rain.” The forecaster lays down projections <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bq%7D+%3D+%5Cvec%7Bq%7D_t+%3D+%28q_1%2C%5Cdots%2Cq_%5Cell%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\vec{q} = \vec{q}_t = (q_1,\dots,q_\ell)}" class="latex" title="{\vec{q} = \vec{q}_t = (q_1,\dots,q_\ell)}" /> for the chance of each outcome. If outcome <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> happens, then the <em>Brier score</em> for that forecast is <a name="Brier"></a></p><a name="Brier">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++B%5E%7B%5Cvec%7Bq%7D%7D%28r%29+%3D+%281+-+q_r%29%5E2+%2B+%5Csum_%7Bj+%5Cneq+r%7D+q_j%5E2.+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  B^{\vec{q}}(r) = (1 - q_r)^2 + \sum_{j \neq r} q_j^2. \ \ \ \ \ (1)" class="latex" title="\displaystyle  B^{\vec{q}}(r) = (1 - q_r)^2 + \sum_{j \neq r} q_j^2. \ \ \ \ \ (1)" /></p>
</a><p><a name="Brier"></a> If the forecaster was certain that <img src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r}" class="latex" title="{r}" /> would happen and so put <img src="https://s0.wp.com/latex.php?latex=%7Bq_r+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_r = 1}" class="latex" title="{q_r = 1}" />, all other <img src="https://s0.wp.com/latex.php?latex=%7Bq_j+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_j = 0}" class="latex" title="{q_j = 0}" />, then the score would be zero. Thus lower is better for the Brier score. </p>
<p>
If you put probability <img src="https://s0.wp.com/latex.php?latex=%7Bq_r+%3C+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_r &lt; 1}" class="latex" title="{q_r &lt; 1}" /> on the outcome that happened, then you get penalized both for the difference and for the remaining probability which you put on outcomes that did not happen. It is possible to <em>decompose</em> the score in another way that changes the emphasis: </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++B%5E%7B%5Cvec%7Bq%7D%7D%28r%29+%3D+1+%2B+Q+-+2q_r+%5Cqquad%5Ctext%7Bwhere%7D%5Cqquad+Q+%3D+%5Csum_%7Bj%3D1%7D%5E%5Cell+q_j%5E2.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  B^{\vec{q}}(r) = 1 + Q - 2q_r \qquad\text{where}\qquad Q = \sum_{j=1}^\ell q_j^2. " class="latex" title="\displaystyle  B^{\vec{q}}(r) = 1 + Q - 2q_r \qquad\text{where}\qquad Q = \sum_{j=1}^\ell q_j^2. " /></p>
<p>
Then <img src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q}" class="latex" title="{Q}" /> is a fixed measure of how you spread your forecasts around, while all the variability in your score comes from how much stock you placed in the outcome that happened. The worst case is having put <img src="https://s0.wp.com/latex.php?latex=%7Bq_r+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_r = 0}" class="latex" title="{q_r = 0}" />, whereupon your Brier penalty is <img src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{2}" class="latex" title="{2}" />. </p>
<p>
We would like our forecasts always to be perfect, but reality gives us situations that are inherently nondeterministic—with unknown “true probabilities” <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bp%7D_t+%3D+%28p_1%2C%5Cdots%2Cp_%5Cell%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\vec{p}_t = (p_1,\dots,p_\ell)}" class="latex" title="{\vec{p}_t = (p_1,\dots,p_\ell)}" />. The vital point is that the forecaster should not try to hit <img src="https://s0.wp.com/latex.php?latex=%7Br+%3D+r_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r = r_t}" class="latex" title="{r = r_t}" /> on the nose at every time <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> but rather to match the true probabilities. Once we postulate <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\vec{p}}" class="latex" title="{\vec{p}}" />, the <em>expected Brier score</em> is </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++%5Cmathsf%7BE%7D_%7B%5Cvec%7Bp%7D%7D%5BB%5E%7B%5Cvec%7Bq%7D%7D%5D+%26%3D%26+%5Csum_%7Bi%3D1%7D%5E%5Cell+p_i+B%5E%7B%5Cvec%7Bq%7D%7D%28i%29%5C%5C+%26%3D%26+%5Csum_%7Bi%3D1%7D%5E%5Cell+p_i+%281+-+2q_i+%2B+Q%29%5C%5C+%26%3D%26+1+%2B+Q+-+2%5Csum_%7Bi%3D1%7D%5E%5Cell+p_i+q_i.+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \begin{array}{rcl}  \mathsf{E}_{\vec{p}}[B^{\vec{q}}] &amp;=&amp; \sum_{i=1}^\ell p_i B^{\vec{q}}(i)\\ &amp;=&amp; \sum_{i=1}^\ell p_i (1 - 2q_i + Q)\\ &amp;=&amp; 1 + Q - 2\sum_{i=1}^\ell p_i q_i. \end{array} " class="latex" title="\displaystyle  \begin{array}{rcl}  \mathsf{E}_{\vec{p}}[B^{\vec{q}}] &amp;=&amp; \sum_{i=1}^\ell p_i B^{\vec{q}}(i)\\ &amp;=&amp; \sum_{i=1}^\ell p_i (1 - 2q_i + Q)\\ &amp;=&amp; 1 + Q - 2\sum_{i=1}^\ell p_i q_i. \end{array} " /></p>
<p>This is uniquely minimized by setting <img src="https://s0.wp.com/latex.php?latex=%7Bq_i+%3D+p_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i = p_i}" class="latex" title="{q_i = p_i}" /> for each <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" />, which defines <img src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{B}" class="latex" title="{B}" /> as a <b>strictly proper</b> scoring rule. Without the second term <img src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bj+%5Cneq+r%7D+q_j%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\sum_{j \neq r} q_j^2}" class="latex" title="{\sum_{j \neq r} q_j^2}" /> in (<a href="https://rjlipton.wordpress.com/feed/#Brier">1</a>) the rule would not be proper for <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell+%3E+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell &gt; 2}" class="latex" title="{\ell &gt; 2}" />. When <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bq%7D+%3D+%5Cvec%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\vec{q} = \vec{p}}" class="latex" title="{\vec{q} = \vec{p}}" />, <img src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Q}" class="latex" title="{Q}" /> becomes equal to <img src="https://s0.wp.com/latex.php?latex=%7BP+%3D+%5Csum_%7Bj%3D1%7D%5E%5Cell+p_j%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P = \sum_{j=1}^\ell p_j^2}" class="latex" title="{P = \sum_{j=1}^\ell p_j^2}" />. Thus <img src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{P}" class="latex" title="{P}" /> represents an unavoidable prediction penalty from the intrinsic variance. If all <img src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i}" class="latex" title="{p_i}" /> are equal, <img src="https://s0.wp.com/latex.php?latex=%7Bp_i+%3D+%5Cfrac%7B1%7D%7B%5Cell%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i = \frac{1}{\ell}}" class="latex" title="{p_i = \frac{1}{\ell}}" />, then the expected score cannot be less than <img src="https://s0.wp.com/latex.php?latex=%7B1+-+%5Cfrac%7B1%7D%7B%5Cell%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1 - \frac{1}{\ell}}" class="latex" title="{1 - \frac{1}{\ell}}" />. </p>
<p>
A second example, the log-likelihood prediction scoring rule, is in the original longer <a href="https://cse.buffalo.edu/~regan/GLL/wspiegelhalterLong.pdf">draft</a> of this post.</p>
<p></p><h2> Spiegelhalter’s Z </h2><p></p>
<p></p><p>
Spiegelhalter’s <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{z}" class="latex" title="{z}" />-score neatly drops out the unavoidable penalty term by taking the difference of the score with the expectation. Schematically it is defined as </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BZ%7D%5BB%5D+%3D+%5Cfrac%7BB+-+%5Cmathsf%7BE%7D%5BB%5D%7D%7B%5Csqrt%7B%5Cmathsf%7BVar%7D%5BB%5D%7D%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{Z}[B] = \frac{B - \mathsf{E}[B]}{\sqrt{\mathsf{Var}[B]}}, " class="latex" title="\displaystyle  \mathsf{Z}[B] = \frac{B - \mathsf{E}[B]}{\sqrt{\mathsf{Var}[B]}}, " /></p>
<p>where <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BVar%7D%5BB%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Var}[B]}" class="latex" title="{\mathsf{Var}[B]}" /> means the projected variance <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BE%7D_%7B%5Cvec%7Bp%7D%7D%5BB%5E2%5D+-+%28%5Cmathsf%7BE%7D_%7B%5Cvec%7Bp%7D%7D%5BB%5D%29%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{E}_{\vec{p}}[B^2] - (\mathsf{E}_{\vec{p}}[B])^2}" class="latex" title="{\mathsf{E}_{\vec{p}}[B^2] - (\mathsf{E}_{\vec{p}}[B])^2}" />. However, here is where it is important to notate the whole series of forecasting situations <img src="https://s0.wp.com/latex.php?latex=%7Bt+%3D+1%2C%5Cdots%2CT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t = 1,\dots,T}" class="latex" title="{t = 1,\dots,T}" /> with outcomes <img src="https://s0.wp.com/latex.php?latex=%7Br_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{r_t}" class="latex" title="{r_t}" /> for each <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" />. The actual statistic is <a name="ZB"></a></p><a name="ZB">
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BZ%7D_%7B%5Cvec%7Bp%7D%7D%5BB%5E%7B%5Cvec%7Bq%7D%7D%5D+%3D+%5Cfrac%7B%5Csum_%7Bt%3D1%7D%5ET+B%5E%7B%5Cvec%7Bq%7D_t%7D%28r_t%29+-+%5Cmathsf%7BE%7D_%7B%5Cvec%7Bp%7D_t%7D%5BB%5E%7B%5Cvec%7Bq%7D_t%7D%5D%7D%7B%5Csqrt%7B%5Csum_%7Bt%3D1%7D%5ET+%5Cmathsf%7BVar%7D_%7B%5Cvec%7Bp%7D_t%7D%5BB%5E%7B%5Cvec%7Bq%7D_t%7D%5D%7D%7D.+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{Z}_{\vec{p}}[B^{\vec{q}}] = \frac{\sum_{t=1}^T B^{\vec{q}_t}(r_t) - \mathsf{E}_{\vec{p}_t}[B^{\vec{q}_t}]}{\sqrt{\sum_{t=1}^T \mathsf{Var}_{\vec{p}_t}[B^{\vec{q}_t}]}}. \ \ \ \ \ (2)" class="latex" title="\displaystyle  \mathsf{Z}_{\vec{p}}[B^{\vec{q}}] = \frac{\sum_{t=1}^T B^{\vec{q}_t}(r_t) - \mathsf{E}_{\vec{p}_t}[B^{\vec{q}_t}]}{\sqrt{\sum_{t=1}^T \mathsf{Var}_{\vec{p}_t}[B^{\vec{q}_t}]}}. \ \ \ \ \ (2)" /></p>
</a><p><a name="ZB"></a> The denominator presumes that the forecast situations are independent so that the variances add. The numerator expands to be </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+%5Cleft%282%5Csum_%7Bi%3D1%7D%5E%7B%5Cell_t%7D+p_%7Bi%2Ct%7D+q_%7Bi%2Ct%7D%5Cright%29+-+2q_%7Br%2Ct%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \sum_{t=1}^T \left(2\sum_{i=1}^{\ell_t} p_{i,t} q_{i,t}\right) - 2q_{r,t}. " class="latex" title="\displaystyle  \sum_{t=1}^T \left(2\sum_{i=1}^{\ell_t} p_{i,t} q_{i,t}\right) - 2q_{r,t}. " /></p>
<p>
The original application is a confidence test of the “null hypothesis” that the projections <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bq%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\vec{q}}" class="latex" title="{\vec{q}}" /> are good. Thus we plug in <img src="https://s0.wp.com/latex.php?latex=%7Bp_%7Bi%2Ct%7D+%3D+q_%7Bi%2Ct%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_{i,t} = q_{i,t}}" class="latex" title="{p_{i,t} = q_{i,t}}" /> for all <img src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{t}" class="latex" title="{t}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> so that we test </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BZ%7D_%7B%5Cvec%7Bq%7D%7D%5BB%5E%7B%5Cvec%7Bq%7D%7D%5D+%3D+2%5Cfrac%7B%5Csum_%7Bt%3D1%7D%5ET+%5Cleft%28%5Csum_%7Bi%3D1%7D%5E%7B%5Cell_t%7D+q_%7Bi%2Ct%7D%5E2+%5Cright%29+-+q_%7Br%2Ct%7D%7D%7B%5Csqrt%7B%5Csum_%7Bt%3D1%7D%5ET+%5Cmathsf%7BVar%7D_%7B%5Cvec%7Bq%7D_t%7D%5BB%5E%7B%5Cvec%7Bq%7D_t%7D%5D%7D%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{Z}_{\vec{q}}[B^{\vec{q}}] = 2\frac{\sum_{t=1}^T \left(\sum_{i=1}^{\ell_t} q_{i,t}^2 \right) - q_{r,t}}{\sqrt{\sum_{t=1}^T \mathsf{Var}_{\vec{q}_t}[B^{\vec{q}_t}]}}. " class="latex" title="\displaystyle  \mathsf{Z}_{\vec{q}}[B^{\vec{q}}] = 2\frac{\sum_{t=1}^T \left(\sum_{i=1}^{\ell_t} q_{i,t}^2 \right) - q_{r,t}}{\sqrt{\sum_{t=1}^T \mathsf{Var}_{\vec{q}_t}[B^{\vec{q}_t}]}}. " /></p>
<p>
To illustrate, suppose we do ten independent trials of an event with four outcomes whose true probabilities are <img src="https://s0.wp.com/latex.php?latex=%7B%280.1%2C0.2%2C0.3%2C0.4%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(0.1,0.2,0.3,0.4)}" class="latex" title="{(0.1,0.2,0.3,0.4)}" />. The sum in parentheses is <img src="https://s0.wp.com/latex.php?latex=%7B10%280.01+%2B+0.04+%2B+0.09+%2B+0.16%29+%3D+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{10(0.01 + 0.04 + 0.09 + 0.16) = 3}" class="latex" title="{10(0.01 + 0.04 + 0.09 + 0.16) = 3}" />. If the outcomes conform exactly to these probabilities then <img src="https://s0.wp.com/latex.php?latex=%7Bq_%7Br%2Ct%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_{r,t}}" class="latex" title="{q_{r,t}}" /> equals <img src="https://s0.wp.com/latex.php?latex=%7B0.1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0.1}" class="latex" title="{0.1}" /> once, <img src="https://s0.wp.com/latex.php?latex=%7B0.2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0.2}" class="latex" title="{0.2}" /> twice, <img src="https://s0.wp.com/latex.php?latex=%7B0.3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0.3}" class="latex" title="{0.3}" /> three times, and <img src="https://s0.wp.com/latex.php?latex=%7B0.4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0.4}" class="latex" title="{0.4}" /> four times. This exactly cancels the <img src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{3}" class="latex" title="{3}" />, so <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bq%7D+%3D+%5Cvec%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\vec{q} = \vec{p}}" class="latex" title="{\vec{q} = \vec{p}}" /> makes <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D_%7B%5Cvec%7Bq%7D%7D%5BB%5E%7B%5Cvec%7Bq%7D%7D%5D+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Z}_{\vec{q}}[B^{\vec{q}}] = 0}" class="latex" title="{\mathsf{Z}_{\vec{q}}[B^{\vec{q}}] = 0}" />, as expected. Most trials will give a nonzero numerator, but in the long run, the numerator divided by <img src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{T}" class="latex" title="{T}" /> tends toward zero and the denominator scales to match it, thus keeping the <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" />-statistic normally distributed.</p>
<p>
A high <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" />, on the other hand—highly positive or highly negative—indicates that the forecasting is way off. That (<a href="https://rjlipton.wordpress.com/feed/#ZB">2</a>) is an aggregate statistic over independent trials justifies treating the <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" />-values as <a href="https://en.wikipedia.org/wiki/Standard_score">standard</a> <a href="https://en.wikipedia.org/wiki/Z-test">scores</a>. This applies also to <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" />-tests made similarly from other scoring rules besides the Brier score. The test thus becomes a verdict on the model. High <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" />-values on certain subsets of the data may reveal biases. </p>
<p>
Our idea is the opposite. Suppose we know that the forecasts are true, or suppose they have biases that are known and correctable over moderately large data sets. We may then be able to fit <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D%5BB%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Z}[B]}" class="latex" title="{\mathsf{Z}[B]}" /> as an unbiased estimator (of zero) over large training sets. Then it can become a judgment of whether the data has become unnatural. </p>
<p>
</p><p></p><h2> Why This Z? </h2><p></p>
<p></p><p>
As I have detailed in <a href="https://rjlipton.wordpress.com/2019/08/15/predicting-chess-and-horses/">numerous</a> <a href="https://rjlipton.wordpress.com/2013/09/17/littlewoods-law/">posts</a> <a href="https://rjlipton.wordpress.com/2018/10/18/london-calling/">on</a> <a href="https://rjlipton.wordpress.com/2013/07/27/thirteen-sigma/">this</a> <a href="https://rjlipton.wordpress.com/2011/10/12/empirical-humility/">blog</a>, my system for detecting cheating with computers at chess already provides several statistical <img src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{z}" class="latex" title="{z}" />-scores. Why would I want another one?</p>
<p>
The motive involves the presence of multiple strong chess-playing programs, each with its own quirks and distribution of values for moves. They are used in two different ways:</p>
<ol>
<li>
As inputs telling the relative values <img src="https://s0.wp.com/latex.php?latex=%7Bv_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v_i}" class="latex" title="{v_i}" /> of moves <img src="https://s0.wp.com/latex.php?latex=%7Bm_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{m_i}" class="latex" title="{m_i}" />, which my model converts into its probability projections <img src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i}" class="latex" title="{q_i}" />. <p></p>
</li><li>
As output predicates telling how often the player chose the move recommended by a specific program and/or quantifying the magnitude of error for different played moves.
</li></ol>
<p>
Having multiple engines helps point 1. My intent to blend the <em>values</em> <img src="https://s0.wp.com/latex.php?latex=%7Bv_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{v_i}" class="latex" title="{v_i}" /> from different engines has been blunted by issues I discussed <a href="https://rjlipton.wordpress.com/2018/09/07/sliding-scale-problems/">here</a>.  Thus I now have to train my model separately (and expensively) for each (new version of each) program. I can then blend the <img src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i}" class="latex" title="{q_i}" />, but point 2 still remains at issue: My tests measure concordance with a specific program. Originally the program Rybka 3 was primary and Houdini 4B secondary. Now Stockfish 7 is primary and Komodo 10.0 secondary—until I update to their latest versions. The second engine is supposed to confirm a positive result from the first one.  This already means that my model is not trying to detect exactly which program was used.</p>
<p>
Nevertheless, my results often vary between testing engines. The engines <a href="https://rjlipton.wordpress.com/2014/12/28/the-new-chess-world-champion/">compete</a> against each other and may be crafted to disagree on certain kinds of moves. They agree with each other barely 75–80% in my tests. I would like to factor these differences out. </p>
<p>
The Spiegelhalter <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" />-test appeals because its reference is not to a particular chess program, but to the prediction quality of my model itself—which per point 1 can be informed by many programs in concert. It gives a way to <em>predicate predictivity</em>. A high value will attest that the sequence of played moves falls outside the range of predictability for human players of the same rated skill level. </p>
<p>
</p><p></p><h2> The Method </h2><p></p>
<p></p><p>
To harness <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D%5BF%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Z}[F]}" class="latex" title="{\mathsf{Z}[F]}" /> for some scoring rule <img src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{F}" class="latex" title="{F}" />, we need to quantify the nature of my model’s <img src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i}" class="latex" title="{q_i}" /> projections. In fact, my model has a clear bias toward conservatism in judging the frequency of particular non-optimal moves. This is discussed in my August <a href="https://rjlipton.wordpress.com/2019/08/15/predicting-chess-and-horses/">post</a> on my model upgrade and shown graphically in an appended <a href="https://cse.buffalo.edu/~regan/chess/computer/ModelTradeoffs.png">note</a> on why the conservative setting of a “gradient” parameter is needed to preserve dynamical stability. The fitting offsets this in a way that creates an opposite bias elsewhere. I hope to correct both biases at the same stroke by a specific means of modeling how the <img src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i}" class="latex" title="{q_i}" /> err with respect to the postulated true probabilities <img src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i}" class="latex" title="{p_i}" />.</p>
<p>
We postulate an original source of error terms <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon_i}" class="latex" title="{\epsilon_i}" /> all <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">i.i.d.</a> as <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BN%7D%280%2C%5Cdelta%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathcal{N}(0,\delta^2)}" class="latex" title="{\mathcal{N}(0,\delta^2)}" />, where <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta}" class="latex" title="{\delta}" /> governs the magnitude of Gaussian noise. This noise can be <em>transformed</em> and related in various ways, e.g.:</p>
<ol>
<li>
<img src="https://s0.wp.com/latex.php?latex=%7Bq_i+%3D+p_i+%5Cpm+%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i = p_i \pm \epsilon_i}" class="latex" title="{q_i = p_i \pm \epsilon_i}" />, <p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7Bq_i+%3D+p_i%281+%5Cpm+%5Cepsilon_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i = p_i(1 \pm \epsilon_i)}" class="latex" title="{q_i = p_i(1 \pm \epsilon_i)}" />, <p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7Bq_i%7D+%3D+%5Cfrac%7B1%7D%7Bp_i%7D+%5Cpm+%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\frac{1}{q_i} = \frac{1}{p_i} \pm \epsilon_i}" class="latex" title="{\frac{1}{q_i} = \frac{1}{p_i} \pm \epsilon_i}" />, <p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Clog%28%5Cfrac%7B1%7D%7Bq_i%7D%29+%3D+%5Clog%28%5Cfrac%7B1%7D%7Bp_i%7D%29+%5Cpm+%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log(\frac{1}{q_i}) = \log(\frac{1}{p_i}) \pm \epsilon_i}" class="latex" title="{\log(\frac{1}{q_i}) = \log(\frac{1}{p_i}) \pm \epsilon_i}" />, <p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Clog%28%5Cfrac%7B1%7D%7Bq_i%7D%29+%3D+%5Clog%28%5Cfrac%7B1%7D%7Bp_i%7D%29%281+%5Cpm+%5Cepsilon_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\log(\frac{1}{q_i}) = \log(\frac{1}{p_i})(1 \pm \epsilon_i)}" class="latex" title="{\log(\frac{1}{q_i}) = \log(\frac{1}{p_i})(1 \pm \epsilon_i)}" />, <p></p>
</li><li>
<img src="https://s0.wp.com/latex.php?latex=%7B%5Cln%28%5Cfrac%7Bq_i%7D%7B1+-+q_i%7D%29+%3D+%5Cln%28%5Cfrac%7Bp_i%7D%7B1+-+p_i%7D%29+%5Cpm+%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ln(\frac{q_i}{1 - q_i}) = \ln(\frac{p_i}{1 - p_i}) \pm \epsilon_i}" class="latex" title="{\ln(\frac{q_i}{1 - q_i}) = \ln(\frac{p_i}{1 - p_i}) \pm \epsilon_i}" />.
</li></ol>
<p>
There are further forms to consider and it is not yet clear from data within my model which one most applies. We would be interested in examples where these representations have been employed and in observations about their natures. </p>
<p>
Given the error terms, we can write each <img src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i}" class="latex" title="{p_i}" /> as a function of <img src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i}" class="latex" title="{q_i}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon_i}" class="latex" title="{\epsilon_i}" />. One issue is having at most <img src="https://s0.wp.com/latex.php?latex=%7B%5Cell-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\ell-1}" class="latex" title="{\ell-1}" /> degrees of freedom among <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_1%2C%5Cdots%2C%5Cepsilon_%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon_1,\dots,\epsilon_\ell}" class="latex" title="{\epsilon_1,\dots,\epsilon_\ell}" />, owing to the constraint that the <img src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i}" class="latex" title="{q_i}" /> as well as <img src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i}" class="latex" title="{p_i}" /> sum to <img src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{1}" class="latex" title="{1}" />. We handle this by choosing some fixed <img src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{k}" class="latex" title="{k}" /> as the “pivot” and using the constraints to eliminate <img src="https://s0.wp.com/latex.php?latex=%7Bp_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_k}" class="latex" title="{p_k}" /> and <img src="https://s0.wp.com/latex.php?latex=%7Bq_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_k}" class="latex" title="{q_k}" />, leaving the other error terms free. In all cases, the proposed method of defining what we notate as <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D_%7B%5Cvec%7Bq%7D%2C%5Cvec%7B%5Cepsilon%7D%7D%5BF%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Z}_{\vec{q},\vec{\epsilon}}[F]}" class="latex" title="{\mathsf{Z}_{\vec{q},\vec{\epsilon}}[F]}" /> is:</p>
<ul>
<li>
Substitute the terms with <img src="https://s0.wp.com/latex.php?latex=%7Bq_i%2C%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i,\epsilon_i}" class="latex" title="{q_i,\epsilon_i}" /> for each free <img src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i}" class="latex" title="{p_i}" /> into <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D_%7B%5Cvec%7Bp%7D%7D%5BF%5E%7B%5Cvec%7Bq%7D%7D%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Z}_{\vec{p}}[F^{\vec{q}}]}" class="latex" title="{\mathsf{Z}_{\vec{p}}[F^{\vec{q}}]}" />. <p></p>
</li><li>
Compute the expectation over <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_i+%5Csim+%5Cmathcal%7BN%7D%280%2C%5Cdelta%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon_i \sim \mathcal{N}(0,\delta^2)}" class="latex" title="{\epsilon_i \sim \mathcal{N}(0,\delta^2)}" /> for the numerator and denominator of (<a href="https://rjlipton.wordpress.com/feed/#ZB">2</a>), separately. <p></p>
</li><li>
Holding the other previously-fitted model parameters in place, fit <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta}" class="latex" title="{\delta}" /> so that <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D_%7B%5Cvec%7Bq%7D%2C%5Cvec%7B%5Cepsilon%7D%7D%5BF%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{Z}_{\vec{q},\vec{\epsilon}}[F]}" class="latex" title="{\mathsf{Z}_{\vec{q},\vec{\epsilon}}[F]}" /> is zero over the training set (or sets, for each level of Elo rating <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" />, so <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta}" class="latex" title="{\delta}" /> becomes a function of <img src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{R}" class="latex" title="{R}" />).
</li></ul>
<p>
If the resulting <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" />-scores parameterized by <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta_R%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta_R}" class="latex" title="{\delta_R}" /> make sense, the last step will be adjusting them to conform to normal distribution, via the resampling process mentioned recently <a href="https://rjlipton.wordpress.com/2019/08/20/our-trip-to-monte-carlo/">here</a> and earlier <a href="https://rjlipton.wordpress.com/2011/10/12/empirical-humility/">here</a>. We are not there yet. But observations from Spiegelhalter tests with <img src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bq%7D+%3D+%5Cvec%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\vec{q} = \vec{p}}" class="latex" title="{\vec{q} = \vec{p}}" /> (equivalently, with <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta_R%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta_R}" class="latex" title="{\delta_R}" /> fixed to zero) suggest that the resulting single, authoritative, “pure” predictivity test may rival the sharpness of my current tests involving specific chess programs.</p>
<p>
</p><p></p><h2> Error Quirks and Queries </h2><p></p>
<p></p><p>
To see a key wrinkle, consider the first error form. It is symmetrical: <img src="https://s0.wp.com/latex.php?latex=%7Bp_i+%3D+q_i+%5Cpm+%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i = q_i \pm \epsilon_i}" class="latex" title="{p_i = q_i \pm \epsilon_i}" />. When we substitute <img src="https://s0.wp.com/latex.php?latex=%7Bq_i+%2B+%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i + \epsilon_i}" class="latex" title="{q_i + \epsilon_i}" /> for <img src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i}" class="latex" title="{p_i}" /> and take <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BE%7D_%7B%5Cepsilon_i+%5Csim+%5Cmathcal%7BN%7D%280%2C%5Cdelta%5E2%29%7D%5B%5Ccdots%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{E}_{\epsilon_i \sim \mathcal{N}(0,\delta^2)}[\cdots]}" class="latex" title="{\mathsf{E}_{\epsilon_i \sim \mathcal{N}(0,\delta^2)}[\cdots]}" />, the symmetry of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon_i}" class="latex" title="{\epsilon_i}" /> around <img src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{0}" class="latex" title="{0}" /> makes it drop out of the numerator of (<a href="https://rjlipton.wordpress.com/feed/#ZB">2</a>), and out of everything in the denominator except one place where <img src="https://s0.wp.com/latex.php?latex=%7Bp_i%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i^2}" class="latex" title="{p_i^2}" /> becomes <img src="https://s0.wp.com/latex.php?latex=%7B%28q_i%5E2+%2B+2%5Cepsilon+q_i+%2B+%5Cepsilon_i%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(q_i^2 + 2\epsilon q_i + \epsilon_i^2)}" class="latex" title="{(q_i^2 + 2\epsilon q_i + \epsilon_i^2)}" />. There is hence nothing for <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta}" class="latex" title="{\delta}" /> to fit and we are basically left with the original Spiegelhalter <img src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{Z}" class="latex" title="{Z}" />. </p>
<p>
In the second form, however, we get <img src="https://s0.wp.com/latex.php?latex=%7Bp_i+%3D+q_i+%5Ccdot+%5Cfrac%7B1%7D%7B1+%2B+%5Cepsilon_i%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i = q_i \cdot \frac{1}{1 + \epsilon_i}}" class="latex" title="{p_i = q_i \cdot \frac{1}{1 + \epsilon_i}}" />. If we presume <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta}" class="latex" title="{\delta}" /> small enough to make the distribution of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BN%7D%280%2C%5Cdelta%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathcal{N}(0,\delta^2)}" class="latex" title="{\mathcal{N}(0,\delta^2)}" /> outside <img src="https://s0.wp.com/latex.php?latex=%7B%28-1%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{(-1,1)}" class="latex" title="{(-1,1)}" /> negligible, then we can use the series expansion to approximate </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++p_i+%5Capprox+q_i%281+-+%5Cepsilon_i+%2B+%5Cepsilon_i%5E2+-+%5Cepsilon_i%5E3+%2B+%5Cepsilon_i%5E4%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  p_i \approx q_i(1 - \epsilon_i + \epsilon_i^2 - \epsilon_i^3 + \epsilon_i^4). " class="latex" title="\displaystyle  p_i \approx q_i(1 - \epsilon_i + \epsilon_i^2 - \epsilon_i^3 + \epsilon_i^4). " /></p>
<p>Under normal expectation, the odd-power terms drop out (so their signs don’t matter) and we get </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BE%7D_%7B%5Cepsilon_i+%5Csim+%5Cmathcal%7BN%7D%280%2C%5Cdelta%5E2%29%7D%5Bq_i%281+-+%5Cepsilon_i+%2B+%5Cepsilon_i%5E2+-+%5Cepsilon_i%5E3+%2B+%5Cepsilon_i%5E4%29%5D+%3D+q_i%281+%2B+%5Cdelta%5E2+%2B+3%5Cdelta%5E4%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{E}_{\epsilon_i \sim \mathcal{N}(0,\delta^2)}[q_i(1 - \epsilon_i + \epsilon_i^2 - \epsilon_i^3 + \epsilon_i^4)] = q_i(1 + \delta^2 + 3\delta^4). " class="latex" title="\displaystyle  \mathsf{E}_{\epsilon_i \sim \mathcal{N}(0,\delta^2)}[q_i(1 - \epsilon_i + \epsilon_i^2 - \epsilon_i^3 + \epsilon_i^4)] = q_i(1 + \delta^2 + 3\delta^4). " /></p>
<p>This credits <img src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i}" class="latex" title="{p_i}" /> as being greater than <img src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i}" class="latex" title="{q_i}" />. Provided the projections for the substituted indices <img src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{i}" class="latex" title="{i}" /> were generally slightly conservative, this has hope of correcting them.</p>
<p>
Already, however, we have traipsed over some pitfalls of methodology. One is that the normal expectation </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BE%7D_%7B%5Cepsilon+%5Csim+%5Cmathcal%7BN%7D%280%2C%5Cdelta%5E2%29%7D%5B%5Cfrac%7B1%7D%7B1%2B%5Cepsilon%7D%5D+%3D+%2B%5Cinfty%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{E}_{\epsilon \sim \mathcal{N}(0,\delta^2)}[\frac{1}{1+\epsilon}] = +\infty, " class="latex" title="\displaystyle  \mathsf{E}_{\epsilon \sim \mathcal{N}(0,\delta^2)}[\frac{1}{1+\epsilon}] = +\infty, " /></p>
<p>regardless of how small <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta}" class="latex" title="{\delta}" /> is. For any <img src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\delta}" class="latex" title="{\delta}" />, regions around the pole <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3D+-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon = -1}" class="latex" title="{\epsilon = -1}" /> get some fixed finite probability. Another is the simple paradox of our second form saying:</p>
<blockquote><p><b> </b> <em> <img src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{q_i}" class="latex" title="{q_i}" /> is an unbiased estimator of <img src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{p_i}" class="latex" title="{p_i}" />, but <img src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{p_i}" class="latex" title="{p_i}" /> is not an unbiased (or even finite) estimator of <img src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" alt="{q_i}" class="latex" title="{q_i}" />. </em>
</p></blockquote>
<p></p><p>
A third curiosity comes from the fourth error form. It gives <img src="https://s0.wp.com/latex.php?latex=%7Bq_i+%3D+p_i+e%5E%7B%5Cepsilon_i%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{q_i = p_i e^{\epsilon_i}}" class="latex" title="{q_i = p_i e^{\epsilon_i}}" />, so <img src="https://s0.wp.com/latex.php?latex=%7Bp_i+%3D+q_i+e%5E%7B-%5Cepsilon_i%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{p_i = q_i e^{-\epsilon_i}}" class="latex" title="{p_i = q_i e^{-\epsilon_i}}" />. We have </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BE%7D_%7B%5Cepsilon+%5Csim+%5Cmathcal%7BN%7D%280%2C%5Cdelta%5E2%29%7D%5Be%5E%7Bb%5Cepsilon%7D%5D+%3D+e%5E%7B0.5b%5E2+%5Cdelta%5E2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{E}_{\epsilon \sim \mathcal{N}(0,\delta^2)}[e^{b\epsilon}] = e^{0.5b^2 \delta^2} " class="latex" title="\displaystyle  \mathsf{E}_{\epsilon \sim \mathcal{N}(0,\delta^2)}[e^{b\epsilon}] = e^{0.5b^2 \delta^2} " /></p>
<p>exactly, without approximation. Again the sign of <img src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\epsilon_i}" class="latex" title="{\epsilon_i}" /> does not matter. So we get </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BE%7D_%7B%5Cvec%7B%5Cepsilon%7D%7D%5Bp_i%5D+%3D+q_i+e%5E%7B0.5%5Cdelta%5E2%7D+%3E+q_i.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{E}_{\vec{\epsilon}}[p_i] = q_i e^{0.5\delta^2} &gt; q_i. " class="latex" title="\displaystyle  \mathsf{E}_{\vec{\epsilon}}[p_i] = q_i e^{0.5\delta^2} &gt; q_i. " /></p>
<p>But by the original fourth equation we get </p>
<p align="center"><img src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BE%7D_%7B%5Cvec%7B%5Cepsilon%7D%7D%5Bq_i%5D+%3D+p_i+e%5E%7B0.5%5Cdelta%5E2%7D+%3E+p_i.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="\displaystyle  \mathsf{E}_{\vec{\epsilon}}[q_i] = p_i e^{0.5\delta^2} &gt; p_i. " class="latex" title="\displaystyle  \mathsf{E}_{\vec{\epsilon}}[q_i] = p_i e^{0.5\delta^2} &gt; p_i. " /></p>
<p>So we have <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BE%7D%5Bq_i%5D+%3E+p_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{E}[q_i] &gt; p_i}" class="latex" title="{\mathsf{E}[q_i] &gt; p_i}" /> and <img src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BE%7D%5Bp_i%5D+%3E+q_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" alt="{\mathsf{E}[p_i] &gt; q_i}" class="latex" title="{\mathsf{E}[p_i] &gt; q_i}" />, with both expectations being over the same noise terms. This is like the famous Lake Wobegon <a href="https://trustedadvisor.com/trustmatters/lake-wobegon-syndrome-believing-were-all-above-average">syndrome</a>. What it indicates is the need for care in where and how to apply these error representations.</p>
<p>
</p><p></p><h2> Open Problems </h2><p></p>
<p></p><p>
Have you seen this idea of directly testing (un)predictability in the literature? Might it improve the currently much-debated statistical tests for quantum supremacy?</p>
<p>
Which error model seems most likely to apply? Where have the paradoxes in our last section been noted?</p>
<p></p><p><br />
[some wording tweaks]</p></div>







<p class="date">
by KWRegan <a href="https://rjlipton.wordpress.com/2019/11/29/predicating-predictivity/"><span class="datestr">at November 29, 2019 02:35 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://corner.mimuw.edu.pl/?p=1093">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/banach.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-US"><a class="entryheader" href="http://corner.mimuw.edu.pl/?p=1093">Call for Invited Talk Nominations: HALG 2020</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="http://corner.mimuw.edu.pl" title="Banach's Algorithmic Corner">Banach's Algorithmic Corner</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en-US">
<p>

5th Highlights of Algorithms conference (HALG 2020)</p>



<p>ETH Zurich, June 3-5, 2020<br />​<br /><a href="http://2020.highlightsofalgorithms.org/" target="_blank" rel="noreferrer noopener">http://2020.highlightsofalgorithms.org/</a></p>



<p></p>



<p>The HALG 2020 conference seeks high-quality nominations for invited talks that will highlight recent advances in algorithmic research. Similarly to previous years, there are two categories of invited talks:</p>



<p>A. survey (60 minutes): a survey of an algorithmic topic that has seen exciting developments in last couple of years.</p>



<p>B. paper (30 minutes): a significant algorithmic result appearing in a paper in 2019 or later.</p>



<p>To nominate, please email <a href="mailto:halg2020.nominations@gmail.com" target="_blank" rel="noreferrer noopener">halg2020.nominations@gmail.com</a> the following information:</p>



<ol><li>Basic details: speaker name + topic (for survey talk) or paper’s title, authors, conference/arxiv + preferable speaker (for paper talk).</li><li>Brief justification: Focus on the benefits to the audience, e.g., quality of results, importance/relevance of topic, clarity of talk, speaker’s presentation skills.</li></ol>



<p>All nominations will be reviewed by the Program Committee (PC) to select speakers that will be invited to the conference.</p>



<p>Nominations deadline: December 20, 2020 (for full consideration).</p></div>







<p class="date">
by sank <a href="http://corner.mimuw.edu.pl/?p=1093"><span class="datestr">at November 29, 2019 01:30 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/28/tenure-track-professor-at-university-of-british-columbia-apply-by-december-15-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/28/tenure-track-professor-at-university-of-british-columbia-apply-by-december-15-2019/">Tenure-Track Professor at University of British Columbia (apply by December 15, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The Department of Computer Science at the University of British Columbia is inviting applications for at least three positions at the rank of Assistant Professor. We invite applications from candidates of outstanding scientific talent in all areas of computer science. Appointment at a higher rank will be considered for an applicant of exceptional qualifications.</p>
<p>Website: <a href="https://www.cs.ubc.ca/our-department/employment/faculty-sessional-positions/tenure-track-faculty-positions-research-stre-0">https://www.cs.ubc.ca/our-department/employment/faculty-sessional-positions/tenure-track-faculty-positions-research-stre-0</a><br />
Email: research-recruiting-chair@cs.ubc.ca</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/28/tenure-track-professor-at-university-of-british-columbia-apply-by-december-15-2019/"><span class="datestr">at November 28, 2019 11:16 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="https://eccc.weizmann.ac.il/report/2019/172">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/eccc.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en-us"><a class="entryheader" href="https://eccc.weizmann.ac.il/report/2019/172">TR19-172 |  Schur Polynomials do not have small formulas if the Determinant doesn&amp;#39;t!  | 

	Chandra Kanta Mohapatra, 

	Mrinal Kumar, 

	Nutan Limaye, 

	Srikanth Srinivasan, 

	Prasad Chaugule, 

	Adrian She</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://eccc.weizmann.ac.il/" title="ECCC - Reports">ECCC papers</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content">
Schur Polynomials are families of symmetric polynomials that have been
classically studied in Combinatorics and Algebra alike. They play a central
role in the study of Symmetric functions, in Representation theory [Sta99], in
Schubert calculus [LM10] as well as in Enumerative combinatorics [Gas96, Sta84,
Sta99]. In recent years, they have also shown up in various incarnations in
Computer Science, e.g, Quantum computation [HRTS00, OW15] and Geometric
complexity theory [IP17].
  However, unlike some other families of symmetric polynomials like the
Elementary Symmetric polynomials, the Power Symmetric polynomials and the
Complete Homogeneous Symmetric polynomials, the computational complexity of
syntactically computing Schur polynomials has not been studied much. In
particular, it is not known whether Schur polynomials can be computed
efficiently by algebraic formulas. In this work, we address this question, and
show that unless \emph{every} polynomial with a small algebraic branching
program (ABP) has a small algebraic formula, there are Schur polynomials that
cannot be computed by algebraic formula of polynomial size. In other words,
unless the algebraic complexity class $\mathrm{VBP}$ is equal to the complexity
class $\mathrm{VF}$, there exist Schur polynomials which do not have polynomial
size algebraic formulas.
  As a consequence of our proof, we also show that computing the determinant of
certain \emph{generalized} Vandermonde matrices is essentially as hard as
computing the general symbolic determinant. To the best of our knowledge, these
are one of the first hardness results of this kind for families of polynomials
which are not \emph{multilinear}. A key ingredient of our proof is the study of
composition of \emph{well behaved} algebraically independent polynomials with a homogeneous polynomial, and might be of independent interest.</div>







<p class="date">
<a href="https://eccc.weizmann.ac.il/report/2019/172"><span class="datestr">at November 28, 2019 06:40 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://tcsplus.wordpress.com/?p=381">
<div class="entryheader">
<span class="face">
	<img class="face" src="images/seminarseries.png">
</span>
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://tcsplus.wordpress.com/2019/11/27/tcs-talk-wednesday-december-4-nihar-shah-cmu/">TCS+ talk: Wednesday, December 4 — Nihar Shah, CMU</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://tcsplus.wordpress.com" title="TCS+">TCS+ seminar series</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>The next TCS+ talk, and last of the Fall season, will take place this coming Wednesday, December 4th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Nihar Shah</strong> from CMU will speak about “<em>Battling Demons in Peer Review</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: Peer review is the backbone of scholarly research. It is however faced with a number of challenges (or “demons”) which cause unfairness to authors, and degrade the overall quality of the process. This talk will present principled and practical approaches to battle these demons in peer review:</p>
<ol>
<li>Subjectivity: How to ensure that all papers are judged by the same yardstick?</li>
<li>Mis-calibration: How to use ratings in presence of arbitrary or adversarial mis-calibration?</li>
<li>Bias: How to rigorously test for existence of (gender/fame/race/…) biases in peer review?</li>
<li>Strategic behavior: How to insulate peer review from strategic behavior of author-reviewers?</li>
<li>Noise: How to assign reviewers to papers to simultaneously ensure fair and accurate evaluations in the presence of review noise?</li>
</ol>
<p>The work uses tools from social choice theory, statistics and learning theory, information theory, game theory and decision theory. No prior knowledge on these topics will be assumed.</p></blockquote>
<p>Bio:<br />
<em><a href="http://cs.cmu.edu/~nihars">Nihar B. Shah</a> is an Assistant Professor in the Machine Learning and Computer Science departments at Carnegie Mellon University (CMU). His research interests include statistics, machine learning, information theory, and game theory, with a focus on applications to learning from people. He is a recipient of the the 2017 David J. Sakrison memorial prize from EECS Berkeley for a “truly outstanding and innovative PhD thesis”, the Microsoft Research PhD Fellowship 2014-16, the Berkeley Fellowship 2011-13, the IEEE Data Storage Best Paper and Best Student Paper Awards for the years 2011/2012, and the SVC Aiya Medal 2010, and has supervised the Best Student Paper at AAMAS 2019.</em></p></div>







<p class="date">
by plustcs <a href="https://tcsplus.wordpress.com/2019/11/27/tcs-talk-wednesday-december-4-nihar-shah-cmu/"><span class="datestr">at November 28, 2019 01:56 AM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>









<div class="channelgroup">
<div class="entrygroup" 
	id="http://cstheory-jobs.org/2019/11/27/faculty-asst-and-assoc-prof-at-university-of-washington-apply-by-december-15-2019/">
<div class="entryheader">
<div class="entryheadertextcontainer">
<div class="entryheadertext">
<h3 lang="en"><a class="entryheader" href="https://cstheory-jobs.org/2019/11/27/faculty-asst-and-assoc-prof-at-university-of-washington-apply-by-december-15-2019/">Faculty (Asst. and Assoc. Prof) at University of Washington (apply by December 15, 2019)</a></h3>
<p style="text-indent: 1.0em;">from 
<a class="entryheader" href="https://cstheory-jobs.org" title="Theoretical Computer Science Jobs">CCI: jobs</a>
<p>
</div>
</div>
</div>
<div class="entry">
<div class="content" lang="en">
<p>This year we have a targeted search in all areas of quantum computing, with a particular emphasis on quantum algorithms and quantum complexity theory.</p>
<p>Website: <a href="https://apply.interfolio.com/64708">https://apply.interfolio.com/64708</a><br />
Email: jrl@cs.washington.edu</p></div>







<p class="date">
by shacharlovett <a href="https://cstheory-jobs.org/2019/11/27/faculty-asst-and-assoc-prof-at-university-of-washington-apply-by-december-15-2019/"><span class="datestr">at November 27, 2019 10:00 PM UTC</span></a>
</p>
<div class="comments">
</div>
</div>
</div>

</div>


</body>

</html>
