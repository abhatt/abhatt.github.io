[{"content": "<![CDATA[<p>Greg #357:</p>\n<ul>Suppose that you have an NP-hard problem L which has a Karp reduction from CircuitSAT, say, and suppose that there is an algorithm A that is touted for solving L but sometimes takes exponential time. Then can you perhaps always use cryptography to create another algorithm B that \u201cembarrasses\u201d A by being fast in some cases where A is slow?</ul>\n<p>That&#8217;s a great question!</p>\n<p>I think you&#8217;ll enjoy <a href=\"http://cs.haifa.ac.il/~ronen/online_papers/pseudo-full.pdf\" rel=\"nofollow\">this paper</a> by Gutfreund, Shaltiel, and Ta-Shma, which directly relates to your question.  Briefly, they show that <i>if</i> NP is hard on average, then given any randomized algorithm A that purports to solve SAT in polynomial time, it&#8217;s possible to efficiently generate instances on which A usually fails.</p>\n<p>Their result simplifies in the deterministic case, where it boils down to one of my favorite tricks in all of complexity theory.  Namely, here&#8217;s what you do: <b>you ask the algorithm A to find a SAT instance for you on which it itself fails!</b>  If A succeeds, then you&#8217;re done, while if A fails, then you&#8217;re also done, since the instance you just gave A is the one you were looking for! <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/icon_biggrin.gif\" alt=\":-D\" class=\"wp-smiley\" /> </p>\n<p>And note that we didn&#8217;t even need to make any strong cryptographic assumption: merely the assumption P&ne;NP.</p>\n<p>Now, a different way to interpret your question would be to say, you don&#8217;t get access to the code of A, but you <i>are</i> allowed to make a cryptographic assumption.  And you want to generate SAT instances that are &#8220;cryptographically hard,&#8221; but that can nevertheless be solved in polynomial time by <i>one particular algorithm</i>, which happens to know a secret that no &#8220;normal&#8221; algorithm would know.</p>\n<p>The obvious way to achieve the above would be to use a trapdoor one-way function, and to encode the trapdoor information into one particular algorithm in order to make the instances easy for that algorithm.  The issue with this approach is that the trapdoor information would presumably take poly(n) bits to specify!  So you&#8217;d only get a class of <i>circuits</i> that can easily solve your SAT instances, rather than a uniform polynomial-time algorithm.</p>\n]]>", "author": "Scott", "published": "2014-03-13 22:21:07+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Greg #378<br />\nThanks a lot! Very interesting!</p>\n]]>", "author": "fred", "published": "2014-03-13 22:28:34+00:00", "title": "By: fred"}, {"content": "<![CDATA[<p>Hi Scott, </p>\n<p>Re #379, could you elaborate on the meaning of &#8220;hard on average&#8221; here? What is the  distribution? As you know I&#8217;m interested in average versus worst-case hardness. This result sounds interesting if we know what &#8220;existence of a distribution&#8221; means in realistic terms. </p>\n<p>Best</p>\n<p>Nick</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Read:Nick.html\">Nick Read</a>", "published": "2014-03-14 00:43:29+00:00", "title": "By: Nick Read"}, {"content": "<![CDATA[<p>Nick: They generate a distribution that&#8217;s samplable in polynomial time, but crucially, the distribution that they generate <i>depends on the particular algorithm A that they&#8217;re fighting against.</i>  If you want to know more, I suggest reading their paper!  It&#8217;s been a while since I read it.</p>\n]]>", "author": "Scott", "published": "2014-03-14 01:21:54+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Scott &#8211; Gutfreund, Shaltiel, and Ta-Shma show that you can easily find instances that are hard for an algorithm A.   But, if I understand correctly, they don&#8217;t show that these easily-found instances are easy for some other algorithm B, right?   They could be hard in general, as best I can tell.</p>\n<p>Of course you have to be able to know A in order to choose B.  Because whatever B is, it could be incorporated into A.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuperberg:Greg.html\">Greg Kuperberg</a>", "published": "2014-03-14 02:28:21+00:00", "title": "By: Greg Kuperberg"}, {"content": "<![CDATA[<p>No, I take it back, I don&#8217;t understand correctly.   I will think about it some more.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuperberg:Greg.html\">Greg Kuperberg</a>", "published": "2014-03-14 02:29:16+00:00", "title": "By: Greg Kuperberg"}, {"content": "<![CDATA[<p>I will make a tangential biological comment, regarding your frog colour analogy. See page 11 of the famous paper of de Grouchy &#8220;Chromosome phylogenies of man, great apes, and old world monkeys&#8221; (or search for &#8220;grouchy great apes&#8221;!), specifically the last sentence quoted below.</p>\n<p><a href=\"http://link.springer.com/content/pdf/10.1007/BF00057436.pdf\u200e\" rel=\"nofollow\">http://link.springer.com/content/pdf/10.1007/BF00057436.pdf\u200e</a></p>\n<p>According to neo-Darwinism, gene mutations are the true means of evolution. By the combined action of geographical isolation and selection, they are supposed to be capable of producing such inter-specific barriers. In fact, such barriers appear to be extremely fragile. Gene mutations, even in great number, cannot prevent individuals from reproducing with their own kind. Neo-Darwinism confounds races and species. Races can differ greatly on morphological and genetic grounds, but they remain capable of reproducing between each other. The example of the seagulls (genus Larus) around the North Pole is remarkable. They are distributed in groups that differ only by the color of their irides and periorbicular regions, and which do not reproduce with each other. If the eyes of members of one group are painted in the colors of another group, members of the two groups immediately reproduce. A little paint is sufficient to break down reproductive barriers between groups which represent true species in the neo-Darwinian concept.</p>\n]]>", "author": "NotEasyBeingYellow", "published": "2014-03-14 03:17:59+00:00", "title": "By: NotEasyBeingYellow"}, {"content": "<![CDATA[<p>Greg #383: Well, it&#8217;s a funny situation.  Gutfreund et al.&#8217;s procedure, in the course of finding instances on which algorithm A fails, also learns the yes-or-no <i>answers</i> to those instances!  For</p>\n<p>1. If A finds an instance &phi; on which it itself fails, then the witness of its failure is a satisfying assignment for &phi; that A failed to find (but that the larger call to A <i>did</i> find).</p>\n<p>2. Conversely, if A <i>doesn&#8217;t</i> find such a &phi;, then we again have a specific instance on which A fails!</p>\n<p>So in a sense, Gutfreund et al.&#8217;s procedure is <i>itself</i> in the business of embarrassing A.  The one catch is that, in case 2 above, A is only &#8220;very mildly&#8221; embarrassed, since our knowledge that A failed on this particular instance is only as good as our <i>starting assumption</i> that every polynomial-time algorithm (so in particular, A) has to fail on <i>some</i> SAT instances.</p>\n]]>", "author": "Scott", "published": "2014-03-14 03:20:24+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Scott, </p>\n<p>We know that, if only P!=NP, then there are, not one, but many many many NPI problems. What would you object to: </p>\n<p>&#8220;Shnoods! We see close calls again and again. There is just not enough space for many NPI problems.&#8221;</p>\n<p>? <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/icon_smile.gif\" alt=\":-)\" class=\"wp-smiley\" /> </p>\n]]>", "author": "Jay", "published": "2014-03-14 03:26:56+00:00", "title": "By: Jay"}, {"content": "<![CDATA[<p>Jay #387: I&#8217;m not sure if you were expecting a serious reply to that, but actually there <i>is</i> one!  The borders of P and NPC should both be thought of tracing out complicated, jagged shapes in some huge number of dimensions.  So there&#8217;s more than enough room for the borders to touch in <i>some</i> regions of problem-space (say, the constraint satisfaction problems)&#8212;thereby creating a need for the electric fence in those regions&#8212;while still being very far apart in other regions of problem-space (say, the number-theoretic problems), leaving lots of room for all the NPI stuff in between.</p>\n]]>", "author": "Scott", "published": "2014-03-14 03:39:07+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>As you guessed my intent was to say that the electric fence argument seems weaker than say, the multiple-surprises argument. But.. hey you&#8217;ve just changed my mind. Thank you!</p>\n]]>", "author": "Jay", "published": "2014-03-14 03:57:49+00:00", "title": "By: Jay"}, {"content": "<![CDATA[<p><a href=\"http://www.scottaaronson.com/blog/?p=1720#comment-102326\" rel=\"nofollow\">Greg</a>:</p>\n<blockquote><p>With graph isomorphism, if you show the algorithm first, then people can find pairs of graphs that defeat it. But if you show the graphs first, then people can make an algorithm to quickly determine if they are the same.</p></blockquote>\n<p>Without disagreeing with your conclusion, I don&#8217;t think either of the specific claims are quite correct. According to <a href=\"http://www.scottaaronson.com/blog/?p=676#comment-24918\" rel=\"nofollow\">my notes</a>, there has only once been an instance found that was hard for the leading algorithm, namely Miyazaki&#8217;s instances were hard for McKay&#8217;s naughty. And it wasn&#8217;t easy to create a new algorithm; it was ten years later that Tener did so.</p>\n<p>Note that there is an asymmetry. If the pair of graphs in the hard instance is not isomorphic, you probably know that because you know a polynomial time invariant that distinguishes them and you can just add that to your algorithm. But if the graphs are isomorphic, as in Miyazaki&#8217;s example, it may be hard to modify the algorithm to quickly accept them without losing correctness.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Knight:Douglas.html\">Douglas Knight</a>", "published": "2014-03-14 04:01:08+00:00", "title": "By: Douglas Knight"}, {"content": "<![CDATA[<p>I regard the &#8220;electric-fence argument&#8221; for NP=!P as rather strong. However, one has to be careful: consider the hardness of &#8220;unique games&#8221; (or small set expansion). In various cases the &#8220;electric fence&#8221; for unique games is even more definite than it is for NP-hardness. I doubt if there is such a common consensus regarding the hardness of unique games.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kalai:Gil.html\">Gil Kalai</a>", "published": "2014-03-14 10:32:17+00:00", "title": "By: Gil Kalai"}, {"content": "<![CDATA[<p>Gil #391: How on earth is there a more definite &#8220;electric fence&#8221; separating unique games from P than there is separating the (known) NP-complete problems from P?  Could you explain what you mean by that?</p>\n]]>", "author": "Scott", "published": "2014-03-14 15:26:11+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Probably a dumb question&#8230; </p>\n<p>1) with a set of integers of size n, e.g. {-3,-2,5,8}, n=4<br />\nfrom that set we generate<br />\n2) all the possible subset sums (size 2^n)<br />\ne.g. {-5,-3,-2,0,2,3,5,6,8,10,11,13} (we omit repetitions)</p>\n<p>How hard is it to do the reverse? I.e. given a large set of numbers, find the minimum possible &#8220;generating&#8221; set for it.<br />\nA sort of compression scheme.</p>\n]]>", "author": "fred", "published": "2014-03-14 15:36:33+00:00", "title": "By: fred"}, {"content": "<![CDATA[<p>Scott: how about a short post explaining the current status of &#8220;unique games&#8221;? I gather it is related to my favorite problem, max cut.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Read:Nick.html\">Nick Read</a>", "published": "2014-03-14 15:41:14+00:00", "title": "By: Nick Read"}, {"content": "<![CDATA[<p>Scott #392</p>\n<p>The &#8220;unique games&#8221; fence:<br />\n<a href=\"http://tinyurl.com/kndxrav\" rel=\"nofollow\">http://tinyurl.com/kndxrav</a></p>\n<p>The P!=NP fence<br />\n<a href=\"http://tinyurl.com/mnh99nl\" rel=\"nofollow\">http://tinyurl.com/mnh99nl</a></p>\n]]>", "author": "fred", "published": "2014-03-14 15:42:47+00:00", "title": "By: fred"}, {"content": "<![CDATA[<p>Scott #346: Levin&#8217;s universal search can also certify the NO instances of problems in NP.  While it looks for a satisfying assignment it just also has to look for a proof that there is no satisfying assignment.  This will always terminate due to the obvious exponential-sized proof by checking all assignments.  If P=NP then P=co-NP and I think this means that NO instances should have short certificates, which universal search should also be able to find, along with a provably correct checking algorithm that (maybe not provably) runs in p-time, but I don&#8217;t see the proof of this (maybe I&#8217;m missing something obvious).  If that&#8217;s right, universal search can look for the triple (short certificate, fixed checking algorithm, fixed proof of checking algorithm) where the last two components are more &#8220;constant overhead&#8221;.</p>\n]]>", "author": "asdf", "published": "2014-03-14 18:12:31+00:00", "title": "By: asdf"}, {"content": "<![CDATA[<p>fred #395: LOL!</p>\n<p>Nick #394: Maybe I&#8217;ll ask Dana if she wants to write such a post.  She&#8217;s certainly better-qualified than me&#8230;</p>\n]]>", "author": "Scott", "published": "2014-03-14 18:47:20+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>asdf #396: Ah!  You raise a good point, but there&#8217;s still a subtle distinction between YES and NO instances with respect to Levin&#8217;s universal search.  Namely, for the YES instances, all you need is the <i>fact</i> of P=NP.  If that&#8217;s so, then Levin&#8217;s algorithm will eventually output a YES-witness for your satisfiable Boolean formula, and you won&#8217;t need to know anything about the reasons why P=NP to <i>recognize</i> it as a YES-witness.</p>\n<p>Now, since P=NP implies P=coNP, it&#8217;s also true (as you point out) that unsatisfiable Boolean formulas will always have succinct NO-witnesses.  However, it&#8217;s only the proof of P=NP that would tell you what the NO-witnesses look like, or how you should recognize them when Levin&#8217;s algorithm spits them out!  Or to say it another way: of the Turing machines you&#8217;re dovetailing over, one of them always satisfies satisfiable instances in polynomial time&#8212;so if it fails to satisfy an instance, then its failure constitutes the NO-witness you want.  But you won&#8217;t know which of TMs you&#8217;re dovetailing over <i>is</i> that one!  Even if a particular TM happens to find YES-witnesses extremely reliably in practice, that still doesn&#8217;t prove that it <i>always</i> finds them.</p>\n<p>Of course, as long as you&#8217;re dovetailing over all TMs, one could argue that you might as well <i>also</i> iterate over all P=NP proofs until you find a valid one, and thereby gain the ability to recognize NO-witnesses!  And that&#8217;s fine, assuming a P=NP proof <i>exists</i> in your favorite formal system (and isn&#8217;t ridiculously longer than the Turing machine it proves correct).  If, say, P=NP but the equality were unprovable in ZF set theory, then even if you used Levin&#8217;s universal search, you&#8217;d generally be S.O.L. in terms of finding NO-witnesses that you could actually be certain about.</p>\n]]>", "author": "Scott", "published": "2014-03-14 19:00:30+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Fred #393,</p>\n<p>I&#8217;d guess it&#8217;s easy using the following hints: the maximum is the sum of all positive integers, the minimum is the sum of all negative integers, the sum of all integers is the same as the sum of the maximum and minimum, every integer is in the subset.</p>\n]]>", "author": "Jay", "published": "2014-03-14 19:20:11+00:00", "title": "By: Jay"}, {"content": "<![CDATA[<p>Jay #399,<br />\nI feel stupid &#8211; it shouldn&#8217;t be too hard considering all the redundancy&#8230;<br />\nBut taking the example where n = 10 (for a total of 2^n=1024 numbers), say we&#8217;re told we have 5 positive ints (Pi) and 5 negative ints (Ni) (in practice we wouldn&#8217;t even know that), then:<br />\nN0+N1+N2+N3+N4+N5= min (1)<br />\nP0+P1+P2+P3+P4+P5= max (2)<br />\n(your third hint is just the sum of (1) and (2), so I don&#8217;t think it helps)<br />\nyour last hint says that all {Ni,Pi} are one of 1024 possible numbers, but if you have ~500 negative numbers, that&#8217;s a hell of a lot of combinations (C(500/5)).<br />\nAnyway I didn&#8217;t mean to derail the thread, I was wondering how NP-C problems (generated solutions) relate to searching a random set of solutions.</p>\n]]>", "author": "fred", "published": "2014-03-14 20:12:55+00:00", "title": "By: fred"}, {"content": "<![CDATA[<p>Jay #399</p>\n<p>I generated some quick test code, with 10 generating numbers<br />\n{-124, -56, -43, -12, -2, 46, 97, 1001, 1230, 1234}<br />\nyou get a list of 1024 elements (with repetitions) that is pretty &#8220;dense&#8221;:<br />\n[-237, -235, -225, -223, -194, -192, -191, -189, -182, -181, -180, ... -70, -68, -67, -65, -58, -57, -56, -55, -55, -53, -51, -49, -45, -43, ... -10, -9, -4, -2, -2, 1, 3, 5, 7, 17, 19, 27,... 3551, 3552, 3553, 3560, 3562, 3563, 3565, 3594, 3596, 3606, 3608]</p>\n]]>", "author": "fred", "published": "2014-03-14 20:55:29+00:00", "title": "By: fred"}, {"content": "<![CDATA[<p>Mikko #392:</p>\n<ul>do we have some kind of hypothesis about the nature of complexity of computational problems that would explain this rather interesting grouping of problems between P and NP.</ul>\n<p>Well, P and NP are far from the only interesting complexity classes!  There&#8217;s also BQP, PH, #P, PSPACE, and <a href=\"https://complexityzoo.uwaterloo.ca/Complexity_Zoo\" rel=\"nofollow\">much more</a>.  I&#8217;d say the centrality of P and NP is partly for historical reasons (and because most <i>other</i> complexity-class separation problems seem of comparable difficulty to P vs. NP anyway), but partly because <i>finding</i> a solution and <i>verifying</i> one (in deterministic polynomial time) really do seem like two of the simplest, most natural notions to care about!  And also, because so many practically-important problems turn out to be NP-complete (<a href=\"http://cstheory.stackexchange.com/questions/20930/why-are-so-few-natural-candidates-for-np-intermediate-status/20945#20945\" rel=\"nofollow\">see here</a> for my speculations about why).</p>\n]]>", "author": "Scott", "published": "2014-03-14 21:06:42+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Greg #378:<br />\nI vaguely remember reading somewhere (maybe in this post about the <a href=\"http://rjlipton.wordpress.com/2011/11/07/the-group-isomorphism-problem-a-possible-polymath-problem/\" rel=\"nofollow\">group isomorphism problem</a>) that the graph isomorphism problem instances derived from the group isomorphism problem for finite indecomposable p-groups (especially 2-groups) are good candidates for hard graph isomorphism problem instances.</p>\n]]>", "author": "gentzen", "published": "2014-03-14 21:12:59+00:00", "title": "By: gentzen"}, {"content": "<![CDATA[<p>Scott #399 oh yes, I see.  If P=NP then there is a polytime algorithm A that recognizes SAT, the succinct witness for this is the satisfying assignment, and the succinct witness for an UNSAT instance is an &#8220;instruction trace&#8221; of running the instance through algorithm A and having it not say &#8220;yes&#8221; within its time bound.  But this is useless if you don&#8217;t know what A is, or even if you do know what it is (i.e.. Levin search) but you don&#8217;t know its runtime&#8211;it very well might not be provable.  Is there something less non-constructive than that?  I remember P=NP => P=co-NP as a basic fact about NP-completeness, but I don&#8217;t see a more useful proof than the one above.</p>\n]]>", "author": "asdf", "published": "2014-03-14 21:39:19+00:00", "title": "By: asdf"}, {"content": "<![CDATA[<p>asdf #405: P=NP => P=coNP <i>is</i> a basic fact, trivial to prove.  But yes, if you only had a nonconstructive proof of it, it&#8217;s totally unclear to me how you&#8217;d exploit that to find the succinct proofs of unsatisfiability that formally exist.  But maybe we shouldn&#8217;t be so surprised!  After all, Levin&#8217;s observation that we <i>could</i> exploit a nonconstructive proof to get proofs of <i>satisfiability</i> in polynomial time, is a fact that&#8217;s as weird and surprising as it is useless, and probably we shouldn&#8217;t expect that sort of &#8220;leveraging of nonconstructive arguments into formally &#8216;constructive&#8217; ones&#8221; to be possible more generally.</p>\n]]>", "author": "Scott", "published": "2014-03-14 21:47:16+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Scott,</p>\n<p>I like the GP (Gravitational Pull) idea.</p>\n<p>My brief moment of thought on this issue is that Turing-complete is the &#8220;right definition&#8221;, which is why it has GP. A simpler example occurs with the real numbers, R. My take is this: </p>\n<p>The naturals, N, are either obvious or &#8220;god given&#8221; (someone said that; Kronecker?). From N, anyone doing simple math would come up with the integers, Z, and rationals, Q. When you start taking roots and doing calculus, it is clear that Q is not enough, but not obvious what to use. At least two usable generalizations (Dedskind cuts, Cauchy sequences) were constructed back in the day. A rite of passage for math grad students is showing the two resulting systems are equivalent. The resulting system is called the reals, R.</p>\n<p>I am not aware of any other generalization of Q,closed under the usual operations of calculus, that is NOT equivalent. Furthermore, one could come up with other constructions for R that ARE equivalent. </p>\n<p>Thus R can be considered the &#8220;right definition&#8221;, and it exerts GP on other constructions!</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Ohio:Raoul.html\">Raoul Ohio</a>", "published": "2014-03-14 22:30:19+00:00", "title": "By: Raoul Ohio"}, {"content": "<![CDATA[<p>Scott, two equivalence classes that surprised everyone by turning out to be equal: IP=PSPACE.  Does that count?</p>\n]]>", "author": "asdf", "published": "2014-03-15 02:15:58+00:00", "title": "By: asdf"}, {"content": "<![CDATA[<p>Raoul, the rules of calculus and the archimedian property of the reals don&#8217;t seem inevitable.  18th century calculus was done with infinitesimals and today, reals with infinitesimals can be formalized as nonstandard analysis.  It was trendy for a while as a potentially better way to teach calculus (no more delta-epsilon) among other things.</p>\n]]>", "author": "asdf", "published": "2014-03-15 02:20:25+00:00", "title": "By: asdf"}, {"content": "<![CDATA[<p>asdf #408: You also could&#8217;ve given the examples of PSPACE and NPSPACE, or PSPACE and QIP, or NL and coNL, or NC<sup>1</sup> and BWBP-5.  However, in not one of these cases would I say that there was <i>ever</i> an &#8220;electric fence&#8221; separating the two complexity classes.  I believe they were all proved to be equal not long after anyone started working on the questions at all (which, of course, doesn&#8217;t detract from how surprising and important the results were).  And unlike with P vs. NP, I don&#8217;t think there was ever (e.g.) a plethora of NC<sup>1</sup> problems for which people struggled to find width-5 branching programs but couldn&#8217;t, or for which they <i>almost</i> could but it always turned out not to work in the end, or anything like that.  At most, there were small picket fences separating these classes.  People &#8220;only&#8221; had to have the insight to <i>try</i> and step over these picket fences, and often within a year or less, they succeeded.</p>\n]]>", "author": "Scott", "published": "2014-03-15 04:19:22+00:00", "title": "By: Scott"}]