[{"content": "<![CDATA[<p>I&#8217;m in agreement with Janne #71. The way it&#8217;s written (a selling point) sounds like Bayesians value having a subjective prior that they know is vastly different from somebody else&#8217;s (and still feel right about their own conclusion).</p>\n<p>The appeal of Bayesianism that it recognizes that you cannot avoid subjective priors, which could be your own, or the only prior in the world available to everybody.</p>\n]]>", "author": "Koray", "published": "2015-08-18 22:47:36+00:00", "title": "By: Koray"}, {"content": "<![CDATA[<p>Marylin #73: In the scenario you amusingly describe, that doesn&#8217;t sound at all like an Aumannian agreement, but&#8212;to whatever extent there any factual questions at issue&#8212;more like agreeing to disagree. <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/simple-smile.png\" alt=\":-)\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" /></p>\n]]>", "author": "Scott", "published": "2015-08-18 23:02:23+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Janne #71 and Koray #75: OK, given that there&#8217;s not really any question of fact at issue, I&#8217;ll happily grant to my Bayesian friends that subjectivity is a weakness rather than a selling point of their approach. <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/simple-smile.png\" alt=\":-)\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" />  I was thinking, for example, of <a href=\"http://perimeterinstitute.ca/personal/cfuchs/\" rel=\"nofollow\">Chris Fuchs</a>, who often sings the praises of Bayesian subjectivity in the context of his QBist (Quantum Bayesian) approach to quantum mechanics.  But maybe a better way to put it would be: Bayesians consider it a selling point of their approach that it&#8217;s explicit about the <i>unavoidability</i> of subjective starting beliefs (as encoded in the prior), rather than trying to sweep that under the rug.</p>\n]]>", "author": "Scott", "published": "2015-08-18 23:08:36+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Sort of relevant, all this talk of rationality reminded me of  this gem from writings of Seth Lloyd:<br />\n&#8220;Ironically, it is customary to assign our own predictable behavior and that of other humans to rationality: we&#8217;re we to behave rationally, we reason, the world would be more predictable. In fact, it is just when we behave rationally, moving logically, like a computer, from step to step, that our behavior becomes provably unpredictable. Rationality combines with self-reference to make our actions intrinsically paradoxical and uncertain.&#8221;</p>\n]]>", "author": "Nick", "published": "2015-08-19 00:03:34+00:00", "title": "By: Nick"}, {"content": "<![CDATA[<p>Nick #78: That statement seems to require some pretty big asterisks&#8230;</p>\n<p>(1) <i>Some</i> computations are trivial to predict.<br />\n(2) &#8220;Behaving rationally&#8221; is not at all the same thing as &#8220;behaving like a computer.&#8221;  A computer can make arbitrarily &#8220;irrational&#8221; decisions if you program it to.<br />\n(3) Sure, there exist mathematically well-defined problems (e.g., predicting the long-time behavior of another program) that provably take a long time to solve, but that doesn&#8217;t mean mathematically ill-defined problems are <i>easier</i> to solve! <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/simple-smile.png\" alt=\":-)\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" /></p>\n]]>", "author": "Scott", "published": "2015-08-19 03:01:28+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Thanks Scott, I suddenly don&#8217;t understand it again! The bottom graphs represent the agents modeling each other? Ah I see, it says so right there in the words. This makes more sense now.</p>\n]]>", "author": "Martin-2", "published": "2015-08-19 07:39:52+00:00", "title": "By: Martin-2"}, {"content": "<![CDATA[<p>Nice read! Scott, is any of this inspired by the short exchange we had regarding courage and homophobia?</p>\n]]>", "author": "Jona", "published": "2015-08-19 16:52:40+00:00", "title": "By: Jona"}, {"content": "<![CDATA[<p>Don&#8217;t the things we disagree about generally derive from differences of values (utility functions) rather than expectations (priors)?</p>\n<p>From there, disagreement over expectations can arise because of hidden priors, hidden motivations, deception (and meta-deception) in service of conflicting values. For example, if a child wanted to short-circuit this stupid game, he could just stand up in round 98.</p>\n<p>I could just be completely wrong, point me to the part of the article that would explain why.</p>\n]]>", "author": "leibniz", "published": "2015-08-19 22:10:34+00:00", "title": "By: leibniz"}, {"content": "<![CDATA[<p>Jona #81: I&#8217;ve been interested in Aumann&#8217;s theorem since I learned about it in 2003.  But while I wasn&#8217;t motivated by any single exchange (and I don&#8217;t even remember the one you mention), I can say that my interest in the questions of how to get rational people to see each other&#8217;s perspectives, and how fundamental disagreements between them can indefinitely persist, were certainly stimulated further by my recent blog experiences.</p>\n]]>", "author": "Scott", "published": "2015-08-20 02:34:19+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>leibniz #82: If differences in moral values lead people to different <i>factual</i> beliefs (i.e., expectations over future experiences), then yes, dishonesty or irrationality (or self-deception, or call it whatever you like) are probably in play&#8212;since presumably we agree that the world is what it is, with how it <i>ought</i> to be having no causal power over its state.  So yes, as the post said, one might be able to account for Aumann&#8217;s theorem&#8217;s failure to describe real people on that ground alone.  But as the post also said, in that case Aumann&#8217;s theorem would still have <i>aspirational</i> significance: it would still describe what our disagreements <i>ought</i> to look like, if we fancy ourselves honest and rational.</p>\n]]>", "author": "Scott", "published": "2015-08-20 02:41:51+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Scott #83: Sorry, I was a bit naive about the amount of discussion you do here <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/simple-smile.png\" alt=\":)\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" /></p>\n<p>What I meant was; here you said,<br />\n&#8220;even if you know that 90% of the populace will join your democratic revolt provided they themselves know 90% will join it, if you can\u2019t make your revolt\u2019s popularity common knowledge, everyone will be stuck second-guessing each other, worried that if they revolt they\u2019ll be an easily-crushed minority.  And because of that very worry, they\u2019ll be correct!&#8221;<br />\nAnd a while back you said, &#8220;it\u2019s more valuable to speak a moral truth that no one in your social circle recognizes than one that everyone in your circle recognize\u201d, and with this later part I disagreed, basically pointing out how sometimes, it could be more important to establish common ground and consensus. Even called you &#8220;phallocentric&#8221; over it.</p>\n]]>", "author": "Jona", "published": "2015-08-20 14:22:59+00:00", "title": "By: Jona"}, {"content": "<![CDATA[<p>How robust is the Muddy Children puzzle to uncertainty? Suppose that it is common knowledge that every child assigns probability 0.5 to having mud on her face, believe the teacher&#8217;s statement to be truthful with probability \\(0.5 + \\epsilon\\), and will stand up once her probability of having mud on her faces exceeds \\( 1 &#8211; \\delta \\). When if ever do the children stand up?</p>\n]]>", "author": "lds951", "published": "2015-08-20 15:25:31+00:00", "title": "By: lds951"}, {"content": "<![CDATA[<p>Hi Scott,<br />\nA few months ago you had a very interesting post about Eigenmorality and a Prisoner Dilemma software competition under Eigenjesus and Eigenmoses criteria.<br />\nI think it would be interesting to have another run of Prisoner Dilemma competition under the Common Knowledge scenario. That is, make source code of all participating programs available to all other programs so that they could make decision not only on the previous history but also on the analysis of the opponent&#8217;s algorithm. That could be an interesting AI challenge: analyze a program that analyzes you analyzing it. <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/simple-smile.png\" alt=\":-)\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" /></p>\n]]>", "author": "Michael P", "published": "2015-08-20 16:46:25+00:00", "title": "By: Michael P"}, {"content": "<![CDATA[<p>Hi Scott-</p>\n<p>I think there are some other options for escaping Aumann&#8217;s theorem that I didn&#8217;t see on a quick read of your post above: (1) Deny that the intuitive notion of &#8220;public information&#8221; (ie common knowledge) actually corresponds to the technical notion of common knowledge. (2) Allow that people sometimes update on false propositions. (3) Allow that people aren&#8217;t perfectly introspective &#8211; sometimes they know things but don&#8217;t know that they know them. </p>\n<p>I go through the details of how (2) and (3) allow escape from Aumann&#8217;s theorem in a paper that recently came out in the Review of Symbolic Logic. [In my own view (1) is also very important, but it&#8217;s obvious how that would work formally. Actually I&#8217;m inclined to think we don&#8217;t have common knowledge in the technical sense.] The paper&#8217;s here:<br />\n<a href=\"http://users.ox.ac.uk/~hert2388/People%20with%20Common%20Priors%20Can%20Agree%20to%20Disagree.pdf\" rel=\"nofollow\">http://users.ox.ac.uk/~hert2388/People%20with%20Common%20Priors%20Can%20Agree%20to%20Disagree.pdf</a> </p>\n<p>The math is elementary, and you can cut through the introductory stuff to the pictures representing the models, Unfortunately these additional ways of resisting the result don&#8217;t seem to have been sufficiently well-recognized. </p>\n<p>(I also show that you only need to relax the assumption that it&#8217;s common knowledge that any of the relevant assumptions hold. Once you see this, I don&#8217;t think Aumann&#8217;s result even puts that much pressure on a very strong form of Bayesianism, as I say in the paper.)</p>\n<p>Harvey</p>\n]]>", "author": "Harvey Lederman", "published": "2015-08-20 20:13:36+00:00", "title": "By: Harvey Lederman"}, {"content": "<![CDATA[<p>[&#8230;] <a href=\"http://www.scottaaronson.com/blog/?p=2410\" rel=\"nofollow\">http://www.scottaaronson.com/blog/?p=2410</a> [&#8230;]</p>\n]]>", "author": "Common Knowledge | Timothy Xu", "published": "2015-08-20 21:02:23+00:00", "title": "By: Common Knowledge | Timothy Xu"}, {"content": "<![CDATA[<p>Michael P #87: Some people at MIRI actually ran such a tournament; they called it &#8220;Modal Kombat.&#8221;  You can <a href=\"http://arxiv.org/abs/1401.5577\" rel=\"nofollow\">read the paper here</a>.</p>\n]]>", "author": "Scott", "published": "2015-08-20 23:13:29+00:00", "title": "By: Scott"}]