[{"content": "<![CDATA[<p>grue: But Wadhwa <i>did</i> make the further, smoking-gun type error (of course, encouraged by Simmons, who I&#8217;ve been happy to blame as well).  And the kid in my and Zach&#8217;s comic also made the further error.</p>\n<p>For me, if a way of talking about an issue leads with wearying predictability to a certain verifiable error&#8212;and having been explaining QC to popular audiences for ~20 years, I&#8217;ve seen pretty much every path to every misconception that it&#8217;s possible to take&#8212;then that way of talking is itself an error to be warned against.  But I agree, the way of talking itself doesn&#8217;t carry the same degree of culpability as the further &#8220;smoking-gun&#8221; errors that it almost inevitably leads to.</p>\n]]>", "author": "Scott", "published": "2018-02-17 22:31:55+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>&#8220;For me, if a way of talking about an issue leads with wearying predictability to a certain verifiable error\u2014and having been explaining QC to popular audiences for ~20 years, I\u2019ve seen pretty much every path to every misconception that it\u2019s possible to take\u2014then that way of talking is itself an error to be warned against.&#8221;</p>\n<p>Well warning against a particular wording as pedagogical point is one thing, but this blog entry and the cartoon both go further, basically calling true statements false/wrong (i.e. qbits are in two states at once and compute in parallel); I would certainly object to this, even if your goal is to reduce the likelihood of some other more serious mistake (e.g. all NP-complete problems can solved efficiently with QCs).</p>\n]]>", "author": "grue", "published": "2018-02-17 22:49:15+00:00", "title": "By: grue"}, {"content": "<![CDATA[<p>grue #143: No, the true statement is that a qubit can be in a <b>superposition</b> of a 0 state and a 1 state.  Saying that it can be in &#8220;both states at once&#8221; is a popular rounding down of that true statement, which would be okay if it didn&#8217;t regularly lead to egregious errors, but it <i>does</i> regularly lead to egregious errors, as we saw in this case (which makes it a terrible test case for your broader point! \ud83d\ude42 ).</p>\n]]>", "author": "Scott", "published": "2018-02-17 22:57:53+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Whether it is in &#8220;superposition&#8221; is a formal claim that all interpretations can agree on.  That it really is in some sense in both states at once is a matter of philosophical interpretation about what it means to be in a superposition, and some interpretations would agree and some wouldn&#8217;t; but it&#8217;s not straightforwardly false, nor is it simply a rounding down.  For instance, I suspect most many-worlds theorists would disagree with you on this point.  </p>\n<p>Also, saying the qbits aren&#8217;t computing in parallel is even more straightforwardly incorrect.</p>\n]]>", "author": "grue", "published": "2018-02-17 23:06:27+00:00", "title": "By: grue"}, {"content": "<![CDATA[<p>Nope, same thing about computing in parallel.  The true statement is that a QC can do many computations in superposition.  Saying it can do the computations in &#8220;parallel&#8221; is a popular rounding down that, again, would be fine if it didn&#8217;t lead to egregious errors, but it does lead to egregious errors.</p>\n<p>My personal experience has been that many-worlders discuss quantum computations the same way anyone else in our field does.  But if a case arose where they were encouraging popular misunderstandings (e.g. QC as massively parallel classical computer), I&#8217;d criticize that the same way I&#8217;d criticize it from anyone else.</p>\n]]>", "author": "Scott", "published": "2018-02-17 23:14:12+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Now you&#8217;re just arguing semantics.  For the most part there&#8217;s little need to engage in philosophical interpretation when doing scientific work in QM/QC, so it&#8217;s to be expected that supporters of different interpretations talk similarly to each other in that context (i.e. focusing on the formalism e.g. superpositions, etc).  That doesn&#8217;t mean they would agree with you on these philosophical interpretive questions (in my experience many in the field are happy to defend these types of claims).  But it&#8217;s hard to know why you see these as rounding-down since you didn&#8217;t respond to my substantive points.</p>\n]]>", "author": "grue", "published": "2018-02-17 23:23:54+00:00", "title": "By: grue"}, {"content": "<![CDATA[<p>Ok, we&#8217;re done arguing here.  I don&#8217;t care that much about differences in semantics or philosophy as long as they don&#8217;t lead people into egregious errors.  But we&#8217;re talking here about situations where they <i>have</i> led people into egregious errors&#8212;errors that basically take everything that&#8217;s been figured out about the capabilities and limitations of quantum computers after a quarter-century of effort and struggle, throw it into a garbage can, and set it on fire.  For me, that&#8217;s the main substantive point here.  And unless you acknowledge the reality of this, I don&#8217;t see that there&#8217;s anything further to discuss.</p>\n]]>", "author": "Scott", "published": "2018-02-17 23:41:51+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Ironically, the (common) mistaken idea that Quantum Computers are doing &#8220;Parallel Computing&#8221; comes from a naive idea that Quantum Computers are executing in discrete time steps just like a classical computer! When in fact, if QCs were subject to discrete time evolution of the wavefunction they would be severely impacted in performance (compared to a continuous evolving wave-function).</p>\n<p>I think that would be the simple thing to emphasise &#8211; classical computers operate in discrete time steps, Quantum Computers use an algorithm based on continuous time evolution of the wave-function.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gallagher:James.html\">James Gallagher</a>", "published": "2018-02-18 00:50:21+00:00", "title": "By: James Gallagher"}, {"content": "<![CDATA[<p>James #149: Nope, that&#8217;s wrong.  The performance hit that you get from taking a continuous-time Hamiltonian, even one with a sum of many 2-qubit terms, and &#8220;Trotterizing&#8221; it to get a discrete sequence of 2-qubit unitary transformations, is quite small: like, logarithmic or so, depending on the desired error.  And most quantum algorithms are just thought about directly in the discrete-time setting.  This has nothing to do with the QC&#8217;s performance advantage, which is all about being able to choreograph interference patterns in a Hilbert space of exponentially large dimension.</p>\n]]>", "author": "Scott", "published": "2018-02-18 02:27:35+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Scott #150,</p>\n<p>Huh, that&#8217;s a very interesting point, and probably should be pointed out more often to people who think that quantum computers are somehow &#8220;just analog computer.&#8221;</p>\n<p>I guess that point never occurred to me because I only think of these things in discrete time steps because it minimizes how much actual physics I need to think about, where continuous stuff feels more physicsy.</p>\n]]>", "author": "Joshua Zelinsky", "published": "2018-02-18 03:12:03+00:00", "title": "By: Joshua Zelinsky"}, {"content": "<![CDATA[<p>Hi guys, regarding the 10-20 qubits in my interview what I say is actually very simple. For the experiments based on  random circuits,  if we measure the correlation between the desired distribution and the noisy distribution for 10-20 qubits, we can *extrapolate* and estimate the point of failure. (I also expect that the correlation will behave like the correlation computed for BosonSampling in my paper with Guy Kindler.) You can read about it in my ICM2018 paper <a href=\"https://arxiv.org/abs/1801.02602\" rel=\"nofollow\">https://arxiv.org/abs/1801.02602</a> .</p>\n<p>(It is true that I&#8217;d guess that the point of failure is closer to 20 than to 50 but this is just a guess.)</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kalai:Gil.html\">Gil Kalai</a>", "published": "2018-02-18 06:46:47+00:00", "title": "By: Gil Kalai"}, {"content": "<![CDATA[<p>@scott #148 &#8220;I don\u2019t care that much about differences in semantics or philosophy as long as they don\u2019t lead people into egregious errors.&#8221;</p>\n<p>That&#8217;s fine, but the problem is that you *are* in fact making strong philosophical claims, e.g. when you dismiss someone as wrong/mistaken for merely suggesting that qbits can be in more than one state at once.  This is not false, and in fact it&#8217;s a widely accepted philosophical interpretation of what it *means* for particles to be in a superposition.  And when people hear these claims from a QM expert they are mislead into thinking they are hearing scientific facts, as opposed to the philosophical opinions of someone who doesn&#8217;t even &#8220;care that much&#8221; about differences in philosophy.</p>\n<p>&#8220;But we\u2019re talking here about situations where they have led people into egregious errors\u2014errors that basically take everything that\u2019s been figured out about the capabilities and limitations of quantum computers after a quarter-century of effort and struggle, throw it into a garbage can, and set it on fire.  For me, that\u2019s the main substantive point here. And unless you acknowledge the reality of this, I don\u2019t see that there\u2019s anything further to discuss.&#8221;</p>\n<p>Honestly I think this is a bit exaggerated, but it certainly is a common misconception that QC algorithms can speed up any problem just by running all the options in parallel, and it&#8217;s great that you&#8217;re helping to clear up this misconception.  I just don&#8217;t think that there&#8217;s any need to make false claims in the process, even if those claims are &#8220;merely&#8221; philosophical.  It&#8217;s perfectly possible to accept that qbits can be in more than one quantum state at a time (and can compute in parallel), and go on to explain the difficulties in extracting the final computational result from a quantum system.  </p>\n<p>So I&#8217;m not denying your substantive problem.  I just think you are going too far in the other direction and calling reasonable claims false, apparently motivated in part by the fact that you see these claims as instrumentally harmful, which is surely a dangerous basis for assessing truth.</p>\n<p>Anyway, I appreciate your taking the time to respond to so many of my comments/questions and hope there&#8217;s no hard feelings.</p>\n]]>", "author": "grue", "published": "2018-02-18 07:37:27+00:00", "title": "By: grue"}, {"content": "<![CDATA[<blockquote><p>It at least includes understanding that the containment NP\u2286BQP is not established, which Wadhwa clearly didn\u2019t (and apparently still doesn\u2019t, even after like 20 different commenters tried to tell him\u2026).</p></blockquote>\n<p>That distinction is not that accessible. I know i&#8217;m still trying to figure out some of the details.</p>\n<p>Like, apparently people want to use QCs to solve NP-Hard optimization problems in the context of machine learning.</p>\n<p>Why should that be possible, and what does it mean? Are they strictly going for the Grover speedup, or something more?</p>\n<p>I honestly don&#8217;t feel like i can blame a reporter for getting this stuff wrong when i&#8217;m still trying to figure it out and there are people like Simmons reinforcing the misconception.</p>\n<p>Also, i think that reaching out to Vivek privately in a more diplomatic way would have been more productive.</p>\n]]>", "author": "Job", "published": "2018-02-18 09:10:48+00:00", "title": "By: Job"}, {"content": "<![CDATA[<p>#grue147 The main criticism of the article is that it portrays, argues, promotes and ultimately embeds the idea that $NP \\subseteq BQP$.  In a sense all else is peripheral since if true a fully fault-tolerant universal quantum computer would have earth-shattering consequences (modulo some unlikely constants). These consequences would likely go beyond any previous technology ever introduced and certainly well beyond the immediate cryptographic implications (erroneously) drawn by the article from pending experiments. But of course the consensus is that $NP \\not\\subseteq BQP$.</p>\n<p>So in an article explicitly designed to educate, excite or warn the public about what quantum computers can do, the egregiousness lies, not in any philosophical interpretations of parallelism but instead from showcasing an example (TSP) that treats $NP \\subseteq BQP$ as fact and hence also treats as fact the earth-shattering transformation awaiting the construction of the world\u2019s first universal quantum computer (when or if it happens there should be enough to get excited about!). </p>\n<p>I agree that this could have been misinterpreted from Michelle Simmon\u2019s TED talk and its juxtaposition of parallelism and the TSP and that this accounts for at least part of Vivek\u2019s article. See also the following from Michelle\u2019s Australia Day <a href=\"https://www.australiaday.com.au/events/australia-day-address/2017-speaker-professor-michelle-y-simmons/\" rel=\"nofollow\"> acceptance speech</a></p>\n<p><blockquote </p>\n<p>\u2026</p>\n<p>Such a computer is called a \u2018quantum computer\u2019 and is predicted to bring with it an exponential speed up in computational power. This is because instead of performing calculations one after the other like a conventional computer, a quantum computer works in parallel, looking at all the possible outcomes at the same time. The result is massively parallel computing, allowing us to solve problems in minutes that otherwise would take thousands of years.<br />\n\u2026.</p>\n<p>I want Australians above all to be known as people who do the hard things</p>\n<p>I wonder about the consensus on the above?</p>\n<p>I guess there is a fine line between aiming to inspire investors/readers/grant-assessors while also tempering with likely boundaries from hard-won theoretical knowledge. I\u2019m not sure where this line should be drawn but it seems reasonable that it should be consistently applied with and to authority.</p>\n]]>", "author": "Ron Monson", "published": "2018-02-18 09:50:42+00:00", "title": "By: Ron Monson"}, {"content": "<![CDATA[<p>Scott re #150: that&#8217;s what I&#8217;d guess too, but do we know that?  So if the solution to a Navier-Stokes equation can blow up with finite energy, which we can&#8217;t exclude yet, then that requires continuous space too, not only continuous time?</p>\n]]>", "author": "jonas", "published": "2018-02-18 12:29:17+00:00", "title": "By: jonas"}, {"content": "<![CDATA[<p>Job #154: I don&#8217;t agree with you that it&#8217;s a subtle or abstruse point.</p>\n<p>No one knows how to use quantum computers to get exponential speedups for NP-hard problems.</p>\n<p>No one knows how to use quantum computers to get exponential speedups for NP-hard problems.</p>\n<p>No one knows how to use quantum computers to get exponential speedups for NP-hard problems.</p>\n<p>(see also: the tagline of this blog \ud83d\ude42 )</p>\n<p>The trouble with this statement is not that it&#8217;s subtle or abstruse, or that there&#8217;s any informed doubt about its truth&#8212;it&#8217;s simply that people have strong emotional reasons not to want to accept it.  For one thing, if true, it would mean that 20 years of popular discussion of quantum computing has been shot through with irresponsible hype and error.  (Yep.)  For the QC skeptics, it would mean that QC is <i>not</i> a magic uber-computer&#8212;that it has a profile of abilities so strange that no sci-fi writer would have had the imagination to invent it&#8212;so maybe we have no prior intuition to tell us that such a thing is impossible.  (Yep.)  Finally, it would mean that understanding QC would actually require learning something about <i>complexity</i>&#8212;e.g., about the difference between NP-hard problems, and problems like factoring that are conjectured to be exponentially hard but <i>without</i> being NP-hard&#8212;rather than mentally collapsing all these things into a single category, and treating the distinctions between them as just irrelevant babbling.  (Yep.)</p>\n<p>Regarding the people who hope to get quantum speedups for NP-hard optimization problems arising in machine learning, I&#8217;d say there&#8217;s a trichotomy.  Either</p>\n<p>(1) they&#8217;re only hoping for a Grover speedup, which we almost certainly <b>can</b> get, or</p>\n<p>(2) they&#8217;re hoping to get more than the Grover speedup over the best known classical algorithms, at least for <i>special instances</i> of NP-hard problems, by using heuristic algorithms like the adiabatic algorithm or QAOA&#8212;something that we can&#8217;t rule out, and that&#8217;s reasonable to think about (and even try to test with the small QCs of the near future) but which is mostly just a hope at present, or</p>\n<p>(3) they&#8217;re engaging in irresponsible hype or have fooled themselves.</p>\n<p>I&#8217;ve seen many examples of all three.  Of course combinations are also possible, e.g. when someone is at (3) but falls back on (2) or (1) when challenged.</p>\n<p>Regarding contacting Wadhwa in private, I could&#8217;ve said the same!  I.e., he could&#8217;ve contacted <i>me</i> in private rather than publishing totally irresponsible statements in the <i>Washington Post</i>.  Even though I&#8217;m on a 1-year journalist sabbatical, I would&#8217;ve gladly put him in touch with a half-dozen colleagues.</p>\n]]>", "author": "Scott", "published": "2018-02-18 13:45:49+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>jonas #156: Yeah, PDEs that require both continuous time <i>and continuous space</i> to simulate are a different animal.  There are even situations where you can build a &#8220;Zeno computer&#8221; using such a PDE, able for example to solve the halting problem in finite time&#8212;indeed, Terry Tao has an ambitious program aimed at proving that Navier-Stokes might be an example of such a PDE.</p>\n<p>(If so, then we&#8217;d of course say that such PDEs are being continued way beyond where they have anything to say about real physics: it won&#8217;t take long until the classical fluctuations being talked about get smaller than the radius of an atom, or smaller than the Planck scale!  But still fun to think about mathematically.)</p>\n<p>Even for computations that are continuous in time but discrete in space (e.g., a sum of nearest-neighbor Hamiltonians acting on an array of qubits), I didn&#8217;t mention it before, but the <i>total energy</i> of the Hamiltonian enters into the time needed to simulate the computation using discrete gates.  Indeed, this <i>has</i> to be true: if it weren&#8217;t, then we could speed up any computation arbitrarily by just replacing the computer&#8217;s Hamiltonian H by 1000*H, or 10<sup>100</sup>*H, or whatever.  In real life, though, we <i>are</i> limited in how much energy we can pump into our computer, so the Church-Turing Thesis is upheld.</p>\n]]>", "author": "Scott", "published": "2018-02-18 13:57:17+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>More references to TSP at <a href=\"http://news.mit.edu/2017/scientists-demonstrate-one-largest-quantum-simulators-yet-51-atoms-1129\" rel=\"nofollow\">http://news.mit.edu/2017/scientists-demonstrate-one-largest-quantum-simulators-yet-51-atoms-1129</a>, though in the context of adiabatic QC. I\u2019m not an expert, but also this sounded a bit misleading in some places, possibly confirming a trend suggested by an earlier comment here.</p>\n]]>", "author": "sandro", "published": "2018-02-18 15:19:18+00:00", "title": "By: sandro"}, {"content": "<![CDATA[<p>>(JR) It says \u201cthis solution\u201d which is important. There are better algorithms than the direct, try-all-possibilities solution.</p>\n<p>Thanks JR.    So when n=22 cities, then  (2^n) x (n^2) is not too big, but by n= 60 cities, exact solutions to TSP might take too long on modern computers? </p>\n<p>If so, maybe it is not such a hanging offense to conflate 22 with 60, since both  n! and 2^n get unreasonably big unreasonably quickly. </p>\n<p>Regarding inequalities involving n!, 2^n ,60, 22, and the WaPo, perhaps the one whose (potential) truth is of greatest importance is  2018 > 2017.</p>\n]]>", "author": "Paul Fabel", "published": "2018-02-18 15:30:09+00:00", "title": "By: Paul Fabel"}, {"content": "<![CDATA[<blockquote><p>they\u2019re hoping to get more than the Grover speedup over the best known classical algorithms, at least for special instances of NP-hard problems,</p></blockquote>\n<p>That&#8217;s almost certainly the case. You&#8217;ll always get a substantial speedup if you target only a subset of instances, in most cases because the problem won&#8217;t be NP-Hard anymore.</p>\n<p>That&#8217;s why it&#8217;s so irrelevant when people talk about a &#8220;partial solution&#8221; to TSP.</p>\n<p>The point i&#8217;d like to understand is why should the constrained subset of instances solvable by a QC align with machine learning more than say the subset of instances with property X, where X enables an efficient classical solution.</p>\n<p>Other than it being twice the size.</p>\n]]>", "author": "Job", "published": "2018-02-18 15:48:06+00:00", "title": "By: Job"}, {"content": "<![CDATA[<p>Scott #130:</p>\n<blockquote><p> On the other hand, it\u2019s true that, if you want the harmonic oscillator to behave the exact way you know and love (something that some physicists might even consider conceptually prior to QM\u2026), then the amplitudes better be complex! Does anyone else have thoughts?</p></blockquote>\n<p>Here is one thought: If you take the &#8220;simple&#8221; Schr\u00f6dinger equation (instead of the relativistically &#8220;more accurate&#8221; Dirac equation or QFT), then the speed of rotation in the complex plane is directly proportional to the energy. But the energy for Schr\u00f6dinger&#8217;s equation is not absolute, since a potential with an arbitrary constant offset still leads to the same physics. And luckily, only the difference in speed of rotation between different modes is relevant for the resulting physics.</p>\n<p>But this &#8220;relativism&#8221; only works if we have the imaginary unit in the Schr\u00f6dinger equation. If that imaginary unit is absent like in the Klein Gordon equation, then we get solutions with positive and negative energy, energy becomes more absolute (i.e. less relative), and we have to fight to explain away the solutions with negative energy. On the other hand, only a derivative of the potential enters into the Klein Gordon equation, so things don&#8217;t break down immediately.</p>\n<p>The background of that observation is that I failed/skipped QM at university, and only came back to it after extensive experience in statistical optics. Funnily, one main reference work was from Max Born and Emil Wolf, and it introduced me to an instrumentalistic position which made dealing with QM much easier for me. I noticed that constructing <a href=\"https://physics.stackexchange.com/questions/134395/how-to-find-optical-toy-models-of-entangled-quantum-mechanical-systems\" rel=\"nofollow\">optical toy models</a> which exhibit some features of QM (even entanglement) is easy, but that my toy models only featured absolute energy instead of relative energy. (Which is not surprising, because the imaginary unit is absent from the equations of optics.)</p>\n]]>", "author": "gentzen", "published": "2018-02-18 15:59:00+00:00", "title": "By: gentzen"}, {"content": "<![CDATA[<p>Job #161:</p>\n<ul>The point i\u2019d like to understand is why should the constrained subset of instances solvable by a QC align with machine learning more than say the subset of instances with property X, where X enables an efficient classical solution.</ul>\n<p>A priori, you could simply hope that there&#8217;s property X that enables an efficient classical solution, and property Y that enables an efficient quantum solution, and Y is strictly broader than X &#8230; since there&#8217;s no reason for it not to be!  But then of course you can wonder: how much broader?  And: broader in a direction that anyone cares about in practice?</p>\n<p>A posteriori, we know that quantum speedups are sometimes obtainable by using quantum tunneling to get over tall, narrow energy barriers.  But is that an effect that really matters for real-world optimization problems?  I don&#8217;t think we know the answer to that yet, and we might not know until we have real QCs to test things out on.</p>\n]]>", "author": "Scott", "published": "2018-02-18 16:22:53+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>asdf says: &#8220;Is anyone saying where these complex amplitudes in quantum states actually come from? Surely physics doesn\u2019t care that C is the algebraic closure of the reals so all polynomials have roots there.&#8221;</p>\n<p>This is like saying &#8220;How can the four color theorem be true? Surely geometry does not care about solutions to x^2 = 2^x&#8221;.</p>\n<p>A better understanding is that C is the right set of numbers for many things (including QM), and C was first discovered by considering roots to polynomials.</p>\n]]>", "author": "Raoul Ohio", "published": "2018-02-18 16:37:18+00:00", "title": "By: Raoul Ohio"}, {"content": "<![CDATA[<p>Scott #135.</p>\n<p>Excellent point:</p>\n<p>&#8220;In their worldview, questions about experiments, or progress toward building QCs, belong to the ordinary category of \u201ctruth\u201d and \u201cfalsehood,\u201d but questions about what you could do with QCs belong to a totally different epistemic realm, where you can just say whatever you want, or whatever your audience or your investor or your funding agency most wants to hear. &#8221;</p>\n<p>Here is a related phenomena. Although few people in TCS believe there is much likelihood that P = NP, they are often asked by journalists and others &#8220;OK, it would be a great theoretical result, but what practical benefits would come from P = NP?&#8221;. Rather than giving the obvious answer of &#8220;I have no clue&#8221;, many instead speculate on all sorts of things that would follow if NP were in FC. Here FC is loosely defined as the &#8220;set of problems that can be feasibly computed,  say O(n^k) for a low value of k, such as 5 or 6&#8221;. </p>\n<p>But P is not in FC, so NP is not in FC irrespective of P = NP or not.</p>\n<p>A common counter argument, that P is in FC, goes like this: Whenever a problem is shown to be in O(n^k) for a big k, pretty soon the result is improved to something like k = 6 or less. </p>\n<p>This is interesting, but so what? It could easily be the case that the only problems anyone has figured out any O(n^k) bounds for are actually easy ones, where the best k are 6 or so.</p>\n]]>", "author": "Raoul Ohio", "published": "2018-02-18 17:33:46+00:00", "title": "By: Raoul Ohio"}, {"content": "<![CDATA[<p>Scott #150</p>\n<p>Yes, but it does effect the size of the Hilbert Space if <i>the whole universe</i> is evolving discretely.</p>\n<p>Your QC algorithms will not work so efficiently as you think if this is the case. If the evolution was pure deterministic, then QC won&#8217;t work at all, but if it&#8217;s random seeded discrete time steps then QC will work OK up to a certain number of qubits, and will begin to fail statistically as the non-infinite size of the Hilbert Space begins to impact on the algorithm. This will vary for different algorithms.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gallagher:James.html\">James Gallagher</a>", "published": "2018-02-18 20:55:59+00:00", "title": "By: James Gallagher"}, {"content": "<![CDATA[<p>James #166: Sorry, I didn&#8217;t understand that in the slightest.  For all we know, time could be discrete at the Planck scale, and no quantum computation that anyone is planning to do in any foreseeable future&#8212;anything below the Planck energy, basically&#8212;would be sensitive to that discreteness at all, any more than a classical computation would be.  We only <i>wish</i> it were that easy to do a terrestrial experiment that would be sensitive to quantum gravity!</p>\n]]>", "author": "Scott", "published": "2018-02-18 21:29:37+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Scott #167</p>\n<p>Why wouldn&#8217;t a quantum computation be sensitive if the Hilbert Space is reduced? You just seem to think I&#8217;m saying a &#8220;Many-Worlds&#8221; with discrete-time branching &#8211; I&#8217;m not, I&#8217;m saying a random jump (microscopically, anywhere in the universe) causing the entire universe to then rotate (evolve unitarily in Hilbert Space) say, on average, every ~planck-time.</p>\n<p>Mathematically speaking, we have a structure with large Hilbert Space and superpositions, but in reality only <i>one real evolution</i> and, QC would struggle once qubit numbers get high</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gallagher:James.html\">James Gallagher</a>", "published": "2018-02-18 21:50:21+00:00", "title": "By: James Gallagher"}, {"content": "<![CDATA[<p>What do you think of my general means of explanation of QC. I think it&#8217;s mostly correct and also innoculates against the &#8220;trying everything in parallel&#8221; school.</p>\n<p>&#8220;A regular computer like yours can be viewed as one that can do two things (1) calculate and (2) flip a coin.</p>\n<p>But let&#8217;s say that every flip of the coin splits the universe, and thanks to compounding interest you end up with, e.g., a trillion universes after 40 coin flips. This gives you a lot of computing power since you have a trillion universes to calculate in. However, if you have to randomly pick a universe to actually go into when you&#8217;re done (for example, by flipping a coin 40 times and going down that path), this clearly gets you nowhere, because you could have just picked beforehand and replaced every coin flip with a check on which universe you&#8217;d end up in.</p>\n<p>Quantum mechanics lets you mix the results of the universes somewhat before picking which one to go into. This gives you some extra power mathematically on all problems that involve searching for an answer (like guessing passwords), but not much (we can get around this by doubling the lengths of our passwords). However, on some problems that involve cyclic patterns in the universes, it can be hugely useful.</p>\n<p>These kinds of problems are generally not useful day-to-day but have been integrated into most Cryptography and thus being able to solve them breaks a lot of stuff we take for granted (for example, the &#8220;s&#8221; in https).&#8221;</p>\n]]>", "author": "kg", "published": "2018-02-18 22:20:43+00:00", "title": "By: kg"}, {"content": "<![CDATA[<blockquote><p><b>Raoul Ohio</b> observes (<i>circa</i> <a href=\"#comment-1754004\" rel=\"nofollow\">#165</a>)&nbsp; &#8220;Few people in TCS believe there is much likelihood that <i>P</i>&nbsp;=&nbsp;<i>NP</i>&nbsp;&hellip;&#8221;</p>\n<p><b>Scott Aaronson</b> poses <a href=\"https://www.edge.org/the-last-question-1\" rel=\"nofollow\">his final <i>Edge Annual Question</i></a>&nbsp; &#8220;Can we program a computer to find a 10,000-bit string that encodes more actionable wisdom than any human has ever expressed?&#8221;</p></blockquote>\n<p>With a view toward evolving better-informed opinions in regard to Scott&#8217;s <i>Edge Question</i>, it is natural to seek human-composed 10,000-bit strings that outstandingly encode &#8220;actionable wisdom&#8221;.  </p>\n<p>Specifically, and with a view toward Raoul Ohio&#8217;s comment, please allow me to commend Emanuele Viola&#8217;s essay, of 16 Feb 2018 on his weblog <i>Thoughts</i>, titled &#8220;<a href=\"https://emanueleviola.wordpress.com/2018/02/16/i-believe-pnp/\" rel=\"nofollow\">I&nbsp;believe <i>P</i>&nbsp;=&nbsp;<i>NP&#8221;</i></a> (note that Emanuele Viola&nbsp;is&nbsp;a well-known/well-respected complexity theorist and computer scientist).  </p>\n<p>Viola&#8217;s essay/manifesto attests: </p>\n<blockquote><p>After worrying about <i>P</i>&nbsp;vs&nbsp;<i>NP</i> for half my life, and having carefully reviewed the available \u201cevidence\u201d I have decided I believe that <i>P</i>&nbsp;=&nbsp;<i>NP</i>.&nbsp;&hellip;  </p>\n<p>The evidence is clear: we have grossly underestimated the reach of efficient computation, in a variety of&nbsp;contexts. All&nbsp;signs indicate that we will continue to&nbsp;see bigger and bigger surprises in upper&nbsp;bounds, and <i>P</i>&nbsp;=&nbsp;<i>NP</i>. </p>\n<p>Do I really believe the formal inclusion <i>P</i>&nbsp;=&nbsp;<i>NP&nbsp;?</i> </p>\n<p>Maybe, let me not pick parameters. What I believe is that the idea that lower bounds are obviously true and we just can\u2019t prove them is not&nbsp;only baseless but even&nbsp;clashes with historical&nbsp;evidence&nbsp;&hellip;</p></blockquote>\n<p>Hmmm&nbsp;&hellip; if it is becoming increasingly credible&nbsp;&mdash; to theorists like Emanuele Viola anyway!&nbsp;&mdash; that <i>P</i>&nbsp;=&nbsp;<i>NP</i>, then how much <i>more</i> credible is the Efficient Church-Turing Thesis (ECT) becoming?</p>\n<p>The ECT can be concisely stated as:</p>\n<blockquote><p>Anything that can be computed can be computed <i>efficiently</i> by a Turing Machine.</p></blockquote>\n<p>Here the interpolated word <i>efficiently</i> distinguishes the &#8220;ECT&#8221; from the (unextended) Church-Turing Thesis as given by Chris Berhnardt in his recent book <i>Turing&#8217;s Vision: the Birth of Computer Science</i> (2016, <i>MIT Press</i>, page 69; a work praised by Scott, in a blurb on the back cover, as &#8220;a delightful introduction for the lay reader to the ideas surrounding Alan Turing&#8217;s great paper of 1936&#8221;).</p>\n<p>For <i>Shtetl Optimized</i> readers, the ECT phrase &#8220;anything that can be computed&#8221; is best understood in its broadest physical sense, as encompassing any and all measurement data that arise from any and all quantum electro-dynamical (QED) experiments.  In a similar vein, &#8220;efficiently&#8221; is understood to mean that the computational cost of simulating the data resulting from any QED experiment (measured in units of Turing Machine steps) is a polynomial function of the entropy-dissipation of that experiment (measured in units of Shannon entropy-bits).</p>\n<p>PS: Claude Shannon&#8217;s article &#8220;Prediction and Entropy of Printed English&#8221; (<i>Bell System Technical Journal</i>, 1951) first established that the information-content of printed English is, to a reasonable approximation, one bit-per-letter (counting spaces as letters, while ignoring punctuation). </p>\n<p>So by Shannon&#8217;s measure, the &#8220;actionable ideas&#8221; in Emanuele Viola&#8217;s &#8220;I&nbsp;believe <i>P</i>&nbsp;=&nbsp;<i>NP&#8221;</i> essay are encoded in ~6,748 bits&nbsp;&hellip; well within Scott&#8217;s <i>Edge</i>-question limit of 10,000 bits!&nbsp;\ud83d\ude42</p>\n]]>", "author": "John Sidles", "published": "2018-02-18 22:22:55+00:00", "title": "By: John Sidles"}, {"content": "<![CDATA[<p>kg #169: Not bad.</p>\n]]>", "author": "Scott", "published": "2018-02-18 22:55:13+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>James #168: I still don&#8217;t understand the actual proposal you have in mind.  What is the distribution over these random jumps?  What are they, like random 1-qubit unitaries?  So can I model this as just a noise process?  Whatever it is, though, it&#8217;s completely different from what I thought had been asked about, which is simply making time discrete rather than continuous (while otherwise leaving QM unchanged).  Let&#8217;s be careful not to confuse the things.</p>\n]]>", "author": "Scott", "published": "2018-02-18 23:00:16+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Scott #172</p>\n<p>Well look, I think if we&#8217;re all honest we can agree that the Copenhagen Interpretation is unsatisfactory, and any idea that humans measuring things has a big impact on the evolution of the Universe a bit silly (Even If we allow that humans have free-will then the impact is only local)</p>\n<p>So now we have to wonder if we actually understand how Quantum Mechanics works.</p>\n<p>And then you make this blog post dissing a silly analysis of Quantum Computing.</p>\n<p>But you, yourself, don&#8217;t actually know if Quantum Computing will work, unless you are a hardcore Copenhagen/MWI advocate &#8211; are you sure it will actually work?</p>\n<p>And then <a href=\"https://www.scottaaronson.com/blog/?p=1318\">I come along</a> (about 5 years ago) when you <i>invited</i> someone to argue that &#8220;Quantum Computing is Bunk&#8221;, with a perfectly reasonable suggestion of how the Universe might be evolving that would comply with most known Quantum Mechanics experiments but wouldn&#8217;t enable large-scale Quantum Computing.</p>\n<p>Anyway, I didn&#8217;t mean to hijack this thread with my speculations, but if it helps anyone to think originally and actually make a real breakthrough in the status-quo wrt to Quantum Mechanics it would be good.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gallagher:James.html\">James Gallagher</a>", "published": "2018-02-18 23:26:10+00:00", "title": "By: James Gallagher"}]