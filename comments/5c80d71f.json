[{"content": "<![CDATA[<p>Scott #14<br />\nWhat I realized is that even if you augment the network with costs (say, each edge has a cost of 1), you can only minimize the total cost of the flow set, but not select among sets of same cost.<br />\nE.g. if the maxflow value is 2 (flows), and the min total cost/length is 8, with two possible solution flow sets: one with lengths {3,5} and one with lengths {4,4}, there is no (efficient) way to select one solution over the other (according to some sorting criteria, like {3,5}<{4,4}).</p>\n]]>", "author": "fred", "published": "2014-07-04 22:34:46+00:00", "title": "By: fred"}, {"content": "<![CDATA[<p>Scott,</p>\n<p>>> replacements for papers: blog posts </p>\n<p>I think there are 2 problems with your approach (in the long run):</p>\n<p>i) persistence: Are you sure your blog will be around 10 years from now (and can people who reference your &#8220;paperlets&#8221; be)?</p>\n<p>ii) search; If everybody would follow your example, we would not find the relevant literature to a certain topic by browsing the arxiv or certain journals, but we would have to use Google (and I for one don&#8217;t trust them) and deal with all kinds of different formats, layouts etc.</p>\n<p>In other words, I would encourage you to upload your articles to the arxiv &#8211; like the untenured folks <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/icon_cool.gif\" alt=\"8-)\" class=\"wp-smiley\" /> </p>\n]]>", "author": "wolfgang", "published": "2014-07-05 03:45:17+00:00", "title": "By: wolfgang"}, {"content": "<![CDATA[<p>You don&#8217;t need tex or a bibliography to post to the arxiv. If the diagrams started life in powerpoint, can you export them to word, add the text, and print to pdf?</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Knight:Douglas.html\">Douglas Knight</a>", "published": "2014-07-05 04:10:50+00:00", "title": "By: Douglas Knight"}, {"content": "<![CDATA[<p>Amateur question here: if I undestand the above, the DigiComp&#8217;s ball-switch-gravity design limits it from being &#8220;circuit complete&#8221;. That immediately reminded me of an equally-limited circuit design: diode logic. </p>\n<p><a href=\"http://en.wikipedia.org/wiki/Diode_logic\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Diode_logic</a></p>\n<p>Diode logic is neat, because the components are entirely passive&#8211;you don&#8217;t need a power supply. (Until enough diodes attenuate the signal away.) But it&#8217;s limited in that you can&#8217;t make all logic gates with it: no NOT. Is this limitation the same as DigiComp&#8217;s? Or are these just two different designs that fail universal computation for different reasons?</p>\n]]>", "author": "JohnD", "published": "2014-07-07 00:34:51+00:00", "title": "By: JohnD"}, {"content": "<![CDATA[<p>JohnD #20: No, from the Wikipedia page, it looks like diode logic corresponds to what complexity theorists call <i>monotone circuits</i> (circuits with AND and OR gates, but no NOT gate), which are a very different type of non-universality than Digi-Comps / comparator circuits.  On the one hand, the circuit value problem for monotone circuits is already P-complete (i.e., just as hard as it is for non-monotone circuits).  On the other hand, for certain <i>specific</i> functions (like Clique and Maximum Matching), the monotone circuit complexity is so well understood that we know how to prove <i>unconditional exponential lower bounds</i> on circuit size&#8212;that was one of the great achievements of complexity in the 1980s, initiated by Razborov.</p>\n<p>With comparator circuits, by contrast, the circuit value problem is not P-complete, but &#8220;merely&#8221; CC-complete.  And you can even throw NOT gates into the circuit, and the problem remains CC-complete.  After you&#8217;ve done that, you have a model that can implement AND, OR, and NOT, yet that <i>still</i> isn&#8217;t universal, basically because you&#8217;re restricted in how you can compose the operations (more specifically, you lack a fanout or COPY gate).</p>\n<p>Whether one can prove interesting unconditional lower bounds on the sizes of comparator circuits or Digi-Comps is a superb question that hadn&#8217;t occurred to me before, but which is immediately suggested by the comparison with monotone circuits.  Does anyone have thoughts?  I mean, if we consider comparator circuits over a sufficiently-large non-Boolean domain, then there&#8217;s the famous &Omega;(n log n) lower bound for comparison-based sorting.  But is there anything more interesting than that, and/or anything for <i>Boolean</i> comparator circuits?</p>\n<p><b>Note added 2 minutes later:</b> OK, thanks to Dana, I just learned about the <a href=\"http://en.wikipedia.org/wiki/Sorting_network#Zero-one_principle\" rel=\"nofollow\">zero-one principle for sorting networks</a>, which allows you to &#8220;lift&#8221; the &Omega;(n log n) lower bound for comparison-based sorting from non-Boolean domains to the Boolean one.  Whether one can prove an interesting lower bound on, e.g., the size of a comparator circuit needed for Stable Marriage or multiplication remains a question that interests me.</p>\n]]>", "author": "Scott", "published": "2014-07-07 00:55:58+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>@Scott</p>\n<p>Completely off-topic, but have you seen this paper by t&#8217; Hofft?</p>\n<p><a href=\"http://arxiv.org/pdf/1405.1548v2.pdf\" rel=\"nofollow\">http://arxiv.org/pdf/1405.1548v2.pdf</a></p>\n]]>", "author": "Nyme", "published": "2014-07-07 02:08:39+00:00", "title": "By: Nyme"}, {"content": "<![CDATA[<p>Nyme #22: Yes, of course I saw it.  Let me just paste an email I sent to a friend who asked me about it when it came out two months ago:</p>\n<p>I only read a few parts of the 200-page manifesto, but it looked like mostly just a recap of what &#8216;t Hooft has been saying over and over for the last decade (or longer).  He&#8217;s become a broken record on this matter.</p>\n<p>Look, &#8216;t Hooft believes in a classical, deterministic cellular automaton underlying all of physics.  So he then faces the obvious problem of how to account for known quantum phenomena such as Bell inequality violation in such a model.  Originally, &#8216;t Hooft dealt with this problem by simply ignoring it (from reading his early papers, I think, to be honest, that he didn&#8217;t fully understand the Bell inequality!).  But after enough people explained it to him, he adopted his current position: that the way out is &#8220;superdeterminism.&#8221;  In other words, when Alice and Bob play (say) the CHSH game, and they win it 85% = cos<sup>2</sup>(&pi;/8) of the time rather than just 3/4 of the time, they might <i>think</i> they&#8217;re doing something that Bell proved to be impossible in a classical, local-realistic universe.  But actually, the initial conditions of the universe conspired so that Alice and Bob were never playing the game they thought they were!</p>\n<p>I.e., there was a conspiracy involving Alice&#8217;s and Bob&#8217;s brains, the random-number generators in their computers, <i>and</i> the elementary particles so that Alice and Bob didn&#8217;t actually have the freedom they thought they had to choose the detector settings!  Rather, the detector settings were secretly prearranged since the beginning of time &#8212; and all just to make it look like the CHSH game can be won 85% of the time!  You&#8217;d think that such a magical ability would let Alice and Bob do something <i>more</i> impressive, like send signals faster than light, or (at the very least) win the CHSH game 90% or 100% of the time.  But no, the sole purpose of this gigantic cosmic conspiracy is to make it look like ordinary textbook quantum mechanics is valid, at least for those experiments that have already been done.</p>\n<p>To me, this sort of &#8220;explanation&#8221; is hardly better than the creationists&#8217; God, who planted the fossils in the ground to confound the paleontologists.  Indeed, it&#8217;s almost like a backhanded compliment to Bell, like a convoluted way of admitting that you&#8217;ve lost the argument.  And crucially, it&#8217;s completely scientifically sterile: I don&#8217;t see how it helps us understand quantum mechanics better; for example, it offers no explanation for why the CHSH game happens to be winnable exactly 85% of the time (just like the creationists&#8217; theory fails to why God would plant dinosaur fossils in the ground, rather than mermaid fossils or anything else).  I don&#8217;t understand why anyone, least of all &#8216;t Hooft, would feel inclined to take such a convoluted non-explanation seriously.</p>\n<p>Yet, in Section 14.3 of his new manifesto, &#8216;t Hooft explicitly affirms that this is indeed his view.  Since he&#8217;d titled the section &#8220;Superdeterminism and Conspiracy,&#8221; I thought &#8216;t Hooft was going to explain there why the critics&#8217; charges are false &#8212; why his view <i>doesn&#8217;t</i> require a ridiculous cosmic conspiracy, and <i>isn&#8217;t</i> anywhere near as absurd as the fossil-planting God.  But no, he basically admits in that section that this is indeed the implication of his view, and we should just swallow it, since it&#8217;s the only way he sees to avoid quantum mechanics.</p>\n<p>For me, the end result is something strange, funny, and a little sad &#8212; fascinating, but not for physics reasons.</p>\n]]>", "author": "Scott", "published": "2014-07-07 11:54:47+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>I believe I have a construction that shows that planar DIGICOMP is equivalent to arbitrary graph DIGICOMP. In terms of the pebble game, the planarity requirement can be viewed as the statement that the piles are in a linear order, and piles can only be merged with their neighbors. If we take &#8220;planar&#8221; just to mean that the DAG lies entirely in the plane (with the source and sink possibly enclosed from the outer face, and a physical machine built in the shape of a cone), then the piles are in a ring; if we take &#8220;planar&#8221; to mean that the digicomp is actually flat, then the piles are in a line segment. These two will also become equivalent, but since the ring-planar version clearly includes the straight-planar version, I&#8217;ll deal with the latter.</p>\n<p>Since the restriction states that we can only manipulate adjacent piles, our problem becomes that of finding a way to exchange two adjacent piles non-destructively. Suppose that we replace each pile of T balls with a pile of 2T balls. Now we have a pile with 2X and 2Y, let&#8217;s say, in that order, that we want to exchange. Split the 2Y pile into Y and Y, split both of those into \u230aY/2\u230b and \u2308Y/2\u2309, those into \u230a\u230aY/2\u230b/2\u230b etc., and so on. At the end we have roughly Y piles, with the lowest &#8216;bit&#8217; of Y on the left. This is necessarily either 0 or 1. We now merge this with the 2X pile. We do the same operation splitting there, of separating out the smallest &#8216;bit&#8217;, and then we merge all the other X piles. If the low bit of Y was 0, then we have 0 and 2X, in that order; if the low bit was 1, we have 1 and 2X. Thus we have exchanged a single pebble of Y and 2X. We then exchange 2X with the next pebble that may or may not be there, and then the next, and the next. Through this we can move all of Y through 2X. We can move the second Y through 2X by the same process, and then merge them on the other side. This lets us &#8216;exchange&#8217; two of our double piles, because it&#8217;s easy for the pebbles to ignore doubles. In the DIGICOMP construction, this would be equivalent to having two pebbles from X at a time flow through a gate to the other side, leaving it unchanged at the end.</p>\n<p>But now the two main operations of the pebbles, that of merging and splitting, need to be done on piles of twice the size. Merging is simple: 2X and 2Y become 2(X+Y). Splitting is a bit more tricky. Say we have 2X and 2Y and want to make 2\u230a(X+Y)/2\u230b and 2\u2308(X+Y)/2\u2309. I think this is best done with a diagram:</p>\n<p>{2X, 2Y}</p>\n<p>{2X+2Y} (merge)</p>\n<p>{X+Y, X+Y} (split)</p>\n<p>{\u230a(X+Y)/2\u230b, \u2308(X+Y)/2\u2309, \u2308(X+Y)/2\u2309, \u230a(X+Y)/2\u230b} (two splits, one going one way and one the other)</p>\n<p>{\u230a(X+Y)/2\u230b, 2\u2308(X+Y)/2\u2309, \u230a(X+Y)/2\u230b} (merge)</p>\n<p>{\u230a(X+Y)/2\u230b, \u230a(X+Y)/2\u230b, 2\u2308(X+Y)/2\u2309} (exchange, which is possible because 2\u2308(X+Y)/2\u2309 is even)</p>\n<p>{2\u230a(X+Y)/2\u230b, 2\u2308(X+Y)/2\u2309} (merge)</p>\n<p>This lets us do all the operations of regular pebbles, by using 2n pebbles. The exchange operations will take a number of gates polynomial in the number of pebbles (specifically, n log n), but this is still within the constraints.</p>\n<p>On a mostly unrelated note: Has anyone looked at quantum version of CC &#8212; if that even exists &#8212; some set of gates that allow phase shifts and sorting? Does this sit at CC, explode into EQP (e.g. by allowing some way of encoding NOT in the phases), or sit somewhere in between?</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meiburg:Alexander.html\">Alexander Meiburg</a>", "published": "2014-07-07 17:27:56+00:00", "title": "By: Alexander Meiburg"}, {"content": "<![CDATA[<p>Alexander #24: That&#8217;s an awesome construction&#8212;thanks so much!  I went through it and I think that it works.</p>\n<p>Now, regarding a quantum version of CC, one immediate problem is that sorting is a non-reversible, and therefore non-unitary, operation.  One could deal with that using, e.g., the superoperator formalism, but it&#8217;s far from obvious to me what a good or interesting superoperator version of CC would be.  Ideas welcome.</p>\n]]>", "author": "Scott", "published": "2014-07-07 18:43:56+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>To make a quantum version of CC you could also try to make sorting reversible. SORT maps 00->00, 01->01, 10 ->01 and 11->11. By adding a ancilla bit in front we get RSORT; 000->000, 001->001, 010->101, 011->011, 100->100, 101->010, 110->110, 111->111.</p>\n<p>RSORT is reversible and provided the first bit is a zero ancilla bit it will act on the last two bits as SORT does. If you discard the ancilla bit after use this will give you CC. Then you can use quantum gates as well. </p>\n<p>If you do not discard the ancilla bit, so basically if you use RSORT as a regular gate, I think we get P. Because then you get to copy a bit in the following way; 1(your bit)1 -> 010 if your bit was 0 and 111 if your bit was 1. Then just take the first and last bit. </p>\n<p>Anybody have any idea as to the power of RSORT with quantum gates (ancilla bits discarded after use)?</p>\n]]>", "author": "David", "published": "2014-07-07 21:16:41+00:00", "title": "By: David"}, {"content": "<![CDATA[<p>As my understanding goes, the act of &#8220;discarding&#8221; a bit is a somewhat forbidden operation in quantum computers. Indeed, if it weren&#8217;t, then /any/ function could be done by storing the original input in one set of bits, and the output in another set (this half is reversible), and then erasing the first half. When a bit is &#8220;erased&#8221; it really just gets the data entangled with the environment in a very complicated, pseudo-random way &#8212; one that alters the phase so unpredictably that quantum interference can no longer be intentionally, and you&#8217;re left with a branch of execution that&#8217;s just a probabilistic classical computer.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meiburg:Alexander.html\">Alexander Meiburg</a>", "published": "2014-07-08 17:07:30+00:00", "title": "By: Alexander Meiburg"}, {"content": "<![CDATA[<p>@Alexander #27, Well you do not erase the bits, you just ignore them for the rest of the computation.</p>\n]]>", "author": "David", "published": "2014-07-08 17:39:03+00:00", "title": "By: David"}, {"content": "<![CDATA[<p>My comment is offtopic but I&#8217;ve found interesting your idea of paperlets.<br />\nI like to know how people do their research work. I also think it might be interesting to other people, especially those who want to start a career as researchers.<br />\nWriting every idea is a fundamental habit for any researcher, but besides that, how do you read scientific books?, is it necessary to check all calculations?, do you take a lot of notes while reading?, and in the case of scientific papers?. A scientist has to conduct research but also to continue acquiring knowledge, how you manage time between these two needs?<br />\nI think a post about these topics would be very helpful.</p>\n]]>", "author": "Nosy", "published": "2014-07-08 19:15:23+00:00", "title": "By: Nosy"}, {"content": "<![CDATA[<p>If figures are really your bottleneck, I suggest using inkscape instead of powerpoint&#8230;  it takes a few hours to learn and you can immediately get nice-looking pdf vector graphics that behave well under rescaling and are always small files.  It never takes me longer than 10-15 minutes to produce a (simple) figure and put it into latex with the size I want.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Harlow:Daniel.html\">Daniel Harlow</a>", "published": "2014-07-08 22:23:57+00:00", "title": "By: Daniel Harlow"}, {"content": "<![CDATA[<p>Nosy #29: I don&#8217;t mind answering any of those questions, but that&#8217;s a <i>lot</i> of questions.  You might want to check out my &#8220;Ask Me Anything&#8221; posts, and if the answers aren&#8217;t there, then wait for the next such post.</p>\n<p>Daniel #30: Thanks for the tip!  I&#8217;ve had coauthors who used TikZ to make graphics, which seemed to be very nice.  Other tools that are on my stack to learn include Python, Matlab, Mathematica, and GAP.  Alas, I feel like years ago I already became too old to learn new tricks&#8230;</p>\n]]>", "author": "Scott", "published": "2014-07-08 23:23:15+00:00", "title": "By: Scott"}]