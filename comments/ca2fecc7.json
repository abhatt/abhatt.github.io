[{"content": "<![CDATA[<p>J #7: No, that&#8217;s not quite what I said.  You can construct a <i>theory</i> of just about anything! <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/icon_smile.gif\" alt=\":-)\" class=\"wp-smiley\" /> </p>\n<p>&#8220;Negative probabilities&#8221; do show up in QFT, as artifacts needing to be tamed, and they also arise when people try to impose quasi-classical pictures on quantum mechanics (e.g., the Wigner function representation).  But the one thing a meaningful physical theory must <b>never</b> do, is predict a negative probability for an actual outcome of an actual measurement.  That&#8217;s the thing that makes as little sense as it sounds.</p>\n<p>I don&#8217;t know the situation regarding &#8220;agravity&#8221; theories&#8212;you should really ask an expert (maybe try Physics StackExchange?).  But from what I read, it seemed clear that there are two possibilities:</p>\n<p>(i) People won&#8217;t manage to &#8220;tame&#8221; these ideas into meaningful quantum theories&#8212;in which case, they&#8217;ll simply reject the ideas (which were never more than speculations anyway), rather than the basic principles of QM.  (Just like, when your restaurant bill doesn&#8217;t add up, you question the waiter, not the rules of arithmetic&#8230;)</p>\n<p>(ii) More optimistically, people <i>will</i> manage to tame the theories&#8212;in which case, they&#8217;ll be perfectly consistent with ordinary QM.</p>\n<p>Either way, unfortunately, it&#8217;s not clear that there would be any implications for computational complexity.</p>\n<p>(Anyone who actually understands these &#8220;agravity&#8221; ideas and can add further clarification, is strongly encouraged to do so.)</p>\n]]>", "author": "Scott", "published": "2014-08-20 00:25:07+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>A consequence of UGC as you have stated: </p>\n<p>&#8221; In particular, if the UGC is true, then for MAX-CUT and dozens of other (NP Hard) important optimization problems, no polynomial-time algorithm can always get you closer to the optimal solution than some semidefinite-programming-based algorithm gets you, unless P=NP.&#8221;</p>\n<p>So, as its consequence, if one gets a polynomial algorithm for some NP Hard problem than could P = NP true?</p>\n<p>I hope you will enjoy to have a look at:  </p>\n<p><a href=\"http://vixra.org/pdf/1204.0019v1.pdf\" rel=\"nofollow\">http://vixra.org/pdf/1204.0019v1.pdf</a></p>\n<p>DPM</p>\n]]>", "author": "D", "published": "2014-08-20 06:02:51+00:00", "title": "By: D"}, {"content": "<![CDATA[<p>D #9: Yes, if you got a polynomial-time algorithm for an NP-hard problem then P=NP would be true.  That&#8217;s almost what NP-hardness <i>means</i>.</p>\n]]>", "author": "Scott", "published": "2014-08-20 13:16:48+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>This question is not specific to Subhash or UGC so I understand if you don&#8217;t wish to answer. But I don&#8217;t understand how one could possibly prove P!=NP (despite the fact it is almost surely the case). It seems to me it would entail proving that there could exist no future algorithm with clever use of as of yet undiscovered mathematical concepts that could solve a problem in polynomial time. How can one do that? Can you give examples of existing problems (obviously not in NP-hard) where it has already been proven that &#8220;it cannot be done quicker, even with the mathematics and clever people of the future&#8221;?</p>\n]]>", "author": "lewikee", "published": "2014-08-20 13:37:28+00:00", "title": "By: lewikee"}, {"content": "<![CDATA[<p>lewikee #11: Yes, I <i>can</i> give such examples.  Most obviously, there&#8217;s the halting problem.  How could anyone possibly say that the as-yet-undiscovered mathematical concepts of the future could <i>never, ever</i> allow us to design a program that decides whether other programs halt?  Well, Turing said it, and his argument remains as valid today as it was in 1936.</p>\n<p>Then there&#8217;s the Time Hierarchy Theorem, which lets us make lots of interesting unconditional statements, such as:</p>\n<ul>There is no polynomial-time algorithm to decide whether White has the win from a given position, in an nxn generalization of chess.</p>\n<p>There is no polynomial-time algorithm to decide whether a statement about integers, involving quantifiers, variables, constants, and addition (but no multiplication), is true or false.  (Even though this problem is decidable, in a stack-of-exponentials running time.)</ul>\n<p>Here are a few other examples of things that we know:</p>\n<ul>There are no polynomial-size circuits of AND and OR gates (and no NOT gates) to solve the CLIQUE or MATCHING problems.  (Even though there <i>are</i> such circuits of exponential size.)</p>\n<p>There is no polynomial-size, constant-depth, unbounded-fanin circuit of AND, OR, and NOT gates to compute the parity or majority of an n-bit string.</p>\n<p>There is no algorithm that solves SAT simultaneously in sublinear space and O(n<sup>1.7</sup>) time.\n</ul>\n<p>Once again, all of these things <i>have been proved</i>, despite the unlimited number of new mathematical concepts and algorithmic techniques that might be invented in the future&#8212;much as Fermat&#8217;s Last Theorem has been proved, despite the unlimited number of ideas that might be invented in the future to search for counterexamples to it.  (Since the time of Euclid, you might say, the entire appeal of math has been its ability to encompass an infinity of cases in a single, finite argument.)</p>\n<p>Obviously, P&ne;NP is vastly harder than any of the examples I mentioned&#8212;if it weren&#8217;t, then it would&#8217;ve been solved already.  But I think the existence of these known lower bounds (and dozens more that I didn&#8217;t mention) should certainly cause one to question the belief that P&ne;NP must be fundamentally unprovable.</p>\n]]>", "author": "Scott", "published": "2014-08-20 17:47:59+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Scott, do you believe the UGC is true?</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/R:Dave.html\">Dave R</a>", "published": "2014-08-20 17:49:31+00:00", "title": "By: Dave R"}, {"content": "<![CDATA[<p>Dave #13: Maybe with ~65% probability.  (I had some statement to that effect in the original draft of this post; don&#8217;t remember why it got edited out.)</p>\n]]>", "author": "Scott", "published": "2014-08-20 19:30:30+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Scott #13: Thanks.  Always like open problems which no one really knows for sure which way to go on.</p>\n<p>lewikee&#8217;s question actually got me thinking &#8211; doesn&#8217;t the core reason that we think we can&#8217;t solve NP-hard problems in P time that there is just not enough information gained at each step?  My intuition is kind of that, for a problem with an exponential number of possible solutions, each of the P steps has to kind of provide an exponential jump in the amount of information we have about the solution.</p>\n<p>Can you provide any info on how this has been explored from an information theoretic perspective?</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/R:Dave.html\">Dave R</a>", "published": "2014-08-20 20:07:59+00:00", "title": "By: Dave R"}, {"content": "<![CDATA[<p>Dave R #15: People have thought along such lines, of course, but the key problem is that it&#8217;s extremely hard to formalize what you mean by the &#8220;information&#8221; an algorithm possesses at a given time.  After all, if you approach the question through Shannon&#8217;s theory, then you&#8217;re forced to say that all the information you want is already &#8220;implicitly&#8221; there, the instant the algorithm receives its input! <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/icon_smile.gif\" alt=\":-)\" class=\"wp-smiley\" />   The trouble, of course, is that while the input might &#8220;implicitly&#8221; contain the desired information (the prime factors of your number, the satisfiability of your formula, etc.), you want an <b>explicit</b> representation of the answer, a distinction that Shannon&#8217;s theory is silent about.</p>\n<p>So, OK then, why not just try to quantify the amount of &#8220;explicit knowledge&#8221; that the algorithm has about the solution, and show that exponentially many steps are needed before there can be an appreciable amount of such knowledge?  Well, to see the difficulty of that strategy, consider any even slightly-interesting polynomial-time algorithm&#8212;e.g., for greatest common divisor, or maximum matching, or finding eigenvalues of a matrix.  Such algorithms work by doing complicated transformations of the input.  If you were just staring at a memory dump, it might not be obvious to you that the algorithm was gaining <i>any knowledge at all</i> about the solution, until everything suddenly came together at the very last step.  Or rather: it wouldn&#8217;t be obvious to you <i>unless</i> you understood how the algorithm worked, at a higher, &#8220;semantic&#8221; level.</p>\n<p>(And in fact, almost every amateur &#8220;proof&#8221; of P&ne;NP I&#8217;ve ever seen could be rejected for the simple reason that nothing it said was specific to NP-complete problems.  I.e., if the argument worked to show any algorithm needed exponential time to &#8220;gather enough information&#8221; to solve a 3SAT instance, then an exactly parallel argument would <i>also</i> have worked for 2SAT.  Yet for 2SAT, we know a linear-time algorithm.)</p>\n<p>Worse yet, Razborov and Rudich&#8217;s <a href=\"http://en.wikipedia.org/wiki/Natural_proof\" rel=\"nofollow\">natural proofs barrier</a> shows that, in a certain sense, any approach to proving P&ne;NP that follows the algorithm step-by-step, and explicitly calculates how &#8220;complicated&#8221; is the information that the algorithm has produced so far (showing that exponentially many steps are needed before it&#8217;s &#8220;complicated enough&#8221;), is inherently doomed to failure.  For, if such an approach worked, we could also use it to efficiently distinguish random functions from pseudorandom functions, which is one of the very problems we were trying to prove was hard.</p>\n<p>This shouldn&#8217;t be interpreted to mean that proving P&ne;NP is <i>impossible</i>: in fact, diagonalization (the thing Turing used to show the unsolvability of the halting problem) is already a technique that evades the barrier pointed out by Razborov and Rudich.  (Unfortunately, diagonalization falls prey to a different barrier, called <i>relativization</i>&#8212;but this comment is getting too long.)  What it means, rather, is that if you&#8217;re trying to prove some problem is not in P, then you&#8217;d better zero in on something highly <i>specific</i> about that problem, rather than just trying to argue that the solution is &#8220;complicated,&#8221; and no algorithm could generate anything so &#8220;complicated&#8221; after a polynomial number of steps.</p>\n<p>(Which is something that one could&#8217;ve realized, and that careful thinkers <i>did</i> realize, even before Razborov and Rudich&#8212;but the latter made the point sharper and more formal.)</p>\n]]>", "author": "Scott", "published": "2014-08-20 20:48:08+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Thanks Scott &#8211; and thanks for always making such thoughtful responses even to pedestrian questions :-).  Very generous of you!</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/R:Dave.html\">Dave R</a>", "published": "2014-08-20 22:04:47+00:00", "title": "By: Dave R"}]