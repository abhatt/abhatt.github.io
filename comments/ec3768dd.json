[{"content": "<![CDATA[<p>I&#8217;ve always found libertarianism&#8217;s arguments extremely elegant & enticing. e.g. Milton Freidman&#8217;s exposition of it. Or Tyler Cowen&#8217;s blog posts etc.  (In fact, I&#8217;ve found almost no one on the left with a good set of arguments)</p>\n<p>My problem is, when I read those libertarian arguments being taken to their logical conclusions, I&#8217;m somehow uncomfortable with ( some of ) the conclusions. It&#8217;s a discomfort originating in gut feelings not rationality but it still makes me uneasy to go with the conclusions. </p>\n<p>e.g. Although I don&#8217;t have a good logical response to Freidman&#8217;s arguments I still want my national parks, US State Universities & doctor licensing.</p>\n]]>", "author": "Rahul", "published": "2015-12-26 04:13:05+00:00", "title": "By: Rahul"}, {"content": "<![CDATA[<p>Rahul #23: In that case, <a href=\"http://raikoth.net/libertarian.html\" rel=\"nofollow\">Scott Alexander&#8217;s FAQ</a> sounds like exactly the thing for you!</p>\n]]>", "author": "Scott", "published": "2015-12-26 04:21:41+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>What&#8217;s the minimum information necessary to determine the number of ancillary bits required for reversible computation?</p>\n<p>For example, assuming that we don&#8217;t know the circuit that will be executed, is it sufficient to know the number of input/output bits and gate set? Or would we also need additional information such as maximum-depth?</p>\n<p>Essentially, given n+k bits, for some k corresponding to the number of ancillary bits, is it always possible to construct an n-bit circuit that cannot be made reversible without increasing k?</p>\n]]>", "author": "Job", "published": "2015-12-27 19:20:05+00:00", "title": "By: Job"}, {"content": "<![CDATA[<p>Scott, I just read that Subset Sum (SS) and 3SAT are in GenericP, which I think means the set of hard instances is sparse.  Does that mean that everything in NP is in GenericP since there&#8217;s a P-time reduction to SS and 3SAT?  In the case of 3SAT it&#8217;s in GenExpP which means the vanishing of hard instances happens fast as the problem size grows.  Does that tell us where we are in the Five Worlds?</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Generic-case_complexity\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Generic-case_complexity</a></p>\n]]>", "author": "asdf", "published": "2015-12-28 01:08:27+00:00", "title": "By: asdf"}, {"content": "<![CDATA[<p>asdf #26: No, there are no implications of that kind, because typical NP-completeness reductions don&#8217;t preserve denseness.  So there&#8217;s no barrier whatsoever to some NP-complete problems being &#8220;hard in the generic case&#8221; and others being &#8220;easy in the generic case,&#8221; and that&#8217;s exactly what we find (assuming, say, the existence of one-way functions).</p>\n]]>", "author": "Scott", "published": "2015-12-28 05:07:15+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Job #25: See <a href=\"http://www.scottaaronson.com/papers/gates.pdf\" rel=\"nofollow\">my, Daniel Grier, and Luke Schaeffer&#8217;s reversible gate classification paper</a> and the references therein.  The short answer is that, if you don&#8217;t care about the <i>number</i> of gates, then you can do reversible computation with only a tiny number of ancilla bits (1 ancilla bit, for many natural reversible gates like Toffoli, and in any case a constant number of ancilla bits depending only on your reversible gate set).</p>\n<p>On the other hand, if you do also care about circuit size, then there can be tradeoffs between the number of gates and the number of ancillas.  There are constructions by Bennett, Lange-McKenzie-Tapp, and others that give different points on the tradeoff curve between gates and ancillas (for taking an arbitrary computation and simulating it reversibly), but I believe it&#8217;s still not known whether these tradeoffs are optimal.  (See <a href=\"http://pages.cs.wisc.edu/~dieter/Papers/williams-reu.pdf\" rel=\"nofollow\">here</a>, <a href=\"http://arxiv.org/abs/quant-ph/0101133\" rel=\"nofollow\">here</a>, <a href=\"http://www.math.ucsd.edu/~sbuss/CourseWeb/Math268_2013W/LMT_ReversibleSpace.pdf\" rel=\"nofollow\">here</a> for example.)  In any case, you can certainly improve over these bounds for certain specific functions or classes of functions.</p>\n]]>", "author": "Scott", "published": "2015-12-28 07:26:54+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Is there a list of problems somewhere which need NP time for even their generic / average case complexities?</p>\n]]>", "author": "Rahul", "published": "2015-12-28 09:36:06+00:00", "title": "By: Rahul"}, {"content": "<![CDATA[<p>Rahul #29: Good question.  I don&#8217;t know of a Garey&#038;Johnson-style list, but <a href=\"http://arxiv.org/abs/cs/0606037\" rel=\"nofollow\">this 2005 survey by Bogdanov and Trevisan</a> is probably a good place to start looking.  Unfortunately, compared to the theory of worst-case NP-hardness, the theory of average-case NP-hardness is a lot messier, with more isolated conjectures and fewer connections between the conjectures.  (Or rather: to the extent that there IS nice theory, it tends to be for distributions over problem instances unlike those that would arise in practice.)  Average-case hardness becomes cleaner if you go up to #P or PSPACE, or down to certain specific NP problems like discrete log.</p>\n]]>", "author": "Scott", "published": "2015-12-28 13:53:44+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>I understand worst case complexity (I think). </p>\n<p>But is there an intuitive way to understand generic complexity & average case complexity?</p>\n<p>The explanations I found on Wikipedia were a tad too confusing. </p>\n<p>e.g.<br />\n<blockquote> &#8220;An algorithm is in GenP (generically polynomial time) if it never gives incorrect answers and if it gives correct answers in polynomial time on a generic set of inputs.&#8221;</p></blockquote>\n<p>So what exactly do they mean by a &#8220;generic set of inputs&#8221;?</p>\n]]>", "author": "Rahul", "published": "2015-12-29 09:45:51+00:00", "title": "By: Rahul"}, {"content": "<![CDATA[<p>Rahul #31: I&#8217;ve never heard of GenP myself.  The ones I know about are AvgP and HeurP.  Yes, as I said, they&#8217;re sort of complicated and non-intuitive, since the &#8220;intuitive&#8221; definitions (e.g., uniform distribution over all inputs) don&#8217;t give you nice reducibilities.  But please try the Bogdanov-Trevisan survey.</p>\n]]>", "author": "Scott", "published": "2015-12-29 11:16:53+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>asdf#26,</p>\n<p>One thing to note is that while there aren&#8217;t results about density in that sense since as Scott notes reductions don&#8217;t preserve density, there are some very weak bounds that sort of amount to this sort of thing if one is willing to go for much weaker than a claim about density. I think it is true that if there&#8217;s an NP-complete problem where there&#8217;s an algorithm which solves the problem in polynomial time with at most O(log n) exceptions, and it responds &#8220;I don&#8217;t know&#8221; to the exceptions rather than giving wrong answers, then NP=P.  The proof is essentially the same as the proof that if NP is in P/log then P=NP. </p>\n<p>Actually, question related to that for Scott: can we get this sort of result for some function that grows faster than O(log n)? We can if we weaken the conclusion from P=NP to the polynomial hierarchy collapsing (by using that NP in P/poly makes the hierarchy collapse) and that allows us also to make the algorithm be outright wrong on the exceptional values, but if we actually want P=NP as the conclusion (and not just hierarchy collapse or breaking ETH) is there any faster growing function we can do this for? It isn&#8217;t even obvious to me that we can do so for log n log* n.</p>\n]]>", "author": "Joshua Zelinsky", "published": "2015-12-30 02:59:03+00:00", "title": "By: Joshua Zelinsky"}, {"content": "<![CDATA[<p>I (a man) used to find special conferences for women to be silly as well but I had a fellow grad student who attended one and she indicated that she liked. This caused me to change my mind , especially since I did not think she was the type who cared that she happened to be in a gender minority. My new view is that a lot of people tend to enjoy seeing people like themselves working in the same area and since gender is a fundamental part of our self-image for most people, they enjoy seeing other people of the same gender working with similar things. </p>\n<p>And I am libertarian enough to say that if people want to hold conferences with only women they are harming nobody and it should be allowed, regardless of motivation. Of course, I would see it as equally legitimate with a male-only conference which I suspect would be subject to a great deal of criticism. .</p>\n<p>I am not a full libertarian, though, for somewhat the same reasons as Scott Alexander. I think it is very difficult to justify treating private property as sacrosanct, though I think there are excellent reasons to treat it with more respect than is done at the moment, and as for the libertarian anti-paternalism we can refute it by noting that no one would apply it to children and yet nothing magical happens when we turn 18 (or 20 or 17 or whatever) that suddenly makes us fully rational and independent. I would not want the government to treat us like children but a little paternalism, sometimes, can probably be good. (I do mean a little, I would  let you use drugs, sell sex and drive without a seat belt, but not all at the same time.)</p>\n]]>", "author": "Jr", "published": "2015-12-30 16:28:17+00:00", "title": "By: Jr"}, {"content": "<![CDATA[<p>I like this idea of generic complexity. With average case complexity, say for the uniform distribution on all the instances of given size, a small number of hard instances could make the average complexity exponential, even though generically it is polynomial. It seems useful to know both statements, as in some physics problems about &#8220;rare events&#8221;.</p>\n<p>I would guess that MAXCUT is generically hard, though the few examples mentioned on Wikipedia don&#8217;t get to things of that type. I&#8217;m not really surprised that 3SAT is in GenP, given what we know about phase transitions for random instances.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Read:Nick.html\">Nick Read</a>", "published": "2015-12-30 20:53:17+00:00", "title": "By: Nick Read"}]