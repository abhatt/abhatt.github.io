[{"content": "<![CDATA[<p>Jay #158: My central contention, if you like, is that real, actual copying of the complete state of your mind would be <b>nothing at all</b> like agreeing intellectually that copying should be possible, and then going back to your life the way it was before.  So for example, you could then perform any task whatsoever by delegating it to your copy.  Other people would also know this, and for that reason, &#8220;would no longer have any need for the original you.&#8221;  You could jump off a bridge just to see what it was like, &#8220;restoring yourself from backup&#8221; in the likely event that you die.  If you played the role of the Chooser in Newcomb&#8217;s Paradox, you would see that every time you chose both boxes, one of the boxes ended up being empty, while every time you chose one box it had a million dollars&#8212;something that, if it happened today, we would call supernatural, or proof of backwards-in-time causal influences.  I can try to make up what &#8220;I&#8221; (to whatever extent it still makes sense to use that pronoun) would say or feel in such circumstances, in a Woody Allen sort of way, but honestly I don&#8217;t know, and neither do you.  The one thing I feel strongly about is that people in these discussions shouldn&#8217;t understate the world-changing implications of copyability, as a consequence of not having fully thought it through.</p>\n]]>", "author": "Scott", "published": "2014-09-03 15:36:03+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>#99 re #74</p>\n<p>1) flips a fair coin (&#8230;)</p>\n<p>If goddy give 1$ to any succefull copy, bet 100/101. If she splits the gain among copies, bet 1/2. Do you know of any example for which confusion remains once who gains what is clearly specified?</p>\n<p>2) the probability that you\u2019ll see Y tomorrow, given that you saw X today? (&#8230;) 3/5|X>+4/5|Y> </p>\n<p>I don&#8217;t understand the problem. If you saw X today, then how could you treated yourself as in a superposition? It seems that, as for Bostrom&#8217;s examples, confusion just arise because of faulty formulation. The first part indicates we consider the point of view of the one who see X, the second part indicates we could the point of view of someone who see yourself seing X or Y. One idea is maybe you measured yourself using some basis and now you know that your state is 3/5|X>+4/5|Y>. But then where is the problem?</p>\n]]>", "author": "Jay", "published": "2014-09-03 17:28:04+00:00", "title": "By: Jay"}, {"content": "<![CDATA[<p>RE#159</p>\n<p>So you&#8217;re really adressing the &#8220;ought&#8221; question, namely you don&#8217;t like the implication of what seems the best picture we have of the mind, even if you agree that&#8217;s the best picture. I prefer the &#8220;is&#8221; question, but also don&#8217;t think this picture is as bad for the &#8220;ought&#8221;. If you&#8217;re a Chooser in some version of Newcomb paradox, you can always restore your knightian freedom by selecting a strategy that depends on what the Predictor will do. If the Predictor never communicates with you, then is as real as any multiverse extension we can imagine. If he communicates with you, then he gives you a place to stand that can move the earth. There&#8217;s even a theorem about that, isn&#8217;t it?</p>\n]]>", "author": "Jay", "published": "2014-09-03 17:42:53+00:00", "title": "By: Jay"}, {"content": "<![CDATA[<p>Jay #160:</p>\n<p>1) No, the gain isn&#8217;t split among the copies, but why should determine a clear answer?  Assume you&#8217;re not an &#8220;altruist&#8221;: you don&#8217;t care about maximizing the gain of any copies of yourself; you only care about maximizing <i>your own gain.</i>  Then the question is, why should you consider yourself 100 times more likely to be in the case with 100 copies, given that &#8220;where you are&#8221; is still the outcome of a fair coin flip?  Also, suppose the 100 copies differed in some trivial way, like having different hair colors, and you knew your hair color.  Would you <i>still</i> be 100 times more likely to be in the case with 100 copies?  Also, suppose that if the coin landed heads, God would make an <i>infinite</i> number of copies of you, while She&#8217;d only make one if it landed tails.  Could you then be <i>certain</i>, without ever leaving your armchair, that the coin must have landed heads?</p>\n<p>2) The problem arises because, by assumption, a state like 3/5|X>+4/5|Y> is a superposition of you seeing X and you seeing Y.  And if MWI&#8212;i.e., the thing that tells us to talk about such states in the first place&#8212;is accepted, then you <i>must</i> be willing to describe such a state in terms of &#8220;two copies of yourself,&#8221; one who sees X and the other who sees Y.  And, provided you agree that the Born rule works, it must make sense to say that with probability (3/5)<sup>2</sup>, &#8220;you&#8217;ll&#8221; find &#8220;yourself&#8221; as the copy who sees X, and with probability (4/5)<sup>2</sup>, &#8220;you&#8217;ll&#8221; find &#8220;yourself&#8221; as the copy who sees Y.  But then you run into the difficulty I described.</p>\n]]>", "author": "Scott", "published": "2014-09-03 17:47:55+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Jay #161: &#8220;If the Predictor never communicates with you, then is as real as any multiverse extension we can imagine.&#8221;  I don&#8217;t understand that sentence&#8212;you&#8217;ll need to clarify that.</p>\n]]>", "author": "Scott", "published": "2014-09-03 17:49:38+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>#162 #1</p>\n<p>> Assume you\u2019re not an \u201caltruist\u201d</p>\n<p>My own gain not that of the copies? Oh then I&#8217;m not a copy. I bet 1/2, and the copies should bet 1. Oh but I don&#8217;t know if I&#8217;m not one of the copies? Then you can&#8217;t say their gain is not mine: 101/100. No hard problem here. Just precise the question and it drops.  </p>\n<p>> suppose that if the coin landed heads, God would make an infinite number of copies of you</p>\n<p>I guess it depends on AC. <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/icon_smile.gif\" alt=\":-)\" class=\"wp-smiley\" /> </p>\n<p>#162 #2</p>\n<p>Sorry, still don&#8217;t get the problem. Probability conditionned to we saw X are straightforward. Probability conditionned we saw X and some alien quantum reversed our measurement, that&#8217;s not straightforward (until we know more about the alien), but that&#8217;s just not the same event.  </p>\n<p>#163 </p>\n<p>Sorry! A predictor with whom we can communicate cannot remain a perfect predictor. A predictor with whom we can&#8217;t communicate should have no consequence on &#8220;ought&#8221; questions.</p>\n]]>", "author": "Jay", "published": "2014-09-03 19:36:04+00:00", "title": "By: Jay"}, {"content": "<![CDATA[<p>#164 re #162#1: 101/100 would be good indeed, but what I meant was 100/101&#8230;</p>\n]]>", "author": "Jay", "published": "2014-09-03 20:08:46+00:00", "title": "By: Jay"}, {"content": "<![CDATA[<p>Jay #164: No, it doesn&#8217;t depend on AC, since we only need a countable number of copies.  I ask again: if there&#8217;s one copy of you if the coin lands heads, and infinitely many copies if it lands tails, <b>are you then certain&#8212;willing to bet your life&#8212;that the coin landed tails?</b>  Because that seems like the logical implication of your 100/101 answer.</p>\n<p>And sorry, the thing about &#8220;a predictor with whom we can communicate can&#8217;t remain a perfect predictor&#8221; is constantly repeated, but seems completely wrong to me.  Why can&#8217;t the predictor simply seal its prediction in an envelope, only opening the envelope after you did the thing it predicted you would do?  (Or send you its prediction in encrypted form, only decrypting it later?)</p>\n]]>", "author": "Scott", "published": "2014-09-03 21:36:49+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Scott #167</p>\n<p>How is this different from this sort of question:</p>\n<p>Say that you were isolated since birth and therefore have no idea how big the earth population is.<br />\nAnd you&#8217;re told that you have a rare genetic defect (e.g. you&#8217;re born with 6 fingers), which, you&#8217;re told, only happens with probability 0.0000000001&#8230; can you deduce, on your own, something about the current population of the earth (i.e. the current number of births a day)?</p>\n]]>", "author": "fred", "published": "2014-09-03 22:34:21+00:00", "title": "By: fred"}, {"content": "<![CDATA[<p>fred #167: Yes, of course it&#8217;s related to other anthropic observer-counting puzzles.  But as I explained in comment #135, it seems qualitatively worse, because if the other copies are really <i>subjectively identical</i> to you (or differ only in some way that&#8217;s designed to be trivial), then you don&#8217;t have the option of excluding them from your reference class.</p>\n]]>", "author": "Scott", "published": "2014-09-03 23:11:14+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>#166</p>\n<p>>No, it doesn\u2019t depend on AC,</p>\n<p>Actually this was a joke, but truth is I have no idea how to treat the infinite case. Would you mind if we stick to a finite number? As large as you wish, but finite please.  </p>\n<p>No, the way I&#8217;ll select my guess is unchanged if it&#8217;s one google or one hundred, conditionned there zero doubts on the terms of the bet. I also have little doubt that, for any bet where the problem seems anthropic, you can find another version with no anthropic problem at all. </p>\n<p>Example? World population, one hundred, all different human beings. God will flip a coin and ask you to bet. If you&#8217;re wrong, everyone&#8217;s dead. If it&#8217;s head and you can guess it, he will spare you and you only. If it&#8217;s tail and you can guess it, he will spare everyone. See that&#8217;s the very same bet? </p>\n<p>>Why can\u2019t the predictor simply seal its prediction</p>\n<p>Then it&#8217;s not interacting, and a predictor with whom we can\u2019t communicate should have no consequence on \u201cought\u201d questions.</p>\n<p>>Or send you its prediction in encrypted form, only decrypting it later?</p>\n<p>Better! Because you could choose to act based on this encrypted message, even if can&#8217;t decipher it. Then, there is no way the predictor can garantee a consistent story where your action based on the encrypted form leads to the deciphered content of this message.</p>\n]]>", "author": "Jay", "published": "2014-09-04 01:35:27+00:00", "title": "By: Jay"}, {"content": "<![CDATA[<p>Scott &#8230;</p>\n<p>&#8220;The intermediate position that I\u2019d like to explore says the following. Yes, consciousness is a property of any suitably-organized chunk of matter. But, in addition to performing complex computations, or passing the Turing Test, or other information-theoretic conditions that I don\u2019t know (and don\u2019t claim to know), there\u2019s at least one crucial further thing that a chunk of matter has to do before we should consider it conscious. Namely, it has to participate fully in the Arrow of Time. More specifically, it has to produce irreversible decoherence as an intrinsic part of its operation. It has to be continually taking microscopic fluctuations, and irreversibly amplifying them into stable, copyable, macroscopic classical records.&#8221;</p>\n<p>Does or can this occur in other than nonequilibrium systems?</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/M:Nick.html\">Nick M</a>", "published": "2014-09-04 04:38:48+00:00", "title": "By: Nick M"}, {"content": "<![CDATA[<p>Nick #170: No, it can only occur in nonequilibrium systems&#8212;that&#8217;s part of the content of this position.</p>\n]]>", "author": "Scott", "published": "2014-09-04 12:00:38+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Jay #169: Sorry, I still don&#8217;t understand your position, but at least it leads to an interesting technical question (see below).</p>\n<p>OK, I&#8217;m fine if you want to restrict to finite numbers.  Again I ask: if there&#8217;s only one copy of you if the coin lands heads, or a trillion copies of you if the coin lands tails, <i>are you essentially certain that the coin landed tails?</i></p>\n<p>Let&#8217;s not make this about betting money, or about equivalence or inequivalence to any other scenario you might name, or about any other issue.  It&#8217;s just a simple question about what you expect to see.</p>\n<p>Regarding the predictor: let&#8217;s say that it unseals the envelope the very second after you make your decision, showing it knew exactly what you would decide.  And let&#8217;s say it does this again and again, 50,000 times, every day of your life.  It&#8217;s not obvious to me that this wouldn&#8217;t change your entire conception of yourself in bizarre and profound ways.</p>\n<p>Now, for the interesting technical question: you say that even if you couldn&#8217;t decode an encrypted message predicting what you would do, you could still do something that <i>depended</i> on it, and thereby foil the predictor&#8217;s prediction.  However, I conjecture that in this scenario, <i>because</i> you can&#8217;t decode the message, it&#8217;s always possible for the predictor (in some appropriate sense) to find &#8220;fixed-points&#8221;: that is, predictions it can make for your behavior that remain valid, <i>even allowing that you can base your behavior on an encrypted version of the prediction.</i>  Let me think about it later and then maybe write another comment.</p>\n]]>", "author": "Scott", "published": "2014-09-04 12:14:07+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Scott #169</p>\n<p>&#8220;because if the other copies are really subjectively identical to you (or differ only in some way that\u2019s designed to be trivial), then[...]&#8221;</p>\n<p>But this entire discussion is about the impossibility of answering &#8220;are these two things subjectively identical?&#8221;, no?</p>\n<p>There seems to be also an assumption that a consciousness is somehow &#8220;tied&#8221; to a particular &#8220;cone&#8221; of space/time/matter.</p>\n<p>But maybe a &#8220;unit&#8221; of consciousness is tied to a particular mathematical structure (call it &#8220;state&#8221;, &#8220;pattern&#8221;, &#8220;information&#8221;&#8230;), which itself can be realized by many cones of space/time/matter (a consciousness also includes to a large extent the state of the surrounding environment).<br />\nSo, cloning a brain perfectly wouldn&#8217;t &#8220;create&#8221; an additional consciousness.<br />\nIn that sense, a consciousness is more like the concept of an algorithm, which can be realized in many different ways.</p>\n<p>As I was rambling about in a previous post, if the brains are true clones of each other (which could be achievable with programs that simulate brains and their environment, and we can then duplicate them at will), then it really doesn&#8217;t matter whether there is 1, 2, or a billion of them in the same state. They all realize one subjective experience.<br />\nAll this would suggest that subjective experience is mathematical in nature, which is what you are trying to avoid.</p>\n]]>", "author": "fred", "published": "2014-09-04 12:57:24+00:00", "title": "By: fred"}, {"content": "<![CDATA[<p>Scott #173</p>\n<p>&#8220;And let\u2019s say it does this again and again, 50,000 times, every day of your life. It\u2019s not obvious to me that this wouldn\u2019t change your entire conception of yourself in bizarre and profound ways.&#8221;</p>\n<p>Hmm, maybe that&#8217;s the modern equivalent of putting a caveman in front of a perfect mirror, then 10 of them, then 1,000 of them? <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/icon_smile.gif\" alt=\":)\" class=\"wp-smiley\" /> </p>\n]]>", "author": "fred", "published": "2014-09-04 13:23:13+00:00", "title": "By: fred"}, {"content": "<![CDATA[<p>Scott #172</p>\n<p>For the technical point, here&#8217;s what I&#8217;ll do. If the encrypted message forms a prime number, I&#8217;ll spell the bits loud. If it doesn&#8217;t, I&#8217;ll spell them in reverse order. Then I&#8217;ll encrypte this message, and again spell the bits if it&#8217;s a prime, spell it in reverse order if it&#8217;s not. And again and again.. </p>\n<p>Intuitively it&#8217;s impossible for Predictor to find a fixed point. If you can prove it, or break that conclusion, even probabistically, I&#8217;m very interested!</p>\n]]>", "author": "Jay", "published": "2014-09-04 13:56:43+00:00", "title": "By: Jay"}, {"content": "<![CDATA[<p>Scott #173<br />\nI&#8217;m noticing that you always strongly articulate your theories on consciousness/free will around the constraint of preserving the idea that we&#8217;re all unique beautiful un-cloneable snowflakes.</p>\n<p>Maybe this preoccupation is more a reflection of your (well-deserved) status in society as an exceptional individual, as someone who will be leaving explicit recognized achievements long after they&#8217;re gone <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/icon_smile.gif\" alt=\":)\" class=\"wp-smiley\" /><br />\nFor someone like myself, one of the very many average/unremarkable/fungible individuals, the concept of clonability isn&#8217;t that disturbing, on the contrary, it&#8217;s almost comforting somehow.</p>\n]]>", "author": "fred", "published": "2014-09-04 14:33:15+00:00", "title": "By: fred"}, {"content": "<![CDATA[<p>Jay #175: OK, I thought some more about the issue of fixed-points&#8212;i.e., can a predictor still predict what you&#8217;re going to do, even if you first get access to an encrypted version of the prediction?  I believe the solution is that it all depends on just how much the predictor is trying to predict about your future behavior.</p>\n<p>To illustrate, let&#8217;s first suppose the predictor is trying to predict your answer to a single yes/no question: that is, a single bit x&isin;{0,1}.  The predictor gives you an encrypted version Enc(x,r) (where r&isin;{0,1}<sup>n</sup> is some random &#8220;padding&#8221; bits, unrelated to x), using some secure digital commitment scheme, which we&#8217;ll assume to exist.  Then you apply some function f (which we&#8217;ll assume the predictor, being a predictor, knows), and you output f(Enc(x,r))&isin;{0,1}.  The question is whether the predictor can find a pair (x,r) such that</p>\n<p>x = f(Enc(x,r)).</p>\n<p>In this case, I claim that the answer is clearly yes: not only can the predictor find such an (x,r), but it can do so <i>easily</i>.  For if such a pair were hard to find, then it would almost always need to be the case that</p>\n<p>f(Enc(x,r)) = 1-x.</p>\n<p>But then taking 1-f(Enc(x,r)) would be an easy way to break the commitment scheme, contradicting the assumption of its security.</p>\n<p>The above argument can be generalized to where f(Enc(x,r)), the aspect of your behavior that the predictor is trying to predict, takes values not just in {0,1}, but in {1,&#8230;,M}, for any M=O(poly(n)).  (In other words, to where the predictor is trying to predict any O(log(n)) bits about your behavior.)  For then you&#8217;d be able to learn x, not with certainty, but with probability 1/poly(n) above random guessing, which would still violate the security assumption.  Furthermore, in this case the predictor could still find a fixed-point efficiently, by simply trying a bunch of random (x,r)&#8217;s until it found one for which f(Enc(x,r))=x happened to be satisfied.</p>\n<p>If you&#8217;re willing to make a <i>very strong</i> security assumption&#8212;e.g., that it&#8217;s hard even to learn x with probability 1/c<sup>n</sup> above random guessing, for some constant c>1&#8212;then you can even get that, for some constant &beta;>0, the predictor can find a fixed-point allowing it to predict &beta;n bits about your behavior.  (In this case, however, <i>finding</i> the fixed-point might be a computationally intractable problem for the predictor.)</p>\n<p>By contrast, if the predictor has to predict an <i>unlimited</i> amount of information about your future behavior, then there&#8217;s a sense in which it&#8217;s trivially impossible for the predictor to succeed, if it first has to give you an encrypted version of its prediction.  (And maybe that&#8217;s what you were trying to get at in your comment; I&#8217;m not sure.)</p>\n<p>To see this: suppose that, if the predictor gives you a k-bit encrypted prediction, then you respond by outputting k+1 bits.  There&#8217;s no encryption scheme with unique decoding that can map a (k+1)-bit plaintext to a k-bit ciphertext; ergo, the equation x = f(Enc(x,r)) can&#8217;t possibly be satisfied.</p>\n<p>Even here, however, there&#8217;s a possible loophole.  Namely, suppose the prediction consisted not of x itself, but of some compressed representation c(x) of x&#8212;say, a computer program that outputs x when run.  The fixed-point equation would then be</p>\n<p>x = f(Enc(c(x),r)).</p>\n<p><i>Now</i> can the predictor find an x that satisfies the equation?  Intuitively, it seems difficult, since it seems like you could always choose f so that f(Enc(c(x),r)) had Kolmogorov complexity a little bit greater than K(x), the Kolmogorov complexity of x itself.  (Either that, or else you could invert the encryption function Enc.)  But maybe something is possible here.</p>\n<p>Even if not, as I said, everything works fine as long as the predictor is trying to predict a number of bits about your behavior, that&#8217;s sufficiently less than the number of bits in the encrypted message that it sends you.  And of course, the predictor could then repeat this feat over and over, in order to make you arbitrarily unsettled about being the locus of your choices.</p>\n<p>(Final note: even if the predictor skips encryption entirely, and just puts its prediction in an envelope, there&#8217;s still a slightly-analogous issue that arises!  Namely, if you can <i>see</i> the envelope, know how large it is, and accordingly have an upper bound on the number of bits in it, then you could try to do something that requires more than that number of bits to describe.  But again, since a single envelope can store a very large number of bits, this doesn&#8217;t seem like a serious problem in practice.)</p>\n]]>", "author": "Scott", "published": "2014-09-04 17:52:12+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Luke #156</p>\n<p>I would say that *if* the quantum computers in comment #151, U_L and U_R, experience conscious awareness then we should treat them as &#8220;observers&#8221; so that both the spin components of the atom are now &#8220;real&#8221;. Subsequently reversing the quantum calculations should not change the reality of the observed spin components. My argument does not make any assumptions about whether the reverse computations are conscious or not.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Eastmond:John.html\">John Eastmond</a>", "published": "2014-09-04 17:23:37+00:00", "title": "By: John Eastmond"}, {"content": "<![CDATA[<p>Addendum to comment #178: OK, I thought more about the case where the predictor is allowed to issue its prediction in the form of a <i>computer program</i> that outputs your behavior when run.  And in this case, I claim, perhaps counterintuitively, that the predictor can <i>always</i> easily find a fixed point that lets it predict your behavior&#8212;regardless of how its program is encrypted before being handed to you, and indeed, even if its program is given to you completely in the clear!</p>\n<p>Why?  <a href=\"http://en.wikipedia.org/wiki/Kleene%27s_recursion_theorem\" rel=\"nofollow\">The Recursion Theorem</a>.</p>\n<p>Let Enc be the encryption function used by the predictor, if any (I&#8217;ll hardwire any random bits r as part of Enc), and let f be the function you apply to generate your behavior.  Then our problem is to find a computer program M that satisfies</p>\n<p>f(Enc(<M>)) = M(),</p>\n<p>where <M> is the code of M and M() is its output.  But the Recursion Theorem ensures that we can <i>always</i> find an M that satisfies this equation.</p>\n]]>", "author": "Scott", "published": "2014-09-04 19:12:19+00:00", "title": "By: Scott"}]