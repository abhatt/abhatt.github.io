[{"content": "<![CDATA[<p>David #164, your wider point is well taken. I think that the sub-K technology will only play into the first wave of QC devices until we have something like topological qubits. On the other hand there is still plenty of opportunity to do innovative things to cut down on the heat load (e.g. develop optical interconnects to replace some of the control lines). D-Wave has been pushing the engineering limits on that end, and I think this will be a real competitive edge for them as long as sub-K is the only QC game in town.</p>\n<p>Off the cuff I&#8217;d estimate there may be be a commercial window of about one to two decades for sub-K QC, and this goes for Google&#8217;s and IBM&#8217;s clean approach as well, so this will be additional impetus to commercialize this as quickly as possible.</p>\n]]>", "author": "Henning Dekant", "published": "2015-12-13 20:40:35+00:00", "title": "By: Henning Dekant"}, {"content": "<![CDATA[<p>Jeremy #164</p>\n<p>It looks like they have sensibly put in enough control overhead that they don&#8217;t have to redesign their fridge for each processor iteration.  What I am talking about though is the asymptotic scaling to very large numbers of qubits/processors, not the next generation or two.</p>\n<p>You say: &#8220;they can use that fixed number of control lines to generate any number of control signals&#8221;.  If we assume you don&#8217;t want the time taken to program and read out the processor to increase, then for this statement to hold you will need to increase the bandwidth of the control lines as you increase the number of qubits.  Increasing the bandwidth will increase the heat load as you can&#8217;t filter the thermal noise coming down the lines as heavily and the signals themselves will dissipate more.  Eventually you will hit a bandwidth limit (at the max speed of the demultiplexers for example) and have to add more lines.  Thus you are back to a linear scaling of the heat load from control lines with qubit number.  A few control lines might not seem like much but at these temperatures you are paying hardware costs upwards of $100k per milliwatt of cooling power!</p>\n<p>At this early stage it&#8217;s practically impossible to know how this will effect the economics of superconducting quantum devices but it&#8217;s certainly a big disadvantage of the technology.</p>\n]]>", "author": "David", "published": "2015-12-13 20:54:39+00:00", "title": "By: David"}, {"content": "<![CDATA[<blockquote><p>Seriously, I think with Google glasses and Apple watches just being the latest examples, the public is quite accustomed to that kind of hype and factors it in accordingly.</p></blockquote>\n<p>Evidence of a quantum speedup by D-Wave&#8217;s system is closer to confirmation of the existence of the Higgs boson than it is to any popular technology trend.</p>\n<p>In fact, D-Wave in this context is closer to CERN than it is to Google or Apple. They&#8217;re putting out a large hadron collider and claiming that the Higgs Boson exists before their particle accelerator has even detected it.</p>\n<p>IMO the eventual confirmation of a quantum speedup is really a turning point and should not be claimed so casually &#8211; I can see how you would have a different opinion if you don&#8217;t appreciate this.</p>\n]]>", "author": "Job", "published": "2015-12-14 00:02:20+00:00", "title": "By: Job"}, {"content": "<![CDATA[<p>Job #167: In a single comment, you&#8217;ve captured what I&#8217;ve been trying to say in these D-Wave threads for the past nine years.  I wish I&#8217;d written that comment.</p>\n]]>", "author": "Scott", "published": "2015-12-14 00:17:25+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>The cost analysis is misleading. It is like saying that it is a waste of money to develop ENIAC because you can outperform what it does easily with the amount of money that was spent on it. It doesn&#8217;t take potential future opportunities into account. The bet is that after it gets to work we can focus on cutting costs and make it available for masses.</p>\n]]>", "author": "Mark", "published": "2015-12-14 00:36:22+00:00", "title": "By: Mark"}, {"content": "<![CDATA[<p>It is also good to point out that we all agree that the D-Wave machine is not classical. By the way, more applied CS researchers don&#8217;t care as much about asymptotic as you think. It is already huge if we can compute usefully large instances 100x faster. That means cutting the costs by ~100x and increasing the profits by ~100x. In practice solutions seldom turn out to be as clean as theoreticians like.</p>\n]]>", "author": "Mark", "published": "2015-12-14 00:51:26+00:00", "title": "By: Mark"}, {"content": "<![CDATA[<p>One final thing: people are already building special purpose machines. If you have a machine that can save considerable time on a task that is performed as Google&#8217;s scale it makes sense to make one. Think about GPUs.</p>\n]]>", "author": "Mark", "published": "2015-12-14 00:54:34+00:00", "title": "By: Mark"}, {"content": "<![CDATA[<p>Mark #170, #171: OK, but we already addressed this.  Once you take into account the cost of encoding onto the Chimera graph, and (especially) the existence of classical algorithms like Selby&#8217;s, it&#8217;s totally unclear that one should expect any constant-factor speedup <i>either</i>.</p>\n<p>And yes, I completely agree that special-purpose machines can be extremely useful.  But among other things, that means that the right comparison going forward&#8212;if people want to talk about practicality at all, rather than about basic science&#8212;is D-Wave vs. what you could&#8217;ve gotten using classical GPUs.</p>\n]]>", "author": "Scott", "published": "2015-12-14 02:32:48+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>[&#8230;] provided this excellent write-up, and the former D-Wave critic-in-chief remains in retirement. Scott Aaronson&#039;s blog entry on the matter strikes a (comparatively) conciliatory tone. One of his comments explains one of the reason for [&#8230;]</p>\n]]>", "author": "D-Wave  Fast Enough to Win my Bet? | Observations on Quantum Computing & Physics", "published": "2015-12-14 03:01:52+00:00", "title": "By: D-Wave \u2013 Fast Enough to Win my Bet? | Observations on Quantum Computing & Physics"}, {"content": "<![CDATA[<p>&#8220;But among other things, that means that the right comparison going forward\u2014if people want to talk about practicality at all, rather than about basic science\u2014is D-Wave vs. what you could\u2019ve gotten using classical GPUs.&#8221;</p>\n<p>yeah, and not GPUs optimized for laptops and cell phones, but the latest sub 20nm gate length GPUs with all the bells and whistles, optimized for server farms, with cooling.</p>\n]]>", "author": "Marnie Dunsmore", "published": "2015-12-14 03:05:00+00:00", "title": "By: Marnie Dunsmore"}, {"content": "<![CDATA[<blockquote><p>If you have a machine that can save considerable time on a task that is performed as Google\u2019s scale it makes sense to make one. Think about GPUs.</p></blockquote>\n<p>That&#8217;s not a particularly good example since Google&#8217;s infrastructure is known to run on &#8220;cheap and fast&#8221; commodity hardware.</p>\n<p>Unspecialized hardware scales better because it&#8217;s cheaper to replace.</p>\n<p>Also, tasks that can be performed ahead of time, such as indexing, don&#8217;t benefit as much from specialized hardware unless it translates to what is essentially an asymptotic speedup, or something close to that.</p>\n]]>", "author": "Job", "published": "2015-12-14 03:37:14+00:00", "title": "By: Job"}, {"content": "<![CDATA[<blockquote><p>OK, but we already addressed this. Once you take into account the cost of encoding onto the Chimera graph, and (especially) the existence of classical algorithms like Selby\u2019s, it\u2019s totally unclear that one should expect any constant-factor speedup either.</p></blockquote>\n<p>On the contrary, if you count the overhead of encoding into the D-Wave&#8217;s connectivity graph (the so-called &#8220;Chimera&#8221; graph, not otherwise known to be an important class of graphs), then you can expect a big slowdown compared to classical computation for a lot of problems, by well worse than a constant factor.</p>\n<p>For instance there is the paper on using D-Wave to calculate Ramsey numbers, in particular to calculate R(3,3) and R(2,8).   Maybe they could use this new device to calculate R(2,10).   No one here should get too excited though, because there is a classical algorithm to calculate R(2,n) for any n that&#8217;s tough to beat.  (Scott already knows this algorithm, so this is not mainly addressed to him.)</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuperberg:Greg.html\">Greg Kuperberg</a>", "published": "2015-12-14 05:11:42+00:00", "title": "By: Greg Kuperberg"}, {"content": "<![CDATA[<p>[&#8230;] concerns a numerical recipe running on Google spiffy new D-Wave quantum computer. I also read \u201cGoogle, D-Wave, and the Case of the Factor-10^8 Speedup for WHAT?\u201d This paper points out that Google is making progress with the D-Wave quantum computer. How much [&#8230;]</p>\n]]>", "author": "Google and Quantum Computing : Stephen E. Arnold @ Beyond Search", "published": "2015-12-14 10:47:58+00:00", "title": "By: Google and Quantum Computing : Stephen E. Arnold @ Beyond Search"}, {"content": "<![CDATA[<p>Popular press messes things up again:</p>\n<p><a href=\"http://www.theguardian.com/commentisfree/2015/dec/13/the-quantum-computing-era-is-coming-qubits-processors-d-wave-google\" rel=\"nofollow\">http://www.theguardian.com/commentisfree/2015/dec/13/the-quantum-computing-era-is-coming-qubits-processors-d-wave-google</a></p>\n<p>Sheesh.</p>\n]]>", "author": "asdf", "published": "2015-12-14 15:11:04+00:00", "title": "By: asdf"}, {"content": "<![CDATA[<p>Job #175</p>\n<p>&#8220;That\u2019s not a particularly good example since Google\u2019s infrastructure is known to run on \u201ccheap and fast\u201d commodity hardware.&#8221;</p>\n<p>A question for you:  &#8220;Why do you refer to the GPUs that Google currently uses, which are optimized for servers, with  state-of-the-art caching, sub 20nm implementation, and massive use of parallelism, with decades of R&D behind these advances, as &#8220;&#8221;cheap and fast&#8221; commodity hardware&#8221;?</p>\n]]>", "author": "Marnie Dunsmore", "published": "2015-12-14 16:59:15+00:00", "title": "By: Marnie Dunsmore"}, {"content": "<![CDATA[<p>Marnie #175,</p>\n<p>Because i&#8217;m not referring to the relatively tiny fraction of Google&#8217;s infrastructure that&#8217;s using advanced GPUs to tackle specific problems.</p>\n]]>", "author": "Job", "published": "2015-12-14 18:02:50+00:00", "title": "By: Job"}, {"content": "<![CDATA[<p>Job #180</p>\n<p>While I admit I don&#8217;t have *inside* knowledge about how Google implements its servers, if you look at the market growth of server GPUs made by companies like AMD and Intel, it&#8217;s obvious that this is a huge and growing market.</p>\n<p>While it might be a tiny fraction of Google&#8217;s infrastructure now (again, I don&#8217;t know), in the coming years, these high end server GPUs will dominate in server farms.</p>\n<p>Why?  Heat and Speed.  </p>\n<p>Heat:  Generating a lot of heat costs energy, which the server farm owners pay for.  They also have to cool these server farms, which is another costly undertaking.  And then there are the environmental issues of server farm heat generation.</p>\n<p>Speed:  For most servers these days, response time really matters, so query look ups have to be fast.  So if Google is running on &#8220;cheap commodity&#8221; GPUs (I&#8217;m wondering about this) it must be because of legacy and replacement cost issues.  </p>\n<p>One thing is for sure:  the cost per bit of a quantum computer (cost defined as investment $$, speed and heat) won&#8217;t be cheaper than conventional server GPUs such as those in the Intel Xeon GPU family any time soon. It is conventional GPUs in this class against which quantum computers should be benchmarked.</p>\n]]>", "author": "Marnie Dunsmore", "published": "2015-12-14 19:19:50+00:00", "title": "By: Marnie Dunsmore"}, {"content": "<![CDATA[<blockquote><p>if you look at the market growth of server GPUs made by companies like AMD and Intel, it\u2019s obvious that this is a huge and growing market.</p></blockquote>\n<p>I imagine that&#8217;s because GPUs have become mainstream and there is demand for hardware-accelerated rendering on the cloud, among other reasons. If Google had tons of GPUs they would have a GPU offering in their cloud services (e.g. to match Amazon&#8217;s) which AFAIK they don&#8217;t.</p>\n<p>Also can we refer to GPUs as &#8220;specialized hardware&#8221; in the context of machine learning or bitcoin mining when 1) they&#8217;re so widely available and 2) they&#8217;re being used for a purpose for which they were not originally intended?</p>\n<p>I admit in 2) that i don&#8217;t know how GPUs are used in this context, but i&#8217;m guessing that it&#8217;s not to render graphics.</p>\n<p>In that sense, a hypothetical data-center filled with printers for the single purpose of PostScript processing is also using &#8220;specialized hardware&#8221;. It&#8217;s a bit misleading.</p>\n<p>Is there a better example of truly specialized hardware operating at a large scale?</p>\n]]>", "author": "Job", "published": "2015-12-14 20:55:38+00:00", "title": "By: Job"}, {"content": "<![CDATA[<p>[&#8230;] computer is maar een rekenmachine, een snelle, maar toch een heel beperkte machine. Dat stellen Scott Aaronson van het befaamde MIT in de VS en\u00a0Matthias Troyer van de niet minder befaamde ETH in Z\u00fcrich\u00a0in [&#8230;]</p>\n]]>", "author": "D-Wave van Google nog ver weg van kwantum'wonder' - Geleerd uitschotGeleerd uitschot", "published": "2015-12-14 21:00:07+00:00", "title": "By: D-Wave van Google nog ver weg van kwantum'wonder' - Geleerd uitschotGeleerd uitschot"}]