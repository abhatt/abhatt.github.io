[{"content": "<![CDATA[<p>RandomOracle #78:</p>\n<p>In my opinion there is only a problem with interpretation of probabilities in quantum mechanics. Without quantum mechanics, the world is fundamentally deterministic, and therefore the only kind of probabilities that can exist are subjective probabilities. And Bayesian probabilities are a perfect formalisation and interpretation of subjective probabilities. The observer role in the Bayesian framework is not mysterious at all, as the probabilities are really just the beliefs of an observer.</p>\n<p>The problem appears because in quantum mechanics the probabilities are, as far as we can tell, objective. And making sense of objective probabilities is something that the philosophers have, again in my opinion, failed to do. There are several interpretations of probability that try to deal with them, which are unsatisfactory for various reasons.</p>\n<p>The fundamental problem with the interpretations of objective probability that I know is that they are all classical, whereas the origin of the objective probabilities is quantum. On the other hand, most interpretations of quantum mechanics do not even try to address the question of what are these objective probabilities.</p>\n<p>The only attempt I know of solving this problem is the Deutsch-Wallace theorem in the Many-Worlds interpretation, and that&#8217;s why I&#8217;m so interested in investigating it.</p>\n]]>", "author": "Mateus Arajo", "published": "2018-02-03 12:01:59+00:00", "title": "By: Mateus Ara\u00fajo"}, {"content": "<![CDATA[<p>Lou Scheffer #80:</p>\n<ul>You are entirely correct that this is (usually) the way humans learn to build hardware. However, devices designed the other way have a far better track record, since biology specializes in building bigger and bigger systems out of crappy devices that don\u2019t work reliably. Neurons have not improved significantly between insects and humans. We just think better using 10^6 times more equally crappy components.</ul>\n<p>OK, but maintaining a quantum superposition with crappy components is a very specific challenge&#8212;harder than doing classical computation with crappy components&#8212;and we have theorems that give us some indication of just how non-crappy the components should be before it starts to work.  And the difference is, <i>D-Wave didn&#8217;t care if it was orders of magnitudes away from the known limits</i>.  Meanwhile, Google, IBM, Intel, Rigetti, and IonQ at least seem to be closing in on the limits, though it remains to be seen how much coherence they&#8217;ll be able to maintain as they scale up.</p>\n]]>", "author": "Scott", "published": "2018-02-03 12:37:25+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>someone #79: Well, the whole thing about a &#8220;spectrum,&#8221; is that it can really hard to know for sure whether someone is on it.  I&#8217;m curious: if someone is nerdy, uncomfortable in many types of social situations, and fond of deploying logical arguments in situations where most people aren&#8217;t&#8212;and if (let&#8217;s suppose) they also rock back and forth and fidget a lot, make funny hand movements, and are hypersensitive to starchy or uncomfortable clothes&#8212;then what else could you learn about the person that would let you say with confidence that they were <i>not</i> on the Asperger spectrum?</p>\n]]>", "author": "Scott", "published": "2018-02-03 12:46:44+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>RandomOracle #78: From my standpoint, the reason why classical probability theory is so much less problematic is that, even in a world where all the particles and fields had a definite &#8220;real&#8221; configuration at any given time&#8212;indeed, even in a world that was completely classical <i>and deterministic</i>&#8212;it would still be easy to tell a story about why rational agents would develop and use Bayesian probability theory to manage their uncertainty.  Whereas, for it to be a good idea for those agents to use quantum mechanics, something <i>different</i> needs to be true about the basic architecture of the world&#8212;so what is that someting?</p>\n<p>I think video of the Messenger Lectures should be available at some point, but I don&#8217;t know where or when.</p>\n]]>", "author": "Scott", "published": "2018-02-03 12:51:33+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>@Scott#84- the counterargument is that overusing it trivializes what people are going through. Look at your own life- there are forms of OCD that cause boys and girls to obsess about sexually harassing or raping people.If you had known that growing up, you might have felt less alone. But you didn\u2019t know that since everyone said \u201cI\u2019m so OCD\u201d when they liked to keep their socks in the drawer a certain way.</p>\n]]>", "author": "Michael", "published": "2018-02-03 13:51:05+00:00", "title": "By: Michael"}, {"content": "<![CDATA[<p>Michael #86: Actually, I&#8217;m not sure at all that that&#8217;s why I didn&#8217;t feel less alone, but OK, point taken. \ud83d\ude42</p>\n<p>And I have it on no less than the authority of Scott Alexander&#8212;maybe the one guy who&#8217;s infiltrated professional psychiatry while maintaining a worldview fundamentally similar to mine&#8212;that the DSM criteria for &#8220;spectrum disorders&#8221; are indeed just as arbitrary and subjective as they look to an outsider, and that psychiatrists are as confused about them as anyone else.</p>\n]]>", "author": "Scott", "published": "2018-02-03 15:11:30+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>The Many-Worlds and Bohmian people seem to me to be in the same game as people historically trying to interpret special relativity as an apparent effect due to contraction of atoms in order to save some pre-conceived notion of absolute time. Missing the fundamental point. In case of quantum mechanics being that it is a probabilistic theory with respect to the knowledge of an observer. Now in my view QBism == Copenhagen under the iso-morphism of replacing oracle sentences by Bohr with oracle sentences by Fuchs. But both at least take this fundamental point serious.</p>\n<p>Which brings me too your &#8220;complex numbers or get out&#8221; question. If you accept observers (conscience, bayesian agents or whatever term you use) as a fundamental part of reality / science, or rephrasing it &#8220;There is no universe / reality if there isn&#8217;t something that experiences it&#8221;. Which is already an accepted line of reasoning in multiverse arguments, so why not on the fundamental nature of physical law. Then the question becomes what is the most general form of physical law for such an observer.</p>\n<p>Simplifying the question of what is an observer to a Bayesian agent you can ask what is the most fundamental mathematical building block needed for a Bayesian agent. This is a question much like what is the most fundamental building block of space. Initially you think vectors but a more precise mathematical analysis reveals the existence of something more fundamental the spinor (in a way the square root of a vector) in the same way a more detailed analysis of probability theory would reveal the existence of a more fundamental norm 2 (again in a way the square root of classical probability). Now I would like to see some classification of probability theory. I suspect it&#8217;s true that norm 2 complex vector is in a definite way the most elementary representation. But maybe by letting go of some assumptions we can get even more fundamental representations which would be a promising starting point for theoretical physics.</p>\n<p>To my mind this way of thinking resolves a lot of fundamental problems. All questions that plaque the classical notion of physics, &#8220;why is it there something?&#8221;, &#8220;who/what created it and why does that exist?&#8221;, &#8220;are we living in a simulation?&#8221;, &#8220;does it matter what &#8220;substrate/encoding&#8221; is used for the simulation?&#8221;, evaporate by placing observer/experience at the same level as reality. Which is what quantum mechanics seems to tell us.</p>\n]]>", "author": "gerben", "published": "2018-02-03 17:45:27+00:00", "title": "By: gerben"}, {"content": "<![CDATA[<p>I agree with Matthew Ocko on this one. </p>\n<p>What he -Matthew- doesn&#8217;t realize is that most academics are this way, only Scott has the guts to say in public things that most academics only say in &#8220;petit comit\u00e9&#8221;. So in way, just as Trump&#8217;s is the most transparent White House in history, one could make the argument that Scott is one of the most transparent academics we have. </p>\n<p>Keep up with the good trolling Scott!!</p>\n]]>", "author": "Gatekeepers is back", "published": "2018-02-03 21:01:46+00:00", "title": "By: Gatekeepers is back"}, {"content": "<![CDATA[<p>Scott #85:</p>\n<p>I think there&#8217;s some confusion here that should be cleared up. When you say you are fine with Bayesian probability in the classical case, you are pretty clearly talking about the objectivist Bayesian interpretation of probability, and I&#8217;m guessing you also have no problem with objectivist Bayesianism in the quantum realm. However, QBism specifically uses the subjectivist Bayesian interpretation (also known as &#8220;personalist&#8221; or &#8220;Dutch book&#8221; Bayesianism), and almost all of your earlier arguments against QBism are really arguments against subjectivist Bayesianism which apply equally well in a purely classical setting. Note that &#8220;Bayesian probability&#8221; by itself doesn&#8217;t imply any interpretation of probability, it just means probabilities are updated according to Bayes&#8217; theorem. Actually it would probably be more accurate to refer to QBism as &#8220;Quantum subjectivism&#8221; because (as far as I understand) the subjective interpretation of probability is essential to the theory while the specific rules for updating probabilities are merely an implementation detail and could just as well be non-Bayesian.</p>\n]]>", "author": "Aula", "published": "2018-02-04 14:23:11+00:00", "title": "By: Aula"}, {"content": "<![CDATA[<p>Aula #90: No, I don&#8217;t see how subjective vs. objective is relevant to my point.  I was simply saying: take a deterministic cellular automaton, like Conway&#8217;s Game of Life.  Let it run until intelligent beings arise in the life-world.  Then it&#8217;s very easy to understand why those intelligent beings would most likely be approximate Bayesians about whatever they didn&#8217;t know.  <i>Subjective</i> Bayesians, of course, because there&#8217;s no objective probability in their world.  By contrast, it seems exceedingly unlikely to find these beings using the Born rule, unless something about their world is actually quantum.</p>\n]]>", "author": "Scott", "published": "2018-02-04 14:40:11+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Scott #84,87  Michael #86</p>\n<p>> I\u2019m curious: if someone is nerdy, uncomfortable in many<br />\n> types of social situations, and fond of deploying logical<br />\n> arguments in situations where most people aren\u2019t\u2014and if<br />\n> (let\u2019s suppose) they also rock back and forth and fidget<br />\n> a lot, make funny hand movements, and are<br />\n> hypersensitive to starchy or uncomfortable clothes\u2014<br />\n> then what else could you learn about the person that<br />\n> would let you say with confidence that they were not on<br />\n> the Asperger spectrum?</p>\n<p>Nerdy: DEFINITELY NOT AT ALL an Asperger symptom.<br />\nuncomfortable in social situations: NOT AT ALL<br />\ndeploys logical arguments: NOT AT ALL</p>\n<p>rock & fidget, make funny hand movements: this is more like it &#8212; depends on how vigorous and uncontrollable. If they have vigorous frequent hand flapping that they cannot stop and do in front of everyone, yeah, that&#8217;s a sign.<br />\nhypersensitive to clothes: also a sign.</p>\n<p>The thing is I&#8217;ve never ever seen a computer scientist or university prof show the last two. Never. Maybe they could hide hypersensitivity to clothes, but I never noticed someone look physically uncomfortable in their skin.<br />\nFlapping, well you can&#8217;t hide that. Come on.</p>\n<p>The first 3: not related at all. That&#8217;s just the prejudice I&#8217;m talking about.</p>\n<p>And about the &#8216;spectrum&#8217;, yeah, it&#8217;s a copout to hide the fact psychiatrists don&#8217;t know anything, as you point out. No one talks about a spectrum of pneumonia or cancer.</p>\n<p>Don&#8217;t know who Scott Alexander is. Who is he?</p>\n]]>", "author": "someone", "published": "2018-02-05 02:22:26+00:00", "title": "By: someone"}, {"content": "<![CDATA[<p>Scott # 83 says &#8220;D-Wave didn\u2019t care if it was orders of magnitudes away from the known limits&#8221;</p>\n<p>This betrays that either you have a clear bias against D-Wave, or you&#8217;re surprisingly naive! Or maybe both. It&#8217;s one thing to be skeptical of D-Wave&#8217;s claims and to be critical of their approach (those are both productive forms of engagement), but counterproductive to make sensational and baseless assertions like &#8220;D-Wave doesn&#8217;t care about noise.&#8221; I really have never understood why people who are critical of the fact that D-Wave&#8217;s qubits are noisy go on to claim that D-Wave doesn&#8217;t care about this. Of course D-Wave cares about this! They aren&#8217;t villains. They&#8217;re not trying to put one over on everybody. They&#8217;re trying to make the best system they can. I think everyone engaged in D-Wave skepticism and criticism should continue to do so, but would do well to stop assuming D-Wave are trying to cut corners and instead look at the work with the assumption that D-Wave is trying to optimize everything (including noise) throughout their development. The main difference from other projects is that, for D-Wave, scale / number of qubits carries a higher weighting in that optimization than noise &#8211; but this is fundamentally rooted in what seems to me to be a very logical and practical view: noise at 50+ qubits will be intrinsically higher than noise at ~2 qubits, so it doesn&#8217;t really matter what you can do at ~2 qubits if it&#8217;s not representative of the system at 50+ qubits. They&#8217;re building the 50+ qubit system to see what it looks like, characterize the noise in a system at that scale, and heavily iterate on that design to vastly improve the noise along the way. Compare the deisgn of their system at 128 qubits to that of ~2000 qubits today &#8211; the qubits are radically smaller. The whole system is significantly optimized. Each qubit in the 2000 qubit system sees considerably less noise than each qubit in the 128 qubit system &#8211; but it was building and testing the 128 qubit system that taught them how to do this.</p>\n<p>&#8220;D-Wave doesn&#8217;t care about noise!&#8221; What nonsense. They&#8217;re battling noise just like everyone else &#8211; they&#8217;re just tackling the problem in a different way.</p>\n]]>", "author": "Jeremy Stanson", "published": "2018-02-05 15:10:37+00:00", "title": "By: Jeremy Stanson"}, {"content": "<![CDATA[<p>Jeremy #93: All else equal, of course D-Wave prefers to have lower noise rather than higher noise.  Especially now that Geordie Rose is gone from the company, and with him his claims like &#8220;the ideas behind gate model QC are not good ideas,&#8221; which are a thousand times more sensational than anything I&#8217;ve ever said, not that people like you would ever hold him responsible for them.</p>\n<p>But the key problem is that D-Wave didn&#8217;t care <i>enough</i> about low noise to do the one thing that mattered: namely, refraining from claiming they had a &#8220;useful QC&#8221; before they got the noise low enough that they could scale to at least a few dozen qubits and clearly see that a <i>quantum computation</i> (in the sense of something hard to simulate classically, for reasons of interference and entanglement) was happening.</p>\n]]>", "author": "Scott", "published": "2018-02-05 15:32:37+00:00", "title": "By: Scott"}]