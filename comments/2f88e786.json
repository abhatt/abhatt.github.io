[{"content": "<![CDATA[<p>#8 James</p>\n<p>&#8216;It seems to me in the real world maximal \u03a6 would not be practical&#8230;.it seems to me that real world consciousness would work with some optimal level of \u03a6 rather than a maximal level.&#8217;</p>\n<p>Exactly.  Basic engineering principles of all types emphasize the desirability of modularity, rather than maximal \u03a6 or anything like that.  Unnecessarily large \u03a6 is bad, and surely it&#8217;s selected against.</p>\n]]>", "author": "Wondering", "published": "2014-06-26 17:05:06+00:00", "title": "By: Wondering"}, {"content": "<![CDATA[<p>@Wondering</p>\n<p>But is nature&#8217;s system design modular? I agree that as modern design low  \u03a6  is good. </p>\n<p>But that need not mean nature also adheres to the principle of selecting against large  \u03a6.</p>\n]]>", "author": "Rahul", "published": "2014-06-26 17:34:19+00:00", "title": "By: Rahul"}, {"content": "<![CDATA[<p>@darrell(#3)</p>\n<p>This post is actually the continuation of a previous discussion on this blog.</p>\n<p><a href=\"http://www.scottaaronson.com/blog/?p=1799\" rel=\"nofollow\">http://www.scottaaronson.com/blog/?p=1799</a></p>\n<p>To make the problem tractable,  Scott proposed limiting the problem to producing a theory that is consistent with our intuitions on what is conscious.  This is still a &#8220;pretty hard problem&#8221; but avoids thorny questions about the essence of consciousness.</p>\n]]>", "author": "Patrick", "published": "2014-06-26 17:49:34+00:00", "title": "By: Patrick"}, {"content": "<![CDATA[<p>#2 Scott</p>\n<p>I don&#8217;t understand why there should necessarily be a large difference in \\(\\phi\\) between a computer operating on a 2D grid of logic gates and a Turing machine with a 1D tape.</p>\n<p>In the case of, say, the Ising model, the structure of the couplings/correlations clearly depends on dimensionality, and I can see why \\(\\phi\\) should be different in 1 vs. 2D. But in the case of the computer program, it seems like \\(\\phi\\) should not be determined by the physical structure of the hardware, but rather by the correlation between abstracts states of the program, which shouldn&#8217;t depend on hardware. Perhaps this is not the case for the current definition of \\(\\phi\\), but it also seems like this would be a straightforward change to make.  </p>\n<p>So perhaps there are issues with ambiguity of what state space or coordinate system to use when computing \\(\\phi\\), but this seems like a relatively fixable problem, rather than a trenchant objection to the idea of \\(\\phi\\) as a necessary (but not sufficient) condition for consciousness.</p>\n]]>", "author": "Phil", "published": "2014-06-26 18:30:18+00:00", "title": "By: Phil"}, {"content": "<![CDATA[<p>Phil #12: FWIW, Giulio himself has been adamant that what matters for calculating &Phi; is the organization of the actual physical components (e.g., the gates and wires), rather than that of some abstract mathematical structure that the components are simulating.  He&#8217;s made that point repeatedly in his papers, claiming that it&#8217;s what rules out <i>existing</i> computers being very conscious.  And he made it again in his response to me, when he remarked that the Vandermonde matrix, being a mathematical abstraction, has no &Phi;-value; only actual logic circuits for applying the Vandermonde transformation have &Phi;-values.</p>\n<p>Of course, other people always have the option of breaking with IIT&#8217;s founder on this point.  But in Giulio&#8217;s defense, if we <i>did</i> go the abstract route, then it seems to me that ambiguities in <i>which</i> abstract structure we should be calculating &Phi; for would become immense, crippling our ability to get any clear predictions about the &#8220;amount of consciousness&#8221; present in anything.  The problem is that the same computational process can be described at many different levels of abstraction&#8212;in terms of the movements of electrons, the transistors, the pipelined process on the chip, the assembly-language instructions, the C/Python/etc. code, a mathematical description of the algorithm&#8212;and I see no reason whatsoever for the value of &Phi; to be robust to changes in the description level.</p>\n]]>", "author": "Scott", "published": "2014-06-26 18:56:49+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>#13 Scott</p>\n<p>I definitely agree that the value of \\(\\phi\\) will not be robust to changes in the level of description of a particular system, but I guess I am more optimistic than you that this problem is resolvable. The issue of ambiguity in absolute information content does not seem unique to IIT. For example, if we wanted to compute the information content of a message written on a piece of paper, we could consider the positions of molecules in the ink and paper, or the detailed shapes or misshapes of the letters, etc etc. But in practice we just consider the identity of the various letters and their frequencies.</p>\n<p>This is a tendentious example, and clearly the problem of ambiguity will be much harder in cases of potentially conscious systems, but again I think there at least some cases where the solution presents itself. If we are performing a whole brain simulation, then the correct level of description will involve the activation states of the &#8220;neurons&#8221; in the simulation, which will not depend on the hardware architecture or programming language, and which certainly leaves open the possibility that a computer simulation could be just as conscious as a real brain. Apparently Tononi does not agree with this view, but I am quite happy to try to rescue the idea of IIT from the idiosyncrasies of his approach.</p>\n<p>It also occurs to me that the issue of ambiguity in computing \\(\\phi\\) is not entirely unlike the ambiguity of classical entropy, where the absolute entropy of some physical system can depend on the scale of partitioning of microstates. We get around this by only considering relative entropy or changes in entropy, so that the constant associated with scale drops out. Perhaps there could be some kind of normalization scheme allowing a similar fix for computing \\(\\phi\\), though  it&#8217;s not clear how this might work (and again I am certainly oversimplifying the problem).</p>\n]]>", "author": "Phil", "published": "2014-06-27 18:24:29+00:00", "title": "By: Phil"}, {"content": "<![CDATA[<p>#13 Scott</p>\n<p>As said in the letter, I always thought it was unfair for phi to claim to be a sufficient measure for consciousness but then afterwards claim that the systems must also be &#8220;physical&#8221;.  If physicalness is a requirement for consciousness, then phi should be in some physical units, not abstract bits.</p>\n]]>", "author": "Virgil", "published": "2014-06-27 21:58:35+00:00", "title": "By: Virgil"}, {"content": "<![CDATA[<p>scott hits the nailon the head &#8211; as there are a countless many models of varying degrees of abstraction that fit a given system&#8217;s dynamics (in a perfectly counterfactual manner) the game of C measures has to be understood in the context of fundamental (or at least adequate) descriptions of the dynamics (physics) of a system under scrutiny.</p>\n<p>In all fairness this is what GT tries to do &#8211; his basic formulation is in terms of a &#8220;neural network&#8221; and the notion of abstraction he suggests is not that abstract &#8211; namely coarse graining (uniformly) in space and time (that is without &#8220;supervening&#8221; higher order structures which could be said to be in the eye of the beholder).</p>\n<p>The only piece missing here is that we have no idea how phi behaves as a function of sptio-temporal grain. Ideally we would like something like thermodynamics and statistical mechanics to be in play here: that is that at some point (grain in space and time) we can measure more or less all that we need (that is have sufficient knowledge) without having to track down every little sub-atomic critter (which of course doesn&#8217;t seem possible, but that&#8217;s another issue). </p>\n<p>GT seems to be claiming that we should expect something like that (at least with brain like systems) &#8211; that there will turn out to be a most informative scale for computing phi, because (his point) that conscious experience seems to have definite grain in space and time. However, given that we understand phi very little, and it seems very volatile (whereas i would think we would like a smooth measure or something) and sensitive to N it seems to me like pie in the sky at this point.</p>\n<p>It&#8217;s interesting to note that maybe a way out here is a multi-scale measure of C, as that in itself offers a &#8220;context free&#8221; signature to some extent.</p>\n]]>", "author": "TF", "published": "2014-06-28 11:49:53+00:00", "title": "By: TF"}, {"content": "<![CDATA[<p>Scott, probably something obvious, but one big difference between 1D grid and a 2D grid: with a 1D grid, the number of possible paths between any 2 nodes (as in maximum flow) is always one while with a 2D grid that number is very quickly gigantic.</p>\n]]>", "author": "fred", "published": "2014-06-28 19:55:39+00:00", "title": "By: fred"}, {"content": "<![CDATA[<p>fred #17: Well, sure, there&#8217;s a huge number of mathematical differences between 1 and 2 dimensions (as between 2 and 3 dimensions, and 3 and 4 dimensions, etc.), and any of them conceivably <i>could</i> be relevant to consciousness.  But the claim that any one of them <i>is</i> relevant to consciousness requires a strong argument.</p>\n]]>", "author": "Scott", "published": "2014-06-28 20:23:21+00:00", "title": "By: Scott"}]