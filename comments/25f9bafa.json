[{"content": "<![CDATA[<p>&#8220;No computation principle&#8221; #91, 107<br />\nDespite I am not very happy about a way of application of this principle to universal quantum computation by Gil (it looks like a try to criticize a hypothetical device suggesting even more hypothetical obstacles), the principle itself can be familiar due to more realistic reasons. </p>\n<p>After proofs about Turing completeness for more and more systems the property became less convenient for classification. </p>\n<p>Rather opposite problem: set of states and transitions that met universality may be small (or even measure zero) between set of whole states for system under consideration. In such a case \u201cno computation principle\u201d is rather not constrain, but extension of the view.</p>\n]]>", "author": "A.Vlasov", "published": "2014-03-05 09:38:33+00:00", "title": "By: A.Vlasov"}, {"content": "<![CDATA[<p>\u201cReversible cellular automata\u201d Brent #22.2, Scott #30.2<br />\nDue to some numerical simulations I myself was interested about porting BH information paradoxes into RCA, but was afraid to ask (c).<br />\nRelating \\(n\\) vs \\(2^n\\) : what about Holevo et al theorems? \u201cInternal\u201d complexity of quantum system can be, indeed, described by necessity to make exponentially big amount of steps, but we may only observe a classical picture. If the picture has a principle difference with stochastic picture often used for description of deterministic model due to lack of data or precision? BTW, reversible classical system (such as CA) also may need exponential number of steps to reach some configuration.</p>\n]]>", "author": "A.Vlasov", "published": "2014-03-05 10:18:16+00:00", "title": "By: A.Vlasov"}, {"content": "<![CDATA[<p>Scott #101:</p>\n<blockquote><p>In summary, \u201ccomputation\u201d doesn\u2019t have to be deterministic, it doesn\u2019t have to be serial, and it doesn\u2019t even have to be discrete. It\u2019s a way of thinking about any rule-governed process whatsoever. </p></blockquote>\n<p>With the Lenny Susskind kind of paper, are we transitioning to a model where computation is not just a convenient way of thinking about a rule-governed process but where computation theory itself provides the rules? The rules & constraints no longer seem an external set but derived from deep within computational complexity itself?</p>\n<p>Or am I confusing things?</p>\n]]>", "author": "Rahul", "published": "2014-03-05 13:32:46+00:00", "title": "By: Rahul"}, {"content": "<![CDATA[<p>Thank you Scott for sharing this awesomeness!</p>\n<p>Regarding the practical implications of T.T. work: I wonder, is there any theoretical obstacle preventing us from building exponentially expanding liquid computer instead of shrinking one? And is there a lower limit on an exponent?</p>\n<p>I mean, we could design the shrinking computer to first store some information at high energy/low scale and then to build an expanding computer so we could retrieve that information. It would be a great tool to explore distances and energy scales inaccessible by other methods. Water is probably too boring to be bothered with, but there may be other more interesting NS liquids (googling \u201cnavier stokes black hole\u201d gives <a href=\"http://arxiv.org/abs/0905.3638\" rel=\"nofollow\">this</a>, other non-atomic NS liquids are much appreciated!).</p>\n<p>I wander what limits relativity theory would impose on NS liquids if atomic discreteness would be somehow bypassed? Would NS equations still hold?</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/S=:Oleg.html\">Oleg S.</a>", "published": "2014-03-05 14:34:59+00:00", "title": "By: Oleg S."}, {"content": "<![CDATA[<p>Rahul #110: Well, if we&#8217;re talking about physics, then the <i>ultimate</i> rules that determine whether a given model of computation is or isn&#8217;t &#8220;reasonable&#8221; will always come from the laws of physics.  On the other hand, a theme that interests me a lot personally is that, once we develop a deep enough understanding of computation, we can often leverage that as a shortcut in situations where we don&#8217;t <i>know</i> all the relevant physics.  One common example is the use of the heuristic: <i>if your model of a physical system would imply the ability to solve NP-complete problems in polynomial time, then your model almost certainly fails to capture all of the relevant physics.</i>  And yes, a second example is the sort of work that Lenny, John Preskill, Patrick Hayden, and others have been doing on black holes.  There, since you don&#8217;t know all the relevant mixing dynamics on the event horizon (and the exact details of the dynamics would probably be complicated and not-terribly-relevant even if you <i>did</i> know them), you decide to study the questions of interest to you by <i>modeling</i> the event horizon as (say) a collection of qubits acted on by a random quantum circuit.  Philosophically, I don&#8217;t think is really different from what physicists have been doing for centuries: i.e., building &#8220;effective models&#8221; that capture some of the qualitative phenomena they care about, using whatever mathematical tools are available.  It&#8217;s just that quantum information and theoretical computer science have expanded the toolkit.</p>\n]]>", "author": "Scott", "published": "2014-03-05 16:45:34+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Oleg #111</p>\n<ul>I wonder, is there any theoretical obstacle preventing us from building exponentially expanding liquid computer instead of shrinking one?</ul>\n<p>Well, there&#8217;s conservation of energy (and of the total mass of liquid)!  But yes, the Navier-Stokes equations are apparently capable of exponential amplification (i.e., chaos), so it&#8217;s an interesting question whether one could &#8220;program&#8221; them to use exponential shrinkage and speedup to solve an arbitrarily hard computational problem within a fixed time bound, then use exponential expansion to make the answer readable again.  On the other hand, I don&#8217;t think there are <i>any</i> phenomena in nature for which the Navier-Stokes equations are a good approximation at anything smaller than the molecular (or conceivably nuclear?) scale.  So this would really be of mathematical interest; practical applications are doubtful. <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/icon_smile.gif\" alt=\":-)\" class=\"wp-smiley\" /> </p>\n]]>", "author": "Scott", "published": "2014-03-05 17:08:08+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Scott: &#8220;my specific difficulty is that I don\u2019t see any way to\u00a0<i>formalize</i>\u00a0a &#8216;mathematical principle of no computation.&#8217; &#8221;</p>\n<p>Thanks much, \u00a0Scott, for turning to the specifics. You are correct that this idea is related to my similar attempts to define &#8220;no computation&#8221; (or, more precisely, &#8220;no quantum fault-tolerance&#8221;) in the context of quantum evolutions. As I said, we even discussed in my debate with Aram this issue in the classical case and I expect the classical case to be even harder, because of the repetition mechanism which allows robust classical memeory. (This was also one of the problems I raised in the concluding post.) Here, I really hoped for a discussion of water computers and not another round of quantum debate, but you are correct that the two issues are related. Your piece on &#8220;why do I think scalable QCs are feasible, if I think scalable water computers are\u00a0<i>not</i>\u00a0feasible?&#8221; is very cute. Also, generally speaking, \u00a0I don&#8217;t see why an &#8220;I don&#8217;t see&#8221;-type argument should carry such an immense weight.</p>\n<p>&#8220;I see ways to formalize\u00a0<i>other</i>\u00a0principles that sometimes have \u201cno universal computation\u201d as side effects. &#8221;</p>\n<p> It will certainly be interesting to know what do you refer to.</p>\n<p>In any case, if 3D NS equations allows for finite-time blow up and universal computation it will be an important question why natural systems do not have this behavior (or perhaps what properties they sometime have that manifest this behavior). In fact, this is an important question even now, and certainly the question: if mathematics allows 3D NS to misbehave why it behaves well in nature is something people already thought a lot about, and I even mentioned one popular answer above.</p>\n<p>&#8220;Can you suggest any additional regularity constraint on the initial data to the Navier-Stokes equations, which you conjecture would prohibit universal computation (or for which such a statement follows from known results)?&#8221;</p>\n<p>Ohh, I think I can see why you cannot see: First (and mainly), why do you think that the &#8220;no computation principle&#8221; should be expressed in terms of initial data? or even, more strictly, regularity of the initial data. Second, while I am perfectly fine with staying with NS on the nose we may need to move to a perturbation of the equation, e.g., to express the molecular truncation that you suggested, or some stochastic version as many people suggested.</p>\n<p>A closely related analogy is the connection between thermodynamics and classical mechanics and later quantum mechanics. In principle, the laws of thermodynamics can be described in terms of initial data (or intermediate data) for the evolution describing nature. (Or perhaps they can even be derived just from the equations themselves.) But we do not wait for such a description to study thermodynamics.</p>\n<p>So if you do not insist that the principle of &#8220;no computation&#8221; should be described in term of initial data there could some interesting directions. Here are four ideas.</p>\n<p>1) We can consider a system to be computation free if it can be approximated in any scale by a bounded depth circuit. (You can see it mentioned above in the context of the 2D NS equation.)</p>\n<p>2) An interesting notion of &#8220;no computation&#8221; (or &#8220;no deep computation&#8221;) in the quantum case is described by &#8220;gapped evolutions with local terms.&#8221; We both attended a lecture in Jerusalem by Frank Verstraete\u00a0where, for certain systems, gapped evolutions with local terms were connected (not in full mathematical rigor) to &#8220;no-deep computation.&#8221; \u00a0We can seek analogous notions for fluids. Even on the technical levels of the analytical tools and inequalities there could be some connection.</p>\n<p>3) Indeed, when we move to stochastic (noisy) versions (or derived systems) of NS we can try to adopt some of the ideas from my work on quantum fault-tolerance. (Correlation behavior, time-smoothing, etc.) A basic difficulty would be that in general these ideas are not in conflict with a repetition mechanism that allows robust classical information.</p>\n<p>4) Still in a different (and vague) direction, one may relate a &#8220;no computation&#8221; principle to properties of Hamiltonian systems in symplectic geometry. We could vaguely hope in such a context a principle that computation occurs only &#8220;globally&#8221; and is not manifested for &#8220;small&#8221; subsystems.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kalai:Gil.html\">Gil Kalai</a>", "published": "2014-03-05 17:52:36+00:00", "title": "By: Gil Kalai"}, {"content": "<![CDATA[<p>Scott #110:</p>\n<blockquote><p> One common example is the use of the heuristic: if your model of a physical system would imply the ability to solve NP-complete problems in polynomial time, then your model almost certainly fails to capture all of the relevant physics.</p></blockquote>\n<p>Interesting. I&#8217;m curious, what are examples of physical systems where this heuristic was practically used & led to our naive models being corrected.</p>\n]]>", "author": "Rahul", "published": "2014-03-05 18:31:27+00:00", "title": "By: Rahul"}, {"content": "<![CDATA[<p>Rahul #115: You can use it to explain why the dynamics are important for protein folding, soap films, and spin glasses.  Also, once you know that quantum computers can only ever give a quadratic speedup for search, and no better, you can use that to upper-bound the spectral gaps of certain condensed-matter systems.  Finally, you can use the heuristic to critique speculative physical theories involving postselection or hypothetical nonlinearities in QM&#8212;such as Yakir Aharonov&#8217;s &#8220;postselected final state,&#8221; the Horowitz-Maldacena black hole final state proposal, and Anthony Valentini&#8217;s non-equilibrium matter.</p>\n<p>In each of these cases, there are also other ways of reaching the same conclusions, without invoking the concepts of NP-completeness or the hardness of search.  But to me, that&#8217;s simply like saying that, if someone rejects a physical proposal because it would violate no-superluminal signalling or the Second Law, there&#8217;s probably a more specific explanation of what&#8217;s wrong with the proposal that doesn&#8217;t invoke those concepts.  I&#8217;d say that, by this point, &#8220;generic search problems are hard&#8221; has proven itself as a fairly useful heuristic in a variety of contexts.</p>\n]]>", "author": "Scott", "published": "2014-03-05 19:23:06+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Scott #96, there is turbulence in 2D NS as well as 3D, I think.  #97 &#8211; the Clay NS problem postulates that the fluid fills all of Euclidean space, so no rectangular boundary.</p>\n]]>", "author": "asdf", "published": "2014-03-05 20:14:13+00:00", "title": "By: asdf"}, {"content": "<![CDATA[<p>Is this Terry Tao paper a contender for potentially bagging the Clay prize open for Navier Stokes? Or does the averaging disqualify it?</p>\n<p>Demonstrating  Finite-time blowup  for the 3D version even  for one specific initial condition is sufficient to win the prize, correct? Are we just waiting for the proof to be fully vetted or are their other problems that would prevent it from winning that prize?</p>\n]]>", "author": "Rahul", "published": "2014-03-06 05:29:19+00:00", "title": "By: Rahul"}, {"content": "<![CDATA[<p>I believe also that non-commutativity of rotation is critical in the Banach-Tarski paradox and why that one applies to 3d but not 2d.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cohen:Bram.html\">Bram Cohen</a>", "published": "2014-03-06 06:10:29+00:00", "title": "By: Bram Cohen"}, {"content": "<![CDATA[<p>asdf #117: So then, you can&#8217;t claim to have built a Navier-Stokes computer unless you&#8217;ve filled the entire universe with water?  I guess that would answer Rahul&#8217;s question about &#8220;practicality.&#8221;</p>\n]]>", "author": "Scott", "published": "2014-03-06 16:26:37+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Rahul #118: The averaging disqualifies it.</p>\n]]>", "author": "Scott", "published": "2014-03-06 16:27:50+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p><i>Is this Terry Tao paper a contender for potentially bagging the Clay prize open for Navier Stokes? Or does the averaging disqualify it?</i></p>\n<p>The latter.  The equations in question are not the Navier-Stokes equations, they&#8217;re an averaged verion.</p>\n<p><i>Demonstrating Finite-time blowup for the 3D version even for one specific initial condition is sufficient to win the prize, correct?</i></p>\n<p>That it would be!</p>\n]]>", "author": "Sniffnoy", "published": "2014-03-06 16:30:45+00:00", "title": "By: Sniffnoy"}, {"content": "<![CDATA[<p>In fact, a toroidal boundary is also okay for Millenium Prize-winning solutions to NS.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hopkins:Sam.html\">Sam Hopkins</a>", "published": "2014-03-06 16:44:29+00:00", "title": "By: Sam Hopkins"}, {"content": "<![CDATA[<p>Is there any hope at using the Navier-Stokes equation to attack the Extended Church Turing thesis? </p>\n<p>A classical simulation surely seems already much slower than letting a physical system evolve under given initial conditions? <i>A la </i> the Boson Sampling experiment. </p>\n<p>What stops us here? Is the computational complexity of solving NSE known? Are lower bounds on complexity known?</p>\n]]>", "author": "Rahul", "published": "2014-03-06 19:58:52+00:00", "title": "By: Rahul"}, {"content": "<![CDATA[<p>Rahul, suppose you have a tank of water and NSE allows blow up. If you give it a try, the blow up will stop at some point because of friction, ebullition, water dissociation or whatever first phenomena will show water does not obey NS all along down to the Planck scale. That seems no threat on ECT. </p>\n<p>Now suppose you can do the same using much stronger material (quark soup?), and blow up is possible, and Planck scale turns out limiting nothing (to the surprize of many, but not all, physicists). Yes, that could disprove the ECT. But don&#8217;t hold your breath. Quark soup is difficult to cook.</p>\n]]>", "author": "Jay", "published": "2014-03-06 20:38:39+00:00", "title": "By: Jay"}, {"content": "<![CDATA[<p>Rahul #124: No, as Jay says, the Navier-Stokes equations are not a threat to the Extended Church-Turing Thesis.  Even assuming the equations theoretically allowed hypercomputation (which hasn&#8217;t been shown yet), we know that they break down at the molecular scale and even above that.  And if you&#8217;re only interested in their behavior above some distance scale, then you can use e.g. finite-element methods to simulate them in polynomial time using a classical computer.  It&#8217;s true that the simulation might be less efficient <i>in practice</i> than just watching a water tank evolve (though even in practice, computer simulations of fluid flow have gotten better and better&#8230;).  But there&#8217;s nothing there that could make the simulation <i>exponentially</i> inefficient as a function of the number of water molecules&#8212;and that&#8217;s the crucial difference from BosonSampling and related quantum proposals.</p>\n]]>", "author": "Scott", "published": "2014-03-06 23:38:30+00:00", "title": "By: Scott"}]