[{"content": "<![CDATA[<p>Scott, is there a link to the conference papers? The results about edit distance and i.o seem pretty cool and I wanted to check out the source material.</p>\n<p>Thanks!</p>\n]]>", "author": "F. Marshall", "published": "2015-06-21 11:10:55+00:00", "title": "By: F. Marshall"}, {"content": "<![CDATA[<p>F. Marshall #17: I <i>did</i> put links in my post to the papers I talked about.  But you can also get the complete STOC program, with links to the full-text papers, <a href=\"http://acm-stoc.org/stoc2015/program.html\" rel=\"nofollow\">here</a>.</p>\n]]>", "author": "Scott", "published": "2015-06-21 13:30:55+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Scott,  I am curious about obfuscated circuits in a less theoretical sense.  The page you linked discussed obfuscating a circuit so that it was impossible to recover the source of the original program, but usually you aren&#8217;t interested in recovering the whole source code, but rather to perform some sort of analysis of the source code (for example, to detect malware).  Is there any sort of research on what sort of analysis it is possible to &#8220;obfuscate away&#8221;?  You mentioned that it&#8217;s not possible to obfuscate away /all/ analysis, but is there more information about what analysis is hard/easy to hide from?  In particular I am interested in code-coverage metrics.</p>\n<p>I suppose the answer might depend on what sort of overhead you are allowed.  Are these theoretical results about practically feasible overheads?  What about for practically stealthy-level overheads? (<1x)</p>\n]]>", "author": "Jeremy", "published": "2015-06-21 14:06:30+00:00", "title": "By: Jeremy"}, {"content": "<![CDATA[<p>I should also comment that there seems to be a distinction between the sort of theoretical obfuscation you are discussing and another sort.    I feel like the obfuscation you discuss is more like asking the question &#8220;is this thing I am measuring really a property of the function I am computing, or just the algorithm I am using to compute it?&#8221;</p>\n<p>It seems like there might be some properties of the function itself that you want to obfuscate. For example, I could imagine asking about some property of the function that is easy to compute given the smallest/simplest program that computes the function, but could be obfuscated with minimal overhead into a form that it is hard to compute the property.</p>\n]]>", "author": "Jeremy", "published": "2015-06-21 14:37:02+00:00", "title": "By: Jeremy"}, {"content": "<![CDATA[<p>Jeremy #19, #20: I&#8217;m not sure if I understand the distinction you&#8217;re getting at.  The point of obfuscation is to give someone code that lets them evaluate some function f, as a &#8220;black box,&#8221; but <i>not</i> give them additional information about f that they could normally get by examining the code.</p>\n<p>Having said that, I completely agree with you that the impossibility result of Barak et al., seminal though it was, is <i>extremely</i> far from the last word in practice.  In practice, first of all, it&#8217;s only particular functionalities that we&#8217;d normally want to obfuscate, not arbitrary ones.  And even more to the point, we wouldn&#8217;t normally have the super-stringent requirement that the adversary learn <i>absolutely nothing</i> beyond what they could learn from black-box access.  Rather, as you suggested, there&#8217;s probably some <i>particular</i> secret (which might vary depending on the application) that we want the adversary not to learn.</p>\n<p>There <i>has</i> been a good deal of work about how to get around the Barak et al. impossibility result by relaxing the assumptions.  As an example, see <a href=\"http://www.cs.columbia.edu/~hoeteck/pubs/obf-point-stoc05.pdf\" rel=\"nofollow\">this paper</a> by my former officemate Hoeteck Wee, which shows how to obfuscate point functions (that is, functions that simply recognize a secret password) given powerful enough one-way functions.  Or see <a href=\"http://theoryofcomputing.org/articles/v009a009/v009a009.pdf\" rel=\"nofollow\">this quantum money paper</a> by myself and Paul Christiano, where we needed a restricted form of obfuscation for testing membership in a &#8220;hidden subspace&#8221; (one that didn&#8217;t reveal a basis for the subspace), and came up with a candidate way to do it, using low-degree polynomials that happen to vanish on the subspace in question.  Or, of course, check out the huge amount of recent work on i.o., which gives you an extremely interesting sort of security guarantee: that, sure, running a program through i.o. might fail to hide some secret about it, but if so, then <i>it wasn&#8217;t your fault</i>, because no other way of obfuscating that program would&#8217;ve hidden that secret about it either!</p>\n<p>Maybe the experts can add some more references.</p>\n]]>", "author": "Scott", "published": "2015-06-21 22:56:35+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Thanks for the links, the introductions were well written and you&#8217;re right that I was confused about the definition of obfuscation.  I think this paragraph from your original post confused me:</p>\n<p><em>>obfuscating a program in such a way that no adversary can figure out anything about which program you started with, among all the possible programs that compute the same function in roughly the same amount of time. </em></p>\n<p>I read this paragraph as saying that it was impossible to pinpoint which original program you started out with, out of the set of programs which compute the same function in roughly the same amount of time.  </p>\n<p>I think what you meant is that it was only possible to figure out things that were easy to figure out from <em>every</em> member of that set.</p>\n]]>", "author": "Jeremy", "published": "2015-06-22 00:49:16+00:00", "title": "By: Jeremy"}, {"content": "<![CDATA[<p>Very interesting. In the past, as a physicist I worked on machine learning algorithms implemented in custom chips for HEP applications. Indeed they got crushed by Moore. I have to look into this again if miniaturization is really stopping. GPUs are another rising resource in HEP applications. What about the coupling of GPUs and machine learning algorithms?</p>\n]]>", "author": "anon", "published": "2015-06-22 13:04:44+00:00", "title": "By: anon"}, {"content": "<![CDATA[<blockquote><p><b>anon</b> wonders (#23)  &#8220;I have to look into this again if miniaturization [algorithms implemented in custom chips] is really stopping.&#8221;</p></blockquote>\n<p>Intel&#8217;s $16B acquisition this month of the programmable-logic device (PLD) manufacturer Altera is a strong market-indicator that the custom-chip revolution isn&#8217;t &#8220;really stopping&#8221;, but rather has pivoted to PLDs.</p>\n]]>", "author": "John Sidles", "published": "2015-06-22 13:58:57+00:00", "title": "By: John Sidles"}, {"content": "<![CDATA[<p>@Rahul Verilog emulation, at the core, consists of</p>\n<p>1. Bring some bits into memory<br />\n2. Perform a simple logical operation on them.<br />\n3. Store the result.</p>\n<p>This sounds a lot like neural network simulation.</p>\n<p>At scale, the hardest problem isn&#8217;t making the ALU go faster, it&#8217;s getting the bits in the right register at the right time.  That is, bandwidth to memory and memory organization/retrieval is the hard part.   Most ALU&#8217;s at this level kind of starve for data.</p>\n<p>I expect neural networks to have the same issues.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gischer:Jay_L=.html\">Jay L. Gischer</a>", "published": "2015-06-22 15:34:50+00:00", "title": "By: Jay L. Gischer"}, {"content": "<![CDATA[<p>@anon 23</p>\n<p><a href=\"http://www.nvidia.com/object/machine-learning.html\" rel=\"nofollow\">http://www.nvidia.com/object/machine-learning.html</a><br />\n<a href=\"http://arxiv.org/pdf/1404.5997v2.pdf\" rel=\"nofollow\">http://arxiv.org/pdf/1404.5997v2.pdf</a><br />\n<a href=\"http://blog.diffbot.com/machine-learning-in-the-cloud/\" rel=\"nofollow\">http://blog.diffbot.com/machine-learning-in-the-cloud/</a></p>\n]]>", "author": "Jay", "published": "2015-06-22 16:38:27+00:00", "title": "By: Jay"}, {"content": "<![CDATA[<p>Jay, John, thanks for the informations!</p>\n]]>", "author": "anon", "published": "2015-06-23 04:30:00+00:00", "title": "By: anon"}, {"content": "<![CDATA[<p>Jay #8 says &#8220;What I don\u2019t get is why a computer scientist would despise the HBP (or, more precisely, the computer part of the HBP).&#8221;</p>\n<p>Many neuroscientists and computer scientists hate the HBP for a common reason &#8211; it&#8217;s a giant waste of a billion euros.   The HBP uses huge and expensive hardware to run very large, brain scale simulations.  But the circuits they wish to simulate are not known, so they must create them using statistical wiring rules.  But the general feeling is that this does not encode enough of how biology does wiring, or learning, or anything else, for any interesting emergent behavior to emerge.  So the HBP is unlikely to advance our understanding of how the brain really works.</p>\n<p>Likewise the HBP is unlikely to advance computer science.  It&#8217;s like lots of other large parallel computer setups.   And like any other hardware it will be obsolete in a few years.</p>\n<p>Basically, it&#8217;s like trying to reach the moon by scaling up an Estes rocket until it costs a billion dollars.  It&#8217;s not going to work and you won&#8217;t even learn anything interesting by its failure.  You would advance the field a lot more by figuring out first how things should work before scaling up.  The opportunity cost that has been squandered by the Blue Brain project is enormous &#8211; a significant fraction of the budget of the field as a whole.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Scheffer:Lou.html\">Lou Scheffer</a>", "published": "2015-06-23 21:30:35+00:00", "title": "By: Lou Scheffer"}]