[{"content": "<![CDATA[<p>Arko #11: Yes, that&#8217;s a good qualitative way to describe what&#8217;s going on in the coffee-cup paper&#8212;that you lose symmetries as you start mixing, but then regain the symmetries (macroscopically, anyway) once the system is fully mixed.  The interesting part is to prove that enough symmetry gets lost, at intermediate times, for the coarse-grained entropy <i>ever</i> to become large.</p>\n]]>", "author": "Scott", "published": "2016-08-01 06:21:05+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>There are two kinds of scientists: those that make mistakes and admit to them, and those that make mistakes and do not admit to them. </p>\n<p>Unfortunately there are plenty of scientists that think they belong to the non-existent category of scientists that do not make mistakes.</p>\n<p>Congratulations to Scott for putting himself in the first category!</p>\n]]>", "author": "Mateus Arajo", "published": "2016-08-01 08:34:31+00:00", "title": "By: Mateus Ara\u00fajo"}, {"content": "<![CDATA[<p>I wonder if the reportedly large number of errors in math publications is due (at least in part) to people trying to prove too much too quickly?<br />\n  Gauss and Godel published very little, and I don&#8217;t think there was a single error in their works.  It would be interesting to know if, by contrast, the extremely prolific great mathematicians (Euler, Erdos) paid the price for their torrential output with (initially) undetected errors&#8230;</p>\n]]>", "author": "Vadim", "published": "2016-08-01 13:09:44+00:00", "title": "By: Vadim"}, {"content": "<![CDATA[<p>Off-topic comment: Since discussions in the comments of this blog tend to become long and tangled with cross-references, I wrote <a href=\"https://gist.github.com/anonymous/1967bdba0afa4c5e3fce7ee5e98fbd09\" rel=\"nofollow\">a script</a> to make following them much easier. Maybe others can find it useful too.</p>\n<p>(Of course, if everyone had just used <code><a href=\"#comment-NNNNN\"></code> to link to comments, I wouldn&#8217;t have to write this, but who has the time for that?)</p>\n]]>", "author": "Anonymous", "published": "2016-08-01 15:40:09+00:00", "title": "By: Anonymous"}, {"content": "<![CDATA[<p>[Direct installation link: <a href=\"https://gist.github.com/anonymous/1967bdba0afa4c5e3fce7ee5e98fbd09/raw/eec030d227154536f90fa33dbb4b13f5fe5cb2f1/shtetl-link.user.js\" rel=\"nofollow\">here</a>]</p>\n]]>", "author": "Anonymous", "published": "2016-08-01 15:41:38+00:00", "title": "By: Anonymous"}, {"content": "<![CDATA[<p>Vadim,</p>\n<p>The errors will be detected eventually, so what is the problem? Here are how it went for three great mathematicians:</p>\n<p>Euler applied Newtonian physics and calculus to every problem that walked, hopped, or crawled down the road, showing everyone how to attack things. He worked harder and faster as he got older and weaker and blinder. Supposedly he died from not stopping writing to eat. (The last is likely an E.T. Bell fiction).</p>\n<p>Gauss published little, and often only after other people published something, and Gauss dug out a paper he had not quite polished yet, with a better version. All of his published stuff was pretty much perfect. So how much did Gauss take to the grave with him? Probably plenty. Who knows what he was working on for fun. Maybe Fermat&#8217;s proof that was a bit too big to fit on the margin? It is a big loss that Gauss did not publish everything he thought about but wasn&#8217;t sure of. Probably plenty of it remains undiscovered to this day.</p>\n<p>Riemann published a lot in his brief life, much of it revolutionary and starting new subject areas in math. For example, much of non euclidean geometry started with Riemann&#8217;s unprepared, ad hoc comments in answering a question from Gauss at his Ph.D. comps. Supposedly Riemann never wrote a correct proof in his life. I think most or all of what he published was basically correct, but his proofs were not solid. Riemann&#8217;s errors provided plenty of material to work on for generations of mathematicians. So what is wrong with that state of affairs?</p>\n]]>", "author": "Raoul Ohio", "published": "2016-08-01 15:57:34+00:00", "title": "By: Raoul Ohio"}, {"content": "<![CDATA[<p>It&#8217;s interesting that the conclusion based on simulation data is the one that ends up being really wrong.<br />\nThe proof isn&#8217;t always in the pudding I guess?!</p>\n]]>", "author": "fred", "published": "2016-08-01 17:28:09+00:00", "title": "By: fred"}, {"content": "<![CDATA[<p>fred #18: I&#8217;ve actually found that to be a common occurrence.  It&#8217;s <i>ridiculously</i> hard to extract reliable conclusions from data; a few experiences trying to do empirical studies gave me enormous respect for the people who do that for their day job.  Rather than simulation data leading to understanding, often it&#8217;s understanding (sometimes, though not always, in the form of a proof) that leads to realizing why your faulty simulation data was misleading you&#8212;and also why you never even needed to collect the data in the first place, had you been thinking clearly enough! \ud83d\ude42  On the other hand, without going to the trouble to get the data, your brain is also much less likely to have been primed for understanding&#8212;so the two things really do feed off each other.</p>\n]]>", "author": "Scott", "published": "2016-08-01 17:42:32+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Please allow me to join in the appreciation of yet another fine <i>Shtetl Optimized</i> essay, which brings several mathematical anecdotes and essays to mind.</p>\n<p>Simplest are the many stories relating to the algebraic geometer Solomon Lefshetz, of whom it was said: &#8220;Never an incorrect theorem; never a correct proof&#8221;.  Lefshetz himself would tell the following story to illustrate (what Lefshetz regarded as) a proper disdain for excessive mathematical rigor:<br />\n<blockquote>E. H. Moore started a lecture by saying: &#8220;Let <i>a</i> be a point and let <i>b</i> be a point.&#8221; Lefshetz interrupted Moore, asking &#8220;Who don&#8217;t you just say &#8216;Let <i>a</i> and <i>b</i> be points?'&#8221; Moore answered &#8220;Because <i>a</i> may equal <i>b</i>.&#8221;  Lefshetz got up and left the lecture room.</p></blockquote>\n<p>A broader literature is illuminated when we consider mathematical mistakes in light of Jacobi&#8217;s maxim <i>&#8220;man muss immer umkehren&#8221;</i> (commonly translated as &#8220;invert, always invert&#8221;). </p>\n<p>Applying Jacobi&#8217;s inversion maxim leads us to inquire whether realms of mathematics exist in which mistakes <i>never</i> (or almost never) are seen.  One such realm is the formal verification (by machine) of the theorems of mathematics that are regarded as fundamentally important and/or exceptionally beautiful.  The Dutch mathematician Freek Wiedijk maintains a <a href=\"http://www.cs.ru.nl/~freek/100/\" rel=\"nofollow\">list of such proofs</a>, and the remarkable &#8220;dog in the night&#8221; phenomenon is that (up to the present time) no widely accepted mathematical theorem has ever been found to be in error.</p>\n<p>How is it, we are led to wonder, that the demonstrably fallible reasoning of mathematicians like Lefshetz has reliably led humanity to results that later are proved to be correct?  How is it that mathematicians can be right in their theorems very much more often than they are right in their proofs?</p>\n<p>Terry Tao&#8217;s essay &#8220;What is good mathematics?&#8221; (2007, arXiv:math/0702396) lists twenty-one elements of good mathematics (emphasizing that no such listing can be complete); the list grounds mathematics in &#8220;insight&#8221;, &#8220;discovery&#8221;, &#8220;vision&#8221;, &#8220;taste&#8221;, &#8220;beauty&#8221;, &#8220;elegance&#8221;, and &#8220;intuition&#8221;.  These are parcellations of cognition at which human mathematicians like Lefshetz (and Tao) excel, but that escape machine-checkable formalization (by any methods that we presently know). </p>\n<p>For concrete examples of &#8220;insight&#8221;, &#8220;discovery&#8221;, &#8220;vision&#8221;, &#8220;taste&#8221;, &#8220;beauty&#8221;, &#8220;elegance&#8221;, and &#8220;intuition&#8221;, a good exemplar (as it seems to me) is Tao&#8217;s recent preprint &#8220;Finite time blowup for Lagrangian modifications of the three-dimensional Euler equation&#8221; (2016, arXiv:1606.08481v1).  The gratifying feature (for me) regarding this preprint, is that Tao begins by rewriting the fluid dynamical equations entirely (as Tao calls it) &#8220;the language of differential geometry.&#8221;  This language, which privileges cognitive naturality and universality relative to physicality, is sufficiently conducive to Lefshetz-style &#8220;insight&#8221;, &#8220;discovery&#8221;, &#8220;vision&#8221;, &#8220;taste&#8221;, &#8220;beauty&#8221;, &#8220;elegance&#8221;, and &#8220;intuition&#8221;, while retaining compatibility with rigor, that by it Tao can <a href=\"https://terrytao.wordpress.com/2016/06/29/finite-time-blowup-for-lagrangian-modifications-of-the-three-dimensional-euler-equation/#comment-470259\" rel=\"nofollow\">concretely aim to solve the Clay Institute&#8217;s Millennium Prize Navier Stokes Problem</a>:<br />\n<blockquote>&#8220;It is widely believed that for the Euler equations (in which the viscosity is zero), finite time blowup should be possible, and perhaps even quite generic. For positive viscosity (Navier-Stokes), there is less consensus, but personally I believe it should be possible to construct some very special solutions which blow up in finite time, even if \u201cgeneric\u201d solutions will continue to exhibit global regularity (for some vaguely defined notion of \u201cgeneric\u201d).&#8221;</p></blockquote>\n<p>So there&#8217;s not much doubt that soft notions of \u201cinsight&#8221;, &#8220;beauty&#8221; (etc.) are entirely compatible with hard-nosed programs for tackling even the hardest open problems in mathematics.</p>\n<p>What relevance can Tao&#8217;s methods have for folks of undistinguished mathematical talent (that would be me) whose interests are quantum dynamical rather than classical?  Two considerations are relevant (as it seems to me).</p>\n<p>The first consideration is that Tao is agnostic regarding the blow-up (or not) of Navier Stokes flows; the natural/universal modern mathematical formalism of differential and algebraic geometry is helpful either way.  Similarly in regard to the achievability of Quantum Supremacy (or not), modern geometric formalisms help focus our &#8220;insight&#8221;, &#8220;discovery&#8221;, &#8220;vision&#8221;, &#8220;taste&#8221;, &#8220;beauty&#8221;, &#8220;elegance&#8221;, and &#8220;intuition&#8221; in reading Terry Rudolph&#8217;s Supremacy-positive &#8220;Why I am optimistic about the silicon-photonic route to quantum computing&#8221; (arXiv:1607.08535), and equally Gil Kalai&#8217;s Metameracy-positive &#8220;The Quantum Computer Puzzle&#8221; (arXiv:1605.00992).</p>\n<p>Here I have borrowed from Aram Harrow (and from my wife Connie, who works in printing) the notion of &#8220;metameracy&#8221;, here regarded as the systematic description by Tao-style geometric formalisms of quantum dynamical systems that look effectively like finite-dimensional Hilbert spaces, while at a deeper level evolving on tensor-product spaces whose algebraic structures are thermodynamically and informatically compatible with the Hilbert-space resolution of those structures (QED being a bountiful source of such systems).  </p>\n<p>Fortunately the translation of traditional quantum &#8220;bra-ket&#8221; language into Tao-style geometric language is sufficiently guided by considerations of naturality and universality, as to largely depend upon technique rather than inspiration.</p>\n<p>The blue-collar lesson-learned (for me at least) from these Tao-style metameric constructions, is that comparably valuable to considerations of &#8220;rigor&#8221; in the study of quantum systems are Tao-style considerations of &#8220;insight&#8221;, &#8220;discovery&#8221;, &#8220;vision&#8221;, &#8220;taste&#8221;, &#8220;beauty&#8221;, &#8220;elegance&#8221;, and &#8220;intuition&#8221;, which as illuminated by modern metameric methods, are valuable in respect to all eventualities in the quest for supremacy&nbsp;&hellip; that is, the supremacy of our quantum understandings over our quantum confusions.</p>\n<p>So which is correct?  Terry Rudolf&#8217;s quantum optimism or Gil Kalai&#8217;s quantum pessimism?  Perhaps this isn&#8217;t the most interesting question we can ask, given that both researchers are providing us with plenty of inspirational grounds for applying Tao-style metameric methods to gain illuminating quantum dynamical insights.</p>\n]]>", "author": "John Sidles", "published": "2016-08-01 20:58:02+00:00", "title": "By: John Sidles"}, {"content": "<![CDATA[<p>Doh!  Now I dearly wish I had added to the above comment the obvious (and comforting) conclusion:</p>\n<p>&#8220;even as we make Lefshetz-style errors along the way&#8221;.  \ud83d\ude42</p>\n<p>Also, thank you Scott, for your years of work and personal commitment in hosting <i>Shetl Optimized</i>, and for your outstanding example of integrity in confessing mistakes and embracing their necessity in research.</p>\n]]>", "author": "John Sidles", "published": "2016-08-01 21:05:45+00:00", "title": "By: John Sidles"}, {"content": "<![CDATA[<p>Joshua (I honestly can&#8217;t find the hash  on my mac keyboard)2</p>\n<p>&#8220;Reading this post makes me curious about how it\u2019s possible to rigorously verify all of the important math work that\u2019s out there&#8230;&#8221;</p>\n<p>At the risk of sounding glib, proof assistants? My supervisor told me a couple of years ago that ten percent of the of papers submitted to a leading CS conference (POPL or one of those) were formally verified, and I believed him.</p>\n<p>I guess my comment is aimed at Scott, too. It&#8217;s easy to say that formal verification would have caught the errors, but that&#8217;s an over-simplification. Better might be to say that it would have led you down different roads, although I&#8217;m struggling for the words here as usual. Anyway, there&#8217;s more to it than that, I wouldn&#8217;t want to have missed your post this morning, for example.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Smith:James.html\">James Smith</a>", "published": "2016-08-02 08:31:01+00:00", "title": "By: James Smith"}, {"content": "<![CDATA[<p>The problem in science today is not &#8216;mistakes&#8217; but the &#8216;publish or perish&#8217; attitude in general.</p>\n<p>&#8216;publish or perish&#8217; means:<br />\n-fast paced writing not adequately reviewed by the authors<br />\n-less time for peer reviewers to review the papers<br />\n-incremental papers, instead of writing a full, mature version of the whole research it is broken down in several not foolproof papers.</p>\n<p>Science is gradually having a &#8216;hedgefund attitude&#8217;.<br />\nFor example back in the days of the Higgs experiments it was pathetic and ridiculous to watch almost everyday CERN officials to speak about their &#8216;research progress&#8217;, presumably to keep &#8216;funders&#8217; happy. h-index, grants and funds tyranny&#8230;</p>\n<p>Fast paced, less than 3-4 person research is error-prone.</p>\n<p>Collaborative research based on large networks of talented people working on the same goals is the future.</p>\n]]>", "author": "George", "published": "2016-08-02 13:18:11+00:00", "title": "By: George"}, {"content": "<![CDATA[<p>James Smith, your comment reminded me of an <i>umkehren</i> (see&nbsp;#20) from Michael Harris&#8217; weblog <i>Mathematics Without Apologies</i>, in his Harris&#8217; essay &#8220;My last word (for now) on HoTT&#8221; (May 2, 2015; search engines find it):<br />\n<blockquote>&#8220;The goal of mathematics is to convert rigorous proofs to heuristics. The latter are in turn used to produce new rigorous proofs, a necessary input (but not the only one) for new heuristics.&#8221;</p></blockquote>\n<p>The many well-reasoned comments that Harris&#8217; essay stimulated touch upon some very high-level considerations regarding the essential nature of what Harris amusingly calls &#8220;the problematic vocation&#8221; of mathematics.&nbsp;\ud83d\ude42</p>\n<p>The literature on proof assistants is vast and ever-expanding; what&#8217;s emerging is not so much one dominant proof assistant, as a strongly interacting ecosystem of proof assistants, with multiple phylums, classes, orders, families, genuses, and species in it.  This accommodates Terry Tao&#8217;s view from his essay &#8220;What is Good Mathematics?&#8221;  (as cited in #20) that<br />\n<blockquote>The concept of mathematical quality is a high-dimensional one and lacks an obvious canonical total ordering.</p></blockquote>\n<p>To borrow a metaphor from Marvin Minsky&#8217;s <i>Society of Mind(s)</i>, the mathematical enterprise can be appreciated as an ever-evolving &#8220;ecosystem of mind(s)&#8221; more naturally and realistically than as an ever-taller hierarchy of ever-more-subtle proofs of ever-broader theorems.</p>\n<p>Henry George Forder&#8217;s <i>umkehren</i> maxim that &#8220;The virtue of a logical proof is not that it compels belief but that it suggests doubts&#8221; inspires us to question the integrity, stability and humanity of the ongoing evolution of the mathematical ecosystem of minds.  </p>\n<p>Shinichi Mochizuki, who is the sole author of a much-discussed claimed proof of the ABC Conjecture, expresses these doubts poignantly in his just-released exposition &#8220;The Mathematics of Mutually Alien Copies: From Gaussian Integrals to Inter-Universal Teichm\u00fcller Theory&#8221; (July 2016), specifically in Mochizuki&#8217;s concluding Section&nbsp;4.4, &#8220;Atavistic resemblance in the development of mathematics&#8221;:<br />\n<blockquote>[Mathematics is] a complicated organism, whose growth is sustained by an intricate mechanism of interaction among a vast multitude of branches, some of which sprout not from branches of relatively recent vintage, but rather from much older, more ancestral branches of the organism that were entirely irrelevant to the recent growth of the organism.&nbsp;&hellip;</p>\n<p>Escaping from the cage of&nbsp;&hellip; narrowly defined deterministic models of mathematical development stands out as an issue of crucial strategic importance from the point of view of charting a sound, sustainable course in the future development of the field of mathematics, i.e., a course that cherishes the priviledge to foster genuinely novel and unforeseen evolutionary branches in its development.</p></blockquote>\n<p>Quite a few mathematicians are concerned about a future in which (for example) Mochizuki&#8217;s proof has been distilled into a machine-verified string of symbols, whose origins and motivations no mathematician (not even Mochizuki) can effectively articulate.  </p>\n<p>Recent stunningly strong go-playing performances by Google&#8217;s AlphaGo program have heightened these cognitive eco-concerns, in that AlphaGo plays wonderfully strong go-moves without providing any explanations (even in principle) in regard to <i>why</i> its go-moves are strong.  </p>\n<p>Indeed, more-and-more simulation algorithms&nbsp;&mdash; dynamical flows on tensor networks in particular&nbsp;&mdash; similarly unite transformational simulation capacity with unsettling cognitive opacity.</p>\n<p>Now these transformational capacities and concomitant cognitive concerns are diffusing throughout the entire STEAM enterprise, like an opaque ink diffusing into the (supposedly) clear water of our STEAM-understanding.  And this observation is not any kind of proper conclusion for an essay, but rather provides a starting point for our contemplation.</p>\n]]>", "author": "John Sidles", "published": "2016-08-02 15:42:01+00:00", "title": "By: John Sidles"}, {"content": "<![CDATA[<p>Incorrect proofs make me think&#8230;</p>\n<p>Why has formalism fallen out of fashion for proofs?  Did Goedel ruin that idea?  I hope not.  Is it possible to avoid proofs relying on ambiguous, non-formalized language, and instead accept the lower-level ambiguity of syntax?  In such a situation, it would just be up to the human reader what they want to infer from the symbol manipulation of the proof.  Why not take complexity and computability theory to the computer?  Could quantum computing proofs be formalized on a classical computer? Maybe then I would be better able to follow some of these proofs and discussions \ud83d\ude42</p>\n]]>", "author": "Jon K.", "published": "2016-08-02 16:34:07+00:00", "title": "By: Jon K."}, {"content": "<![CDATA[<p>John Sidles #20 (I&#8217;ve found the hash on my mac now)</p>\n<p>You write:</p>\n<p>&#8220;the list grounds mathematics in \u201cinsight\u201d, \u201cdiscovery\u201d, \u201cvision\u201d, \u201ctaste\u201d, \u201cbeauty\u201d, \u201celegance\u201d, and \u201cintuition\u201d. These are parcellations of cognition at which human mathematicians like Lefshetz (and Tao) excel, but that escape machine-checkable formalization (by any methods that we presently know).&#8221;</p>\n<p>Well, for balance, see Thomas Hales&#8217; talk:</p>\n<p><a href=\"https://www.youtube.com/watch?v=Is_lycvOkTA\" rel=\"nofollow\">https://www.youtube.com/watch?v=Is_lycvOkTA</a></p>\n<p>If you paraphrased parts of Prof. Hales talk you might find that many words such as &#8220;elegance&#8221;, &#8220;intuition&#8221;, etc could be attributed to the formalisation process. </p>\n<p>In essence, his original proof was technical to say the least and accessible to all but a very few. On the other hand, large parts of what he calls the blueprint proof, that is the comprehensively revised proof amenable to formalisation, were done in what he calls &#8216;mathematics land&#8217; and not, as I read it, hunched in front of a computer at all. And, for emphasis, I could repeat that many of Tao&#8217;s criteria could be attributed to both the process and outcome.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Smith:James.html\">James Smith</a>", "published": "2016-08-02 20:35:22+00:00", "title": "By: James Smith"}, {"content": "<![CDATA[<p>Jon K. #25: No, the reasons why automated proof-checkers aren&#8217;t more widely used in math have nothing to do with G&ouml;del&#8217;s Theorem.</p>\n<p>The most important current application of automated proof-checkers, <i>by far</i>, is to software verification.  There you have proofs that are both (a) straightforward enough that existing programs can find them, and (b) so boring that no human being would want the job of finding them.</p>\n<p>A second, newer application is to verifying the proofs of important theorems&#8212;as in Thomas Hales&#8217; breakthrough work on the Kepler conjecture&#8212;but so far, only <i>after</i> all the conceptual dust has settled.</p>\n<p>Of course, human mathematicians have used software for lots of things, from numerical calculations to symbolic manipulation to huge combinatorial searches.  But I can&#8217;t think of a single example (can anyone else?) where a human mathematician verified their ideas with an automated proof-checker in course of doing original, exploratory research.  It would just slow you down too much&#8212;as if you were an architect who couldn&#8217;t sketch anything with paper or CAD programs, and you needed to build out each idea in your head with actual wood and bricks before being confident enough to move on to the next idea.</p>\n<p>Of course, most of the &#8220;blueprints&#8221; that mathematicians create <i>never</i> get turned into actual &#8220;houses&#8221; (that is, formalized ZFC proofs), because there&#8217;s not enough point in doing so: no one can live in these &#8220;houses,&#8221; and the real purpose of these blueprints is simply to inspire (or be reused in) other blueprints by other architects.</p>\n<p>Eventually, automated proof-checkers might become so good that human mathematicians switch to a norm of using them, but we&#8217;re not there yet.</p>\n]]>", "author": "Scott", "published": "2016-08-02 21:55:36+00:00", "title": "By: Scott"}]