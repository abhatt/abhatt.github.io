[{"content": "<![CDATA[<p>Those particularly interested in practical aspects of cryptography and security will be rewarded by reading Bruce Schneier at <a href=\"https://www.schneier.com/\" rel=\"nofollow\">https://www.schneier.com/</a> and/or signing up for his newsletter. Bruce analyzes everything from many angles. He is actually the guy who wrote the book(s).</p>\n]]>", "author": "Raoul Ohio", "published": "2017-10-25 19:42:45+00:00", "title": "By: Raoul Ohio"}, {"content": "<![CDATA[<p>Who would have guessed we would wind up quoting George W. Bush: <a href=\"http://phdcomics.com/comics.php?f=1980\" rel=\"nofollow\">http://phdcomics.com/comics.php?f=1980</a></p>\n]]>", "author": "Raoul Ohio", "published": "2017-10-25 19:49:35+00:00", "title": "By: Raoul Ohio"}, {"content": "<![CDATA[<p>Dear Ashley  Many thanks. The paper and especially the ability to use classical error correction to handle noise is indeed fascinating!  What about doing it for general quantum circuits? Certainly an analogous result for BosonSampling or other form of Bosonic fault tolerance would be great!</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kalai:Gil.html\">Gil Kalai</a>", "published": "2017-10-25 19:54:41+00:00", "title": "By: Gil Kalai"}, {"content": "<![CDATA[<p>Gil: thanks! Though I should stress that the noise model we considered is somehow very classical itself (it can be seen as random bit flips in the classical samples obtained at the end of the circuit). Extending this idea to more general noise or for more general quantum circuits could be quite challenging&#8230; but maybe fun \ud83d\ude42</p>\n]]>", "author": "Ashley", "published": "2017-10-25 20:30:26+00:00", "title": "By: Ashley"}, {"content": "<![CDATA[<p>Joshua #58,</p>\n<p>Here&#8217;s an exemple for which we can demonstrate the &#8220;living down below&#8221;: any set of instances of generalized Go for which the board is n*n and the komi is above n^2 (yes&#8230; black pass and win). This set of instances belongs to generalized Go, but it can be solved in constant time.  </p>\n<p>Here&#8217;s another exemple, for which I&#8217;m not sure of the complexity: any set of instances of generalized Go for which the number of move is below n^2 and the komi above 7 (yes, as most human games). I&#8217;ll be glad to learn if anyone knows a good lower bound, but I doubt the complexity is &#8220;generalized Go-complete&#8221;.</p>\n<p>So, if I can rephrase my question: do you see any garantee that the set of instance that corresponds to Go-as-played-by-humans doesn&#8217;t belong to a special set?</p>\n]]>", "author": "Jay", "published": "2017-10-25 20:43:53+00:00", "title": "By: Jay"}, {"content": "<![CDATA[<p>Job #50</p>\n<p>Right, since quantum systems can behave &#8220;classically&#8221;, QM is a super-set of classical systems in that sense.<br />\nThere&#8217;s clearly some extra &#8220;power&#8221; in QM, otherwise, why would nature have bothered with it? (it can&#8217;t even create stable atoms without it!)</p>\n<p>It then looks cheap to notice that there&#8217;s gotta be some quantum system that classical systems can&#8217;t simulate efficiently, repackage them as a &#8220;computation&#8221;, and then declare quantum supremacy.</p>\n<p>The real question is whether &#8220;qbits&#8221; are possible, as some uber version of classical bits, which truly are &#8220;perfect&#8221; (stable, independent of implementation, and infinitely scalable) digital logic abstractions.</p>\n]]>", "author": "fred", "published": "2017-10-25 21:32:47+00:00", "title": "By: fred"}, {"content": "<![CDATA[<p>bozo #61: Yes, as far as we know approximate BosonSampling remains hard for m=n, and <i>even beyond that</i>: one could try m<n, all the way down until the n<sup>m</sup>-time or whatever classical simulation becomes efficient enough.</p>\n<p>Of course, just like in applied crypto, the more liberties you take with parameters, the greater the probability of entering the &#8220;danger zone&#8221; where the problem might succumb to some algorithm that we don&#8217;t yet know about.  Especially if you choose parameters that no one had much occasion to think about before.</p>\n<p>Your question about whether there&#8217;s some &#8220;hard threshold&#8221; in the variation distance error &epsilon; is an excellent one, and we don&#8217;t yet know the answer to it.  In our paper, we just dealt with the assumption that approximate BosonSampling has no poly(n,m,1/&epsilon;) algorithm&#8212;which means, we implicitly imagined an experimenter who could get the error down to some arbitrary &epsilon;>0 using &#8220;poly(1/&epsilon;) experimental effort.&#8221;  Of course that isn&#8217;t quite realistic.</p>\n<p>If you fix &epsilon; to (say) 1/10, then&#8212;this story should sound familiar by now&#8212;our reductions don&#8217;t go through quite as nicely, but as far as anyone knows the problem is still hard.</p>\n<p>I&#8217;m aware of some work in progress that touches a bit on this question&#8212;showing that, in the presence of sufficiently severe errors of other kinds, BosonSampling <i>can</i> become classically easy below a certain threshold in the variation distance error.  But I don&#8217;t want to write more about it without the authors&#8217; permission.</p>\n]]>", "author": "Scott", "published": "2017-10-25 21:57:39+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Ashley #28:</p>\n<ul>it looks like the ideas you\u2019ve outlined are designed to deal with the problem of producing n input photons at once, rather than the problem of loss within the circuit. This could also become a big problem once n is large, as each photon will have to go through something like either n^2 optical components (using a standard Reck scheme) or at best n (using your+Alex\u2019s optimised low-depth circuits).</ul>\n<p>Two replies:</p>\n<p>First, if we do want to discuss near-term experiments rather than asymptotics, I&#8217;m told that loss at the beamsplitters is almost completely negligible compared to the problems of building efficient single-photon sources and detectors.  It&#8217;s very unlikely to be a limiting factor in practice.</p>\n<p>But secondly, as far as anyone knows, approximate BosonSampling might be classically hard even with a <i>constant</i>-depth beamsplitter network!  Certainly exact BosonSampling remains hard for constant depth; Daniel Brod has a <a href=\"https://arxiv.org/abs/1412.6788\" rel=\"nofollow\">paper</a> where he shows that.</p>\n<p>In the approximate case, of course we no longer get submatrices that are i.i.d. Gaussian, so our hardness assumption for the permanent will have to involve a matrix ensemble that&#8217;s messier to describe.  But I know of no result saying that the permanents that arise from constant-depth beamsplitter networks are actually easy&#8212;do you?</p>\n]]>", "author": "Scott", "published": "2017-10-25 22:07:09+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Jay 66,</p>\n<p>Ah, I see your point. Yes, there&#8217;s no real guarantee like that at all, and the nature of a lot constructions to prove these sorts of things suggest that such a result would be probably false. For that matter, thinking about this more, the board sizes needed to argue even that Go is NP-hard are already very large. For example the construction used in Moore and Merten&#8217;s includes gadgets which are as large as 8 by 8, so even a 19 by 19 board can only model a very tiny circuit. I don&#8217;t know if anyone has tried to make these gadgets more efficient.</p>\n]]>", "author": "Joshua Zelinsky", "published": "2017-10-26 01:22:27+00:00", "title": "By: Joshua Zelinsky"}, {"content": "<![CDATA[<p>[&#8230;] at the University of Texas at Austin and the head of its Quantum Information Center, said in a recent blog post that IBM\u2019s paper on quantum supremacy did not diminish the importance of Google\u2019s quantum [&#8230;]</p>\n]]>", "author": "New Twists in the Road to Quantum Supremacy  Utilities Up2date", "published": "2017-10-26 02:48:39+00:00", "title": "By: New Twists in the Road to Quantum Supremacy \u2013 Utilities Up2date"}, {"content": "<![CDATA[<p>For AlphaGo the obvious question is if they can repeat the trick. That is train version 2 using version 1 so that version 2 always beats version 1 (as they have already shown). Then train version 3 using version 2 so that version 3 always beats version 2 and repeat.</p>\n<p>Have they already tried this? I assume they must have done.</p>\n]]>", "author": "Carl", "published": "2017-10-26 09:07:51+00:00", "title": "By: Carl"}, {"content": "<![CDATA[<p>Scott #68</p>\n<p>Interestingly, if m = n then I believe you can simulate boson sampling exactly in something like 1.55^n time (using a variant of our exact sampling algorithm) which I think pushes the threshold for quantum supremacy to around n = 80.</p>\n<p>Experimentally you would also have to have 80 reliable photon counters which might be difficult in the short term on its own (I am told).</p>\n]]>", "author": "Raphael", "published": "2017-10-26 09:17:21+00:00", "title": "By: Raphael"}, {"content": "<![CDATA[<p>Scott #69: no, I don&#8217;t know of any result that says that constant-depth BS could be easier than general BS&#8230; but on the other hand, constant-depth (or even log-depth) BS using standard integrated photonic circuits could indeed be easier to simulate, as these correspond to nearest-neighbour interactions. It seems likely (though I didn&#8217;t work out the details) that you&#8217;d need long-range interactions to get computational hardness here, which would in turn involve waveguides crossing each other, hence inducing loss etc.</p>\n<p>About the point on beamsplitter loss in &#8220;near-term&#8221; experiments, I guess it depends how near-term you mean, but although at present this doesn&#8217;t seem so significant (with experiments with under 10 photons), it could become more important when we need to have photons propagating through more than, say, 50 reconfigurable beamsplitters. Even loss induced by the underlying medium could start becoming an issue here.</p>\n<p>As a counterpoint, there are the recent impressive achievements by experimental groups working in this area. So I guess we&#8217;ll see once such experiments start being built \ud83d\ude42</p>\n]]>", "author": "Ashley", "published": "2017-10-26 10:19:03+00:00", "title": "By: Ashley"}, {"content": "<![CDATA[<p>Not actually related to your post, but I figured I&#8217;d ask somewhere: I&#8217;m given to understand that you&#8217;re giving a talk on quantum complexity at the Hebrew University in Jerusalem, but I haven&#8217;t been able to find much information about it (time, place, verification of the existence of such an event). Any info on that would be appreciated</p>\n]]>", "author": "Eliana", "published": "2017-10-26 12:09:21+00:00", "title": "By: Eliana"}, {"content": "<![CDATA[<p>Scott #59</p>\n<p>Is &#8216;t Hooft calling you a baboon?!  \ud83d\ude1b</p>\n<p><a href=\"https://arxiv.org/pdf/1709.02874.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1709.02874.pdf</a></p>\n<p>&#8220;I am aware of the large numbers of baboons around me whose brains have arrived at different conclusions: they proved that hidden variables do not exist.  But the theorems applied in these proofs contain small print.  It is not taken into account that the particles and all other objects in our aquarium will tend to be strongly correlated. They howl at me that this is \u2018super-determinism\u2019, and would lead to\u2018conspiracy\u2019. Yet I see no objections against superdeterminism, while \u2018conspiracy\u2019 is an ill-defined concept, which only exists in the eyes of the beholder.&#8221;</p>\n]]>", "author": "fred", "published": "2017-10-26 13:14:49+00:00", "title": "By: fred"}, {"content": "<![CDATA[<p>Joshua #70, there is a proof here <a href=\"https://senseis.xmp.net/?RobsonsProofThatGOIsEXPTimeHard\" rel=\"nofollow\">https://senseis.xmp.net/?RobsonsProofThatGOIsEXPTimeHard</a><br />\ntha tseems to give sligthy lower size of gadgets.</p>\n]]>", "author": "Jr", "published": "2017-10-26 13:40:03+00:00", "title": "By: Jr"}, {"content": "<![CDATA[<p>Raphael #73: Wow!  So, you&#8217;re not claiming that you can compute the permanent of an n&times;n unitary U in 1.55<sup>n</sup> time, but you <i>are</i> claiming that you can accept with probability |Per(U)|<sup>2</sup> in 1.55<sup>n</sup> time?  Is that written down anywhere?  And where does the 1.55 come from?  Sounds like a great result!</p>\n]]>", "author": "Scott", "published": "2017-10-26 14:20:37+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Ashley #74: For constant-depth BS with spatially local beamsplitters, I guess the question is: do we have an efficient classical algorithm for the permanents of band-diagonal matrices, of constant width w?  Like n<sup>w</sup> or 2<sup>w</sup> or something?  I seem to remember that the answer is yes, but I don&#8217;t remember the algorithm&#8230;  (and of course, even knowing the algorithm, we&#8217;d then have to turn it into a sampling procedure)</p>\n]]>", "author": "Scott", "published": "2017-10-26 14:36:30+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Scott, </p>\n<p>Boson sampling texts refer to some simultaneity:</p>\n<p>&#8220;We only count the runs in which n detectors register a photon simultaneously, even if such runs are exponentially unlikely&#8221;</p>\n<p>or in the wiki</p>\n<p>&#8220;Then, the probability of generating simultaneously M single photons is \u03b5^M&#8221;</p>\n<p>What does &#8220;simultaneous&#8221; mean in this context?<br />\nIs this about defining an actual practical &#8220;time window&#8221; of +/- some fraction of nanosec (or pico seconds) where the events need to happen, or is this a quantum property of the entire system where we know that the events are either simultaneous or they&#8217;re not?</p>\n]]>", "author": "fred", "published": "2017-10-26 14:42:58+00:00", "title": "By: fred"}, {"content": "<![CDATA[<p>fred #76: Yeah, I guess he <i>is</i> calling me a baboon!  Rather than reply in kind, I&#8217;ll simply say it&#8217;s sad for me to witness his evolution on this subject.  His first papers on a classical cellular automaton underlying physics strongly suggest that, hard as others find it to believe, he simply didn&#8217;t understand Bell&#8217;s Theorem or its importance.  Then, after dozens of people must have explained to him that Bell&#8217;s Theorem <i>really, actually does rule out what he wants</i>, rather than admitting that his initial idea didn&#8217;t work, he started advocating a &#8220;cure&#8221; that&#8217;s clearly, obviously a trillion times worse than the disease&#8212;something that would make science impossible (as ordinary QM does not) and that has no new explanatory power (as ordinary QM does)&#8212;all, one suspects, so that he wouldn&#8217;t have to admit he was wrong.</p>\n]]>", "author": "Scott", "published": "2017-10-26 14:45:32+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Eliana #75: Yes, I&#8217;m speaking at HUJI on Wednesday November 8, about my new result on &#8220;shadow tomography&#8221; of quantum states!  But I don&#8217;t yet know the time or room either.  I just emailed Dorit Aharonov and Guy Kindler to ask.</p>\n]]>", "author": "Scott", "published": "2017-10-26 14:47:13+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Carl #72,</p>\n<p>Since AlphaGo Zero plays against itself to learn already, having a separate program play against itself shouldn&#8217;t really make a substantial difference.</p>\n]]>", "author": "Joshua Zelinsky", "published": "2017-10-26 14:50:38+00:00", "title": "By: Joshua Zelinsky"}, {"content": "<![CDATA[<p>fred #80: &#8220;Simultaneous&#8221; in this context means, close enough in time (compared to the wavelength of the photons) that there&#8217;s which-path ambiguity, so one needs to calculate the amplitudes by summing over exponentially many different permutations of the photons, so one gets a hard computational problem (in the simplest case, the permanent).</p>\n]]>", "author": "Scott", "published": "2017-10-26 14:55:47+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Scott,<br />\nSome major cryptosystems will still be broken, but every time it will be because a serious flaw is discovered in the cryptosystem. Often, this serious flaw will reduce the running time of the fastest known algorithm to break the system from, say, 2^n to, say, 2^{n/100}. In such cases, we could have just taken our key sizes to be 100x as long, but that&#8217;s not really the main issue. The main issue is that the designers of the cryptosystem missed something huge.</p>\n<p>To be clear, I&#8217;m talking about symmetric-key crypto. (For public-key crypto, the problems are so structured that I think it&#8217;s much more reasonable to expect major algorithmic progress. For lattices, the only area in which I&#8217;m an expert, I&#8217;m aggressively trying to tell people that there probably will be major algorithmic progress.) For symmetric-key crypto, it&#8217;s sort of kind of maybe reasonable to say that, since &#8220;almost every&#8221; function is basically a perfect PRF/CRHF. (Obviously, the flaw in this argument is that (1) most functions are not efficiently computable; and (2) existential proofs via the probabilistic method are often extremely hard to derandomize in practice.) And, cryptographers have devised various ways of making this argument slightly less absurd in the case of specific systems like AES.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stephens=Davidowitz:Noah.html\">Noah Stephens-Davidowitz</a>", "published": "2017-10-26 14:55:28+00:00", "title": "By: Noah Stephens-Davidowitz"}, {"content": "<![CDATA[<p>Jr #77,</p>\n<p>Thanks for the link. It looks like some of their gadgets are smaller, but I&#8217;m not sure that lists every gadget you need. I&#8217;ll have to look at that in detail.</p>\n]]>", "author": "Joshua Zelinsky", "published": "2017-10-26 14:56:08+00:00", "title": "By: Joshua Zelinsky"}, {"content": "<![CDATA[<p>* should read &#8220;since &#8216;almost every&#8217; function is basically a perfect PRF/CRHF, we should be able to find nearly perfect PRFs/CRHFs.&#8221;</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stephens=Davidowitz:Noah.html\">Noah Stephens-Davidowitz</a>", "published": "2017-10-26 14:56:52+00:00", "title": "By: Noah Stephens-Davidowitz"}, {"content": "<![CDATA[<p>Carl #72</p>\n<p>Adding a little to what Joshua Zelinsky said:</p>\n<p>Moreover, it is not the case, as you seem to think, that AlphaGo Zero (the latest version) was trained to beat AlphaGo Fan / AlphaGo Lee. The first version of AlphaGo Zero was random and each version was trained to beat the previous version of AlphaGo Zero. So no information from AlphaGo Fan, Lee, etc. was used in AlphaGo Zero. This is part of what makes AlphaGo Zero&#8217;s 3-day training time so impressive &#8211; it was really starting from a blank slate.</p>\n<p>I lied a little: &#8220;each version was trained to beat the previous version of AlphaGo Zero&#8221; is actually an oversimplification &#8211; each version is trained to predict a Monte Carlo Tree Search built from the previous version of AlphaGo, which is more sophisticated.</p>\n]]>", "author": "Will", "published": "2017-10-26 16:07:21+00:00", "title": "By: Will"}]