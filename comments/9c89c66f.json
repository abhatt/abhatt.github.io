[{"content": "<![CDATA[<p>What does your (Adjusted) Coarse-Graining Experiment output for Complexity and Entropy of a fractal pattern? Of a Mona Lisa image? Does it match your intuition?</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nux:Shmi.html\">Shmi Nux</a>", "published": "2014-05-29 23:34:48+00:00", "title": "By: Shmi Nux"}, {"content": "<![CDATA[<p>The cream/coffee picture to first approximation shows the Navier-Stokes equations, which are deterministic but chaotic (have sensitive dependence on initial conditions, abbreviated SDIC).  The mathematical NS equations push energy to arbitrarily fine scales, but for physical liquids this stops at the molecular scale.  The mathematical equations are deterministic, but physical liquids experience random thermal motion of the molecules, which are then magnified to the visible scale by the SDIC of the NS equations, increasing the visible entropy.  Eventually (long before molecular scale) stuff is too noised over to distinguish within the camera&#8217;s pixel resolution, so the visible entropy decreases.</p>\n]]>", "author": "asdf", "published": "2014-05-30 00:33:44+00:00", "title": "By: asdf"}, {"content": "<![CDATA[<p>Here is another example of a system where complexity first goes up, and then down, although depending not on time, but on temperature. Imagine a computer (0) simulation of a cellular automaton (1) which is in a configuration of <a href=\"http://en.wikipedia.org/wiki/Rule_110\" rel=\"nofollow\">universal computer</a> (1), programmed to simulate CA (2) with the same rules as CA (1) and having initial configuration, corresponding to computer (1) + some additional memory. </p>\n<p>At zero temperature CA (1) behavior is strictly deterministic, and it has a lower bound on complextropy, corresponding to entropy of computer (0). As temperature of CA (1) goes up, complextropy of computer (1) increases because a) entropy increases and b) to achieve exponentially bounded errors at (1) or (2) level some error correcting code must be introduces, which complicates structure of computer (1). At high temperatures no polynomial amount of error correcting is sufficient to make computations (proof?), CA (1) becomes random, and complextropy goes down.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/S=:Oleg.html\">Oleg S.</a>", "published": "2014-05-30 09:53:30+00:00", "title": "By: Oleg S."}, {"content": "<![CDATA[<p>Scott #39</p>\n<p>Here&#8217;s an example of simple ordering rule:</p>\n<p>Whenever, a cream cell is surrounded by coffee cells, the group of cells is allowed to persist unchanged for some defined time period.</p>\n<p>Let&#8217;s imagine a matrix of cells 10 x 10. </p>\n<p>There is a cream cell in (6,2) and coffee cells in (5,2), (6,1), (6,3) and (7,2). This cluster of 5 cells is allowed to persist unchanged for some time period. In other words, any random selection of an operation relating to these cells is simply skipped until the time period expires. Or perhaps any operation that leaves the structure unchanged might be allowed.. For example, swapping one of the coffee cells with another coffee cell.</p>\n<p>This is just an example of a rule not necessarily one I would recommend. </p>\n<p>This is based on what to me is an intuitive sense that complexity needs some degree of persistence of structure to  create greater complexity.</p>\n<p>On an unrelated topic, David Layzer had the idea that order could arise in an expanding universe because potential entropy was greater than actual entropy. Can your coffee cup expand?</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cross:James.html\">James Cross</a>", "published": "2014-05-30 11:26:32+00:00", "title": "By: James Cross"}, {"content": "<![CDATA[<p>Of course there&#8217;s been a huge amount of work on complexity in dynamical systems in the 70s and 80s. Regarding the automaton, there&#8217;s the classification scheme by Wolfram-so it would be interesting to understand just how the automaton defined here enters the scheme. The new feature of cellular automata with respect to the dynamical systems of the 70s and 80s is that the latter are ultra-local in space and evolve in time only. The automata evolve in space and time, so they define lattice field theories. Some simple rules were studied in this way by &#8216;t Hooft in the 90s.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nicolis:Stam.html\">Stam Nicolis</a>", "published": "2014-05-30 12:01:15+00:00", "title": "By: Stam Nicolis"}, {"content": "<![CDATA[<p>Incidentally, a measure that is used to label the behavior of cellular automata is &#8220;damage spreading&#8221;: One takes two initial configurations, that differ in a fraction of their site variables (so one computes what&#8217;s technically known as the &#8220;Hamming distance&#8221;) and monitors how this fraction evolves as a function of time: In particular, if it remains finite at &#8220;long times&#8221;, this is a good indicator that the space of configurations is &#8220;complex&#8221; (once one has averaged over the initial configurations, of course, to eliminate global symmetries). While there has been a lot of numerical work here and in the characterization of the spatial &#8220;fronts&#8221;, a deeper understanding of the critical behavior is, still elusive. </p>\n<p>For, indeed, the idea itself, that if one starts with a &#8220;few&#8221; degrees of freedom, the system&#8217;s complexity is expected to be &#8220;low&#8221;; for &#8220;many&#8221; degrees of freedom it&#8217;s expected to be high and for &#8220;very many&#8221; it&#8217;s expected to &#8220;decrease&#8221; as an averaging process becomes appropriate, has been a major way of characterizing chaotic behavior in time evolution, since deterministic chaos, precisely, offers a sort of counterexample to this picture, since it shows that &#8220;few&#8221; degrees of freedom *can* display &#8220;complex&#8221; behavior. </p>\n<p>So the open issue is, indeed, how to characterize complex behavior of spatially extended systems.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nicolis:Stam.html\">Stam Nicolis</a>", "published": "2014-05-30 13:55:29+00:00", "title": "By: Stam Nicolis"}, {"content": "<![CDATA[<p>I&#8217;m still struggling to understand what this post is about.<br />\nIs it something along the lines:<br />\nEvery game of chess starts with a well defined configuration. Then, as the game progresses, there&#8217;s a point where the board looks pretty &#8220;messy&#8221;, meaning that at this point, the number of possible valid moves is quite high on average (not sure it&#8217;s necessarily true), or because the pieces are all over the place. Then, as the game ends, usually there are less pieces on the board (and typically the number of end states can be listed explicitly).<br />\nBut are we trying to compare the relative complextropy evolution of various board games inherent in their rule set? Like Checkers seems to have way less maximum complextropy than Chess as a system, and Go seems to have way more complextropy than Chess?</p>\n]]>", "author": "fred", "published": "2014-05-30 14:04:15+00:00", "title": "By: fred"}, {"content": "<![CDATA[<p>My examples above with board games implicitly assumes that each game rule set also includes some additional rules/heuristics for deciding what move to make (either random or based on some objective function, etc), so that they&#8217;re finite automaton, like a game of life board, or the milk/coffee mix automaton.<br />\nBut my question is whether we&#8217;re trying to find a good &#8220;complextropy&#8221; metrics to classify all those systems, and if we&#8217;re also trying to predict the maximum compextropy of a board game given its rules (do chess games played by masters lead to more complextropy on average than games played by total noobs?)</p>\n]]>", "author": "fred", "published": "2014-05-30 14:15:52+00:00", "title": "By: fred"}, {"content": "<![CDATA[<p>Sean #26, Scott #27:</p>\n<p>Before you get too excited about the Ising model, you should know that it is such a good example that it has been studied quite extensively by the Santa Fe school since the 80s!  The main object of study there has been the complexity-entropy curve of these systems.  A very nice short review is <a href=\"http://csc.ucdavis.edu/~chaos/papers/Crutchfield.NaturePhysics2012.pdf\" rel=\"nofollow\">here</a>.  A longer discussion can be found <a href=\"http://arxiv.org/pdf/0806.4789.pdf\" rel=\"nofollow\">here</a>.  </p>\n<p>The basic takeaway is that, for things like the statistical complexity or excess entropy, this behavior is very common, but seems to not be universal in the sense that different systems exhibit fundamentally different complexity entropy curves in contrast to the hopes of the early 80s.  Of course, who knows what it looks like for these new ideas like resource bound sophistications!</p>\n]]>", "author": "Brent", "published": "2014-05-30 16:34:00+00:00", "title": "By: Brent"}, {"content": "<![CDATA[<p>A reference you guys seem not aware of that I believe is relevant:</p>\n<p>Neural Comput. 2001 Nov;13(11):2409-63.<br />\nPredictability, complexity, and learning.<br />\nBialek W, Nemenman I, Tishby N.</p>\n<p>Abstract<br />\nWe define predictive information I(pred)(T) as the mutual information between the past and the future of a time series. Three qualitatively different behaviors are found in the limit of large observation times T:I(pred)(T) can remain finite, grow logarithmically, or grow as a fractional power law. If the time series allows us to learn a model with a finite number of parameters, then I(pred)(T) grows logarithmically with a coefficient that counts the dimensionality of the model space. In contrast, power-law growth is associated, for example, with the learning of infinite parameter (or nonparametric) models such as continuous functions with smoothness constraints. There are connections between the predictive information and measures of complexity that have been defined both in learning theory and the analysis of physical systems through statistical mechanics and dynamical systems theory. Furthermore, in the same way that entropy provides the unique measure of available information consistent with some simple and plausible conditions, we argue that the divergent part of I(pred)(T) provides the unique measure for the complexity of dynamics underlying a time series. Finally, we discuss how these ideas may be useful in problems in physics, statistics, and biology.</p>\n]]>", "author": "Ken", "published": "2014-05-30 21:37:49+00:00", "title": "By: Ken"}]