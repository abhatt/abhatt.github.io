[{"content": "I have been on only one PC that used a geometric percentage scale and I really didn&#39;t like it.  The problem is that it doesn&#39;t distinguish between huge ranges of papers - leaving a whole bunch of papers in the region around what should be the acceptance threshold with indistinguishable numerical ratings despite very different PC member attitudes and reviews.<br /><br />PC members probably do a bad job of estimating the overall percentages anyway given their small samples.   <br /><br />The high rated and low rated papers don&#39;t need quite so much distinction.   The most important area where numerical ratings matter is in helping the committee understand the attitudes of their fellow PC members near the borderline.   In theory conference this is in the 20%-40% range given that PCs generally accept between 25 and 33% of submissions.  In other communities it may be at a different level.<br /><br />Very often I find that PC members end up not wanting to make firm statements but having leanings that have a big affect on their behavior in PC discussions.   The more that numerical ratings can tease these out at the start, the better.", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Beame:Paul.html\">Paul Beame</a>", "published": "2014-08-19 18:05:17.520000-04:00", "title": ""}, {"content": "Several years ago some conference decided to simplify the scoring system -3 ... 3 to 0 to 3 (or something like that), with the comments that anything that was in the range -3...0 should be now names 0, since in the past all papers with the scores in that range were rejected anyway; this is somehow similar to your criticism of the scores 1-3. PC chair was very direct and told all PC members that they should think about the scale -3 ... 3 and give 0 for papers with scores in the range -3 ...0. However, the outcome was that the average score was higher than 1, since everyone was calibrating to the range 0..3.", "author": "Anonymous", "published": "2014-08-19 19:05:03.681000-04:00", "title": ""}, {"content": "Below is the scale a machine learning conference uses. I like it because it assigns particular interpretations of each score (if you give a paper a 1 and it gets in you should boycott the conference).<br />10: Top 5% of accepted NIPS papers, a seminal paper for the ages.<br />I will consider not reviewing for NIPS again if this is rejected.<br />9: Top 15% of accepted NIPS papers, an excellent paper, a strong accept.<br />I will fight for acceptance.<br />8: Top 50% of accepted NIPS papers, a very good paper, a clear accept.<br />I vote and argue for acceptance.<br />7: Good paper, accept.<br />I vote for acceptance, although would not be upset if it were rejected.<br />6: Marginally above the acceptance threshold.<br />I tend to vote for accepting it, but leaving it out of the program would be no great loss.<br />5: Marginally below the acceptance threshold.<br />I tend to vote for rejecting it, but having it in the program would not be that bad.<br />4: An OK paper, but not good enough. A rejection.<br />I vote for rejecting it, although would not be upset if it were accepted.<br />3: A clear rejection.<br />I vote and argue for rejection.<br />2: A strong rejection. I&#39;m surprised it was submitted to this conference.<br />I will fight for rejection.<br />1: Trivial or wrong or known. I&#39;m surprised anybody wrote such a paper.<br />I will consider not reviewing for NIPS again if this is accepted.<br /><br />From http://nips.cc/Conferences/2013/PaperInformation/ReviewerInstructions", "author": "Anonymous", "published": "2014-08-20 23:39:14.934000-04:00", "title": ""}, {"content": "Paul:<br /><br />What you see as a problem I see as a benefit.  The geometric scale rather consistently divides the papers into 3 categories:  <br />Definitely accept<br />Definitely reject<br />Discuss<br /><br />Papers that need to be discussed should, actually, be discussed.<br /><br />I find that your suggestion that the numerical rankings help &quot;tease out&quot; what is going on with borderline papers (at least with a 10 point scale) just isn&#39;t the case in my experience;  different people don&#39;t use the number consistently in these ranges, and they are not sufficiently accurate scorers in their first pass.  More scores just increases the arbitrariness.  And, I believe, it makes it harder to clarify the &quot;clearly accept&quot; and &quot;clearly reject&quot; papers (although probably more the latter than the former).  (As I said, a 5-point non-geometric scale still seems fine;  I just prefer it a little less.)    <br /><br />Anonymous #3:  The NIPS scale is really funny.  Thanks for posting it!<br /><br />But again, it highlights the point that scores of 1/2/10 are probably rarely used (and fairly arbitrary;  they&#39;re more likely to be used by PC members that like to give strong opinions rather than based on the actual merit of the paper).  Also, the distinction between scores 4 and 5 (and 6 and 7) seems so vague and reviewer-dependent that it again increases arbitrariness.  <br />", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mitzenmacher:Michael.html\">Michael Mitzenmacher</a>", "published": "2014-08-21 09:31:58.482000-04:00", "title": ""}]