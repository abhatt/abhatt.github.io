[{"content": "<![CDATA[<p>Job #50:</p>\n<ul>That\u2019s sensible but computationally it\u2019s still at odds with the Church-Turing thesis.</ul>\n<p>Well, it&#8217;s at odds with the <i>Extended</i> Church-Turing Thesis.  The original Church-Turing Thesis, the one about computability, isn&#8217;t challenged in the least&#8212;nor, for that matter, is the thesis that NP-complete problems are intractable in the physical world.  The claim is simply that, just like in the 1970s we had to enlarge our notion of the efficiently computable slightly, from P to BPP (i.e., to encompass randomized algorithms), so in the 1990s we had to enlarge it slightly further, from BPP to BQP.  And moreover, while today it looks like ultimately P=BPP, today it also looks likely that P&ne;BQP.</p>\n<p>And yes, the fact that quantum computing genuinely challenges the ECT is why some of us decided to spend our careers studying it!  It&#8217;s not exactly something that escaped notice, or that we sweep under the rug. <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/icon_smile.gif\" alt=\":-)\" class=\"wp-smiley\" /></p>\n<p>As a close analogy, the Bell inequality was interesting in the 1960s because it really, genuinely challenged local realism.  So as soon as Bell published his papers, the <i>failure</i> of local realism immediately became the scientifically conservative option, not certain but the thing for anyone who understood the issues to bet on&#8212;even though the failure hadn&#8217;t yet been shown by any direct experiment, even though experimental confirmation would need to wait a couple more decades.  The Jobs of the 1960s could have said: &#8220;That&#8217;s sensible but it&#8217;s still at odds with the local realism thesis.&#8221;  Yes, yes it is!  Or: &#8220;if I&#8217;d previously imagined the universe as a 3-dimensional network of classical computing elements, something like a classical cellular automaton, then this would require me to change my view.&#8221;  Yes, yes, it would!  Glad you noticed!  But by now, we&#8217;ve been through a century of the &#8220;quantum mechanics is true&#8221; thesis obliterating every other thesis with which it comes into conflict.</p>\n]]>", "author": "Scott", "published": "2015-02-14 14:39:46+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p><i>&#8230;the failure of local realism immediately became the scientifically conservative option, not certain but the thing for anyone who understood the issues to bet on&#8230;</i></p>\n<p>FWIW, i&#8217;m trying to understand the issues, but i can&#8217;t see that from <i>QM works</i> it follows that <i>QCs scale</i>.</p>\n<p>We&#8217;re discussing whether a process that likely occurs <i>outside</i> of the Universe will remain consistent at a large scale &#8211; yes, i need experimental evidence for stuff that happens outside the Universe in order to push back the threshold of what&#8217;s efficiently computable.</p>\n<p><i>And moreover, while today it looks like ultimately P=BPP&#8230;</i></p>\n<p>That was the sound of P obliterating a complexity class it came into conflict with. <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/icon_smile.gif\" alt=\":)\" class=\"wp-smiley\" /></p>\n]]>", "author": "Job", "published": "2015-02-14 15:56:17+00:00", "title": "By: Job"}, {"content": "<![CDATA[<p>Job #53: I don&#8217;t know why you keep talking about &#8220;outside the universe.&#8221;  A quantum computer wouldn&#8217;t require intervention from outside the universe to operate&#8212;at least, no more than an electric stapler requires intervention from outside the universe.  It would simply exploit the fact that <i>the</i> universe&#8212;<i>our</i> universe&#8212;is a quantum-mechanical one.</p>\n<p>And yes, of course it&#8217;s conceivable that P=BQP.  That&#8217;s why a lot of us study quantum complexity theory, because we want to answer such questions!  Note that my work with Arkhipov, as well as that of Bremner-Jozsa-Shepherd, shows that if your proof of P=BQP yielded a little more (a quantum sampler), it would imply a collapse of the polynomial hierarchy.  That raises the stakes.</p>\n]]>", "author": "Scott", "published": "2015-02-14 18:03:55+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p><i>Note that my work with Arkhipov, as well as that of Bremner-Jozsa-Shepherd, shows that if your proof of P=BQP yielded a little more (a quantum sampler), it would imply a collapse of the polynomial hierarchy. That raises the stakes.</i></p>\n<p>In a way that also strengthens P=BQP &#8211; in the same way that the absurd consequences of QM alternatives strengthen QC.</p>\n<p>P=BQP is just possible enough.</p>\n<p><i>I don\u2019t know why you keep talking about \u201coutside the universe.\u201d</i></p>\n<p>I guess it&#8217;s the same reason why there are many-world interpretations of QM. Where does interference occur, and what drives it? It&#8217;s a computationally expensive process.</p>\n<p>One might argue that it&#8217;s simply a property of the Universe but IMO that&#8217;s alot like <i>smuggling the exponentiality somewhere that isn\u2019t being explicitly considered</i>.</p>\n<p>I&#8217;ll accept that <i>if</i> amplitudes are <i>exactly</i> like probabilities, then interference is a basic property of the Universe and there&#8217;s no clear reason why QCs won&#8217;t scale. Is that acceptable?</p>\n]]>", "author": "Job", "published": "2015-02-15 01:51:09+00:00", "title": "By: Job"}, {"content": "<![CDATA[<p>Job #55:</p>\n<ul>In a way that also strengthens P=BQP \u2013 in the same way that the absurd consequences of QM alternatives strengthen QC.</ul>\n<p>Sorry, I think you missed a negation somewhere.  We observed that there&#8217;s an &#8220;absurd&#8221; consequence (namely, collapse of the polynomial hierarchy) of a slight strengthening of P=BQP.  That can only make it <i>less</i> likely that quantum computers can be efficiently simulated classically (at least, simulated in a strong sense).</p>\n<ul>One might argue that it\u2019s simply a property of the Universe but IMO that\u2019s alot like smuggling the exponentiality somewhere that isn\u2019t being explicitly considered.</ul>\n<p>But it <i>is</i> being explicitly considered!  The whole point of QC is to explicitly consider it.  Fundamentally, the memcomputing and similar folks go astray because they don&#8217;t appreciate the intellectual <i>enormity</i> of what they&#8217;re trying to do (namely, solve NP-complete problems in polynomial time), so they don&#8217;t perceive the need for a new idea commensurate with that enormity.  But you can&#8217;t accuse QC researchers of similar blindness.  Challenging the ECT&#8212;not even solving NP-complete problems, just pushing the boundaries of efficient computation by a little bit&#8212;would clearly require one of the most profound ideas in the history of the world, but <i>quantum mechanics is indisputably such an idea.</i>  If the ECT is going to fall, quantum mechanics seems like a worthy conqueror.</p>\n<ul>I\u2019ll accept that if amplitudes are <i>exactly</i> like probabilities&#8230;</ul>\n<p>Well, of course they&#8217;re not <i>exactly</i> like probabilities!  If they were, they&#8217;d <b>be</b> probabilities.</p>\n<p>Amplitudes are similar to probabilities in that they&#8217;re not directly observable, and they evolve only via norm-preserving linear transformations, and you need 2<sup>n</sup> of them to describe a configuration of n bits, and they enter physics only as tools for calculating the likelihood that one thing or another is going to happen.</p>\n<p>I regret that this conversation is going around in circles, so we should probably draw it to a close.  But, to try to explain my position one last time: <i>we don&#8217;t get to decide</i> whether we want interference of amplitudes to be a basic property of the universe or not.  Nature has decided, and there&#8217;s nothing Nature could possibly have done to make it clearer that interference of amplitudes <i>is</i> a basic property of the universe.  So to whatever extent our intuitions have trouble with that, the problem that falls to us is to update our intuitions.  The ratchet of physics turns and turns, explaining observed phenomena using mathematical structures that are more and more removed from intuition, but I can&#8217;t think of a single case in history where this ratchet has ever turned backwards, and where the analogues of the QM skeptics have turned out to be right.</p>\n]]>", "author": "Scott", "published": "2015-02-15 03:32:41+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Job #53<br />\n<i></p>\n<p>FWIW, i\u2019m trying to understand the issues, but i can\u2019t see that from QM works it follows that QCs scale.</i></p>\n<p>QM tells us what happens with superpositions and interference. This can be used for computing. The use for computing is called QC.</p>\n<p>QCs are just QM. If QCs don&#8217;t scale, QM doesn&#8217;t scale. QM scales.</p>\n]]>", "author": "JollyJoker", "published": "2015-02-15 14:55:12+00:00", "title": "By: JollyJoker"}, {"content": "<![CDATA[<p><i>&#8230;they enter physics only as tools&#8230;</i></p>\n<p>IMO that&#8217;s the key difference.</p>\n<p>Probabilities enter the classical Universe only as a <i>description</i>, but amplitudes are <i>tools</i> used by&#8230; what? The process that drives interference?</p>\n<p>I was hoping to understand what that process is so that i might reason about why it may or may not scale.</p>\n<p>Anyway, i hope this rather long discussion was at least on topic. Thanks for taking the time.</p>\n]]>", "author": "Job", "published": "2015-02-15 19:27:41+00:00", "title": "By: Job"}, {"content": "<![CDATA[<p>I am definitely not up on my physics (I went to a liberal arts school and never took physics in college), but I had read a really interesting article recently about Stephen Hawking claiming that &#8220;there are no black holes.&#8221;</p>\n<p><a href=\"http://www.pbs.org/newshour/updates/hawking-meant-black-holes/\" rel=\"nofollow\">http://www.pbs.org/newshour/updates/hawking-meant-black-holes/</a></p>\n<p>Apparently, what Hawking meant by this is that there is no such thing as an event horizon&#8211;only an &#8220;apparent horizon&#8221; that holds matter and information for a while, and then releases it.</p>\n<p>Does Hawking&#8217;s assertion that there are no black holes (event horizons) have any relevance to memcomputing?</p>\n<p>Please pardon my weak physics education&#8230;</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/White:Philip.html\">Philip White</a>", "published": "2015-02-15 20:10:50+00:00", "title": "By: Philip White"}, {"content": "<![CDATA[<p>God is very mad at MIT. That is why Boston is experiencing a biblical snowcopalypse. Repent yee MIT sinners.</p>\n]]>", "author": "rrtucci", "published": "2015-02-16 01:36:17+00:00", "title": "By: rrtucci"}, {"content": "<![CDATA[<p>Philip #59</p>\n<p>I&#8217;m gonna go out on a limb and say no.</p>\n<p>But we could have fun speculating what happens if you throw an iPhone into a black hole. I&#8217;m going to guess, purely on the basis that NP-complete problems should be hard, that it takes exponential time/energy to get back.</p>\n]]>", "author": "Lopsy", "published": "2015-02-16 02:02:14+00:00", "title": "By: Lopsy"}, {"content": "<![CDATA[<p>Scott # 56:</p>\n<p><q>The ratchet of physics turns and turns, &#8230; but I can\u2019t think of a single case in history where this ratchet has ever turned backwards&#8230;</q></p>\n<p>What you note here is mostly true, where &#8220;mostly&#8221; means something like at 99.9% times or so. [That is one of the reasons why physics often is held as <i>the</i> model of doing science.] But the number is not 100%; the ratchet has at some rare times moved back too. Even in physics. </p>\n<p>The most salient counter-example would be the atomic theory/approach. Dalton and Agogadro had it as early in the early 19th century, but physicists as a community accepted it only after Jean Perin&#8217;s Nobel-winning work in the early 20th century (based on Einstein&#8217;s 1905 suggestion). </p>\n<p>Suppose you say that it all was chemistry, not physics. Ok.  How about Clausius -> Maxwell -> Boltzmann&#8217;s development of the kinetic theory/statistical mechanics? The essentials were already in place by the end of the 1860s, but in the subsequent three decades, physicists took the success of Maxwell&#8217;s own EM theory, and used its fields or continuum nature in <i>their</i> arguments in order to deny reality to the idea of atoms! They lionized thermodynamics not only because of the general nature of its truths, but also because it was a continuum theory, unlike stat mech. Planck&#8217;s subsequent application of thermodynamics and EM to the cavity radiation problem (a problem already highlighted by Kirchhoff in the mid-1880s) had entirely proceeded without deriving any benefit (even a mere conceptual level benefit) from the particles approach. (In contrast, Rayleigh did feel free to use the Boltzmann statistics. But the point is, there was no unianiminity.)</p>\n<p>A re-examination of the issues in the 21st century (by Gibbs, and separately, by Einstein, Perin) led to the necessary correction, and the ratchet began moving &#8220;forward&#8221; again. </p>\n<p>In short, there was a three decades-long period even in the late 19th century, when the ratchet in an important area of fundamental physics had, at least in part, definitely turned backwards.</p>\n<p>The issue of fields and particles seems to have always confused physicists. Newton suggested a corpuscular nature of light, but Huygens&#8217; idea of pulses (sort of like the sharp characteristics implied by the hyperbolic equations), properly developed into Young&#8217;s wave theory, came to rule physics. And then came cavity radiation. </p>\n<p>And, we know what happened next: the ratchet first splintered into pieces; then each splinter became vague; then some of the splinters even began running back in time&#8212;who cares what other people think, you know? And then, the splintering continued at an ever greater speed, with each splinter either getting guided by a thoroughly ghostly potential (exactly like those angels shoving the planets forward in their orbits) if not idly multiplying universes at an ever-incrasing rate.</p>\n<p>But, of course, as far as these later ideas in physics are concerned, the unidirectionality of their &#8220;development&#8221; is abundantly clear: it always results in ever more increasingly sophisticated mathematical structures&#8212;mathematical structures that are more and more removed from intuition. Who can argue about that point? Answer: None.</p>\n<p>Best,</p>\n<p>&#8211;Ajit<br />\n[E&OE]</p>\n]]>", "author": "Ajit R. Jadhav", "published": "2015-02-16 14:33:21+00:00", "title": "By: Ajit R. Jadhav"}, {"content": "<![CDATA[<p>If its already a post-classical computing thread, I challenge the following question,<br />\nWhy no one is yet to consider the physical efficiency of analog computers like these one , that do not use bits/qubits at all?</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Differential_analyser\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Differential_analyser</a><br />\n<a href=\"http://en.wikipedia.org/wiki/Op_amp_integrator\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Op_amp_integrator</a><br />\n<a href=\"https://www.youtube.com/watch?v=NAsM30MAHLg&#038;src_vid=6dW6VYXp9HM&#038;feature=iv&#038;annotation_id=annotation_2160609135\" rel=\"nofollow\">https://www.youtube.com/watch?v=NAsM30MAHLg&#038;src_vid=6dW6VYXp9HM&#038;feature=iv&#038;annotation_id=annotation_2160609135</a></p>\n<p>they solve computational problems that are used everyday in engineering like Integrals, Differential equations and Fourier transform that appear in &#8220;nature computation&#8221; = physics.<br />\nThere is known to be real progress in these problems in both classical and probabilistic solutions ( Monte Carlo for example ) .<br />\nBut in theoretical CS those problems are classified under RE and not in R!<br />\n<a href=\"http://www-math.mit.edu/~poonen/papers/sampler.pdf\" rel=\"nofollow\">http://www-math.mit.edu/~poonen/papers/sampler.pdf</a><br />\nand not really known and famous field of research.</p>\n<p>How do we know that any analog computer will not stop in the same problem instance as classic computer ? , and if will stop wouldn&#8217;t that be post-Turing device?</p>\n<p>Also I have an interesting assumption, I think that in physical energy* time terms (this amount is in physical action units ), the amount of time and energy needed by those analog devices for the same computation- that do stops ( without preparation of he input ) is far less than any classical computing device .</p>\n]]>", "author": "Itai", "published": "2015-02-16 16:45:13+00:00", "title": "By: Itai"}, {"content": "<![CDATA[<p>To Lopsy #61:<br />\n&#8220;What happens if you throw an iPhone into a black hole?&#8221;</p>\n<p>Steve Jobs is going to be waiting inside the black hole and he will immediately throw the iPhone back at you stating &#8220;how many times have I told you A-holes that you can&#8217;t destroy information !   <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/icon_smile.gif\" alt=\":-)\" class=\"wp-smiley\" /></p>\n]]>", "author": "William Hird", "published": "2015-02-16 19:35:01+00:00", "title": "By: William Hird"}, {"content": "<![CDATA[<p>Scott: You mentioned above that TSP is easy if all the cities are in a straight line. So TSP gets a lot harder when you go from R^1 to R^2. Can anything be said about TSP in R^m compared to in R^n, for m .LT. n? (using Fortran .LT. to avoid fighting the system). </p>\n<p>My guess is not much, because R^2 is already npc, and the tools for working within npc are pretty blunt.</p>\n]]>", "author": "Raoul Ohio", "published": "2015-02-16 23:01:07+00:00", "title": "By: Raoul Ohio"}]