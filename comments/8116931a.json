[{"content": "<![CDATA[<p>James #162, </p>\n<p>You said the body weight heuristic does not fail for any case that you know of. All I&#8217;m saying is there exist many counterexamples :<br />\n-several Bee species exist, some social some solitary; same brain to body weight ratio<br />\n-Heterocephalus glaber forms colonies with some surprizing social-insect-looking features, not solitary talpas; same brain to body mass ratio<br />\n-elephants are considered more clever than rats, despite lower brain to body weight ratio<br />\n-horses are considered more clever than frogs, despite lower  brain to body weight ratio<br />\n-ants and small birds are not believed far clever than most humans, despite the lower brain to body weight ratio of the latters</p>\n<p>&#8230;hope it can help.</p>\n]]>", "author": "Jay", "published": "2014-05-26 17:07:14+00:00", "title": "By: Jay"}, {"content": "<![CDATA[<p>J #171:</p>\n<ul>Actually Scott what do you think about the Indian Institutes of Technology?</ul>\n<p>I&#8217;m all for them!  Some of my best friends are IIT grads.</p>\n<ul>Does the hype that 90% of their alumni being over achieving geniuses hold good?</ul>\n<p>I don&#8217;t think there&#8217;s ever been an institution in human history, 90% of whose graduates were &#8220;geniuses.&#8221;</p>\n]]>", "author": "Scott", "published": "2014-05-26 17:27:03+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Jay</p>\n<p>The ratio is a useful guideline not a law.</p>\n<p>It stands to reason it might breakdown somewhat at the lower or higher extremes of body masses.</p>\n<p>What is fairly clear, however, is that species that we think to be highly conscious have a relatively high brain to body mass ratio.</p>\n<p>At any rate the Eurasian Magpie seems to exhibit signs of conscious behavior and is clearly intelligent. It has a much smaller brain than apes so, unless the information in the magpie brain is a lot more integrated than the information in the ape brain, the amount of integrated information has no direct relationship with consciousness with the possible exception of some threshold amount might be required to enable consciousness at all.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cross:James.html\">James Cross</a>", "published": "2014-05-26 18:27:33+00:00", "title": "By: James Cross"}, {"content": "<![CDATA[<p>Scott #167:  you said:</p>\n<p><i>&#8220;&#8230; you\u2019re not talking about computational intractability, but about chaos or something else.&#8221;</i></p>\n<p>If I&#8217;m understanding correctly, if initial conditions really are known precisely, and nature really is deterministic, there is no such thing as intractability because the incremental changes from one state of the entire system to the next are always tractable?</p>\n<p>But I thought one of the fundamental changes about quantum theory was that outcomes are not deterministic for the same inputs? Doesn&#8217;t nature have to do some kind of &#8220;probability computation&#8221; every time a measurement is made?</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Burgan:Darrell.html\">Darrell Burgan</a>", "published": "2014-05-26 20:00:32+00:00", "title": "By: Darrell Burgan"}, {"content": "<![CDATA[<p>Darrell #175: If the only issue is that your desired output is probabilistic, then that&#8217;s not an issue of computational intractability <i>either</i>.  You simply need to phrase the problem appropriately: either ask for a computation of the probabilities, or ask for a sample from the distribution (in which case, of course you&#8217;ll need a randomized algorithm to have any chance, but that&#8217;s commonly considered and not a big deal).  </p>\n<p>Quantum mechanics does raise questions of computational efficiency, but the reason it raises those questions is not because of the mere occurrence of probabilities in the theory!  Rather, it&#8217;s because of the appearance of <i>amplitudes</i>, which are the things you square to get probabilities, but which evolve in a totally different way than probabilities before a measurement is made.  A quantum computer would take advantage of precisely those differences&#8212;using amplitudes to solve problems that we think are intractable even for a classical probabilistic computer.</p>\n<p>In general, there&#8217;s only ever an issue of computational intractability if you have inputs and outputs, and there&#8217;s a well-defined mathematical relationship between the two, and the issue is just that getting to the outputs given the inputs, although possible in principle with a computer, requires an inordinately large number of steps.  Anything else is something other than computational intractability.  <b>Accept no substitutes!</b> <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/icon_smile.gif\" alt=\":-)\" class=\"wp-smiley\" /> </p>\n]]>", "author": "Scott", "published": "2014-05-26 20:38:46+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Dear Scott,</p>\n<p>I hope that all &#8220;IIT researchers&#8221; of this world regain &#8220;conscienceless&#8221; after reading your post with its elegant counterexample, and realize that they should start working on something else.</p>\n<p>Somewhere in your post you make a side note about the complexity of computing the entropy (&#8220;calculating entropy exactly is a #P-hard problem&#8221;)</p>\n<p>i am unfamiliar with this result and I&#8217;d like to learn about it. Do you have a pointer to a reference where this result is explained/proved?</p>\n]]>", "author": "Niek", "published": "2014-05-26 22:21:32+00:00", "title": "By: Niek"}, {"content": "<![CDATA[<p>correction: i wanted to write &#8220;regain consciousness&#8221;.<br />\n(I am not a native English speaker, and my ios autocorrect function made it even worse)</p>\n]]>", "author": "Niek", "published": "2014-05-26 22:24:09+00:00", "title": "By: Niek"}, {"content": "<![CDATA[<p>Scott, you seem to misunderstand how birthdays are supposed to work, you are supposed to receive gifts, rather than share such a nice one as this IIT take-down with the world <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/icon_smile.gif\" alt=\":-)\" class=\"wp-smiley\" /> </p>\n<p>Anyhow, thanks for this great post, very impressive how exquisitely polite you are about it.  In my own mind I always translated IIT into another two letter acronym that starts with a B and ends in S.  </p>\n<p>But at least they were mathematical about it and so allowed you to work your magic.</p>\n]]>", "author": "quax", "published": "2014-05-26 23:11:24+00:00", "title": "By: quax"}, {"content": "<![CDATA[<p>Niek #177: A canonical #P-complete problem is counting the number S of satisfying assignments to a Boolean formula &phi;(x<sub>1</sub>,&#8230;,x<sub>n</sub>).  Now, it&#8217;s easy to produce a single bit that equals 1 with probability p=S/2<sup>n+1</sup>: to do so, simply flip a coin, and if it lands tails then output 0, and if it lands heads then pick a uniformly random assignment to &phi; and output 1 if and only if the assignment is satisfying.  But then, from exact knowledge of the Shannon entropy H(p), you could work backwards to determine p, and therefore S.  This proves that calculating the Shannon entropy of an efficiently-samplable probability distribution is a #P-complete problem.</p>\n<p>However, the whole point of that remark was to explain that exact computation is the <b>wrong thing</b> to focus on&#8212;you can get that the problem of computing &Phi; is incredibly hard, but that hardness result is purely an artifact of demanding the answer to an absurd precision, and tells you nothing interesting about &Phi; itself.  For that reason, a much better question to ask is the computational complexity of <i>approximating</i> &Phi; to a reasonable precision.  In that case, it&#8217;s known that the problem of estimating the Shannon entropy of an efficiently-samplable probability distribution is a complete problem for SZK (Statistical Zero Knowledge)&#8212;see for example <a href=\"http://www.wisdom.weizmann.ac.il/~oded/COL/entropy.pdf\" rel=\"nofollow\">this survey</a> by Goldreich and Vadhan.  And using that, one can derive that the problem of approximating &Phi; is in the complexity class AM (Arthur-Merlin).  I leave it as a challenge for others to show that approximating &Phi; is NP-hard, and also perhaps to achieve a better upper bound than AM.</p>\n]]>", "author": "Scott", "published": "2014-05-27 00:47:25+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>quax #179: Thanks!  I&#8217;ve been working on being more polite in my criticisms, as I know that&#8217;s an area where I need to improve.  It definitely helps when others are polite to me, as they&#8217;ve been in this discussion.</p>\n]]>", "author": "Scott", "published": "2014-05-27 00:51:37+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Scott #173: Regarding the IITs in India. Thank you for your response. Since you mentioned you have students from IIT, would you take a non-IITian that eats chicken as a student from India (India is a huge country and IIT takes in just 500 cs students every year in its different campuses and most of those happen to be vegetarians)? All the IITians I know are vegetarians and I assume the sample size is large enough to assume most IITians are vegetarians. I also know the IIT almuni have made good contributions to CS and have respect for many of them.</p>\n]]>", "author": "J", "published": "2014-05-27 01:40:01+00:00", "title": "By: J"}, {"content": "<![CDATA[<p>I have to say I&#8217;m a little confused, after skimming the Max Tegmark article linked to in the post, that Max would challenge Scott on IIT.  In the article, he gives several other important properties of conscious systems (such as autonomy), and says &#8220;Just like integration, autonomy is postulated to be a necessary but not sufficient condition for a system to be conscious.&#8221;  </p>\n<p>I think he even goes on to talk about error-correcting codes as examples of things with large Phi value, and I don&#8217;t think anyone thinks extremely long error-correcting codes are as conscious as humans.  It seems clear that many IIT researchers think of integration as the core of consciousness.  However, it didn&#8217;t seem like Max is one of them.<br />\n(And yes, I might just be defending Max since I love his MUH ideas.)</p>\n]]>", "author": "Tyler", "published": "2014-05-27 02:41:32+00:00", "title": "By: Tyler"}, {"content": "<![CDATA[<p>Scott,<br />\nI have been a silent reader of your blog for a while now, but could not resist posting my own (perhaps senseless, as would be determined by your delightful criticism <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/icon_smile.gif\" alt=\":)\" class=\"wp-smiley\" />  ) two cents after I read this very interesting piece on IIT.</p>\n<p>Unfortunately, I have not had the time to read through ALL the 181 comments (I have no clue how you manage it), so maybe I am just going to unnecessarily repeat an idea here. </p>\n<p>As convincingly pointed out by you, IIT seems to be a necessary condition for consciousness. So, what would be a sufficient condition? I believe it is the condition of showing a tendency to lower the entropy of stored information in a system. Allow me to elaborate.</p>\n<p>Take a system A with a memory device B that can store information. With just this much definition, A can be a conscious as well as an unconscious system. However, as soon as we endow this system with the ability to &#8220;process&#8221; information, we find that this system begins to show emerging signs of consciousness.</p>\n<p>The essential test of consciousness (IMHO) seems to be the presence of a bias in the possible responses to stimuli. The higher the randomness is in the response of system A to stimuli, it would appear that lesser the degree of consciousness is in A.</p>\n<p>But bias shows low entropy. As soon as system A begins to process information in such a way as to show a statistical bias in its responses (decisions/conclusions, etc.) to information (which is what any stimulus is, at an elementary level), I think we can agree that this system begins to show signs of consciousness. The quicker such a bias is shown towards larger and larger sets of stimuli, the higher the consciousness.</p>\n<p>Thus, it seems that a possible sufficient test for consciousness could be whether a system A shows a natural tendency to lower the entropy of stored information. </p>\n<p>Of course, one may object that from this perspective even the Google translator would seem conscious. And indeed, I think it is: to a degree, that is. Consciousness must be determined against specific sets of stimuli. The Google translator is conscious to the set of many possible input sentences in many languages. Human beings are unconscious to incident nutrinos. </p>\n<p>Thus, a DVD player is conscious to digitally stored information in a DVD, while human beings are not conscious to such information, until a DVD player decodes it for him, allowing the human being to process &#8211; hence store &#8211; this information in a low entropy form in his brain.</p>\n<p>Thus, to repeat (possibly fallaciously), a system&#8217;s consciousness is determined by:</p>\n<p>1) Specific sets of input information. (E.g., I will show very biased responses to jazz, but very random responses to a nutrino shower.)<br />\n2) Whether this system shows a natural tendency to lower the entropy of such information after storing it in its memory device.<br />\n3) The greater the number of sets of input information whose entropy a system can lower in its memory device, and the quicker it can achieve this, and with fewer external intervention, determines the extent to which the system is conscious.</p>\n<p>Of course, I think it is clear that all forms and levels of consciousness will remain vulnerable to the type of recursion that Timothy Gowers referred to: consciousness does not provide an escape from sufficiently sophisticated illusions. Interestingly, the Hindu religion refers to this problem as &#8220;Maya&#8221;.</p>\n<p>All forms of severe criticism wholeheartedly welcome. <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/icon_smile.gif\" alt=\":)\" class=\"wp-smiley\" /> </p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bose:Arko.html\">Arko Bose</a>", "published": "2014-05-27 03:12:40+00:00", "title": "By: Arko Bose"}, {"content": "<![CDATA[<p>What! No way! I&#8217;ve never been accused of being polite before. Scott, your insults and innuendo really cut to the quick! I&#8217;ll never read this blog again (although I may post comments in it)</p>\n]]>", "author": "rrtucci", "published": "2014-05-27 03:46:35+00:00", "title": "By: rrtucci"}, {"content": "<![CDATA[<p>Scott #176:</p>\n<p><i>&#8220;<b>Accept no substitutes!</b>&#8220;</i></p>\n<p>I guess I keep clinging to the idea that all the phenomena in nature could be completely simulated by a computer, if only we had the right algorithms, that computation is more fundamental than physics. I think my world view must be backwards. Computation emerges from nature, not the other way around.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Burgan:Darrell.html\">Darrell Burgan</a>", "published": "2014-05-27 04:29:38+00:00", "title": "By: Darrell Burgan"}, {"content": "<![CDATA[<p>By the way, Scott, I wanted to thank you for being such a good teacher, in addition to being such a tremendous scientist. Your patience in dealing with folks like me who are completely new to quantum computing is really appreciated.</p>\n<p>I&#8217;m about 1/3 of the way through your book now. Trying not to go on to the next chapter until I have a fair understanding of the previous one, but not always successful.  <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/icon_smile.gif\" alt=\":-)\" class=\"wp-smiley\" /> </p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Burgan:Darrell.html\">Darrell Burgan</a>", "published": "2014-05-27 04:33:42+00:00", "title": "By: Darrell Burgan"}, {"content": "<![CDATA[<p>Happy birthday Scott, and here is some new unrelated weirdness from David Deutsch and Chiara Marletto that you probably already know about:</p>\n<p><a href=\"http://www.scientificamerican.com/article/a-new-theory-of-everything-reality-emerges-from-cosmic-copyright-law/\" rel=\"nofollow\">http://www.scientificamerican.com/article/a-new-theory-of-everything-reality-emerges-from-cosmic-copyright-law/</a></p>\n]]>", "author": "asdf", "published": "2014-05-27 05:32:50+00:00", "title": "By: asdf"}, {"content": "<![CDATA[<p>J #182:</p>\n<ul>would you take a non-IITian that eats chicken as a student from India</ul>\n<p>Eating chicken certainly wouldn&#8217;t disqualify such a student.  Eating quail might.</p>\n]]>", "author": "Scott", "published": "2014-05-27 10:52:17+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Darrell #186: Nothing I said was incompatible with the view that &#8220;nature emerges from computation.&#8221;  If there&#8217;s no way to exploit some phenomenon in order to solve a hard problem, then one could say: there&#8217;s no evidence that <i>Nature itself</i> has to solve a hard problem in order to generate that phenomenon for our viewing pleasure!  The supposed &#8220;hardness&#8221; might only ever have been in a particular human representation of the phenomenon.</p>\n]]>", "author": "Scott", "published": "2014-05-27 10:59:05+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>A consciousness needs also to be defined in terms of a specific environment in which it exists. An intelligence is only as &#8220;good&#8221; as its own environment &#8211; the environment needs to be stable, but also rich and dynamic.<br />\nI think this is one of the reasons of the lack of progress in AI in those last decades &#8211; the illusion that somehow we can replicate/define an intelligence out of a vacuum.<br />\nThe patterns of the planet earth from a billion years ago were already showing the signs of the potential for the apparition of consciousness. How far back is this true? The very fundamental laws of how the physical world assembles must carry this potential&#8230; do atoms have in themselves the intelligence to assemble cities?<br />\nA consciousness closed on itself (imagine severing all sensory inputs on an adult brain) would degenerate into an incoherent mess of fragmented shards riddled with psychosis. The consistency of the external reality is helping keeping our minds together, like an anchor.</p>\n<p>To me one of the biggest mysteries is what it means to &#8220;implement&#8221; a consciousness (the same question applies to what it means to &#8220;implement&#8221; any computation).<br />\nBecause the state of a brain (its evolution both in space and time, including the in/out sensory input data) can always be described by a unique integer (being different based on the representation). What does it then mean to then find that same integer in a different context (say, in the patterns of grains of sands on a beach)&#8230; it probably goes back to the nature of &#8220;information&#8221;, i.e. is there an absolute definition of information or is it a strictly relative/subjective concept?</p>\n]]>", "author": "fred", "published": "2014-05-27 13:13:19+00:00", "title": "By: fred"}, {"content": "<![CDATA[<p>asdf #188: Thanks for the link!  Yes, I saw that paper; Deutsch was kind enough to send me a draft a few weeks ago.  I wish I had something more insightful to say about it.  I enjoyed reading it (as I enjoyed Deutsch&#8217;s previous paper on constructor theory); I very much like the idea of taking &#8220;which tasks can and can&#8217;t be achieved&#8221; as the basic notion in physics.  Of course, with any paper of this kind, the real question is what payoff you get in return for accepting the starting assumptions.  And so far, the payoff seems to consist in the reorganization of certain concepts (like information and copyability) that many people would be content just to take as primitives, or to define in particular contexts (like classical or quantum information theory) where they acquire more mathematical &#8220;meat.&#8221;</p>\n<p>On the other hand, had I read Deutsch&#8217;s original papers on quantum computing in the 1980s, my reaction probably would&#8217;ve been similar!  &#8220;Yes, this is a very lovely idea, so it&#8217;s a shame that nothing too spectacular seems to come out of it yet&#8230;&#8221;  In that case, I would&#8217;ve been right in a sense, but I should&#8217;ve been <i>looking myself</i> for the spectacular thing that Deutsch correctly intuited was there! <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/icon_smile.gif\" alt=\":-)\" class=\"wp-smiley\" /> </p>\n<p>At any rate, I think it&#8217;s a gross exaggeration to say, as the SciAm article does, that &#8220;physicists have no clear definition for what &#8216;quantum information&#8217; even is or how it relates to classical information.&#8221;  It&#8217;s almost exactly like saying that &#8220;physicists have no clear definition for matter or energy.&#8221;  Maybe not, but physics isn&#8217;t about discovering the &#8220;metaphysical essence&#8221; of things&#8212;it&#8217;s about understanding what things <i>do</i>!</p>\n<p>And the idea that &#8220;if only we knew what quantum information truly was, we could discover more quantum algorithms&#8221; would be more convincing if there were even one example of it.  It&#8217;s true that most of the quantum algorithms we know were discovered in a more-or-less haphazard fashion, but so were most of the <i>classical</i> algorithms we know!  Why should we think there&#8217;s anything going on here other than the fact that designing nontrivial new algorithms (like proving nontrivial new theorems) is <i>hard</i>?</p>\n<p>Finally, it&#8217;s worth noting that there&#8217;s been other work on creating common frameworks for classical and quantum information, including that of Lucien Hardy, that of Chiribella et al., and that of <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.3399&#038;rep=rep1&#038;type=pdf\" rel=\"nofollow\">Martin et al.</a> on &#8220;algebraic information theory.&#8221;  It would be interesting to compare constructor theory to these other frameworks.</p>\n]]>", "author": "Scott", "published": "2014-05-27 14:49:01+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>In comment #46 I wrote (in answer to a question):</p>\n<ul>I\u2019m sure you could define a quantum generalization of \u03a6, as you can define a quantum generalization of just about anything. But I\u2019ll leave that as a challenge for others. <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/icon_wink.gif\" alt=\";-)\" class=\"wp-smiley\" /> </ul>\n<p>A less flippant answer is that people&#8212;including yours truly, as it happens&#8212;<i>have</i> thought about measures of the &#8220;global entanglement&#8221; of n-qubit quantum states, where you ask whether the bipartite entanglement is large across a <i>random</i> partition of the qubits into two subsets A and B.  See, in particular, Section 5 of my old <a href=\"http://www.scottaaronson.com/papers/mlinsiam.pdf\" rel=\"nofollow\">Multilinear Formulas and Skepticism of Quantum Computing</a> paper.</p>\n<p>Now, the measure I use there isn&#8217;t <i>exactly</i> like \u03a6, for two reasons: first, I was only concerned with there being large entanglement across a <i>random</i> bipartition; I didn&#8217;t care about the entanglement being large across <i>every</i> balanced bipartition.  And second, I was only interested in static measures of &#8220;global entanglement&#8221; for an single n-qubit state; I wasn&#8217;t looking at dynamics or evolution operators.  However, what I studied there is obviously <i>related</i> to \u03a6&#8212;and in fact, in order to make the global entanglement large, I&#8217;m driven to Vandermonde matrices and error-correcting codes for pretty much the same reasons as in this post!</p>\n<p>The discussion in Section 5 of my paper is also relevant to the comment of Ninguem #128, about the conditions under which the Vandermonde matrix has the &#8220;information integration&#8221; property we want (e.g., it doesn&#8217;t if the field size is too small).</p>\n]]>", "author": "Scott", "published": "2014-05-27 16:13:48+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Sorry for my silly question and thank you for your patience.</p>\n]]>", "author": "J", "published": "2014-05-27 16:53:01+00:00", "title": "By: J"}, {"content": "<![CDATA[<p>Very nice post. Thanks for writing it.</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reichman:Daniel.html\">Daniel Reichman</a>", "published": "2014-05-27 20:52:19+00:00", "title": "By: Daniel Reichman"}]