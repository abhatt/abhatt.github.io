[{"content": "<![CDATA[<p>jonathan #71: You did a very good job of summarizing my view.</p>\n<p>I don&#8217;t have strong grounds for my skepticism about human-level AI in the near future&#8212;just, like, enough grounds to justify to myself <i>not immediately dropping everything I do to worry about this.</i>  As such, while I&#8217;ll share my personal hunches if asked, I&#8217;m totally fine if those hunches aren&#8217;t compelling to others, and if others <i>do</i> want to drop everything else to worry about AI risk.</p>\n<p>On the other hand, I would strongly contend that the &#8220;AI soon&#8221; side doesn&#8217;t have compelling grounds for <i>its</i> views either&#8212;that we&#8217;re both just trying to extrapolate from wildly-inadequate data points to an event that there&#8217;s never been anything like in human history, and that&#8217;s also not governed by any well-established physics or math (which makes it very different from, say, human-caused climate change).</p>\n<p>Sure, one could say that there are impressive advances in AI in recent years, especially Watson and DeepMind and other systems that exploit deep learning.  But one could point to almost <i>any</i> time in the last 65 years and say that impressive advances in AI were happening then (regardless of whether people appreciated it at the time).  So this doesn&#8217;t really give much evidence about &#8220;how much longer until human-level&#8221;: could another thousand years of similarly-impressive advances be needed?</p>\n<p>Personally, I find that skepticism about &#8220;AI soon&#8221; emerges as a more-or-less inevitable byproduct of trying to maintain consistency across all my beliefs.  E.g., I think of humanity&#8217;s future as involving the interplay of many different factors: continued innovation in software and other industries, degradation of (what&#8217;s left of) the environment and flooding of coastal areas, demographic changes, the continued rise of Islamism and continued economic ascendance of China, probably some unforeseen scientific breakthroughs, etc.  I also think of scientific challenges like building a scalable quantum computer, proving P&ne;NP, or understanding the dark matter as being near the limits of what I can imagine happening in my lifetime, and I think of human-level AI as vastly harder than any of those.  And I&#8217;m well-aware that human-level AI is a single factor that would render everything else irrelevant.  And I have a meta-heuristic that tells me to be <b>extremely</b> skeptical of all single-factor hypotheses about the future, to treat them as science-fiction plots or very-low-probability events rather than as plausible.</p>\n<p>Now admittedly, if I were an Inca around 1520, it would no doubt seem to me like there would be many factors affecting the future of the Inca empire: wars against neighboring peoples, the corn harvest, internal Inca power struggles, further development of the Khipu &#8216;writing&#8217; system, etc.  When in reality, there would soon be a single factor&#8212;namely, the arrival of the Spanish&#8212;that would render everything else irrelevant.</p>\n<p>So then the main issue is simply that I don&#8217;t yet see any Spanish ships on the horizon!  And while I do know a few Incas who speculate about the Inca empire being conquered by another civilization, those people disagree among themselves whether the conquerors will be men or beasts or gods, and whether they&#8217;ll come up from the ground, or down from the mountaintops, or from the sun or the moon, and no one seems to have any idea what countermeasures to take.  Maybe these white-skinned Spanish sailors could be essential allies protecting us Incas from the conquerors?</p>\n<p>I will say this, though: if I ever become genuinely convinced that a Singularity is imminent, <b>I&#8217;ll drop everything else I&#8217;m doing and work on it!</b>  I can&#8217;t compartmentalize at all, and would never be able to handle the cognitive dissonance of agreeing intellectually that a coming Singularity would render everything else irrelevant, while still spending my own time doing something else.</p>\n]]>", "author": "Scott", "published": "2016-04-26 17:51:08+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Scott #74</p>\n<p>&#8220;the continued rise of Islamism and continued economic ascendance of China&#8221;</p>\n<p>I&#8217;m quite surprised you put those two things in the same sentence&#8230;</p>\n]]>", "author": "fred", "published": "2016-04-26 17:56:47+00:00", "title": "By: fred"}, {"content": "<![CDATA[<p>fred #75: I wasn&#8217;t expressing a value judgment about either of them (or trying to tie them together)&#8212;just mentioning them as two sociopolitical developments that are happening and that one can plausibly predict will continue to happen.</p>\n]]>", "author": "Scott", "published": "2016-04-26 18:07:02+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Anonymous Bosch #73: Thanks!  I don&#8217;t particularly disagree with anything you say.</p>\n<p>I&#8217;d stress, though, that if someone admits their hypothesis has zero empirical evidence so far, but insists that it <i>could</i> have evidence in the future, it helps if they can at least specify what sort of evidence they have in mind, and what we&#8217;d need to do to acquire it.  So for example, the hypothesis that the dark matter is the lightest supersymmetric particle, Roger Penrose&#8217;s hypothesis that sufficiently large masses trigger an objective reduction of the wavefunction, the hypothesis that a &#8220;pure&#8221; libertarian society (or pure communist society) would be a paradise&#8212;all have obvious testable implications.</p>\n<p>By contrast, even assuming that the world is a simulation being run by superintelligent aliens, <i>and that this is testable</i>, I have hard time seeing how to extract a &#8220;robust prediction&#8221; from that hypothesis, without any auxiliary assumptions about the aliens&#8217; motivations.  Peter Shor likes to joke that we&#8217;ll confirm the hypothesis when we try to test quantum gravity, and instead &#8220;crash the universe,&#8221; since quantum gravity is just an inconsistent edge case that the universe&#8217;s programmers never bothered to account for.  It&#8217;s hard to design a test that <i>doesn&#8217;t</i> sound like a joke!</p>\n<p>And yes, absolutely, I feel the same way about the multiverse.  As <a href=\"http://www.scottaaronson.com/blog/?p=1753\">this blog&#8217;s archives will confirm</a>, I&#8217;ve maintained for years that multiverse speculation is fine, but it needs to play by exactly the same rules as any other kind of science.  I.e., if there are novel, non-obvious observable consequences for this world, then wonderful, go and check them!  If you can&#8217;t find any, try to simplify the theory by cutting the other worlds out, and don&#8217;t ascribe them a great deal of reality in any case.</p>\n<p>One final remark: in talking about the deep differences between gods and simulating aliens, you might be biased toward &#8220;modern&#8221; religions, or Abrahamic ones!  The Greek and Roman vision of the gods, as &#8220;toying with humans for shits and giggles,&#8221; actually seems like an excellent fit to a literal reading of the simulation hypothesis.</p>\n]]>", "author": "Scott", "published": "2016-04-26 18:37:55+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Scott #77</p>\n<p>&#8220;By contrast, even assuming that the world is a simulation being run by superintelligent aliens, and that this is testable,[&#8230;] we\u2019ll confirm the hypothesis when we try to test quantum gravity, and instead \u201ccrash the universe,\u201d&#8221;</p>\n<p>I sure hope they&#8217;re running the world in debug mode and not in release mode.</p>\n]]>", "author": "fred", "published": "2016-04-26 19:12:35+00:00", "title": "By: fred"}, {"content": "<![CDATA[<p>Scott:</p>\n<p>Thanks for the detailed response! I&#8217;m mostly in the same boat as you, though I&#8217;m much less confident in my belief that &#8220;the singularity is far&#8221;, and correspondingly more liable to wonder whether I should drop everything and work on (F)AI.</p>\n<p>Incidentally, given that you think AI is pretty far off, what do you think about alternative routes to the Singularity? I&#8217;m thinking mainly of ways to directly improve human cognition, with the potential to produce intelligence explosion style feedback effects; things like genetic engineering, augmenting mental capabilities through cybernetics, and possibly brain uploads. Do you see all of these technologies as beyond our current planning horizon?</p>\n]]>", "author": "jonathan", "published": "2016-04-26 20:42:44+00:00", "title": "By: jonathan"}, {"content": "<![CDATA[<p>jonathan #79: We could <i>already</i> increase human intelligence, a lot, were it not for the obvious ethical difficulties!  Even if we leave aside old-fashioned selective breeding&#8212;which could plausibly produce some pretty dramatic results within a few generations&#8212;we&#8217;ve already identified plenty of genes that seem implicated in intelligence somehow, and we could edit them today into a human embryo, were we willing to accept the sorts of &#8220;failure rates&#8221; that we accept with rats and mice (as we&#8217;re not, and shouldn&#8217;t be).</p>\n<p>My guess is that gene therapy on humans will become more and more of a reality within our lifetimes&#8212;starting with treating terrible genetic impairments (where the moral case is the strongest and the counterarguments are weakest), and then slowly creeping from there into &#8220;enhancement&#8221; territory (as more and more traits get redefined as &#8220;impairments&#8221;), as we&#8217;ve seen happen with many ordinary drugs.</p>\n<p>The &#8220;bioethicists,&#8221; of course, will fight this process every step of the way, and seem likely to succeed in slowing it down by at least several decades (probably longer in some countries than in others).  But it&#8217;s far from obvious to me that it&#8217;s worth fighting!  Yes, I&#8217;m scared about tinkering this directly with human nature, but I&#8217;m even more scared about <i>humans as they currently exist</i> continuing to destroy the planet.  And gene therapy available to everyone seems infinitely preferable to the cruder eugenics that most educated progressives advocated in the early twentieth century, before the Nazi horrors (understandably) swung the pendulum about 10 quadrillion light years in the opposite direction.  And if there existed a completely safe, routine, effective intervention that could give my kid (say) the intelligence of Alan Turing, I&#8217;d consider myself a terrible parent if I didn&#8217;t use it&#8212;my inaction would seem little different from letting my kid chug lead paint.</p>\n<p>So yes, absolutely, these are issues that we as a society should be paying attention to and debating!  (The answers are far from obvious.)</p>\n<p>Brain uploads are a different matter; that seems at least as hard as AGI (almost tautologically so, since brain-uploading would be a way to <i>achieve</i> AGI).</p>\n<p>As for cybernetics, do you mean like Google Glass? \ud83d\ude42  If there existed a less clunky version&#8212;say, one that could be installed inside your skull, and give you direct neural access to Google and Wikipedia and a calculator, etc.&#8212;I&#8217;d certainly be interested in having that installed.  And such a thing could certainly increase my &#8220;effective intelligence,&#8221; although &#8220;only&#8221; in the same sort of way that Google and Wikipedia <i>themselves</i> increased my &#8220;effective intelligence.&#8221;</p>\n]]>", "author": "Scott", "published": "2016-04-26 21:41:34+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Scott foresees:<br />\n<blockquote>The &#8220;bioethicists,&#8221; of course, will fight this process [of&nbsp;gene therapy] every step of the way.</p></blockquote>\n<p>With comparable justice, mightn&#8217;t bioethicists foresee:<br />\n<blockquote>The &#8220;rationalists,&#8221; of course, will employ scare-quote criticism [of bioethics] every step of the way.</p></blockquote>\n<p>Seriously, is there any real shortage of <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/24325152\" rel=\"nofollow\">high-quality bioethical research</a>?  How do &#8220;scare-quote&#8221; critiques contribute positively to bioethical discussions that <i>already</i> have plenty of overlap with <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/20429137\" rel=\"nofollow\">real-world applications of quantum science</a>&nbsp;&hellip; with more applications to come, no doubt?</p>\n<p>In my reading of history, a more scrupulous attention to ethics would have been no bad thing even in Turing&#8217;s generation! \ud83d\ude42</p>\n]]>", "author": "John Sidles", "published": "2016-04-26 22:44:44+00:00", "title": "By: John Sidles"}, {"content": "<![CDATA[<p>Scott,</p>\n<p>I think the solution to AGI is what I call &#8216;the ontological approach&#8217; (or &#8216;the automated philosopher&#8217;).  We are simply looking for a general method of &#8216;reality modelling&#8217;.</p>\n<p>This can be precisely defined.   By &#8216;reality modelling&#8217;, I mean an ONTOLOGY or DOMAIN MODEL (including CLASS DIAGRAMS) of all reality &#8211; a specific technical example of a modelling language would be UML (Unified Modelling Language).  So we are looking for a modelling language general enough to encapsulate all of reality.</p>\n<p>As you should know from programming, once you have the domain model, the &#8216;hard bit&#8217; is actually done!  The rest is just &#8216;cranking the handle&#8217; .</p>\n<p>So in my view, nearly all  AGI researchers (including MIRI etc.) are looking at the problem on the wrong level of abstraction.  The only bits human should be involved with are the &#8216;interfaces&#8217; (the *symbolic representation* of the things we want, expressed in terms of a high-level modelling language).  The rest of the process should be handled by automated software.  </p>\n<p>For example, take two scientific fields that are very important for AGI research:  &#8216;decision theory&#8217; and &#8216;Bayesian inference&#8217;.  My view is that it&#8217;s the AGI program itself that should be working all that out, not humans. This is because the level of abstraction is too low!  These fields are not on the symbolic level.</p>\n<p>Instead, we want a symbolic modelling language in which we can encapsulate our desire to have decision theory and Bayesian inference solved.  Attempting to solve these problems ourselves won&#8217;t work (we&#8217;re just not smart enough).</p>\n<p>Over the previous decade, I fully completed the top-level domain model of reality.  It decomposes into 27 super-classes.  There are three key principles:</p>\n<p>*Reality should be considered to be a self-modelling system<br />\n*Infinite recursion is always at most 3-levels deep<br />\n*The structure of knowledge is a fractal.</p>\n<p>Any given knowledge domain of non-finite complexity can be  viewed in this fashion.  There are always 3 levels of abstraction:</p>\n<p>*The structural level:      The intrinsic properties of an object. &#8216;What&#8217;s the object made of?&#8217;<br />\n*The functional level:       Extrinsic properties, relations between the object and other objects, &#8216;What does the object do?&#8217;<br />\n*The representational level:  How to symbolically represent the object, &#8216;How can we talk about the object?&#8217;</p>\n<p>It is only the representational (symbolic) levels that humans should be concerned with.  These are the &#8216;interfaces&#8217; (or maps) of knowledge.   We want to express our (human) desires in terms of the maps or interfaces, and leave all the technical (lower-level) details to our automated software.</p>\n]]>", "author": "mjgeddes", "published": "2016-04-26 23:10:13+00:00", "title": "By: mjgeddes"}, {"content": "<![CDATA[<p>mjgeddes,<br />\n&#8220;As you should know from programming, once you have the domain model, the \u2018hard bit\u2019 is actually done! The rest is just \u2018cranking the handle\u2019 .&#8221;<br />\nWhat about go? The domain model is easy, but writing a program good at go is extremely hard.  One would expect AGI to be strictly more difficult than go.</p>\n]]>", "author": "Luke G", "published": "2016-04-27 02:40:43+00:00", "title": "By: Luke G"}, {"content": "<![CDATA[<p>Luke #83</p>\n<p>Surely the lesson from AlphaGo is that writing a program to play strong Go turned out to be far easier than everyone expected?</p>\n<p>AlphaGo perfectly illustrates the points I made in my above post!  Virtually all the hard work was done on the symbolic level &#8211;   a high-level planning method (MonteCarlo Tree Search) and getting the right initial representations for machine learning.</p>\n<p>Once the lower-level machine learning methods got to work (&#8216;cranking the handle&#8217; pattern recognition and prediction), it did most of the learning itself, even with very little prior knowledge of Go.</p>\n]]>", "author": "mjgeddes", "published": "2016-04-27 03:17:35+00:00", "title": "By: mjgeddes"}, {"content": "<![CDATA[<p>In the above discussion of &#8220;The Singularity&#8221;, I missed exactly what singularity was being discussed. Can anyone fill me in? A couple potential singularities that have been receiving press in recent years include:</p>\n<p>1. When we eat enough vitamins, we will all live forever.</p>\n<p>2. The computers taking over. (In an old SF story, this caused all the phones in the world to ring. As it turns out, we just wake up with Windows 10 installing itself. I don&#8217;t think any SF author predicted that.)</p>\n<p>3. As AI passes some &#8220;tipping point&#8221;, everything will zoom through the stratosphere. </p>\n<p>It is not clear that these will all be a &#8220;good thing&#8221;, and I will probably keep plugging away at stuff that I am good at rather than jumping on board. </p>\n<p>Here is some &#8220;singularity info&#8221; for those without a tech background. I assume the word singularity is borrowed from function theory and/or differential equations. </p>\n<p>In function theory, singularities are classified (from not bad at all, &#8230;, to major bummer) as jump, pole, logarithmic, and essential. I think poles are the picture people talking about singularities have in mind.</p>\n<p>The simplest mathematical model of whatever it is you are yammering about is a scalar function of time; x(t). The evolution of x(t) is likely to be controlled by an ordinary differential equation. Linear ODE&#8217;s have no singularities, so nonlinear is de rigueur. When wearing your physicist hat, one is required to try the easiest thing first, the simplest nonlinear ODE:</p>\n<p>x&#8217; = x^2,</p>\n<p>with initial condition x(t_0) = x_0.</p>\n<p>Defining t_s = t_0 + 1 / x_0, the solution to the IVP (Initial Value Problem) is</p>\n<p>x(t) = 1 / (t_s &#8211; t).</p>\n<p>If you graph |x(t)|, you will see why this called a &#8220;pole&#8221;. The pole is at t = t_s. In this old days, this was illustratively called &#8220;finite time to blowup&#8221;. </p>\n<p>If you have some data, say x(t_1), x(t_2), x(t_3), you can try to fit these to a curve for 1 / (t_s &#8211; t), and thereby estimate t_s, the time of the singularity. Usually it turns out that the quantity x is loosely defined, so it is hard to get good data.</p>\n<p>Here is one that might be doable: Can anyone estimate the relative difficulty of a computer beating the best human in chess and the best human in go? These have well defined times, so maybe you can predict when this particular singularity will occur. For example, if mastering chess (1997) is 1 unit of hard, and mastering go (2016) is Z units of hard, than </p>\n<p>t_s = 1997 [(2016/1997)Z &#8211; 1] / [Z &#8211; 1], </p>\n<p>so if Z = 5, the singularity is predicted for late 2020. Of course, if Trump or Cruz is President, there might be another kind of singularity about then.</p>\n]]>", "author": "Raoul Ohio", "published": "2016-04-27 04:54:20+00:00", "title": "By: Raoul Ohio"}, {"content": "<![CDATA[<p>Lessons from the past for the present (#74):<br />\n<blockquote>If I were an Inca around 1520 [&hellip;] there would soon be a single factor&nbsp;&mdash; namely, the arrival of the Spanish&nbsp;&mdash; that would render everything else irrelevant. [&hellip;] So then the main issue is simply that I don\u2019t yet see any Spanish ships on the horizon!</p></blockquote>\n<p>Recent genomic research suggests that *FAR* worse than depredations of spanish soldiers was the devastation wrought by introduced diseases.  High-resolution genome surveys like Llamas <i>et al.</i> &#8220;Ancient mitochondrial DNA provides high-resolution time scale of the peopling of the Americas&#8221; (2016) point toward <a href=\"http://advances.sciencemag.org/content/2/4/e1501385\" rel=\"nofollow\">widespread extinction-level population losses</a>:<br />\n<blockquote>None of the 84 lineages they [Lamas and colleagues] found are even traceable past contact because not a single living person who belongs to any of them has been found.</p></blockquote>\n<p>This level of complete population extermination is far beyond what a few hundred Spanish soldiers could feasibly achieve; and indeed, far beyond what any of the technology-assisted genocidal holocausts of the 20th century ever did achieve.</p>\n<p><b>Implication</b>&nbsp; The first Horseman of the Apocalypse, namely Pestilence, is still abroad in the 21st century, and humanity&#8217;s emerging geonomic history suggests that this particular Horseman may yet appear as &#8220;a Spanish ship on our horizon&#8221;.</p>\n<p><b>A personal note</b> in the spring of 2015 my son and I were hiking in the still-snowy high Cascades, where we came upon an obviously sick out-of-hibernation bat fluttering weakly on an icy lake.  My son and I reported the encounter to the state Wildlife Commission, but we did not attempt to capture the bat for biopsy (a decision that we subsequently regretted).  </p>\n<p>Now in the spring of 2016,  biopsy of a similar out-of-hibernation bat has sadly confirmed that the deadly plague of White-Nose Syndrome (SNW) has spread to pacific northwest bat populations. \ud83d\ude41</p>\n<p>So yes, we mammals remain highly vulnerable to pestilence-driven extinction events.</p>\n<p><b>Respecting diversity</b>&nbsp; Does this mean we should all &#8220;drop everything else we&#8217;re doing and work on this problem&#8221; (in Scott&#8217;s phrase of #74)?  Especially because quantum/nanoscale phenomena are so intimately bound-up in the development of next-generation biomedical research technologies?   </p>\n<p>Here it is best (as it seems to me) for a diversity of opinion to prevail.  But the plain lesson of genomic history is that &#8220;Spanish ships of pestilence&#8221; are real.</p>\n]]>", "author": "John Sidles", "published": "2016-04-27 07:11:04+00:00", "title": "By: John Sidles"}, {"content": "<![CDATA[<p>Raoul:</p>\n<p>I think of the &#8220;Singularity&#8221; as referring to the intelligence explosion idea &#8212; i.e. that if a species figures out how to directly increase its intelligence, it can then use its greater intelligence to figure out further ways to increase its intelligence, etc.</p>\n<p>For instance, suppose that the intelligence of our computers is x, which is increasing at growth rate g, which is a function of the intelligence of AI researchers y:</p>\n<p>dx/dt = g(y) * x</p>\n<p>Where y is fairly constant over time, and g(y) is probably quite small, given the difficulty of the AI problem to human-level intelligence.</p>\n<p>Now suppose that at some point, x rises above y. Then the computers would take over the AI research, and their intelligence would start increasing at rate:</p>\n<p>dx/dt = g(x) * x</p>\n<p>Of course, what happens next depends on the function g. Suppose that g is linear in x, i.e. g(x) = ax. Suppose we normalize x(0)=1. Then:</p>\n<p>x(t) = 1 / (1-at)</p>\n<p>So x blows up as t -> 1/a. That&#8217;s your singularity.</p>\n]]>", "author": "jonathan", "published": "2016-04-27 12:10:20+00:00", "title": "By: jonathan"}, {"content": "<![CDATA[<p>Raoul:</p>\n<p>Ah, I see that I neglected to read the second half of your comment. It seems we are thinking along similar lines \ud83d\ude09</p>\n]]>", "author": "jonathan", "published": "2016-04-27 12:12:14+00:00", "title": "By: jonathan"}, {"content": "<![CDATA[<p>Following up on jonathan:</p>\n<p>That is a good stage 1 for math modeling (try to solve the simplest system that plausibly captures the key elements). Here is a slightly better version;</p>\n<p>p(t): intelligence of AI programs<br />\nr(t): intelligence of (human) AI researchers.</p>\n<p>This system is likely controlled to first order by</p>\n<p>dp/dt = Ap + Br<br />\ndr/dt = C + Dp</p>\n<p>for positive constants A, B, C, and D, and an initial condition</p>\n<p>Ap muchLessThan Br </p>\n<p>(don&#8217;t try writing any lessThan signs here). This system is linear, so it will not have a singularity, but the growth is controlled by exp(\\lambda t) with </p>\n<p>\\lambda = (1/2) [A  + sqrt(A^2 + 4BD)].</p>\n<p>I will think about this some more and attempt to identify the leading nonlinear term.</p>\n]]>", "author": "Raoul Ohio", "published": "2016-04-27 17:18:42+00:00", "title": "By: Raoul Ohio"}]