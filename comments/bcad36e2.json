[{"content": "<![CDATA[<p>David #35: Yes, thanks for that!  I&#8217;d estimate that maybe a third of theoretical computer science boils down in one way or another to query complexity. <img src=\"http://www.scottaaronson.com/blog/wp-includes/images/smilies/simple-smile.png\" alt=\":-)\" class=\"wp-smiley\" style=\"height: 1em; max-height: 1em;\" />  You see it even in seemingly remote places, like cryptography and computational economics, as well as of course in basic algorithms and data structures.</p>\n]]>", "author": "Scott", "published": "2015-06-19 13:04:30+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>From your description it sounds like the fourth-power speedup over D(g) is the result of accumulating smaller \u221an speedups (using Grover&#8217;s and the fanout enhancement?). Is that how it is?</p>\n<p>If so, what&#8217;s stopping us from using that approach to get increasingly larger speedups for similar problems? (e.g by adding more dimensions to the table)</p>\n]]>", "author": "Job", "published": "2015-06-19 19:41:35+00:00", "title": "By: Job"}, {"content": "<![CDATA[<p>Job #37: Yes, although the two &radic;n speedups come from different sources, so it&#8217;s not like you can keep accumulating more of them willy-nilly.</p>\n<p>You could try adding more dimensions to the table, but in the existing constructions, both of the dimensions serve very specific purposes, so you&#8217;d have to invent a purpose for the new dimensions to serve.  My <i>guess</i> (I might be wrong) is that you&#8217;ll get diminishing returns this way&#8212;at least, that&#8217;s what&#8217;s happened in the past whenever I&#8217;ve tried to produce larger separations between query complexity measures by generalizing a 2-dimensional problem to 3 or more dimensions.  Note also that, no matter what you do, we <i>know</i> that for total Boolean functions, you can&#8217;t get any separations bigger than D~R<sub>0</sub><sup>2</sup> (which Ambainis et al. already achieved), or D~R<sup>3</sup>, or R<sub>0</sub>~R<sup>2</sup>, or D~Q<sup>6</sup>.  So while it&#8217;s conceivable that adding dimensions could help in some way, there must be a limit beyond which it stops helping.</p>\n]]>", "author": "Scott", "published": "2015-06-19 20:02:49+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Sorry, unrelated to query complexity: optimizing packing in the real world: <a href=\"http://www.quora.com/How-did-game-developers-pack-entire-games-into-so-little-memory-twenty-five-years-ago\" rel=\"nofollow\">http://www.quora.com/How-did-game-developers-pack-entire-games-into-so-little-memory-twenty-five-years-ago</a></p>\n<p>> my packer used a variety of algorithms (first-fit, best-fit, etc.) to try to find the best packing, including a stochastic search akin to the gradient descent process used in Simulated annealing. Basically, I had a whole bunch of different packing strategies, and would try them all and use the best result.</p>\n<p>I wonder if a CS expert would have suggested something better.</p>\n<p>> The problem with using a random guided search like that, though, is that you never know if you&#8217;re going to get the same result again. Some Crash levels fit into the maximum allowed number of pages (I think it was 21) only by virtue of the stochastic packer &#8220;getting lucky&#8221;. This meant that once you had the level packed, you might change the code for a turtle and never be able to find a 21-page packing again.</p>\n]]>", "author": "Shmi Nux", "published": "2015-06-19 22:35:47+00:00", "title": "By: Shmi Nux"}, {"content": "<![CDATA[<p>Scott,</p>\n<p>Has anyone looked for how lowarge query complexity separations between quantum computers and the sort of hidden variable models you were looking at? You had the N^{1/3} database search example which beats Grover&#8217;s N^{1/2}. Can this be turned into a Boolean function separation?</p>\n]]>", "author": "Joshua Zelinsky", "published": "2015-06-20 14:26:23+00:00", "title": "By: Joshua Zelinsky"}, {"content": "<![CDATA[<p>Joshua #40: That&#8217;s a wonderful question; thanks for asking it!</p>\n<p>Let DQ(f) be the &#8220;dynamical quantum query complexity&#8221; (i.e., quantum query complexity in the hidden-variable model I defined in 2005).</p>\n<p>Then I <i>conjecture</i> that DQ(f)=&Omega;(bs(f)<sup>1/3</sup>) for all f; this would follow if a cube-root speedup was the best you could get for the Grover search problem.  In my original paper, I had sketched what I claimed was a proof for this, but the proof was wrong&#8212;see my blog post about this, or the later <a href=\"http://arxiv.org/abs/1412.6507\" rel=\"nofollow\">paper</a> with Bouland, Fitzsimons, and Lee.</p>\n<p>Anyway, <i>if</i> the cube-root lower bound is true&#8212;or true for some particular class of hidden-variable theories&#8212;then for those theories, combining with D(f)=O(bs(f)<sup>3</sup>) would yield D(f)=O(DQ(f)<sup>9</sup>).</p>\n<p>What <i>separations</i> are possible between D(f) and DQ(f), or between R(f) and DQ(f), etc. in light of Ambainis et al.&#8217;s results, is a great question that I&#8217;ll take up in a subsequent comment.</p>\n]]>", "author": "Scott", "published": "2015-06-20 17:07:46+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>I think $D~R_0^{3/2}$ comes from the fact that:</p>\n<p>1) If there are more than n^{3/2} zeroes on the board, then random sampling will knock out a column after O(sqrt(n)) samples. Unless the distribution is very uneven, random sampling of un-eliminated columns will eliminate all but sqrt(n) columns within O(n * sqrt(n)) samples; and then you can just brute force the rest.</p>\n<p>2) If there are fewer than n^{3/2} zeroes, then fully scanning randomly chosen columns, then following all the chains, and iterating on un-eliminated columns, becomes viable. If there is a long chain of zeroes, like there must be if there&#8217;s a solution, then you&#8217;ll tend to hit it at the halfway mark when you sample a column, cutting the un-eliminated columns in half.</p>\n<p>3) I&#8217;m not exactly sure how to solve the case where there isn&#8217;t a long chain of zeros and there&#8217;s less than n^{3/2} zeroes. I think it has to do with finding columns A and B such that A doesn&#8217;t lead to B and B doesn&#8217;t lead to A proving there&#8217;s no solution. An adversary trying to avoid that disproof being easy to stumble upon might have to connect columns in ways that mean searching outward from one column must lead to at least half of the other columns on average, eliminating them.</p>\n]]>", "author": "Craig Gidney", "published": "2015-06-20 20:34:02+00:00", "title": "By: Craig Gidney"}, {"content": "<![CDATA[<p>Joshua #40: I&#8217;ve now thought a bit about your question of getting bigger separations between between classical and &#8220;hidden-variable&#8221; query complexities, than we know between classical and quantum.  The situation turns out to be remarkably rich and interesting.</p>\n<p>The fundamental difficulty is that, with the hidden-variable models, you only get to see a hidden-variable history <b>once</b>.  I.e., you don&#8217;t get to sample multiple hidden-variable histories in superposition, then sample the history of a hidden variable through <i>that</i> quantum computation, and so on recursively.  Because of this, hidden-variable algorithms don&#8217;t compose in the way you would like.  And since the quantum algorithms for the Ambainis et al. functions all work by composing multiple calls to Grover&#8217;s algorithm, this failure of composition is a serious problem.</p>\n<p>As an easier warmup case, let&#8217;s consider an OR of &radic;N AND&#8217;s of &radic;N variables each.  This function is well-known to have quantum query complexity &Omega;(&radic;N).  Thus, by analogy to the case of the OR function, one might hope that its hidden-variable query complexity would be O(N<sup>1/3</sup>).  The trouble is that that would require recursion: if we run the recursive Grover algorithm, then sampling the hidden variable will speed up the search for a subtree of the OR that evaluates to 1, but it&#8217;s not going to speed up the search <i>within</i> each subtree.  For this reason, the best upper bound I&#8217;m able to get on the hidden-variable query complexity of the OR/AND tree is</p>\n<p>O( (&radic;N)<sup>1/3</sup> (&radic;N)<sup>1/2</sup>) = O(N<sup>5/12</sup>).</p>\n<p>This is better than &radic;N, but not all the way down to N<sup>1/3</sup>.</p>\n<p>And for Ambainis et al.&#8217;s function h&#8212;the one that gives a cubic separation between R<sub>0</sub> and Q&#8212;I&#8217;m not able to get <i>any</i> upper bound on hidden-variable query complexity better than the upper bound on Q.  Basically, hidden-variable sampling only obviously helps you for the <i>last step</i> of the algorithm, the one where you&#8217;re verifying that the cycle of &#8220;marked rows&#8221; really is marked.  But when you plug in m<sup>1/3</sup> rather than &radic;m for the query complexity of the last step, the linear program that tells you how to optimize the parameters still doesn&#8217;t return a better solution than T=(mn)<sup>1/3</sup> queries.</p>\n<p>Likewise, for Ambainis et al.&#8217;s f function&#8212;the one they use to achieve a quartic separation between D and Q&#8212;I also currently can&#8217;t get anything better when I consider the hidden-variable query complexity.  The reason is that we have D=&Omega;(nm) assuming n&ge;2m, while (ignoring log factors)</p>\n<p>Q = O(&radic;n + &radic;m),</p>\n<p>and the hidden-variable complexity is O(&radic;n + m<sup>1/3</sup>).  But as long as we need the assumption n&ge;2m, improving that &radic;m to m<sup>1/3</sup> just doesn&#8217;t make a difference.</p>\n<p>OK, but having said all that: there&#8217;s also an alternative model we could consider, where you get to make &#8220;multiple non-collapsing measurements&#8221; of a state, then use the (classical) results of those measurements as inputs to your next quantum computation, and so on.  (Note, however, that you still don&#8217;t get to invoke the &#8220;multiple non-collapsing measurements&#8221; oracle in superposition.)  This is basically the model that Bouland, Fitzsimons, Lee, and I considered.</p>\n<p>In this stronger model, first of all, using T queries, you can prepare a superposition that has a probability mass of T<sup>2</sup>/N on the &#8216;1&#8217; branch of the OR/AND tree.  You can then keep sampling from that superposition over and over, and for each branch you see, check whether it&#8217;s indeed a &#8216;1&#8217; branch in O(N<sup>1/6</sup>) queries.  So if a &#8216;1&#8217; branch exists, then you&#8217;ll succeed in finding one after (N/T<sup>2</sup>)*N<sup>1/6</sup> queries total.  Setting</p>\n<p>T = N<sup>7/6</sup>/T<sup>2</sup></p>\n<p>and solving for T, we find that a non-collapsing algorithm can evaluate the OR/AND tree with</p>\n<p>O(N<sup>7/18</sup>) ~ O(N<sup>0.389</sup>)</p>\n<p>queries, which is better than the</p>\n<p>O(N<sup>5/12</sup>) ~ O(N<sup>0.417</sup>)</p>\n<p>for the hidden-variable algorithm, but still not all the way down to O(N<sup>1/3</sup>).</p>\n<p>In the non-collapsing measurements model, I also get that, by setting k=N<sup>2/7</sup> and m=N<sup>6/7</sup> (where N is the total number of cells), we can get an O(N<sup>2/7</sup>)-query algorithm for the function h.  This yields a 3.5<sup>th</sup>-power separation between R<sub>0</sub> and the non-collapsing query complexity, better than the cubic separation that Ambainis et al. got between R<sub>0</sub> and Q.</p>\n<p>Likewise, for Ambainis et al.&#8217;s function f, I get that there&#8217;s a non-collapsing algorithm for the first stage (the one that finds the candidate marked column) that uses</p>\n<p>O( min{ (n/k)<sup>1/3</sup>k, k<sup>1/3</sup>&radic;(n/k) } ) = O(n<sup>7/15</sup>)</p>\n<p>queries, and an algorithm for the second stage (the one that verifies the marked column) that uses O(m<sup>1/3</sup>) queries (as usual, ignoring log factors).  Combining this with the lower bound D(f)=&Omega;(nm) that holds when n&ge;2m, we find that the non-collapsing query complexity of this function is</p>\n<p>O(D(f)<sup>7/30</sup>) ~ O(D(f)<sup>0.233</sup>).</p>\n<p>So, again, this is slightly better than the quartic separation between D and Q.</p>\n]]>", "author": "Scott", "published": "2015-06-20 21:15:04+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>OK, everyone&#8212;Andris has kindly sent me a proof of the n<sup>2/3</sup> randomized upper bound for GPW&#8217;s original function (or rather, a taller, skinnier variant thereof), which I&#8217;m reproducing below.  It is, indeed, broadly along the lines that Craig Gidney #42 suggested.</p>\n<p>&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;</p>\n<p>here is the algorithm. Set the number of columns to n^{1/3}, the number of rows to n^{2/3}.</p>\n<p>1. Sample n^{1/3} log n random entries in each column, eliminate all columns with at least one 0. W.h.p. this eliminates all columns in which the total number of 0s is at least C n^{1/3}.</p>\n<p>2. Repeat C log n times, for an appropriately chosen C:</p>\n<p>2a. Choose a random column in which we do not know any 0s, query all elements in it.</p>\n<p>2b. If the column is all-1, test if f=1.</p>\n<p>2c. If not, take every 0 in the column and pursue the sequence of pointers starting at it until the sequence ends or visits the same column for the second time.</p>\n<p>3. If an all-1 column was not found in step 2, answer &#8220;f=0&#8243;</p>\n<p>The number of queries is:</p>\n<p>Step 1: n^{1/3} log n in each of n^{1/3} columns, total of n^{2/3} log n;</p>\n<p>Step 2: in each iteration, we have:<br />\n&#8211; n^{2/3} queries to query all the elements of column;<br />\n&#8211; C n^{2/3} queries to pursue c n^{1/3} sequences of pointers, each of which is of length <= n^{1/3};<br />\nSince the number of iterations is O(log n), the total number of queries is O(n^{2/3} log n).</p>\n<p>Correctness: Let S_i be the set of columns that contain 0s but for which we have not yet queried any zeros, after i repetitions of step i. I would like to claim that, if f=1, E[S_{i+1}]<=E[S_i/2]. This implies that, after O(log n) repetitions, S_i will be empty w.h.p.</p>\n<p>The proof of E[S_{i+1}]<=E[S_i/2] is as follows:<br />\n&#8211; if f=1, there is the good path of pointers;<br />\n&#8211; all columns of S_i are on this path;<br />\n&#8211; if we choose column k in step 2a, we end up eliminating from S_i all the columns which are after column k on the path;<br />\n&#8211; since k is chosen randomly, every other column in S_i is after k with probability 1/2.</p>\n]]>", "author": "Scott", "published": "2015-06-20 22:22:20+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Hello, Scott.  Thanks for this writeup, it&#8217;s a nice introduction to query complexities.  It seems that this is a TCS topic that would deserve to be known better.  David Speyer #35: nice comment, thanks.</p>\n<p>I agree with Sniffnoy #20 that the way you leave quantifiers implicit in the definitions of R and R_0 is confusing.  Even after your reasoning in #23, I think you should try to improve that part of the description.</p>\n]]>", "author": "jonas", "published": "2015-06-20 23:31:04+00:00", "title": "By: jonas"}]