[{"content": "<![CDATA[<p>@Scott:<br />\nYou are so right about learning and time. I think not realizing that early enough might have been one of the biggest mistakes in my whole life.<br />\nThe other thing is that it&#8217;s really good for one&#8217;s character to live without much money &#8211; helps you to stay closer to the roots.</p>\n<p>@Ewin:<br />\nNice one! Keep up the good work \ud83d\ude42</p>\n]]>", "author": "C. Sowak", "published": "2018-07-13 11:46:24+00:00", "title": "By: C. Sowak"}, {"content": "<![CDATA[<p>J-K #8: Cool!  For what?</p>\n]]>", "author": "Scott", "published": "2018-07-13 12:33:00+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>Vaso #5: The implementation of this will require working through some of the assumptions in the paper. For one, a &#8220;low-overhead data structure&#8221;. Maybe it could be an extension of an already well-engineered framework for building recommendation systems.</p>\n<p>Scott #10: Here&#8217;s a paper where researchers predicted cancer drug response using a recommendation system: <a href=\"https://www.biorxiv.org/content/early/2017/11/07/215327\" rel=\"nofollow\">https://www.biorxiv.org/content/early/2017/11/07/215327</a></p>\n]]>", "author": "Brian", "published": "2018-07-13 14:09:52+00:00", "title": "By: Brian"}, {"content": "<![CDATA[<p>Shot in the dark, just because I happened to be skimming arxiv.org/abs/1803.04189 yesterday and it seems to do with training machines on sparse data &#8212; related?</p>\n]]>", "author": "pst", "published": "2018-07-13 16:47:55+00:00", "title": "By: pst"}, {"content": "<![CDATA[<p>After reading the post and then coming back the next day and reading the title again, I see what a great title it was. As my nephews say, sweet!</p>\n]]>", "author": "JimV", "published": "2018-07-14 14:19:52+00:00", "title": "By: JimV"}, {"content": "<![CDATA[<p>Scott,<br />\n    You say: &#8220;Now, though, I feel compelled to point out that, in addition to the potentially lucrative application to Amazon and Netflix, research on low-rank matrix sampling algorithms might someday find many other, more economically worthless applications as well.&#8221;  Don&#8217;t you mean more economically worthwhile applications as well? Or maybe that&#8217;s a joke.</p>\n]]>", "author": "rick samborski", "published": "2018-07-14 21:46:07+00:00", "title": "By: rick samborski"}, {"content": "<![CDATA[<p>rick #14: Reread that sentence in the context of the whole paragraph and see if you get the joke.</p>\n]]>", "author": "Scott", "published": "2018-07-14 23:18:52+00:00", "title": "By: Scott"}, {"content": "<![CDATA[<p>I glance at this blog occasionally in hopes that some day I&#8217;ll understand s.t., and that it will make a difference.  Alas, I&#8217;m a linguist with some computing knowledge (I understand the basics of algorithmic complexity), but most of this is over my head.</p>\n<p>But I have a question about a linguistic application that *sounds* like it might be related to this recommendation problem.  In morphology, we have languages with multiple inflection classes, like Spanish -ar/-er/-ir verbs (similar systems in other Romance languages); the set of affixes is the same for all verbs within an inflection class, for example Spanish hablar (&#8220;to speak&#8221;) and arreglar (&#8220;to arrange&#8221;) both belong to the ar class, and therefore have all the same suffixes.  Often some of the affixes are the same across all classes, for instance -o is the first person present indicative for -ar, -er and -ir verbs.</p>\n<p>Some languages have far more than three such systems; and these inflection classes can occur for nouns and adjectives as well as for verbs.  And languages tend to have lots more nouns than verbs.</p>\n<p>To some extent cross-cutting these inflection classes are alternations in the stem.  The Spanish verb tener (&#8220;to have&#8221;) is -er class, and venir (&#8220;to come&#8221;) is -ir class; and although they have some affixes that are different because of this, they have many of the same changes in their stem (= the part before the suffix), like tienes (&#8220;you have&#8221;) and vienes (&#8220;you come&#8221;), where the &#8216;e&#8217; of the stem has changed to &#8216;ie&#8217;.</p>\n<p>A linguistic term that I&#8217;ll need to use is &#8216;lexeme&#8217;: it means a word, including all its affixed affixed forms.  For English, an example would be RUN, with inflected forms run, runs, running, and ran.</p>\n<p>Spanish dictionaries of course tell you all this, i.e. given a lexeme, the dictionary tells you enough that you can construct all its affixed forms.  But if you&#8217;re working on a less studied language, you may not have a good dictionary, indeed you may be creating the language&#8217;s first dictionary from words you find in corpora (texts).  And except for the most common words (and sometimes not even then), you won&#8217;t find every form of every word in the corpus.  In other words (pun intended), the matrix of inflected forms of lexemes is sparse (the matrix would contain, for each of the m lexemes, all n of its affixed forms, including stem changes).</p>\n<p>Your job as a lexicographer is then to decide which inflection class and which kinds of stem change a given lexeme has (or could have, since the answer may be ambiguous given the attested forms).  For any given a column (let&#8217;s say different verbs are different columns) that&#8217;s the most similar to that column, in some sense of similar (same affixes, same stem changes).</p>\n<p>I&#8217;m guessing there&#8217;s something about this problem that is different from the recommender problem, but I&#8217;m not seeing what it is.  (Actually, I know of one such difference: it has to do with the number of inflection classes you get in real languages, which is far lower than what you could get in principle, given the number of distinct affixes.  But part of my interest lies in why this constraint exists in real languages.)</p>\n]]>", "author": "<a href=\"http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maxwell:Mike.html\">Mike Maxwell</a>", "published": "2018-07-15 02:29:41+00:00", "title": "By: Mike Maxwell"}, {"content": "<![CDATA[<p>Hi Ewin (hopefully you are reading this),<br />\nI&#8217;m going to start my thesis soon and I would love to try writing it about your paper. Of course I would not want to steal your own paper (plus you will probably be in a better position to write such a paper :)). Are you planning to try looking into lowering the polylog degree yourself soon, or is it free for grabs?</p>\n]]>", "author": "Yovel", "published": "2018-07-15 05:54:55+00:00", "title": "By: Yovel"}, {"content": "<![CDATA[<p>Mike #16: </p>\n<p>That does seem like a sensible application! I will note, though, that if n is sufficiently small (as it sounds like it might be in this case), the brute-force ways to do these types of linear algebra tasks become pretty fast (not much more time than it would take to upload the corpus into the matrix).</p>\n<p>Yovel #17:</p>\n<p>Feel free! I&#8217;m not currently planning to try lowering the exponents. From my perspective, the easiest way of doing this would be to abandon my algorithm strategy completely and do something more direct. It&#8217;s reasonable to me that such an approach could improve the exponents by at least half.</p>\n]]>", "author": "Ewin", "published": "2018-07-16 01:50:44+00:00", "title": "By: Ewin"}]