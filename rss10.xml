<?xml version="1.0"?>
<rdf:RDF
	xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:foaf="http://xmlns.com/foaf/0.1/"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns="http://purl.org/rss/1.0/"
>
<channel rdf:about="http://feedworld.net/toc/">
	<title>Theory of Computing Blog Aggregator</title>
	<link>http://feedworld.net/toc/</link>
	<description>Theory of Computing Blog Aggregator - http://feedworld.net/toc/</description>

	<items>
		<rdf:Seq>
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-8890204.post-1981496856222450939" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-21129445.post-5068773389660939042" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-21129445.post-6779029706562261496" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-8890204.post-7043570491844045663" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-786333285568106173.post-1026413759476501924" />
			<rdf:li rdf:resource="http://scottaaronson.com/blog/?p=289" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-3722233.post-1803801999677267283" />
			<rdf:li rdf:resource="urn:lj:livejournal.com:atom1:11011110:122872" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-21129445.post-7965628234354150147" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-3722233.post-7474713628728312297" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-786333285568106173.post-4523236087108089662" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-6555947.post-7585347013212777424" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-24557460.post-2854266465146635359" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-8890204.post-1712064760270605257" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-3722233.post-1058652655864746931" />
			<rdf:li rdf:resource="http://valis.cs.uiuc.edu/blog/?p=582" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-8890204.post-3006523453610323626" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-21129445.post-7738672841942859637" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-21129445.post-1119155832406315866" />
			<rdf:li rdf:resource="urn:lj:livejournal.com:atom1:11011110:122451" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-24557460.post-1332612161725071533" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-21129445.post-6626684414924792895" />
			<rdf:li rdf:resource="http://valis.cs.uiuc.edu/blog/?p=581" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-3722233.post-7898198581379740078" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-786333285568106173.post-5617705239701882774" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-19908808.post-2463145648836843366" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-8890204.post-427748167370258964" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-3722233.post-590068913766798483" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-21129445.post-1379588522591355591" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-21129445.post-6373976828242878807" />
			<rdf:li rdf:resource="urn:lj:livejournal.com:atom1:11011110:122234" />
			<rdf:li rdf:resource="http://scottaaronson.com/blog/?p=288" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-24557460.post-7598394102565279509" />
			<rdf:li rdf:resource="http://valis.cs.uiuc.edu/blog/?p=579" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-6555947.post-3226414513067858242" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-19908808.post-2103400202051567382" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-786333285568106173.post-2438490600907784895" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-786333285568106173.post-2313336436402789910" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-8890204.post-5433972626114909100" />
			<rdf:li rdf:resource="http://valis.cs.uiuc.edu/blog/?p=578" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-3722233.post-7923404146102863031" />
			<rdf:li rdf:resource="http://scottaaronson.com/blog/?p=287" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-21129445.post-201749644150278311" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-6555947.post-4183661568479832784" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-24557460.post-4632993069802503752" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-21129445.post-4258004195073149003" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-21129445.post-6542647513654459695" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-21129445.post-6015739328873969962" />
			<rdf:li rdf:resource="http://valis.cs.uiuc.edu/blog/?p=577" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-8890204.post-7279019469812918459" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-3722233.post-3692047415217655802" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-786333285568106173.post-907551030840605610" />
			<rdf:li rdf:resource="http://scottaaronson.com/blog/?p=285" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-24557460.post-1536275794160740147" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-3722233.post-7713603410258963272" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-21129445.post-2881972860195179604" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-21129445.post-2365602615433074246" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-21129445.post-5657519165989673226" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-8890204.post-2941888456432670699" />
			<rdf:li rdf:resource="tag:blogger.com,1999:blog-24557460.post-3045113359766124017" />
		</rdf:Seq>
	</items>
</channel>

<item rdf:about="tag:blogger.com,1999:blog-8890204.post-1981496856222450939">
	<title>My Biased Coin: David Parkes talk Monday, 4pm</title>
	<link>http://mybiasedcoin.blogspot.com/2007/11/david-parkes-talk-monday-4pm.html</link>
	<content:encoded>&lt;a href=&quot;http://www.eecs.harvard.edu/%7Eparkes/&quot;&gt;David Parkes&lt;/a&gt; will be giving a talk on &lt;a href=&quot;http://tools.fas.harvard.edu/cgi-bin/calendar/exporter.cgi?view=event_detail&amp;id=86076233&quot;&gt;computational mechanism design&lt;/a&gt; in Maxwell-Dworkin G115 at 4pm on the 19th.  This is what at Harvard we call the &quot;tenure talk&quot; -- it's a talk he gives giving an overview of his research and research area as he's coming up for tenure.  If you're a theorist in the area and want to hear a perhaps slightly different point of view on mechanism design and work at the intersection of economics/cs (David proves things, but he comes from the AI side), you should come.  Heck, if you're local, in any area, and interested in the economics/cs interface you should come.  It should be a very nice talk, and it would be great to have the room full to bursting as a sign of support of David.</content:encoded>
	<dc:date>2007-11-19T00:03:00+00:00</dc:date>
</item>
<item rdf:about="tag:blogger.com,1999:blog-21129445.post-5068773389660939042">
	<title>my slice of pizza: Le Balloon Rouge</title>
	<link>http://mysliceofpizza.blogspot.com/2007/11/le-balloon-rouge.html</link>
	<content:encoded>Fall days mean walk in the city with swirling leaves, flying hair, humans in scarves and coats, and children walking with their heads down, eyes averted. Some days I feel like an adult or world thrusts that upon me, and then there are days I feel like a boy or the world yanks the boy out of me. Today, I found a red balloon hobbling on the street, I picked it up and let it go, it flew, but followed me for an avenue block, so, I figured, like Lamorisse's hero, that it wants to with me, and I walked it home. Walking down the city blocks is a danger in itself some will tell you, walking with a red balloon in hand is even more so; I was glad when I managed to get home and shut the door behind safely. The red balloon, my treasure, is still holding its breath and hovering over me. Here is the media on the 1956 movie and its accidental enactment today.&lt;br /&gt;&lt;br /&gt;&lt;table border=&quot;0&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt; &lt;br /&gt;&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;&lt;br /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;table border=&quot;0&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot;&gt;&lt;tbody&gt;&lt;br /&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;&lt;br /&gt;&lt;a href=&quot;http://bp0.blogger.com/_P70CDtPH4Rs/Rz82134YBwI/AAAAAAAAAQg/aWlTT6U2a6I/s1600-h/photo.jpg&quot;&gt;&lt;img style=&quot;float:left; margin:0 0px 0px 0;cursor:pointer; cursor:hand;&quot; src=&quot;http://bp0.blogger.com/_P70CDtPH4Rs/Rz82134YBwI/AAAAAAAAAQg/aWlTT6U2a6I/s200/photo.jpg&quot; border=&quot;0&quot; alt=&quot;&quot; id=&quot;BLOGGER_PHOTO_ID_5133882399387027202&quot; /&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;&lt;br /&gt;&lt;a href=&quot;http://bp2.blogger.com/_P70CDtPH4Rs/Rz823X4YBxI/AAAAAAAAAQo/MkmgZvsU5ds/s1600-h/photo1.jpg&quot;&gt;&lt;img style=&quot;float:left; margin:0 0px 0px 0;cursor:pointer; cursor:hand;&quot; src=&quot;http://bp2.blogger.com/_P70CDtPH4Rs/Rz823X4YBxI/AAAAAAAAAQo/MkmgZvsU5ds/s200/photo1.jpg&quot; border=&quot;0&quot; alt=&quot;&quot; id=&quot;BLOGGER_PHOTO_ID_5133882425156830994&quot; /&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;&lt;br /&gt;&lt;a href=&quot;http://bp0.blogger.com/_P70CDtPH4Rs/Rz82334YByI/AAAAAAAAAQw/PjZKa8kVkl8/s1600-h/photo4.jpg&quot;&gt;&lt;img style=&quot;float:left; margin:0 0px 0px 0;cursor:pointer; cursor:hand;&quot; src=&quot;http://bp0.blogger.com/_P70CDtPH4Rs/Rz82334YByI/AAAAAAAAAQw/PjZKa8kVkl8/s200/photo4.jpg&quot; border=&quot;0&quot; alt=&quot;&quot; id=&quot;BLOGGER_PHOTO_ID_5133882433746765602&quot; /&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;&lt;br /&gt;&lt;a href=&quot;http://bp0.blogger.com/_P70CDtPH4Rs/Rz82434YB0I/AAAAAAAAARA/9vHa_aeYa6Q/s1600-h/photo2.jpg&quot;&gt;&lt;img style=&quot;float:left; margin:0 0px 0px 0;cursor:pointer; cursor:hand;&quot; src=&quot;http://bp0.blogger.com/_P70CDtPH4Rs/Rz82434YB0I/AAAAAAAAARA/9vHa_aeYa6Q/s200/photo2.jpg&quot; border=&quot;0&quot; alt=&quot;&quot; id=&quot;BLOGGER_PHOTO_ID_5133882450926634818&quot; /&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;&lt;br /&gt;&lt;a href=&quot;http://bp2.blogger.com/_P70CDtPH4Rs/Rz824X4YBzI/AAAAAAAAAQ4/lUPNcSyN934/s1600-h/photo3.jpg&quot;&gt;&lt;img style=&quot;float:left; margin:0 0px 0px 0;cursor:pointer; cursor:hand;&quot; src=&quot;http://bp2.blogger.com/_P70CDtPH4Rs/Rz824X4YBzI/AAAAAAAAAQ4/lUPNcSyN934/s200/photo3.jpg&quot; border=&quot;0&quot; alt=&quot;&quot; id=&quot;BLOGGER_PHOTO_ID_5133882442336700210&quot; /&gt;&lt;/a&gt;&lt;br /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;</content:encoded>
	<dc:date>2007-11-17T11:40:00+00:00</dc:date>
	<dc:creator>metoo</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-21129445.post-6779029706562261496">
	<title>my slice of pizza: A Past</title>
	<link>http://mysliceofpizza.blogspot.com/2007/11/past.html</link>
	<content:encoded>This is the season of conference deadlines (STOC/SIGMOD/SIGMETRICS and many others), holidays, and the surge and ebb of traditions. In a break from a tradition, I have put myself outside the process of conferences (and embraced the holidays), and am now willing myself towards the end of the year. I will be away in India for a week, and on reflecting, found myself thinking of my &lt;span style=&quot;font-style:italic;&quot;&gt;alma mate&lt;/span&gt;r, &lt;a href=&quot;http://www.iitkgp.ac.in/&quot;&gt;IIT Kharagpur&lt;/a&gt; (the least known of the IITs in Theory &lt;a href=&quot;http://www.cse-web.iitkgp.ernet.in/&quot;&gt;CS&lt;/a&gt;---there is always a linear combination in which one leads---but it is a &lt;a href=&quot;http://sheermelody.blogspot.com/2006/12/alma-mater.html&quot;&gt;gem&lt;/a&gt; of course). The CS dept is &lt;a href=&quot;http://cse.iitkgp.ac.in/silver/index.php?q=home&quot;&gt;celebrating&lt;/a&gt; its 25 years of existence this winter. I remembered the last time I visited the institute, it was 10 years ago, and googling yielded &lt;a href=&quot;http://www.iarcs.org.in/activities/newsletter/vol3-1/randwkshp.html&quot;&gt;this blurb&lt;/a&gt;  by &lt;a href=&quot;http://www.cs.chalmers.se/~dubhashi/&quot;&gt;Devdatt Dubhashi&lt;/a&gt;. This and other pasts often intrude in strange ways: watching a movie in a theater far away from Kharagpur, if someone yells &quot;Tarapodo!&quot;, the &lt;a href=&quot;http://sheermelody.blogspot.com/2006/12/alma-mater.html&quot;&gt;legend &lt;/a&gt;lives on; in a party in the village, I see Umesh and remember his fantastic &lt;a href=&quot;http://www.iarcs.org.in/activities/newsletter/vol3-1/randwkshp.html&quot;&gt;lecture&lt;/a&gt; at Kharagapur in 97.</content:encoded>
	<dc:date>2007-11-17T10:00:54+00:00</dc:date>
	<dc:creator>metoo</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-8890204.post-7043570491844045663">
	<title>My Biased Coin: Should IP issues affect our research?</title>
	<link>http://mybiasedcoin.blogspot.com/2007/11/should-ip-issues-affect-our-research.html</link>
	<content:encoded>Since I've done some legal consulting, I've become interested in  intellectual property (IP) issues, and the question of whether IP issues should affect my research agenda has recently occurred to me.&lt;br /&gt;&lt;br /&gt;Let me explain.  As an algorithmist, a major interest for me is designing algorithms that people will actually use -- perhaps not in the original form that I describe them in, of course, but that the ideas make it out into the real world.   And while it is not my aim that every project I'm involved in should have a clear potential practical impact, it is the case that this goal guides my overall research agenda quite significantly. &lt;br /&gt;&lt;br /&gt;IP issues have the potential to limit the practical impact of an algorithm I design.  For example, suppose someone has a patent on SuperHashSchemeX (SHSX).   (If you want something more specific, suppose Bloom filters were patented.)  I might come up to an improvement to SHSX, or a better analysis of SHSX, but if the patent is written well, the patent owner may have rights controlling who could use SHSX, including my improvement.   There may really be no hope that anyone else will actually be able to make use of my ideas, until the patent runs out, or unless the user is willing to buy a license.  Indeed, unless I go through the process of obtaining a patent, it's possible that my work might simply add value for the initial patent-holder, and I've actually rewarded those who are keeping my work from being used.  &lt;br /&gt;&lt;br /&gt;Perhaps this wouldn't bother me if my theory blood ran pure, but I like being an &quot;applied theorist&quot;.   In my mind, this affects the calculus I use in thinking about what I should work on.&lt;br /&gt;&lt;br /&gt;Should it?  Has anyone actually run into this problem?</content:encoded>
	<dc:date>2007-11-17T00:26:00+00:00</dc:date>
</item>
<item rdf:about="tag:blogger.com,1999:blog-786333285568106173.post-1026413759476501924">
	<title>WebDiarios de Motocicleta: Good Friday</title>
	<link>http://infoweekly.blogspot.com/2007/11/good-friday.html</link>
	<content:encoded>&lt;pre&gt;&lt;br /&gt;~/work% mkdir 4D&lt;br /&gt;~/work$ cp ~/bs/preamble.tex 4D/paper.tex&lt;br /&gt;~/work$ cp ~/bs/hacked-article.cls 4D/article.cls&lt;br /&gt;~/work$ cp ~/bs/hacked-size11.clo 4D/size11.clo&lt;br /&gt;&lt;/pre&gt;To: coauthor-1.  Sorry, I am a bit delayed due to another paper, and I still don't have a complete draft, but the paper is falling nicely into place. I promise to have a first draft out by Monday morning.&lt;br /&gt;&lt;br /&gt;TODO:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;title?&lt;/li&gt;&lt;li&gt;abstract&lt;/li&gt;&lt;li&gt;intro. Track down fractional cascading refs!&lt;br /&gt;&lt;/li&gt;&lt;li&gt;figure out complete list of coauthors&lt;/li&gt;&lt;li&gt;figure out how to analyze Bob's side of the rectangle (NB: we're losing smthg like a log or log^2 in the density of queries, is the intuition still true?)&lt;/li&gt;&lt;li&gt;technical section&lt;/li&gt;&lt;li&gt;make nice drawing with 2 coupled trees&lt;/li&gt;&lt;li&gt;does a 2-sided lb seem doable? Figure out whether to claim it &quot;in the full version&quot;.&lt;/li&gt;&lt;li&gt;announce social event Monday 4am, G575. Buy cookies for event.&lt;/li&gt;&lt;li&gt;confirm postdeadline beer @CBC, 8:30pm.&lt;/li&gt;&lt;/ul&gt;</content:encoded>
	<dc:date>2007-11-16T14:10:27+00:00</dc:date>
	<dc:creator>MiP</dc:creator>
</item>
<item rdf:about="http://scottaaronson.com/blog/?p=289">
	<title>Shtetl-Optimized: ‘Tis the week before deadline</title>
	<link>http://scottaaronson.com/blog/?p=289</link>
	<content:encoded>&lt;p&gt;and at least over here&lt;/p&gt;
&lt;p&gt;Not a blogger is stirring&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;http://research.microsoft.com/research/sv/STOC08/stoc2008-cfp.htm&quot;&gt;reason&lt;/a&gt; is clear&lt;/p&gt;</content:encoded>
	<dc:date>2007-11-15T11:37:22+00:00</dc:date>
</item>
<item rdf:about="tag:blogger.com,1999:blog-3722233.post-1803801999677267283">
	<title>Computational Complexity: More on the Tables Problem</title>
	<link>http://weblog.fortnow.com/2007/11/more-on-tables-problem.html</link>
	<content:encoded>Recall the tables problem, which I now state
more clearly than I did in my last post:

&lt;blockquote&gt;
n couples go to a resturant.
They will sit at a rectangular table
that has room for n on each side.
Each person sits either next to
across from their darling.
How many ways can they sit?
&lt;/blockquote&gt;

Most people got the correct answer in the comments
on my last blog. But some other interesting points
were raised that I will address.

&lt;ol&gt;
&lt;li&gt;
I had commented that this was asked on the 2007
&lt;a href=&quot;http://www.math.umd.edu/highschool/mathcomp/PartI2007.pdf&quot;&gt;
Maryland High School Math Olympiad, Part I,&lt;/a&gt;, problem 23,
which is  with Multiple Choice, for the case of n=5.
Some commenter wanted to know what the choices were:
360, 768, 5040, 19200, 30720.

&lt;li&gt;
Someone commented that the problem `was not new'
and gave &lt;a href=&quot;http://mathcircle.berkeley.edu/original/Combinatorics1.pdf&quot;&gt;this&lt;/a&gt;, problem 4,
as a pointer to where it had been asked before.
Here is the problem they were referring to:

&lt;blockquote&gt;
Define a domino to be a 1x2 rectangle.
In how many ways can a nx2 rectangle be tiled
by dominos?
&lt;/blockquote&gt;

This raises an interesting question:
When are two problems equivalent?
Does phrasing matter (I had people, they have dominos)?
Does making certain things distinguiable or not matter
(Dominos are not distinguiable, couples are)?
Does having different answers matter
(his is Fib(n+1) mine is a Fib(n+1) x 2^n x n!)?
More generally, its not clear when a problem is new.


&lt;li&gt;
Just to reiterate- there is math all around you if
you know where to look.

&lt;/li&gt;&lt;/li&gt;&lt;/li&gt;&lt;/ol&gt;</content:encoded>
	<dc:date>2007-11-15T11:28:12+00:00</dc:date>
	<dc:creator>GASARCH</dc:creator>
</item>
<item rdf:about="urn:lj:livejournal.com:atom1:11011110:122872">
	<title>0xDE: O'Rourke at Swarthmore</title>
	<link>http://11011110.livejournal.com/122872.html</link>
	<content:encoded>&lt;a href=&quot;http://www.sccs.swarthmore.edu/org/daily/2007/11/14/orourke-on-geometric-reductionism/&quot;&gt;O'Rourke on Geometric Reductionism&lt;/a&gt;. Student newspaper article by Neena Cherayil on a talk by Joe O'Rourke, “Geometric Folding Algorithms: Linkages, Origami, and Polyhedra,” including a description of the Houdini one-cut trick and segueing into a discussion of whether computational geometry is useful.</content:encoded>
	<dc:date>2007-11-14T17:19:45+00:00</dc:date>
</item>
<item rdf:about="tag:blogger.com,1999:blog-21129445.post-7965628234354150147">
	<title>my slice of pizza: Hot off email</title>
	<link>http://mysliceofpizza.blogspot.com/2007/11/hot-off-email.html</link>
	<content:encoded>&lt;a href=&quot;http://math.rejecta.org/&quot;&gt;&lt;img style=&quot;float:left; margin:0 10px 10px 0;cursor:pointer; cursor:hand;width: 200px;&quot; src=&quot;http://math.rejecta.org/files/barron_logo.gif&quot; border=&quot;0&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;br /&gt;I really shouldn't publicize getting an email with the Call For Papers for a new journal: &lt;a href=&quot;http://math.rejecta.org/&quot;&gt;Rejecta Mathematica&lt;/a&gt;. As the website says: &lt;blockquote&gt;&lt;/blockquote&gt;&lt;span style=&quot;font-style:italic;&quot;&gt;Rejecta Mathematica is a new, open access, online journal that publishes only papers that have been rejected from peer-reviewed journals (or conferences with comparable review standards) in the mathematical sciences.&lt;/span&gt;&lt;blockquote&gt;&lt;/blockquote&gt; Further, the website clarifies that the journal is interested in forms of &quot;&lt;span style=&quot;font-style:italic;&quot;&gt;mapping the blind alleys of science, reinventing the wheel, applications of cold fusion, misunderstood genius&lt;/span&gt;&quot; etc. Enjoy!&lt;br /&gt;&lt;br /&gt;Btw, you can imagine what an email invitation would say: &quot;...in light of your reputation in the research community, we would like to extend a specific invitation to you to contribute a paper...&quot; :) The website above counsels authors whose submissions to this journal get rejected!&lt;br /&gt;&lt;br /&gt;ps: Mike Wakin, Chris Rozell, Mark Davenport and Jason Laska are involved. All are signal processors.</content:encoded>
	<dc:date>2007-11-14T14:38:40+00:00</dc:date>
	<dc:creator>metoo</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-3722233.post-7474713628728312297">
	<title>Computational Complexity: Math Problems from everyday life</title>
	<link>http://weblog.fortnow.com/2007/11/math-problems-from-everyday-life.html</link>
	<content:encoded>Often I see something in real life
that inspires a math problem. Could be a math
problem for an exam or a student project
or (more rarely) serious research.
(e.g., I give my 9 year old great nephew seven crayons
and he colors the numbers 1,...,2000 without
any monochromatic 3-AP's so I get a new VDW
number out of it.)

&lt;br /&gt;
&lt;br /&gt;

Here is one that inspired a problem that
ended up on the Maryland Math Olympiad,
Part I (which is 25 multiple choices questions).
(5 choices, 4 points for a correct answer,
2 points for a wrong answer. You really really
do not want to guess.)

&lt;br /&gt;
&lt;br /&gt;

Here is what happened: I went to dinner
with my darling, and my two sister-in-laws
and their husbands.
I sat across from my darling but the
other couples sat next to each other.
So here is the question:
I immediately thought about the following
question:

&lt;blockquote&gt;
$n$ couples go to dinner. They sit at
a rectangular table, but nobody sits at
the ends.  Each couple either sits
ACROSS FROM or NEXT TO their darling.
How many ways can they be seated?
&lt;/blockquote&gt;

The problem on the exam was asked for 5 couples
and gave choices.
&lt;br /&gt;
&lt;br /&gt;

Its not a hard problem for the readers of this blog, so I leave it to
my commenters to solve it.  (If nobody does I'll post the solution later.)
Note that it would be hard for a high school student-
very few got it correct. (We suspected this would be the case. We try to order
the questions by difficulty and this was question 23.)</content:encoded>
	<dc:date>2007-11-14T08:53:13+00:00</dc:date>
	<dc:creator>GASARCH</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-786333285568106173.post-4523236087108089662">
	<title>WebDiarios de Motocicleta: Is Google paying you?</title>
	<link>http://infoweekly.blogspot.com/2007/11/is-google-paying-you.html</link>
	<content:encoded>Kamal Jain gave a talk today on &quot;atomic economics&quot;. Here is my own description of what he meant, though it seemed Kamal disagrees with at least some of my description :)&lt;br /&gt;&lt;br /&gt;Think of credit cards. Whenever I pay with one, the store only gets 1-x% of the posted price, i.e. the the credit card company makes x% of the money. That is nice profit for the CC company, but as always, competition is driving all profits towards zero. Since there are multiple banks willing to give me a CC, &lt;span style=&quot;font-style: italic;&quot;&gt;and they all retain the same percentage from a merchant&lt;/span&gt;, they need to compete on the customer side --- they have to fight for the honor of counting me as their customer. So they start giving me points / miles / cash back / etc. Getting 1% of your money back is already quite standard, and customers need more to be impressed. To some extent, this incentivizing actually works (personally, I tend to use my AmEx card due to the better reward system).&lt;br /&gt;&lt;br /&gt;Now, Google and Yahoo are making money because of me (merchants pay them when I click on an ad). In principle, this means we are not at an equilibrium: since these companies are competing for customers (equal ad revenue), one of them should start offering me a share of their profits whenever I click, and the other will have to follow suit.&lt;br /&gt;&lt;br /&gt;Kamal sees several problems (market inefficiencies) that will prevent this welfare-maximizing scenario. I also see them as problems, but I have a different opinion on the degree of surmontability. Problems:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Customer abuse: profit sharing has to be tied to some actual buying event, or otherwise I will have an incentive to spend my day clicking on ads (and not buying anything), driving the merchant out of business. This is a problem for branding ads (e.g. eBay likes to display worthless ads all the time to build its name into collective conscience), and for ads that are separated from the actual purchase (e.g. an ad to a restaurant). But for e-commerce, which seems to be the raison d'etre of search advertising, this detail can be worked out by sharing information between Google and the merchant.&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;&lt;li&gt;Merchant fraud: the merchant marks the price up by $5, adds $5 to the advertising budget (getting a top slot), and tells me that I will get $5 cash-back from Google (which means I don't care their price was $5 higher than the competition). Realistically, this won't happen, for several well-understood reasons. Common sense dictates that Google will only share a percentage of their profit, so I'll only get, say, $2.5 back, which means I care about the price markup. Like credit-card companies, Google and Yahoo have a weapon to enforce this common sense: customer differentiation. This is oligopolistic behavior, but one that is remarkably stable. What happens is that frequent buyers (resp. people with good credit histories) get better cash-bask deals, because they are presumed to have more authority to &quot;negotiate&quot;.  Then, the merchant gets a mix of people with different cash-back percentages, but who look the same to the merchant.&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;&lt;li&gt;Bounded rationality, a heavily used buzzword, and the really exciting consideration. Unlike traditional advertising, Google and Yahoo have really tiny costs. This means they can still make a ton of money with a razor-thin cut of the merchant's profit. For instance, if Google is paid 0.1% percent of the transaction value, what could they offer me as an incentive? Maybe 0.05% of the price. But that will be irrelevant to me -- the mental cost of remembering that Google gives me more cash back on lingerie gifts vastly outweighs the 0.05% payoff. &lt;br /&gt;&lt;/li&gt;&lt;/ul&gt;Kamal's point is that the last phenomenon is happening more and more often, as the economy moves to an atomic scale, making the relative cost of rationality in each decision increase. He draws a funny parallel to the Heisenberg uncertainty principle, which IMHO only goes as far as &quot;on a small enough scale, things become funky&quot;.&lt;br /&gt;&lt;br /&gt;The market inefficiency generated by the small scale of Web transactions is great for Web-based companies like Google, since it means they can make huge profits below the radar of competition forces. The sad news (for a scientist) is that it all becomes a social competition, instead of a scientific/economic one. It's much more important if I, the customer, like Google's colors than if I'm getting my fair share of cash back.&lt;br /&gt;&lt;br /&gt;On a larger scale, is this a social problem? I guess so, but not a very big one. If our threshold for  irrationality is below 1%, the total income of such companies cannot grow above 1% of the money we all spend. But we're already paying the Federal government some 30% of what we make, and quite possibly many of us are happier with the success of Gmail than the hearts-and-minds campaign in Iraq.&lt;br /&gt;&lt;br /&gt;PS: Given the topic, the number of jokes and anecdotes in Kamal's talk was too small by an order of magnitude. This reinforces my belief that we shouldn't be doing this kind of thing (i.e. study pure economics, because it is has something to do with networks or computers). If such pursuits will be more than wasted theoretical brain-cycles, they need to impact managers and policy-makers. But for that to happen, we need to give talks at the level of political speeches, and no self-respecting theorist is willing to do that. People in economics (who incidentally serve on policy-making boards every now and then) have already developed the art of communicating to politicians.</content:encoded>
	<dc:date>2007-11-13T21:17:09+00:00</dc:date>
	<dc:creator>MiP</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-6555947.post-7585347013212777424">
	<title>The Geomblog: A day in the life...</title>
	<link>http://geomblog.blogspot.com/2007/11/day-in-life.html</link>
	<content:encoded>Here's a list of things I did today:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;I taught one lecture of my class&lt;/li&gt;&lt;li&gt;I attended a departmental committee meeting&lt;/li&gt;&lt;li&gt;I had a meeting with collaborators to work out a paper outline for something we're submitting&lt;/li&gt;&lt;li&gt;I met with my student and did some nontechnical advising&lt;/li&gt;&lt;li&gt;I had  a (brief) discussion with a collaborator about a pending grant proposal.&lt;/li&gt;&lt;/ul&gt;In other words, I covered the entire gamut of activities one might expect of an academic - with one significant exception. Did you notice it ?&lt;br /&gt;&lt;br /&gt;Nowhere in there was any actual research done ! Gaaaah !&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~f/TheGeomblog?a=zJ6j7WB&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~f/TheGeomblog?i=zJ6j7WB&quot; border=&quot;0&quot; /&gt;&lt;/a&gt;
&lt;/div&gt;</content:encoded>
	<dc:date>2007-11-13T02:52:26+00:00</dc:date>
	<dc:creator>Suresh</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-24557460.post-2854266465146635359">
	<title>in theory: Impagliazzo Hard-Core Sets via &quot;Finitary Ergodic-Theory&quot;</title>
	<link>http://in-theory.blogspot.com/2007/11/impagliazzo-hard-core-sets-via-finitary.html</link>
	<content:encoded>In the Impagliazzo hard-core set theorem we are a given a function $g:\{ 0, 1 \}^n \rightarrow \{ 0,1\}$ such that every algorithm in a certain class makes errors at least a $\delta$ fraction of the times when given a random input. We think of $\delta$ as small, and so of $g$ as exhibiting a weak form of average-case complexity. We want to find a large set $H\subseteq \{ 0,1 \}^n$ such  that $g$ is average-case hard in a stronger sense when restricted to $H$. This stronger form of average-case complexity will be that no efficient algorithm can make noticeably fewer errors while computing $g$ on $H$ than a trivial algorithm that always outputs the same value regardless of the input. The formal statement of what we are trying to do (see also the discussion in &lt;a href=&quot;http://in-theory.blogspot.com/2007/11/impagliazzo-hard-core-set-theorem.html&quot;&gt;this previous post&lt;/a&gt;) is:&lt;br /&gt;&lt;br /&gt;&lt;blockquote&gt;&lt;br /&gt;&lt;b&gt;Impagliazzo Hard-Core Set Theorem, &quot;Constructive Version&quot;&lt;/b&gt;&lt;br /&gt;Let $g:\{0,1\}^n \rightarrow \{0,1\}$ be a boolean function, $s$ be a size parameter, $\epsilon,\delta&gt;0$ be given. Then there is a size parameter $s' = poly(1/\epsilon,1/\delta) \cdot s +  exp(poly(1/\epsilon,1/\delta))$ such that the following happens.&lt;br /&gt;&lt;br /&gt;Suppose that for every function $f:\{0,1\}^n \rightarrow \{0,1\}$ computable by a circuit of size $s'$ we have&lt;br /&gt;&lt;br /&gt;$Pr_{x \in \{0,1\}^n} [ f(x) = g(x) ] \leq 1-\delta$&lt;br /&gt;&lt;br /&gt;Then there is a set $H$ such that: (i) $H$ is recognizable by circuits of size $\leq s'$; (ii) $|H| \geq \delta 2^n$, and in fact the number of $x$ in $H$ such that $g(x)=0$ is at least $\frac 12 \delta 2^n$, and so is the number of $x$ in $H$ such that $g(x)=1$; and (iii) for every $f$ computable by a circuit of size $\leq s$,&lt;br /&gt;&lt;br /&gt;$Pr_{x\in H} [ g(x) = f(x) ] \leq max \{ Pr_{x\in H}[ g(x) = 0] , Pr_{x\in H} [g(x)=1] \} + \epsilon$&lt;br /&gt;&lt;/blockquote&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Our approach will be to look for a &quot;regular partition&quot; of $\{0,1\}^n$. We shall construct a partition $P= (B_1,\ldots,B_m)$ of $\{0,1\}^n$ such that: (i) given $x$, we can efficiently compute what is the block $B_i$ that $x$ belongs to; (ii) the number $m$ of blocks does not depend on $n$; (iii) $g$ restricted to most blocks $B_i$ behaves like a random function of the same density. (By &quot;density&quot; of a function we mean the fraction of inputs on which the function evaluates to one.)&lt;br /&gt;&lt;br /&gt;In particular, we will use the following form of (iii): for almost all the blocks $B_i$, no algorithm has advantage more than $\epsilon$ over a constant predictor in computing $g$ in $B_i$.&lt;br /&gt;&lt;br /&gt;Let $M_0$ be the union of all majority-0 blocks (that is, of blocks $B_i$ such that $g$ takes the value 0 on a majority of elements of $B_i$) and let $M_1$ be the union of all majority-1 blocks.&lt;br /&gt;&lt;br /&gt;I want to claim that no algorithm can do noticeably better on $M_0$ than the constant algorithm that always outputs 0. Indeed, we know that within (almost) all of the blocks that compose $M_0$ no algorithm can do noticeably better than the always-0 algorithm, so this must be true for a stronger reason for the union. The same is true for $M_1$, with reference to the constant algorithm that always outputs 1. Also, if the partition is efficiently computable, then(in a non-uniform setting) $M_0$ and $M_1$ are efficiently recognizable. It remains to argue that either $M_0$ or $M_1$ is large and not completely unbalanced.&lt;br /&gt;&lt;br /&gt;Recalling that we are in a non-uniform setting (where by &quot;algorithms&quot; we mean &quot;circuits&quot;) and that the partition is efficiently computable, the following is a well defined efficient algorithm for attempting to compute $g$:&lt;br /&gt;&lt;br /&gt;&lt;blockquote&gt;&lt;br /&gt;&lt;b&gt;Algorithm. Local Majority&lt;/b&gt;&lt;br /&gt;On input $x$:&lt;br /&gt;determine the block $B_i$ that $x$ belongs to;&lt;br /&gt;output $1$ if $Pr_{z\in B_i} [g(z)=1] \geq \frac 12$; &lt;br /&gt;otherwise output $0$&lt;br /&gt;&lt;/blockquote&gt;&lt;br /&gt;&lt;br /&gt;(The majority values of $g$ in the various blocks are just a set of $m$ bits that can be hard-wired into the circuit.)&lt;br /&gt;&lt;br /&gt;We assumed that every efficient algorithm must make at least a $\delta$ fraction of errors. The set of $\geq \delta 2^n$ inputs where the Local Majority algorithm makes mistakes is the union, over all blocks $B_i$, of the &quot;minority inputs&quot; of the block $B_i$. (If $b$ is the majority value of $g$ in a block $B$, then the &quot;minority inputs&quot; of $B$ are the set of inputs $x$ such that $g(x) = 1-b$.)&lt;br /&gt;&lt;br /&gt;Let $E_0$ be the set of minority inputs (those where our algorithm makes a mistake) in $M_0$ and $E_1$ be the set of minority  inputs in $M_1$. Then at least one of $E_0$ and $E_1$ must have size at least $\frac {\delta}{2} 2^n$, because the size of their union is at least $\delta 2^n$. If $E_b$ has size at least $\frac {\delta}{2} 2^n$, then $M_b$ has all the properties of the set $H$ we are looking for.&lt;br /&gt;&lt;br /&gt;It remains to construct the partition. We describe an iterative process to construct it. We begin with the trivial partition $P = (B_1)$ where $B_1 = \{ 0,1\}^n$. At a generic step of the construction, we have a partition $P = (B_1,\ldots,B_m)$, and we consider $M_0, M_1,E_0,E_1$ as above. Let $b$ be such that $E_b \geq \frac 12 \delta 2^n$. If there is no algorithm that has noticeable advantage in computing $g$ over $M_b$, we are done. Otherwise, if there is such an algorithm $f$, we refine the partition by splitting each block according to the values that $f$ takes on the elements of the block. &lt;br /&gt;&lt;br /&gt;After $k$ steps of this process, the partition has the following form: there are $k$ functions $f_1,\ldots,f_k$ and each of the (at most) $2^k$ blocks of the partition corresponds to a bit string $b_1,\ldots,b_k$ and it contains all inputs $x$ such that $f_1(x)=b_1,\ldots,f_k(x)=b_k$. In particular, the partition is efficiently computable.&lt;br /&gt;&lt;br /&gt;We need to argue that this process terminates with $k=poly(1/\epsilon,1/\delta)$. To this end, we define a potential function that measures the &quot;imbalance&quot; of $g$ inside the blocks the partition &lt;br /&gt;&lt;br /&gt;$\Psi(B_1,\ldots,B_m) := \sum_{i=1}^m \frac {|B_i|}{2^n} \left( Pr_{x\in B_i} [g(x) = 1] \right)^2 $&lt;br /&gt;&lt;br /&gt;and we can show that this potential function increases by at least $poly(\epsilon,\delta)$ at each step of the iteration. Since the potential function can be at most 1, the bound on the number of iterations follows.&lt;br /&gt;&lt;br /&gt;A reader familiar with the proof of the Szemeredi Regularity Lemma will recognize the main ideas of iterative partitioning, of using a &quot;counterexample&quot; to the regularity property required of the final partition to do a refinement step, and of using a potential function argument to bound the number of refinement steps. &lt;br /&gt;&lt;br /&gt;In which way can we see them as &quot;finitary ergodic theoretic&quot; techniques? As somebody who does not know anything about ergodic theory, I may not be in an ideal position to answer this question. But this kind of difficulty has not stopped me before, so I may attempt to answer this question in a future post.</content:encoded>
	<dc:date>2007-11-12T20:53:00+00:00</dc:date>
</item>
<item rdf:about="tag:blogger.com,1999:blog-8890204.post-1712064760270605257">
	<title>My Biased Coin: An Experiment in Viral Marketing : Buying Tickets Online</title>
	<link>http://mybiasedcoin.blogspot.com/2007/11/experiment-in-viral-marketing-buying.html</link>
	<content:encoded>(Warning:  the following post is highly crass and commercial.  If that offends you in any way, please don't read on.)&lt;br /&gt;&lt;br /&gt;Thanks to some friends who are working in Web marketing, I have a chance to try a viral marketing experiment right here on the blog.  I have been give some discount codes for a new ticket site -- &lt;a href=&quot;http://ticketheroes.com/&quot;&gt;ticketheroes.com&lt;/a&gt; -- which offers tickets for sports/concerts/other events.  Anyone can use them, at least until 1/1/08, so you can use them, or tell a friend to use them, or have a friend tell a friend, or whatever.   According to that whole 6 degrees of separation thing, it's possible that &lt;span style=&quot;font-style: italic;&quot;&gt;everyone in the world&lt;/span&gt; could learn my discount codes, which by the way are:&lt;br /&gt;&lt;br /&gt;MMBLOG1 [$5 off any ticket purchase]&lt;br /&gt;MMBLOG2 [5% off any ticket purchase]&lt;br /&gt;[Yes, you can only use one of them...]&lt;br /&gt;&lt;br /&gt;To be clear, I am officially acting as a consultant for the SEM company that is trying to drive traffic to TicketHeroes.  I &lt;span style=&quot;font-style: italic;&quot;&gt;am not&lt;/span&gt; getting a direct cut back from sales using these codes.  (Though if this experiment is hugely successful, perhaps I'll try to negotiate one!)  But I will get back data -- I'll get to see how many sales used the codes.  (Don't worry, &lt;span style=&quot;font-style: italic;&quot;&gt;I&lt;/span&gt; won't get your credit card number.)  I find this all very interesting.  Can this little blog, focusing primarily on academic topics, help sell things?  Really?  I kind of doubt it, but hey, I remember thinking around the year 1999 that it would be a really impressive thing if Google became a 1 billion dollar company, so clearly I don't know as much as I should about marketing.  &lt;br /&gt;&lt;br /&gt;I've done my own checks and the site's prices look to be slightly less than or the same as other similar sites, even without the discount.   So this should be a win for anyone looking for tickets, although of course you shouldn't trust me, you should compare.&lt;br /&gt;&lt;br /&gt;Next post, we'll return to less crass, commercial topics, like how to re-finance your mortgage.</content:encoded>
	<dc:date>2007-11-12T20:44:00+00:00</dc:date>
</item>
<item rdf:about="tag:blogger.com,1999:blog-3722233.post-1058652655864746931">
	<title>Computational Complexity: COMPUTATIONAL COMPLEXITY CONF 2008 SUBMISSIONS WEBSITE OPEN!</title>
	<link>http://weblog.fortnow.com/2007/11/computational-complexity-conf-2008_12.html</link>
	<content:encoded>Computational Complexity Conference 2008 (CCC 2008)
submissions website is now open:
&lt;a href=&quot;http://ccc08.cs.washington.edu&quot;&gt;go here for submissions
website&lt;/a&gt;
or go
&lt;a href=&quot;http://facweb.cs.depaul.edu/jrogers/complexity/&quot;&gt;here
for more info on the conference&lt;/a&gt;

&lt;ol&gt;
&lt;li&gt;
Submission deadline is Dec 6, 2007, 17:59 PST
&lt;li&gt;
Notification will be by Feb 8, 2008.
&lt;li&gt;
Conference will be in College Park Maryland
(details on that will be on the conference website
soon)
&lt;li&gt;
It will be &lt;b&gt;awesome!&lt;/b&gt;
&lt;/li&gt;&lt;/li&gt;&lt;/li&gt;&lt;/li&gt;&lt;/ol&gt;

&lt;br /&gt;
&lt;br /&gt;

Trivial question: Name everyone who has been to every single COMPLEXITY conference,
including when it as called STRUCTURES?
(I've been to all but one, Jack Lutz has been to all but two,
so we do not qualify.)</content:encoded>
	<dc:date>2007-11-12T08:56:20+00:00</dc:date>
	<dc:creator>GASARCH</dc:creator>
</item>
<item rdf:about="http://valis.cs.uiuc.edu/blog/?p=582">
	<title>Vanity of Vanities, all is Vanity: The igTurn award?</title>
	<link>http://valis.cs.uiuc.edu/blog/?p=582</link>
	<content:encoded>&lt;p&gt;So, there is no nobel prize in CS, but there is the &amp;#8220;equivalent&amp;#8221; Turing award. But we still dont have an equivalent to the ig Nobel prize. I suggest to establish a new prize to this effect named igTurn. As close as the winners are going to get to the Turing award (distance measured in edit distance, naturally).&lt;/p&gt;</content:encoded>
	<dc:date>2007-11-12T02:03:32+00:00</dc:date>
</item>
<item rdf:about="tag:blogger.com,1999:blog-8890204.post-3006523453610323626">
	<title>My Biased Coin: Koblitz follow-up</title>
	<link>http://mybiasedcoin.blogspot.com/2007/11/koblitz-follow-up.html</link>
	<content:encoded>&lt;a href=&quot;http://in-theory.blogspot.com/2007/11/december-issue-of-notices-of-ams.html&quot;&gt;Luca points to&lt;/a&gt; the &lt;a href=&quot;http://www.ams.org/notices/200711/&quot;&gt;letters to the editor&lt;/a&gt; (and Koblitz's response) regarding &lt;a href=&quot;http://mybiasedcoin.blogspot.com/2007/08/how-mathematicians-view-computer.html&quot;&gt;the Koblitz affair&lt;/a&gt;.</content:encoded>
	<dc:date>2007-11-10T22:15:00+00:00</dc:date>
</item>
<item rdf:about="tag:blogger.com,1999:blog-21129445.post-7738672841942859637">
	<title>my slice of pizza: IKEA</title>
	<link>http://mysliceofpizza.blogspot.com/2007/11/ikea.html</link>
	<content:encoded>&lt;a href=&quot;http://www.freespiritart.com/kokanee-salmon.php&quot;&gt;&lt;img style=&quot;float:left; margin:0 10px 10px 0;cursor:pointer; cursor:hand; height:200px; width: 250px;&quot; src=&quot;http://www.freespiritart.com/images/kokaneesalmon.jpg&quot; border=&quot;0&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;I went to IKEA, an European equivalent of McDonalds (yes, $0.99 cheesburgers, $9.99 lava lamps). I know retailing is a difficult business and I not knocking their hard work. But retailing food is a harder business. I am mainly talking about the Salmon, the poor specimen that was born in captivity, lived within a large mesh I am sure, and eventually made its way, via the steam route, to one's plate: the piece was soggy,  served on a plate that was poorly washed, and accompanied by soggier vegetables; the patrons look at the plate without appetite, use the fork to cut it to pieces, move it around and play hockey, all the while talking, eating fries and drinking. I felt the need to rush home, paint a large, healthy, lively Salmon, and hang it somewhere in homage.</content:encoded>
	<dc:date>2007-11-10T16:15:53+00:00</dc:date>
	<dc:creator>metoo</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-21129445.post-1119155832406315866">
	<title>my slice of pizza: Sponsored search market index</title>
	<link>http://mysliceofpizza.blogspot.com/2007/11/sponsored-search-market-index.html</link>
	<content:encoded>&lt;a href=&quot;http://www.cs.umass.edu/%7Emicah/&quot;&gt;Micah Adler&lt;/a&gt; visited and it helped me recall shared experiences working on problems in a different world and context (parallel algorithms, scheduling problems, and a rope burning puzzle from long ago). Of course both of us had a life within a different context now, and so, we discussed a different set of problems.&lt;br /&gt;&lt;br /&gt;In particular,  I got a chance to pose a problem I think of as a big picture question of some larger relevance (I am not an expert on markets, and this may not be such an interesting/challenging problem, but those disclaimers aside, in any case, here it is): Much like &lt;a href=&quot;http://en.wikipedia.org/wiki/Stock_market_index&quot;&gt;indices&lt;/a&gt; for tracking the market for various materials/stocks, can we design one for tracking the market of sponsored search ads? Definitions aside, the engineering part of collecting relevant data may be fun too.&lt;br /&gt;&lt;br /&gt;ps: &lt;a href=&quot;http://intermexfreemarket.blogspot.com/2007/11/consultant-in-chief-what-mitt-romney.html&quot;&gt;Mitt Romney: I Love Data&lt;/a&gt;.</content:encoded>
	<dc:date>2007-11-10T11:00:54+00:00</dc:date>
	<dc:creator>metoo</dc:creator>
</item>
<item rdf:about="urn:lj:livejournal.com:atom1:11011110:122451">
	<title>0xDE: Halloween photos</title>
	<link>http://11011110.livejournal.com/122451.html</link>
	<content:encoded>Nowhere near as many photos from Halloween as usual this year, mostly because the poor air quality from all the wildfires led to all the local neighborhood parties being canceled (funny, that didn't happen in 2003 when the smoke was as bad). But I did take a few snapshots of my kids and their pumpkins. I doubt anyone but me and my family cares about these ones, but &lt;a href=&quot;http://www.ics.uci.edu/~eppstein/pix/halloween07/index.html&quot;&gt;here they are&lt;/a&gt;. Note Timothy's Aang pumpkin: he's an Avatar fan (as am I). We tried using little muffin shaped electric lights in place of the candles in the pumpkins — the pumpkin itself warms the color of the light so it doesn't look very different, but lasts better and is less annoying to set up.</content:encoded>
	<dc:date>2007-11-10T07:40:48+00:00</dc:date>
</item>
<item rdf:about="tag:blogger.com,1999:blog-24557460.post-1332612161725071533">
	<title>in theory: The December Issue of the Notices of the AMS</title>
	<link>http://in-theory.blogspot.com/2007/11/december-issue-of-notices-of-ams.html</link>
	<content:encoded>The &lt;a href=&quot;http://www.ams.org/notices/200711/&quot;&gt;December issue&lt;/a&gt; of the Notices of the AMS is now available online, and it includes letters written by Oded Goldreich, Boaz Barak, Jonathan Katz, and Hugo Krawczyk in response to Neal Koblitz's article which appeared in the &lt;a href=&quot;http://in-theory.blogspot.com/2007/08/swift-boating-of-modern-cryptography.html&quot;&gt;September issue&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Despite this, the readers of the Notices remain the losers in this &quot;controversy.&quot; Koblitz's petty personal attacks and straw man arguments appeared in the same space that is usually reserved, in the Notices, for expository articles and obituaries of mathematicians. It is from those pages that I learned about the &lt;a href=&quot;http://www.ams.org/notices/200103/fea-tao.pdf&quot;&gt;Kakeya problem&lt;/a&gt; and about the &lt;a href=&quot;http://www.ams.org/notices/200409/fea-grothendieck-part1.pdf&quot;&gt;life&lt;/a&gt; of &lt;a href=&quot;http://www.ams.org/notices/200410/fea-grothendieck-part2.pdf&quot;&gt;Grothendieck&lt;/a&gt; (who, I should clarify, is not dead, except perhaps in Erdos' use of the word).&lt;br /&gt;&lt;br /&gt;I find it strange enough that Koblitz would submit his piece to such a venue, but I find it as mind-boggling that the editors would run his piece as if they had commissioned Grothendieck's biographical article to a disgruntled ex-lover, who would focus most of the article on fabricated claims about his personal hygiene.&lt;br /&gt;&lt;br /&gt;I can only hope that the editors will soon run on those pages one or more expository articles on modern cryptography, not as rebuttals to Koblitz's piece (which has already been discussed more than enough), but as a service to the readers.&lt;br /&gt;&lt;br /&gt;And while I am on the subject of Notices article, let me move on to &lt;a href=&quot;http://www.ams.org/notices/200711/tx071101507p.pdf&quot;&gt;this article&lt;/a&gt; on how to write papers.&lt;br /&gt;&lt;br /&gt;All beginning graduate students find the process of doing research mystifying, and I do remember feeling that way. (Not that such feelings have changed much in the intervening years.) One begins with a sense of hopelessness, &lt;i&gt;how am I going to solve a problem that people who know much more than I do and who are smarter than me have not been able to solve?&lt;/i&gt;; then a breakthrough comes, out of nowhere, and one wonders, how is this ever going to happen &lt;i&gt;again&lt;/i&gt;? Finally it's time to write up the results, and mathematical proofs definitely don't write themselves, not to mention coherent and compelling introductory sections. I think it's great when more experienced scholars take time to write advice pieces that can help students navigate these difficulties. And the number of atrociously badly written papers in circulation suggests that such pieces are good not just for students, but for many other scholars as well.&lt;br /&gt;&lt;br /&gt;But I find that advice on &quot;how to publish,&quot; rather than &quot;how to write well&quot; (like advice on &quot;how to get a job&quot; rather than &quot;how to do research&quot;) misses the point (I am thinking of one of the few times I thought Lance Fortnow gave &lt;a href=&quot;http://weblog.fortnow.com/2004/06/dont-make-it-too-easy-or-too-much.html&quot;&gt;bad advice&lt;/a&gt;). For this reason, I found the first section of the Notices article jarring, and the following line (even if it was meant as a joke) made me cringe&lt;br /&gt;&lt;blockquote&gt;&lt;br /&gt;I have written more than 150 articles myself. (...) I have never written an article and then been unable to publish it.&lt;br /&gt;&lt;/blockquote&gt;&lt;br /&gt;I think that this calls for an &lt;a href=&quot;http://scottaaronson.com/blog/?p=40&quot;&gt;Umeshism&lt;/a&gt; in response.</content:encoded>
	<dc:date>2007-11-09T21:53:00+00:00</dc:date>
</item>
<item rdf:about="tag:blogger.com,1999:blog-21129445.post-6626684414924792895">
	<title>my slice of pizza: Brooklyn Blues</title>
	<link>http://mysliceofpizza.blogspot.com/2007/11/brooklyn-blues.html</link>
	<content:encoded>&lt;a href=&quot;http://profile.myspace.com/index.cfm?fuseaction=user.viewprofile&amp;friendID=69633190&quot;&gt;&lt;img style=&quot;float:left; margin:0 10px 10px 0;cursor:pointer; cursor:hand; height:200px; width: 150px;&quot; src=&quot;http://a985.ac-images.myspacecdn.com/images01/18/m_605aadfd52fa7564da388734e4db3990.jpg&quot; border=&quot;0&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;br /&gt;What will move me several blocks away from my aptment, over the river and into Brooklyn? The Blues, beats and Jazz. Fridays and Saturdays, free and sweet at BAM, to be bottled at the moment, at the source, and sipped right then and later. Today, &lt;a href=&quot;http://www.likewatermusic.com/html/itinerary.php&quot;&gt;Chanda Rule&lt;/a&gt; performed and freed the many who had gathered there after the long week.</content:encoded>
	<dc:date>2007-11-09T20:18:07+00:00</dc:date>
	<dc:creator>metoo</dc:creator>
</item>
<item rdf:about="http://valis.cs.uiuc.edu/blog/?p=581">
	<title>Vanity of Vanities, all is Vanity: Now you know…</title>
	<link>http://valis.cs.uiuc.edu/blog/?p=581</link>
	<content:encoded>&lt;p&gt;Why does the zebra wears pajamas? &lt;a href=&quot;http://youtube.com/watch?v=HO-4NAgESns&quot;&gt;The answer&lt;/a&gt;.&lt;/p&gt;</content:encoded>
	<dc:date>2007-11-09T16:21:28+00:00</dc:date>
</item>
<item rdf:about="tag:blogger.com,1999:blog-3722233.post-7898198581379740078">
	<title>Computational Complexity: Richard Beigel is at NSF</title>
	<link>http://weblog.fortnow.com/2007/11/richard-beigel-is-at-nsf.html</link>
	<content:encoded>Richard Beigel is now one of five Program Director at
NSF for CISE/CCS/TF (TF= Theoretical Foundations)
This is the job previously held by William Steiger
(who will stay on part time for two months, but is
already back at Rutgers.)
 
&lt;br /&gt;
&lt;br /&gt;
 
Lets all wish him well on the job- if he does well,
we do well.</content:encoded>
	<dc:date>2007-11-09T14:17:58+00:00</dc:date>
	<dc:creator>GASARCH</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-786333285568106173.post-5617705239701882774">
	<title>WebDiarios de Motocicleta: Branding and STOC/FOCS</title>
	<link>http://infoweekly.blogspot.com/2007/11/branding-and-stocfocs.html</link>
	<content:encoded>&lt;a href=&quot;http://weblog.fortnow.com/2007/11/resubmitting-rejected-focs-paper-to.html&quot;&gt;Kamal Jain&lt;/a&gt; gives some advice on resubmitting papers that got rejected from STOC/FOCS. That reminds me of a question I've had for while: can you build up a name by only submitting papers in the right places (i.e. where they get accepted)?&lt;br /&gt;&lt;br /&gt;Personally, I feel very strongly about self-selection. In my (short) career, I have had a total of 2 rejects:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;a paper rejected from ICALP, which got accepted to the subsequent SODA ;)&lt;/li&gt;&lt;li&gt;a &quot;short paper&quot; rejected from SODA, which essentially disappeared (we came up with new ideas and obtained much better results). Incidentally, I'm glad the short track at SODA was stopped. It was a temptation to do mediocre work -- if you write a really good result in 2 pages, you will always get accepted even without the short track.&lt;/li&gt;&lt;/ul&gt;I self-select due to my own ethical beliefs, independent of the environment in the community. However, I'm often told that this is not career-optimal from a selfish perspective: if you shoot for a reasonable reject rate, say under 50%, you can often push papers up a notch, making for a better CV.&lt;br /&gt;&lt;br /&gt;Is this advice short-sighted? Does having a low reject rate help you in getting more attention from reviewers? If a reviewer knows you're not a &quot;deceiver&quot;, he will feel uneasy about rejecting, and will think about it for another minute -- at the very least, he knows one person in the world (you, the author) finds the paper interesting. If a reveiwer has already rejected a couple of your papers, he won't have so many moral problems -- after all, it's possible that you yourself don't think the paper is good enough.&lt;br /&gt;&lt;br /&gt;What do you think? Can you get reviewer sympathy by active self-selection?</content:encoded>
	<dc:date>2007-11-09T13:41:38+00:00</dc:date>
	<dc:creator>MiP</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-19908808.post-2463145648836843366">
	<title>Andy's Math/CS page: Limits of Transcendence: Our Failure to Protect Prisoners</title>
	<link>http://andysresearch.blogspot.com/2007/11/limits-of-transcendance-and-our-failure.html</link>
	<content:encoded>One thing I'd long been planning to post about is just how positive a role math has played in my own life.  Since I came to enjoy it (relatively late--around senior year in high school), I have become: less restless; less materialistic; clearer and more self-reliant in my thinking; able to think longer and more fruitfully about all kinds of things, with nothing but a pen and paper.&lt;br /&gt;&lt;br /&gt;All of these beneficial effects constitute, to my mind, a 'practical application' for math that warrants mentioning in the same breath as its uses in science and technology.  In particular, I always thought these virtues seemed especially well-suited to empower and improve the constrained lives of prisoners.  I know of one example--&lt;a href=&quot;http://www-groups.dcs.st-and.ac.uk/~history/Biographies/Turan.html&quot;&gt;Paul Turan&lt;/a&gt; seems to have invented extremal graph theory in a Hungarian labor camp during WWII--but Turan was admittedly already a professional mathematician.  Of course math isn't for everyone, but I would be very interested to know whether it's been taught specifically for enjoyment in prisons, and with what results.&lt;br /&gt;&lt;br /&gt;It's an exciting idea to me, and I hope it excites others.  However, it'd be irresponsible and dishonest to pursue it without acknowledging math's limits as a life practice.  Coping with boredom is one thing, but, speaking personally, my mathematical thinking goes to pieces when I'm in acute distress or suffering.  There's a limit to what we should reasonably expect to transcend through math, just as with art, religion, meditation, and other such 'soft' techniques... which brings me to what actually prompted this post: news from California that makes me ashamed of the state of affairs in my home state.  &lt;br /&gt;&lt;br /&gt;&lt;br /&gt;According to &lt;a href=&quot;http://www.prnewswire.com/cgi-bin/stories.pl?ACCT=104&amp;STORY=/www/story/10-15-2007/0004682327&amp;EDATE=&quot;&gt;this article&lt;/a&gt; (see also this &lt;a href=&quot;http://www.nytimes.com/2007/10/19/opinion/19fri4.html?_r=1&amp;oref=slogin&quot;&gt;Times editorial&lt;/a&gt;), Gov. Schwarzenegger recently vetoed a bill that would allow nonprofits to distribute condoms in prisons. (He claimed it would conflict with the law against sexual contact between inmates.)  It seems he did something similar two years ago.  There are some condom distributions in CA prisons, and Schwarzenegger authorized one new, small-scale pilot program, but he derailed the state legislature's decision to make such distributions a statewide reality.&lt;br /&gt;&lt;br /&gt;I hope the cruelty and stupidity of this decision speak for themselves, but I'll make a few comments to encourage further thought, because a decision this bad needs to be understood as well as denounced.  To try and make (partial) sense of his decision, let me suggest that, in policymaking and debate around HIV/AIDS and other sexually transmitted infections, there are various gradations of the position that would block direct public health initiatives.  &lt;br /&gt;&lt;br /&gt;First, there are those who simply disapprove of a particular behavior, be it pre- or extramarital sex, same-gender sex, sex in prisons, or intravenous drug use.  It doesn't necessarily lead them to disregard other social issues or form a blanket policy statement.&lt;br /&gt;&lt;br /&gt;Then there are those who, through a combination of wishful thinking and manipulation, reach the idea that the health threat posed by risky behaviors might itself be the best deterrent against these behaviors, superior from a moral &lt;span style=&quot;font-style:italic;&quot;&gt;and&lt;/span&gt; public health perspective to interventions that would make those behaviors safer (condom distributions, needle exchanges, HPV vaccinations, etc).  They accept arguments to this effect without sufficient critical thought, and neglect contrary evidence.&lt;br /&gt;&lt;br /&gt;Then there are those who publicly espouse this best-of-both-worlds notion, but whose prolonged refusal to engage with the evidence leads one to suspect that either &lt;span style=&quot;font-style:italic;&quot;&gt;(a)&lt;/span&gt; they no longer feel compassion for victims of infection from their designated 'deviant' population groups, or &lt;span style=&quot;font-style:italic;&quot;&gt;(b)&lt;/span&gt; they feel that infection's (supposed) role in discrediting and deterring 'deviant' behavior outweighs its human costs (so, in a sense, they &lt;span style=&quot;font-style:italic;&quot;&gt;ally themselves&lt;/span&gt; with the epidemic).&lt;br /&gt;&lt;br /&gt;Finally, at the far end you have those who feel only contempt for 'deviants', and openly exult in public health crises which they see as just punishment.&lt;br /&gt;&lt;br /&gt;But can we explain Schwarzenegger's behavior as occupying a point along this spectrum?  I'm not so sure.  As soon as incarceration comes into play, the debate seems to get much more complex (i.e., crazier).  Are these notions of morality and deterrence even operative in the (supposedly kinda-socially-liberal) Governor's decision?  Is he offering a feeble cover for the prison industry from the already-widespread recognition that, in many cases, they fail to protect their inmates from sexual coercion? (For a discussion of the state of available statistics on prison rape see &lt;a href=&quot;http://www.hrw.org/reports/2001/prison/report1.html&quot;&gt;this Human Rights Watch page&lt;/a&gt;.)  Or is he merely bowing to the time-honored tradition of abusing prisoners?&lt;br /&gt;&lt;br /&gt;I would love to hear what others think, because I can't decide the issue myself.  All that's clear to me is that he's taken citizens in a position of enforced vulnerability, who are contracting HIV at &lt;a href=&quot;http://www.medicalnewstoday.com/articles/49988.php&quot;&gt;eight times&lt;/a&gt; the rate outside prison, and cut off one of the most important means of protecting them.  (I don't believe the state was even asked to &lt;span style=&quot;font-style:italic;&quot;&gt;pay&lt;/span&gt; for the condoms....!)&lt;br /&gt;&lt;br /&gt;Meanwhile--Governor Schwarzenegger, withdraw your veto!</content:encoded>
	<dc:date>2007-11-08T15:14:40+00:00</dc:date>
	<dc:creator>Andy D</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-8890204.post-427748167370258964">
	<title>My Biased Coin: Service and the NSF</title>
	<link>http://mybiasedcoin.blogspot.com/2007/11/service-and-nsf.html</link>
	<content:encoded>I recently returned from an NSF (National Science Foundation) review panel.  (Of course, I'm not allowed to say when, or for what.)  On the way back, at the airport, I ran into a more senior colleague (from an entirely different field -- not EECS) coming back on the same flight.  It came up that he had &lt;span style=&quot;font-style: italic;&quot;&gt;never&lt;/span&gt; served on an NSF review panel.  He admittedly seemed a bit sheepish about it, but offered the excuse that there were more important things to do, and I will give him the benefit of the doubt and assume that he meant other service activities, not research and such.  Essentially, his argument seemed to be that NSF panels were a waste of his time.&lt;br /&gt;&lt;br /&gt;Having served on a half dozen or so NSF review panels, I have some sympathy for this point of view.  In particular, being anti-travel, I'm not sure why we have to fly in for a two-day meeting to make decisions;  it's expensive for the NSF and time-consuming for me.  (It's not clear to me that decisions are any better because of the face-to-face meeting.  Indeed, arguably, they can be worse, depending on the group interaction.  But government process is government process, so unlike for PCs, not having the face-to-face meeting is not currently an option...)&lt;br /&gt;&lt;br /&gt;However, despite the time, I've tried to make myself available when the calls for NSF panels go out, because I've always figured that if the NSF is paying my summer salary (which, in large part, they do), they have the right to ask for some of my time.  Indeed, my gut reaction (without working through the arguments) is that it's objectionable that they don't actually &lt;span style=&quot;font-style: italic;&quot;&gt;require&lt;/span&gt; people who have received funding to serve on a review panel at some point during the term of their grant, though I suppose the implementation of that rule could become unpleasant.  In short, my moral interpretation is that by taking their money, I'm part of their system, so when they come calling, I should respond, even without an explicit quid pro quo.&lt;br /&gt;&lt;br /&gt;Even if one does not subscribe to this implicit understanding, I think it's important for us individually and as a community (including non-academics who don't get NSF funding directly) to do our service for the NSF, particularly in the way of review panels.  In general, citizens should be keeping an eye out on how the government uses their money, and in this specific case, as scientists, we should be paying especially close attention to how the government is distributing research money to scientists in our own and related fields, and we should be attempting to influence the process to move in the right directions as much as possible.   The panels give us some opportunity to influence the process, and these otherwise near-useless face-to-face meetings give us some opportunity to influence the NSF administrators who shape and make the decisions that affect us all.&lt;br /&gt;&lt;br /&gt;So, with all due respect to my senior colleague, and any of you other slackers out there, get over yourselves, and go serve on an NSF panel every other year or so.</content:encoded>
	<dc:date>2007-11-08T12:50:00+00:00</dc:date>
</item>
<item rdf:about="tag:blogger.com,1999:blog-3722233.post-590068913766798483">
	<title>Computational Complexity: Resubmitting Rejected FOCS paper to STOC</title>
	<link>http://weblog.fortnow.com/2007/11/resubmitting-rejected-focs-paper-to.html</link>
	<content:encoded>(Guest post by
&lt;i&gt;Kamal Jain&lt;/i&gt;

Resubmitting the rejected papers from FOCS to STOC?

&lt;br /&gt;
&lt;br /&gt;

If your paper was rejected by FOCS and you're submitting it to STOC, here 
are my thoughts on how you can increase your chances of acceptance. Given the
low acceptance rate for FOCS, I am sure many of us will be resubmitting
our rejected papers to STOC. Many of us will be incorporating the FOCS PC 
comments. And there's also a realistic chance that FOCS PC misunderstood our
papers. So what should we do so that STOC committee does not repeat the
misunderstanding?

&lt;br /&gt;
&lt;br /&gt;

Let me first describe the general methods to contain the chances of
misunderstanding. I will then describe why the chance of misunderstanding 
has increased for STOC PC on resubmitted papers by giving you an insider view
of FOCS 2007 PC. We can then discuss what we could do to minimize that.

&lt;br /&gt;
&lt;br /&gt;

&lt;b&gt;Contain the chances of misunderstanding&lt;/b&gt;

&lt;br /&gt;
&lt;br /&gt;

Well of course removing the items from the paper which gave rise to misunderstanding
could be beneficial. These items could arise either due to lack of explanation,
positioning of clarification, or overselling the results. Lack of explanation
happens because we fail to realize as authors that our mind is pre-conditioned
while researching on the paper and the reviewer's mind won't be pre-conditioned
in the same way. Therefore things which look clear to us may be confusing to a
reviewer. Positioning of clarification is very important because not every paper
is read word to word. So it is very important to put the clarification or a pointer
to it as close as possible to the place where confusion could potentially arise.
Overselling does not improve the chances of a paper getting accepted.
Overselling of results typically puts the reviewer in a defensive position.
A reviewer could look at other existing papers that have introduced similar                   
techniques and be at a loss for what is new, unique, and real about what this paper promises.

&lt;br /&gt;
&lt;br /&gt;


So how do you address these problems? One thing is to prepare the paper early
and seek feedback. Do not expect somebody, who is not genuinely interested in
your work, to provide you good quality feedback for free. You would need to pay.
How? Offer the same high quality service on their papers as you
 expect on your own papers. Posting your papers online, e.g., as a technical
report in some archive could also bring some early readership, which may
provide you feedback and opportunities to exchange feedback.

&lt;br /&gt;
&lt;br /&gt;
If you really need to sell your paper, what's the best way? Give talks --
as many as possible. Try to accept every invitation and try to get yourself
invited by marketing the results. In order to market the paper be open to 
discussing your results in small chats without pen and paper, e.g., over a
lunch table. Acknowledge all pre-publication discussions, including those which
were not explicitly used in the paper. Mentioning the names of the people
is very important, and in case of explicit usefulness, mentioning it
explicitly is equally important too. This is so that your colleagues feel
acknowledged and positively reinforced to collaborate with you in the future. In
the short term, these colleagues are also likely to see the papers more
positively vs the case if they find their assistance is not fully acknowledged.

&lt;br /&gt;
&lt;br /&gt;
What else can you do if you do not yet have enough opportunities to talk about
your paper? We have not done so, but there are cheap as well as free software
using which we can easily make a high quality screencast. For me personally,
a high quality screencast provides 80% of the benefit of watching
the talk in person. Much of the benefit of the remaining 20% can also be 
obtained if there is an open forum associated with the screencast to ask questions
which can either be answered by the authors or other viewers in a
relatively short time. Readers do not have the patience unless they are genuinely
interested in your result. And expect to count the number of the latter
on your fingers.:)

&lt;br /&gt;
&lt;br /&gt;

&lt;b&gt;An insider's view of FOCS:&lt;/b&gt;

&lt;br /&gt;
&lt;br /&gt;

What's specific about paper reviewing these days? As part of the FOCS committee
we had access to reviews submitted by the previous STOC committee. We
paid a great deal of attention to whether the version we had had responded
to the STOC PC's reasons of rejecting the papers. Similarly expect STOC 2008
PC to have FOCS 2007 PC's reviews available. The intersection between STOC 2008 PC
and FOCS 2007 PC is non-empty. Even if you think FOCS PC misunderstood
your paper, and responding to those misunderstandings would make the
paper less readable, you should still try to respond to those misunderstandings
instead of ignoring them. In such cases you can respond to those
misunderstandings either in appropriate footnotes or in a one page appendix in
the end. If your footnotes and appendix are just for STOC PC, do mention &quot;for
the reviewers only, will be removed from the published version.&quot;

&lt;br /&gt;
&lt;br /&gt;
What about the feedback that FOCS PC kept confidential and did not transmit
 to the authors? This part of the feedback must not be used by STOC committee
for three reasons. First, it was understood that only FOCS PC share that
 feedback. Second, this part of the feedback was a part of the process and
not the net outcome. The net outcome ideally must be included in the &quot;send
to author&quot; part of the feedback. Third, since this part of the feedback was
 not transmitted to the authors, they can't be expected to respond. If this
 part of the feedback did contain a reason why the paper should be rejected
 then authors must be sent the reason. If this was not done in some cases,
then STOC PC must work hard to rediscover the same reason for rejection.

&lt;br /&gt;
&lt;br /&gt;
This is my view from both having submitted (and received rejections) papers
 as well as been part of various PCs. I hope these give you additional practical
tical tips on how to best position your papers. I welcome other ideas so that
we could continue to improve the quality of our submissions.

&lt;br /&gt;
&lt;br /&gt;


Thanks,

&lt;br /&gt;
&lt;br /&gt;


Kamal Jain.

&lt;br /&gt;
&lt;br /&gt;


Note: FOCS means FOCS 2007. STOC means STOC 2008. Previous STOC means STOC =
2007.</content:encoded>
	<dc:date>2007-11-07T17:24:31+00:00</dc:date>
	<dc:creator>GASARCH</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-21129445.post-1379588522591355591">
	<title>my slice of pizza: Helvetica</title>
	<link>http://mysliceofpizza.blogspot.com/2007/11/helvetica.html</link>
	<content:encoded>&lt;a href=&quot;http://graphics8.nytimes.com/images/2007/09/12/arts/12helv600.jpg&quot;&gt;&lt;img style=&quot;float:left; margin:0 10px 10px 0;cursor:pointer; cursor:hand;height: 200px;width: 300px;&quot; src=&quot;http://graphics8.nytimes.com/images/2007/09/12/arts/12helv600.jpg&quot; border=&quot;0&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;I should have &lt;a href=&quot;http://geomblog.blogspot.com/2007/04/its-sans-serif-smackdown.html&quot;&gt;listened to Suresh&lt;/a&gt; and watched &lt;a href=&quot;http://www.helveticafilm.com/index.html&quot;&gt;the movie&lt;/a&gt; long time ago. Instead, I watched it two days ago. There is the story of the NY grandmothers who would sit at their window and watch their 10 yr old grandson walk down the block, and the adventurous ones, let their grandson walk down the avenue block! Well, those NY grandmothers would have found it easy to track me as I walked all of 0.5 street blocks from my aptment to the IFC theater to watch it late night, the movie by now almost a midnight cult affair. The theater had 2 men fiddling with pen and paper, 2 couples (one hetero), an oldman with bulging backpack and beard flowing down to trip him when he walked, and me. I swear someone had a Mac in the theater, the movie was mainly a sublime homage to Apple. It was encouraging to see grown men (and 1 grown woman) speak passionately about fonts. The movie (&lt;a href=&quot;http://movies.nytimes.com/2007/09/12/movies/12helv.html&quot;&gt;NY Times Review&lt;/a&gt; gets it right: &quot;&lt;span style=&quot;font-style:italic;&quot;&gt;you’re guaranteed to spend the next few days scanning the world for Helvetica like a child on a cross-country car trip playing I Spy.&lt;/span&gt;&quot;) was visually pleasing, moving from New to Old amsterdam/world, and I returned home, vowing to experiment with fonts. Save me.</content:encoded>
	<dc:date>2007-11-07T07:51:35+00:00</dc:date>
	<dc:creator>metoo</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-21129445.post-6373976828242878807">
	<title>my slice of pizza: Math in NYer</title>
	<link>http://mysliceofpizza.blogspot.com/2007/11/math-in-nyer.html</link>
	<content:encoded>Finally, two of my favorites collide. NYer has a &lt;a href=&quot;http://www.newyorker.com/talk/2007/11/12/071112ta_talk_widdicombe&quot;&gt;fun writeup&lt;/a&gt; on FireFighters Theorem, and quotes the work of Barry Cipra. Enjoy!</content:encoded>
	<dc:date>2007-11-07T07:31:54+00:00</dc:date>
	<dc:creator>metoo</dc:creator>
</item>
<item rdf:about="urn:lj:livejournal.com:atom1:11011110:122234">
	<title>0xDE: Six recent arXiv papers</title>
	<link>http://11011110.livejournal.com/122234.html</link>
	<content:encoded>Some recent uploads to &lt;a href=&quot;http://arxiv.org&quot;&gt;arxiv.org&lt;/a&gt; that have caught my attention:&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Lens sequences&lt;/b&gt;, by J. Kocik, &lt;a href=&quot;http://arxiv.org/abs/0710.3226&quot;&gt;arXiv:0710.3226&lt;/a&gt;. &lt;a name=&quot;cutid1&quot;&gt;&lt;/a&gt;Considers problems in which the lens-shaped space between two equal-radius circles is filled by a sequence of other circles, so that each circle inside the lens is tangent to the lens boundaries and to two other circles. The reciprocals of the radii of these circles form bi-infinite sequences in which the values are in many cases all integers; when this happens, the sequence is formed by products of consecutive values in some simpler &quot;underground&quot; integer sequence. For instance, one example has radii 1/3, 1/15, 1/35, 1/63, ...; the denominators of these numbers are products of consecutive odd numbers, so the underground sequence is the sequence of odd numbers. Other underground sequences involve Fibonacci numbers and Lucas numbers, or the sequences formed by shuffling pairs of such sequences.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Generalizations of Schöbi's tetrahedral dissection&lt;/b&gt;, by Sloane and Vaishampayan, &lt;a href=&quot;http://arxiv.org/abs/0710.3857&quot;&gt;arXiv:0710.3857&lt;/a&gt;. &lt;a name=&quot;cutid2&quot;&gt;&lt;/a&gt; If you start with a triangle with vertices (0,0), (0,1), and (1,1), cut off  a corner, and reattach the cut corner to the remaining part along the diagonal, you get a 1/2 &amp;times; 1 prism. If you start with a tetrahedron with vertices (0,0,0), (0,0,1), (0,1,1), and (1,1,1), and make two cuts perpendicular to the main diagonal passing through the two non-main-diagonal vertices, the pieces can be rearranged to form an equilateral-triangle prism. (These two cuts partition the main diagonal of the tetrahedron into three equal pieces that become the edges of the prism, and partition the shorter diagonals into two equal pieces.) This paper generalizes this to a family of n-piece dissections of n-dimensional simplices (affine transforms of the &quot;orthoschemes&quot;, simplices with 0-1 coordinates as above), into (n-1)-dimensional prisms. Iterating the construction leads to an n!-piece dissection of an orthoscheme into a parallelepiped.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Simple, linear-time modular decomposition&lt;/b&gt;, by Tedder, Corneil, Habib, and Paul, &lt;a href=&quot;http://arxiv.org/abs/0710.3901&quot;&gt;arxiv:0710.3901&lt;/a&gt;. &lt;a name=&quot;cutid3&quot;&gt;&lt;/a&gt;A module in a graph is a set of vertices that all have the same pattern of connections to the rest of the graph: they may have an arbitrary set of edges among themselves, but if one vertex in the set has an edge to a vertex outside the set, the rest of them must also have an edge to the same vertex. If a graph is disconnected or co-disconnected, the components are trivial modules, but one can also have modules of more complex types; in the nontrivial case the maximal modules form a partition of the graph (counting a single vertex as a module). Modular decomposition is what you repeatedly form this partition, then look for submodules within each maximal module, etc. There have been linear time algorithms known for doing this for over ten years now, and I've thought about adding them to &lt;a href=&quot;http://www.ics.uci.edu/~eppstein/PADS/&quot;&gt;PADS&lt;/a&gt;, but they're rather complicated. This paper seems a step in the right direction: still linear time, but closer to being implementable.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;The lonely vertex problem&lt;/b&gt;, by D. Frettlöh and A. Glazyrin, &lt;a href=&quot;http://arxiv.org/abs/0710.4870&quot;&gt;arxiv:0710.4870&lt;/a&gt;. &lt;a name=&quot;cutid4&quot;&gt;&lt;/a&gt;It's possible to surround a point in the plane by finitely many convex polygons in such a way that it's the vertex of only two of them: for instance, consider the vertices of the standard &quot;brick wall&quot; tiling of the plane by dominos. But it's not possible to surround a point by convex polygons in such a way that it's a vertex of just one, because the complement of the polygon would form an angle strictly between 180 and 360 degrees, which cannot be filled by the 180 degree angles of nonvertex boundary points of polygons. But what about in higher dimensions? Can one surround a point of some higher dimensional Euclidean space by finitely many convex polytopes in such a way that one of the polytopes has a vertex that's not also a vertex of any other polytope? As the authors show, the answer is no. The 3d result can also be phrased in terms of partitions of a sphere surrounding the point: if one defines a &quot;wedge&quot; to be the shape bounded between two longitudes on a globe, one can't partition the globe into finitely many wedges and one leftover convex region.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Triangular peg solitaire unlimited&lt;/b&gt;, by G. I. Bell, &lt;a href=&quot;http://arxiv.org/abs/0711.0486&quot;&gt;arxiv:0711.0486&lt;/a&gt;. &lt;a name=&quot;cutid5&quot;&gt;&lt;/a&gt;You all know what peg solitaire is, right? Start with a grid of pegs, all but one empty, and then repeatedly jump one peg over another, removing the jumped peg, until all are gone or (more likely) you get stuck with a few scattered pegs and have to try again. If not, go to &lt;a href=&quot;http://www.geocities.com/gibell.geo/pegsolitaire/tindex.html&quot;&gt;George Bell's triangular peg solitaire page&lt;/a&gt; and try playing his &lt;a href=&quot;http://www.geocities.com/gibell.geo/pegsolitaire/Tools/Triangular/Triang.htm&quot;&gt;javascript peg solitaire applet&lt;/a&gt;. In this paper, Bell studies two problems related to this puzzle: how few moves are possible (if one counts a sequence of jumps by the same peg as a single move), and how many jumps are possible on the last move? In a single jump sequence, the points that can be positions of the jumping peg and the points that can be jumped turn out to be disjoint, so the problem of maximizing jump sequence length is related to forming Eulerian subgraphs of the triangular grid, but not every graph of this type can be formed as part of a valid solution to the whole puzzle.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Permutations defining convex permutominoes&lt;/b&gt;, by Bernini, Disanto, Pinzani, and Rinaldi, &lt;a href=&quot;http://arxiv.org/abs/0711.0582&quot;&gt;arxiv:0711.0582&lt;/a&gt;. &lt;a name=&quot;cutid6&quot;&gt;&lt;/a&gt;A permutomino is just a polyomino (polygon with axis-aligned sides and integer vertex coordinates) with the property that no two sides lie on the same line, the vertical sides lie on a contiguous sequence of vertical integer grid lines, and the horizontal sides lie on a contiguous sequence of horizontal integer grid lines. The authors describe a way of encoding a permutomino with two permutations, but not every pair of permutations works: some give disconnected boundaries or self-crossing boundaries. They characterize the permutations that do lead to simple polygons. This caught my attention because it seems relevant for &lt;a href=&quot;http://11011110.livejournal.com/tag/xyz+graphs&quot;&gt;xyz graphs&lt;/a&gt;, and specifically for the problem of finding three permutations of the coordinates that eliminate all crossings of an xyz graph representation, if possible.</content:encoded>
	<dc:date>2007-11-07T06:14:42+00:00</dc:date>
</item>
<item rdf:about="http://scottaaronson.com/blog/?p=288">
	<title>Shtetl-Optimized: MIT sues Frank Gehry over Stata Center</title>
	<link>http://scottaaronson.com/blog/?p=288</link>
	<content:encoded>&lt;p style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;http://www.scottaaronson.com/statactr.jpg&quot; /&gt;&lt;/p&gt;
&lt;p&gt;When I first saw the &lt;a href=&quot;http://www.nytimes.com/aponline/us/AP-MIT-Suit-Architect.html?_r=2&amp;hp=&amp;oref=slogin&amp;pagewanted=print&amp;oref=slogin&quot;&gt;headline&lt;/a&gt;, I assumed it was from The Onion or (more likely) some local MIT humor publication.  But no, it&amp;#8217;s from the Associated Press.&lt;/p&gt;</content:encoded>
	<dc:date>2007-11-07T02:42:40+00:00</dc:date>
</item>
<item rdf:about="tag:blogger.com,1999:blog-24557460.post-7598394102565279509">
	<title>in theory: The Impagliazzo Hard-Core-Set Theorem</title>
	<link>http://in-theory.blogspot.com/2007/11/impagliazzo-hard-core-set-theorem.html</link>
	<content:encoded>The &lt;a href=&quot;http://www.cs.ucsd.edu/~russell/hardcore.ps&quot;&gt;Impagliazzo hard-core set theorem&lt;/a&gt; is one of the bits of magic of complexity theory. Say you have a function $g:\{ 0, 1 \}^n \rightarrow \{ 0,1\}$ such that every efficient algorithm makes errors at least $1%$ of the times when computing $g$ on a random input. (We'll think of $g$ as exhibiting a weak form of average-case complexity.) Clearly, different algorithms will fail on a different $1%$ of the inputs, and it seems that, intuitively, there should be functions for which no particular input is harder than any particular other input, per se. It's just that whenever you try to come up with an algorithm, some set of mistakes, dependent on the algorithmic technique, will arise. &lt;br /&gt;&lt;br /&gt;As a good example, think of the process of generating $g$ at random, by deciding for every input $x$ to set $g(x)=1$ with probability $99%$ and $g(x)=0$ with probability $1%$. (Make the choices independently for different inputs.) With very high probability, every efficient algorithm fails with probability at least about $1%$, but, if we look at every efficiently recognizable large set $H$, we see that $g$ takes the value 1 on approximately $99%$ of the elements of $H$, and so the trivial algorithm that always outputs 1 has a pretty good success probability. &lt;br /&gt;&lt;br /&gt;Consider, however, the set $H$ of size $\frac {2}{100} 2^n$ that you get by taking the $\approx \frac{1}{100} 2^n$ inputs $x$ such that $g(x)=0$ plus a random sample of $\frac{1}{100} 2^n$ inputs $x$ such that $g(x)=1$. Then we can see that no efficient algorithm can compute $g$ on much better than $50%$ of the inputs of $H$. This is the highest form of average-case complexity for a boolean function: on such a set $H$ no algorithm does much better in computing $g$ than an algorithm that makes a random guess.&lt;br /&gt;&lt;br /&gt;The Impagliazzo hard-core theorem states that it is always possible to find such a set $H$ where the average-case hardness is &quot;concentrated.&quot; Specifically, it states that if every efficient algorithm fails to compute $g$ on a $\geq \delta$ fraction of inputs, then there is a set $H$ of size $\geq \delta 2^n$ such that every efficient algorithm fails to compute $g$ on at least a $\frac 12 - \epsilon$ fraction of the elements of $H$. This is true for every $\epsilon,\delta$, and if &quot;efficient&quot; is quantified as &quot;circuits of size $s$&quot; in the premise, then &quot;efficient&quot; is quantified as &quot;circuits of size $poly(\epsilon,\delta) \cdot s$&quot; in the conclusion.&lt;br /&gt;&lt;br /&gt;The example of the biased random function given above implies that, if one wants to prove the theorem for arbitrary $g$, then the set $H$ cannot be efficiently computable itself. (The example does not forbid, however, that $H$ be efficiently computable given oracle access to $g$, or that a random element of $H$ be samplable given a sampler for the distribution $(x,g(x))$ for uniform $x$.)&lt;br /&gt;&lt;br /&gt;A number of proofs of the hard core theorem are known, and connections have been found with the process of &lt;a href=&quot;http://www.cs.utexas.edu/~klivans/newhab3.ps&quot;&gt;&lt;i&gt;boosting&lt;/i&gt; in learning theory&lt;/a&gt; and with the &lt;a href=&quot;http://www.cs.berkeley.edu/~luca/pubs/xordecoding.ps&quot;&gt;construction and the decoding of certain error-correcting codes&lt;/a&gt;. Here is a precise statement.&lt;br /&gt;&lt;br /&gt;&lt;blockquote&gt;&lt;br /&gt;&lt;b&gt;Impagliazzo Hard-Core Set Theorem&lt;/b&gt;&lt;br /&gt;Let $g:\{0,1\}^n \rightarrow \{0,1\}$ be a boolean function, $s$ be a size parameter, $\epsilon,\delta&gt;0$ be given. Then there is a $c(\epsilon,\delta) = poly(1/\epsilon,1/\delta)$ such that the following happens.&lt;br /&gt;&lt;br /&gt;Suppose that for every function $f:\{0,1\}^n \rightarrow \{0,1\}$ computable by a circuit of size $\leq c\cdot s$ we have &lt;br /&gt;&lt;br /&gt;$Pr_{x \in \{0,1\}^n} [ f(x) = g(x) ] \leq 1-\delta$&lt;br /&gt;&lt;br /&gt;Then there is a set $H$ of size $\geq \delta 2^n$ such that for every function $f$ computable by a circuit of size $\leq s$ we have&lt;br /&gt;&lt;br /&gt;$Pr_{x\in H} [ f(x) = g(x) ] \leq \frac 12 + \epsilon$&lt;br /&gt;&lt;/blockquote&gt;&lt;br /&gt;&lt;br /&gt;Using the &quot;finitary ergodic theoretic&quot; approach of iterative partitioning, we (Omer Reingold, Madhur Tulsiani, Salil Vadhan and I) are able to prove the following variant.&lt;br /&gt;&lt;br /&gt;&lt;blockquote&gt;&lt;br /&gt;&lt;b&gt;Impagliazzo Hard-Core Set Theorem, &quot;Constructive Version&quot;&lt;/b&gt;&lt;br /&gt;Let $g:\{0,1\}^n \rightarrow \{0,1\}$ be a boolean function, $s$ be a size parameter, $\epsilon,\delta&gt;0$ be given. Then there is a $c(\epsilon,\delta) = exp(poly(1/\epsilon,1/\delta))$ such that the following happens.&lt;br /&gt;&lt;br /&gt;Suppose that for every function $f:\{0,1\}^n \rightarrow \{0,1\}$ computable by a circuit of size $\leq c\cdot s$ we have&lt;br /&gt;&lt;br /&gt;$Pr_{x \in \{0,1\}^n} [ f(x) = g(x) ] \leq 1-\delta$&lt;br /&gt;&lt;br /&gt;Then there is a set $H$ such that: (i) $H$ is recognizable by circuits of size $\leq c\cdot s$; (ii) $|H| \geq \delta 2^n$, and in fact the number of $x$ in $H$ such that $g(x)=0$ is at least $\frac 12 \delta 2^n$, and so is the number of $x$ in $H$ such that $g(x)=1$; and (iii) for every $f$ computable by a circuit of size $\leq s$,&lt;br /&gt;&lt;br /&gt;$Pr_{x\in H} [ g(x) = f(x) ] \leq max \{ Pr_{x\in H}[ g(x) = 0] , Pr_{x\in H} [g(x)=1] \} + \epsilon$&lt;br /&gt;&lt;/blockquote&gt;&lt;br /&gt;&lt;br /&gt;The difference is that $H$ is now an efficiently recognizable set (which is good), but we are not able to derive the same strong average-case complexity of $g$ in $H$ (which, as discussed as the beginning, is impossible in general). Instead of proving that a &quot;random guess algorithm&quot; is near-optimal on $H$, we prove that a &quot;fixed answer algorithm&quot; is near-optimal on $H$. That is, instead of saying that no algorithm can do better than a random guess, we say that no algorithm can do better than either always outputting 0 or always outputting 1. Note that this conclusion is meaningless if $g$ is, say, always equal to 1 on $H$, but in our construction we have that $g$ is not exceedingly biased on $H$, and if $\epsilon  \delta/2$, say, then the conclusion is quite non-trivial.&lt;br /&gt;&lt;br /&gt;One can also find a set $H'$ with the same type of average-case complexity as in the original Impagliazzo result by putting into $H'$ a $\frac 12 \delta 2^n$ size sample of elements $x$ of $H$ such that $g(x)=0$ and an equal size sample of elements of $H$ such that $g$ equals 1. (Alternatively, put in $H'$ all the elements of $H$ on which $g$ achieves the minority value of $g$ in $H$, then add a random sample of as many elements achieving the majority value.) Then we recover the original statement except that $c(\epsilon,\delta)$ is exponential instead of polynomial.&lt;br /&gt;&lt;br /&gt;Coming up next, the proof of the &quot;constructive hard core set theorem&quot; and my attempt at explaining what the techniques have to do with &quot;finitary ergodic theory.&quot;</content:encoded>
	<dc:date>2007-11-06T21:45:00+00:00</dc:date>
</item>
<item rdf:about="http://valis.cs.uiuc.edu/blog/?p=579">
	<title>Vanity of Vanities, all is Vanity: The purpose of computing is insight, not numbers</title>
	<link>http://valis.cs.uiuc.edu/blog/?p=579</link>
	<content:encoded>&lt;blockquote&gt;&lt;p&gt;The Institute for Advanced Study in Princeton, in my opinion, has ruined more good scientists than any institution has created, judged by what they did before they came and judged by what they did after. Not that they weren&amp;#8217;t good afterwards, but they were superb before they got there and were only good afterwards.
&lt;p&gt;
&amp;#8211; &lt;a href=&quot;http://en.wikipedia.org/wiki/Richard_Hamming&quot;&gt;Richard Hamming&lt;/a&gt;. &lt;/p&gt;
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;From &lt;a href=&quot;http://www.cc.gatech.edu/~agray/hamming-on-research.pdf&quot;&gt;here&lt;/a&gt;&lt;a&gt; - worth reading.&lt;/a&gt;&lt;/p&gt;</content:encoded>
	<dc:date>2007-11-06T21:37:07+00:00</dc:date>
</item>
<item rdf:about="tag:blogger.com,1999:blog-6555947.post-3226414513067858242">
	<title>The Geomblog: &quot;As good as your best result&quot;</title>
	<link>http://geomblog.blogspot.com/2007/11/as-good-as-your-best-result.html</link>
	<content:encoded>&lt;a href=&quot;http://infoweekly.blogspot.com/2007/11/numbers-game.html&quot;&gt;Mihai makes the argument&lt;/a&gt; that in theoryCS, you're as famous as your best result. I think the number of good results does matter a bit more than that, but getting well known for your best result is the best way to make a first splash and get on the radar.&lt;br /&gt;&lt;br /&gt;I've actually heard the opposite claim made by systems researchers, and this has been extended to other empirical researchers as well. Namely, &quot;you're as good as your worst result&quot; (presumably thresholded to ignore grad school). The rationale here appears to be that in the conservative empirical sciences, where a badly designed experiment can cast a shadow on all your later work, it's your worst result that matters.&lt;br /&gt;&lt;br /&gt;I can see how this works (sort of): in the more mathematical disciplines, a &quot;proof&quot; can to a large extent be validated independent of the author, so mistakes in past proofs can be tolerated (though if this becomes an endemic problem, then ...). &lt;a href=&quot;http://en.wikipedia.org/wiki/Louis_de_Branges_de_Bourcia&quot;&gt;Louis de Branges&lt;/a&gt; is famous for his proof of a conjecture in complex analysis known as the Bieberbach conjecture, and is currently well known for his claimed proof of the Riemann Hypothesis (Karl Sabbagh's book on this has more on de Branges). His proof of the Bieberbach conjecture was not without some initial skepticism from other mathematicians, because of some earlier false starts. However, it was soon seen as correct, and has opened up new areas of mathematics as well. As a consequence, his proofs of the Riemann hypothesis have received more cautious consideration as well (although I am not aware of the current status of his claims).&lt;br /&gt;&lt;br /&gt;On the other hand, validating empirical work is largely on trust: you trust that the scientists have made faithful measurements, have kept proper lab notes etc, and I can see how a weakening of that trust can lead to much higher skepticism.&lt;br /&gt;&lt;br /&gt;In this light, it's interesting to read &lt;a href=&quot;http://gladwell.typepad.com/gladwellcom/2007/11/serial-killers.html&quot;&gt;Malcolm Gladwell's latest article&lt;/a&gt; for the New Yorker. He profiles the &quot;profilers&quot;: the psych experts who help the FBI track down serial killers, and comes to this conclusion:&lt;br /&gt;&lt;blockquote&gt;He seems to have understood only that, if you make a great number of predictions, the ones that were wrong will soon be forgotten, and the ones that turn out to be true will make you famous.&lt;br /&gt;&lt;/blockquote&gt;&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~f/TheGeomblog?a=kwzwnHB&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~f/TheGeomblog?i=kwzwnHB&quot; border=&quot;0&quot; /&gt;&lt;/a&gt;
&lt;/div&gt;</content:encoded>
	<dc:date>2007-11-06T21:13:25+00:00</dc:date>
	<dc:creator>Suresh</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-19908808.post-2103400202051567382">
	<title>Andy's Math/CS page: Goofball DNA-Bonanza</title>
	<link>http://andysresearch.blogspot.com/2007/11/goofball-dna-bonanza.html</link>
	<content:encoded>Since this blog's readership is, I'm guessing, essentially a (very-)proper subset of the readership of &lt;a href=&quot;http://www.scottaaronson.com/blog/&quot;&gt;Shtetl-Optimized&lt;/a&gt;, you may be familiar with his &lt;a href=&quot;http://scottaaronson.com/blog/?p=287&quot;&gt;recent musings and observations&lt;/a&gt; on DNA rewriting rules (to which I refer readers before looking further at my post).  I've been finding this stuff a fun alternative to my usual research (the last time I got this worked up over DNA might've been when I heard about &lt;a href=&quot;http://www.grg.org/OHSUmonkey.htm&quot;&gt;ANDi the monkey&lt;/a&gt;), and have come up with three simple results in the past couple of days.  The latest is a bit too long for Scott's comments section, so I wrote it up in LaTex and am putting it &lt;a href=&quot;http://people.csail.mit.edu/andyd/dnacode.pdf&quot;&gt;here&lt;/a&gt;.  &lt;br /&gt;&lt;br /&gt;I encourage all readers to generate, pose, and/or answer additional questions on related themes!  Collaborative online research is lots of fun in my experience, and something I'm considering trying to do more of with this blog.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Let me explain the third result after reviewing the other two.  But before I begin, I want to emphasize that I make no claims as to the biological realism of any of the models under consideration, though I'm happy to try to work out results in more realistic models if they still seem like interesting puzzles.  &lt;br /&gt;&lt;br /&gt;First, I showed that, in synthesizing a string &lt;span style=&quot;font-style:italic;&quot;&gt;x&lt;/span&gt; via Scott's model of DNA rewrite rules, the substring-reversal rule can be eliminated at the cost of increasing by constant factors the number of steps and the size of intermediate strings.&lt;br /&gt;&lt;br /&gt;Then, I showed that a factor-2 increase in steps allows one to hold the intermediate string to within a factor 2 of the final string length at all times.  This result, by constrast, holds whether one is trying to synthesize &lt;span style=&quot;font-style:italic;&quot;&gt;x&lt;/span&gt; de novo or to transform some other string &lt;span style=&quot;font-style:italic;&quot;&gt;x'&lt;/span&gt; into &lt;span style=&quot;font-style:italic;&quot;&gt;x&lt;/span&gt; (although I only wrote up the first case, the other is essentially the same analysis).  In the process, I was especially pleased to introduce an analytical tool called 'garbage monsters'; Scott and I hope the terminology will catch on in mainstream biology.&lt;br /&gt;&lt;br /&gt;The &lt;a href=&quot;http://people.csail.mit.edu/andyd/dnacode.pdf&quot;&gt;newest result&lt;/a&gt; is the following: suppose you want to implement an error-correcting code &lt;span style=&quot;font-style:italic;&quot;&gt;x --&gt; C(x)&lt;/span&gt; via DNA rewrite rules, where the choices of operations made can depend on &lt;span style=&quot;font-style:italic;&quot;&gt;x&lt;/span&gt;; then unfortunately, something on the order of &lt;span style=&quot;font-style:italic;&quot;&gt;n/(lg n)&lt;/span&gt; operations are needed in the encoding process to ensure that the code is actually robust.  A matching upper bound can be easily extracted from Scott's observations in his comments section, and it's a dispiriting bound because for that price, you could synthesize &lt;span style=&quot;font-style:italic;&quot;&gt;C(x)&lt;/span&gt; without even manipulating the string &lt;span style=&quot;font-style:italic;&quot;&gt;x&lt;/span&gt; that you're given! (A similar, simpler result holds for the decoding process, but I haven't written this up.)&lt;br /&gt;&lt;br /&gt;For completeness, I'll reproduce my sketch proofs for the first two results below... but first let me note that monkeys &lt;a href=&quot;http://www.nytimes.com/2007/11/06/science/06tier.html?em&amp;ex=1194498000&amp;en=c15bbb924188a5d6&amp;ei=5087%0A&quot;&gt;continue to make news&lt;/a&gt; (thanks again, Chaya!).&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;span style=&quot;font-weight:bold;&quot;&gt;***First Result, Comment 21 on &lt;a href=&quot;http://scottaaronson.com/blog/?p=287&quot;&gt;Scott's post&lt;/a&gt;***&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&quot;So how powerful is the reversal operation, really?&lt;br /&gt;&lt;br /&gt;Suppose we can produce string x in k applications of operations (1)-(4). I claim we can do it in 4k + 1 operations, without reversals. (Of course, this will show that the Lempel-Ziv approach is also a constant-factor approximation in our setting.)&lt;br /&gt;&lt;br /&gt;First, note that, if you can produce x in k steps with (1)-(4), you can also produce x^R, i.e. the reversal of x, in k steps with (1)-(4), by ‘flipping’ each operation.&lt;br /&gt;&lt;br /&gt;Second, note that you can produce x.x^R, their concatenation, in 2k steps by running both productions in parallel, interleaving steps on the two halves. If the i’th step in the original production of x was s, then the 2i’th step in the new production will be s.s^R.&lt;br /&gt;&lt;br /&gt;Now, in this ‘joint’ production we eliminate reversals as follows: say in the production of x, at the i’th stage we wish to move from&lt;br /&gt;&lt;br /&gt;s = a.b.c to s’ = a.b^R.c …&lt;br /&gt;&lt;br /&gt;well, in the joint production, at stage 2i we’ve got the string&lt;br /&gt;&lt;br /&gt;a.b.c.c^R.b^R.a^R&lt;br /&gt;&lt;br /&gt;By two copy and two deletion operations, we reach&lt;br /&gt;&lt;br /&gt;a.b^R.c.c^R.b.a^R,&lt;br /&gt;&lt;br /&gt;as needed to advance both sides’ progress.&lt;br /&gt;&lt;br /&gt;Finally, when the joint production’s finished, delete the right-hand side to yield x. That’s 4k + 1 steps.&quot;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;span style=&quot;font-weight:bold;&quot;&gt;***Second Result, Comments 30-31 on &lt;a href=&quot;http://scottaaronson.com/blog/?p=287&quot;&gt;Scott's post&lt;/a&gt;***&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&quot;I believe I can show that, if we’re producing a string x of length n, we can keep the intermediate strings to length at most 2n, while incurring at most a factor-2 blowup in the number of steps used.&lt;br /&gt;&lt;br /&gt;As Scott suggested to me in conversation, let’s keep track of which symbols in the intermediate workstring will and won’t make it into the final string x without deletion. If they won’t, call them ‘nonterminals’. (If a symbol will be replicated, and at least one of its descendants will be included in the final string, it’s not considered a nonterminal.)&lt;br /&gt;&lt;br /&gt;First note that, if in a production of x there appears at some stage a block of consecutive nonterminals, we can modify the production so that they are treated as a ‘unit’ in subsequent steps, i.e. no string operation takes an endpoint that’s properly inside the block. They get shuffled around together and eventually destroyed together. The production’s length is not affected.&lt;br /&gt;&lt;br /&gt;In a stage of such a production, for analysis purposes, we call a maximal block of nonterminals a ‘garbage monster’. Each time a new garbage monster is created (and many may be created by a single copy operation–even if there were no nonterminals in the original substring), we give it an unused, goofy monster name.&lt;br /&gt;&lt;br /&gt;When string manipulations cause two garbage monsters to become adjacent, we combine them under one of their two names (and say that that one ‘eats’ the other one).&lt;br /&gt;&lt;br /&gt;Key claim: in our modified type of production, at most one garbage monster gets eaten or destroyed at every step! Draw some pictures to see if you agree.&lt;br /&gt;&lt;br /&gt;Now, with this fact in hand, we can modify the production again and argue it gives what we want.&lt;br /&gt;Here’s the strategy: given a production of x of the type we described, modify the production so that between each step, we stop and delete all garbage monsters, one deletion per monster.&lt;br /&gt;&lt;br /&gt;This is a bit wasteful since some of these might’ve been eaten by other garbage monsters; but if our production had k steps to begin with, only at most k monsters got eaten total, and all garbage monsters must die somehow, so we only add at most k new steps.&lt;br /&gt;&lt;br /&gt;Finally, observe that the string is culled to length at most n after every new round of ‘executions’, and since the workstring’s length is at most doubled by any string operation, its length is bounded by 2n throughout.&quot;</content:encoded>
	<dc:date>2007-11-06T16:35:37+00:00</dc:date>
	<dc:creator>Andy D</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-786333285568106173.post-2438490600907784895">
	<title>WebDiarios de Motocicleta: Cute Problem: Wooden Convex Hull</title>
	<link>http://infoweekly.blogspot.com/2007/11/cute-problem-wooden-convex-hull.html</link>
	<content:encoded>I didn't manage to keep an earlier promise to post olympiad-style problems... But here is a cute one.&lt;br /&gt;&lt;br /&gt;You have &lt;span style=&quot;font-style: italic;&quot;&gt;n&lt;/span&gt; trees at given coordinates in the plane. You want to build a polygonal fence around the trees. However, the only way to get wood for the fence is to cut down some trees in the first place! Let's say each tree gives you enough wood for &lt;span style=&quot;font-style: italic;&quot;&gt;M&lt;/span&gt; meters of fence. The goal is to find the minimum &lt;span style=&quot;font-style: italic;&quot;&gt;k&lt;/span&gt;, such that you can cut down some &lt;span style=&quot;font-style: italic;&quot;&gt;k&lt;/span&gt; trees, and surround the rest by a fence of &lt;span style=&quot;font-style: italic;&quot;&gt;M&lt;/span&gt;*&lt;span style=&quot;font-style: italic;&quot;&gt;k&lt;/span&gt; meters.&lt;br /&gt;&lt;br /&gt;What is the best algorithm you can find? Express your answer in terms on &lt;span style=&quot;font-style: italic;&quot;&gt;n&lt;/span&gt; and &lt;span style=&quot;font-style: italic;&quot;&gt;k&lt;/span&gt;.&lt;br /&gt;&lt;br /&gt;As a bonus challenge, find a fast algorithm for the case when trees give you a different amount of wood &lt;span style=&quot;font-style: italic;&quot;&gt;M&lt;/span&gt;&lt;sub&gt;1&lt;/sub&gt;, ..., &lt;span style=&quot;font-style: italic;&quot;&gt;M&lt;/span&gt;&lt;sub style=&quot;font-style: italic;&quot;&gt;i&lt;/sub&gt;.</content:encoded>
	<dc:date>2007-11-06T15:02:04+00:00</dc:date>
	<dc:creator>MiP</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-786333285568106173.post-2313336436402789910">
	<title>WebDiarios de Motocicleta: Napkins</title>
	<link>http://infoweekly.blogspot.com/2007/11/napkins.html</link>
	<content:encoded>The inexhaustible &lt;a href=&quot;http://www.cs.rutgers.edu/%7Emuthu/&quot;&gt;Muthu&lt;/a&gt; came up with the fantastic &lt;a href=&quot;http://mysliceofpizza.blogspot.com/2007/11/formal-call-for-napkin-notes-and-other.html&quot;&gt;idea&lt;/a&gt; of assembling napkins used for research-related work. This could turn out to be very interesting.&lt;br /&gt;&lt;br /&gt;Here is my own contribution: a Starbucks napkin filled during a 2 hour caffeinated discussion with &lt;a href=&quot;http://www.mit.edu/%7Eandoni/&quot;&gt;Alex&lt;/a&gt;. It contains the proof of a communication-complexity lower bound for near-neighbor search in L&lt;sub&gt;∞&lt;/sub&gt;, matching the &lt;a href=&quot;http://people.csail.mit.edu/indyk/li3.ps&quot;&gt;current upper bound&lt;/a&gt; of &lt;a href=&quot;http://people.csail.mit.edu/indyk/&quot;&gt;Piotr&lt;/a&gt; from FOCS'98. We were working on this problem during our summer internship at IBM Almaden (yes, yes, we're officemates but we had to go to California to write a paper together).  As you might expect, the fact that we're now trying to finish the details has some correlation with the &lt;a href=&quot;http://research.microsoft.com/research/sv/STOC08/stoc2008-cfp.htm&quot;&gt;Nov 19&lt;/a&gt; fireworks party :)&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;http://bp3.blogger.com/_u9iBO1Zjoks/Ry_EEbftlQI/AAAAAAAAAFA/I4TJcza4erY/s1600-h/Scan001.JPG&quot;&gt;&lt;img style=&quot;margin: 0px auto 10px; display: block; text-align: center; cursor: pointer; width: 399px; height: 485px;&quot; src=&quot;http://bp3.blogger.com/_u9iBO1Zjoks/Ry_EEbftlQI/AAAAAAAAAFA/I4TJcza4erY/s400/Scan001.JPG&quot; alt=&quot;&quot; id=&quot;BLOGGER_PHOTO_ID_5129534080977573122&quot; border=&quot;0&quot; /&gt;&lt;/a&gt;</content:encoded>
	<dc:date>2007-11-05T20:50:13+00:00</dc:date>
	<dc:creator>MiP</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-8890204.post-5433972626114909100">
	<title>My Biased Coin: Graduate Students, Socialize!</title>
	<link>http://mybiasedcoin.blogspot.com/2007/11/graduate-students-socialize.html</link>
	<content:encoded>The &lt;a href=&quot;http://mybiasedcoin.blogspot.com/2007/11/breadth-requirements.html&quot;&gt;discussion that in my mind connected breadth requirements to social networks&lt;/a&gt; reminded me of my time as a graduate student at Berkeley, where there were lots of informal social opportunities for graduate students to meet and talk.  The CS grad student association had a weekly bagel/donut hour that was always well attended (mmmm...donuts....);  the theory students arranged a Friday afternoon seminar (I believe called TGIF) where theory students would present something (a practice talk, some ongoing research, a paper they liked), &lt;span style=&quot;font-style: italic;&quot;&gt;no faculty allowed&lt;/span&gt;;  there was a regular Friday pre-weekend grad student get-together that would be at a semi-randomly chosen local bar (usually upwards of a dozen or so people would show, with a high percentage of theorists, who didn't have to spend the weekend coding or running jobs);  a grad student poker game broke out once a month or so (again, with a high percentage of theorists, who'd add complexity to the games).&lt;br /&gt;&lt;br /&gt;Besides helping create a more pleasant graduate student atmosphere and experience, these activities let graduate students get to know more about each other and what they were doing.  And I believe expanding your own work-related social network pays dividends in the long run.&lt;br /&gt;&lt;br /&gt;I don't know what the current state of graduate student life is like these days, but if you don't think there's enough of this sort of stuff where you are, I encourage you, take action!  (Don't wait for the department to do it;  you'll do it better anyhow.)  Try to get a grad student poker game going.  Or a biweekly pub night.  Or start an informal seminar.  Or an open problems session.  Or whatever it is you want to do, where graduate students can just hang out together, without faculty, with it being about fun, instead of or combined with work.  Besides having more fun, you'll open up more opportunities for the serendipitous events in your work-life that lead to exciting projects.</content:encoded>
	<dc:date>2007-11-05T20:04:00+00:00</dc:date>
</item>
<item rdf:about="http://valis.cs.uiuc.edu/blog/?p=578">
	<title>Vanity of Vanities, all is Vanity: Colonless captions in latex</title>
	<link>http://valis.cs.uiuc.edu/blog/?p=578</link>
	<content:encoded>&lt;p&gt;Generating a caption in latex without a colon can be done by using the package &lt;a href=&quot;http://www.ctan.org/tex-archive/macros/latex/contrib/caption/&quot;&gt;caption&lt;/a&gt;.  Then all you have to do is&lt;/p&gt;
&lt;pre&gt;
   \captionsetup{labelsep=space}
   \begin{figure}
         \includegraphics...
         \caption{}
        \label{super:duper:wonderful:figure:oh:yeh}
   \end{figure}
   \captionsetup{labelsep=default}
&lt;/pre&gt;
&lt;p&gt;This is useful if you have a very short figure that you dont want to put any text into the caption, but you want to be able to refer to it.&lt;/p&gt;
&lt;p&gt;This package can do much more, naturally. Its probably installed on your system by default.&lt;/p&gt;</content:encoded>
	<dc:date>2007-11-05T18:58:44+00:00</dc:date>
</item>
<item rdf:about="tag:blogger.com,1999:blog-3722233.post-7923404146102863031">
	<title>Computational Complexity: It was a stupid question!!!!!!!!!! or...</title>
	<link>http://weblog.fortnow.com/2007/11/it-was-stupid-question-or.html</link>
	<content:encoded>On my last blog I asked the following:
TRUE OR FALSE:

&lt;blockquote&gt;
For every coloring of R (the reals) with a countable number of colors
there exists x,y,z,w of the same color such that
x+y=z+w.
&lt;/blockquote&gt;

&lt;br /&gt;
&lt;br /&gt;

And I pointed out that when I asked this in seminar I got
5 thought it was TRUE,
4 thought it was FALSE,
5 thought it was a STUPID QUESTION.

&lt;br /&gt;
&lt;br /&gt;

The answer is: ITS A STUPID QUESTION.
More rigorously the following is true and was proven by Erdos:

&lt;blockquote&gt;
The statment above is true iff the Continuum Hypothesis is false.
(See
&lt;a href=&quot;http://www.cs.umd.edu/~gasarch/BLOGPAPERS/radozfc.pdf&quot;&gt; this (pdf)&lt;/a&gt;
or
&lt;a href=&quot;http://www.cs.umd.edu/~gasarch/BLOGPAPERS/radozfc.ps&quot;&gt; this (ps)&lt;/a&gt;.
for an exposition of the proof.
&lt;/blockquote&gt;

&lt;br /&gt;
&lt;br /&gt;

SO, what to make of this?
This is a &lt;b&gt;natural&lt;/b&gt; question that is ind of  ZFC.
How Natural is it? Erdos worked on it, not some logician
looking around for a problem to be ind of ZFC.

&lt;br /&gt;
&lt;br /&gt;

Does this make us think CH is true or false?
Actually, more is known:
&lt;blockquote&gt;
Let L(x&lt;sub&gt;1&lt;/sub&gt;,..., x&lt;sub&gt;n&lt;/sub&gt;) be a linear form over
the reals (but not x&lt;sub&gt;1&lt;/sub&gt;-x&lt;sub&gt;2&lt;/sub&gt;).
If CH is true then there is a coloring of the
reals with a countable number of colors such that
there is no
e&lt;sub&gt;1&lt;/sub&gt;,..., e&lt;sub&gt;n&lt;/sub&gt; which are all the same
color such that
L(e&lt;sub&gt;1&lt;/sub&gt;,..., e&lt;sub&gt;n&lt;/sub&gt;)=0.
(Exposition of proof in same document linked to above.)
&lt;/blockquote&gt;

If CH is true then the entire theory of countable colorings
and linear forms is known. And boring.
If CH is false then much more interesting things happen.
Jacob Fox proved the following:

&lt;br /&gt;
&lt;br /&gt;

Let STAT(s) be the statement

&lt;blockquote&gt;
For every coloring of R with a countable number of colors
there exists x&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;2&lt;/sub&gt;, ..., x&lt;sub&gt;{s+3}&lt;/sub&gt; such that

they are all the same color, and

x&lt;sub&gt;1&lt;/sub&gt; + sx&lt;sub&gt;2&lt;/sub&gt; = x&lt;sub&gt;3&lt;/sub&gt; + x&lt;sub&gt;4&lt;/sub&gt; + ... + x&lt;sub&gt;{s+3}&lt;/sub&gt;
&lt;/blockquote&gt;

THEN

STAT(s) is true iff 2&lt;sup&gt;&amp;alefsym;&lt;sub&gt;0&lt;/sub&gt;&lt;/sup&gt; &gt; &amp;alefsym;&lt;sub&gt;s&lt;/sub&gt;

&lt;br /&gt;
&lt;br /&gt;

Jacob Fox is also (judging from his resume) not a logician.
He is a combinatorist. Actually he's a graduate student
so it may be too early to say what he &lt;i&gt;is&lt;/i&gt;.

&lt;br /&gt;
&lt;br /&gt;To determine CH should we use its consequences as
reasons for or against assuming it? Even if we do,
do you want the entire theory to be known and boring?
I ask this non-rhetorically.
See &lt;a href=&quot;http://www.math.rutgers.edu/~zeilberg/Opinion68.html&quot;&gt;Opinion 68
of Zeilberg's blog&lt;/a&gt; or
The papers of Penelope Maddy:
&lt;a href=&quot;http://www.cs.umd.edu/~gasarch/BLOGPAPERS/belaxioms1.pdf&quot;&gt;believing the
axioms I&lt;/a&gt;.
and
&lt;a href=&quot;http://www.cs.umd.edu/~gasarch/BLOGPAPERS/belaxioms2.pdf&quot;&gt;believing the
axioms II&lt;/a&gt;.</content:encoded>
	<dc:date>2007-11-05T13:48:20+00:00</dc:date>
	<dc:creator>GASARCH</dc:creator>
</item>
<item rdf:about="http://scottaaronson.com/blog/?p=287">
	<title>Shtetl-Optimized: Deoxyribononapproximability</title>
	<link>http://scottaaronson.com/blog/?p=287</link>
	<content:encoded>&lt;p&gt;&lt;a href=&quot;http://scottaaronson.com/blog/#114&quot;&gt;[Updates]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Alright, here&amp;#8217;s a problem for all you bioinformatistas and inapproximabistas out there, which was inspired by &lt;a href=&quot;http://www.overcomingbias.com/2007/11/natural-selecti.html&quot;&gt;this post of Eliezer Yudkowsky&lt;/a&gt; at &lt;a href=&quot;http://www.overcomingbias.com&quot;&gt;Overcoming Bias&lt;/a&gt; (see also the comments there).&lt;/p&gt;
&lt;p&gt;Let a DNA sequence be an element of {A,C,G,T}*, and suppose we&amp;#8217;re allowed the following primitive operations: (1) insert a base pair anywhere we want, (2) delete any substring, (3) reverse any substring, and (4) copy any substring into any other part of the string.  Then given a DNA sequence S, how hard is it to estimate the minimum number of operations needed to produce S starting from the empty string?&lt;/p&gt;
&lt;p&gt;Closely related is the following problem: by starting from the empty string and applying o(n) operations, can we produce a &amp;#8220;pseudorandom DNA sequence&amp;#8221; of length n &amp;#8212; that is, a sequence that can&amp;#8217;t be distinguished in polynomial time from a uniform random one?&lt;/p&gt;
&lt;p&gt;&lt;font size=&quot;-1&quot;&gt;(&lt;em&gt;Note 1:&lt;/em&gt; For both problems, we might also want to stipulate that every intermediate sequence should have size at most polynomial in n.  Or better yet, maybe one can prove that such an assumption is without loss of generality.)&lt;/font&gt;&lt;/p&gt;
&lt;p&gt;&lt;font size=&quot;-1&quot;&gt;(&lt;em&gt;Note 2:&lt;/em&gt; I&amp;#8217;m also very interested in what happens if we disallow the powerful operation of reversal.)&lt;/font&gt;&lt;/p&gt;
&lt;p&gt;For all I know, these problems might have trivial (or at any rate, known) answers; I just came up with them and haven&amp;#8217;t thought them through.&lt;/p&gt;
&lt;p&gt;What the problems are &lt;em&gt;really&lt;/em&gt; getting at is this: is the &amp;#8220;effective number of bits&amp;#8221; in your genome (that is, the number of bits from a polynomial-time algorithm&amp;#8217;s perspective) limited by how many ancestors you&amp;#8217;ve had since life on Earth began?  Or can it be vastly greater?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;font color=&quot;red&quot;&gt;&lt;a title=&quot;114&quot; name=&quot;114&quot;&gt;&lt;/a&gt;Update (11/4):&lt;/font&gt;&lt;/strong&gt;  Rereading the last few paragraphs of Eliezer&amp;#8217;s post, I see that he actually argues for his central claim &amp;#8212; that the human genome can&amp;#8217;t contain more than 25MB of &amp;#8220;meaningful DNA&amp;#8221; &amp;#8212; on different (and much stronger) grounds than I thought!  My apologies for not reading more carefully.&lt;/p&gt;
&lt;p&gt;In particular, the argument has nothing to do with the number of generations since the dawn of time, and instead deals with the maximum number of DNA bases that can be simultaneously protected, &lt;em&gt;in steady state&lt;/em&gt;, against copying errors.  According to Eliezer, copying a DNA sequence involves a ~10&lt;sup&gt;-8&lt;/sup&gt; probability of error per base pair, which &amp;#8212; because only O(1) errors per generation can be corrected by natural selection &amp;#8212; yields an upper bound of ~10&lt;sup&gt;8&lt;/sup&gt; on the number of &amp;#8220;meaningful&amp;#8221; base pairs in any given genome.&lt;/p&gt;
&lt;p&gt;However, while this argument is much better than my straw-man based on the number of generations, there&amp;#8217;s still an interesting loophole.  Even with a 10&lt;sup&gt;-8&lt;/sup&gt; chance of copying errors, one could imagine a genome reliably encoding far more than 10&lt;sup&gt;8&lt;/sup&gt; bits (in fact, arbitrarily many bits) by using an error-correcting code.  I&amp;#8217;m not talking about the &amp;#8220;local&amp;#8221; error-correction mechanisms that we know DNA has, but about something more global &amp;#8212; by which, say, copying errors in any small set of genes could be completely compensated by other genes.   The interesting question is whether natural selection could read the syndrome of such a code, and then correct it, using O(1) randomly-chosen insertions, deletions, transpositions, and reversals.  I admit that this seems unlikely, and that even if it&amp;#8217;s possible in principle, it&amp;#8217;s probably irrelevant to real biology.  For apparently there are examples where changing even a single base pair leads to horrible mutations.  And on top of that, we can&amp;#8217;t have the error-correcting code be &lt;em&gt;too&lt;/em&gt; good, since otherwise we&amp;#8217;ll suppress beneficial mutations!&lt;/p&gt;
&lt;p&gt;Incidentally, Eliezer&amp;#8217;s argument makes the falsifiable prediction that we shouldn&amp;#8217;t find &lt;em&gt;any&lt;/em&gt; organism, &lt;em&gt;anywhere&lt;/em&gt; in nature, with more than 25MB of functional DNA.  Does anyone know of a candidate counterexample?  (I know there are organisms with far more than humans&amp;#8217; 3 billion base pairs, but I have no idea how many of the base pairs are functional.)&lt;/p&gt;
&lt;p&gt;Lastly, in spite of everything above, I&amp;#8217;d still like a solution to my &amp;#8220;pseudorandom DNA sequence&amp;#8221; problem.  For &lt;em&gt;if&lt;/em&gt; the answer were negative &amp;#8212; if given any DNA sequence, one could efficiently reconstruct a nearly-optimal sequence of insertions, transpositions, etc. producing it &amp;#8212; then even my original straw-man misconstrual of Eliezer&amp;#8217;s argument could put up a decent fight!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;font color=&quot;red&quot;&gt;Update (11/5):&lt;/font&gt;&lt;/strong&gt; Piotr Indyk pointed me to a &lt;a href=&quot;http://www.cs.sfu.ca/~funda/PUBLICATIONS/fsttcs.ps&quot;&gt;paper&lt;/a&gt; by Ergün, Muthukrishnan, and Sahinalp from FSTTCS&amp;#8217;2003, which basically solves my problem in the special case of no reversals.   It turns out that you can estimate the number of insert, delete, and copy operations needed to produce a given DNA sequence to within a factor of 4, by just applying Lempel-Ziv compression to the sequence.  Thanks, Piotr!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;font color=&quot;red&quot;&gt;Another Update (11/5):&lt;/font&gt;&lt;/strong&gt; Andy Drucker has pointed out that, in the case where reversals &lt;em&gt;are&lt;/em&gt; allowed, we can approximate the number of insert, delete, copy, and reverse operations needed to produce a given DNA sequence to within a factor of 16, by combining the Lempel-Ziv approach of Ergün et al. with a clever trick: maintain both the sequence and its reversal at all times!  Interestingly, though, this trick &lt;em&gt;doesn&amp;#8217;t&lt;/em&gt; seem to work for transforming one sequence into another (a more general problem than I asked about, and the one considered by Ergün et al).&lt;/p&gt;</content:encoded>
	<dc:date>2007-11-04T20:50:24+00:00</dc:date>
</item>
<item rdf:about="tag:blogger.com,1999:blog-21129445.post-201749644150278311">
	<title>my slice of pizza: Parallel Algorithms</title>
	<link>http://mysliceofpizza.blogspot.com/2007/11/parallel-algorithms.html</link>
	<content:encoded>Some of us worked on parallel algorithms theory a long time ago, and many others on parallel systems. Parallel computing theory continues to remain in the backdrop, providing  a hum to the motor ride that Theory of Computing is on;  the (mesh, hypercube, connection) machines have disappeared, occupying real estate somewhere; people reinvented themselves, some went on to wireless networking, others to internet algorithms and beyond; wonder what happened to the software (BSP-based or otherwise). &lt;br /&gt;&lt;br /&gt;The word on the street is that there is a revival, with the trend of multi-core processors. I hear that compilers for these systems are a rave. Will we see beautiful parallel algorithmics again, with list ranking, Euler Tour techniques and &lt;a href=&quot;http://portal.acm.org/citation.cfm?id=383347&amp;dl=&quot;&gt;randomized matching&lt;/a&gt;?</content:encoded>
	<dc:date>2007-11-04T12:07:17+00:00</dc:date>
	<dc:creator>metoo</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-6555947.post-4183661568479832784">
	<title>The Geomblog: While I wait for inspiration to strike..</title>
	<link>http://geomblog.blogspot.com/2007/11/while-i-wait-for-inspiration-to-strike.html</link>
	<content:encoded>I need to find all the &lt;a href=&quot;http://mysliceofpizza.blogspot.com/2007/11/formal-call-for-napkin-notes-and-other.html&quot;&gt;napkins I scribble things&lt;/a&gt; on...&lt;br /&gt;&lt;br /&gt;What a great idea ! &lt;a href=&quot;http://postsecret.blogspot.com/&quot;&gt;PostSecret&lt;/a&gt; for the researcherati....&lt;div class=&quot;feedflare&quot;&gt;
&lt;a href=&quot;http://feeds.feedburner.com/~f/TheGeomblog?a=SPTuG9B&quot;&gt;&lt;img src=&quot;http://feeds.feedburner.com/~f/TheGeomblog?i=SPTuG9B&quot; border=&quot;0&quot; /&gt;&lt;/a&gt;
&lt;/div&gt;</content:encoded>
	<dc:date>2007-11-04T00:44:01+00:00</dc:date>
	<dc:creator>Suresh</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-24557460.post-4632993069802503752">
	<title>in theory: Harder, Better, Faster, Stronger</title>
	<link>http://in-theory.blogspot.com/2007/11/harder-better-faster-stronger.html</link>
	<content:encoded>An amazing video to Daft Punk's &lt;i&gt;Harder, Better, Faster, Stronger&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Don't be discouraged by the slow first minute; it does get better, faster, and harder.&lt;br /&gt;&lt;br /&gt;Doing the same with a different Daft Punk song, however, can be &lt;a href=&quot;http://www.youtube.com/watch?v=nPLOiBM8hLk&quot;&gt;less impressive&lt;/a&gt;.</content:encoded>
	<dc:date>2007-11-03T21:58:00+00:00</dc:date>
</item>
<item rdf:about="tag:blogger.com,1999:blog-21129445.post-4258004195073149003">
	<title>my slice of pizza: Guest blog by a summer NYer</title>
	<link>http://mysliceofpizza.blogspot.com/2007/11/guest-blog-by-nyer-amit-chakrabarti.html</link>
	<content:encoded>&lt;span style=&quot;font-style:italic;&quot;&gt;Guest blog by the NYer, &lt;a href=&quot;http://www.cs.dartmouth.edu/%7Eac/&quot;&gt;Amit Chakrabarti&lt;/a&gt;: he dalayed a little, I dallied a lot, and this post appears now, several months after his visit, and the summer, alas, already gone.&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;http://bp1.blogger.com/_P70CDtPH4Rs/Ry1JoATKpVI/AAAAAAAAAQQ/3LJEAY3GWNQ/s1600-h/DSC_0054.JPG&quot;&gt;&lt;img style=&quot;float:left; margin:0 10px 10px 0;cursor:pointer; cursor:hand;&quot; src=&quot;http://bp1.blogger.com/_P70CDtPH4Rs/Ry1JoATKpVI/AAAAAAAAAQQ/3LJEAY3GWNQ/s200/DSC_0054.JPG&quot; border=&quot;0&quot; alt=&quot;&quot; id=&quot;BLOGGER_PHOTO_ID_5128836502268847442&quot; /&gt;&lt;/a&gt;A visit to New York somehow seems like a return home to this &lt;a href=&quot;http://www.urbandictionary.com/define.php?term=bangalorean&quot;&gt;Bangalorean&lt;/a&gt;, despite the fact that until summer 2007, he had never lived there.  Thanks to  &lt;a href=&quot;http://www.cs.rutgers.edu/~muthu/braids2.jpg&quot;&gt;The Braided Algorithmus&lt;/a&gt;, that has now changed.  It was a real pleasure living in midtown Manhattan during July 2007 and interacting with the sea of talent at Google NYC.&lt;br /&gt;&lt;br /&gt;New York somehow manages to show you something fascinating every day. On this Fourth of July, the Sight of the Day was not the fireworks on the East River (quite spectacular, thank you), but this rather prominent advertisement of &lt;a href=&quot;http://www.vldb.org/conf/2003/papers/S15P01.pdf&quot;&gt;some of my host's research&lt;/a&gt; (See the photo).</content:encoded>
	<dc:date>2007-11-03T21:34:24+00:00</dc:date>
	<dc:creator>metoo</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-21129445.post-6542647513654459695">
	<title>my slice of pizza: Formal Call for Napkin, Notes and Other Math Scribblings</title>
	<link>http://mysliceofpizza.blogspot.com/2007/11/formal-call-for-napkin-notes-and-other.html</link>
	<content:encoded>&lt;a href=&quot;http://www.barik.net/journalimg/2005-08-18/napkin.jpg&quot;&gt;&lt;img style=&quot;margin: 0pt 10px 10px 0pt; float: left; cursor: pointer; width: 200px;&quot; src=&quot;http://www.barik.net/journalimg/2005-08-18/napkin.jpg&quot; alt=&quot;&quot; border=&quot;0&quot; /&gt;&lt;/a&gt;Napkins are special: they are easily available in restaurants, cafes, and bars. While regular folks use  them to make lists and progress on chores or  get telephone numbers and progress human race, researchers use them to do math and progress knowledge. Somewhere between  chores, race and knowledge, comes entertainment: there are napkin &lt;a href=&quot;http://books.google.com/books?id=W6BJtPQYxMsC&amp;dq=dinner+for+architects+a+collection+of+napkin+sketches&amp;pg=PP1&amp;ots=XhqXbY4_R2&amp;sig=h0deCVdJiFNIy9733jgmyyAa4xI&amp;prev=http://www.google.com/search%3Fq%3DDinner%2Bfor%2BArchitects%253A%2BA%2BCollection%2Bof%2BNapkin%2BSketches%26sourceid%3Dnavclient-ff%26ie%3DUTF-8%26rlz%3D1B3GGGL_enIN176US243&amp;sa=X&amp;oi=print&amp;ct=title&amp;cad=one-book-with-thumbnail&quot;&gt;sketches&lt;/a&gt;, napkin &lt;a href=&quot;http://www.cafewriting.com/2007/10/10/scribble-on-a-digital-napkin/&quot;&gt;writing&lt;/a&gt;, napkin &lt;a href=&quot;http://www.napkinproject.co.uk/&quot;&gt;art&lt;/a&gt;, etc.&lt;br /&gt;&lt;br /&gt;I am very excited about math we do in scribbles, on pieces of paper we can easily find: napkins, receipts, business cards, or whatever.&lt;br /&gt;&lt;br /&gt;&lt;span style=&quot;color: rgb(255, 0, 0);&quot;&gt;So, here is a formal call for my pet project:&lt;/span&gt; please photograph or scan such gems, and mail (smewtoo@gmail.com) them to me, with your story, if you are willing to share, and I will collect them and create an online exhibition of how in these driblets, we force math and methods to progress.</content:encoded>
	<dc:date>2007-11-03T21:08:29+00:00</dc:date>
	<dc:creator>metoo</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-21129445.post-6015739328873969962">
	<title>my slice of pizza: The Week Ends</title>
	<link>http://mysliceofpizza.blogspot.com/2007/11/week-ends.html</link>
	<content:encoded>Some weeks have setbacks, stories of friends' relationships that struggle at the threshold, and multiple murders.  About setbacks, all I can say is a few weeks before a conference deadline, we, the researchers, sift through the pile of papers and sometimes a gem emerges, other times one gets a paper cut. I can speak about the murders more. On Friday, I chose to watch the movie &lt;a href=&quot;http://www.angelikafilmcenter.com/angelika_film.asp?ID=991r9x8.3q2165057wn03318t.25&quot;&gt;Before the Devil Knows You are Dead&lt;/a&gt;, a tale of murders, more or less projected on to family members, very disturbing! Then today, in a spur, I went to watch a &lt;a href=&quot;http://www.theatresource.org/&quot;&gt;lowkey adaptation&lt;/a&gt;  (&lt;span style=&quot;font-style: italic;&quot;&gt;Wicked Tavern Tales&lt;/span&gt;; Welcome to Hell's Hound... where the Beer Flows Freely and the Tales are Tall... Or Are They? Written by Greg Oliver Bodine; Directed by Amber Estes) of three tales due to Edgar Allan Poe, of  cats, wine connoisseurs and men with cat-like eyes, all murdered, bricked off into cellars, plaster walls or simply stifled, but their sounds exact revenge.</content:encoded>
	<dc:date>2007-11-03T19:19:28+00:00</dc:date>
	<dc:creator>metoo</dc:creator>
</item>
<item rdf:about="http://valis.cs.uiuc.edu/blog/?p=577">
	<title>Vanity of Vanities, all is Vanity: UIUC CS department email blues</title>
	<link>http://valis.cs.uiuc.edu/blog/?p=577</link>
	<content:encoded>&lt;p&gt;So, the CS department now uses an exchange server that just feels the need to rewrite your name in the email headers to what it considers to be your true name. Which might be your true name, but is surely not what you want to be known as (for example, your biggest secret is that your middle name is Caligula). In fact, in some cases nobody even can recognize a person if they use their official name.&lt;/p&gt;
&lt;p&gt;The trick to overcome this problem, is to send the outgoing email using the department IMAP server &lt;a href=&quot;http://mail.cs.uiuc.edu/&quot;&gt;http://mail.cs.uiuc.edu/&lt;/a&gt;. This is almost enough. Sadly, if your email is intended for people inside the department, the exchange server still rewrite your email address. The trick is to configure your email program so that it uses an email address that the exchange server does not know (since it knows only one such email address).  Thus, if your email address is shogi@uiuc.edu, change it to shogi@cs.uiuc.edu (or vice versa). Note, that you would have to change both the &amp;#8220;From&amp;#8221; header email address and the &amp;#8220;Reply To&amp;#8221; email address for it to work.&lt;/p&gt;
&lt;p&gt;In fact, it might be enough just to flip the email address - the IMAP server might not be necessary&amp;#8230;&lt;/p&gt;</content:encoded>
	<dc:date>2007-11-02T21:43:36+00:00</dc:date>
</item>
<item rdf:about="tag:blogger.com,1999:blog-8890204.post-7279019469812918459">
	<title>My Biased Coin: Breadth Requirements</title>
	<link>http://mybiasedcoin.blogspot.com/2007/11/breadth-requirements.html</link>
	<content:encoded>In our department we're looking at the class requirements for Ph.D. students, and in particular breadth requirements.&lt;br /&gt;&lt;br /&gt;I'll state clearly that my opinion is that breadth requirements are a good thing.  Breadth requirements for Ph.D. students are like vegetables for kids -- they don't always like them, but they're good for them.  When I was at Berkeley the breadth requirements were reasonably onerous and I think they were very positive for me (even if, as contemporaries will suggest, it seemed like I slept through most of them).  I especially think it's good for theoretical people to have a background in many areas of computer science outside theory, and classes are arguably the best way to get that background.   And certainly I would argue that people in systems need some theory in their background.&lt;br /&gt;&lt;br /&gt;I'd enjoy hearing arguments from the other side.  You can argue that breadth requirements in general are a bad idea, or specifically that theory people shouldn't have to take classes in other CS areas (they should be allowed to just do all theory!), or that systems people shouldn't have to take a theory class.  Any EE readers or readers from other areas should chime in with arguments based on their experiences as well!</content:encoded>
	<dc:date>2007-11-02T21:19:00+00:00</dc:date>
</item>
<item rdf:about="tag:blogger.com,1999:blog-3722233.post-3692047415217655802">
	<title>Computational Complexity: Equations and Colorings: Rado's theorem</title>
	<link>http://weblog.fortnow.com/2007/11/equations-and-colorings-rados-theorem.html</link>
	<content:encoded>Is the following TRUE or FALSE?

&lt;blockquote&gt;
For every 17-coloring of N (the naturals- not including 0)
there exists x, y, z such that
x,y,z are distinct x,y,z that are  same color such that
2x+3x-6z = 0
&lt;/blockquote&gt;

It turns out that this is FALSE.

We'll call a set b&lt;sub&gt;1&lt;/sub&gt;,...,b&lt;sub&gt;n&lt;/sub&gt; REGULAR if

&lt;blockquote&gt;
for every c, for every c-coloring of N, there exists
x&lt;sub&gt;1&lt;/sub&gt;,....,x&lt;sub&gt;n&lt;/sub&gt; such that

x&lt;sub&gt;1&lt;/sub&gt;,....,x&lt;sub&gt;n&lt;/sub&gt; are all the same color, and

b&lt;sub&gt;1&lt;/sub&gt;x&lt;sub&gt;1&lt;/sub&gt; + ... + b&lt;sub&gt;n&lt;/sub&gt;x&lt;sub&gt;n&lt;/sub&gt; = 0

&lt;/blockquote&gt;

The following is known as (abridged) Rado's Theorem.
Rado proved it in 1933.

&lt;blockquote&gt;
(b&lt;sub&gt;1&lt;/sub&gt;,...,b&lt;sub&gt;n&lt;/sub&gt;)
is regular iff some nonempty subset of the b&lt;sub&gt;i&lt;/sub&gt;'s
sum to 0.
&lt;/blockquote&gt;

For an exposition of the  proof see &lt;i&gt;Ramsey Theory&lt;/i&gt;
by Graham, Rothchild,and Spencer or see
&lt;a href=&quot;http://www.cs.umd.edu/~gasarch/BLOGPAPERS/rado.pdf&quot;&gt;my writeup&lt;/a&gt;

&lt;br /&gt;NOW- here is a question to which the answer is &lt;b&gt;known&lt;/b&gt;, and
I'll tell you the answer in my next post.

&lt;br /&gt;
&lt;br /&gt;

TRUE OR FALSE:

&lt;blockquote&gt;
For every coloring of R (the reals) with a countable number of colors
there exists distinct x,y,z,w

x,y,z,w same color

x+y=z+w.
&lt;/blockquote&gt;

When I asked this in seminar I got

&lt;ol&gt;
&lt;li&gt;
5 thought it was TRUE
&lt;li&gt;
4 thought it was FALSE
&lt;li&gt;
5 thought it was a STUPID QUESTION.
&lt;/li&gt;&lt;/li&gt;&lt;/li&gt;&lt;/ol&gt;

&lt;br /&gt;</content:encoded>
	<dc:date>2007-11-02T16:21:22+00:00</dc:date>
	<dc:creator>GASARCH</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-786333285568106173.post-907551030840605610">
	<title>WebDiarios de Motocicleta: A Numbers Game</title>
	<link>http://infoweekly.blogspot.com/2007/11/numbers-game.html</link>
	<content:encoded>I have said &lt;a href=&quot;http://infoweekly.blogspot.com/2007/09/chernoff-about-stocfocs.html&quot;&gt;before&lt;/a&gt; that I believe theory/CS is a numbers game. During a few discussions &lt;span style=&quot;font-style: italic;&quot;&gt;sub vino&lt;/span&gt;, I was challenged on this issue. Two main points seemed to emerge:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;What matters is your best result (L&lt;sub&gt;∞&lt;/sub&gt;).&lt;/li&gt;&lt;li&gt;&lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_0&quot;&gt;STOC&lt;/span&gt;/&lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_1&quot;&gt;FOCS&lt;/span&gt; get 10-15 obvious accepts, and a heavy tail of competent papers, among which it's hard to choose. If we care about making &lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_2&quot;&gt;STOC&lt;/span&gt;/&lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_3&quot;&gt;FOCS&lt;/span&gt; a quality measure, why not accept just those 10-15 papers?&lt;/li&gt;&lt;/ul&gt;One of my main points is that I don't actually trust ourselves to get either of these 2 schemes to work. The first degenerates into the power of public hype and recommendation letters, since none of us will actually &lt;span class=&quot;blsp-spelling-corrected&quot; id=&quot;SPELLING_ERROR_4&quot;&gt;solve&lt;/span&gt; P vs NP (or any other result that is universally considered important). The second degenerates into a bloody political struggle, since the top few places depend too much on randomness.&lt;br /&gt;&lt;br /&gt;I have seen enough examples arguing for pessimism, mostly while I was acting as a reviewer. But since I can't actually talk about those, let me give 2 personal examples.&lt;br /&gt;&lt;br /&gt;&lt;span style=&quot;font-weight: bold;&quot;&gt;Predecessor. &lt;/span&gt;In &lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_5&quot;&gt;STOC&lt;/span&gt;'06, Mikkel and I had our &lt;a href=&quot;http://web.mit.edu/%7Emip/www/papers/index.html#pred&quot;&gt;predecessor lower bound&lt;/a&gt;. The main claim to fame of this paper is that it was the first lower bound to &lt;span&gt;separate linear and polynomial space&lt;/span&gt;.&lt;br /&gt;&lt;br /&gt;When it comes to such a claim, in my opinion, it doesn't  actually matter if you remember anything from the data structures class you took in college. It doesn't matter if you know that the bound is showing optimality of a very &lt;a href=&quot;http://infoweekly.blogspot.com/2007/09/love-thy-predecessor-iii-van-emde-boas.html&quot;&gt;influential data structure&lt;/a&gt; from &lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_6&quot;&gt;FOCS&lt;/span&gt;'75. It doesn't matter if you know this was one of the most well-studied problems in data-structure lower bounds, and people were actually betting the previous lower bound was tight.&lt;br /&gt;&lt;br /&gt;No. Somebody comes and tells you that we've been toiling at this lower bound game for more than 20 years, and we haven't yet managed to show that a data structure with space &lt;span style=&quot;font-style: italic;&quot;&gt;n&lt;/span&gt;&lt;sup&gt;10&lt;/sup&gt; is sometimes better than one with space O(&lt;span style=&quot;font-style: italic;&quot;&gt;n&lt;/span&gt;). Now, they tell you, such a bound is finally here, and a vast array of problems from Algorithms 101 have switched from &lt;span class=&quot;blsp-spelling-corrected&quot; id=&quot;SPELLING_ERROR_7&quot;&gt;unapproachable&lt;/span&gt; to hopefully solvable. That should be a worthy result.&lt;br /&gt;&lt;br /&gt;How did it feel from the inside? This was a paper we had worked on for 2 years. Just the conceptual understanding that we needed to develop felt like one of the  longest journeys I have ever embarked on. And in the end, it was actually a clean result, which indeed needed conceptual developments, but not so much technical pain! Mikkel, who never ceases to amaze me with his youthful optimism, was betting on a best paper award. I was of course at a university, and had a better estimator of the community hype factor. I knew that &lt;a href=&quot;http://www.cs.huji.ac.il/%7Edinuri/mypapers/combpcp.pdf&quot;&gt;&lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_8&quot;&gt;Irit's&lt;/span&gt; PCP&lt;/a&gt; was unbeatable for an award, but I was still convinced our paper would go down with a splash.&lt;br /&gt;&lt;br /&gt;The actual outcome? Moderately positive reviews. Nobody took notice. And we weren't even invited to the special issue, which was a genuine shock for us.&lt;br /&gt;&lt;br /&gt;&lt;span style=&quot;font-weight: bold;&quot;&gt;Range Counting. &lt;/span&gt;&lt;span&gt;In &lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_9&quot;&gt;STOC&lt;/span&gt;'07, I had a &lt;/span&gt;&lt;a href=&quot;http://web.mit.edu/%7Emip/www/papers/index.html#sum2D&quot;&gt;&lt;span&gt;lower bound for 2&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;a href=&quot;http://web.mit.edu/%7Emip/www/papers/index.html#sum2D&quot;&gt;D range counting&lt;/a&gt;. After the predecessor paper, I quickly decided this was the next big goal in the field. Range queries are very, very well-studied, play a crucial role in our understanding of data structures and geometry, and are a major bridge to worlds outside theory, such as databases (see this &lt;a href=&quot;http://infoweekly.blogspot.com/2007/10/range-queries-i.html&quot;&gt;irrefutable argument&lt;/a&gt;). Armed with some appreciation of the techniques in the field, this also felt like a make-or-break problem: we were either going to solve it, or get stuck again for 20 years.&lt;br /&gt;&lt;br /&gt;Though I like to look at cell-probe lower bounds, range counting has a life of its own. This class of problems was studied from the dawn of computer science, way before our understanding of models crystallized. People have been proving many excellent lower bounds for range queries in the so-called &lt;span style=&quot;font-style: italic;&quot;&gt;&lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_10&quot;&gt;semigroup&lt;/span&gt; model&lt;/span&gt;: each input point is associated with weights from a &lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_11&quot;&gt;semigroup&lt;/span&gt;; the goal is to compute the sum in a &quot;range&quot; (rectangle, circle, &lt;span class=&quot;blsp-spelling-corrected&quot; id=&quot;SPELLING_ERROR_12&quot;&gt;wizard's&lt;/span&gt; hat etc), using only the &lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_13&quot;&gt;semigroup&lt;/span&gt; addition. In this model, we obtain a good understanding of geometry: it is enough to characterize a partial sum of a subset of points by the extreme points in a few directions. Addition only allows us to &quot;build up&quot; subsets, so we need to understand &lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_14&quot;&gt;composability&lt;/span&gt; of constant-dimensional shapes, dictated by such extreme points.&lt;br /&gt;&lt;br /&gt;While the bounds for &lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_15&quot;&gt;semigroups&lt;/span&gt; were making very nice progress, we remained clueless about even the slightly stronger group model, where subtractions are allowed. This is the gap between understanding low-dimensional geometry and understanding computation (a very high-dimensional, unstructured space, such as the &lt;span style=&quot;font-style: italic;&quot;&gt;n&lt;/span&gt;-&lt;span class=&quot;blsp-spelling-corrected&quot; id=&quot;SPELLING_ERROR_16&quot;&gt;dimensional&lt;/span&gt; space of linear combinations). Proving bounds in the group model became the first lower-bound question in the surveys of &lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_17&quot;&gt;Pankaj&lt;/span&gt; &lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_18&quot;&gt;Agarwal&lt;/span&gt; and Jeff Erickson. Thus, the group model seemed hard enough; never mind the cell-probe model, which is significantly stronger than a model with additions and subtractions.&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;About a year after predecessor (I had the final critical insight one week before &lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_19&quot;&gt;STOC&lt;/span&gt;), I managed to show an optimal lower bound for 2D range counting in the cell-probe and (of course) the group model. Again, it does not seem to me like a paper where you need to be an expert to understand that something interesting is happening. Citing from the abstract:&lt;br /&gt;&lt;blockquote&gt;Proving such bounds in the group model has been regarded as an important challenge at least since [&lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_20&quot;&gt;Fredman&lt;/span&gt;, &lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_21&quot;&gt;JACM&lt;/span&gt; 1982] and [&lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_22&quot;&gt;Chazelle&lt;/span&gt;, &lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_23&quot;&gt;FOCS&lt;/span&gt; 1986].&lt;br /&gt;&lt;/blockquote&gt;It seems to me that whenever somebody claims to address a challenge from &lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_24&quot;&gt;JACM&lt;/span&gt;'82 and &lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_25&quot;&gt;FOCS&lt;/span&gt;'86, the only possible answers are:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;You are lying.&lt;/li&gt;&lt;li&gt;The paper is wrong.&lt;br /&gt;&lt;/li&gt;&lt;li&gt;It seemed like an interesting problem back then, but things have changed and we no longer care. (... and I don't see this applying to range queries.)&lt;br /&gt;&lt;/li&gt;&lt;li&gt;Yes, this is a very good paper.&lt;br /&gt;&lt;/li&gt;&lt;/ul&gt;The actual outcome? Lukewarm reviews: &quot;seems like an interesting result, and there is a comma missing on page 7&quot;. No student paper award, which again I was expecting because my hype-meter was well-aware of &lt;a href=&quot;http://math.ias.edu/%7Eyekhanin/Papers/nice_PIR.pdf&quot;&gt;&lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_26&quot;&gt;Sergey's&lt;/span&gt; paper&lt;/a&gt;. (In another great proof of our intellectual maturity, this otherwise nice paper might go down in history for using &lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_27&quot;&gt;Mersenne&lt;/span&gt; primes, rather then understanding something about locally &lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_28&quot;&gt;decodable&lt;/span&gt; codes. Oh well, those who can't put primes in their abstracts might as well &lt;a href=&quot;http://scottaaronson.com/blog/?p=142&quot;&gt;stick to cannabis&lt;/a&gt;.)&lt;br /&gt;&lt;br /&gt;Finally, the paper was not invited to the special issue. By this time, it was not something that had never crossed my mind; it was just a disappointment.&lt;br /&gt;&lt;span style=&quot;font-weight: bold;&quot;&gt;&lt;br /&gt;&lt;br /&gt;&lt;/span&gt;&lt;span style=&quot;font-weight: bold;&quot;&gt;Conclusion.&lt;/span&gt; You should work on major open problems. If you solve them, it makes for papers which are reject-proof (well, almost). But don't count on that &quot;one big result&quot; plan, as you may be in for a nasty surprise. As a community, we are reliable enough to get some kind of average right, but we're not doing so well on L&lt;sub&gt;∞&lt;/sub&gt;.&lt;br /&gt;&lt;br /&gt;So when you get your first &lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_29&quot;&gt;STOC&lt;/span&gt;/&lt;span class=&quot;blsp-spelling-error&quot; id=&quot;SPELLING_ERROR_30&quot;&gt;FOCS&lt;/span&gt; paper, go open the champagne or light up the cigar. You are officially part of the game. Now, it's time to play.&lt;br /&gt;&lt;br /&gt;Next round: November 19.</content:encoded>
	<dc:date>2007-11-02T11:49:10+00:00</dc:date>
	<dc:creator>MiP</dc:creator>
</item>
<item rdf:about="http://scottaaronson.com/blog/?p=285">
	<title>Shtetl-Optimized: Unparadox Contest</title>
	<link>http://scottaaronson.com/blog/?p=285</link>
	<content:encoded>&lt;p&gt;In a recent talk at MIT, Umesh Vazirani appealed to the famous &lt;a href=&quot;http://en.wikipedia.org/wiki/Birthday_paradox&quot;&gt;Birthday Paradox&lt;/a&gt; to say that two random subsets of {1,&amp;#8230;,N}, each of size o(√N), probably wouldn&amp;#8217;t intersect each other.  Of course we all understood what he meant, but it occurred to me that Umesh was actually appealing to the Birthday &lt;em&gt;Unparadox&lt;/em&gt;: &amp;#8220;If you put three people in a room, chances are no two of them will have the same birthday.&amp;#8221;&lt;/p&gt;
&lt;p&gt;Once I realized that, I started seeing unparadoxes everywhere I looked:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Banach-Tarski Unparadox:&lt;/strong&gt; If you cut an orange into five pieces using a standard knife, then put them back together, the result will have exactly the same volume as the original orange.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Braess&amp;#8217; Unparadox:&lt;/strong&gt; If you add an extra lane to a highway, one possible result will be to decrease congestion.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hempel&amp;#8217;s Unparadox:&lt;/strong&gt; If you observe a bunch of ravens and find that all of them are black, this might increase your likelihood for the statement &amp;#8220;All ravens are black.&amp;#8221;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Russell&amp;#8217;s Unparadox:&lt;/strong&gt; The set of all sets that contain themselves as a member, might or might not contain itself as a member (either way is fine).&lt;/p&gt;
&lt;p&gt;In the spirit of my highly-successful &lt;a href=&quot;http://scottaaronson.com/blog/?p=40&quot;&gt;Best Umeshism&lt;/a&gt; and &lt;a href=&quot;http://scottaaronson.com/blog/?p=101&quot;&gt;Best Anthropicism&lt;/a&gt; contests (remember those?), I now open the floor to you: come up with the best unparadox!  The winner will receive absolutely nothing.  (If you have to ask what the point is, this contest isn&amp;#8217;t for you.)&lt;/p&gt;</content:encoded>
	<dc:date>2007-11-01T18:59:17+00:00</dc:date>
</item>
<item rdf:about="tag:blogger.com,1999:blog-24557460.post-1536275794160740147">
	<title>in theory: The &quot;Complexity Theory&quot; Proof of a Theorem of Green-Tao-Ziegler</title>
	<link>http://in-theory.blogspot.com/2007/11/complexity-theory-proof-of-theorem-of.html</link>
	<content:encoded>We want to prove that a dense subset of a pseudorandom set is indistinguishable from a truly dense set.&lt;br /&gt;&lt;br /&gt;Here is an example of what this implies: take a pseudorandom generator of output length $n$, choose in an arbitrary way a 1% fraction of the possible seeds of the generator, and run the generator on a random seed from this restricted set; then the output of the generator is indistinguishable from being a random element of a set of size $\frac 1 {100} \cdot 2^n$.&lt;br /&gt;&lt;br /&gt;(Technically, the theorem states the existence of a distribution of min-entropy $n - \log_2 100$, but one can also get the above statement by standard &quot;rounding&quot; techniques.)&lt;br /&gt;&lt;br /&gt;As a slightly more general example, if you have a generator $G$ mapping a length-$t$ seed into an output of length $n$, and $Z$ is a distribution of seeds of min-entropy at least $t-d$, then $G(Z)$ is indistinguishable from a distribution of min-entropy $n-d$. (This, however, works only if $d = O(\log n)$.)&lt;br /&gt;&lt;br /&gt;It's time to give a formal statement. Recall that we say that a distribution $D$ is $\delta$-dense in a distribution $R$ if &lt;br /&gt;&lt;br /&gt;$\forall x. Pr[R=x] \geq \delta \cdot Pr [D=x]$&lt;br /&gt;&lt;br /&gt;(Of course I should say &quot;random variable&quot; instead of &quot;distribution,&quot; or write things differently, but we are between friends here.)&lt;br /&gt;&lt;br /&gt;We want to say that if $F$ is a class of tests, $R$ is pseudorandom according to a moderately larger class $F'$, and $D$ is $\delta$-dense in $R$, then there is a distribution $M$ that is indistinguishable from $D$ according to $F$ and that is $\delta$-dense in the uniform distribution.&lt;br /&gt;&lt;br /&gt;The Green-Tao-Ziegler proof of this result becomes slightly easier in our setting of interest (where $F$ contains boolean functions) and gives the following statement:&lt;br /&gt;&lt;blockquote&gt;&lt;br /&gt;&lt;b&gt;Theorem (Green-Tao-Ziegler, Boolean Case)&lt;/b&gt;&lt;br /&gt;Let $\Sigma$ be a finite set, $F$ be a class of functions $f:\Sigma \to \{0,1\}$, $R$ be a distribution over $\Sigma$, $D$ be a $\delta$-dense distribution in $R$, $\epsilon&gt;0$ be given.&lt;br /&gt;&lt;br /&gt;Suppose that for every $M$ that is $\delta$-dense in $U_\Sigma$ there is an $f\in F$ such that&lt;br /&gt;$| Pr[f(D)=1] - Pr[f(M)] = 1| &gt;\epsilon$&lt;br /&gt;&lt;br /&gt;Then there is a function $h:\Sigma \rightarrow \{0,1\}$ of the form $h(x) = g(f_1(x),\ldots,f_k(x))$ where $k = poly(1/\epsilon,1/\delta)$ and $f_i \in F$ such that&lt;br /&gt;$| Pr [h(R)=1] - Pr [ h(U_\Sigma) =1] | &gt; poly(\epsilon,\delta)$&lt;br /&gt;&lt;/blockquote&gt;&lt;br /&gt;Readers should take a moment to convince themselves that the above statement is indeed saying that if $R$ is pseudorandom then $D$ has a model $M$, by equivalently saying that if no model $M$ exists then $R$ is not pseudorandom.&lt;br /&gt;&lt;br /&gt;The problem with the above statement is that $g$ can be arbitrary and, in particular, it can have circuit complexity exponential in $k$, and hence in $1/\epsilon$.&lt;br /&gt;&lt;br /&gt;In our proof, instead, $g$ is a linear threshold function, realizable by a $O(k)$ size circuit. Another improvement is that $k=poly(1/\epsilon,\log 1/\delta)$.&lt;br /&gt;&lt;br /&gt;Here is the proof by Omer Reingold, Madhur Tulsiani, Salil Vadhan, and me. Assume $F$ is closed under complement (otherwise work with the closure of $F$), then the assumption of the theorem can be restated without absolute values&lt;br /&gt;&lt;br /&gt;&lt;blockquote&gt;&lt;br /&gt;for every $M$ that is $\delta$-dense in $U_\Sigma$ there is an $f\in F$ such that&lt;br /&gt;$Pr[f(D)=1] - Pr[f(M) = 1] &gt;\epsilon$&lt;br /&gt;&lt;/blockquote&gt;&lt;br /&gt;&lt;br /&gt;We begin by finding a &quot;universal distinguisher.&quot;&lt;br /&gt;&lt;blockquote&gt;&lt;br /&gt;&lt;b&gt;Claim&lt;/b&gt;&lt;br /&gt;There is a function $\bar f:\Sigma \rightarrow [0,1]$ which is a convex combination of functions from $F$ and such that that for every $M$ that is $\delta$-dense in $U_\Sigma$,&lt;br /&gt;$E[\bar f(D)] - E[\bar f(M)]  &gt;\epsilon$&lt;br /&gt;&lt;/blockquote&gt;&lt;br /&gt;This can be proved via the min-max theorem for two-players games, or, equivalently, via linearity of linear programming, or, like an analyst would say, via the Hahn-Banach theorem.&lt;br /&gt;&lt;br /&gt;Let now $S$ be the set of $\delta |\Sigma|$ elements of $\Sigma$ where $\bar f$ is largest. We must have&lt;br /&gt;(1)  $E[\bar f(D)] - E[\bar f(U_S)] &gt;\epsilon$&lt;br /&gt;which implies that there must be a threshold $t$ such that&lt;br /&gt;(2)  $Pr[\bar f(D)\geq t] - Pr[\bar f(U_S) \geq t]  &gt;\epsilon$  &lt;br /&gt;So we have found a boolean distinguisher between $D$ and $U_S$. Next,&lt;br /&gt;we claim that the same distinguisher works between $R$ and $U_\Sigma$.&lt;br /&gt;&lt;br /&gt;By the density assumption, we have&lt;br /&gt;$Pr[\bar f(R)\geq t] \geq \delta \cdot Pr[\bar f(D) \geq t]$&lt;br /&gt;&lt;br /&gt;and since $S$ contains exactly a $\delta$ fraction of $\Sigma$, and  since the condition $\bar f(x) \geq t$ always fails outside of $S$ (why?), we then have&lt;br /&gt;$Pr[\bar f(U_\Sigma)\geq t] = \delta \cdot Pr[\bar f(U_S) \geq t]$&lt;br /&gt;and so&lt;br /&gt;(3)  $Pr[\bar f(R)\geq t] - Pr[\bar f(U_\Sigma) \geq t]  &gt;\delta \epsilon $  &lt;br /&gt;&lt;br /&gt;&lt;br /&gt;Now, it's not clear what the complexity of $\bar f$ is: it could be a convex combination involving &lt;i&gt;all&lt;/i&gt; the functions in $F$. However, by Chernoff bounds, there must be functions $f_1,\ldots,f_k$ with $k=poly(1/\epsilon,\log 1/\delta)$ such that $\bar f(x)$ is well approximated by $\sum_i f_i(x) / k$ for all $x$ but for an exceptional set having density less that, say, $\delta\epsilon/10$, according to both  $R$ and $U_\Sigma$.&lt;br /&gt;&lt;br /&gt;Now $R$ and $U_\Sigma$ are distinguished by the predicate $\sum_{i=1}^k f_i(x) \geq tk$, which is just a linear threshold function applied to a small set of functions from $F$, as promised.&lt;br /&gt;&lt;br /&gt;Actually I have skipped an important step: outside of the exceptional set, $\sum_i f_i(x)/k$ is going to be &lt;i&gt;close&lt;/i&gt; to $\bar f(x)$ but not identical, and this could lead to problems. For example, in (3) $\bar f(R)$ might typically be larger than $t$ only by a tiny amount, and $\sum_i f_i(x)/k$ might consistently underestimate $\bar f$ in $R$. If so, $Pr [ \sum_{i=1}^k f_i(R) \geq tk ]$ could be a completely different quantity from $Pr [\bar f(R)\geq t]$.&lt;br /&gt;&lt;br /&gt;To remedy this problem, we note that, from (1), we can also derive the more &quot;robust&quot; distinguishing statement&lt;br /&gt;(2') $Pr[\bar f(D)\geq t+\epsilon/2] - Pr[\bar f(U_S) \geq t]  &gt;\epsilon/2$&lt;br /&gt;from which we get&lt;br /&gt;(3') $Pr[\bar f(R)\geq t+\epsilon/2] - Pr[\bar f(U_\Sigma) \geq t]  &gt;\delta \epsilon/2 $  &lt;br /&gt;&lt;br /&gt;And now we can be confident that even replacing $\bar f$ with an approximation we still get a distinguisher.&lt;br /&gt;&lt;br /&gt;The statement needed in number-theoretic applications is stronger in a couple of ways. One is that we would like $F$ to contain bounded functions $f:\Sigma \rightarrow [0,1]$ rather than boolean-valued functions. Looking back at our proof, this makes no difference. The other is that we would like $h(x)$ to be a function of the form $h(x) = \Pi_{i=1}^k f_i(x)$ rather than a general composition of functions $f_i$. This we can achieve by approximating a threshold function by a polynomial of degree $poly(1/\epsilon,1/\delta)$ using the Weierstrass theorem, and then choose the most distinguishing monomial. This gives a proof of the following statement, which is equivalent to Theorem 7.1 in the &lt;a href=&quot;http://front.math.ucdavis.edu/0610.5050&quot;&gt;Tao-Ziegler paper&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;&lt;blockquote&gt;&lt;br /&gt;&lt;b&gt;Theorem (Green-Tao-Ziegler, General Case)&lt;/b&gt;&lt;br /&gt;Let $\Sigma$ be a finite set, $F$ be a class of functions $f:\Sigma \to [0,1]$, $R$ be a distribution over $\Sigma$, $D$ be a $\delta$-dense distribution in $R$, $\epsilon&gt;0$ be given.&lt;br /&gt;&lt;br /&gt;Suppose that for every $M$ that is $\delta$-dense in $U_\Sigma$ there is an $f\in F$ such that&lt;br /&gt;$| Pr[f(D)=1] - Pr[f(M)] = 1| &gt;\epsilon$&lt;br /&gt;&lt;br /&gt;Then there is a function $h:\Sigma \rightarrow \{0,1\}$ of the form $h(x) = \Pi_{i=1}^k f_i(x)$ where $k = poly(1/\epsilon,1/\delta)$ and $f_i \in F$ such that&lt;br /&gt;$| Pr [f(R)=1] - Pr [ f(U_\Sigma) =1] | &gt; exp(-poly(1/\epsilon,1/\delta))$&lt;br /&gt;&lt;/blockquote&gt;&lt;br /&gt;In this case, we too lose an exponential factor. Our proof, however, has some interest even in the number-theoretic setting because it is somewhat simpler than and genuinely different from the original one.</content:encoded>
	<dc:date>2007-11-01T17:51:00+00:00</dc:date>
</item>
<item rdf:about="tag:blogger.com,1999:blog-3722233.post-7713603410258963272">
	<title>Computational Complexity: Do we root for how a problem will go?</title>
	<link>http://weblog.fortnow.com/2007/11/do-we-root-for-how-problem-will-go.html</link>
	<content:encoded>&lt;br /&gt;
&lt;br /&gt;

When you are working on a problem do you have a rooting
interest in which way it goes? Sometimes yes, sometimes no.
A story:

&lt;br /&gt;
&lt;br /&gt;

A &lt;b&gt;3-free set&lt;/b&gt; is a set with no arithmetic progressions of length 3.
Large 3-free sets of {1,...,n } were used in the best known Matrix Multiplication
algorithm. It is known that there are such sets of size n&lt;sup&gt;1-o(1)&lt;/sup&gt; but
there cannot be such sets of size &amp;Omega;(n) (slight tighter results are known).

&lt;br /&gt;
&lt;br /&gt;

I was finishing up a
&lt;a href=&quot;http://www.cs.umd.edu/~gasarch/BLOGPAPERS/paper3ap.pdf&quot;&gt;paper&lt;/a&gt;
on large 3-free sets. The paper was not about applying these
to anything; however, there was a short sections that
mentioned some applications.
I needed to know, just for some refs and background knowledge,
if larger 3-free sets would lead to better Matrix Multiplication algorithms.
So I emailed some people involved with Matrix Mult and one of them,
Robert Kleinberg, responded.  To paraphase the emails back and fourth
he said the following (italics are mine):

&lt;blockquote&gt;
The known algorithm uses that there are 3-free sets of {1,...,n}
of size n&lt;sup&gt;{1-o(1)}&lt;/sup&gt;.
Improvements to the current constructions of large 3-free
sets will not help matrix mult algorithms.
To improve matrix mult algorithms you need sets with more complicated
conditions on them.
&lt;i&gt;Sorry the answer is not what you wanted it to be&lt;/i&gt;
&lt;/blockquote&gt;
Actually I was happy to know this. I did not really
have a rooting interest.
Do we root for a result do go a certain way?
Do we want to see P=NP (better algorithms) or
P\ne NP (better crypto)? (I'd go for better algorithms
and let the crypto people find other problems to base
systems on- some of which I think has already happened.)
Do we want to see P=BPP (confirm our current intuition)
or P\ne BPP (confirm our 1980 intuition)?
Do we want to see GI\in P or GI \notin P?
Do we want to see PH collapse or not collapse?
Do we have a rooting interest in any of these problems?

&lt;br /&gt;
&lt;br /&gt;

I would think algorithms people root for finding faster algorithms
rather than showing a problem is NP complete.
Complexity people are happy to either seperate or collapse classes.
If only we do it more often.</content:encoded>
	<dc:date>2007-11-01T12:14:16+00:00</dc:date>
	<dc:creator>GASARCH</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-21129445.post-2881972860195179604">
	<title>my slice of pizza: NY beats NY</title>
	<link>http://mysliceofpizza.blogspot.com/2007/10/ny-beats-ny.html</link>
	<content:encoded>&lt;a href=&quot;http://farm1.static.flickr.com/102/266042647_5c27b56034.jpg?v=0&quot;&gt;&lt;img style=&quot;margin: 0pt 10px 10px 0pt; float: left; cursor: pointer; width: 200px;&quot; src=&quot;http://farm1.static.flickr.com/102/266042647_5c27b56034.jpg?v=0&quot; alt=&quot;&quot; border=&quot;0&quot; /&gt;&lt;/a&gt;Halloween parade just finished, the floats have gone by, staged statements have ended, the cops have moved the barricades and cleared the streets, and the cleaning trucks are out sweeping up the city, but the theatrical mood of the village persists. This year, like many others, NY outdoes NY. The parade is only a small part. Everyone does a little something or a whole lot of big things, and fill the city block after block. The whole city of strangers collaborate and the result is a make-believe world of outsized masks and wigs, striking boots and costumes, self-made or paid-for, and just personalities. Some pictures from 06 &lt;a href=&quot;http://halloweenparadenyc.smugmug.com/gallery/2119479#109670513&quot;&gt;here&lt;/a&gt;.</content:encoded>
	<dc:date>2007-10-31T20:20:10+00:00</dc:date>
	<dc:creator>metoo</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-21129445.post-2365602615433074246">
	<title>my slice of pizza: Let us Assume and Accept</title>
	<link>http://mysliceofpizza.blogspot.com/2007/10/let-us-assume-and-accept.html</link>
	<content:encoded>&lt;a href=&quot;http://bp0.blogger.com/_P70CDtPH4Rs/RyNGEQTKpSI/AAAAAAAAAP4/X_XK8WEsAJU/s1600-h/it_st430.gif&quot;&gt;&lt;img style=&quot;margin: 0pt 10px 10px 0pt; float: left; cursor: pointer;&quot; src=&quot;http://bp0.blogger.com/_P70CDtPH4Rs/RyNGEQTKpSI/AAAAAAAAAP4/X_XK8WEsAJU/s200/it_st430.gif&quot; alt=&quot;&quot; id=&quot;BLOGGER_PHOTO_ID_5126017839786534178&quot; border=&quot;0&quot; /&gt;&lt;/a&gt;&lt;a href=&quot;http://bp1.blogger.com/_P70CDtPH4Rs/RyNGEgTKpTI/AAAAAAAAAQA/F1-dwwjakcc/s1600-h/it_st649.gif&quot;&gt;&lt;img style=&quot;margin: 0pt 10px 10px 0pt; float: left; cursor: pointer;&quot; src=&quot;http://bp1.blogger.com/_P70CDtPH4Rs/RyNGEgTKpTI/AAAAAAAAAQA/F1-dwwjakcc/s200/it_st649.gif&quot; alt=&quot;&quot; id=&quot;BLOGGER_PHOTO_ID_5126017844081501490&quot; border=&quot;0&quot; /&gt;&lt;/a&gt;I caught a few minutes of the &lt;a href=&quot;http://www.hbo.com/apps/schedule/ScheduleServlet?ACTION_DETAIL=DETAIL&amp;FOCUS_ID=635585&quot;&gt;201 version&lt;/a&gt; of the social sciences take on America by Mr. Wuhl in a classroom format on HBO. Mr. Wuhl does a taut, standup routine, debunks and rebunks myths of pop culture and history. He spins fun stories, and truth or not, explains many things (why do we hold a middle finger up: the English demonstrating they can still &quot;Pluck U&quot; to the French, it is a long story, imagine one of the many wars and the role of the middle finger in plucking longbow arrows).&lt;br /&gt;&lt;br /&gt;I was reminded of such an anecdote, this one due to Inimitable Luigi Laura. The English flag is the Cross of St George, patron saint of England and curiously, Genova. Story is, English ships borrowed the Genovese flag of the Cross of St. George to be under the protection of the powerful Genovese kingdom in the Mediterranean, and later adopted it as a national flag. Apparently, money was involved in this &quot;borrowing&quot;. Is it true? Who cares, it is a fun story. Here is the &quot;&lt;a href=&quot;http://www.crwflags.com/fotw/flags/it-genoa.html&quot;&gt;research&lt;/a&gt;&quot;, and some &quot;&lt;a href=&quot;http://www.francobampi.it/liguria/varie/welcome1992.htm&quot;&gt;quote&lt;/a&gt;&quot;.</content:encoded>
	<dc:date>2007-10-31T15:06:47+00:00</dc:date>
	<dc:creator>metoo</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-21129445.post-5657519165989673226">
	<title>my slice of pizza: Typical CA</title>
	<link>http://mysliceofpizza.blogspot.com/2007/10/typical-ca.html</link>
	<content:encoded>&lt;a href=&quot;http://quake.usgs.gov/recenteqs/Anim/latest_sf.gif&quot;&gt;&lt;img style=&quot;margin: 0pt 10px 10px 0pt; float: left; cursor: pointer; width: 200px;&quot; src=&quot;http://quake.usgs.gov/recenteqs/Anim/latest_sf.gif&quot; alt=&quot;&quot; border=&quot;0&quot; /&gt;&lt;/a&gt;&lt;br /&gt;I was in the Bay area for a few days. It was cloudy and grim as it is nearly every time I visit, with even some rain. This visit was unique: Over dinner yesterday, I managed to feel the vibrations, surmise relying on the collective consciousness of people around me that it was an earthquake, sprint out and feel sheepish bonding with others in the same situation. Magnitude 5.6 earthquake, and other stats at &lt;a href=&quot;http://quake.usgs.gov/recenteqs/Quakes/nc40204628.htm&quot;&gt;usgs site&lt;/a&gt;. Best things about CA are one's friends, aged, ageless, and new.</content:encoded>
	<dc:date>2007-10-31T15:04:29+00:00</dc:date>
	<dc:creator>metoo</dc:creator>
</item>
<item rdf:about="tag:blogger.com,1999:blog-8890204.post-2941888456432670699">
	<title>My Biased Coin: New Book : Algorithmic Game Theory</title>
	<link>http://mybiasedcoin.blogspot.com/2007/10/new-book-algorithmic-game-theory.html</link>
	<content:encoded>&lt;div&gt;I recently received my &quot;desk copy&quot; of the new book &lt;a href=&quot;http://www.amazon.com/gp/product/0521872820?ie=UTF8&amp;tag=michaelmitzen-20&amp;linkCode=as2&amp;camp=1789&amp;creative=9325&amp;creativeASIN=0521872820&quot;&gt;Algorithmic Game Theory&lt;/a&gt;&lt;img style=&quot;border: medium none ; margin: 0px;&quot; alt=&quot;&quot; src=&quot;http://www.assoc-amazon.com/e/ir?t=michaelmitzen-20&amp;l=as2&amp;o=1&amp;a=0521872820&quot; border=&quot;0&quot; height=&quot;1&quot; width=&quot;1&quot; /&gt;, edited by Nisan, Roughgarden, Tardos, and Vazirani. &lt;/div&gt;&lt;br /&gt;&lt;div&gt;While I haven't read it cover-to-cover yet, I'm very impressed by the book.  It's taken a large area with a fairly short history, and broken it up into reasonable-sized chunks each written by an expert, with most chunks covering a new and active research area.  For example, Michael Kearns writes about Graphical Games, Christos Papadimitriou explains the complexity of finding Nash equilibria, Jon Kleinberg discusses cascading behavior in networks and the corresponding economic issues, Ramesh Johari and David Parkes and Joan Feigenbaum and so many others have chapters on their specialties, and so on.  Overall I count 45 contributors!  The result is a solid tome that really combines breadth and depth to create a resource that I assume works well for people working in the area and is certainly useful for an outsider trying to look in and see what's going on.  There are also exercises in some chapters;  it could certainly be used as a textbook. &lt;/div&gt;&lt;br /&gt;&lt;div&gt; &lt;/div&gt;I'd like to see other books of this form, built up by a coalition of experts to cover emerging areas.  You do lose something with this approach -- for example, many concepts are defined multiple times in various chapters, and while the authors have made an effort pointing out relations among chapters, you don't really have the sense of coherence you get from most textbooks or books about research written by a single author.   On the other hand, you do get a much broader coverage of topics than you'd probably see from a single-author textbook, and I assume that it was easier to spread the workload among authors.  It's not clear to me any single author (or group of 3-4) could have put something like this together in any reasonable amount of time.  Kudos to the editors (and authors).&lt;br /&gt;&lt;div&gt; &lt;/div&gt;&lt;br /&gt;&lt;div&gt;What other topics could benefit from a treatment like this?&lt;/div&gt;</content:encoded>
	<dc:date>2007-10-31T14:08:00+00:00</dc:date>
</item>
<item rdf:about="tag:blogger.com,1999:blog-24557460.post-3045113359766124017">
	<title>in theory: Dense Subsets of Pseudorandom Sets</title>
	<link>http://in-theory.blogspot.com/2007/10/dense-subsets-of-pseudorandom-sets.html</link>
	<content:encoded>The Green-Tao theorem states that &lt;a href=&quot;http://front.math.ucdavis.edu/math.NT/0404188&quot;&gt;the primes contain arbitrarily long arithmetic progressions&lt;/a&gt;; its proof can be, somewhat inaccurately, broken up into the following two steps:&lt;br /&gt;&lt;br /&gt;Thm1: Every constant-density subset of a pseudorandom set of integers contains arbitrarily long arithmetic progressions.&lt;br /&gt;&lt;br /&gt;Thm2: The primes have constant density inside a pseudorandom set.&lt;br /&gt;&lt;br /&gt;Of those, the main contribution of the paper is the first theorem, a &quot;relative&quot; version of Szemeredi's theorem. In turn, its proof can be (even more inaccurately) broken up as&lt;br /&gt;&lt;br /&gt;Thm 1.1: For every constant density subset D of a pseudorandom set there is a &quot;model&quot; set M that has constant density among the integers and is indistinguishable from D.&lt;br /&gt;&lt;br /&gt;Thm 1.2 (Szemeredi) Every constant density subset of the integers contains arbitrarily long arithmetic progressions, and many of them.&lt;br /&gt;&lt;br /&gt;Thm 1.3 A set with many long arithmetic progressions cannot be indistinguishable from a set with none.&lt;br /&gt;&lt;br /&gt;Following this scheme is, of course, easier said than done. One wants to work with a definition of pseudorandomness that is weak enough that (2) is provable, but strong enough that the notion of indistinguishability implied by (1.1) is in turn strong enough that (1.3) holds. From now on I will focus on (1.1), which is a key step in the proof, though not the hardest.&lt;br /&gt;&lt;br /&gt;Recently, Tao and Ziegler proved that the &lt;a href=&quot;http://arxiv.org/abs/math/0610050&quot;&gt;primes contain arbitrarily long &quot;polynomial progressions&quot;&lt;/a&gt; (progressions where the increments are given by polynomials rather than linear functions, as in the case of arithmetic progressions). Their paper contains a very clean formulation of (1.1), which I will now (accurately, this time) describe. (It is Theorem 7.1 in the paper. The language I use below is very different but equivalent.)&lt;br /&gt;&lt;br /&gt;We fix a finite universe $\Sigma$; this could be $\{ 0,1\}^n$ in complexity-theoretic applications or $Z/NZ$ in number-theoretic applications. Instead of working with subsets of $\Sigma$, it will be more convenient to refer to probability distributions over $\Sigma$; if $S$ is a set, then $U_S$ is the uniform distribution over $S$. We also fix a family $F$ of &quot;easy&quot; function $f: \Sigma \rightarrow [0,1]$. In a complexity-theoretic applications, this could be the set of boolean functions computed by circuits of bounded size. We think of two distributions $X,Y$ as being $\epsilon$-indistinguishable according to $F$ if for every function $f\in F$ we have&lt;br /&gt;&lt;br /&gt;$| E [f(X)] - E[f(Y)] | \leq \epsilon$&lt;br /&gt;&lt;br /&gt;and we think of a distribution as pseudorandom if it is indistinguishable  from the uniform distribution $U_\Sigma$. (This is all standard in cryptography and complexity theory.)&lt;br /&gt;&lt;br /&gt;Now let's define the natural analog of &quot;dense subset&quot; for distributions. We say that a distribution $A$ is $\delta$-dense in $B$ if for every $x\in \Sigma$ we have&lt;br /&gt;&lt;br /&gt;$Pr [ B=x] \geq \delta Pr [A=x]$&lt;br /&gt;&lt;br /&gt;Note that if $B=U_T$ and $A=U_S$ for some sets $S,T$, then $A$ is $\delta$-dense in $B$ if and only if $S\subseteq T$ and $|S| \geq \delta |T|$.&lt;br /&gt;&lt;br /&gt;So we want to prove the following: &lt;br /&gt;&lt;br /&gt;&lt;blockquote&gt; &lt;b&gt;Theorem (Green, Tao, Ziegler)&lt;/b&gt;&lt;br /&gt;Fix a family $F$ of tests and an $\epsilon&gt;0$; then there is a &quot;slightly larger&quot; family $F'$ and an $\epsilon'&gt;0$ such that if $R$ is an $\epsilon'$-pseudorandom distribution according to $F'$ and $D$ is $\delta$-dense in $R$, then there is a distribution $M$ that is $\delta$-dense in $U_\Sigma$ and that is $\epsilon$-indistinguishable from $D$ according to $F$.&lt;br /&gt;&lt;/blockquote&gt;&lt;br /&gt;&lt;br /&gt;[The reader may want to go back to (1.1) and check that this is a meaningful formalization of it, up to working with arbitrary distributions rather than sets. This is in fact the &quot;inaccuracy&quot; that I referred to above.]&lt;br /&gt;&lt;br /&gt;In a complexity-theoretic setting, we would like to say that if $F$ is defined as all functions computable by circuits of size at most $s$, then $\epsilon'$ should be $poly (\epsilon,\delta)$ and $F'$ should contain only functions computable by circuits of size $s\cdot poly(1/\epsilon,1/\delta)$. Unfortunately, if one follows the proof and makes some simplifications asuming $F$ contains only boolean functions, one sees that $F'$ contains functions of the form $g(x) = h(f_1(x),\ldots,f_k(x))$, where $f_i \in F$, $k = poly(1/\epsilon,1/\delta)$, and $h$ could be arbitrary and, in general, have circuit complexity exponential in $1/\epsilon$ and $1/\delta$. Alternatively one may approximate $h()$ as a low-degree polynomial and take the &quot;most distinguishing monomial.&quot; This will give a version of the Theorem (which leads to the actual statement of Thm 7.1 in the Tao-Ziegler paper)  where $F'$ contains only functions of the form $\Pi_{i=1}^k f_i(x)$, but then $\epsilon'$ will be exponentially small in $1/\epsilon$ and $1/\delta$. This means that one cannot apply the theorem to &quot;cryptographically strong&quot; notions of pseudorandomness and indistinguishability, and in general to any setting where $1/\epsilon$ and $1/\delta$ are super-logarithmic (not to mention super-linear).&lt;br /&gt;&lt;br /&gt;This seems like an unavoidable consequence of the &quot;finitary ergodic theoretic&quot; technique of iterative partitioning and energy increment used in the proof, which always yields at least a singly exponential complexity.&lt;br /&gt;&lt;br /&gt;Omer Reingold, Madhur Tulsiani, Salil Vadhan and I have recently come up with a different proof where both $\epsilon'$ and the complexity of $F'$ are polynomial. This gives, for example, a new characterization of the notion of pseudoentropy. Our proof is quite in the spirit of Nisan's proof of Impagliazzo's &lt;a href=&quot;http://www.cs.ucsd.edu/~russell/hardcore.ps&quot;&gt;hard-core set theorem&lt;/a&gt;, and it is relatively simple. We can also deduce a version of the theorem where, as in Green-Tao-Ziegler, $F'$ contains only bounded products of functions in $F$. In doing so, however, we too incur an exponential loss, but the proof is somewhat simpler and demonstrates the applicability of complexity-theoretic techniques in arithmetic combinatorics.&lt;br /&gt;&lt;br /&gt;Since we can use (ideas from) a proof of the hard core set theorem to prove the Green-Tao-Ziegler result, one may wonder whether one can use the &quot;finitary ergodic theory&quot; techniques of iterative partitioning and energy increment to prove the hard-core set theorem. Indeed, we do this too. In our proof, the reduction loses a factor that is exponential in certain parameters (while other proofs are polynomial), but one also gets a more &quot;constructive&quot; result.&lt;br /&gt;&lt;br /&gt;If readers can stomach it, a forthcoming post will describe the complexity-theory-style proof  of the Green-Tao-Ziegler result as well as the ergodic-theory-style proof of the Impagliazzo hard core set theorem.</content:encoded>
	<dc:date>2007-10-31T01:39:00+00:00</dc:date>
</item>

</rdf:RDF>
