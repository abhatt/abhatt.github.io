<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-09-16T15:21:51Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>https://differentialprivacy.org/private-pac/</id>
    <link href="https://differentialprivacy.org/private-pac/" rel="alternate" type="text/html"/>
    <title>Differentially Private PAC Learning</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The study of differentially private PAC learning runs all the way from
its introduction in 2008 <a href="https://arxiv.org/abs/0803.0924" title="Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What Can We Learn Privately? FOCS 2008"><strong>[KLNRS08]</strong></a> to a best paper award at the
Symposium on Foundations of Computer Science (FOCS) this year <a href="https://arxiv.org/abs/2003.00563" title="Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. FOCS 2020"><strong>[BLM20]</strong></a>.
In this post, we’ll recap the history of this line of work, aiming for
enough detail for a rough understanding of the results and methods.</p>

<p>Before we get to the “what” and “how” of private PAC learning, it’s
worth thinking about the “why”. One motivation for this line of work is
that it neatly captures a fundamental question: does privacy in machine
learning come at a price? Machine learning is now sufficiently
successful and widespread for this question to have real import. But to
even start to address this question, we need a formalization of machine
learning that allows us to reason about possible trade-offs in a
rigorous way. Statistical learning theory, and its computational
formalization as PAC learning, provide one such clean and well-studied
model. We can therefore use PAC learning as a testbed whose insights we
might carry to other less idealized forms of learning.</p>

<p>With this motivation in mind, the rest of this post is structured as
follows. The first section covers the basics of the PAC model, and
subsequent sections gradually build up a chronology of results. When
possible, we give short sketches of the accompanying techniques.</p>

<h1 id="pac-learning">PAC Learning</h1>

<p>We’ll start with a brief overview of PAC learning absent any privacy
restrictions. Readers familiar with PAC learning can probably skip this
section while noting that</p>

<ol>
  <li>
    <p>(the cardinality version of) Occam’s razor is a baseline learner
using \(O(\log|\mathcal{H}|)\) samples,</p>
  </li>
  <li>
    <p>VC dimension characterizes non-private PAC learning,</p>
  </li>
  <li>
    <p>we’ll focus on the sample complexity of realizable PAC learning,</p>
  </li>
  <li>
    <p>we’ll usually omit dependencies on accuracy and success probability
parameters, and</p>
  </li>
  <li>
    <p>we’ll usually ignore computational efficiency.</p>
  </li>
</ol>

<p>For readers needing a refresher on PAC learning, the basic element of
the “probably approximately correct” (PAC) framework <a href="https://dl.acm.org/doi/10.1145/1968.1972" title="Leslie G Valiant. A theory of the learnable. Communications of the ACM, 1984"><strong>[Val84]</strong></a> is a
<em>hypothesis</em>. Each hypothesis is a function
\(h \colon \mathcal{X}\to \{-1,1\}\) mapping <em>examples</em> from some space
\(\mathcal{X}\) to binary labels. A collection of hypotheses is a
<em>hypothesis class</em> \(\mathcal{H}\), e.g., thresholds (a.k.a. perceptrons),
rectangles, conjunctions, and so on. In the <em>realizable</em> setting, a
learner receives examples drawn from some unknown distribution and
labeled by an unknown \(h^\ast \in \mathcal{H}\). The learner’s goal is to
with high probability (“probably”) output a hypothesis that mostly
matches the labels of \(h^\ast\) on future examples from the unknown example
distribution (“approximately correct”). In the <em>agnostic</em> setting,
examples are not necessarily labeled by any \(h
\in \mathcal{H}\), and the goal is only to output a hypothesis that
approximates the best error of any hypothesis from \(\mathcal{H}\). As
mentioned above, we focus on the realizable setting unless otherwise
specified. In the <em>proper</em> setting, the learner must output a hypothesis
from \(\mathcal{H}\) itself. In the <em>improper</em> setting, this requirement
is removed.</p>

<p>In general, we say an algorithm \((\alpha,\beta)\)-PAC learns
\(\mathcal{H}\) with sample complexity \(n\) if \(n\) samples are sufficient
to with probability at least \(1-\beta\) obtain error at most \(\alpha\)
over new examples from the distribution. For the purposes of this post,
we generally omit these dependencies on \(\alpha\) and \(\beta\), as they
typically vary little or not at all when switching between non-private
and private PAC learning.</p>

<p>Fortunately, we always have a simple baseline learner based on empirical
risk minimization: given a set of labeled examples, iterate over all
hypotheses \(h \in \mathcal{H}\), check how many of the labeled examples
each \(h\) mislabels, and output a hypothesis that mislabels the fewest
examples. Using this learner, which is sometimes called “Occam’s razor,”
\(O(\log|\mathcal{H}|)\) samples suffice to PAC learn \(\mathcal{H}\).</p>

<p>At the same time, \(|\mathcal{H}|\) is a pretty coarse measure of
hypothesis class complexity, as it would immediately rule out learning
any infinite hypothesis class (of which there are many). Thus, as you
might expect, we can do better. We do so using <em>VC dimension</em>.
\(\mathsf{VCD}\left(\mathcal{H}\right)\) is the size of the largest
possible collection of examples such that, for every labeling of the
examples, \(\mathcal{H}\) contains a hypothesis with that labeling. With
VC dimension, we can essentially swap \(\log|\mathcal{H}|\) with
\(\mathsf{VCD}\left(\mathcal{H}\right)\) in the Occam’s razor bound and
PAC learn with \(O(\mathsf{VCD}\left(\mathcal{H}\right))\) samples. In
fact, the “Fundamental Theorem of Statistical Learning” says that PAC
learnability (realizable or agnostic) is equivalent to finite VC
dimension. In this sense, \(\mathsf{VCD}\left(\mathcal{H}\right)\) is a
good measure of how hard it is to PAC learn \(\mathcal{H}\). As a
motivating example that will re-appear later, note that for the
hypothesis class of 1-dimensional thresholds over \(T\) points,
\(\log |\mathcal{H}| = \log T\), while
\(\mathsf{VCD}\left(\mathcal{H}\right)\) is only 1.</p>

<p><img alt="Example: a one-dimensional threshold function" src="https://differentialprivacy.org/images/thresh.png" style="margin: auto; display: block;" width="400"/>
An illustration of 1-dimensional thresholds. A given threshold is determined by some point \(x^\ast \in [T]\): any example \(x \leq x^\ast\) receives label \(-1\), and any example \(x &gt; x^\ast\) receives label 1.</p>

<h1 id="a-simple-private-pac-learner">A Simple Private PAC Learner</h1>

<p>It is straightforward to add a differential privacy constraint to the
PAC framework: the hypothesis output by the learner must be a
differentially private function of the labeled examples
\((x_1, y_1), \ldots, (x_n, y_n)\). That is, changing any one of the
examples — even to one with an inconsistent label — must not affect
the distribution over hypotheses output by the learner by too much.</p>

<p>Since we haven’t talked about any other PAC learner, we may as well
start with the empirical risk minimization-style Occam’s razor discussed
in the previous section, which simply selects a hypothesis that
minimizes empirical error. A private version becomes easy if we view
this algorithm in the right light. All it is doing is assigning a score
to each possible output (the hypothesis’ empirical error) and outputting
one with the best (lowest) score. This makes it a good candidate for
privatization by the <em>exponential mechanism</em> <a href="https://dl.acm.org/doi/10.1109/FOCS.2007.41" title="Frank McSherry, Kunal Talwar. Mechanism Design via Differential Privacy. FOCS 2007."><strong>[MT07]</strong></a>.</p>

<p>Recall that the exponential mechanism uses a scoring function over
outputs to release better outputs with higher probability, subject to
the privacy constraint. More formally, the exponential mechanism
requires a scoring function \(u(X,h)\) mapping (database, output) pairs to
real-valued scores and then selects a given output \(h\) with probability
proportional to \(\exp\left(\tfrac{\varepsilon
u(X,h)}{2\Delta(u)}\right)\). Thus a lower \(\varepsilon\) (stricter
privacy requirement) and larger \(\Delta(u) := \sup_h \sup_{X \sim X’} u(X,h) - u(X’,h) \) (scoring function more sensitive to changing one element in the database \(X\) to make \(X’\)) both lead to a more uniform (more
private) output distribution.</p>

<p>Fortunately for our PAC learning setting, empirical error is not a very
sensitive scoring function: changing one sample only changes empirical
error by 1. We can therefore use (negative) empirical error as our
scoring function \(u(X,h)\), apply the exponential mechanism, and get a
“private Occam’s razor.” This was exactly what Kasiviswanathan, Lee,
Nissim, Raskhodnikova, and Smith <a href="https://arxiv.org/abs/0803.0924" title="Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What Can We Learn Privately? FOCS 2008"><strong>[KLNRS08]</strong></a> did when they introduced
differentially private PAC learning in 2008. The resulting sample
complexity bounds differ from the generic Occam’s razor only by an
\(\varepsilon\) factor in the denominator, and
\(O(\log|\mathcal{H}|/\varepsilon)\) samples suffice to privately PAC
learn \(\mathcal{H}\).</p>

<p>Of course, our experience with non-private PAC learning suggests that we
shouldn’t be satisfied with this \(\log
|\mathcal{H}|\) dependence. Maybe VC dimension characterizes private PAC
learning, too?</p>

<h1 id="characterizing-pure-private-pac-learning">Characterizing Pure Private PAC Learning</h1>

<p>As it turns out, answering this question will take some time. We start
with a partial negative answer. Specifically, we’ll see a class with VC
dimension 1 and (a restricted form of) private sample complexity
arbitrarily larger than 1. We’ll also cover the first in a line of
characterization results for private PAC learning.</p>

<p>We first consider learners that satisfy <em>pure</em> privacy. Recall that pure
\((\varepsilon,0)\)-differential privacy forces output distributions that
may only differ by a certain \(e^\varepsilon\) multiplicative factor (like
the exponential mechanism above). The strictly weaker notion of
approximate \((\varepsilon,\delta)\)-differential privacy also allows a
small additive \(\delta\) factor. Second, we restrict ourselves to
<em>proper</em> learners, which may only output hypotheses from the learned
class \(\mathcal{H}\).</p>

<p>With these assumptions in place, in 2010, Beimel, Kasiviswanathan, and
Nissim <a href="https://dl.acm.org/doi/10.1007/978-3-642-11799-2_26" title="Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. TCC 2010"><strong>[BKN10]</strong></a> studied a hypothesis class called \(\mathsf{Point}_d\).
\(\mathsf{Point}_d\) consists of \(2^d\) hypotheses, one for each vector in
\(\{0,1\}^d\). Taking the set of examples \(\mathcal{X}\) to be \(\{0,1\}^d\)
as well, we define each hypothesis in \(\mathsf{Point}_d\) to label only
its associated vector as 1, and the remaining \(2^d-1\) examples as
-1. <a href="https://dl.acm.org/doi/10.1007/978-3-642-11799-2_26" title="Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. TCC 2010"><strong>[BKN10]</strong></a> showed that the hypothesis class \(\mathsf{Point}_d\) requires
\(\Omega(d)\) samples for proper pure private PAC learning. In contrast,
\(\mathsf{VCD}\left(\mathsf{Point}_d\right) = 1\), so this \(\Omega(d)\)
lower bound shows us that VC dimension does <em>not</em> characterize proper
pure private PAC learning.</p>

<p>This result uses the classic “packing” lower bound method, which powers
many lower bounds for pure differential privacy. The general packing
method is to first construct a large collection of databases which are
all “close enough” to each other but nonetheless all have different
“good” outputs. Once we have such a collection, we use <em>group privacy</em>.
Group privacy is a corollary of differential privacy that requires
databases differing in \(k\) elements to have \(k\varepsilon\)-close output
distributions. Because of group privacy, if we start with a collection
of databases that are close together, then the output distributions for
any two databases in the collection cannot be too different. This
creates a tension: utility forces the algorithm to produce different
output distributions for different databases, but privacy forces
similarity. The packing argument comes down to arguing that, unless the
databases are large, privacy wins out, and when privacy wins out then
there is some database where the algorithm probably produces a bad
output.</p>

<p>For \(\mathsf{Point}_d\), we sketch the resulting argument as follows.
Suppose we have an \(\varepsilon\)-private PAC learner that uses \(m\)
samples. Then we can define a collection of different databases of size
\(m\), one for each hypothesis in \(\mathsf{Point}_d\). By group privacy,
the output distribution for our private PAC learner changes by at most
\(e^{m\varepsilon}\) between any two of the databases in this collection.
Thus we can pick any \(h \in \mathsf{Point}_d\) and know that the
probability of outputting the wrong hypothesis is at least roughly
\(2^d \cdot e^{-m\varepsilon}\). Since we need this probability to be
small, rearranging implies \(m =
\Omega(d/\varepsilon)\).</p>

<p><a href="https://dl.acm.org/doi/10.1007/978-3-642-11799-2_26" title="Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. TCC 2010"><strong>[BKN10]</strong></a> then contrasted this result with an <em>improper</em> pure private PAC
learner. This learner applies the exponential mechanism to a class
\(\mathsf{Point}_d’\) of hypotheses derived from \(\mathsf{Point}_d\) —
but <em>not</em> necessarily a subset of \(\mathsf{Point}_d\) — gives an
improper pure private PAC learner with sample complexity \(O(\log
d)\). Since this learner is improper, it circumvents the “one database
per hypothesis” step of the packing lower bound. Moreover, <a href="https://dl.acm.org/doi/10.1007/978-3-642-11799-2_26" title="Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. TCC 2010"><strong>[BKN10]</strong></a> gave a
still more involved improper pure private PAC learner requiring only
\(O(1)\) samples. This separates proper pure private PAC learning from
improper pure private PAC learning. In contrast, the sample complexities
of proper and improper PAC learning absent privacy are the same up to
logarithmic factors in \(\alpha\) and \(\beta\).</p>

<p>In 2013, Beimel, Nissim, and Stemmer <a href="https://arxiv.org/abs/1402.2224" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of private learners. ITCS 2013"><strong>[BNS13]</strong></a> proved a more general
result. They gave the first characterization of pure (improper) private
PAC learning by defining a new hypothesis class measure called the
<em>representation dimension</em>, \(\mathsf{REPD}\left(\mathcal{H}\right)\).
Roughly, the representation dimension considers the collection of all
distributions \(\mathcal{D}\) over sets of hypotheses, not necessarily
from \(\mathcal{H}\), that “cover” \(\mathcal{H}\). By “cover,” we mean that
for any \(h
\in \mathcal{H}\), with high probability a set drawn from covering
distribution \(\mathcal{D}\) includes a hypothesis that mostly produces
labels that agree with \(h\). With this collection of distributions
defined, \(\mathsf{REPD}\left(\mathcal{H}\right)\) is the minimum over all
such covering distributions of the logarithm of the size of the largest
set in its support. Thus a hypothesis class that can be covered by a
distribution over small sets of hypotheses will have a small
representation dimension. With the notion of representation dimension in
hand, <a href="https://arxiv.org/abs/1402.2224" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of private learners. ITCS 2013"><strong>[BNS13]</strong></a> gave the following result:</p>

<blockquote>
  <p><strong>Theorem 1</strong> (<a href="https://arxiv.org/abs/1402.2224" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of private learners. ITCS 2013"><strong>[BNS13]</strong></a>). The sample complexity to pure private PAC learn \(\mathcal{H}\) is \(\Theta(\mathsf{REPD}\left(\mathcal{H}\right))\).</p>
</blockquote>

<p>Representation dimension may seem like a strange definition, but a
sketch of the proof of this result helps illustrate the connection to
private learning. Recall from our private Occam’s razor, and the
improper pure private PAC learner above, that if we can find a good and
relatively small set of hypotheses to choose from, then we can apply the
exponential mechanism and call it a day. It is exactly this kind of
“good set of hypotheses” that representation dimension aims to capture.
A little more formally, given an upper bound on
\(\mathsf{REPD}\left(\mathcal{H}\right)\), we know there is some covering
distribution whose largest hypothesis set is not too big. That means we
can construct a learner that draws a hypothesis set from this covering
distribution and applies the exponential mechanism to it. Just as we
picked up a \(\log|\mathcal{H}|\) sample complexity dependence using
private Occam’s razor, since \(\mathsf{REPD}\left(\mathcal{H}\right)\)
measures the logarithm of the size of the largest hypothesis set in the
support, this pure private learner picks up a
\(\mathsf{REPD}\left(\mathcal{H}\right)\) sample complexity dependence
here. This gives us one direction of
Theorem 1.</p>

<p>This logic works in the other direction as well. To go from a pure
private PAC learner with sample complexity \(m\) to an upper bound on
\(\mathsf{REPD}\left(\mathcal{H}\right)\), we return to the group privacy
trick used by <a href="https://dl.acm.org/doi/10.1007/978-3-642-11799-2_26" title="Amos Beimel, Shiva Prasad Kasiviswanathan, and Kobbi Nissim. Bounds on the sample complexity for private learning and private data release. TCC 2010"><strong>[BKN10]</strong></a>. Suppose we fix a database of size \(m\) and pass it
to the learner. By group privacy and the learner’s accuracy guarantee,
if we fix some concept \(c\), the learner has probability at least roughly
\(e^{-m}\) of outputting a hypothesis that mostly agrees with \(c\). Thus if
we repeat this process roughly \(e^{m}\) times, we probably get at least
one hypothesis that mostly agrees with \(c\). In other words, this
repeated calling of the learner on the arbitrary database yields a
covering distribution for \(\mathcal{H}\). Since we called the learner
approximately \(e^m\) times, the logarithm of this is \(m\), and we get our
upper bound on \(\mathsf{REPD}\left(\mathcal{H}\right)\).</p>

<p>To recap, we now know that proper pure private PAC learning is strictly
harder than improper pure private PAC learning, which is characterized
by representation dimension. A picture sums it up. Note the dotted line,
since we don’t yet have any evidence separating finite representation
dimension and finite VC dimension.</p>

<p><img alt="Landscape of Private PAC, take 1" src="https://differentialprivacy.org/images/private_pac_1.png" style="margin: auto; display: block;" width="400"/></p>

<h1 id="separating-pure-and-approximate-private-pac-learning">Separating Pure and Approximate Private PAC Learning</h1>

<p>So far, we’ve focused only on pure privacy. In this section, we move on
to the first separations between pure and approximate private PAC
learning, as well as the first connection between private learning and
<em>online</em> learning.</p>

<p>Our source is a pair of interconnected papers from around 2014. Among
other things, Feldman and Xiao <a href="https://arxiv.org/abs/1402.6278" title="Vitaly Feldman and David Xiao. Sample complexity bounds on differentially private learning via communication complexity. COLT 2014"><strong>[FX14]</strong></a> introduced <em>Littlestone
dimension</em> to private PAC learning. By connecting representation
dimension to results from communication complexity to Littlestone
dimension, they proved the following:</p>

<blockquote>
  <p><strong>Theorem 2</strong> (<a href="https://arxiv.org/abs/1402.6278" title="Vitaly Feldman and David Xiao. Sample complexity bounds on differentially private learning via communication complexity. COLT 2014"><strong>[FX14]</strong></a>). The sample complexity to pure private PAC learn \(\mathcal{H}\) is \(\Omega(\mathsf{LD}\left(\mathcal{H}\right))\).</p>
</blockquote>

<p>Littlestone dimension \(\mathsf{LD}\left(\mathcal{H}\right)\) is, roughly,
the maximum number of mistakes an adversary can force an <em>online</em>
PAC-learning algorithm to make <a href="https://link.springer.com/article/10.1023/A:1022869011914" title="Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine learning, 1988"><strong>[Lit88]</strong></a>. We always have
\(\mathsf{VCD}\left(\mathcal{H}\right) \leq \mathsf{LD}\left(\mathcal{H}\right) \leq \log|\mathcal{H}|\),
but these inequalities can be strict. For example, denoting by
\(\mathsf{Thresh_T}\) the class of thresholds over \(\{1, 2, \ldots,
T\}\), since an adversary can force \(\Theta(\log T)\) wrong answers from
an online learner binary searching over \(\{1,2, \ldots, T\}\),
\(\mathsf{LD}\left(\mathsf{Thresh_T}\right) = \Omega(\log T)\). In
contrast, \(\mathsf{VCD}\left(\mathsf{Thresh_T}\right) = 1\).</p>

<p>At first glance it’s not obvious what
Theorem 2 adds over
Theorem 1. After all,
Theorem 1 gives an equivalence, not just a lower bound. One
advantage of
Theorem 2 is that Littlestone dimension is a known
quantity that has already been studied in its own right. We can now
import results like the lower bound on
\(\mathsf{LD}\left(\mathsf{Thresh_T}\right)\), whereas bounds on
\(\mathsf{REPD}\left(\cdot\right)\) are not common. A second advantage is
that Littlestone dimension conceptually connects private learning and
online learning: we now know that pure private PAC learning is no easier
than online PAC learning.</p>

<p>A second paper by Beimel, Nissim, and Stemmer <a href="https://arxiv.org/abs/1407.2674" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approximate differential privacy. APPROX-RANDOM 2013"><strong>[BNS13b]</strong></a> contrasted this
\(\Omega(\log T)\) lower bound for pure private learning of thresholds
with a \(2^{O(\log^\ast T)}\) upper bound for <em>approximate</em> private PAC
learning \(\mathsf{Thresh_T}\). Here \(\log^\ast\) denotes the very
slow-growing iterated logarithm, the number of times we must take the
logarithm of the argument to bring it \(\leq 1\). (We’re not kidding about
“very slow-growing” either:
\(\log^\ast(\text{number of atoms in universe}) \approx
4\).) With Feldman and Xiao’s result, this separates pure private PAC
learning from approximate private PAC learning. It also shows that
representation dimension does <em>not</em> characterize approximate private PAC
learning.</p>

<p>At the same time, Feldman and Xiao observed that the connection between
pure private PAC learning and Littlestone dimension is imperfect. Again
borrowing results from communication complexity, they observed that the
hypothesis class \(\mathsf{Line_p}\) (which we won’t define here) has
\(\mathsf{LD}\left(\mathsf{Line_p}\right) = 2\) but
\(\mathsf{REPD}\left(\mathsf{Line_p}\right)
= \Theta(\log(p))\). In contrast, they showed that an <em>approximate</em>
private PAC learner can learn \(\mathsf{Line_p}\) using
\(O\left(\tfrac{\log(1/\beta)}{\alpha}\right)\) samples. Since this
entails no dependence on \(p\) at all, it improves the separation between
pure and approximate private PAC learning given by <a href="https://arxiv.org/abs/1407.2674" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approximate differential privacy. APPROX-RANDOM 2013"><strong>[BNS13b]</strong></a>.</p>

<p>Let’s pause to recap what’s happened so far. We learned in the last
section that representation dimension characterizes pure private PAC
learning <a href="https://arxiv.org/abs/1402.2224" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of private learners. ITCS 2013"><strong>[BNS13]</strong></a>. We learned in this section that Littlestone dimension
gives lower bounds for pure private PAC learning but, as shown by
\(\mathsf{Line_p}\), these bounds are sometimes quite loose <a href="https://arxiv.org/abs/1402.6278" title="Vitaly Feldman and David Xiao. Sample complexity bounds on differentially private learning via communication complexity. COLT 2014"><strong>[FX14]</strong></a>.
\(\mathsf{Thresh_T}\) shows that representation dimension does not
characterize approximate private PAC learning <strong>[<a href="https://arxiv.org/abs/1402.6278" title="Vitaly Feldman and David Xiao. Sample complexity bounds on differentially private learning via communication complexity. COLT 2014">FX14</a>
; <a href="https://arxiv.org/abs/1407.2674" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Private learning and sanitization: Pure vs. approximate differential privacy. APPROX-RANDOM 2013">BNS13b</a>]</strong>, and we
still have no privacy-specific lower bounds for approximate private
learners. So the picture now looks like this:</p>

<p><img alt="Landscape of Private PAC, take 2" src="https://differentialprivacy.org/images/private_pac_2.png" style="margin: auto; display: block;" width="400"/></p>

<p>In particular, we might still find that VC dimension characterizes
approximate private PAC learning!</p>

<h1 id="lower-bounds-for-approximate-private-pac-learning">Lower Bounds for Approximate Private PAC Learning</h1>

<p>We now dash this hope. In 2015, Bun, Nissim, Stemmer, and
Vadhan <a href="https://arxiv.org/abs/1504.07553" title="Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil Vadhan. Differentially private release and learning of threshold functions. FOCS 2015"><strong>[BNSV15]</strong></a> gave the first nontrivial lower bound for approximate
private PAC learning. They showed that learning \(\mathsf{Thresh_T}\) has
<em>proper</em> approximate private sample complexity \(\Omega(\log^\ast(T))\) and
\(O(2^{\log^\ast(T)})\).</p>

<p>We’ll at least try to give some intuition for the presence of \(\log^\ast\)
in the lower bound. Informally, the lower bound relies on an inductive
construction of a sequence of hard problems for databases of size
\(n=1, 2,
\ldots\). The \(k^{th}\) hard problem relies on a distribution over
databases of size \(k\) whose data universe is of of size exponential in
the size of the data universe for the \((k-1)^{th}\) distribution. The
base case is the uniform distribution over the two singleton databases
\(\{0\}\) and \(\{1\}\), and they show how to inductively construct
successive problems such that a solution for the \(k^{th}\) problem
implies a solution for the \((k-1)^{th}\) problem. Unraveling the
recursive relationship between the problem domain sizes implies a
general lower bound of roughly \(\log^\ast|X|\) for domain \(X\).</p>

<p>The inclusion of \(\log^\ast\) makes this is an extremely mild lower bound.
However, \(\log^\ast(T)\) can still be arbitrarily larger than 1, so this is
the first definitive evidence that proper approximate privacy introduces
a cost over non-private PAC learning.</p>

<p>In 2018, Alon, Livni, Malliaris, and Moran <a href="https://arxiv.org/abs/1806.00949" title="Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies finite Littlestone dimension. STOC 2019"><strong>[ALMM19]</strong></a> extended this
\(\Omega(\log^\ast T)\) lower bound for \(\mathsf{Thresh_T}\) to <em>improper</em>
approximate privacy. More generally, they gave concrete evidence for the
importance of thresholds, which have played a seemingly outsize role in
the work so far. They did so by relating a class’ Littlestone dimension
to its ability to “contain” thresholds. Here, we say \(\mathcal{H}\)
“contains” \(m\) thresholds if there exist \(m\) (unlabeled) examples
\(x_1,\ldots,x_m\) and hypotheses \(h_1, \ldots, h_m \in \mathcal{H}\) such
that the hypotheses “behave like” thresholds on the \(m\) examples, i.e., 
\(h_i(x_j) = 1 \Leftrightarrow j \geq
i\). With this language, they imported a result from model theory to show
that any hypothesis class \(\mathcal{H}\) contains
\(\log(\mathsf{LD}\left(\mathcal{H}\right))\) thresholds. This implies
that learning \(\mathcal{H}\) is at least as hard as learning
\(\mathsf{Thresh_T}\) with
\(T = \log(\mathsf{LD}\left(\mathcal{H}\right))\). Since
\(\log^\ast(\log(\mathsf{LD}\left(\mathcal{H}\right)))
= \Omega(\log^\ast(\mathsf{LD}\left(\mathcal{H}\right)))\), combining these
two results puts the following limit on private PAC learning:</p>

<blockquote>
  <p><strong>Theorem 3</strong> (<a href="https://arxiv.org/abs/1806.00949" title="Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies finite Littlestone dimension. STOC 2019"><strong>[ALMM19]</strong></a>). The sample complexity to approximate private PAC learn \(\mathcal{H}\) is \(\Omega(\log^\ast(\mathsf{LD}\left(\mathcal{H}\right)))\).</p>
</blockquote>

<p>Littlestone dimension characterizes online PAC learning, so we now know
that online PAC learnability is necessary for private PAC learnability.
Sufficiency, however, remains an open question. This produces the
following picture, where the dotted line captures the question of
sufficiency.</p>

<p><img alt="Landscape of Private PAC, take 3" src="https://differentialprivacy.org/images/private_pac_3.png" style="margin: auto; display: block;" width="400"/></p>

<h1 id="characterizing-approximate-private-pac-learning">Characterizing Approximate Private PAC Learning</h1>

<p>Spurred by this question, several advances in private PAC learning have
appeared in the last year. First, Gonen, Hazan, and Moran strengthened
Theorem 3 by giving a constructive method for converting
<em>pure</em> private learners to online learners <a href="https://arxiv.org/abs/1905.11311" title="Alon Gonen, Elad Hazan, and Shay Moran. Private learning implies online learning: An efficient reduction. NeurIPS 2019"><strong>[GHM19]</strong></a>. Their result
reaches back to the 2013 characterization of pure private learning in
terms of representation dimension by using the covering distribution to
generate a collection of “experts” for online learning. Again revisiting
\(\mathsf{Thresh_T}\), Kaplan, Ligett, Mansour, Naor, and
Stemmer <a href="https://arxiv.org/abs/1911.10137" title="Haim Kaplan, Katrina Ligett, Yishay Mansour, Moni Naor, and Uri Stemmer. Privately learning thresholds: Closing the exponential gap. COLT 2020"><strong>[KLMNS20]</strong></a> significantly reduced the \(O(2^{\log^\ast(T)})\) upper
bound of <a href="https://arxiv.org/abs/1504.07553" title="Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil Vadhan. Differentially private release and learning of threshold functions. FOCS 2015"><strong>[BNSV15]</strong></a> to just \(O((\log^\ast(T))^{1.5})\). And Alon, Beimel,
Moran, and Stemmer <a href="https://arxiv.org/abs/2003.04509" title="Noga Alon, Amos Beimel, Shay Moran, and Uri Stemmer. Closure properties for private classification and online prediction. COLT 2020"><strong>[ABMS20]</strong></a> justified this post’s focus on realizable
private PAC learning by giving a transformation from a realizable
approximate private PAC learner to an agnostic one at the cost of
slightly worse privacy and sample complexity. This built on an earlier
transformation that only applied to <em>proper</em> learners <a href="https://arxiv.org/abs/1407.2662" title="Amos Beimel, Kobbi Nissim, and Uri Stemmer. Learning privately with labeled and unlabeled examples. SODA 2015"><strong>[BNS15]</strong></a>.</p>

<p>Finally, Bun, Livni, and Moran <a href="https://arxiv.org/abs/2003.00563" title="Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. FOCS 2020"><strong>[BLM20]</strong></a> answered the open question posed
by <a href="https://arxiv.org/abs/1806.00949" title="Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies finite Littlestone dimension. STOC 2019"><strong>[ALMM19]</strong></a>:</p>

<blockquote>
  <p><strong>Theorem 4</strong> (<a href="https://arxiv.org/abs/2003.00563" title="Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. FOCS 2020"><strong>[BLM20]</strong></a>). The sample complexity to approximate private PAC learn \(\mathcal{H}\) is \(2^{O({\mathsf{LD}\left(\mathcal{H}\right)})}\).</p>
</blockquote>

<p>To prove this, <a href="https://arxiv.org/abs/2003.00563" title="Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. FOCS 2020"><strong>[BLM20]</strong></a> introduced the notion of a <em>globally stable</em>
learner and showed how to convert an online learner to a globally stable
learner to a private learner. Thus, combined with the result of <a href="https://arxiv.org/abs/1806.00949" title="Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies finite Littlestone dimension. STOC 2019"><strong>[ALMM19]</strong></a>,
we now know that the sample complexity of private PAC learning any
\(\mathcal{H}\) is at least
\(\Omega(\log^\ast(\mathsf{LD}\left(\mathcal{H}\right)))\) and at most
\(2^{O({\mathsf{LD}\left(\mathcal{H}\right)})}\). In this sense, online
learnability characterizes private learnability.</p>

<p><img alt="Landscape of Private PAC, final take" src="https://differentialprivacy.org/images/private_pac_4.png" style="margin: auto; display: block;" width="400"/></p>

<p>Narrowing the gap between the lower and upper bounds above is an open
question. Note that we cannot hope to close the gap completely. For the
lower bound, the current \(\mathsf{Thresh_T}\) upper bound implies that no
general lower bound can be stronger than
\(\Omega((\log^\ast(\mathsf{LD}\left(\mathcal{H}\right)))^{1.5})\). For the
upper bound, there exist hypotheses classes \(\mathcal{H}\) with
\(\mathsf{VCD}\left(\mathcal{H}\right) = \mathsf{LD}\left(\mathcal{H}\right)\)
(e.g., \(\mathsf{VCD}\left(\mathsf{Point}_d\right) = \mathsf{LD}\left(\mathsf{Point}_d\right)= 1\)), so since non-private PAC learning requires
\(\Omega(\mathsf{VCD}\left(\mathcal{H}\right))\) samples, the best
possible private PAC learning upper bound is
\(O(\mathsf{LD}\left(\mathcal{H}\right))\). Nevertheless, proving either
bound remains open.</p>

<h1 id="conclusion">Conclusion</h1>

<p>This concludes our post, and with it our discussion of this fundamental
question: the price of privacy in machine learning. We now know that in
the PAC model, proper pure private learning, improper pure private
learning, approximate private learning, and non-private learning are all
strongly separated. By the connection to Littlestone dimension, we also
know that approximate private learnability is equivalent to online
learnability. However, many questions about computational efficiency and
tight sample complexity bounds remain open.</p>

<p>As mentioned in the introduction, we focused on the clean yet widely
studied and influential model of PAC learning. Having characterized how
privacy enters the picture in PAC learning, we can hopefully convey this
understanding to other models of learning, and now approach these
questions from a rigorous and grounded point of view.</p>

<p>Congratulations to Mark Bun, Roi Livni, and Shay Moran on their best
paper award — and to the many individuals who paved the way before
them!</p>

<h1 id="acknowledgments">Acknowledgments</h1>

<p>Thanks to Kareem Amin and Clément Canonne for helpful feedback while
writing this post.</p></div>
    </summary>
    <updated>2020-09-16T18:00:00Z</updated>
    <published>2020-09-16T18:00:00Z</published>
    <author>
      <name>Matthew Joseph</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2020-09-16T14:21:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4962</id>
    <link href="https://www.scottaaronson.com/blog/?p=4962" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4962#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4962" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">In a world like this one, take every ally you can get</title>
    <summary xml:lang="en-US">For the past few months, I’ve alternated between periods of debilitating depression and (thankfully) longer stretches when I’m more-or-less able to work. Triggers for my depressive episodes include reading social media, watching my 7-year daughter struggle with prolonged isolation, and (especially) contemplating the ongoing apocalypse in the American West, the hundreds of thousands of pointless […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>For the past few months, I’ve alternated between periods of debilitating depression and (thankfully) longer stretches when I’m more-or-less able to work.  Triggers for my depressive episodes include reading social media, watching my 7-year daughter struggle with prolonged isolation, and (especially) contemplating the ongoing apocalypse in the American West, the hundreds of thousands of pointless covid deaths, and an election in 48 days that <em>if I didn’t know such things were impossible in America</em> would seem <a href="https://www.washingtonpost.com/health/2020/09/14/michael-caputo-coronavirus-cdc/">likely</a> to produce a terrifying standoff as a despot and millions of his armed loyalists refuse to cede control.  Meanwhile, catalysts for my relatively functional periods have included teaching my undergrad <a href="https://www.scottaaronson.com/qclec.pdf">quantum information class</a>, Zoom calls with my students, <a href="https://www.nature.com/articles/s41550-020-1174-4">life on Venus?!?</a> (my guess is no, but almost entirely due to priors), learning new math (fulfilling a decades-old goal, I’m finally learning Paul Cohen’s celebrated <a href="https://en.wikipedia.org/wiki/Forcing_(mathematics)">proof</a> of the independence of the Continuum Hypothesis—more about that later!).</p>



<p>Of course, when you feel crushed by the weight of the world’s horribleness, it improves your mood to be able even just to prick the horribleness with a pin.  I was gratified that, in response to a <a href="https://www.scottaaronson.com/blog/?p=4942">previous post</a>, <em>Shtetl-Optimized</em> readers contributed at least $3,000, the first $2,000 of which I matched, mostly to the <a href="https://secure.actblue.com/donate/duforjoe">Biden-Harris campaign</a> but a little to the <a href="https://lincolnproject.us/donate/">Lincoln Project</a> of anti-Trump Republicans.  (Feel free to continue donating on the blog’s behalf!)</p>



<p>Alas, a <a href="https://www.scottaaronson.com/blog/?p=4942#comment-1856922">commenter</a> was unhappy with the latter:</p>



<blockquote class="wp-block-quote"><p>Lincoln Project? Really? … Pushing the Overton window rightward during a worldwide fascist dawn isn’t good. I have trouble understanding why even extremely smart people have trouble with this sort of thing.</p></blockquote>



<p>Since this is actually important, I’d like to spend the rest of this post responding to it.</p>



<p>For me it’s simple.</p>



<p>What is the goal?  To defeat Trump.  In the US right now, that’s the prerequisite to <strong>every other</strong> sane political goal.</p>



<p>What will it take to achieve that goal? Turnout, energizing the base, defending the election process … but also, if possible, <em>persuading a sliver of Trump supporters in swing states to switch sides</em>, or at least vote third party or abstain.  (It’s precisely <em>because</em> I expect Trump to refuse to concede, that the margin of his loss will matter as much as the fact of it.)</p>



<p>Who is actually effective at the goal?  Well, no one knows for sure.  But while I thought the Biden campaign had some decent ads, the Lincoln Project’s best stuff seems <a href="https://www.youtube.com/watch?v=4TGhgzZ8JG8">better</a> to me, just savagely good.</p>



<p><em>Why</em> is the Lincoln Project effective?  The answer seems obvious: for the same reason why a jilted ex is a more dangerous foe than a stranger.  If <em>anyone</em> understood how to deprogram a Republican from the Trump cult, who would it be: Alexandria Ocasio-Cortez, or a fellow Republican who successfully broke from the cult?</p>



<p>Do I agree with the Lincoln Republicans about most of the “normal” issues that Americans once argued about?  Not at all.  Do I hold them, in part, morally responsible for creating the preconditions to the current nightmare?  Certainly.</p>



<p>And should any of that cause me to boycott them?  Not in <em>my</em> moral universe!  If Churchill and FDR could team up with Stalin, then surely we in the Resistance can temporarily ally ourselves with the rare Republicans who chose their stated principles over power when tested—their very rarity attesting to the nontriviality of their choice.</p>



<p>To my mind, turning one’s back on would-be allies, in a conflict whose stakes obviously overshadow what’s bad about those allies, is simultaneously one of the dumbest <em>and</em> the ugliest things that human beings can do.  It abandons reason for moral purity and ends up achieving neither.</p></div>
    </content>
    <updated>2020-09-16T07:51:47Z</updated>
    <published>2020-09-16T07:51:47Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Rage Against Doofosity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-09-16T15:02:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.07269</id>
    <link href="http://arxiv.org/abs/2009.07269" rel="alternate" type="text/html"/>
    <title>Positivity-preserving extensions of sum-of-squares pseudomoments over the hypercube</title>
    <feedworld_mtime>1600214400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kunisky:Dmitriy.html">Dmitriy Kunisky</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.07269">PDF</a><br/><b>Abstract: </b>We introduce a new method for building higher-degree sum-of-squares lower
bounds over the hypercube $\mathbf{x} \in \{\pm 1\}^N$ from a given degree 2
lower bound. Our method constructs pseudoexpectations that are positive
semidefinite by design, lightening some of the technical challenges common to
other approaches to SOS lower bounds, such as pseudocalibration.
</p>
<p>We give general "incoherence" conditions under which degree 2 pseudomoments
can be extended to higher degrees. As an application, we extend previous lower
bounds for the Sherrington-Kirkpatrick Hamiltonian from degree 4 to degree 6.
(This is subsumed, however, in the stronger results of the parallel work of
Ghosh et al.) This amounts to extending degree 2 pseudomoments given by a
random low-rank projection matrix. As evidence in favor of our construction for
higher degrees, we also show that random high-rank projection matrices (an
easier case) can be extended to degree $\omega(1)$. We identify the main
obstacle to achieving the same in the low-rank case, and conjecture that while
our construction remains correct to leading order, it also requires a
next-order adjustment.
</p>
<p>Our technical argument involves the interplay of two ideas of independent
interest. First, our pseudomoment matrix factorizes in terms of certain
multiharmonic polynomials. This observation guides our proof of positivity.
Second, our pseudomoment values are described graphically by sums over forests,
with coefficients given by the M\"{o}bius function of a partial ordering of
those forests. This connection guides our proof that the pseudomoments satisfy
the hypercube constraints. We trace the reason that our pseudomoments can
satisfy both the hypercube and positivity constraints simultaneously to a
combinatorial relationship between multiharmonic polynomials and this
M\"{o}bius function.
</p></div>
    </summary>
    <updated>2020-09-16T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.07268</id>
    <link href="http://arxiv.org/abs/2009.07268" rel="alternate" type="text/html"/>
    <title>An improved quantum-inspired algorithm for linear regression</title>
    <feedworld_mtime>1600214400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gily=eacute=n:Andr=aacute=s.html">András Gilyén</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Song:Zhao.html">Zhao Song</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tang:Ewin.html">Ewin Tang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.07268">PDF</a><br/><b>Abstract: </b>We give a classical algorithm for linear regression analogous to the quantum
matrix inversion algorithm [Harrow, Hassidim, and Lloyd, Physical Review
Letters '09] for low-rank matrices [Chakraborty et al., ICALP'19], when the
input matrix $A$ is stored in a data structure applicable for QRAM-based state
preparation.
</p>
<p>Namely, if the input model supports efficient $\ell_2$-norm importance
sampling, and given $A \in \mathbb{C}^{m\times n}$ with minimum singular value
$\sigma$ and $b \in \mathbb{C}^m$ as input, we can output a description of an
$x$ such that $\|x - A^+b\| \leq \varepsilon\|A^+b\|$ in
$\tilde{\mathcal{O}}\left(\frac{\|A\|_{\mathrm{F}}^6\|A\|^2}{\sigma^8\varepsilon^4}\right)$
time, improving on previous "quantum-inspired" algorithms in this line of
research by a factor of $\frac{\|A\|^{14}}{\sigma^{14}\varepsilon^2}$ [Chia et
al., STOC'20]. The algorithm is stochastic gradient descent, and the analysis
bears similarities to results of [Gupta and Sidford, NeurIPS'18]. Unlike
earlier works, this is a promising avenue that could lead to feasible
implementations of classical regression in a quantum-inspired setting, for
comparison against future quantum computers.
</p></div>
    </summary>
    <updated>2020-09-16T01:21:09Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.07248</id>
    <link href="http://arxiv.org/abs/2009.07248" rel="alternate" type="text/html"/>
    <title>Approximation Algorithms for The Generalized Incremental Knapsack Problem</title>
    <feedworld_mtime>1600214400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Faenza:Yuri.html">Yuri Faenza</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Segev:Danny.html">Danny Segev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Lingyi.html">Lingyi Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.07248">PDF</a><br/><b>Abstract: </b>We introduce and study a discrete multi-period extension of the classical
knapsack problem, dubbed generalized incremental knapsack. In this setting, we
are given a set of $n$ items, each associated with a non-negative weight, and
$T$ time periods with non-decreasing capacities $W_1 \leq \dots \leq W_T$. When
item $i$ is inserted at time $t$, we gain a profit of $p_{it}$; however, this
item remains in the knapsack for all subsequent periods. The goal is to decide
if and when to insert each item, subject to the time-dependent capacity
constraints, with the objective of maximizing our total profit. Interestingly,
this setting subsumes as special cases a number of recently-studied incremental
knapsack problems, all known to be strongly NP-hard.
</p>
<p>Our first contribution comes in the form of a polynomial-time
$(\frac{1}{2}-\epsilon)$-approximation for the generalized incremental knapsack
problem. This result is based on a reformulation as a single-machine sequencing
problem, which is addressed by blending dynamic programming techniques and the
classical Shmoys-Tardos algorithm for the generalized assignment problem.
Combined with further enumeration-based self-reinforcing ideas and
newly-revealed structural properties of nearly-optimal solutions, we turn our
basic algorithm into a quasi-polynomial time approximation scheme (QPTAS).
Hence, under widely believed complexity assumptions, this finding rules out the
possibility that generalized incremental knapsack is APX-hard.
</p></div>
    </summary>
    <updated>2020-09-16T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.07106</id>
    <link href="http://arxiv.org/abs/2009.07106" rel="alternate" type="text/html"/>
    <title>Drawing outer-1-planar graphs revisited</title>
    <feedworld_mtime>1600214400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Therese Biedl <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.07106">PDF</a><br/><b>Abstract: </b>In a recent article (Auer et al, Algorithmica 2016) it was claimed that every
outer-1-planar graph has a planar visibility representation of area $O(n\log
n)$. In this paper, we show that this is wrong: There are outer-1-planar graphs
that require $\Omega(n^2)$ area in any planar drawing.
</p></div>
    </summary>
    <updated>2020-09-16T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-09-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.06921</id>
    <link href="http://arxiv.org/abs/2009.06921" rel="alternate" type="text/html"/>
    <title>Optimal Decision Trees for Nonlinear Metrics</title>
    <feedworld_mtime>1600214400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Emir Demirović, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stuckey:Peter_J=.html">Peter J. Stuckey</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.06921">PDF</a><br/><b>Abstract: </b>Nonlinear metrics, such as the F1-score, Matthews correlation coefficient,
and Fowlkes-Mallows index, are often used to evaluate the performance of
machine learning models, in particular, when facing imbalanced datasets that
contain more samples of one class than the other. Recent optimal decision tree
algorithms have shown remarkable progress in producing trees that are optimal
with respect to linear criteria, such as accuracy, but unfortunately nonlinear
metrics remain a challenge. To address this gap, we propose a novel algorithm
based on bi-objective optimisation, which treats misclassifications of each
binary class as a separate objective. We show that, for a large class of
metrics, the optimal tree lies on the Pareto frontier. Consequently, we obtain
the optimal tree by using our method to generate the set of all nondominated
trees. To the best of our knowledge, this is the first method to compute
provably optimal decision trees for nonlinear metrics. Our approach leads to a
trade-off when compared to optimising linear metrics: the resulting trees may
be more desirable according to the given nonlinear metric at the expense of
higher runtimes. Nevertheless, the experiments illustrate that runtimes are
reasonable for majority of the tested datasets.
</p></div>
    </summary>
    <updated>2020-09-16T01:22:48Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.06897</id>
    <link href="http://arxiv.org/abs/2009.06897" rel="alternate" type="text/html"/>
    <title>Steady and ranging sets in graph persistence</title>
    <feedworld_mtime>1600214400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bergomi:Mattia_G=.html">Mattia G. Bergomi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Ferri:Massimo.html">Massimo Ferri</a>, Antonella Tavaglione <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.06897">PDF</a><br/><b>Abstract: </b>Generalised persistence functions (gp-functions) are defined on $(\mathbb{R},
\le)$-indexed diagrams in a given category. A sufficient condition for
stability is also introduced. In the category of graphs, a standard way of
producing gp-functions is proposed: steady and ranging sets for a given
feature. The example of steady and ranging hubs is studied in depth; their
meaning is investigated in three concrete networks.
</p></div>
    </summary>
    <updated>2020-09-16T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-09-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.06879</id>
    <link href="http://arxiv.org/abs/2009.06879" rel="alternate" type="text/html"/>
    <title>Bounded-Degree Spanners in the Presence of Polygonal Obstacles</title>
    <feedworld_mtime>1600214400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Renssen:Andr=eacute=_van.html">André van Renssen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wong:Gladys.html">Gladys Wong</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.06879">PDF</a><br/><b>Abstract: </b>Let $V$ be a finite set of vertices in the plane and $S$ be a finite set of
polygonal obstacles. We show how to construct a plane $2$-spanner of the
visibility graph of $V$ with respect to $S$. As this graph can have unbounded
degree, we modify it in three easy-to-follow steps, in order to bound the
degree to $7$ at the cost of slightly increasing the spanning ratio to 6.
</p></div>
    </summary>
    <updated>2020-09-16T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-09-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.06784</id>
    <link href="http://arxiv.org/abs/2009.06784" rel="alternate" type="text/html"/>
    <title>Learning Mixtures of Permutations: Groups of Pairwise Comparisons and Combinatorial Method of Moments</title>
    <feedworld_mtime>1600214400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mao:Cheng.html">Cheng Mao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Yihong.html">Yihong Wu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.06784">PDF</a><br/><b>Abstract: </b>In applications such as rank aggregation, mixture models for permutations are
frequently used when the population exhibits heterogeneity. In this work, we
study the widely used Mallows mixture model. In the high-dimensional setting,
we propose a polynomial-time algorithm that learns a Mallows mixture of
permutations on $n$ elements with the optimal sample complexity that is
proportional to $\log n$, improving upon previous results that scale
polynomially with $n$. In the high-noise regime, we characterize the optimal
dependency of the sample complexity on the noise parameter. Both objectives are
accomplished by first studying demixing permutations under a noiseless query
model using groups of pairwise comparisons, which can be viewed as moments of
the mixing distribution, and then extending these results to the noisy Mallows
model by simulating the noiseless oracle.
</p></div>
    </summary>
    <updated>2020-09-16T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.06778</id>
    <link href="http://arxiv.org/abs/2009.06778" rel="alternate" type="text/html"/>
    <title>Spatio-Temporal Top-k Similarity Search for Trajectories in Graphs</title>
    <feedworld_mtime>1600214400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Driemel:Anne.html">Anne Driemel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mutzel:Petra.html">Petra Mutzel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Oettershagen:Lutz.html">Lutz Oettershagen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.06778">PDF</a><br/><b>Abstract: </b>We study the problem of finding the $k$ most similar trajectories in a graph
to a given query trajectory. Our work was inspired by the work of Grossi et al.
[6] that considered trajectories as walks in a graph in which each visited
vertex is accompanied by a time-interval. Grossi et al. define a similarity
function which is able to capture both temporal and spatial aspects. We modify
this similarity function in order to derive a spatio-temporal distance function
for which we can show that a specific type of triangle inequality is
satisfied.% This distance function builds the basis for our index structure
which is able to quickly answer queries for the top-$k$ most similar
trajectories. Our evaluation on real-world and synthetic data sets shows that
our new approaches outperform the baselines with respect to indexing time,
query time, and quality of results.
</p></div>
    </summary>
    <updated>2020-09-16T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/09/15/linkage</id>
    <link href="https://11011110.github.io/blog/2020/09/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Closed quasigeodesics on the dodecahedron (\(\mathbb{M}\)), paths that start at a vertex and go straight across each edge until coming back to the same vertex from the other side. Original paper, arXiv:1811.04131, doi:10.1080/10586458.2020.1712564. I saw this on Numberphile a few months back (video linked in article) but now it’s on Quanta.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://www.quantamagazine.org/mathematicians-report-new-discovery-about-the-dodecahedron-20200831/">Closed quasigeodesics on the dodecahedron</a> (<a href="https://mathstodon.xyz/@11011110/104785420838924796">\(\mathbb{M}\)</a>), paths that start at a vertex and go straight across each edge until coming back to the same vertex from the other side. Original paper, <a href="https://arxiv.org/abs/1811.04131">arXiv:1811.04131</a>, <a href="https://doi.org/10.1080/10586458.2020.1712564">doi:10.1080/10586458.2020.1712564</a>. I saw this on Numberphile a few months back (video linked in article) but now it’s on <em>Quanta</em>.</p>
  </li>
  <li>
    <p><a href="https://blog.graphicine.com/lorenz-stoer-geometric-landscapes/">Lorenz Stöer’s geometric landscapes</a> (<a href="https://mathstodon.xyz/@11011110/104799765760054680">\(\mathbb{M}\)</a>). <a href="https://11011110.github.io/blog/2014/09/30/linkage-for-end.html">In 2014 I linked a different page</a> with a few of Stöer’s 16th-century proto-surrealist combinations of landscape and geometry, but they were black and white. This one has more of them, in color.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Ideal_polyhedron">Ideal polyhedron</a>, a polyhedron in hyperbolic space with all vertices at infinity, and <a href="https://en.wikipedia.org/wiki/Sylvester%E2%80%93Gallai_theorem">Sylvester–Gallai theorem</a>, that every finite set of points in the Euclidean plane has a line that either passes through all of them or through exactly two of them. Both newly promoted to Good Article status on Wikipedia (<a href="https://mathstodon.xyz/@11011110/104803212564257211">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://merveilles.town/@neauoire/104779168858836970">Escherian wiener-dog Cerberus fetches three impossible things</a>.</p>
  </li>
  <li>
    <p>Flamebait post of the day: <a href="http://nautil.us/issue/89/the-dark-side/why-mathematicians-should-stop-naming-things-after-each-other">Why mathematicians should stop naming things after each other</a> (<a href="https://mathstodon.xyz/@11011110/104813883920721252">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=24385389">via</a>).  For once the via-link discussion is worth reading (main point: the alternative, using common English words to describe specialized technical concepts, can be even more confusing).</p>
  </li>
  <li>
    <p>Early Renaissance painter Piero della Francesca was also an accomplished mathematician, and his book on polyhedra, <em>De quinque corporibus regularibus</em> (subject of a <a href="https://en.wikipedia.org/wiki/De_quinque_corporibus_regularibus">new Wikipedia article</a>; <a href="https://mathstodon.xyz/@11011110/104820183749646319">\(\mathbb{M}\)</a>) has an interesting history that deserves to be better known. Rediscovery of the mathematics of Archimedes! “First full-blown case of plagiarism in the history of mathematics” (by Luca Pacioli, in Divina proportione)! Maybe owned by John Dee! Long lost and found centuries later in the Vatican Library!</p>
  </li>
  <li>
    <p><a href="https://cameroncounts.wordpress.com/2020/08/30/moonlighting/">Peter Cameron gives a nice roundup of two recent online conferences on group theory and combinatorics</a> (<a href="https://mathstodon.xyz/@11011110/104833470202099899">\(\mathbb{M}\)</a>) that he attended more-or-less simultaneously, something that would have been impossible for physical conferences. The parts on synchronizing automata and twin-width particularly caught my attention as stuff I should look up and find out more about.</p>
  </li>
  <li>
    <p><a href="https://twitter.com/RodBogart/status/455123609195802624">An hourglass that demonstrates Archimedes’ theorem that the volume of a cylinder is the sum of the volumes of its inscribed sphere and cone</a> (<a href="https://mathstodon.xyz/@mjd/104836143207567957">\(\mathbb{M}\)</a>), from Rod Bogart’s twitter feed.</p>
  </li>
  <li>
    <p>The <a href="https://11011110.github.io/blog/2020/09/07/eberhards-theorem-bipartite.html">hexagon-minimizing simple bipartite polyhedra of my recent blog post</a> make nice shapes when converted to <a href="https://arxiv.org/abs/0912.0537">simple orthogonal polyhedra</a> (<a href="https://mathstodon.xyz/@11011110/104842837266010559">\(\mathbb{M}\)</a>): a squared-off amphitheater with L-shaped terraces of increasing length as they rise, or a diagonal staircase with congruent L-shaped steps. In each case the outer \(2n\)-gon is the underside of the polygon and the inner cycles are the horizontal faces.</p>

    <p style="text-align: center;"><img alt="Hexagon-minimizing simple bipartite polyhedra represented as simple orthogonal polyhedra" src="https://11011110.github.io/blog/assets/2020/orthogonal-eberhard.svg"/></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2008.11933">Open is not forever: a study of vanished open access journals</a> (<a href="https://mathstodon.xyz/@11011110/104850663521092974">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=24422593">via</a>, <a href="https://www.sciencemag.org/news/2020/09/dozens-scientific-journals-have-vanished-internet-and-no-one-preserved-them">via</a>). This study shows the need for systematic archiving and redundant copying of online open journals, but I suspect that the problem for small hand-run print-based journals without much library pickup might be much worse.</p>
  </li>
  <li>
    <p><a href="https://www.thisiscolossal.com/2018/10/a-prickly-structure-made-of-70000-reusable-hexapod-particles/">A prickly structure made of 70,000 reusable hexapod particles</a> (<a href="https://mathstodon.xyz/@11011110/104856358909259046">\(\mathbb{M}\)</a>). Sort of like those <a href="https://en.wikipedia.org/wiki/Tetrapod_(structure)">seawalls they build by jumbling together giant concrete caltrops</a>, only with pieces that are not quite so big and with usable spaces left void within it. Sometimes the article says “hexapod” and sometimes “decapod”; the pictures appear to show structures that mix two different kinds of particle.</p>
  </li>
  <li>
    <p><em><a href="http://math.sfsu.edu/beck/ct/board.php">Combinatorial Theory</a></em> (<a href="https://mathstodon.xyz/@bremner/104859257534118058">\(\mathbb{M}\)</a>, <a href="https://twitter.com/wtgowers/status/1305253478047068160">see also</a>), a new open-access combinatorics journal formed from the mass resignation of the Elsevier <em>JCTA</em> editorial board.</p>
  </li>
  <li>
    <p><a href="https://www.nytimes.com/2020/09/14/us/caputo-virus.html">Trump officials are now telling their supporters to buy guns and ammunition to use against scientists for being anti-Trump</a>. <a href="https://thehill.com/policy/healthcare/516319-top-hhs-official-accuses-scientists-of-plotting-against-trump-tells">No, seriously</a> (<a href="https://mathstodon.xyz/@11011110/104865069277930004">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><em><a href="https://www.cambridge.org/us/academic/subjects/mathematics/recreational-mathematics/origametry-mathematical-methods-paper-folding">Origametry: Mathematical Methods in Paper Folding</a></em> (<a href="https://mathstodon.xyz/@11011110/104870325812873444">\(\mathbb{M}\)</a>), new book coming out October 31 by Tom Hull. I haven’t seen anything more than the blurb linked here and the <a href="https://books.google.com/books?id=LdX7DwAAQBAJ">limited preview on Google Books</a>, but it looks interesting and worth waiting for.</p>
  </li>
</ul></div>
    </content>
    <updated>2020-09-15T22:15:00Z</updated>
    <published>2020-09-15T22:15:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-09-16T05:18:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17600</id>
    <link href="https://rjlipton.wordpress.com/2020/09/15/ken-regan-turned-61/" rel="alternate" type="text/html"/>
    <title>Ken Regan Turned 61</title>
    <summary>Happy birthday to Ken Ken Regan is of course my partner on GLL. He is faculty in the computer science department at the University of Buffalo. His PhD was in 1986 from Oxford University and it was titled On the separation of complexity classes. He was the PhD student of Dominic Welsh who was a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><span style="color: #0044cc;"><br/>
<em>Happy birthday to Ken</em><br/>
</span></p>
<table class="image alignright">
<tbody>
<tr>
<td><a href="https://rjlipton.wordpress.com/2020/09/15/ken-regan-turned-61/collage-2/" rel="attachment wp-att-17602"><img alt="" class="aligncenter size-full wp-image-17602" height="450" src="https://rjlipton.files.wordpress.com/2020/09/collage.jpg?w=600&amp;h=450" width="600"/></a></td>
</tr>
</tbody>
</table>
<p>Ken Regan is of course my partner on GLL. He is faculty in the computer science department at the University of Buffalo. His PhD was in 1986 from Oxford University and it was titled <i>On the separation of complexity classes</i>. He was the PhD student of Dominic Welsh who was a student of John Hammersley.</p>
<p>Today I would like to wish Ken a happy birthday.</p>
<p>He is now 61 years young. I hope you will join me and wish him many more birthdays. His age is <a href="https://en.wikipedia.org/wiki/61_(number)">special</a> for many reasons:</p>
<ul>
<li>It is a twin prime.</li>
<li>It is equal to <img alt="{5^{2} + 6^{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B5%5E%7B2%7D+%2B+6%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{5^{2} + 6^{2}}"/>.</li>
<li>It is the ninth Mersenne prime: <img alt="{2^{61} - 1 = 2,305,843,009,213,693,951}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B61%7D+-+1+%3D+2%2C305%2C843%2C009%2C213%2C693%2C951%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{61} - 1 = 2,305,843,009,213,693,951}"/>.</li>
</ul>
<p>There are three big <b>I’s</b> in his life. Let’s talk about two of them.</p>
<h2>Interest in Cricket</h2>
<p>Ken loves sports in general and especially cricket. Last Sunday he told me he watched his Bills win their first NFL game while he watched a cricket match. I have no idea how cricket works, but here is Ken’s <a href="https://cse.buffalo.edu/~regan/Writing/CricketBaseball.html">explanation</a>: <i>Are Cricket and Baseball sister games?</i></p>
<table style="margin: auto;">
<tbody>
<tr>
<td><a href="https://rjlipton.wordpress.com/2020/09/15/ken-regan-turned-61/cr1/" rel="attachment wp-att-17603"><img alt="" class="aligncenter size-full wp-image-17603" height="450" src="https://rjlipton.files.wordpress.com/2020/09/cr1.jpg?w=600&amp;h=450" width="600"/></a></td>
</tr>
</tbody>
</table>
<ul>
<li>In a baseball game you see pitchers on the field.<br/>
In a cricket match you see fielders on the pitch.</li>
<li>In baseball, a bad delivery is called a “Ball”.<br/>
In cricket, it’s a “No Ball”.</li>
<li>In baseball, if a batter carries his bat, he’s out.<br/>
In cricket, the batsmen always carry their bat, and an opening batsman who “carries his bat” is never out.</li>
<li>In baseball, an innings is called a half-inning.<br/>
In cricket, an inning is called an innings.</li>
<li>In baseball, a batter hit by a pitched ball gets a free pass to First Base.<br/>
In cricket, such a batter can be Out Leg Before Wicket.</li>
<li>In baseball, if a ball is caught over the boundary, “yer out!”<br/>
In cricket, you score 6 runs.</li>
<li>In baseball, when a batter “walks”, he gets a free pass to first and is not out.<br/>
In cricket, it means the batsman declares himself out before the umpire has a chance to make the call. This classic show of sportsmanship is considered unsportsmanlike in baseball.<p/>
<h2>Interest in Chess</h2>
<p>When the chess world wants to know if someone has cheated, they call Ken. He is an international chess master, and has worked on stopping cheating for years. It is important these days, since most tournaments are now online. And cheating is easier when no one is directly able to watch you. Ken is busy.</p>
<p>Let’s look at the cheating problem. Suppose that Alice and Bob are playing an online game of chess. Alice makes her own moves, but she wonders if Bob could be cheating. He could be using advice from another “player”, Sally. There are several points:</p>
<ol>
<li>Sally is a stronger player than anyone—she can easily beat Alice and Bob.</li>
<li>Sally not only says “here is my move”—she will sometimes give several good moves.</li>
<li>Sally is a program that is deterministic—given a position she gives the same answer.The issue for Ken is: When Alice played Bob did Bob make the moves or did he consult Sally?
</li></ol>
<p>There are many complexities:</p>
<ol>
<li>What if Bob agreed with all Sally’s moves? Then he certainly did cheat.</li>
<li>What if Bob was just lucky and played above his strength? Then he did not cheat.</li>
<li>What if Bob used Sally for some positions but not others? Then he did cheat, but it may be hard to be sure.</li>
<li>And so on.</li>
</ol>
<p>What Ken has done is create both a theory and programs to determine whether Bob did indeed cheat. I find the general problem of telling if one cheats online at chess to be fascinating. See us <a href="https://rjlipton.wordpress.com/2014/06/18/the-problem-of-catching-chess-cheaters/">before</a> for more details and also see <a href="http://www.uschess.org/index.php/June/How-To-Catch-A-Chess-Cheater-Ken-Regan-Finds-Moves-Out-Of-Mind.html">this</a>.</p>
<h2>Open Problems</h2>
<p>Ken is one of the nicest people I know. Hope he has many more birthdays and many more twin primes.</p>
<p> </p></li>
</ul></div>
    </content>
    <updated>2020-09-15T16:24:01Z</updated>
    <published>2020-09-15T16:24:01Z</published>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Oldies"/>
    <category term="People"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-09-16T15:20:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.06117</id>
    <link href="http://arxiv.org/abs/2009.06117" rel="alternate" type="text/html"/>
    <title>The Platform Design Problem</title>
    <feedworld_mtime>1600128000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Papadimitriou:Christos.html">Christos Papadimitriou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vodrahalli:Kiran.html">Kiran Vodrahalli</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yannakakis:Mihalis.html">Mihalis Yannakakis</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.06117">PDF</a><br/><b>Abstract: </b>On-line firms deploy suites of software platforms, where each platform is
designed to interact with users during a certain activity, such as browsing,
chatting, socializing, emailing, driving, etc. The economic and incentive
structure of this exchange, as well as its algorithmic nature, have not been
explored to our knowledge; we initiate their study in this paper. We model this
interaction as a Stackelberg game between a Designer and one or more Agents. We
model an Agent as a Markov chain whose states are activities; we assume that
the Agent's utility is a linear function of the steady-state distribution of
this chain. The Designer may design a platform for each of these
activities/states; if a platform is adopted by the Agent, the transition
probabilities of the Markov chain are affected, and so is the objective of the
Agent. The Designer's utility is a linear function of the steady state
probabilities of the accessible states (that is, the ones for which the
platform has been adopted), minus the development cost of the platforms. The
underlying optimization problem of the Agent -- that is, how to choose the
states for which to adopt the platform -- is an MDP. If this MDP has a simple
yet plausible structure (the transition probabilities from one state to another
only depend on the target state and the recurrent probability of the current
state) the Agent's problem can be solved by a greedy algorithm. The Designer's
optimization problem (designing a custom suite for the Agent so as to optimize,
through the Agent's optimum reaction, the Designer's revenue), while NP-hard,
has an FPTAS. These results generalize, under mild additional assumptions, from
a single Agent to a distribution of Agents with finite support. The Designer's
optimization problem has abysmal "price of robustness", suggesting that
learning the parameters of the problem is crucial for the Designer.
</p></div>
    </summary>
    <updated>2020-09-15T23:26:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-09-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.06107</id>
    <link href="http://arxiv.org/abs/2009.06107" rel="alternate" type="text/html"/>
    <title>Statistical Query Algorithms and Low-Degree Tests Are Almost Equivalent</title>
    <feedworld_mtime>1600128000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brennan:Matthew.html">Matthew Brennan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bresler:Guy.html">Guy Bresler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hopkins:Samuel_B=.html">Samuel B. Hopkins</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jerry.html">Jerry Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schramm:Tselil.html">Tselil Schramm</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.06107">PDF</a><br/><b>Abstract: </b>Researchers currently use a number of approaches to predict and substantiate
information-computation gaps in high-dimensional statistical estimation
problems. A prominent approach is to characterize the limits of restricted
models of computation, which on the one hand yields strong computational lower
bounds for powerful classes of algorithms and on the other hand helps guide the
development of efficient algorithms. In this paper, we study two of the most
popular restricted computational models, the statistical query framework and
low-degree polynomials, in the context of high-dimensional hypothesis testing.
Our main result is that under mild conditions on the testing problem, the two
classes of algorithms are essentially equivalent in power. As corollaries, we
obtain new statistical query lower bounds for sparse PCA, tensor PCA and
several variants of the planted clique problem.
</p></div>
    </summary>
    <updated>2020-09-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.06106</id>
    <link href="http://arxiv.org/abs/2009.06106" rel="alternate" type="text/html"/>
    <title>Breaking the $n$-Pass Barrier: A Streaming Algorithm for Maximum Weight Bipartite Matching</title>
    <feedworld_mtime>1600128000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:S=_Cliff.html">S. Cliff Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Song:Zhao.html">Zhao Song</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Hengjie.html">Hengjie Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.06106">PDF</a><br/><b>Abstract: </b>Given a weighted bipartite graph with $n$ vertices and $m$ edges, the
$\mathit{maximum~weight~bipartite~matching}$ problem is to find a set of
vertex-disjoint edges with the maximum weight. This classic problem has been
extensively studied for over a century.
</p>
<p>In this paper, we present a new streaming algorithm for the maximum weight
bipartite matching problem that uses $\widetilde{O}(n)$ space and
$\widetilde{O}(\sqrt{m})$ passes, which breaks the $n$-pass barrier. All the
previous algorithms either require $\Omega(n \log n)$ passes or only find an
approximate solution.
</p>
<p>To achieve this pass bound, our algorithm combines a number of techniques
from different fields such as the interior point method (IPM), symmetric
diagonally dominant (SDD) system solving, the isolation lemma, and LP duality.
To the best of our knowledge, this is the first work that implements the SDD
solver and IPM in the streaming model in $\widetilde{O}(n)$ spaces for graph
matrix. All the previous IPMs only focus on optimizing the running time,
regardless of the space usage. The LP solver for general matrix is impossible
to be implemented in $\widetilde{O}(n)$ spaces.
</p></div>
    </summary>
    <updated>2020-09-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.06090</id>
    <link href="http://arxiv.org/abs/2009.06090" rel="alternate" type="text/html"/>
    <title>Cut-Equivalent Trees are Optimal for Min-Cut Queries</title>
    <feedworld_mtime>1600128000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Abboud:Amir.html">Amir Abboud</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krauthgamer:Robert.html">Robert Krauthgamer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Trabelsi:Ohad.html">Ohad Trabelsi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.06090">PDF</a><br/><b>Abstract: </b>Min-Cut queries are fundamental: Preprocess an undirected edge-weighted
graph, to quickly report a minimum-weight cut that separates a query pair of
nodes $s,t$. The best data structure known for this problem simply builds a
cut-equivalent tree, discovered 60 years ago by Gomory and Hu, who also showed
how to construct it using $n-1$ minimum $st$-cut computations. Using
state-of-the-art algorithms for minimum $st$-cut (Lee and Sidford, FOCS 2014)
<a href="http://export.arxiv.org/abs/1312.6713">arXiv:1312.6713</a>, one can construct the tree in time $\tilde{O}(mn^{3/2})$,
which is also the preprocessing time of the data structure. (Throughout, we
focus on polynomially-bounded edge weights, noting that faster algorithms are
known for small/unit edge weights.)
</p>
<p>Our main result shows the following equivalence: Cut-equivalent trees can be
constructed in near-linear time if and only if there is a data structure for
Min-Cut queries with near-linear preprocessing time and polylogarithmic
(amortized) query time, and even if the queries are restricted to a fixed
source. That is, equivalent trees are an essentially optimal solution for
Min-Cut queries. This equivalence holds even for every minor-closed family of
graphs, such as bounded-treewidth graphs, for which a two-decade old data
structure (Arikati et al., J.~Algorithms 1998) implies the first near-linear
time construction of cut-equivalent trees.
</p>
<p>Moreover, unlike all previous techniques for constructing cut-equivalent
trees, ours is robust to relying on approximation algorithms. In particular,
using the almost-linear time algorithm for $(1+\epsilon)$-approximate minimum
$st$-cut (Kelner et al., SODA 2014), we can construct a
$(1+\epsilon)$-approximate flow-equivalent tree (which is a slightly weaker
notion) in time $n^{2+o(1)}$. This leads to the first
$(1+\epsilon)$-approximation for All-Pairs Max-Flow that runs in time
$n^{2+o(1)}$, and matches the output size almost-optimally.
</p></div>
    </summary>
    <updated>2020-09-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.06063</id>
    <link href="http://arxiv.org/abs/2009.06063" rel="alternate" type="text/html"/>
    <title>On Fault Tolerant Feedback Vertex Set</title>
    <feedworld_mtime>1600128000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Misra:Pranabendu.html">Pranabendu Misra</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.06063">PDF</a><br/><b>Abstract: </b>The study of fault-tolerant data structures for various network design
problems is a prominent area of research in computer science. Likewise, the
study of NP-Complete problems lies at the heart of computer science with
numerous results in algorithms and complexity. In this paper we raise the
question of computing fault tolerant solutions to NP-Complete problems; that is
computing a solution that can survive the "failure" of a few constituent
elements. This notion has appeared in a variety of theoretical and practical
settings such as estimating network reliability, kernelization (aka instance
compression), approximation algorithms and so on. In this paper, we seek to
highlight these questions for further research.
</p>
<p>As a concrete example, we study the fault-tolerant version of the classical
Feedback Vertex Set (FVS) problem, that we call Fault Tolerant Feedback Vertex
Set (FT-FVS). Recall that, in FVS the input is a graph $G$ and the objective is
to compute a minimum subset of vertices $S$ such that $G-S$ is a forest. In
FT-FVS, the objective is to compute a minimum subset $S$ of vertices such that
$G - (S \setminus \{v\})$ is a forest for any $v \in V(G)$. Here the vertex $v$
denotes a single vertex fault. We show that this problem is NP-Complete, and
then present a constant factor approximation algorithm as well as an
FPT-algorithm parameterized by the solution size. We believe that the question
of computing fault tolerant solutions to various NP-Complete problems is an
interesting direction for future research.
</p></div>
    </summary>
    <updated>2020-09-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.06043</id>
    <link href="http://arxiv.org/abs/2009.06043" rel="alternate" type="text/html"/>
    <title>Simple, Deterministic, Constant-Round Coloring in the Congested Clique</title>
    <feedworld_mtime>1600128000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Czumaj:Artur.html">Artur Czumaj</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Davies:Peter.html">Peter Davies</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Parter:Merav.html">Merav Parter</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.06043">PDF</a><br/><b>Abstract: </b>We settle the complexity of the $(\Delta+1)$-coloring and $(\Delta+1)$-list
coloring problems in the CONGESTED CLIQUE model by presenting a simple
deterministic algorithm for both problems running in a constant number of
rounds. This matches the complexity of the recent breakthrough randomized
constant-round $(\Delta+1)$-list coloring algorithm due to Chang et al.
(PODC'19), and significantly improves upon the state-of-the-art $O(\log
\Delta)$-round deterministic $(\Delta+1)$-coloring bound of Parter (ICALP'18).
</p>
<p>A remarkable property of our algorithm is its simplicity. Whereas the
state-of-the-art randomized algorithms for this problem are based on the quite
involved local coloring algorithm of Chang et al. (STOC'18), our algorithm can
be described in just a few lines. At a high level, it applies a careful
derandomization of a recursive procedure which partitions the nodes and their
respective palettes into separate bins. We show that after $O(1)$ recursion
steps, the remaining uncolored subgraph within each bin has linear size, and
thus can be solved locally by collecting it to a single node. This algorithm
can also be implemented in the Massively Parallel Computation (MPC) model
provided that each machine has linear (in $n$, the number of nodes in the input
graph) space.
</p>
<p>We also show an extension of our algorithm to the MPC regime in which
machines have sublinear space: we present the first deterministic
$(\Delta+1)$-list coloring algorithm designed for sublinear-space MPC, which
runs in $O(\log \Delta + \log\log n)$ rounds.
</p></div>
    </summary>
    <updated>2020-09-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.06024</id>
    <link href="http://arxiv.org/abs/2009.06024" rel="alternate" type="text/html"/>
    <title>Extracting Optimal Solution Manifolds using Constrained Neural Optimization</title>
    <feedworld_mtime>1600128000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singh:Gurpreet.html">Gurpreet Singh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Soumyajit.html">Soumyajit Gupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lease:Matthew.html">Matthew Lease</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.06024">PDF</a><br/><b>Abstract: </b>Constrained Optimization solution algorithms are restricted to point based
solutions. In practice, single or multiple objectives must be satisfied,
wherein both the objective function and constraints can be non-convex resulting
in multiple optimal solutions. Real world scenarios include intersecting
surfaces as Implicit Functions, Hyperspectral Unmixing and Pareto Optimal
fronts. Local or global convexification is a common workaround when faced with
non-convex forms. However, such an approach is often restricted to a strict
class of functions, deviation from which results in sub-optimal solution to the
original problem. We present neural solutions for extracting optimal sets as
approximate manifolds, where unmodified, non-convex objectives and constraints
are defined as modeler guided, domain-informed $L_2$ loss function. This
promotes interpretability since modelers can confirm the results against known
analytical forms in their specific domains. We present synthetic and realistic
cases to validate our approach and compare against known solvers for
bench-marking in terms of accuracy and computational efficiency.
</p></div>
    </summary>
    <updated>2020-09-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-09-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.05870</id>
    <link href="http://arxiv.org/abs/2009.05870" rel="alternate" type="text/html"/>
    <title>Open Problem: Average-Case Hardness of Hypergraphic Planted Clique Detection</title>
    <feedworld_mtime>1600128000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Luo:Yuetian.html">Yuetian Luo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Anru_R=.html">Anru R. Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.05870">PDF</a><br/><b>Abstract: </b>We note the significance of hypergraphic planted clique (HPC) detection in
the investigation of computational hardness for a range of tensor problems. We
ask if more evidence for the computational hardness of HPC detection can be
developed. In particular, we conjecture if it is possible to establish the
equivalence of the computational hardness between HPC and PC detection.
</p></div>
    </summary>
    <updated>2020-09-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-09-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.05833</id>
    <link href="http://arxiv.org/abs/2009.05833" rel="alternate" type="text/html"/>
    <title>Kunneth Theorems for Vietoris-Rips Homology</title>
    <feedworld_mtime>1600128000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rieser:Antonio.html">Antonio Rieser</a>, Alejandra Trujillo <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.05833">PDF</a><br/><b>Abstract: </b>We prove a Kunneth theorem for the Vietoris-Rips homology and cohomology of a
semi-uniform space. We then interpret this result for graphs, where we show
that the Kunneth theorem holds for graphs with respect to the strong graph
product. We finish by computing the Vietoris-Rips cohomology of the torus
endowed with diferent semi-uniform structures.
</p></div>
    </summary>
    <updated>2020-09-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-09-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.05823</id>
    <link href="http://arxiv.org/abs/2009.05823" rel="alternate" type="text/html"/>
    <title>On Achieving Fairness and Stability in Many-to-One Matchings</title>
    <feedworld_mtime>1600128000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Narang:Shivika.html">Shivika Narang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Biswas:Arpita.html">Arpita Biswas</a>, Y Narahari <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.05823">PDF</a><br/><b>Abstract: </b>Matching algorithms have been classically studied with the goal of finding
stable solutions. However, in many important societal problems, the degree of
fairness in the matching assumes crucial importance, for instance when we have
to match COVID-19 patients to care units. We study the problem of finding a
stable many-to-one matching while satisfying fairness among all the agents with
cardinal utilities. We consider various fairness definitions from fair
allocation literature, such as envy-freeness (EF) and leximin optimal fairness.
We find that EF and its weaker versions are incompatible with stability, even
under a restricted setting with isometric utilities. We focus on leximin
optimal fairness and show that finding such a matching is NP-Hard, even under
isometric utilities. Next, we narrow our focus onto ranked isometric utilities
and provide a characterisation for the space of stable matchings. We present a
novel and efficient algorithm that finds the leximin optimal stable matching
under ranked isometric utilities. To the best of our knowledge, we are the
first to address the problem of finding a leximin optimally fair and stable
matching.
</p></div>
    </summary>
    <updated>2020-09-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.05820</id>
    <link href="http://arxiv.org/abs/2009.05820" rel="alternate" type="text/html"/>
    <title>Empty axis-parallel boxes</title>
    <feedworld_mtime>1600128000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bukh:Boris.html">Boris Bukh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chao:Ting=Wei.html">Ting-Wei Chao</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.05820">PDF</a><br/><b>Abstract: </b>We show that, for every set of $n$ points in $d$-dimensional unit cube, there
is an empty axis-parallel box of volume at least $\Omega(d/n)$. In the opposite
direction, we give a construction without an empty axis-parallel box of volume
$O(d^2\log d/n)$. These improve on the previous best bounds of $\Omega(\log
d/n)$ and $O(2^{7d}/n)$ respectively.
</p></div>
    </summary>
    <updated>2020-09-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-09-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.05776</id>
    <link href="http://arxiv.org/abs/2009.05776" rel="alternate" type="text/html"/>
    <title>Terminating cases of flooding</title>
    <feedworld_mtime>1600128000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hussak:Walter.html">Walter Hussak</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Trehan:Amitabh.html">Amitabh Trehan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.05776">PDF</a><br/><b>Abstract: </b>Basic synchronous flooding proceeds in rounds. Given a finite undirected
(network) graph $G$, a set of sources $I \subseteq G$ initiate flooding in the
first round by every node in $I$ sending the same message to all of its
neighbours. In each subsequent round, nodes send the message to all of their
neighbours from which they did not receive the message in the previous round.
Flooding terminates when no node in $G$ sends a message in a round. The
question of termination has not been settled - rather, non-termination is
implicitly assumed to be possible.
</p>
<p>We show that flooding terminates on every finite graph. In the case of a
single source $g_0$, flooding terminates in $e$ rounds if $G$ is bipartite and
$j$ rounds with $e &lt; j \leq e+d+1$ otherwise, where $e$ and $d$ are the
eccentricity of $g_0$ and diameter of $G$ respectively. For
communication/broadcast to all nodes, this is asymptotically time optimal and
obviates the need for construction and maintenance of spanning structures. We
extend to dynamic flooding initiated in multiple rounds with possibly multiple
messages. The cases where a node only sends a message to neighbours from which
it did not receive {\it any} message in the previous round, and where a node
sends some highest ranked message to all neighbours from which it did not
receive {\it that} message in the previous round, both terminate. All these
cases also hold if the network graph loses edges over time. Non-terminating
cases include asynchronous flooding, flooding where messages have fixed delays
at edges, cases of multiple-message flooding and cases where the network graph
acquires edges over time.
</p></div>
    </summary>
    <updated>2020-09-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.05736</id>
    <link href="http://arxiv.org/abs/2009.05736" rel="alternate" type="text/html"/>
    <title>Robust production planning with budgeted cumulative demand uncertainty</title>
    <feedworld_mtime>1600128000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guillaume:Romain.html">Romain Guillaume</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kasperski:Adam.html">Adam Kasperski</a>, Pawel Zielinski <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.05736">PDF</a><br/><b>Abstract: </b>This paper deals with a problem of production planning, which is a version of
the capacitated single-item lot sizing problem with backordering under demand
uncertainty, modeled by uncertain cumulative demands. The well-known interval
budgeted uncertainty representation is assumed. Two of its variants are
considered. The first one is the discrete budgeted uncertainty, in which at
most a specified number of cumulative demands can deviate from their nominal
values at the same time.The second variant is the continuous budgeted
uncertainty, in which the sum of the deviations of cumulative demands from
their nominal values, at the same time, is at most a bound on the total
deviation provided. For both cases, in order to choose a production plan that
hedges against the cumulative demand uncertainty, the robust minmax criterion
is used. Polynomial algorithms for evaluating the impact of uncertainty in the
demand on a given production plan in terms of its cost, called the adversarial
problem, and for finding robust production plans under the discrete budgeted
uncertainty are constructed. Hence, in this case, the problems under
consideration are not much computationally harder than their deterministic
counterparts. For the continuous budgeted uncertainty, it is shown that the
adversarial problem and the problem of computing a robust production plan along
with its worst-case cost are NP-hard. In the case, when uncertainty intervals
are non-overlapping, they can be solved in pseudopolynomial time and admit
fully polynomial timeapproximation schemes. In the general case, a
decomposition algorithm for finding a robust plan is proposed.
</p></div>
    </summary>
    <updated>2020-09-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-09-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2009.05685</id>
    <link href="http://arxiv.org/abs/2009.05685" rel="alternate" type="text/html"/>
    <title>Linear Shannon Capacity of Cayley Graphs</title>
    <feedworld_mtime>1600128000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guruswami:Venkatesan.html">Venkatesan Guruswami</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Riazanov:Andrii.html">Andrii Riazanov</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2009.05685">PDF</a><br/><b>Abstract: </b>The Shannon capacity of a graph is a fundamental quantity in zero-error
information theory measuring the rate of growth of independent sets in graph
powers. Despite being well-studied, this quantity continues to hold several
mysteries. Lov\'asz famously proved that the Shannon capacity of $C_5$ (the
5-cycle) is at most $\sqrt{5}$ via his theta function. This bound is achieved
by a simple linear code over $\mathbb{F}_5$ mapping $x \mapsto 2x$. Motivated
by this, we introduce the notion of $\textit{linear Shannon capacity}$ of
graphs, which is the largest rate achievable when restricting oneself to linear
codes. We give a simple proof based on the polynomial method that the linear
Shannon capacity of $C_5$ is $\sqrt{5}$. Our method applies more generally to
Cayley graphs over the additive group of finite fields $\mathbb{F}_q$. We
compare our bound to the Lov\'asz theta function, showing that they match for
self-complementary Cayley graphs (such as $C_5$), and that our bound is smaller
in some cases. We also exhibit a quadratic gap between linear and general
Shannon capacity for some graphs.
</p></div>
    </summary>
    <updated>2020-09-15T23:20:43Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-09-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1712.03660</id>
    <link href="http://arxiv.org/abs/1712.03660" rel="alternate" type="text/html"/>
    <title>Parallel Mapper</title>
    <feedworld_mtime>1600128000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hajij:Mustafa.html">Mustafa Hajij</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Assiri:Basem.html">Basem Assiri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rosen:Paul.html">Paul Rosen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1712.03660">PDF</a><br/><b>Abstract: </b>The construction of Mapper has emerged in the last decade as a powerful and
effective topological data analysis tool that approximates and generalizes
other topological summaries, such as the Reeb graph, the contour tree, split,
and joint trees. In this paper, we study the parallel analysis of the
construction of Mapper. We give a provably correct parallel algorithm to
execute Mapper on multiple processors and discuss the performance results that
compare our approach to a reference sequential Mapper implementation. We report
the performance experiments that demonstrate the efficiency of our method.
</p></div>
    </summary>
    <updated>2020-09-15T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-09-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/09/14/postdoc-position-in-theoretical-computer-science-foundations-of-ai-at-aarhus-university-denmark-apply-by-october-9-2020/</id>
    <link href="https://cstheory-jobs.org/2020/09/14/postdoc-position-in-theoretical-computer-science-foundations-of-ai-at-aarhus-university-denmark-apply-by-october-9-2020/" rel="alternate" type="text/html"/>
    <title>Postdoc position in Theoretical Computer Science/Foundations of AI at Aarhus University, Denmark (apply by October 9, 2020)</title>
    <summary>A 2-year postdoc position in Theoretical Computer Science/Algorithmic Foundations of AI is available in the Algorithms and Data Structures group at Aarhus University, Denmark. Candidates should have a recent PhD in Computer Science, Mathematics, or Economics on topics that fall within algorithmic game theory or computational social choice, and a strong publication record. Website: https://international.au.dk/about/profile/vacant-positions/job/department-of-computer-science-is-looking-for-a-post-doc-in-theoretical-computer-science-algorithm/ […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A 2-year postdoc position in Theoretical Computer Science/Algorithmic Foundations of AI is available in the Algorithms and Data Structures group at Aarhus University, Denmark.</p>
<p>Candidates should have a recent PhD in Computer Science, Mathematics, or Economics on topics that fall within algorithmic game theory or computational social choice, and a strong publication record.</p>
<p>Website: <a href="https://international.au.dk/about/profile/vacant-positions/job/department-of-computer-science-is-looking-for-a-post-doc-in-theoretical-computer-science-algorithm/">https://international.au.dk/about/profile/vacant-positions/job/department-of-computer-science-is-looking-for-a-post-doc-in-theoretical-computer-science-algorithm/</a><br/>
Email: iannis@cs.au.dk</p></div>
    </content>
    <updated>2020-09-14T17:10:58Z</updated>
    <published>2020-09-14T17:10:58Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-09-16T15:20:47Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6364597152143042105</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6364597152143042105/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/an-interesting-serendipitous-number.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6364597152143042105" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6364597152143042105" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/an-interesting-serendipitous-number.html" rel="alternate" type="text/html"/>
    <title>An interesting serendipitous number</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> Last seek I blogged about two math problems of interest to me <a href="https://blog.computationalcomplexity.org/2020/09/two-math-problems-of-interest-at-least.html">here</a>.</p><p>One of them two people posted answers, which was great since I didn't know how to solve them and now I do. Yeah! I blogged about that <a href="https://blog.computationalcomplexity.org/2020/09/when-are-both-x23y-and-y23y-both.html">here</a>.</p><p><br/></p><p>The other problem got no comments, so I suppose it was of interest to me but not others. I was interested in it because the story behind it is interesting, and the answer is interesting.</p><p><br/></p><p>it is from the paper </p><p>An interesting and serendipitous number by John Ewing and Ciprian Foias, which is a chapter in the wonderful book </p><p>Finite vs Infinite: Contributions to an eternal dilemma</p><p>Here is the story, I paraphrase the article (I'll give pointers  later).</p><p>In the mid 1970's a student asked Ciprian about the following math-competition problem:</p><p>x(1)&gt;0    x(n+1) =  (1 + (1/x(n)))^n. For which x(1) does x(n) --&gt; infinity?</p><p>It turned out this was a misprint. The actual problem was</p><p>x(1)&gt;0  x(n+1)=(1+(1/x(n))^{x(n)}. For which x(1) does x(n) --&gt; infinity.</p><p><br/></p><p>The actual math-comp problem  (with exp x(n)) is fairly easy (I leave it to you.) But this left the misprinted problem (with exp n).  Crispian proved that there is exactly ONE x(1) such that x(n)--&gt; infinity. </p><p>Its approx 1.187... and may be trans.</p><p><br/></p><p>I find the story and the result interesting, but the proof is to long for a blog post.</p><p>I tried to find the article online and could not. A colleague found the following:</p><p><br/></p><p>A preview of the start of the article <a href="https://link.springer.com/chapter/10.1007/978-1-4471-0751-4_8">here</a></p><p>Wikipedia Page on the that number, called the Foias constant, <a href="https://en.wikipedia.org/wiki/Foias_constant">here</a></p><p>Mathworld page on that number <a href="https://mathworld.wolfram.com/FoiasConstant.html">here</a></p><p>Most of the article but skips two pages <a href="https://books.google.com/books?id=Bjb0BwAAQBAJ&amp;pg=PA119&amp;lpg=PA119&amp;dq=serendipitous+number+John+Ewing+and+Ciprian+Foias&amp;source=bl&amp;ots=4tn1sk3XEA&amp;sig=ACfU3U3GM9VtlyWxTjq302E5Uf7Tmr49Hw&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwiewLSF9OjrAhVkmXIEHRYEAMA4ChDoATAAegQICRAB#v=onepage&amp;q=serendipitous%20number%20John%20Ewing%20and%20Ciprian%20Foias&amp;f=false">here</a></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2020-09-14T16:16:00Z</updated>
    <published>2020-09-14T16:16:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-09-16T08:22:02Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2020-09-14-broadcast-from-agreement-and-agreement-from-broadcast/</id>
    <link href="https://decentralizedthoughts.github.io/2020-09-14-broadcast-from-agreement-and-agreement-from-broadcast/" rel="alternate" type="text/html"/>
    <title>Broadcast from Agreement and Agreement from Broadcast</title>
    <summary>In this post, we highlight the connection between Broadcast and Agreement in the synchronous model. Broadcast and Agreement: How can you implement one from the other? We defined Agreement and Broadcast in a previous post, here is a recap: Agreement A set of $n$ nodes where each node $i$ has...</summary>
    <updated>2020-09-14T14:07:00Z</updated>
    <published>2020-09-14T14:07:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2020-09-15T23:38:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/139</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/139" rel="alternate" type="text/html"/>
    <title>TR20-139 |  The Coin Problem with Applications to Data Streams | 

	Mark Braverman, 

	Sumegha Garg, 

	David Woodruff</title>
    <summary>Consider the problem of computing the majority of a stream of $n$ i.i.d. uniformly random bits. This problem, known as the {\it coin problem}, is central to a number of counting problems in different data stream models. We show that any streaming algorithm for solving this problem with large constant advantage must use $\Omega(\log n)$ bits of space. We extend our lower bound to proving tight lower bounds for solving multiple, randomly interleaved copies of the coin problem, as well as for solving the OR of multiple copies of a variant of the coin problem. Our proofs involve new measures of information complexity that are well-suited for data streams. 

We use these lower bounds to obtain a number of new results for data streams. In each case there is an underlying $d$-dimensional vector $x$ with additive updates to its coordinates given in a stream of length $m$. The input streams arising from our coin lower bound have nice distributional properties, and consequently for many problems for which we only had lower bounds in general turnstile streams, we now obtain the same lower bounds in more natural models, such as the bounded deletion model, in which $\|x\|_2$ never drops by a constant fraction of what it was earlier, or in the random order model, in which the updates are ordered randomly. In particular, in the bounded deletion model, we obtain nearly tight lower bounds for approximating $\|x\|_{\infty}$ up to additive error $\frac{1}{\sqrt{k}} \|x\|_2$, approximating $\|x\|_2$ up to a multiplicative $(1 + \epsilon)$ factor (resolving a question of Jayaram and Woodruff in PODS 2018), and solving the Point Query and $\ell_2$-Heavy Hitters Problems. In the random order model, we also obtain new lower bounds for the Point Query and $\ell_2$-Heavy Hitters Problems. 
We also give new algorithms complementing our lower bounds and illustrating the tightness of the models we consider, including an algorithm for approximating $\|x\|_{\infty}$ up to additive error $\frac{1}{\sqrt{k}} \|x\|_2$ in turnstile streams (resolving a question of Cormode in a 2006 IITK Workshop), and an algorithm for finding $\ell_2$-heavy hitters in randomly ordered insertion streams (which for random order streams, resolves a question of Nelson in a 2018 Warwick Workshop).</summary>
    <updated>2020-09-14T02:02:37Z</updated>
    <published>2020-09-14T02:02:37Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-16T15:20:34Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2020-09-13-synchronous-consensus-omission-faults/</id>
    <link href="https://decentralizedthoughts.github.io/2020-09-13-synchronous-consensus-omission-faults/" rel="alternate" type="text/html"/>
    <title>Commit-Notify Paradigm for Synchronous Consensus with Omission Faults</title>
    <summary>We continue our series of posts on State Machine Replication (SMR). In this post, we move from consensus under crash failures to consensus under omission failures. We still keep the synchrony assumption. Let’s begin with a quick overview of what we covered in previous posts: Upper bound: We can tolerate...</summary>
    <updated>2020-09-13T19:09:00Z</updated>
    <published>2020-09-13T19:09:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2020-09-15T23:38:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17570</id>
    <link href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/" rel="alternate" type="text/html"/>
    <title>Convex Algorithms</title>
    <summary>Continuous can beat discrete Nisheeth Vishnoi is a professor at Yale University in the computer science department. The faculty there is impressive and includes many of the top researchers in the world. The CS faculty is pretty good too. As Nisheeth’s PhD advisor, years ago, I am proud that he is at Yale. Today I […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Continuous can beat discrete</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/nv/" rel="attachment wp-att-17573"><img alt="" class="alignright  wp-image-17573" src="https://rjlipton.files.wordpress.com/2020/09/nv.png?w=150" width="150"/></a>
</td>
</tr>
<tr>
</tr>
</tbody>
</table>
<p>
Nisheeth Vishnoi is a professor at Yale University in the computer science department. The faculty <a href="https://cpsc.yale.edu/people/faculty">there</a> is impressive and includes many of the top researchers in the world. The CS faculty is pretty good too. As Nisheeth’s PhD advisor, years ago, I am proud that he is at Yale. </p>
<p>
Today I wish to discuss a new book by Nisheeth. </p>
<p>
The title is <a href="https://convex-optimization.github.io/ACO-v1.pdf">Algorithms for Convex Optimization</a>. Let me jump ahead and say that I like the book and especially this insight: </p>
<blockquote><p><b> </b> <em> <i>One way to solve discrete problems is to apply continuous methods.</i> </em>
</p></blockquote>
<p>This is not a new insight, but is an important one. Continuous math is older than discrete and often is more powerful. Some examples of this are:</p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> Analytic number theory is <a href="https://en.wikipedia.org/wiki/Analytic_number_theory">based</a> on the behavior of continuous functions. Some of the deepest theorems on prime numbers use such methods. Think of the Riemann zeta function 	</p>
<p align="center"><img alt="\displaystyle  \zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s} = \frac{1}{1^s} + \frac{1}{2^s} + \frac{1}{3^s} + \cdots " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Czeta%28s%29+%3D+%5Csum_%7Bn%3D1%7D%5E%5Cinfty+%5Cfrac%7B1%7D%7Bn%5Es%7D+%3D+%5Cfrac%7B1%7D%7B1%5Es%7D+%2B+%5Cfrac%7B1%7D%7B2%5Es%7D+%2B+%5Cfrac%7B1%7D%7B3%5Es%7D+%2B+%5Ccdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s} = \frac{1}{1^s} + \frac{1}{2^s} + \frac{1}{3^s} + \cdots "/></p>
<p>as a function of complex numbers <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/>.</p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> Additive number theory is <a href="https://encyclopediaofmath.org/wiki/Additive_number_theory">based</a> on the behavior of continuous functions. Think of generating functions and Fourier methods. </p>
<p>
The power of continuous methods is one that I sometimes forget. Nisheeth’s book is a testament to the power of this idea. </p>
<p>
</p><p/><h2> Convexity </h2><p/>
<p/><p>
Nisheeth’s book uses another fundamental idea from complexity theory. This is: restrict problems in some way. Allowing too large a class usually makes complexity high. For example, trees are easier in general than planar graphs, and sparse graphs are easier than general graphs. Of course “in general” must be controlled, but restricting the problem types does often reduce complexity. </p>
<p>
Convexity adds to this tradition since <em>convex</em> generalizes the notion of <em>linear</em>. And convex problems of all kinds are abundant in practice, abundant in theory, and are important.</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/mw/" rel="attachment wp-att-17574"><img alt="" class="aligncenter size-full wp-image-17574" src="https://rjlipton.files.wordpress.com/2020/09/mw.png?w=600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
The MW dictionary says <a href="https://www.merriam-webster.com/dictionary/convex">convex</a> means: </p>
<blockquote><p><b> </b> <em> : being a continuous function or part of a continuous function with the property that a line joining any two points on its graph lies on or above the graph. </em>
</p></blockquote>
<p>
</p><p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/con2/" rel="attachment wp-att-17575"><img alt="" class="aligncenter size-full wp-image-17575" src="https://rjlipton.files.wordpress.com/2020/09/con2.png?w=600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
Here is a passage by Roman Dwilewicz on the <a href="http://www.mathem.pub.ro/dgds/v11/D11-DW.pdf">history</a> of the convexity concept:</p>
<blockquote><p><b> </b> <em> It was known to the ancient Greeks that there are only five regular <i>convex</i> polyhedra. </em></p><em>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/solid/" rel="attachment wp-att-17576"><img alt="" class="aligncenter size-full wp-image-17576" src="https://rjlipton.files.wordpress.com/2020/09/solid.png?w=600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>It seems that the first more rigorous definition of convexity was given by <a href="https://en.wikipedia.org/wiki/Archimedes">Archimedes</a> of Syracuse, (ca 287 – ca 212 B.C.) in his treatise: <a href="https://en.wikipedia.org/wiki/On_the_Sphere_and_Cylinder">On the sphere and cylinder</a>. </p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/arch/" rel="attachment wp-att-17578"><img alt="" class="aligncenter size-full wp-image-17578" src="https://rjlipton.files.wordpress.com/2020/09/arch.png?w=600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
</em><p><em>
These definitions and postulates of Archimedes were dormant for about two thousand years! </em>
</p></blockquote>
<p>I say it’s lucky that Archimedes was not up for tenure.</p>
<p>
</p><p/><h2> Nisheeth’s Book </h2><p/>
<p/><p>
Nisheeth’s book is now available at this <a href="https://convex-optimization.github.io">site</a>. I have just started to examine it and must say I like the book. Okay, I am not an expert on convex algorithms, nor am I an expert on this type of geometric theory. But I definitely like his viewpoint. Let me explain in a moment. </p>
<p>
First I cannot resist adding some statistics about his book created <a href="https://countwordsfree.com/">here</a>:</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/stat/" rel="attachment wp-att-17579"><img alt="" class="aligncenter size-full wp-image-17579" src="https://rjlipton.files.wordpress.com/2020/09/stat.png?w=600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
No way I can read the book in nine hours. But I like seeing how many characters and so on the book has. I will have to calculate the same for other books. </p>
<p>
</p><p/><h2> Discrete vs Continuous Methods </h2><p/>
<p/><p>
Nisheeth in his introduction explains how continuous methods help in many combinatorial problems, like finding flows on graphs. He uses the <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/>—<img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> flow problem as his example. The <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/>—<img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>-maximum flow problem arises in real-world scheduling problems, but is also a fundamental combinatorial problem that can be used to find a maximum matching in a bipartite graph, for example.</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/13/convex-algorithms/flow-3/" rel="attachment wp-att-17581"><img alt="" class="aligncenter size-full wp-image-17581" src="https://rjlipton.files.wordpress.com/2020/09/flow.png?w=600"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<p>
<i>Combinatorial algorithms for the maximum flow problem</i>. He points out that by building on the Ford-Fulkerson method, various polynomial-time results were proved and other bounds were improved. But he states that the <i>improvements stopped in 1998.</i> Discrete methods seem to be unable to improve complexity for flow problems. </p>
<p>
<i>Convex programming-based algorithms</i>. He adds: </p>
<blockquote><p><b> </b> <em> Starting with the <a href="https://arxiv.org/pdf/1010.2921.pdf">paper</a> by Paul Christiano, Jonathan Kelner, Aleksander Mądry, Daniel Spielman, Shang-Hua Teng<br/>
the last decade has seen striking progress on the <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{s}"/>–<img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{t}"/> maximum flow problem. One of the keys to this success has been to abandon combinatorial approaches and view the <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{s}"/>–<img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{t}"/> maximum flow problem through the lens of continuous optimization.</em></p><em>
</em><p><em>
Thus, at this point it may seem like we are heading in the wrong direction. We started off with a combinatorial problem that is a special type of a linear programming problem, and here we are with a nonlinear optimization formulation for it. Thus the questions arise: which formulation should we chose? and, why should this convex optimization approach lead us to faster algorithms? </em>
</p></blockquote>
<p>Indeed. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Take a look at Nisheeth’s <a href="https://convex-optimization.github.io">site</a> for the answers.</p>
<p>
I wish I were better informed about continuous methods in general. They are powerful and pretty. Maybe I could solve an open problem that I have thought about if I knew this material better. Hmmm. Maybe it will help you solve some open problem of your own. Take a look at his book.</p>
<p>[Edited] </p></font></font></div>
    </content>
    <updated>2020-09-13T18:27:14Z</updated>
    <published>2020-09-13T18:27:14Z</published>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="People"/>
    <category term="Algorithms"/>
    <category term="continuous"/>
    <category term="convexity"/>
    <category term="discrete mathematics"/>
    <category term="flow"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-09-16T15:20:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/138</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/138" rel="alternate" type="text/html"/>
    <title>TR20-138 |  Pseudorandom Generators for Unbounded-Width Permutation Branching Programs | 

	William Hoza, 

	Edward Pyne, 

	Salil Vadhan</title>
    <summary>We prove that the Impagliazzo-Nisan-Wigderson (STOC 1994) pseudorandom generator (PRG) fools ordered (read-once) permutation branching programs of unbounded width with a seed length of $\widetilde{O}(\log d + \log n \cdot \log(1/\varepsilon))$, assuming the program has only one accepting vertex in the final layer. Here, $n$ is the length of the program, $d$ is the degree (equivalently, the alphabet size), and $\varepsilon$ is the error of the PRG. In contrast, we show that a randomly chosen generator requires seed length $\Omega(n \log d)$ to fool such unbounded-width programs. Thus, this is an unusual case where an explicit construction is "better than random."

Except when the program's width $w$ is very small, this is an improvement over prior work. For example, when $w = \text{poly}(n)$ and d = 2, the best prior PRG for permutation branching programs was simply Nisan's PRG (Combinatorica 1992), which fools general ordered branching programs with seed length $O(\log(wn/\varepsilon) \log n)$. We prove a seed length lower bound of $\widetilde{\Omega}(\log d + \log n \cdot \log(1/\varepsilon))$ for fooling these unbounded-width programs, showing that our seed length is near-optimal. In fact, when $\varepsilon \leq 1 / \log n$, our seed length is within a constant factor of optimal. Our analysis of the INW generator uses the connection between the PRG and the derandomized square of Rozenman and Vadhan (RANDOM 2005) and the recent analysis of the latter in terms of unit-circle approximation by Ahmadinejad et al. (FOCS 2020).</summary>
    <updated>2020-09-13T03:39:42Z</updated>
    <published>2020-09-13T03:39:42Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-16T15:20:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/137</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/137" rel="alternate" type="text/html"/>
    <title>TR20-137 |  Deterministic and Efficient Interactive Coding from Hard-to-Decode Tree Codes | 

	Zvika Brakerski, 

	Yael Tauman Kalai, 

	Raghuvansh Saxena</title>
    <summary>The field of Interactive Coding studies how an interactive protocol can be made resilient to channel errors. Even though this field has received abundant attention since Schulman's seminal paper (FOCS 92), constructing interactive coding schemes that are both deterministic and efficient, and at the same time resilient to adversarial errors (with constant information and error rates), remains an elusive open problem.

An appealing approach towards resolving this problem is to show efficiently encodable and decodable constructions of a combinatorial object called tree codes (Schulman, STOC 93). After a lot of effort in this direction, the current state of the art has deterministic constructions of tree codes that are efficiently encodable but require a logarithmic (instead of constant) alphabet (Cohen, Haeupler, and Schulman, STOC 18). However, we still lack (even heuristic) candidate constructions that are efficiently decodable. 

In this work, we show that tree codes that are efficiently encodable, {\em but not efficiently decodable}, also imply deterministic and efficient interactive coding schemes that are resilient to adversarial errors. Our result immediately implies a deterministic and efficient interactive coding scheme with a logarithmic alphabet (i.e., $1/\log \log$ rate). We show this result using a novel implementation of hashing through deterministic tree codes that is powerful enough to yield interactive coding schemes.</summary>
    <updated>2020-09-11T16:55:01Z</updated>
    <published>2020-09-11T16:55:01Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-16T15:20:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/136</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/136" rel="alternate" type="text/html"/>
    <title>TR20-136 |  Explicit and structured sum of squares lower bounds from high dimensional expanders | 

	Irit Dinur, 

	Yuval Filmus, 

	Prahladh Harsha, 

	Madhur Tulsiani</title>
    <summary>We construct an explicit family of 3XOR instances which is hard for Omega(sqrt(log n)) levels of the Sum-of-Squares hierarchy. In contrast to earlier constructions, which involve a random component, our systems can be constructed explicitly in deterministic polynomial time.
Our construction is based on the high-dimensional expanders devised by Lubotzky, Samuels and Vishne, known as LSV complexes or Ramanujan complexes, and our analysis is based on two notions of expansion for these complexes: cosystolic expansion, and a local isoperimetric inequality due to Gromov.
Our construction offers an interesting contrast to the recent work of Alev, Jeronimo and the last author (FOCS 2019). They showed that 3XOR instances in which the variables correspond to vertices in a high-dimensional expander are easy to solve. In contrast, in our instances the variables correspond to the edges of the complex.</summary>
    <updated>2020-09-11T04:58:17Z</updated>
    <published>2020-09-11T04:58:17Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-09-16T15:20:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17542</id>
    <link href="https://rjlipton.wordpress.com/2020/09/10/hybrid-versus-remote-teaching/" rel="alternate" type="text/html"/>
    <title>Hybrid Versus Remote Teaching</title>
    <summary>Which is best for students? Cropped from Wikipedia src Moshe Vardi holds multiple professorships at Rice University. He is also the Senior Editor of Communications of the ACM. His is therefore a voice to be reckoned with in the current debate over how best to teach during the pandemic. Much of the debate is over […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Which is best for students?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/10/hybrid-versus-remote-teaching/330px-moshe_vardi_img_0010/" rel="attachment wp-att-17544"><img alt="" class="alignright wp-image-17544" height="150" src="https://rjlipton.files.wordpress.com/2020/09/330px-moshe_vardi_img_0010.jpg?w=121&amp;h=150" width="121"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from Wikipedia <a href="https://en.wikipedia.org/wiki/Moshe_Vardi#/media/File:Moshe_Vardi_IMG_0010.jpg">src</a></font></td>
</tr>
</tbody>
</table>
<p>
Moshe Vardi holds multiple professorships at Rice University. He is also the Senior Editor of <em>Communications of the ACM</em>. His is therefore a voice to be reckoned with in the current debate over how best to teach during the pandemic. Much of the debate is over whether all should hear his voice the same way, or some hear it in the classroom while others hear it remotely. </p>
<p>
Today we note his recent <a href="https://medium.com/@vardi/covid-19-the-ford-pinto-and-american-higher-ed-2b191920b065">column</a> for <em>Medium</em> advocating the former. Then I (Ken) give some of my own impressions.</p>
<p>
His September 5 column followed an August 8 <a href="https://www.ricethresher.org/article/2020/08/return-to-campus-but-to-what-end">opinion</a> given to the Rice student newspaper. Both begin with concern over the conflict between <em>safety</em> and <em>value</em> for students. Much of the value of college—<em>most</em> according to statistics he cites—comes from being collegial: outside the classroom. But many such activities, not only evening parties but informal games and gatherings, are the most unsafe. </p>
<p>
We will focus however on what Moshe says about the nature of instruction for lecture courses. Certainly for laboratory courses there is a sharp trade-off between safety and in-person interaction. But we focus here on what he says about the nature of teaching in the lecture hall, where one can take safety as a given requirement. </p>
<p>
</p><p/><h2> An In-Person Remoteness Paradox </h2><p/>
<p/><p>
I have just returned from sabbatical at the University at Buffalo (UB) and am teaching this fall a small elective 4xx/5xx theory course. It has 15 students, smaller than the 25 in the hypothetical class Moshe describes but of the same order of magnitude. In the spring I will be teaching a larger undergraduate course which is also on target for his concerns. I have taught such a class every spring for a decade. While this assignment is not a new to me, the issue of safety raises tough choices about the delivery options. My options are: </p>
<ol>
<li>
<em>remote-only</em>; <p/>
</li><li>
<em>in-person only</em>; <p/>
</li><li>
<em>hybrid</em> use of 1 and 2 for designated course components; <p/>
</li><li>
<em>hybrid-flexible</em>, meaning 1 and 2 are conducted simultaneously with students free to choose either option, even on a per-lecture basis.
</li></ol>
<p>
I have committed to hybrid-flexible. For my current fall course, I made this commitment in early summer when there was uncertainty over in-person instruction requirements for student visas and registration. I believe that my larger course will be implemented as safely in a large room as my current course. The question is quality.</p>
<p>
Moshe notes right away a paradox for his hypothetical class that could apply to any of modes 2–4; to include the last expressly, I’ve inserted the word “even”:</p>
<blockquote><p> <em> …I realized that [even] the students <b>in the classroom</b> will have to be communicating with me on Zoom, to be heard and recorded. All this, while both the students and I are wearing face masks. It dawned on me then that I will be conducting remote teaching <b>in the classroom</b>. </em>
</p></blockquote>
<p/><p/>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/09/10/hybrid-versus-remote-teaching/lecture/" rel="attachment wp-att-17545"><img alt="" class="size-full wp-image-17545" src="https://rjlipton.files.wordpress.com/2020/09/lecture.jpg?w=600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"><em>Business Insider</em> <a href="https://www.businessinsider.com/us-colleges-shutting-down-coronavirus-impact-when-classes-move-online-2020-3">source</a>—yet another variation</font>
</td>
</tr>
</tbody></table>
<p>
In fact, I have one volunteer now in the room logging into Zoom to help with interaction from those attending remotely. This helps because my podium has less space to catch faces and detect questions right away. I do repeat questions so they are picked up in the recording and often redirect them to the class. Still, the mere fact of my not seeing faces alongside the notes and interactive drawings I am sharing makes me feel Moshe’s paradox all the time. This is even though my room allows denser spacing than at Rice, so a class of 25 could sit closer.  Let me, however, say why I love stand-up teaching before addressing his paramount question of what is best for the students at this time.</p>
<p>
</p><p/><h2> From Whiteboards to Tutorials </h2><p/>
<p/><p>
Dick once wrote a <a href="https://rjlipton.wordpress.com/2013/11/07/in-praise-of-chalk-talks/">post</a>, “In Praise of Chalk Talks.” First, with reference to talks pre-made using PowerPoint or LaTeX slides, Dick wrote:</p>
<blockquote><p><b> </b> <em> Such talks can be informative and easy to follow, yet sometimes PowerPoint is not well suited to giving a proof. The slides do not hold enough <b>state</b> for us to easily follow the argument. </em>
</p></blockquote>
<p/><p>
Moreover, when I contributed to the open-problems session of the workshop at IAS in Princeton, NJ that we <a href="https://rjlipton.wordpress.com/2018/06/06/princeton-is-invariant/">covered</a> two years ago, Avi Wigderson insisted that everyone use chalk, not slides. I’ve used slides for UB’s data-structures and programming languages courses, but I think students benefit from seeing proofs and problem-solving ideas <em>grow</em>.</p>
<p>
I find furthermore that the feel of immersion in a process of discovery is enhanced by an in-person presence. I had this in mind when I followed Dick’s post with a long one <a href="https://rjlipton.wordpress.com/2013/11/15/the-graph-of-math/">imagining</a> Kurt Gödel expounding the distinctive points of his set theory (joint with Paul Bernays and John von Neumann), all on one chalkboard. My classes are not as interactive as in that post, but I prepare junctures in lectures for posing questions and doing a little bit of Socratic method. And I try to lead this with body language as well as voice inflection, whether at a whiteboard or drawing on hard paper via a document camera. </p>
<p>
Still, it exactly this “extra” that gets diminished for those who are remote. When I share my screen for notes or a drawing (both in <a href="https://www.mathcha.io/">MathCha</a>), they see my movements only in a small second window if at all. They do hear my voice—but I do not hear theirs even if they unmute themselves. Nor can I read their state of following as I do in the room. Without reiterating the safety factor as Moshe does, I can reformulate his key question as:</p>
<blockquote><p><b> </b> <em> Does the non-uniformity and inequality of hybrid delivery outweigh the benefits of making in-person instruction available to some? </em>
</p></blockquote>
<p/><p>
I must quickly add that in-person teaching is perceived as a collective need at UB. The web form I filled for Spring 2021 stated that some in-person classes must be available at all levels, 1xx through 7xx. I am happy to oblige. But the fact that I chose a flexible structure, especially in a small class, does allow the students to give opinion on this question, as well as on something Moshe says next:</p>
<blockquote><p><b> </b> <em> “Remote teaching” actually can do a better job of reproducing the intimacy that we take for granted in small classes. </em>
</p></blockquote>
<p/><p>
Toward this end, I am implementing a remote version of the <a href="https://en.wikipedia.org/wiki/Tutorial_system">tutorial system</a> I was part of for two eight-week terms at Oxford while a junior fellow of Merton College. When Cambridge University <a href="https://www.bbc.com/news/education-52732814">declared</a> already last May that there would be no in-person lectures all the way through summer 2021, this is because most lectures there are formally optional anyway. The heart of required teaching is via weekly tutorial hours in groups of one-to-three students. They are organized separately by each of thirty-plus constituent colleges rather than by department-centered staff, so the numbers are divided to be manageable. In my math-course tutorials the expectation was for each student to present a solved problem and participate in discussions that build on the methods. </p>
<p>
I am doing this every other week this fall, alternating with weeks of problem-set review that will be strictly optional and classed as enhanced office hours. All UB office hours must be remote anyway. The tutorial requirement was agreed by student voice-vote in a tradeoff with lowering the material in timed exams to compensate for differences in home situations. After a few weeks of this, the class will take stock for opinions on which delivery options work best. UB has already committed to being remote-only after Thanksgiving, and it is possible that the on-campus medical situation will trigger an earlier conversion anyway.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
We would like to throw the floor open for comment on Moshe’s matters that we’ve highlighted and on his other opinions about the university mission amid the current crisis more generally. </p>
<p>
[edited to reflect that at Rice too, the hypothetical class could be in any of modes 2–4, and that spacing is further than in my UB room.  “Princeton IAS” -&gt; “IAS in Princeton, NJ”]</p></font></font></div>
    </content>
    <updated>2020-09-10T22:26:40Z</updated>
    <published>2020-09-10T22:26:40Z</published>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Teaching"/>
    <category term="education quality"/>
    <category term="hybrid instruction"/>
    <category term="Moshe Vardi"/>
    <category term="online courses"/>
    <category term="pandemic"/>
    <category term="remote instruction"/>
    <category term="teaching"/>
    <category term="universities"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-09-16T15:20:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7794</id>
    <link href="https://windowsontheory.org/2020/09/10/sigact-research-highlights-call-for-nominations/" rel="alternate" type="text/html"/>
    <title>SIGACT research highlights – call for nominations</title>
    <summary>TL;DR: Know of a great recent paper that should be highlighted to the theory community and beyond? Email a nomination to sigact.highlights.nominations@outlook.com by October 19th. The goal of the SIGACT Research Highlights Committee is to help promotetop computer science theory research via identifying results that are ofhigh quality and broad appeal to the general computer […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>TL;DR:</strong> Know of a great recent paper that should be highlighted to the theory community and beyond? Email a nomination to <a href="mailto:sigact.highlights.nominations@outlook.com" rel="noreferrer noopener" target="_blank">sigact.highlights.nominations@outlook.com</a> by October 19th.</p>



<p>The goal of the SIGACT Research Highlights Committee is to help promote<br/>top computer science theory research via identifying results that are of<br/>high quality and broad appeal to the general computer science audience.<br/>These results would then be recommended for consideration for the <a href="http://cacm.acm.org">CACM</a> <em>Research Highlights</em> section as well as other general-audience computer science research outlets.</p>



<p><strong>Nomination and Selection Process:</strong></p>



<p>The committee solicits two types of nominations:</p>



<p>1) <strong>Conference nominations.</strong> Each year, the committee will ask the PC<br/>chairs of theoretical computer science conferences to send a selection<br/>of up to three top papers from these conferences (selected based on both<br/>their technical merit and breadth of interest to non-theory audience)<br/>and forwarding them to the committee for considerations.</p>



<p>2) <strong>Community nominations. </strong>The committee will accept nominations from the members of the community. Each such nomination should summarize the contribution of the nominated paper and also argue why this paper is<br/>suitable for broader outreach. The nomination should be no more than a<br/>page in length and can be submitted at any time by emailing it to<br/><a href="mailto:sigact.highlights.nominations@outlook.com" rel="noreferrer noopener" target="_blank">sigact.highlights.nominations@outlook.com</a>. Self-nominations are<br/>discouraged.</p>



<p>The nomination deadline is <strong>Monday, October 19, 2020 </strong>.</p>



<p><strong>Committee:</strong></p>



<p>The SIGACT Research Highlights Committee currently comprises the<br/>following members:</p>



<p>Boaz Barak, Harvard University<br/>Irit Dinur, Weizmann Institute of Science<br/>Aleksander Mądry, Massachusetts Institute of Technology (chair)<br/>Jelani Nelson, University of California, Berkeley</p></div>
    </content>
    <updated>2020-09-10T14:17:37Z</updated>
    <published>2020-09-10T14:17:37Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-09-16T15:21:03Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8975826682758317937</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8975826682758317937/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/when-are-both-x23y-and-y23y-both.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8975826682758317937" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8975826682758317937" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/09/when-are-both-x23y-and-y23y-both.html" rel="alternate" type="text/html"/>
    <title>When are both x^2+3y and y^2+3x both squares, and a more general question</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> In my last post (see <a href="https://blog.computationalcomplexity.org/2020/09/two-math-problems-of-interest-at-least.html">here</a>) I asked two math questions. In this post I discuss one of them. (I will discuss the other one later, probably Monday Sept 14.)</p><p><br/>For which positive naturals x,y are x^2+3y and y^2+3x both squares?</p><p>I found this in a math contest book and could not solve it, so I posted it to see what my readers would come up with. They came up with two solutions, which you can either read in the comments on that post OR read my write up <a href="http://www.cs.umd.edu/~gasarch/BLOGPAPERS/sq3.pdf">here</a>.)</p><p>The problem raises two more general questions</p><p>1) I had grad student Daniel Smolyak write a program that showed that if  1\le x,y \le 1000 then the only solutions were (1,1) and (11,16) and (16,11).  (See write up for why the program did not have to look like anything close to all possibly (x,y).)  </p><p>Is there some way to prove that if the only solutions for 1\le x,y\le N (some N) are the three given above, then there are no other solutions?</p><p><br/></p><p>2) Is the following problem solvable: Given p,q in Z[x,y] determine if the number of a,b such that both p(a,b) and q(a,b) are squares is finite or infinite.  AND if finite then determine how many, or a bound on how many.</p><p><br/></p><p>Can replace squares with other sets, but lets keep it simple for now. </p></div>
    </content>
    <updated>2020-09-10T13:33:00Z</updated>
    <published>2020-09-10T13:33:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-09-16T08:22:02Z</updated>
    </source>
  </entry>
</feed>
