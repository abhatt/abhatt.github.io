<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-10-20T03:22:13Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6623846086903606036</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6623846086903606036/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/nature-vs-nurture-close-to-my-birthday.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6623846086903606036" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6623846086903606036" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/nature-vs-nurture-close-to-my-birthday.html" rel="alternate" type="text/html"/>
    <title>Nature vs Nurture close to my birthday</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> Since I was born on Oct 1, 1960 (that's not true---if I posted  my real birthday I might get my  identity stolen), I will do a nature vs nurture post based on my life, which seems less likely to offend then doing it on someone else's life. I'll just rattle off some random points on Nature vs Nurture.</p><p>1) Is it plausible that I was born with some math talent? Is plausible that I was born with some talent to understand the polynomial van der Warden theorem? What is the granularity of nature-given or nurture-given abilities?</p><p>2) My dad was a HS English teacher and later Vice-Principal. My mom taught English at a Community college. Readers of the blog might think, given my spelling and grammar, that I was switched at birth. My mom says (jokingly?) that I was switched at birth since she thinks I am good at math.</p><p>a) I am not THAT good at math. Also see next point.</p><p>b) While there are some math families, there are not many. See my post <a href="https://blog.computationalcomplexity.org/2009/02/baseball-families-and-math-families.html">here</a>.</p><p>c) I think being raised in an intellectual atmosphere by two EDUCATORS who had the money to send me to college and allowed me the freedom to study what I wanted to  is far more important than the rather incidental matter of what field I studied.</p><p>d) Since my parents never went into math or the sciences it is very hard to tell  if they were `good at math' or even what that means.</p><p>3) There were early signs I was INTERESTED in math, though not that I was good at it.</p><p>a) In fourth grade I wanted to know how many SECONDS were in a century so I spend some time figuring it out on paper. Did I get the right answer?  I forgot about leap years.</p><p>b) I was either a beneficiary of, or a victim of, THE NEW MATH. So I learned about comm. and assoc. operations in 8th grade. We were asked to come up with our own operations. I wanted to come up with an operation that was comm. but not assoc. I did! Today I would write it as f(x,y) = |x-y|. This is the earliest I can think of where I made up a nice math problem. Might have been the last time I made up a nice math problem AND solved it without help. </p><p>c) In 10th grade I took some Martin Gardner books out of the library. The first theorem I learned not-in-school was that a graph is Eulerian iff every vertex has even degree. I read the chapter on Soma cubes and bought a set. (Soma cubes are explained <a href="https://en.wikipedia.org/wiki/Soma_cube">here</a>.) </p><p>d) I had a talent (nature?) at Soma Cubes.  I did every puzzle in the book in a week, diagrammed them, and even understood (on some level) the proofs that some could not be done. Oddly I am NOT good at 3-dim geom. Or even 2-dim geom.  For 1-dim I hold my own!</p><p>e) Throughout my childhood I noticed odd logic and odd math things that were said: </p><p>``Here at WCOZ (a radio station) we have an AXIOM, that's like a saying man, that weekends should be SEVEN DAYS LONG'' (Unless that axiom resolves CH, I don't think it should be assumed.) </p><p>``To PROVE we have the lowest prices in town we will give you a free camera!'' (how does that prove anything?) </p><p>``This margarine tastes JUST LIKE BUTTER'' (Okay-- so why not just buy butter?)</p><p>e) In 9th grade when I learned the quadratic formula I re-derived it once-a-month since I though it was important that one can prove such things.  I heard (not sure from where) that there was no 5th degree equation. At that very moment I told my parents:</p><p><i>I am going to major in math so I can find out why there is no 5th degree equation.</i></p><p>There are worse things for parents to hear from their children. See <a href="https://blog.computationalcomplexity.org/2019/06/a-proof-that-227-pi-0-and-more.html">here</a> for dad's reaction. </p><p>f) When I learned that the earth's orbit around the sun is an ellipse and that the earth was one of the foci, I wondered where the other foci is and if its important. I still wonder about this one. Google has not helped me here, though perhaps I have not phrased the question properly. If you know the answer please leave a comment. </p><p>g) I also thought about The Ketchup problem and other problems, that I won't go into since I already blogged about them  <a href="https://blog.computationalcomplexity.org/2012/06/ketchup-problem.html">here</a></p><p>4) I was on the math team in high school, but wasn't very good at it. I WAS good at making up math team problems. I am now on the committee that makes up the Univ of MD HS math competition. I am still not good at solving the problems but good at making them up. </p><p>5) From 9th grade on before I would study for an exam by making up what I thought would be a good exam and doing that. Often my exam was a better test of knowledge than the exam given. In college I helped people in Math and Physics by making up exams for them to work on as practice. </p><p>6) I was good at reading, understanding, and explaining papers. </p><p>7) I was never shy about asking for help. My curiosity exeeded by ego... by a lot!</p><p>8) Note that items 5,6, and 7 above do not mention SOLVING problems. The papers I have written are of three (overlapping) types:</p><p>a) I come up with the problem, make some inroads on it based on knowledge, and then have people cleverer (this is often) or with more knowledge (this is rarer) help me solve the problems.</p><p>b) I come up with the problem, and combine two things I know from other papers to solve it. </p><p>c) Someone else asks for my help on something and I have the knowledge needed. I can only recall one time where this lead to a paper. </p><p>NOTE- I do not think I have ever had a clever or new technique. CAVEAT: the diff between combining  known knowledge in new ways and having a clever or new technique is murky. </p><p>8) Over time these strengths and weaknesses have gotten more extreme. It has become a self-fulfilling prophecy where I spend A LOT of time making up problems without asking for help, but when I am trying to solve a problem I early on ask for help. Earlier than I should? Hard to know. </p><p>9) One aspect is `how good am I at math' But a diff angle is that I like to work on things that I KNOW are going to work out, so reading an article is better than trying to create new work. This could be a psychological thing. But is that nature or nurture?  </p><p>10) Could I be a better problem solver? Probably. Could I be a MUCH better problem solver? NO. Could I have been a better problem solver  I did more work on that angle when I was younger? Who knows? </p><p>11) Back to the Quintic: I had the following thought in ninth grade, though I could not possibly have expressed it: The question of, given a problem, how hard is it, upper and lower bounds, is a fundamental one that is worth a lifetime of study. As such my interest in complexity theory and recursion theory goes back to ninth grade or even further. My interest in Ramsey Theory for its own sake (and not in the service of complexity theory) is much more recent and does not quite fit into my narrative. But HEY- real life does not have as well defined narratives as fiction does. </p><p>12) Timing and Luck: IF I had been in grad student at a slight diff time I can imagine doing work on  algorithmic  Galois theory. <a href="https://singer.math.ncsu.edu/Algorithmic_slides.pdf">Here</a>  is a talk on Algorithmic  Galois theory. Note that one of the earliest results is by Landau and Miller from 1985---I had a course from Miller on Alg. Group Theory in 1982. This is NOT a wistful `What might have been' thought. Maybe I would have sucked at it, so its just as well I ended up doing recursion theory, then Ramsey theory, then recursion-theoretic Ramsey Theory, then muffins. </p><p><br/></p><p><br/></p><div><br/></div></div>
    </content>
    <updated>2020-10-19T15:55:00Z</updated>
    <published>2020-10-19T15:55:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-10-20T02:07:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7803</id>
    <link href="https://windowsontheory.org/2020/10/18/understanding-generalization-requires-rethinking-deep-learning/" rel="alternate" type="text/html"/>
    <title>Understanding generalization requires rethinking deep learning?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Yamini Bansal, Gal Kaplun, and Boaz Barak (See also paper on arxiv, code on gitlab, upcoming talk by Yamini&amp;Boaz, video of past talk) A central puzzle of deep learning is the question of generalization. In other words, what can we deduce from the training performance of a neural network about its test performance on fresh … <a class="more-link" href="https://windowsontheory.org/2020/10/18/understanding-generalization-requires-rethinking-deep-learning/">Continue reading <span class="screen-reader-text">Understanding generalization requires rethinking deep learning?</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em><a href="https://yaminibansal.com/about/">Yamini Bansal</a>, <a href="https://www.galkaplun.com/">Gal Kaplun</a>, and Boaz Barak</em></p>



<p>(See also <em><a href="https://arxiv.org/abs/2010.08508">paper on arxiv</a></em>,  <a href="https://gitlab.com/harvard-machine-learning/rationality-generalization">code on gitlab</a>,  <a href="https://cmsa.fas.harvard.edu/10-28-2020-new-technologies-in-mathematics-seminar/">upcoming talk by Yamini&amp;Boaz</a>,  <a href="https://youtu.be/89ixhju1hJ0">video of past talk</a>)</p>



<p>A central <a href="https://windowsontheory.org/2019/11/15/puzzles-of-modern-machine-learning/">puzzle</a> of deep learning is the question of <em>generalization</em>. In other words, what can we deduce from the <em>training performance</em> of a neural network about its <em>test performance</em> on <em>fresh unseen examples</em>. An <a href="https://arxiv.org/abs/1611.03530">influential paper</a> of Zhang, Bengio, Hardt, Recht, and Vinyals showed that the answer could be “nothing at all.” </p>



<p>Zhang et al. gave examples where modern deep neural networks achieve 100% accuracy on classifying their training data, but their performance on unseen data may be no better than chance. Therefore we cannot give meaningful guarantees for deep learning using traditional “generalization bounds” that bound the difference between test and train performance by some quantity that tends to zero as the number of datapoints <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="n"/> increases. This is why (to quote their title), Zhang et al. claimed that <strong>“understanding deep learning requires rethinking generalization”</strong>.</p>



<p>But what if the issue isn’t that we’ve been doing generalization bounds wrong, but rather that we’ve been doing deep learning (or more accurately, supervised deep learning) wrong?</p>



<h3>Self Supervised + Simple fit (SSS) learning</h3>



<p>To explain what we mean, let’s take a small detour to contrast “traditional” or “end-to-end” supervised learning with a different approach to supervised learning, which we’ll call here “Self-Supervised + Simple fit” or “SSS algorithms.” (While the name “SSS algorithms” is new, the approach itself has a <a href="http://people.idsia.ch/~juergen/FKI-126-90_%28revised%29bw_ocr.pdf">long history</a> and has recently been used with great success in practice; our work gives no new methods—only new analysis.)</p>



<p>The classical or “end-to-end” approach for supervised learning can be phrased as <em>“ask and you shall receive”</em>. Given labeled data, you ask (i.e., run an optimizer) for a complex classifier (e.g., a deep neural net) that fits the data (i.e., outputs the given labels on the given data points) and hope that it will be successful on future, unseen, data points as well. End-to-end supervised learning achieves state-of-art results for many classification problems, particularly for computer vision datasets ImageNet and CIFAR-10.</p>



<p>However, end-to-end learning does not directly correspond to the way humans learn to recognize objects (see also <a href="https://youtu.be/7I0Qt7GALVk?t=2475">this talk of LeCun</a>). A baby may see millions of images in the first year of her life, but most of them do not come with explicit labels. After seeing those images, a baby can make future classifications using very few labeled examples. For example, it might be enough to show her once what is a dog and what is a cat for her to correctly classify future dogs and cats, even if they look quite different from these examples.</p>



<figure class="wp-block-image"><img alt="End-to-end learning vs SSS algorithms." src="https://i.imgur.com/LXRaTQq.png"/><strong>Figure 1:</strong> Cartoon of end-to-end vs SSS learning </figure>



<p>In recent years, practitioners have proposed algorithms that are more similar to human learning than supervised learning. Such methods separate the process into two stages. In the <em>first stage</em>, we do <strong>representation learning</strong> whereby we use <em>unlabeled</em> data to learn a <em>representation</em>: a complex map (e.g., a deep neural net) mapping the inputs into some “representation space.” In the <em>second stage</em>, we fit a simple classifier (e.g., a linear threshold function) to the representation of the datapoints and the given labels. We call such algorithms <strong>“Self-Supervision + Simple fit”</strong> or <strong>SSS algorithms</strong>. (Note that, unlike other representation-learning based classifiers, the complex representation is “frozen” and not “fine-tuned” in the second stage, where only a simple classifier is used on top of it.)</p>



<p>While we don’t have a formal definition, a “good representation” should make downstream tasks easier, in the sense of allowing for fewer examples or simpler classifiers. We typically learn a representation via <strong>self supervision</strong> , whereby one finds a representation minimizing an objective function that intuitively requires some “insight” into the data. Approaches for self-supervision include reconstruction, where the objective involves recovering data points from partial information (e.g., recover missing <a href="https://arxiv.org/abs/1810.04805">words</a> or <a href="https://arxiv.org/abs/1604.07379">pixels</a>), and <em><a href="https://arxiv.org/abs/2002.05709">contrastive learning</a></em>, where the objective is to find a representation that make similar points close and dissimilar points far (e.g., in Euclidean space).</p>



<p>SSS algorithms have been traditionally used in natural language processing, where unlabeled data is plentiful, but labeled data for a particular task is often scarce. But recently SSS algorithms were also <a href="https://arxiv.org/abs/1902.06162">used with great success</a> even for vision tasks such as ImageNet and CIFAR10 where <em>all data is labeled!</em> While SSS algorithms do not yet beat the state-of-art supervised learning algorithms, they do get <a href="https://arxiv.org/abs/2006.10029">pretty close</a>. SSS algorithms also have other practical advantages over “end-to-end supervised learning”: they can make use of unlabeled data, the representation could be useful for non-classification tasks, and may have improved out of distribution performance. There has also been recent theoretical analysis of contrastive and reconstruction learning under certain statistical assumptions (see <a href="https://arxiv.org/abs/1902.09229">Arora et al</a> and  <a href="https://arxiv.org/abs/2008.01064">Lee et al</a>).</p>



<h3>The generalization gap of SSS algorithms</h3>



<p>In <a href="https://arxiv.org/abs/2010.08508">a recent paper</a>, we show that SSS algorithms not only work in practice, but work in theory too.</p>



<p>Specifically, we show that such algorithms have <strong>(1)</strong> small generalization gap and <strong>(2)</strong> we can <strong>prove</strong> (under reasonable assumptions) that their generalization gap tends to zero with the number of samples, with bounds that are meaningful for many modern classifiers on the CIFAR-10 and ImageNet datasets. We consider the setting where all data is labeled, and the <em>same dataset</em> is used for both learning the representation and fitting a simple classifier. The resulting classifier includes the overparameterized representation, and so we cannot simply apply “off the shelf” generalization bounds. Indeed, a priori it’s not at all clear that the generalization gap for SSS algorithms should be small.</p>



<p>To get some intuition for the generalization gap of SSS algorithms, consider the experiment where we inject some <em>label noise</em> into our distribution. That is, we corrupt an <img alt="\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta"/> fraction of the labels in both the train and test set, replacing them with random labels. Already in the noiseless case (<img alt="\eta=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%3D0&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta=0"/>), the generalization gap of SSS algorithms is noticeably smaller than that of end-to-end supervised learning. As we increase the noise, the difference becomes starker. End-to-end supervised learning algorithms can always achieve 100% training accuracy, even as the test accuracy deteriorates, since they can “memorize” all the training labels they are given. In contrast, for SSS algorithms, both training and testing accuracy decrease together as we increase the noise, with training accuracy correlating with test performance. </p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/10/oct-18-2020-12-44-16.gif"><img alt="" class="wp-image-7826" src="https://windowsontheory.files.wordpress.com/2020/10/oct-18-2020-12-44-16.gif?w=812"/></a><strong>Figure 2:</strong> Generalization gap of end-to-end and SSS algorithms on CIFAR 10 as a function of noise (since there are 10 classes, 90% noisy samples corresponds to the Zhang et al experiment). See also <a href="https://plotly.com/~yaminibansal/1.embed" rel="noreferrer noopener" target="_blank">interactive version</a>.</figure>



<p/>



<p>Our main theoretical result is a formal proof of the above statement. To do so, we consider training with a small amount of label noise (say <img alt="\eta=5\%" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%3D5%5C%25&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta=5\%"/>) and define the following quantities:</p>



<ul><li>The <strong>robustness gap</strong> is the amount by which training accuracy degrades between the “clean” (<img alt="\eta=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%3D0&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta=0"/>) experiment and the noisy one. (In this and all other quantities, the training accuracy is measured with respect to the original uncorrupted labels.)</li><li>The <strong>memorization gap</strong> considers the noisy experiment (<img alt="\eta=5\%" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%3D5%5C%25&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta=5\%"/>) and measures the amount by which performance on the corrupted data samples (where we received the wrong label) is worse than performance on the overall training set. If the algorithm can memorize all given labels, it will be perfectly wrong on the corrupted data samples, leading to a large memorization gap.</li><li>The <strong>rationality gap</strong> is the difference between the performance on the corrupted data samples and performance on unseen test examples. For example, if <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> is an image of a dog, then it measures the difference between the probability that <img alt="f(x)=\text{&quot;dog&quot;}" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29%3D%5Ctext%7B%22dog%22%7D&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x)=\text{&quot;dog&quot;}"/> when <img alt="(x,\text{&quot;cat&quot;})" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2C%5Ctext%7B%22cat%22%7D%29&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x,\text{&quot;cat&quot;})"/> is in the training set and the probability that <img alt="f(x)=\text{&quot;dog&quot;}" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29%3D%5Ctext%7B%22dog%22%7D&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x)=\text{&quot;dog&quot;}"/> when <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> is not in the training set at all. Since intuitively, getting the wrong label should be worse than getting no label at all, we typically expect the rationality gap to be around zero or negative. Formally we define the rationality gap to the maximum between <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="0"/> and the difference above, so it is always non-negative. We think of an algorithm with a significant positive rationality gap as “irrational.”</li></ul>



<p>By summing up the quantities above, we get the following inequality, which we call the <strong>RRM bound</strong></p>



<p><em><span class="has-inline-color" style="color: #0693e3;">generalization gap</span></em> <img alt="\leq" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleq&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="\leq"/> <em><span class="has-inline-color" style="color: #00d084;">robustness gap</span></em> <img alt="+" class="latex" src="https://s0.wp.com/latex.php?latex=%2B&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="+"/> <em><span class="has-inline-color" style="color: #fcb900;">rationality gap</span></em> <img alt="+" class="latex" src="https://s0.wp.com/latex.php?latex=%2B&amp;bg=eeeeee&amp;fg=666666&amp;s=0&amp;c=20201002" title="+"/> <em><span class="has-inline-color" style="color: #cf2e2e;">memorization gap</span></em></p>



<p>In practice, the <strong>robustness</strong> and <strong>rationality</strong> gaps are always small, both for end-to-end supervised algorithms (which have a large generalization gap), and for SSS algorithms (which have a small generalization gap). Thus the main contribution to the generalization gap comes from the <strong>memorization gap</strong>. Roughly speaking, our main result is the following:</p>



<p><em>If the complexity of the second-stage classifier of an SSS algorithm is smaller than the number of samples then the generalization gap is small.</em></p>



<p>See the <a href="https://arxiv.org/abs/2010.08508">paper</a> for the precise definition of “complexity,” but it is bounded by the number of bits that it takes to describe the simple classifier (no matter how complex is the representation used in the first stage). Our bound yields non-vacuous results in various practical settings; see the figures below or their <a href="https://share.streamlit.io/yaminibansal/streamlit-apps/main/figure1.py" rel="noreferrer noopener" target="_blank">interactive version</a>. </p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/10/intro-cifar.png"><img alt="" class="wp-image-7815" src="https://windowsontheory.files.wordpress.com/2020/10/intro-cifar.png?w=1024"/></a><strong>Figure 3:</strong> Empirical study of the generalization gap of a variety of of SSS algorithms on CIFAR-10. Each vertical line corresponds to one model, sorted by generalization gap. The RRM bound is typically near-tight, and our complexity upper bound is often non vacuous. Use <a href="https://share.streamlit.io/yaminibansal/streamlit-apps/main/figure1.py" rel="noreferrer noopener" target="_blank">this webpage</a> to interact with figures 3 and 4.</figure>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2020/10/imagenet_gaps.png"><img alt="" class="wp-image-7817" src="https://windowsontheory.files.wordpress.com/2020/10/imagenet_gaps.png?w=1024"/></a><strong>Figure 4:</strong> Empirical study of gaps for the ImageNet dataset. Because of limited computational resources, we only evaluated the theoretical bound for two models in this dataset.</figure>



<h3>What’s next</h3>



<p>There are still many open questions. Can we prove rigorous bounds on robustness and rationality? We have some preliminary results in the paper, but there is much room for improvement. Similarly, our complexity-based upper bound is far from tight at the moment, though the RRM bound itself is often surprisingly tight. Our work only applies to SSS algorithms, but people have the intuition that even end-to-end supervised learning algorithms implicitly learn a representation. So perhaps these tools can apply to such algorithms as well. As mentioned, we don’t yet have formal definitions for “good representations,” and the choice of the self-supervision task is still somewhat of a “black art” – can we find a more principled approach?</p></div>
    </content>
    <updated>2020-10-19T00:30:40Z</updated>
    <published>2020-10-19T00:30:40Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-10-20T03:21:17Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.08513</id>
    <link href="http://arxiv.org/abs/2010.08513" rel="alternate" type="text/html"/>
    <title>Learnable Graph-regularization for Matrix Decomposition</title>
    <feedworld_mtime>1603065600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Penglong Zhai, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Shihua.html">Shihua Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.08513">PDF</a><br/><b>Abstract: </b>Low-rank approximation models of data matrices have become important machine
learning and data mining tools in many fields including computer vision, text
mining, bioinformatics and many others. They allow for embedding
high-dimensional data into low-dimensional spaces, which mitigates the effects
of noise and uncovers latent relations. In order to make the learned
representations inherit the structures in the original data,
graph-regularization terms are often added to the loss function. However, the
prior graph construction often fails to reflect the true network connectivity
and the intrinsic relationships. In addition, many graph-regularized methods
fail to take the dual spaces into account. Probabilistic models are often used
to model the distribution of the representations, but most of previous methods
often assume that the hidden variables are independent and identically
distributed for simplicity. To this end, we propose a learnable
graph-regularization model for matrix decomposition (LGMD), which builds a
bridge between graph-regularized methods and probabilistic matrix decomposition
models. LGMD learns two graphical structures (i.e., two precision matrices) in
real-time in an iterative manner via sparse precision matrix estimation and is
more robust to noise and missing entries. Extensive numerical results and
comparison with competing methods demonstrate its effectiveness.
</p></div>
    </summary>
    <updated>2020-10-19T23:25:41Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.08512</id>
    <link href="http://arxiv.org/abs/2010.08512" rel="alternate" type="text/html"/>
    <title>An Approximation Algorithm for Optimal Subarchitecture Extraction</title>
    <feedworld_mtime>1603065600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wynter:Adrian_de.html">Adrian de Wynter</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.08512">PDF</a><br/><b>Abstract: </b>We consider the problem of finding the set of architectural parameters for a
chosen deep neural network which is optimal under three metrics: parameter
size, inference speed, and error rate. In this paper we state the problem
formally, and present an approximation algorithm that, for a large subset of
instances behaves like an FPTAS with an approximation error of $\rho \leq |{1-
\epsilon}|$, and that runs in $O(|{\Xi}| + |{W^*_T}|(1 +
|{\Theta}||{B}||{\Xi}|/({\epsilon\, s^{3/2})}))$ steps, where $\epsilon$ and
$s$ are input parameters; $|{B}|$ is the batch size; $|{W^*_T}|$ denotes the
cardinality of the largest weight set assignment; and $|{\Xi}|$ and
$|{\Theta}|$ are the cardinalities of the candidate architecture and
hyperparameter spaces, respectively.
</p></div>
    </summary>
    <updated>2020-10-19T23:22:17Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.08445</id>
    <link href="http://arxiv.org/abs/2010.08445" rel="alternate" type="text/html"/>
    <title>Barrington Plays Cards: The Complexity of Card-based Protocols</title>
    <feedworld_mtime>1603065600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Pavel Dvořák, Michal Koucký <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.08445">PDF</a><br/><b>Abstract: </b>In this paper we study the computational complexity of functions that have
efficient card-based protocols. Card-based protocols were proposed by den Boer
[EUROCRYPT '89] as a means for secure two-party computation. Our contribution
is two-fold: We classify a large class of protocols with respect to the
computational complexity of functions they compute, and we propose other
encodings of inputs which require fewer cards than the usual 2-card
representation.
</p></div>
    </summary>
    <updated>2020-10-19T23:20:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-10-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.08423</id>
    <link href="http://arxiv.org/abs/2010.08423" rel="alternate" type="text/html"/>
    <title>Restless reachability in temporal graphs</title>
    <feedworld_mtime>1603065600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thejaswi:Suhas.html">Suhas Thejaswi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gionis:Aristides.html">Aristides Gionis</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.08423">PDF</a><br/><b>Abstract: </b>We study a family of temporal reachability problems under waiting-time
restrictions. In particular, given a temporal graph and a set of source
vertices, we find the set of vertices that are reachable from a source via a
time-respecting path, and such that the difference in timestamps between
consecutive edges is at most a resting time. This kind of problems have several
interesting applications in understanding the spread of a disease in a network,
tracing contacts in epidemic outbreaks, and finding signaling pathways in the
brain network.
</p>
<p>We present an algebraic algorithm based on constrained multilinear sieving
for solving the restless reachability problems we propose. With an open-source
implementation we demonstrate that the algorithm can scale to large temporal
graphs with tens of millions of edges, despite the problem being NP-hard. The
implementation is efficiently engineered and highly optimized. For instance, we
can solve the restless reachability problem by restricting the path length to
$9$ in a real-world graph dataset with over 36 million directed edges in less
than one hour on a 4-core Haswell desktop.
</p></div>
    </summary>
    <updated>2020-10-19T23:22:23Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.08356</id>
    <link href="http://arxiv.org/abs/2010.08356" rel="alternate" type="text/html"/>
    <title>A note on stochastic subgradient descent for persistence-based functionals: convergence and practical aspects</title>
    <feedworld_mtime>1603065600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Carri=egrave=re:Mathieu.html">Mathieu Carrière</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chazal:Fr=eacute=d=eacute=ric.html">Frédéric Chazal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Glisse:Marc.html">Marc Glisse</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Ike:Yuichi.html">Yuichi Ike</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kannan:Hariprasad.html">Hariprasad Kannan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.08356">PDF</a><br/><b>Abstract: </b>Solving optimization tasks based on functions and losses with a topological
flavor is a very active and growing field of research in Topological Data
Analysis, with plenty of applications in non-convex optimization, statistics
and machine learning. All of these methods rely on the fact that most of the
topological constructions are actually stratifiable and differentiable almost
everywhere. However, the corresponding gradient and associated code is always
anchored to a specific application and/or topological construction, and do not
come with theoretical guarantees. In this article, we study the
differentiability of a general functional associated with the most common
topological construction, that is, the persistence map, and we prove a
convergence result of stochastic subgradient descent for such a functional.
This result encompasses all the constructions and applications for topological
optimization in the literature, and comes with code that is easy to handle and
mix with other non-topological constraints, and that can be used to reproduce
the experiments described in the literature.
</p></div>
    </summary>
    <updated>2020-10-19T23:30:14Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-10-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.08288</id>
    <link href="http://arxiv.org/abs/2010.08288" rel="alternate" type="text/html"/>
    <title>A symmetric attractor-decomposition lifting algorithm for parity games</title>
    <feedworld_mtime>1603065600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Marcin Jurdziński, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Morvan:R=eacute=mi.html">Rémi Morvan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Ohlmann:Pierre.html">Pierre Ohlmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thejaswini:K=_S=.html">K. S. Thejaswini</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.08288">PDF</a><br/><b>Abstract: </b>Progress-measure lifting algorithms for solving parity games have the best
worst-case asymptotic runtime, but are limited by their asymmetric nature, and
known from the work of Czerwi\'nski et al. (2018) to be subject to a matching
quasi-polynomial lower bound inherited from the combinatorics of universal
trees. Parys (2019) has developed an ingenious quasi-polynomial McNaughton-
Zielonka-style algorithm, and Lehtinen et al. (2019) have improved its
worst-case runtime. Jurdzi\'nski and Morvan (2020) have recently brought
forward a generic attractor-based algorithm, formalizing a second class of
quasi-polynomial solutions to solving parity games, which have runtime
quadratic in the size of universal trees. First, we adapt the framework of
iterative lifting algorithms to computing attractor-based strategies. Second,
we design a symmetric lifting algorithm in this setting, in which two lifting
iterations, one for each player, accelerate each other in a recursive fashion.
The symmetric algorithm performs at least as well as progress-measure liftings
in the worst-case, whilst bypassing their inherent asymmetric limitation.
Thirdly, we argue that the behaviour of the generic attractor-based algorithm
of Jurdzinski and Morvan (2020) can be reproduced by a specific deceleration of
our symmetric lifting algorithm, in which some of the information collected by
the algorithm is repeatedly discarded. This yields a novel interpretation of
McNaughton-Zielonka-style algorithms as progress-measure lifting iterations
(with deliberate set-backs), further strengthening the ties between all known
quasi-polynomial algorithms to date.
</p></div>
    </summary>
    <updated>2020-10-19T23:29:55Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.08276</id>
    <link href="http://arxiv.org/abs/2010.08276" rel="alternate" type="text/html"/>
    <title>Training Data Generating Networks: Linking 3D Shapes and Few-Shot Classification</title>
    <feedworld_mtime>1603065600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Biao.html">Biao Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wonka:Peter.html">Peter Wonka</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.08276">PDF</a><br/><b>Abstract: </b>We propose a novel 3d shape representation for 3d shape reconstruction from a
single image. Rather than predicting a shape directly, we train a network to
generate a training set which will be feed into another learning algorithm to
define the shape. Training data generating networks establish a link between
few-shot learning and 3d shape analysis. We propose a novel meta-learning
framework to jointly train the data generating network and other components. We
improve upon recent work on standard benchmarks for 3d shape reconstruction,
but our novel shape representation has many applications.
</p></div>
    </summary>
    <updated>2020-10-19T23:31:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-10-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.08142</id>
    <link href="http://arxiv.org/abs/2010.08142" rel="alternate" type="text/html"/>
    <title>Improved Approximation Algorithms for Stochastic-Matching Problems</title>
    <feedworld_mtime>1603065600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Adamczyk:Marek.html">Marek Adamczyk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brubach:Brian.html">Brian Brubach</a>, Fabrizio Grandoni, Karthik A. Sankararaman, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Srinivasan:Aravind.html">Aravind Srinivasan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Pan.html">Pan Xu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.08142">PDF</a><br/><b>Abstract: </b>We consider the Stochastic Matching problem, which is motivated by
applications in kidney exchange and online dating. In this problem, we are
given an undirected graph. Each edge is assigned a known, independent
probability of existence and a positive weight (or profit). We must probe an
edge to discover whether or not it exists. Each node is assigned a positive
integer called a timeout (or a patience). On this random graph we are executing
a process, which probes the edges one-by-one and gradually constructs a
matching. The process is constrained in two ways. First, if a probed edge
exists, it must be added irrevocably to the matching (the query-commit model).
Second, the timeout of a node $v$ upper-bounds the number of edges incident to
$v$ that can be probed. The goal is to maximize the expected weight of the
constructed matching.
</p>
<p>For this problem, Bansal et al. (Algorithmica 2012) provided a
$0.33$-approximation algorithm for bipartite graphs and a $0.25$-approximation
for general graphs. We improve the approximation factors to $0.39$ and $0.269$,
respectively.
</p>
<p>The main technical ingredient in our result is a novel way of probing edges
according to a not-uniformly-random permutation. Patching this method with an
algorithm that works best for large-probability edges (plus additional ideas)
leads to our improved approximation factors.
</p></div>
    </summary>
    <updated>2020-10-19T23:27:17Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.08083</id>
    <link href="http://arxiv.org/abs/2010.08083" rel="alternate" type="text/html"/>
    <title>Near-Linear Time Homomorphism Counting in Bounded Degeneracy Graphs: The Barrier of Long Induced Cycles</title>
    <feedworld_mtime>1603065600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bera:Suman_K=.html">Suman K. Bera</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pashanasangi:Noujan.html">Noujan Pashanasangi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Seshadhri:C=.html">C. Seshadhri</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.08083">PDF</a><br/><b>Abstract: </b>Counting homomorphisms of a constant sized pattern graph $H$ in an input
graph $G$ is a fundamental computational problem. There is a rich history of
studying the complexity of this problem, under various constraints on the input
$G$ and the pattern $H$. Given the significance of this problem and the large
sizes of modern inputs, we investigate when near-linear time algorithms are
possible. We focus on the case when the input graph has bounded degeneracy, a
commonly studied and practically relevant class for homomorphism counting. It
is known from previous work that for certain classes of $H$, $H$-homomorphisms
can be counted exactly in near-linear time in bounded degeneracy graphs. Can we
precisely characterize the patterns $H$ for which near-linear time algorithms
are possible?
</p>
<p>We completely resolve this problem, discovering a clean dichotomy using
fine-grained complexity. Let $m$ denote the number of edges in $G$. We prove
the following: if the largest induced cycle in $H$ has length at most $5$, then
there is an $O(m\log m)$ algorithm for counting $H$-homomorphisms in bounded
degeneracy graphs. If the largest induced cycle in $H$ has length at least $6$,
then (assuming standard fine-grained complexity conjectures) there is a
constant $\gamma &gt; 0$, such that there is no $o(m^{1+\gamma})$ time algorithm
for counting $H$-homomorphisms.
</p></div>
    </summary>
    <updated>2020-10-19T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.08042</id>
    <link href="http://arxiv.org/abs/2010.08042" rel="alternate" type="text/html"/>
    <title>Ranked enumeration of MSO logic on words</title>
    <feedworld_mtime>1603065600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bourhis:Pierre.html">Pierre Bourhis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grez:Alejandro.html">Alejandro Grez</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jachiet:Louis.html">Louis Jachiet</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Riveros:Cristian.html">Cristian Riveros</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.08042">PDF</a><br/><b>Abstract: </b>In the last years, enumeration algorithms with bounded delay have attracted a
lot of attention for several data management tasks. Given a query and the data,
the task is to preprocess the data and then enumerate all the answers to the
query one by one and without repetitions. This enumeration scheme is typically
useful when the solutions are treated on the fly or when we want to stop the
enumeration once the pertinent solutions have been found. However, with the
current schemes, there is no restriction on the order how the solutions are
given and this order usually depends on the techniques used and not on the
relevance for the user.
</p>
<p>In this paper we study the enumeration of monadic second order logic (MSO)
over words when the solutions are ranked. We present a framework based on MSO
cost functions that allows to express MSO formulae on words with a cost
associated with each solution. We then demonstrate the generality of our
framework which subsumes, for instance, document spanners and regular complex
event processing queries and adds ranking to them. The main technical result of
the paper is an algorithm for enumerating all the solutions of formulae in
increasing order of cost efficiently, namely, with a linear preprocessing phase
and logarithmic delay between solutions. The novelty of this algorithm is based
on using functional data structures, in particular, by extending functional
Brodal queues to suit with the ranked enumeration of MSO on words.
</p></div>
    </summary>
    <updated>2020-10-19T23:25:03Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.07990</id>
    <link href="http://arxiv.org/abs/2010.07990" rel="alternate" type="text/html"/>
    <title>An Algorithm for Learning Smaller Representations of Models With Scarce Data</title>
    <feedworld_mtime>1603065600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wynter:Adrian_de.html">Adrian de Wynter</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.07990">PDF</a><br/><b>Abstract: </b>We present a greedy algorithm for solving binary classification problems in
situations where the dataset is either too small or not fully representative of
the problem being solved, and obtaining more data is not possible. This
algorithm is of particular interest when training small models that have
trouble generalizing. It relies on a trained model with loose accuracy
constraints, an iterative hyperparameter pruning procedure, and a function used
to generate new data. Analysis on correctness and runtime complexity under
ideal conditions and an extension to deep neural networks is provided. In the
former case we obtain an asymptotic bound of
$O\left(|\Theta^2|\left(\log{|\Theta|} + |\theta^2| + T_f\left(|
D|\right)\right) + \bar{S}|\Theta||{E}|\right)$, where $|{\Theta}|$ is the
cardinality of the set of hyperparameters $\theta$ to be searched; $|{E}|$ and
$|{D}|$ are the sizes of the evaluation and training datasets, respectively;
$\bar{S}$ and $\bar{f}$ are the inference times for the trained model and the
candidate model; and $T_f({|{D}|})$ is a polynomial on $|{D}|$ and $\bar{f}$.
Under these conditions, this algorithm returns a solution that is $1 \leq r
\leq 2(1 - {2^{-|{\Theta}|}})$ times better than simply enumerating and
training with any $\theta \in \Theta$. As part of our analysis of the
generating function we also prove that, under certain assumptions, if an open
cover of $D$ has the same homology as the manifold where the support of the
underlying probability distribution lies, then $D$ is learnable, and viceversa.
</p></div>
    </summary>
    <updated>2020-10-19T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.07960</id>
    <link href="http://arxiv.org/abs/2010.07960" rel="alternate" type="text/html"/>
    <title>Efficient constructions of the Prefer-same and Prefer-opposite de Bruijn sequences</title>
    <feedworld_mtime>1603065600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Evan Sala, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sawada:Joe.html">Joe Sawada</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alhakim:Abbas.html">Abbas Alhakim</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.07960">PDF</a><br/><b>Abstract: </b>The greedy Prefer-same de Bruijn sequence construction was first presented by
Eldert et al.[AIEE Transactions 77 (1958)]. As a greedy algorithm, it has one
major downside: it requires an exponential amount of space to store the length
$2^n$ de Bruijn sequence. Though de Bruijn sequences have been heavily studied
over the last 60 years, finding an efficient construction for the Prefer-same
de Bruijn sequence has remained a tantalizing open problem. In this paper, we
unveil the underlying structure of the Prefer-same de Bruijn sequence and solve
the open problem by presenting an efficient algorithm to construct it using
$O(n)$ time per bit and only $O(n)$ space. Following a similar approach, we
also present an efficient algorithm to construct the Prefer-opposite de Bruijn
sequence.
</p></div>
    </summary>
    <updated>2020-10-19T23:24:40Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2010.07346</id>
    <link href="http://arxiv.org/abs/2010.07346" rel="alternate" type="text/html"/>
    <title>Online Learning with Vector Costs and Bandits with Knapsacks</title>
    <feedworld_mtime>1603065600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kesselheim:Thomas.html">Thomas Kesselheim</a>, Sahil Singla <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2010.07346">PDF</a><br/><b>Abstract: </b>We introduce online learning with vector costs (\OLVCp) where in each time
step $t \in \{1,\ldots, T\}$, we need to play an action $i \in \{1,\ldots,n\}$
that incurs an unknown vector cost in $[0,1]^{d}$. The goal of the online
algorithm is to minimize the $\ell_p$ norm of the sum of its cost vectors. This
captures the classical online learning setting for $d=1$, and is interesting
for general $d$ because of applications like online scheduling where we want to
balance the load between different machines (dimensions).
</p>
<p>We study \OLVCp in both stochastic and adversarial arrival settings, and give
a general procedure to reduce the problem from $d$ dimensions to a single
dimension. This allows us to use classical online learning algorithms in both
full and bandit feedback models to obtain (near) optimal results. In
particular, we obtain a single algorithm (up to the choice of learning rate)
that gives sublinear regret for stochastic arrivals and a tight $O(\min\{p,
\log d\})$ competitive ratio for adversarial arrivals.
</p>
<p>The \OLVCp problem also occurs as a natural subproblem when trying to solve
the popular Bandits with Knapsacks (\BwK) problem. This connection allows us to
use our \OLVCp techniques to obtain (near) optimal results for \BwK in both
stochastic and adversarial settings. In particular, we obtain a tight $O(\log d
\cdot \log T)$ competitive ratio algorithm for adversarial \BwK, which improves
over the $O(d \cdot \log T)$ competitive ratio algorithm of Immorlica et al.
[FOCS'19].
</p></div>
    </summary>
    <updated>2020-10-19T23:29:38Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-10-19T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint</id>
    <link href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html" rel="alternate" type="text/html"/>
    <title>Polyhedra without disjoint faces</title>
    <summary>Some research I’ve been doing led me to consider the (prism,\(K_{3,3}\))-minor-free graphs. It’s not always easy to go from forbidden minors to the graphs that forbid them, or vice versa, but in this case I think there’s a nice characterization, which I’m posting here because it doesn’t fit into the research writeup: these are the graphs whose nontrivial triconnected components are \(K_5\), wheel graphs, or the graph \(K_5-e\) of the triangular bipyramid. The illustration below shows an example of a graph with this structure, with its nontrivial triconnected components colored red and yellow. There’s a simpler and more geometric way to say almost the same thing: the only convex polyhedra that do not have two vertex-disjoint faces are the pyramids and the triangular bipyramid.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Some research I’ve been doing led me to consider the (prism,\(K_{3,3}\))-minor-free graphs. It’s not always easy to go from <a href="https://en.wikipedia.org/wiki/Forbidden_graph_characterization">forbidden minors</a> to the graphs that forbid them, or vice versa, but in this case I think there’s a nice characterization, which I’m posting here because it doesn’t fit into the research writeup: these are the graphs whose nontrivial triconnected components are \(K_5\), <a href="https://en.wikipedia.org/wiki/Wheel_graph">wheel graphs</a>, or the graph \(K_5-e\) of the <a href="https://en.wikipedia.org/wiki/Triangular_bipyramid">triangular bipyramid</a>. The illustration below shows an example of a graph with this structure, with its nontrivial triconnected components colored red and yellow. There’s a simpler and more geometric way to say almost the same thing: the only convex polyhedra that do not have two vertex-disjoint faces are the pyramids and the triangular bipyramid.</p>

<p style="text-align: center;"><img alt="A (prism, K_{3,3})-minor-free graph, with its nontrivial triconnected components colored red and yellow" src="https://11011110.github.io/blog/assets/2020/prism-k33-free.svg"/></p>

<p>Some definitions:</p>

<ul>
  <li>
    <p>Here by the prism graph I mean the graph of the triangular prism. Any other prism has this one as a minor, and so is irrelevant as a forbidden minor. However, the pyramids in this structure can have any polygon as their base, corresponding to wheel graphs with arbitrarily many vertices.</p>
  </li>
  <li>
    <p>\(K_{3,3}\) is a complete bipartite graph with three vertices on each side of its bipartition, famous as the <a href="https://en.wikipedia.org/wiki/Three_utilities_problem">utility graph</a>, one of the two forbidden minors for planar graphs. The triangular prism graph and \(K_{3,3}\) are the only two <a href="https://en.wikipedia.org/wiki/Cubic_graph">3-regular graphs</a> with six vertices.</p>
  </li>
</ul>

<p style="text-align: center;"><img alt="The prism graph and K_{3,3}" src="https://11011110.github.io/blog/assets/2020/prism-k33.svg"/></p>

<ul>
  <li>
    <p>The triconnected components of a graph are the graphs associated with the nodes of its <a href="https://en.wikipedia.org/wiki/SPQR_tree">SPQR tree</a>, or of the SPQR trees of its biconnected components. These are cycle graphs, dipole multigraphs, or 3-connected graphs, and by “nontrivial” I mean the ones that are not cycles or dipoles. A triconnected component might not be a subgraph of the given graph, because it can have additional edges that correspond to paths in the given graph. For instance, subdividing the edges of any graph into paths, or more generally replacing edges by arbitrary series-parallel graphs, does not change its set of nontrivial triconnected components.</p>
  </li>
  <li>
    <p>I’m using “face” in the usual three-dimensional meaning, a two-dimensional subset of the boundary of the polyhedron. For higher-dimensional polytopes, “face” has a different meaning that also includes vertices and edges, and “facet” would be used to refer to the \((d-1)\)-dimensional faces, but using that terminology seems overly pedantic here.</p>
  </li>
</ul>

<p>Sketch of proof of the characterization of polyhedra without two disjoint faces: Consider any polyhedron without disjoint faces. If one face shares an edge with all the others, it’s a <a href="https://en.wikipedia.org/wiki/Halin_graph">Halin graph</a>, a graph formed by linking the leaves of a tree into a cycle; if the tree is a star, it’s a pyramid, and otherwise contracting all but one of the interior edges of the tree, and then all but four of the cycle edges, will produce a prism minor. In the remaining case, some two faces share only a vertex \(v\), which must have degree four or more. Each face that is disjoint from \(v\) must touch all that faces incident to \(v\), which can only happen when there is one face disjoint from \(v\) (a pyramid) or two faces disjoint from \(v\), neither of which has an edge disjoint from the other one (a bipyramid).</p>

<p>Sketch of a lemma that every convex polyhedron with two disjoint faces has a prism minor: glue a pyramidal cap into each of the two faces, producing a larger convex polyhedron which by either <a href="https://en.wikipedia.org/wiki/Steinitz%27s_theorem">Steinitz’s theorem</a> or <a href="https://en.wikipedia.org/wiki/Balinski%27s_theorem">Balinski’s theorem</a> is necessarily 3-connected, and find three vertex-disjoint paths between the apexes of the attached pyramids. The parts of these paths outside the two glued pyramids, together with the boundaries of the two faces, form a subdivision of a prism.</p>

<p>Sketch of proof of the characterization of (prism,\(K_{3,3}\))-minor-free graphs: The nontrivial triconnected components are exactly the maximal triconnected minors of the given graph, so if either of the two triconnected forbidden minors is to be found in the given graph, it will be found in one of the triconnected components. \(K_5\) and the triangular bipyramid are too small to have one of the forbidden minors. The only 3-connected minors of the pyramid graphs are smaller pyramids, obtained by contracting one of the cycle edges of the pyramid, so these also do not have a forbidden minor. Therefore the graphs of the stated form are all (prism,\(K_{3,3}\))-minor-free.</p>

<p>In the other direction, suppose that a graph is (prism,\(K_{3,3}\))-minor-free.
Each triconnected component is a minor, so it must also be (prism,\(K_{3,3}\))-minor-free. What can these components look like? Forbidding \(K_{3,3}\) as a minor rules out nonplanar components other than \(K_5\), by a theorem of Wagner<sup id="fnref:wagner"><a class="footnote" href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html#fn:wagner">1</a></sup> and Hall.<sup id="fnref:hall"><a class="footnote" href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html#fn:hall">2</a></sup> So the remaining components that we need to consider are triconnected planar graphs with no prism minor. These cannot have two disjoint faces by the lemma, and so they can only be pyramids or the triangular bipyramid.</p>

<div class="footnotes">
  <ol>
    <li id="fn:wagner">
      <p>K. Wagner. Über eine Erweiterung des Satzes von Kuratowski. <em>Deutsche Mathematik</em>, 2:280–285, 1937. <a class="reversefootnote" href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html#fnref:wagner">↩</a></p>
    </li>
    <li id="fn:hall">
      <p>D. W. Hall. A note on primitive skew curves. <em>Bulletin of the American Mathematical Society</em>, 49(12):935–936, 1943. <a href="https://doi.org/10.1090/ S0002-9904-1943-08065-2">doi:10.1090/ S0002-9904-1943-08065-2</a>. <a class="reversefootnote" href="https://11011110.github.io/blog/2020/10/18/polyhedra-without-disjoint.html#fnref:hall">↩</a></p>
    </li>
  </ol>
</div>

<p>(<a href="https://mathstodon.xyz/@11011110/105058649830809584">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-10-18T17:06:00Z</updated>
    <published>2020-10-18T17:06:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-10-19T00:37:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/155</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/155" rel="alternate" type="text/html"/>
    <title>TR20-155 |  Log-rank and lifting for AND-functions | 

	Sam McGuire, 

	Shachar Lovett, 

	Alexander Knop, 

	Weiqiang Yuan</title>
    <summary>Let $f: \{0,1\}^n \to \{0, 1\}$ be a boolean function, and let $f_\land (x, y) = f(x \land y)$ denote the AND-function of $f$, where $x \land y$ denotes bit-wise AND. We study the deterministic communication complexity of $f_\land$ and show that, up to a $\log n$ factor, it is bounded by a polynomial in the logarithm of the real rank of the communication matrix of $f_\land$. This comes within a $\log n$ factor of establishing the log-rank conjecture for AND-functions with no assumptions on $f$. Our result stands in contrast with previous results on special cases of the log-rank 
conjecture, which needed significant restrictions on $f$ such as monotonicity or low $\mathbb{F}_2$-degree. Our techniques can also be used to prove (within a $\log n$ factor) a lifting theorem for AND-functions, stating that the deterministic communication complexity of $f_\land$ is polynomially-related to the AND-decision tree complexity of $f$.

The results rely on a new structural result regarding boolean functions $f:\{0, 1\}^n \to \{0, 1\}$ with a sparse polynomial representation, which may be of independent interest. We show that if the polynomial computing $f$ has few monomials then the set system of the monomials has a small hitting set, of size poly-logarithmic in its sparsity. We also establish extensions of this result to multi-linear polynomials $f:\{0,1\}^n \to \mathbb{R}$ with a larger range.</summary>
    <updated>2020-10-18T14:44:03Z</updated>
    <published>2020-10-18T14:44:03Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-10-20T03:20:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=493</id>
    <link href="https://tcsplus.wordpress.com/2020/10/16/tcs-talk-wednesday-october-21-aayush-jain-ucla/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, October 21 — Aayush Jain, UCLA</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, October 21th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Aayush Jain from UCLA will speak about “Indistinguishability Obfuscation from Well-Founded Assumptions” (abstract below). You can reserve a spot as an individual or a group to join us […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, October 21th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Aayush Jain</strong> from UCLA will speak about “<em>Indistinguishability Obfuscation from Well-Founded Assumptions</em>” (abstract below). </p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our  website</a> on the day of the talk, so people who did not sign up will still be able to  watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<p class="wp-block-quote">Abstract: We present a construction of an indistinguishability obfuscation scheme, whose security rests on the subexponential hardness of four well-founded assumptions. We show the existence of an indistinguishability Obfuscation scheme for all circuits assuming sub-exponential security of the following assumptions:</p>



<ul class="wp-block-quote"><li>The Learning with Errors (LWE) assumption with arbitrarily small subexponential modulus-to-noise ratio,</li><li>The SXDH assumption with respect to bilinear groups of prime order <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="p"/>,</li><li>Existence of a Boolean Pseudorandom Generator (PRG) in <img alt="\mathsf{NC}^0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7BNC%7D%5E0&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\mathsf{NC}^0"/> with arbitrary polynomial stretch, that is, mapping <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="n"/> bits to <img alt="n^{1+\tau}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7B1%2B%5Ctau%7D&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="n^{1+\tau}"/> bits, for any constant \tau&gt;0.</li><li>The Learning Parity with Noise (LPN) assumption over <img alt="\mathbb{Z}_p" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BZ%7D_p&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\mathbb{Z}_p"/> with error-rate <img alt="\ell^{-\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell%5E%7B-%5Cdelta%7D&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\ell^{-\delta}"/>, where <img alt="\ell" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\ell"/> is the dimension of the secret and <img alt="\delta&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%3E0&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\delta&gt;0"/> is an arbitrarily small constant.<br/>Further, assuming only polynomial security of these assumptions, there exists a compact public-key functional encryption scheme for all circuits.</li></ul>



<p class="wp-block-quote">The main technical novelty is the introduction and construction of a cryptographic pseudorandom generator that we call a Structured-Seed PRG (sPRG), assuming LPN over <img alt="\mathbb{Z}_p" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BZ%7D_p&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\mathbb{Z}_p"/> and PRGs in <img alt="\mathsf{NC}^0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathsf%7BNC%7D%5E0&amp;bg=fff&amp;fg=444444&amp;s=0&amp;c=20201002" title="\mathsf{NC}^0"/>. During the talk, I will discuss how structured seed PRGs have evolved from different notions of novel pseudorandom generators proposed in the past few years, and how an interplay between different areas of theoretical computer science played a major role in providing valuable insights leading to this work. Time permitting, I will go into the details of how to construct sPRGs. <br/><br/>Joint work with Huijia (Rachel) Lin and Amit Sahai</p></div>
    </content>
    <updated>2020-10-16T06:33:00Z</updated>
    <published>2020-10-16T06:33:00Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-10-20T03:21:28Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/10/15/linkage</id>
    <link href="https://11011110.github.io/blog/2020/10/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Mirzakhani and meanders (\(\mathbb{M}\)). On some more-than-coincidental similarities in formulas found by Mirzakhani for numbers of geodesics on hyperbolic surfaces and by Vincent Delecroix, Elise Goujard, Peter Zograf, and Anton Zorich in a new preprint for numbers of meanders, closed curves with a given number of intersections with a line.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://scilogs.spektrum.de/hlf/mirzakhani-and-meanders/">Mirzakhani and meanders</a> (<a href="https://mathstodon.xyz/@11011110/104963847400612388">\(\mathbb{M}\)</a>). On some more-than-coincidental similarities in formulas found by Mirzakhani for numbers of geodesics on hyperbolic surfaces and by Vincent Delecroix, Elise Goujard, Peter Zograf, and Anton Zorich <a href="https://arxiv.org/abs/1705.05190">in a new preprint</a> for numbers of <a href="https://en.wikipedia.org/wiki/Meander_(mathematics)">meanders</a>, closed curves with a given number of intersections with a line.</p>
  </li>
  <li>
    <p><a href="https://mycqstate.wordpress.com/2020/09/29/it-happens-to-everyonebut-its-not-fun/">Retraction of “a proof of soundness of the Raz-Safra low-degree test against entangled-player strategies, a key ingredient in the proof of the quantum low-degree test, itself a key ingredient in the \(\mathsf{MIP}^*=\mathsf{RE}\) paper”</a> (<a href="https://mathstodon.xyz/@11011110/104969573344196233">\(\mathbb{M}\)</a>). \(\mathsf{MIP}^*=\mathsf{RE}\) is patched and remains believed true but not fully refereed. This post provides a lot more than the standard we-found-a-bug notice: a good description of what happened, what it implies technically, and how it affects the authors and community.</p>
  </li>
  <li>
    <p><a href="https://blogs.lse.ac.uk/impactofsocialsciences/2020/09/30/for-academic-publishing-to-be-trans-inclusive-authors-must-be-allowed-to-retroactively-change-their-names/">For academic publishing to be trans-inclusive, authors must be allowed to retroactively change their names</a> (<a href="https://mathstodon.xyz/@11011110/104972193066839079">\(\mathbb{M}\)</a>, <a href="https://retractionwatch.com/2020/10/03/weekend-reads-unicorn-poo-and-other-fraudulent-covid-19-treatments-disgraced-researchers-and-drug-company-payouts-a-fictional-account-of-real-fraud/">via</a>). I agree — more than once in researching Wikipedia bios I found past publications under deadnames. If the authors prefer this to be better hidden, while continuing to be credited for their past work, we should try to honor that preference.</p>
  </li>
  <li>
    <p>It’s easy to point and laugh at the <a href="https://tex.stackexchange.com/questions/565387/mathbb-r-is-not-showing-in-reference-bibtex">researcher who thought bibtex from Google scholar was usable</a> (<a href="https://mathstodon.xyz/@11011110/104980666583964923">\(\mathbb{M}\)</a>), but their question brings up a more serious question: why is Google’s bibtex so bad? Even the junk I get from <code class="language-plaintext highlighter-rouge">curl -LH "Accept: application/x-bibtex" http://doi.org/...</code> is mostly usable in comparison. I’m tempted to suggest that they go to MathSciNet for the good stuff but I’m worried they won’t have access.</p>
  </li>
  <li>
    <p><a href="https://boingboing.net/2020/09/30/ten-kinetic-sculptures-by-anne-lilly.html">Ten kinetic sculptures by Anne Lilly</a> (<a href="https://mathstodon.xyz/@11011110/104988795486796768">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://jix.one/the-assembly-language-of-satisfiability/">The assembly language of satisfiability</a> (<a href="https://mathstodon.xyz/@jix/104971574457861322">\(\mathbb{M}\)</a>). Why Boolean satisfiability is too low-level to work well as a way to express the kind of problems satisfiability-solvers can solve, and how <a href="https://en.wikipedia.org/wiki/Satisfiability_modulo_theories">satisfiability modulo theories</a> can help.</p>
  </li>
  <li>
    <p><a href="https://cp4space.hatsya.com/2020/10/01/subsumptions-of-regular-polytopes/">Which regular polytopes have their vertices a subset of other regular polytopes in the same dimension</a> (<a href="https://mathstodon.xyz/@11011110/104998010300898992">\(\mathbb{M}\)</a>)? We don’t know! The answer is closely connected to the existence of <a href="https://en.wikipedia.org/wiki/Hadamard_matrix">Hadamard matrices</a>, which are famously conjectured to exist in dimensions divisible by four. A solution to the Hadamard matrix existence problem would also solve the polytope problem.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/computer-scientists-break-traveling-salesperson-record-20201008/">Computer scientists break traveling salesperson record</a> (<a href="https://mathstodon.xyz/@11011110/105006269895209659">\(\mathbb{M}\)</a>). I <a href="https://11011110.github.io/blog/2020/07/15/linkage.html">linked to this back in July</a> when <a href="https://arxiv.org/abs/2007.01409">Karlin, Klein, and Gharan’s preprint</a> giving a \((1/2-\varepsilon)\)-approximation to TSP first came out, but now it’s getting wider publicity in <em>Quanta</em>. See also <a href="https://www.sciencenews.org/article/shayan-oveis-gharan-theoretical-computer-scientist-sn-10-scientists-watch">an earlier (paywalled) piece on the same story in <em>ScienceNews</em></a>.</p>
  </li>
  <li>
    <p>Symmetry, quasisymmetry, and kite-rhomb tessellations in the mathematical modeling of virus surface structures: <a href="https://ima.org.uk/721/fighting-infections-with-symmetry/">IMA</a>,
<a href="https://inference-review.com/article/mathematical-virology"><em>Inference</em></a>,
<a href="https://archive.bridgesmathart.org/2018/bridges2018-237.pdf">Bridges</a> (<a href="https://mathstodon.xyz/@11011110/105009372623320055">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://shop.deutschepost.de/freies-quadrat-briefmarke-zu-1-70-eur-10er-bogen">New German postage stamp features the missing square puzzle</a> (<a href="https://muensterland.social/@rgx/105007333917605810">\(\mathbb{M}\)</a>, <a href="https://en.wikipedia.org/wiki/Missing_square_puzzle">see also</a>).</p>
  </li>
  <li>
    <p><a href="https://www.newstatesman.com/international/science-tech/2020/07/ra-fisher-and-science-hatred">R. A. Fisher and the science of hatred</a> (<a href="https://mathstodon.xyz/@11011110/105020588148970072">\(\mathbb{M}\)</a>). If you’ve been wondering why noted academics of yesteryear like <a href="https://en.wikipedia.org/wiki/Ronald_Fisher">R. A. Fisher</a> (a major figure in statistics) and <a href="https://en.wikipedia.org/wiki/David_Starr_Jordan">David Starr Jordan</a> (founding president of Stanford University) have been having their names taken off things lately, the link looks like a good explainer of their views on eugenics, and why those views are now regarded as deeply racist, even for their times.</p>
  </li>
  <li>
    <p><a href="http://hardmath123.github.io/minimal-surface.html">Sol LeWitt and the soapy pit</a> (<a href="https://mathstodon.xyz/@11011110/105023612862469185">\(\mathbb{M}\)</a>, <a href="https://abhikjain360.github.io/2020/08/01/The-186th-Carnival-of-Mathematics.html">via</a>, <a href="https://aperiodical.com/2020/10/carnival-of-mathematics-186/">via2</a>). LeWitt was an artist who in 1974 made a piece exhibiting all of the possible subsets of edges of the cube. The comfortably numbered blog examines what you get if you use these as frames for making soap films.</p>
  </li>
  <li>
    <p><a href="http://landezine.com/index.php/2013/02/funenpark-by-landlab/">Funenpark</a> (<a href="https://mathstodon.xyz/@11011110/105029637909838642">\(\mathbb{M}\)</a>). To be clear, Funenpark is not a fun-park. It is a high-density residential development on former industrial land near Amsterdam. What interests me is their <a href="https://www.flickr.com/photos/shiratski/2242870712/">pentagonal tiles</a>. It’s not one of the <a href="https://en.wikipedia.org/wiki/Pentagonal_tiling">15 monohedral pentagon tilings</a>: the tiles have two shapes, one forming half of a regular hexagon (all angles \(&gt; 60^\circ\)) and another surrounding the hexagons (sharp angle \(= 60^\circ\)). Still, a nice pattern.</p>
  </li>
  <li>
    <p>Sometimes when I’ve been doing big literature searches on jstor (manually clicking on dozens of links because jstor’s search results don’t tell me which book is being reviewed, delayed by maybe a second or so per click so that I don’t get stopped by jstor’s anti-bot filters) I then get locked out of Google Scholar for a day or so on the same IP address because Google thinks I’m a bot. It doesn’t happen when I search Scholar directly. Has anyone else noticed this? Any idea how to avoid it? (<a href="https://mathstodon.xyz/@11011110/105037500352288970">\(\mathbb{M}\)</a>)</p>
  </li>
  <li>
    <p>While I’m linking Dutch pentagonal tiling architecture, here’s <a href="https://www.19hetatelier.nl/nieuws/wiskundige-vijfhoek-op-gevel-basisschool-de-garve-lochem/">an elementary school in Lochem decorated with the Mann–McLoud–Von Derau tile</a> (<a href="https://mathstodon.xyz/@11011110/105042753206122004">\(\mathbb{M}\)</a>, <a href="https://twitter.com/alexvdbrandhof/status/1004661466149085184">via</a>), which in 2015 became the 15th and final Euclidean monohedral pentagonal tile to be found. The link is in Dutch but Google translate works well except at one point: the school’s name, “De Garve”, means “the sheaf”, and the article remarks that this is appropriate for a pattern that looks like ears of corn.</p>
  </li>
</ul></div>
    </content>
    <updated>2020-10-15T22:15:00Z</updated>
    <published>2020-10-15T22:15:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-10-19T00:37:05Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-4895611208507130576</id>
    <link href="https://blog.computationalcomplexity.org/feeds/4895611208507130576/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/50-years-of-pbs.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4895611208507130576" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4895611208507130576" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/50-years-of-pbs.html" rel="alternate" type="text/html"/>
    <title>50 Years of PBS</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Public Broadcasting Service (PBS) launched fifty years ago this month in the United States. The New York Times talks about its <a href="https://www.nytimes.com/2020/10/13/arts/television/pbs-50-anniversary.html">fifty reasons</a> how the network mattered. I'll throw in my thoughts.</p><p>I was just slightly too old for shows like Sesame Street, Electric Company, Mr. Rogers and Zoom, not that that stopped me from watching them. My kids grew up on Barney and Friends. My daughter even had a toy Barney that interacted with the show, which went <a href="https://blog.computationalcomplexity.org/2012/02/barney-evil-dinosaur.html">as well as you'd expect</a>. </p><p>PBS introduced me to those great British TV shows for young nerds like me including Monty Python and Doctor Who. I wasn't into Nova but did watch Carl Sagan's Cosmos religiously in high school.</p><p>My favorite PBS show was the American Experience, short documentaries about US culture. I remember learning about this history of Coney Island and the quiz show scandals before Robert Redford made a movie about it.</p><p>Siskel and Ebert got their start on PBS and became my go to source for movie reviews.</p><p>In 1987 PBS broadcasted Ivy League football games. One Saturday I sat down expecting to watch my alma mater and instead got supreme court hearings. Only on PBS could Cornell football get Borked.</p></div>
    </content>
    <updated>2020-10-15T13:00:00Z</updated>
    <published>2020-10-15T13:00:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-10-20T02:07:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7800</id>
    <link href="https://windowsontheory.org/2020/10/14/itc-2021-guest-post-by-benny-applebaum/" rel="alternate" type="text/html"/>
    <title>ITC 2021 (guest post by Benny Applebaum)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Following last year’s successful launch, we are happy to announce the second edition of the conference on Information-Theoretic Cryptography (ITC). The call for papers for ITC 2021 is out, and, to cheer you up during lockdowns, we prepared a short theme song https://youtu.be/kZT1icVoTp8   Feel free to add your own verse 😉 The submission deadline is … <a class="more-link" href="https://windowsontheory.org/2020/10/14/itc-2021-guest-post-by-benny-applebaum/">Continue reading <span class="screen-reader-text">ITC 2021 (guest post by Benny Applebaum)</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Following last year’s successful launch, we are happy to announce the second edition of the conference on <a href="https://itcrypto.github.io/" rel="noreferrer noopener" target="_blank"><em>Information-Theoretic Cryptography</em></a><em> (ITC)</em>.</p>



<p>The <a href="https://itcrypto.github.io/2021/" rel="noreferrer noopener" target="_blank">call for papers</a> for ITC 2021 is out, and, to cheer you up during lockdowns, we prepared a short theme song <a href="https://youtu.be/kZT1icVoTp8" rel="noreferrer noopener" target="_blank">https://youtu.be/kZT1icVoTp8</a>  </p>



<p>Feel free to add your own verse <img alt="&#x1F609;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f609.png" style="height: 1em;"/></p>



<p>The submission deadline is <strong>February 1st</strong>. Please submit your best work to ITC 2021! We hope to see many of you there!</p></div>
    </content>
    <updated>2020-10-14T22:05:24Z</updated>
    <published>2020-10-14T22:05:24Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-10-20T03:21:17Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://blogs.princeton.edu/imabandit/?p=1437</id>
    <link href="https://blogs.princeton.edu/imabandit/2020/10/13/2020/" rel="alternate" type="text/html"/>
    <title>2020</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>My latest post on this blog was on December 30th 2019. It seems like a lifetime away. The rate at which paradigm shifting events have been happening in 2020 is staggering. And it might very well be that the worst … <a href="https://blogs.princeton.edu/imabandit/2020/10/13/2020/">Continue reading <span class="meta-nav">→</span></a></p></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>My latest post on this blog was on December 30th 2019. It seems like a lifetime away. The rate at which paradigm shifting events have been happening in 2020 is staggering. And it might very well be that the worst of 2020 is ahead of us, especially for those of us currently in the USA. </p>
<p>When I started communicating online broadly (blog, twitter) I promised myself to keep it strictly about science (or very closely neighboring topics), so the few lines above is all I will say about the current worldwide situation.</p>
<p>In other news, as is evident from the 10 months hiatus in blogging, I have taken elsewhere (at least temporarily) my need for rapid communication about theorems that currently excite me. Namely to <a class="liinternal" href="https://www.youtube.com/sebastienbubeck">youtube</a>. Since the beginning of the pandemic I have been recording home videos of what would have been typically blog posts, with currently 5 such videos:</p>
<ol>
<li><a class="liinternal" href="https://youtu.be/uRarIjJGmhs">A law of robustness for neural networks</a> : I explain the conjecture we recently made that, for random data, any interpolating two-layers neural network must have its Lipschitz constant larger than the squareroot of the ratio between the size of the data set and the number of neurons in the network. This would prove that overparametrization is *necessary* for robustness.</li>
<li><a class="liinternal" href="https://youtu.be/U-XsUB69mvc">Provable limitations of kernel methods</a> : I give the proof by Zeyuan Allen-Zhu and Yuanzhi Li that there are simple noisy learning tasks where *no kernel* can perform well while simple two-steps procedures can learn.</li>
<li><a class="liinternal" href="https://youtu.be/6-GBDpe2kuI">Memorization with small neural networks</a> : I explain old (classical combinatorial) and new (NTK style) construction of optimally-sized interpolating two-layers neural networks.</li>
<li><a class="liinternal" href="https://youtu.be/HIwZH2C--nA">Coordination without communication</a> : This video is the only one in the current series where I don’t talk at all about neural networks. Specifically it is about the cooperative multiplayer multiarmed bandit problem. I explain the strategy we devised with Thomas Budzinski to solve this problem (for the stochastic version) without *any* collision at all between the players.</li>
<li><a class="liinternal" href="https://youtu.be/O84mcq7P_es">Randomized smoothing for certified robustness</a> : Finally, in the first video chronologically, I explain the only known technique for provable robustness guarantees in neural networks that can scale up to large models.</li>
</ol>
<p>The next video will be about basic properties of tensors, and how it can be used for smooth interpolation (in particular in the context of our law of robustness conjecture). After that, we will see, maybe more neural networks, maybe more bandits, maybe some non-convex optimization ….</p>
<p>Stay safe out there!</p><p/></div>
    </content>
    <updated>2020-10-14T03:14:07Z</updated>
    <published>2020-10-14T03:14:07Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Sebastien Bubeck</name>
    </author>
    <source>
      <id>https://blogs.princeton.edu/imabandit</id>
      <link href="https://blogs.princeton.edu/imabandit/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://blogs.princeton.edu/imabandit" rel="alternate" type="text/html"/>
      <subtitle>Random topics in optimization, probability, and statistics. By Sébastien Bubeck</subtitle>
      <title>I’m a bandit</title>
      <updated>2020-10-19T23:34:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5013</id>
    <link href="https://www.scottaaronson.com/blog/?p=5013" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5013#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5013" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Vote in person if you can</title>
    <summary xml:lang="en-US">[If you’re not American, or you’re American but a masochist who enjoys the current nightmare, this post won’t be relevant to you—sorry!] Until recently, this blog had a tagline that included “HOLD THE NOVEMBER US ELECTION BY MAIL.” So I thought I should warn readers that circumstances have changed in ways that have important practical […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><figure class="wp-block-image"><img alt="Image may contain: 1 person, eyeglasses and closeup" src="https://scontent.faus1-1.fna.fbcdn.net/v/t1.0-9/121693905_4150116585002989_4795318421355902630_n.jpg?_nc_cat=103&amp;_nc_sid=730e14&amp;_nc_ohc=BM8zCXAwmT4AX_q5CWD&amp;_nc_ht=scontent.faus1-1.fna&amp;oh=9fa3f3ccf4147209ce95a470d3dfef58&amp;oe=5FABBE94"/></figure>



<p><em>[If you’re not American, or you’re American but a masochist who enjoys the current nightmare, this post won’t be relevant to you—sorry!]</em></p>



<p>Until recently, this blog had a tagline that included “HOLD THE NOVEMBER US ELECTION BY MAIL.”  So I thought I should warn readers that circumstances have changed in ways that have important practical implications over the next few weeks.  It’s no longer that we <em>don’t know</em> whether Trump and Pence will acknowledge a <a href="https://projects.fivethirtyeight.com/2020-election-forecast/">likely loss</a>—rather, it’s that <em>we know they won’t</em>.  They were repeatedly asked; we all heard their answers.</p>



<p>That means that the <em>best</em> case, the ideal scenario, is already without precedent in the country’s 240-year history.  It’s a president who never congratulates the winner, who refuses to meet him or coordinate a transfer of power, who skips the inauguration, and who’s basically dragged from the White House on January 20, screaming to his supporters (and continuing to scream until his dying breath) that the election was faked.</p>



<p>As I said, that banana-republic outcome is now the <em>best</em> case.  But it’s also plausible that Trump simply declares himself the winner on election night, because the mail-in votes, urban votes, yet-to-be-counted votes, or any other votes that trend the wrong way are fake; social media and the Murdoch press amplify this fantasy; Trump calls on Republican-controlled state legislatures to set aside the “rigged” results and appoint their own slates of electors; the legislatures dutifully comply; and the Supreme Court A-OKs it all.  If you think none of that could happen, <a href="https://www.theatlantic.com/magazine/archive/2020/11/what-if-trump-refuses-concede/616424/?utm_source=facebook&amp;utm_medium=social&amp;utm_campaign=share&amp;fbclid=IwAR28w8ZJIZSlvqoOT-To3UWgMj4wVc9PkDA7Vbs60S_J7T0kOo1_RyVGe98">read this <em>Atlantic</em> article</a> from a few weeks ago, carefully to the end, and be more terrified than you’ve ever been in your life.  And don’t pretend that you know what would happen next.</p>



<p>I know, I know, I’m mentally ill, it’s Trump Derangement Syndrome, I see Nazis behind every corner just because they killed most of my relatives, a little global pandemic here and economic collapse there and riots and apocalyptic fires and resurgent fascism and I act as though it’s the whole world coming to an end.  A few months from now, after everything has gone swimmingly, this post will still be here and you can come back and tell me how crazy I was.  I accept that risk.</p>



<p>For now, though, the best chance to avert a catastrophe is for Trump not merely to lose, but <strong>lose in a landslide that’s already clear by election night</strong>.  Which means: as Michelle Obama <a href="https://www.vox.com/2020/8/18/21373177/michelle-obama-dnc-2020-speech-voting-post-office">advised</a> already in August, put on your mask, brave the virus, and vote in person if you can—<em>especially</em> if you live in a state that’s in play, and that won’t start tallying mail-in ballots till after election day.  If your state allows it, and if early votes will be counted by election night (check this!), vote early, when the lines are shorter.  That’s what Dana and I did this morning; Texas <a href="https://www.washingtonpost.com/outlook/2020/10/04/joe-biden-win-texas/">going blue</a> on election night would be one dramatic way to foreclose shenanigans.  If you can’t vote in person, or if your state counts mail-in ballots earlier, then vote by mail or drop-box, but <em>do it now</em>, so you have a chance to fix any problems well before Election Day.  (Note that, even in normal circumstances—which these aren’t—a substantial fraction of all mail-in ballots get rejected because of trivial errors.)  I welcome other tips in the comments, from the many readers more immersed in this stuff than I am.</p>



<p>And if this post helped spur you in any way, please say so in the comments.  It will improve my mood, thereby helping me finish my next post, which will be on the Continuum Hypothesis.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update:</span></strong> It’s always fascinating to check my comments and see the missives from parallel universes, where Trump is a normal candidate who one might decide to vote for based on normal criteria, rather than what he himself has announced he is: a knife to the entire system that underlies such decisions.  For a view from <em>this</em> universe, see (e.g.) <a href="https://www.nature.com/articles/d41586-020-02852-x?utm_source=twitter&amp;utm_medium=social&amp;utm_content=organic&amp;utm_campaign=NGMT_USG_JC01_GL_Nature&amp;fbclid=IwAR1tLfLbZwXPd6U3a8BnqxvvMxo3SUDPWxhQccXYz9BU9_dL8x8deY3_dRc">today’s <em>Nature</em> editorial</a>.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Another Update:</span></strong> If it allays anyone’s fears, I was pleasantly surprised by the level of pandemic preparedness when Dana and I went to vote.  It was in a huge, cavernous gym on the UT campus, the lines were very short, masks and 6ft distancing were strictly enforced, and finger-coverings and hand sanitizer were offered to everyone.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Unrelated Update (10/16):</span></strong> For those who are interested, here’s a <a href="https://mattasher.com/2020/10/16/scott-aaronson-on-the-hunt-for-real-randomness/">new podcast with me and Matt Asher</a>, where we talk about the use of quantum mechanics (especially Bell inequality violations) to generate certified random numbers.</p></div>
    </content>
    <updated>2020-10-14T01:35:53Z</updated>
    <published>2020-10-14T01:35:53Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Rage Against Doofosity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-10-16T15:33:42Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6616178737091923837</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6616178737091923837/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/hugh-woodin-kurt-godel-dwayne-rock.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6616178737091923837" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6616178737091923837" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/hugh-woodin-kurt-godel-dwayne-rock.html" rel="alternate" type="text/html"/>
    <title>Hugh Woodin, Kurt Godel, Dwayne `The Rock' Johnson, Robert De Niro, David Frum, Tom Selleck: Do I care what they think? Should I?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> MATH:</p><p>My last <a href="https://blog.computationalcomplexity.org/2020/10/revisiting-continuum-hypothesis.html">post</a> on CH mentioned that Hugh Woodin used to think NOT(CH) but now thinks CH. In both cases his reasons have some math content to them. Also, note that Hugh Woodin seems to believe that CH somehow HAS an answer. Kurt Godel also thought CH HAS an answer. It has been said that he could have announced  his result that CH is consistent by saying  L is THE model, and the problem is now solved. </p><p>Should we care what Hugh Woodin and Kurt Godel think about CH?</p><p>YES- they have both studied the issue A LOT. If you think CH should have an answer, then surely you would care what they think. </p><p>NO-  CH has no answer so there opinions are no better than mine. If you think CH does not have an answer then you might think this; however, I think you should still be at least INTERESTED in what people who have thought about the problem A LOT have to say, even if you will disagree with them.</p><p>But with MATH there are people who clearly know more than you on topics you care about, so it is worth hearing what they have to say. </p><p>POLITICS:</p><p>Recently Dwayne THE ROCK Johnson (by Wikipedia: actor, producer, businessman, and former professional wrestler) ENDORSED Joe Biden. Should we care about his opinion? Maybe, if wrestling fans and former pro wrestler tend to be Republicans, so this may indicate a shift. I do not know if this is the case. </p><p>Robert De Niro was in favor of impeaching Donald Trump. He also said that Trump was like a Gangster. He would know because he was in the movie GOODFELLOWS and later THE IRISHMAN (about Jimmy Hoffa). To be fair I do not think he said that is how he would know. Even so, I don't think I care what he thinks, unless he has some specialized knowledge I do not know about. </p><p>David Frum is a republican who had a break with the party NOT over Donald Trump, but over Obamacare- which you may recall was originally a CONSERVATIVE response to Hillarycare by the Heritage Foundation.  He has a good article on this <a href="https://www.theatlantic.com/politics/archive/2017/03/the-republican-waterloo/520833/">here</a>. Because he is an intelligent  republican in favor of Obamacare (or some version of it) he is worth listening to.</p><p>In POLITICS its trickier- who is worth listening to and why. For all I know, THE ROCK has made a detailed study of the Republican and Democratic platforms (actually this cannot be true since the Republicans did not have a platform this time). </p><p>COMMERCIALS:</p><p>Tom Selleck (Actor-Magnum PI a while back, Blue Bloods now)  does commercials for reverse mortgages. A while back I asked a group of people WHY he is doing them. Here were some answers and reactions</p><p>a) He needs the money. Not likely, he seems to have done well and does not seem to have the kind of bad habits (e.g., drugs) that need money. Maybe he has expensive tastes (my only expensive tastes is in fine European Kit Kat bars--- which actually are not that expensive). </p><p>b) He likes doing commercials. Maybe.</p><p>c) He believes in the product. At this, everyone cracked up in laughter.</p><p>This raises a more general point: Why does ANYONE believe ANY commercial since we KNOW the actor is being PAID to say it. I ask non rhetorically as always. </p></div>
    </content>
    <updated>2020-10-12T17:49:00Z</updated>
    <published>2020-10-12T17:49:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-10-20T02:07:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5005</id>
    <link href="https://www.scottaaronson.com/blog/?p=5005" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5005#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5005" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">My second podcast with Lex Fridman</title>
    <summary xml:lang="en-US">Here it is—enjoy! (I strongly recommend listening at 2x speed.) We recorded it a month ago—outdoors (for obvious covid reasons), on a covered balcony in Austin, as it drizzled all around us. Topics included: Whether the universe is a simulation Eugene Goostman, GPT-3, the Turing Test, and consciousness Why I disagree with Integrated Information Theory […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://www.youtube.com/watch?v=nAMjv0NAESM&amp;list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4">Here it is—enjoy!</a>  (I strongly recommend listening at 2x speed.)</p>



<p>We recorded it a month ago—outdoors (for obvious covid reasons), on a covered balcony in Austin, as it drizzled all around us.  Topics included:</p>



<ul><li>Whether the universe is a simulation</li><li>Eugene Goostman, GPT-3, the Turing Test, and consciousness</li><li>Why I disagree with Integrated Information Theory</li><li>Why I disagree with Penrose’s ideas about physics and the mind</li><li>Intro to complexity theory, including P, NP, PSPACE, BQP, and SZK</li><li>The US’s catastrophic failure on covid</li><li>The importance of the election</li><li>My objections to cancel culture</li><li>The role of love in my life (!)</li></ul>



<p>Thanks so much to Lex for his characteristically probing questions, apologies as always for my verbal tics, and here’s our <a href="https://lexfridman.com/scott-aaronson-2/">first podcast</a> for those who missed that one.</p></div>
    </content>
    <updated>2020-10-12T14:38:07Z</updated>
    <published>2020-10-12T14:38:07Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Metaphysical Spouting"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-10-16T15:33:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17669</id>
    <link href="https://rjlipton.wordpress.com/2020/10/11/are-black-holes-necessary/" rel="alternate" type="text/html"/>
    <title>Are Black Holes Necessary?</title>
    <summary>Our congratulations on the 2020 Nobel Prize in Physics Composite crop of src1, src2 Roger Penrose, Reinhard Genzel, and Andrea Ghez have won the 2020 Nobel Prize in Physics. The prize is divided half to Penrose for theoretical work and half to Genzel and Ghez for finding a convincing and appreciably large practical example. Today […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc">
<em>Our congratulations on the 2020 Nobel Prize in Physics</em>
<font color="#000000">


</font></font></p><table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2020/10/penrosegenzelghez.jpg"><img alt="" class="alignright wp-image-17675" height="115" src="https://rjlipton.files.wordpress.com/2020/10/penrosegenzelghez.jpg?w=300" width="320"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite crop of <a href="https://www.bbc.com/news/science-environment-54439150">src1</a>, <a href="https://theconversation.com/nobel-prize-how-penrose-genzel-and-ghez-helped-put-black-holes-at-the-centre-of-modern-astrophysics-147613">src2</a></font></td>
</tr>
</tbody>
</table><font color="#0044cc"><font color="#000000">



<p>
Roger Penrose, Reinhard Genzel, and Andrea Ghez have won the 2020 Nobel Prize in Physics. The prize is divided half to Penrose for theoretical work and half to Genzel and Ghez for finding a convincing and appreciably large practical example.

</p><p>
Today we congratulate the winners and give further musings on the nature of knowledge and the role of theory.

</p><p>
The physics Nobel has always had the rule that it cannot be for a theory alone, no matter how beautiful and how many mathematical discoveries follow from its development. Stephen Hawking’s theory of black-hole <a href="https://en.wikipedia.org/wiki/Hawking_radiation">radiation</a> is almost universally accepted, despite its association with <a href="https://en.wikipedia.org/wiki/Black_hole_information_paradox">paradox</a>, yet it was said that only an empirical confirmation such as mini-black holes being discovered to explode in an accelerator core would have brought it a Nobel. The official citation to Sir Roger says that his prize is:

</p><p>

</p><blockquote><b> </b> <em> “for the discovery that black hole formation is a robust prediction of the general theory of relativity.” </em>
</blockquote>




<p>
What is a “robust” prediction? The word strikes us as having overtones of <em>necessity</em>. Necessary knowledge is the kind we deal with in mathematics. The citation to Genzel and Ghez stays on empirical grounds:

</p><p>

</p><blockquote><b> </b> <em> “for the discovery of a supermassive compact object at the centre of our galaxy.” </em>
</blockquote>


<p>



</p><p>
The “object” <em>must</em> be a black hole—given relativity and its observed gravitational effects, it cannot be otherwise. Among many possible witnesses for the reality of black holes—one being the evident origin of the gravitational waves whose <a href="https://rjlipton.wordpress.com/2016/02/16/waves-hazards-and-guesses/">detection</a> brought the 2017 Nobel—the centers of galaxies are hefty examples. The combination of these citations opens several threads we’d like to discuss.

</p><p>



</p><p>

</p><h2> The Proof Horizon of a Black Hole </h2>


<p>



</p><p>
Dick and I are old enough to remember when black holes had the status of conjecture. One of my childhood astronomy books stated that the <a href="https://en.wikipedia.org/wiki/Cygnus_X-1">Cygnus X-1</a> X-ray source was the best known candidate for a black hole. In 1974, Hawking bet Kip Thorne that it was not a black hole. The bet lasted until 1990, when Hawking conceded. He wrote the following in his famous <a href="https://en.wikipedia.org/wiki/A_Brief_History_of_Time">book</a>, <em>A Brief History of Time</em>:

</p><p>

</p><blockquote><b> </b> <em> This was a form of insurance policy for me. I have done a lot of work on black holes, and it would all be wasted if it turned out that black holes do not exist. But in that case, I would have the consolation of winning my bet. … When we made the bet in 1975, we were 80% certain that Cygnus X-1 was a black hole. By now [1988], I would say that we are about 95% certain, but the bet has yet to be settled. </em>
</blockquote>


<p>



</p><p>
In the 1980s, I was a student and then postdoc in Penrose’s department, so I was imbued with the ambience of black holes and never had a thought about doubting their existence. I even once spent an hour with John Wheeler, who coined the term “black hole,” when Penrose delegated me to accompany Wheeler to Oxford’s train station for his return to London. But it seems from the record that the progression to regarding black holes as proven entities was as gradual as many argue the act of crossing a large black hole’s event horizon to be. Although the existence of a central black hole from data emanating from Sagittarius had been proposed at least as far back as 1971, the work by Ghez and then Genzel cited for their prize began in 1995. The official <a href="https://www.nobelprize.org/prizes/physics/2002/press-release/">announcement</a> for Riccardo Giacconi’s share of the 2002 physics Nobel stated:

</p><p>

</p><blockquote><b> </b> <em> “He also detected sources of X-rays that most astronomers now consider to contain black holes.” </em>
</blockquote>


<p>



</p><p>
This speaks lingering doubt at least about <em>where</em> black holes might be judged to exist, if not their existence at all.

</p><p>
However their time of confirmation might be pinpointed, it is the past five years that have given by far the greatest flood of evidence, including the first visual <a href="https://www.jpl.nasa.gov/edu/news/2019/4/19/how-scientists-captured-the-first-image-of-a-black-hole/">image</a> of a black hole last year. The fact of their presence in our universe is undeniable. But <em>necessity</em> is a separate matter, and with Penrose this goes back to 1964.

</p><p>



</p><p>

</p><h2> Relativity and Necessity </h2>


<p>



</p><p>
We have <a href="https://rjlipton.wordpress.com/2011/10/31/an-interview-with-kurt-gdel/">mentioned</a> Kurt Gödel’s solution to the equations of general relativity (GR) in which time travel is possible. This does not mean that time travel must be possible, or that it is possible in our universe. A “solution” to GR is more like a <em>model</em> in logic: it may satisfy a theory’s axioms but have other properties that are contingent (unless the theory is <em>categorical</em>, meaning that all of its models are isomorphic). Gödel’s model has a negative value for Einstein’s <a href="https://en.wikipedia.org/wiki/Cosmological_constant">cosmological constant</a>; the 2011 physics Nobel went to the discovery that in our universe the constant has a tiny <a href="https://rjlipton.wordpress.com/2011/10/12/empirical-humility/">positive</a> value. GR also allows solutions in which some particles (called <em>tachyons</em>) travel faster than light.

</p><p>
That GR has solutions allowing black holes had been known from its infancy in work by Karl Schwarzschild and Johannes Droste. There are also solutions without black holes; a universe with no mass is legal in GR in <a href="https://en.wikipedia.org/wiki/Vacuum_solution_(general_relativity)">many ways</a> besides the case of special relativity. Penrose took the opposite tack, of giving minimal conditions under which black holes are <em>necessary</em>. Following this <a href="https://theconversation.com/nobel-prize-how-penrose-genzel-and-ghez-helped-put-black-holes-at-the-centre-of-modern-astrophysics-147613">article</a>, we list them informally as follows:

</p><p>


</p><ol> 
<li>
Sufficiently large concentrations of mass exerting gravity exist. 
</li><li>
Gravity always attracts, never repels. 
</li><li>
No physical effect can travel faster than light. 
</li><li>
Gravity determines how light bends and moves. 
</li><li>
The space-time manifold is metrically complete. 
</li></ol>



<p>
Penrose showed that any system obeying these properties and evolving in accordance with GR must develop black holes. He showed this without any symmetry assumptions on the system. Thus he derived black holes as a prediction with the force of a theorem derived from minimal axioms.

</p><p>
His 1965 <a href="http://quantum-gravitation.de/media/2d2cde3ec9c38fffffff80d0fffffff1.pdf">paper</a> actually used a proof by contradiction. He derived five properties needed in order for the system to avoid forming a singularity. Then he showed they are mutually inconsistent—a proof by contradiction. Here is the crux of his paper:

</p><p>




</p><p>
</p><table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2020/10/penrosediagram1965.jpg"><img alt="" class="aligncenter wp-image-17673" height="442" src="https://rjlipton.files.wordpress.com/2020/10/penrosediagram1965.jpg?w=600" width="600"/></a>
<p>
</p></td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ Snip from <a href="http://quantum-gravitation.de/media/2d2cde3ec9c38fffffff80d0fffffff1.pdf">paper</a> ]</font>
</td>
</tr>
</tbody></table>



<p>
In the diagram, time flows up. The point in a nutshell—a very tight nutshell—is that once a surface flows inside the cylinder at the Schwarzschild radius then light and any other motion from it can go only inward toward a singularity. The analysis is possible without the kind of symmetry assumption that had been used to tame the algebraic complexity of the equations of GR. The metric completeness mandates a singularity apart from any symmetries; a periodic equilibrium is ruled out by analysis of Cauchy surfaces.

</p><p>



</p><p>

</p><h2> Necessary For Us? </h2>


<p>



</p><p>
Like Richard Feynman’s famous diagrams for quantum field theory, Penrose developed his diagrams as tools for shortcutting the vicissitudes of GR. We could devote entire other posts to his famous <a href="https://en.wikipedia.org/wiki/Penrose_tiling">tiles</a> and <a href="https://en.wikipedia.org/wiki/Penrose_triangle">triangle</a> and other combinatorial inventions. His tools enable quantifying black-hole formation from observations in our universe.

</p><p>
The question of <em>necessity</em>, however, pertains to other possible universes. Let us take for granted that GR and quantum theory are facets of a physical theory that governs the entire cosmos—the long-sought “theory of everything”—and let us also admit the contention of inflationary theorists that multiple universes are a necessary consequence of any inflation theory. The question remains, <em>are black holes necessary in those universes?</em>

</p><p>
It is possible that those universes might not satisfy axiom 1 above, or might have enough complexity for existence of black holes but not large-scale formation of them. The question then becomes whether black holes must exist in any universe rich enough for sentient life forms such as ourselves to develop. This is a branch of the <a href="https://en.wikipedia.org/wiki/Anthropic_principle">anthropic principle</a>.

</p><p>
Lee Smolin <a href="https://en.wikipedia.org/wiki/Cosmological_natural_selection">proposed</a> a mechanism via which black holes engender new universes and so propagate the complexity needed for their large-scale formation. Since complexity also attends the development of sentient life forms, this would place our human existence in the wake of consequence, as opposed to the direction of logic when reasoning by the anthropic principle.

</p><p>



</p><p>

</p><h2> A Little More About Science </h2>


<p>



</p><p>
The 2020 Nobel Prize in Chemistry was awarded this week to Jennifer Doudna and Emmanuelle Charpentier for their lead roles in developing the <a href="https://en.wikipedia.org/wiki/CRISPR">CRISPR</a> gene-editing technology, specifically around the protein <a href="https://en.wikipedia.org/wiki/Cas9">Cas9</a>. 

</p><p>
We argue that two more different types of results cannot be found: 

</p><p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\bullet }"/> Penrose shows that black holes and general relativity are connected, which is a math result. We still cannot create black holes in a lab to experiment with—or maybe we could but should be very afraid of going anywhere near doing so. It was not clear that there could ever be a real application of this result.

</p><p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\bullet }"/> Charpentier and Doudna discover that an existing genetic mechanism could be used to edit genetic material. Clearly this can and was experimented on in labs. Also clear that there are applications of this result. Actually it is now a standard tool used in countless labs. There even are patent battles over the method.

</p><p>
We like the fact that Nobels are given for such diverse type of research. It is not just that one is for astrophysics and one for chemistry. It is that Nobels can be given for very different types of research. We think this is important.

</p><p>
But wait. These results do have something in common, something that sets them apart from any research we can do in complexity theory. Both operate like this:

</p><p>

</p><blockquote><b> </b> <em> 	Observe something important from nature. Something that is there independent of us. Then in Penrose’s case explain why it is true. Then in Charpentier and Doudna’s case, use it to solve some important problems. </em>
</blockquote>


<p>



</p><p>
We wonder if anything like this could be done in our research world—say in complexity theory?

</p><p>



</p><p>

</p><h2> Open Problems </h2>


<p>



</p><p>
Besides our congratulations to all those mentioned in this post, Ken expresses special thanks to Sir Roger among other Oxford Mathematical Institute fellows for the kindness recorded <a href="https://rjlipton.wordpress.com/2011/03/09/tex-is-great-what-is-tex/#comment-11172">here</a>.

</p>

<p>
[changed note about massless universe]</p></font></font>



<p/></div>
    </content>
    <updated>2020-10-11T19:19:58Z</updated>
    <published>2020-10-11T19:19:58Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="News"/>
    <category term="Proofs"/>
    <category term="Andrea Ghez"/>
    <category term="black hole"/>
    <category term="Chemistry"/>
    <category term="Emmanuelle Charpentier"/>
    <category term="Jennifer Doudna"/>
    <category term="John Wheeler"/>
    <category term="knowledge"/>
    <category term="Lee Smolin"/>
    <category term="necessity"/>
    <category term="Nobel Prize"/>
    <category term="Physics"/>
    <category term="progress in knowledge"/>
    <category term="Reinhard Genzel"/>
    <category term="relativity"/>
    <category term="Riccardo Giacconi"/>
    <category term="Roger Penrose"/>
    <category term="science"/>
    <category term="scientific proof"/>
    <category term="Stephen Hawking"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-10-20T03:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/154</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/154" rel="alternate" type="text/html"/>
    <title>TR20-154 |  A Structural Theorem for Local Algorithms with Applications to Coding, Testing, and Privacy | 

	Marcel Dall&amp;#39;Agnol, 

	Tom Gur, 

	Oded Lachish</title>
    <summary>We prove a general structural theorem for a wide family of local algorithms, which includes property testers, local decoders, and PCPs of proximity. Namely, we show that the structure of every algorithm that makes $q$ adaptive queries and satisfies a natural robustness condition admits a sample-based algorithm with $n^{1- 1/O(q^2 \log^2 q)}$ sample complexity, following the definition of Goldreich and Ron (TOCT 2016). We prove that this transformation is nearly optimal. Our theorem also admits a scheme for constructing privacy-preserving local algorithms. 

Using the unified view that our structural theorem provides, we obtain results regarding various types of local algorithms, including the following.


- We strengthen the state-of-the-art lower bound for relaxed locally decodable codes, obtaining an exponential improvement on the dependency in query complexity; this resolves an open problem raised by Gur and Lachish (SODA 2020).
- We show that any (constant-query) testable property admits a sample-based tester with sublinear sample complexity; this resolves a problem left open in a work of Fischer, Lachish, and Vasudev (FOCS 2015) by extending their main result to adaptive testers.
- We prove that the known separation between proofs of proximity and testers is essentially maximal; this resolves a problem left open by Gur and Rothblum (ECCC 2013, Computational Complexity 2018) regarding sublinear-time delegation of computation.

Our techniques strongly rely on relaxed sunflower lemmas and the Hajnal–Szemerédi theorem.</summary>
    <updated>2020-10-10T11:21:31Z</updated>
    <published>2020-10-10T11:21:31Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-10-20T03:20:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/153</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/153" rel="alternate" type="text/html"/>
    <title>TR20-153 |  Total Functions in the Polynomial Hierarchy | 

	Robert Kleinberg, 

	Daniel Mitropolsky, 

	Christos Papadimitriou</title>
    <summary>We identify several genres of search problems beyond NP for which existence of solutions is guaranteed.  One class that seems especially rich in such problems is PEPP (for "polynomial empty pigeonhole principle"), which includes problems related to existence theorems proved through the union bound, such as finding a bit string that is far from all codewords, finding an explicit rigid matrix, as well as a problem we call Complexity, capturing Complexity Theory's quest.  When the union bound is generous, in that solutions constitute at least a polynomial fraction of the domain, we have a family of seemingly weaker classes $\alpha$-PEPP, which are inside FP}$^{\text{NP}}|$poly.  Higher in the hierarchy, we identify the constructive version of the Sauer-Shelah lemma and the appropriate generalization of PPP that contains it.  The resulting total function hierarchy turns out to be more stable than the polynomial hierarchy: it is known that, under oracles, total functions within FNP may be easy, but total functions a level higher may still be harder than FP$^{\text{NP}}$.</summary>
    <updated>2020-10-08T22:31:10Z</updated>
    <published>2020-10-08T22:31:10Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-10-20T03:20:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/152</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/152" rel="alternate" type="text/html"/>
    <title>TR20-152 |  Variants of the Determinant polynomial and VP-completeness | 

	Prasad Chaugule, 

	Nutan Limaye, 

	Shourya Pandey</title>
    <summary>The determinant is a canonical VBP-complete polynomial in the algebraic complexity setting. In this work, we introduce two variants of the determinant polynomial which we call $StackDet_n(X)$ and $CountDet_n(X)$ and show that they are VP and VNP complete respectively under $p$-projections. The definitions of the polynomials are inspired by a combinatorial characterisation of the determinant developed by Mahajan and Vinay (SODA 1997). We extend the combinatorial object in their work, namely clow sequences, by introducing additional edge labels on the edges of the underlying graph. The idea of using edge labels is inspired by the work of Mengel (MFCS 2013).</summary>
    <updated>2020-10-08T22:27:25Z</updated>
    <published>2020-10-08T22:27:25Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-10-20T03:20:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=1380</id>
    <link href="https://francisbach.com/hermite-polynomials/" rel="alternate" type="text/html"/>
    <title>Polynomial magic III : Hermite polynomials</title>
    <summary>After two blog posts earlier this year on Chebyshev and Jacobi polynomials, I am coming back to orthogonal polynomials, with Hermite polynomials. This time, in terms of applications to machine learning, no acceleration, but some interesting closed-form expansions in positive-definite kernel methods. Definition and first properties There are many equivalent ways to define Hermite polynomials....</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">After two blog posts earlier this year on <a href="https://francisbach.com/chebyshev-polynomials/">Chebyshev</a> and <a href="https://francisbach.com/jacobi-polynomials/">Jacobi</a> polynomials, I am coming back to orthogonal polynomials, with Hermite polynomials. </p>



<p class="justify-text">This time, in terms of applications to machine learning, no acceleration, but some interesting closed-form expansions in positive-definite kernel methods. </p>



<h2>Definition and first properties</h2>



<p class="justify-text">There are many equivalent ways to define Hermite polynomials. A natural one is through the so-called <a href="https://en.wikipedia.org/wiki/Rodrigues%27_formula">Rodrigues’ formula</a>: $$H_k(x) = (-1)^k e^{x^2} \frac{d^k}{d x^k}\big[ e^{-x^2} \big],$$ from which we can deduce \(H_0(x) = 1\), \(H_1(x) =\   – e^{x^2} \big[ -2x e^{-x^2} \big] = 2x\), \(H_2(x) = e^{x^2} \big[ (-2x)^2e^{-x^2} -2 e^{-x^2}  \big] =  4x^2 – 2\), etc.</p>



<p class="justify-text">Other simple properties which are consequences of the definition (and can be shown by recursion) are that \(H_k\) is a polynomial of degree \(k\), with the same parity as \(k\), and with a leading coefficient equal to \(2^k\).</p>



<p class="justify-text"><strong>Orthogonality for Gaussian distribution.</strong> Using integration by parts, one can show (see end of the post) that for \(k \neq \ell\), we have $$\int_{-\infty}^{+\infty}  \!\!\!H_k(x) H_\ell(x) e^{-x^2} dx =0, $$ and that for \(k=\ell\), we have $$\int_{-\infty}^{+\infty} \!\!\! H_k(x)^2 e^{-x^2}dx = \sqrt{\pi} 2^k k!.$$ </p>



<p class="justify-text">In other words, the Hermite polynomials are orthogonal for the Gaussian distribution with mean \(0\) and variance \(\frac{1}{2}\). Yet in other words, defining the <em>Hermite functions</em> as \( \displaystyle \psi_k(x) = (\sqrt{\pi} 2^k k!)^{-1/2} H_k(x) e^{-x^2/2}\), we obtain an orthonormal basis of \(L_2(dx)\). As illustrated below, the Hermite functions, as the index \(k\) increases, have an increasing “support” (the support is always the entire real line, but most of the mass is concentrated in centered balls of increasing sizes, essentially at \(\sqrt{k}\)) and, like cosines and sines, an increasingly oscillatory behavior.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-4579" height="305" src="https://francisbach.com/wp-content/uploads/2020/08/hermite.gif" width="349"/>Plot of Hermite functions \(\psi_k(x) = (\sqrt{\pi} 2^k k!)^{-1/2} H_k(x) e^{-x^2/2}\), from \(k=0\) to \(k=20\).</figure></div>



<p class="justify-text">Among such orthonormal bases, the Hermite functions happen to be diagonalizing the Fourier tranform operator.  In other words, the Fourier transform of \(\psi_k\) (for the definition making it an isometry of \(L_2(dx)\)) is equal to $$ \mathcal{F}(\psi_k)(\omega)  =  \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{+\infty} \psi_k(x) e^{- i \omega x} dx = (-i)^k \psi_k(\omega).$$ (note that the eigenvalues are all of unit modulus as we have an isometry). See a proof at the end of the post. I am not aware of any applications of this property in machine learning or statistics (but there are probably some).</p>



<p class="justify-text"><strong>Recurrence.</strong> In order to compute Hermite polynomials, the following recurrence relation is the most useful $$ H_{k+1}(x) = 2x H_k(x) \ – 2k H_{k-1}(x). \tag{1}$$  Such recursions are always available for orthogonal polynomials (see [4]), but it takes here a particularly simple form (see a proof at the end of the post).</p>



<p class="justify-text"><strong>Generating function.</strong> The following property is central in many proofs of properties of Hermite polynomials: for all \(t \in \mathbb{R}\), we have $$\sum_{k=0}^\infty \frac{t^k}{k!} H_k(x) =e^{ 2xt \ – \ t^2}, \tag{2}$$ with a proof at the end of the post based on the residue theorem.</p>



<h2>Further (less standard) properties</h2>



<p class="justify-text">For the later developments, we need other properties which are less standard (there are many other interesting properties, which are not useful for this post, see <a href="https://en.wikipedia.org/wiki/Hermite_polynomials">here</a>).</p>



<p class="justify-text"><strong>Mehler formula. </strong>For \(|\rho| &lt; 1\), it states: $$ \exp \Big( – \frac{\rho}{1- \rho^2} (x-y)^2\Big) = \sqrt{1-\rho^2} \sum_{k=0}^\infty \frac{\rho^k}{2^k k!} H_k(x) H_k(y) \exp \Big( – \frac{\rho}{1+\rho} (x^2 + y^2) \Big).$$ The proof is significantly more involved; see [<a href="https://academic.oup.com/jlms/article-pdf/s1-8/3/194/2363185/s1-8-3-194.pdf">1</a>] for details (with a great last sentence: “Prof. Hardy tells me that he has not seen his proof in print, though the inevitability of the successive steps makes him think that it is unlikely to be new”). Note that we will in fact obtain a new proof from the relationship with kernel methods (see below).</p>



<p class="justify-text"><strong>Expectation for Gaussian distributions. </strong>We will need this property for \(|\rho|&lt;1\) (see proof at the end of the post), which corresponds to the expectation of \(H_k(x)\) for \(x\) distributed as a non-centered Gaussian distribution: $$\int_{-\infty}^\infty H_k(x) \exp\Big( – \frac{(x-\rho y)^2}{1-\rho^2} \Big)dx= \sqrt{\pi} \rho^k \sqrt{1-\rho^2} H_k (y). \tag{3}$$</p>



<p class="justify-text">Given the relationship with the Gaussian distribution, it is no surprise that Hermite polynomials pop up whenever Gaussians are used, as distributions or kernels. Before looking into it, let’s first give a brief review of kernel methods.</p>



<h2>From positive-definite kernel to Hilbert spaces</h2>



<p class="justify-text">Given a prediction problem with inputs in a set \(\mathcal{X}\), a traditional way of parameterizing real-valued functions on \(\mathcal{X}\) is to use <em>positive-definite kernels</em>.</p>



<p class="justify-text">A positive-definite kernel is a function \(K: \mathcal{X} \times \mathcal{X} \to \mathbb{R}\) such that for all sets \(\{x_1,\dots,x_n\}\) of \(n\) elements of \(\mathcal{X}\), the “kernel matrix” in \(\mathbb{R}^{n \times n}\) composed of pairwise evaluations is symmetric positive semi-definite. This property happens to be equivalent to the existence of a Hilbert feature space \(\mathcal{H}\) and a feature map \(\varphi: \mathcal{X} \to \mathcal{H}\) such that $$K(x,x’) = \langle \varphi(x), \varphi(x’) \rangle_{\mathcal{H}},$$ with an elegant constructive proof [<a href="https://www.ams.org/journals/tran/1950-068-03/S0002-9947-1950-0051437-7/S0002-9947-1950-0051437-7.pdf">15</a>].</p>



<p class="justify-text">This allows to define the space of linear functions on the features, that is, functions of the form $$f(x) = \langle f, \varphi(x) \rangle_{\mathcal{H}},$$ for \(f \in \mathcal{H}\). </p>



<p class="justify-text">This space is often called the <a class="" href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">reproducing kernel Hilbert space</a> (RKHS) associated to the kernel \(K\) (we can prove that it is indeed uniquely defined). In such a space, we can also define the squared norm of the function \(f\), namely \(\| f\|_{\mathcal{H}}^2\), which can be seen as a specific regularization term in kernel methods.</p>



<p class="justify-text">The space satisfies the so-called reproducing property (hence its name): \(f(x) = \langle f, K(\cdot,x) \rangle_{\mathcal{H}}\). In other words, the feature \(\varphi(x)\) is the kernel function evaluated at \(x\), that is,  \(\varphi(x) = K(\cdot,x)\). These spaces have been a source of many developments in statistics [5] and machine learning [6, 7].</p>



<p class="justify-text"><strong>Orthonormal basis.</strong> A difficulty in working with infinite-dimensional Hilbert spaces of functions is that it is sometimes hard to understand what functions are actually considered. One simple way to enhance understanding of the regularization property is to have an orthonormal basis (in very much the same way as the Fourier basis), as we can then identify \(\mathcal{H}\) to the space of squared-integrable sequences.</p>



<p class="justify-text">For kernel-based Hilbert spaces, if we have an orthonormal basis \((g_k)_{k \geqslant 0}\) of the Hilbert space \(\mathcal{H}\), then, by decomposing \(\varphi(x)\) in the basis, we have $$\varphi(x) = \sum_{k =0}^\infty \langle \varphi(x), g_k \rangle_\mathcal{H} g_k,$$ we get $$K(x,y) = \langle \varphi(y), \varphi(x) \rangle = \sum_{k =0}^\infty \langle \varphi(x), g_k \rangle_\mathcal{H} \langle  \varphi(y), g_k \rangle_\mathcal{H} =\sum_{k=0}^\infty g_k(x) g_k(y), \tag{4}$$ that is, we have an expansion of the kernel as an infinite sum (note here, that we ignore summability issues).</p>



<p class="justify-text">Among orthonormal bases, some are more interesting than others. The ones composed of eigenfunctions for particular operators are really more interesting, in particular for the covariance operator that we now present, and their use in statistical learning theory.</p>



<h2>Analyzing ridge regression through covariance operators</h2>



<p class="justify-text">The most classical problem where regularization by RKHS norms occurs is <em>ridge regression</em>, where, given some observations \((x_1,y_1),\dots,(x_n,y_n) \in \mathcal{X} \times \mathbb{R}\), one minimizes with respect to \(f \in \mathcal{H}\): $$ \frac{1}{n} \sum_{i=1}^n \big( y_i \ – \langle f, \varphi(x_i) \rangle_{\mathcal{H}} \big)^2 +  \lambda \| f\|_{\mathcal{H}}^2.$$</p>



<p class="justify-text">In finite dimensions, the convergence properties are characterized by the (non-centered) covariance matrix \(\Sigma = \mathbb{E} \big[ \varphi(x) \otimes \varphi(x) \big]\), where the expectation is taken with respect to the underlying distribution of the observations \(x_1,\dots,x_n\) (which are assumed independently and identically distributed for simplicity). If \(\mathcal{H} = \mathbb{R}^d\), then \(\Sigma\) is a \(d \times d\) matrix. </p>



<p class="justify-text">For infinite-dimensional \(\mathcal{H}\), the same expression \(\Sigma = \mathbb{E} \big[ \varphi(x) \otimes \varphi(x) \big]\) defines a linear <em>operator</em> from \(\mathcal{H}\) to  \(\mathcal{H}\), so that for \(f,g \in \mathcal{H}\), we have $$\langle f, \Sigma g \rangle_{\mathcal{H}} = \mathbb{E} \big[ \langle f, \varphi(x)\rangle_{\mathcal{H}}\langle g, \varphi(x)\rangle_{\mathcal{H}}\big] = \mathbb{E} \big[ f(x) g(x) \big].$$</p>



<p class="justify-text">The generalization property of ridge regression has been thoroughly studied (see, e.g., [8, 9]), and if there exists \(f_\ast \in \mathcal{H}\) such that \(y_i = \langle f_\ast, \varphi(x_i) \rangle + \varepsilon_i\) for a noise \(\varepsilon_i\) which is independent of \(x_i\), with zero mean and variance equal to \(\sigma^2\), then the expected error on unseen data is asymptotically upper-bounded by $$\sigma^2 + \lambda \| f_\ast\|_{\mathcal{H}}^2 + \frac{\sigma^2}{n} {\rm tr} \big[ \Sigma ( \Sigma + \lambda I)^{-1} \big].$$ The first term \(\sigma^2\) is the best possible expected performance, the term \(\lambda \| f_\ast\|_{\mathcal{H}}^2\) is usually referred to as the <em>bias</em> term and characterizes the bias introduced by regularizing towards zero, while the third term \(\frac{\sigma^2}{n} {\rm tr} \big[ \Sigma ( \Sigma + \lambda I)^{-1} \big]\) is the <em>variance</em> term, which characterizes the loss in performance due to the observation of only \(n\) observations.</p>



<p class="justify-text">The quantity \({\rm df}(\lambda) = {\rm tr} \big[ \Sigma ( \Sigma + \lambda I)^{-1} \big]\) is often referred to as the degrees of freedom [10]. When \(\lambda\) tends to infinity, then \({\rm df}(\lambda)\) tends to zero; when \(\lambda\) tends to zero, then \({\rm df}(\lambda)\) tends to the number of non-zero eigenvalues of \(\Sigma\). Thus, in finite dimension, this typically leads to the underlying dimension. Given the usual variance term in \(\sigma^2 \frac{d}{n}\) for ordinary least-squares with \(d\)-dimensional features, \({\rm df}(\lambda)\) is often seen as an implicit number of parameters for kernel ridge regression.</p>



<p class="justify-text">In infinite dimensions, under mild assumptions, there are infinitely many eigenvalues for \(\Sigma\), which form a decreasing sequence \((\lambda_i)_{i \geqslant 0}\) that tends to zero (and is summable, with a sum equal to the trace of \(\Sigma\)). The rate of such a decay is key to understanding the generalization capabilities of kernel methods. With the following classical types of decays:</p>



<ul class="justify-text"><li><em>Polynomial decays</em>: If \(\lambda_i \leqslant \frac{C}{(i+1)^{\alpha}}\) for \(\alpha &gt; 1\), then one can upper bound the sum by an integral as $$ {\rm tr} \big[ \Sigma ( \Sigma + \lambda I)^{-1} \big] = \sum_{i=0}^\infty \frac{\lambda_i}{\lambda_i + \lambda} \leqslant \sum_{i=1}^\infty \frac{1}{1  + \lambda i^\alpha / C} \leqslant \int_0^\infty \frac{1}{1+\lambda t^\alpha / C} dt.$$ With the change of variable \(u = \lambda t^\alpha / C\), we get that \({\rm df}(\lambda) = O(\lambda^{-\alpha})\). We can then balance bias and variance with \(\lambda \sim n^{-\alpha/(\alpha+1)}\) and an excess risk proportional to \(n^{-\alpha/(\alpha+1)}\). This type of decay is typical of <a href="https://en.wikipedia.org/wiki/Sobolev_space">Sobolev spaces</a>.</li><li><em>Exponential decays</em>: If \(\lambda_i \leqslant {C}e^{-\alpha i}\), for some \(\alpha &gt;0\), we have $$ {\rm tr} \big[ \Sigma ( \Sigma + \lambda I)^{-1} \big]  \leqslant \sum_{i=0}^\infty \frac{{C}e^{-\alpha i}}{  \lambda + {C}e^{-\alpha i}} \leqslant \int_{0}^\infty \frac{{C}e^{-\alpha t}}{ \lambda + {C}e^{-\alpha t}}dt.$$ With the change of variable \(u = e^{-\alpha t}\), we get an upper bound $$\int_{0}^1 \frac{C}{\alpha}\frac{1}{ \lambda + {C}u}du = \frac{1}{\alpha}\big[ \log(\lambda + C) \ – \log (\lambda) \big] = \frac{1}{\alpha} \log \big( 1+\frac{C}{\lambda} \big).$$ We can then balance bias and variance with \(\lambda \sim 1/n \) and an excess risk proportional to \((\log n) / n \), which is very close to the usual parametric (finite-dimensional) rate in \(O(1/n)\). We will see an example of this phenomenon for the Gaussian kernel.</li></ul>



<p class="justify-text">In order to analyze the generalization capabilities, we consider a measure \(d \mu\) on \(\mathcal{X}\), and the following (non-centered) <em>covariance operator</em> defined above as $$\mathbb{E} \big[ \varphi(x) \otimes \varphi(x) \big],$$ which is now an self-adjoint operator from \(\mathcal{H}\) to \(\mathcal{H}\) with a finite trace. The traditional empirical estimator \(\hat\Sigma = \frac{1}{n} \sum_{i=1}^n \varphi(x_i) \otimes \varphi(x_i)\), whose eigenvalues are the same as the eigenvalues of \(1/n\) times the \(n \times n\) kernel matrix of pairwise kernel evaluations (see simulation below).</p>



<p class="justify-text"><strong>Characterizing eigenfunctions.</strong> If \((g_k)\) is the eigenbasis associated to the eigenfunctions of \(\Sigma\), then it has to be an orthogonal family that span the entire space \(\mathcal{H}\) and such that \(\Sigma g_k = \lambda_k g_k\). Applying it to \(\varphi(y) = K(\cdot,y)\), we get $$ \langle K(\cdot,y), \Sigma g_k \rangle_{\mathcal{H}} = \mathbb{E} \big[ K(x,y) g_k(x) \big] = \lambda_k \langle g_k, \varphi(y)\rangle_\mathcal{H} =  \lambda_k g_k(y),$$ which implies that the functions also have to be eigenfunctions of the self-adjoint so-called <em>integral operator</em> \(T\) defined on \(L_2(d\mu)\) as \(T f(y) = \int_{\mathcal{X}} K(x,y) f(y) d\mu(y)\). Below, we will check this property. Note that this other notion of integral operator (defined on \(L_2(d\mu)\) and not in \(\mathcal{H}\)), which has the same eigenvalues and eigenfunctions, is important to deal with mis-specified models (see [9]). Note that the eigenfunctions \(g_k\) are orthogonal for both dot-products in \(L_2(d\mu)\) and \(\mathcal{H}\), but that the normalization to unit norm differs. If \(\| g_k \|_{L_2(d\mu)}=1\) for all \(k \geqslant 0\), then we have \( \| g_k \|^2_\mathcal{H}=  \lambda_k^{-1} \langle g_k, \Sigma g_k \rangle_\mathcal{H} = \lambda_k^{-1}\mathbb{E} [ g_k(x)^2] =\lambda_k^{-1}\) , and thus, \(\| \lambda_k^{1/2} g_k \|_{\mathcal{H}}=1\), and we have the kernel expansion from an orthonormal basis of \(\mathcal{H}\): $$K(x,y) = \sum_{k=0}^\infty\lambda_k g_k(x) g_k(y),$$ which will lead to a new proof for Mehler formula.</p>



<h2>Orthonormal basis for the Gaussian kernel</h2>



<p class="justify-text">Hermite polynomials naturally lead to orthonormal basis of some reproducing kernel Hilbert spaces (RKHS). For simplicity, I will focus on one-dimensional problems, but this extends to higher dimension. </p>



<p class="justify-text"><strong>Translation-invariant kernels.</strong> We consider a function \(q: \mathbb{R} \to \mathbb{R}\) which is integrable, with <a href="https://en.wikipedia.org/wiki/Fourier_transform">Fourier transform</a> (note the different normalization than before) which is defined for all \(\omega \in \mathbb{R}\) because of the integrability: $$\hat{q}(\omega) = \int_{\mathbb{R}} q(x) e^{-i \omega x} dx.$$ We consider the kernel $$K(x,y) = q(x-y).$$ It can be check that as soon as  \(\hat{q}(\omega) \in \mathbb{R}_+\)  for all \(\omega \in \mathbb{R}\), then the kernel \(K\) is positive-definite.</p>



<p class="justify-text">For a translation-invariant kernel, we can write using the inverse Fourier transform formula: $$K(x,y) = q(x-y) = \frac{1}{2\pi} \int_{\mathbb{R}} \hat{q}(\omega) e^{i \omega ( x- y)} d \omega = \int_{\mathbb{R}} \varphi_\omega(x)^* \varphi_\omega(y) d \omega,$$ with \(\varphi_\omega(x) = \sqrt{\hat{q}(\omega) / (2\pi) } e^{i\omega x}\). Intuitively, for a function \(f: \mathbb{R} \to \mathbb{R}\), with \(\displaystyle f(x) = \frac{1}{2\pi} \int_{\mathbb{R}} \hat{f}(\omega)e^{i\omega x} d\omega = \int_{\mathbb{R}} \frac{\hat{f}(\omega)  }{\sqrt{2 \pi \hat{q}(\omega)}}\varphi_\omega(x) d\omega\), which is a “dot-product” between the family \((\varphi_\omega(x))_\omega\) and \(\Big( \frac{\hat{f}(\omega)  }{\sqrt{2 \pi \hat{q}(\omega)}} \Big)_\omega\), the squared norm \(\| f\|_{\mathcal{H}}^2\) is equal to the corresponding “squared norm” of \(\Big( \frac{\hat{f}(\omega)  }{\sqrt{2 \pi \hat{q}(\omega)}}\Big)_\omega\), and we thus have $$ \| f\|_{\mathcal{H}}^2 = \int_{\mathbb{R}} \Big| \frac{\hat{f}(\omega)  }{\sqrt{2 \pi \hat{q}(\omega)}} \Big|^2 d\omega =  \frac{1}{2\pi} \int_{\mathbb{R}} \frac{ | \hat{f}(\omega) |^2}{\hat{q}(\omega)} d\omega,$$ where \(\hat{f}\) is the Fourier transform of \(f\). While the derivation above is not rigorous, the last expression is.</p>



<p class="justify-text">In this section, I will focus on the Gaussian kernel defined as \(K(x,y) = q(x-y) =  \exp \big( – \alpha ( x- y )^2 \big)\), for which \(\displaystyle \hat{q}(\omega)= \sqrt{\frac{\pi}{\alpha}} \exp\big( – \frac{\omega^2}{4 \alpha} \big)\).</p>



<p class="justify-text">Given that \(\displaystyle \frac{1}{\hat{q}(\omega)} = \sqrt{\frac{\alpha}{\pi}} \exp\big(  \frac{\omega^2}{4 \alpha} \big)= \sqrt{\frac{\alpha}{\pi}} \sum_{k=0}^\infty  \frac{\omega^{2k}}{(4 \alpha)^k k!} \), the penalty \(\|f\|_\mathcal{H}^2\) is a linear combination of squared \(L_2\)-norm of \(\omega^k \hat{f}(\omega)\), which is the squared \(L_2\)-norm of the \(k\)-th derivative of \(f\). Thus, functions in the RKHS are infinitely differentiable, and thus very smooth (this implies that to have the fast rate \((\log n) / n \) above, the optimal regression function has to be very smooth).</p>



<p class="justify-text"><strong>Orthonormal basis of the RKHS</strong>. As seen in Eq. (4), an expansion in an infinite sum is necessary to obtain an orthonormal basis. We have: $$K(x,y) = e^{-\alpha x^2} e^{-\alpha y^2} e^{2 \alpha x y} = e^{-\alpha x^2} e^{-\alpha y^2} \sum_{k=0}^\infty \frac{ (2\alpha)^k}{k!} x^k y^k.$$ Because of Eq. (4), with \(g_k(x) = \sqrt{ \frac{(2\alpha)^k}{k!}} x^k \exp \big( – \alpha x^2 \big)\), we have a good candidate for an orthonornal basis. Let us check that this is the case. Note that the expansion above alone cannot be used as a proof that \((g_k)\) is an orthonormal basis of \(\mathcal{H}\).</p>



<p class="justify-text">Given the function \(f_k(x) = x^k \exp \big( – \alpha x^2 \big)\), we can compute its Fourier transform as $$ \hat{f}_k(\omega) = i^{-k} ( 4 \alpha)^{-k/2} \sqrt{\frac{\pi}{\alpha}} H_k \Big( \frac{\omega}{\sqrt{4 \alpha}} \Big) \exp\big( – \frac{\omega^2}{4 \alpha} \big) .$$ Indeed, we have, from Rodrigues’ formula, $$H_k \Big( \frac{\omega}{\sqrt{4 \alpha}} \Big) \exp\big( – \frac{\omega^2}{4 \alpha} \big) =(-1)^k (4 \alpha)^{k/2} \frac{d^k}{d \omega^k}\big[ \exp\big( – \frac{\omega^2}{4 \alpha} \big) \big],$$ and thus its inverse Fourier transform is equal to \((ix)^k\) times the one of \((-1)^k (4 \alpha)^{k/2} \exp\big( – \frac{\omega^2}{4 \alpha} \big)\), which is thus equal to \((-i)^k (4 \alpha)^{k/2} \sqrt{ \alpha / \pi }e^{-\alpha x^2} \), which leads to the Fourier transform formula above.</p>



<p class="justify-text">We can now compute the RKHS dot products, to show how to obtain the orthonormal basis described in [11]. This leads to $$ \langle f_k, f_\ell \rangle = \frac{1}{2\pi} \sqrt{\frac{\pi}{\alpha}} ( 4 \alpha)^{-k/2}( 4 \alpha)^{-\ell/2}  \int_{\mathbb{R}} H_k \Big( \frac{\omega}{\sqrt{4 \alpha}} \Big) H_\ell \Big( \frac{\omega}{\sqrt{4 \alpha}} \Big) \exp\big( – \frac{\omega^2}{4 \alpha} \big)  d\omega,$$ which leads to, with a change of variable $$ \langle f_k, f_\ell \rangle = \frac{1}{2\pi} \sqrt{\frac{\pi}{\alpha}} ( 4 \alpha)^{-k/2}( 4 \alpha)^{-\ell/2} \sqrt{4 \alpha} \int_{\mathbb{R}} H_k (u) H_\ell (u)  \exp(-u^2) du,$$ which is equal to zero if \(k \neq \ell\), and equal to \(\frac{1}{2\pi} \sqrt{\frac{\pi}{\alpha}} ( 4 \alpha)^{-k} \sqrt{4 \alpha} \sqrt{\pi} 2^k k!   = ( 2 \alpha)^{-k} k!\) if \(k = \ell\). Thus the sequence \((f_k)\) is an orthogonal basis of the RKHS, and the sequence \((g_k)\) defined as \(g_k(x) = \sqrt{ \frac{(2\alpha)^k}{k!}} f_k(x)\) is an orthonormal basis of the RKHS, from which, using the expansion as in Eq. (4), we recover the expansion: $$K(x,y) = \sum_{k=0}^\infty g_k(x) g_k(y) = e^{-\alpha x^2} e^{-\alpha y^2} \sum_{k=0}^\infty \frac{ (2\alpha)^k}{k!} x^k y^k.$$</p>



<p class="justify-text">This expansion can be used to approximate the Gaussian kernel by finite-dimensional explicit feature spaces, by just keeping the first basis elements (see an application to optimal transport in [<a href="https://arxiv.org/pdf/1810.10046">12</a>], with an improved behavior using an adaptive low-rank approximation through the Nyström method in [<a href="https://papers.nips.cc/paper/8693-massively-scalable-sinkhorn-distances-via-the-nystrom-method.pdf">13</a>]).</p>



<h2>Eigenfunctions for the Gaussian kernels</h2>



<p class="justify-text">In order to obtain explicit formulas for the eigenvalues of the covariance operator, we need more than a mere orthonormal basis, namely an eigenbasis.</p>



<p class="justify-text">An orthogonal basis will now be constructed with arguably better properties as it is also an orthonormal basis for both the RKHS and \(L_2(d\mu)\) for a Gaussian measure, that diagonalizes the integral operator associated to this probability measure, as well as the covariance operator.</p>



<p class="justify-text">As seen above, we simply need an orthogonal family \((f_k)_{k \geqslant 0}\), such that given a distribution \(d\mu\),  \((f_k)_{k \geqslant 0}\) is a family in \(L_2(d\mu)\) such that $$\int_{\mathbb{R}} f_k(x) K(x,y) d\mu(x) = \lambda_k f_k(y), \tag{5}$$ for eigenvalues \((\lambda_k)\). In the next paragraph, we will do exactly this for the Gaussian kernel \(K(x,y) = e^{-\alpha (x-y)^2}\) for \(\alpha = \frac{\rho}{1- \rho^2}\) for some \(\rho \in (0,1)\); this particular parameterization in \(\rho\) is to make the formulas below not (too) complicated.</p>



<p class="justify-text">With \(f_k(x) = \frac{1}{\sqrt{N_k}} H_k(x) \exp \Big( – \frac{\rho}{1+\rho} x^2 \Big)\), where \(N_k = {2^k k!} \sqrt{ \frac{1-\rho}{1+\rho}}\), then \((f_k)_{k \geqslant 0}\) is an <em>orthonormal</em> basis for \(L_2(d\mu)\) for \(d\mu\) the Gaussian distribution with mean zero and variance \(\frac{1}{2} \frac{1+\rho}{1-\rho}\) (this is a direct consequence of the orthogonality property of Hermite polynomials).</p>



<p class="justify-text">Moreover, the moment of the Hermite polynomial in Eq. (3) exactly leads to Eq. (5) for the chosen kernel and \(\lambda_k = (1-\rho) \rho^k\). Since the eigenvalues sum to one, and the trace of \(\Sigma\) is equal to one (as a consequence of \(K(x,x)=1\)), the family \((f_k)\) has to be a basis of \(\mathcal{H}\).</p>



<p>From properties of the eigenbasis, since \((f_k)\) is an orthonormal eigenbasis of \(L_2(d\mu)\) and the eigenvalues are \(\lambda_k = (1-\rho)\rho^k\),  we get: $$ K(x,y) = \exp \Big( – \frac{\rho}{1- \rho^2} (x-y)^2\Big) = \sum_{k=0}^\infty (1-\rho)\rho^k f_k(x) f_k(y),$$ which is exactly the Mehler formula, and thus we obtain an alternative proof.</p>



<p class="justify-text">We then get an explicit basis and the exponential decay of eigenvalues, which was first outlined by [2]. See an application to the estimation of the Poincaré constant in [<a href="https://hal.archives-ouvertes.fr/hal-02327453v1/document">14</a>] (probably a topic for another post in a few months).</p>



<p class="justify-text"><strong>Experiments.</strong> In order to showcase the exact eigenvalues of the expectation \(\Sigma\) (for the correct combination of Gaussian kernel and Gaussian distribution), we compare the eigenvalues with the ones of the empirical covariance operator \(\hat\Sigma\), for various values of the number of observations. We see that as \(n\) increases, the empirical eigenvalues match the exact ones for higher \(k\).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-4889" height="307" src="https://francisbach.com/wp-content/uploads/2020/10/gaussian_kernel-1.gif" width="363"/>Eigenvalues of the covariance operator \(\Sigma\) (“expectation”) compared to the ones of the empirical covariance operator \(\hat\Sigma\), averaged over 20 replications (“empirical”), for several values of \(n\).</figure></div>



<h2>Conclusion</h2>



<p class="justify-text">In this post, I only presented applications of Hermite polynomials to the Gaussian kernel, but these polynomials appear in many other areas of applied mathematics, for other types of kernels within machine learning such as dot-product kernels [3], in random matrix theory (see <a href="https://terrytao.wordpress.com/2011/02/20/topics-in-random-matrix-theory/">here</a>), in statistics for <a href="https://en.wikipedia.org/wiki/Edgeworth_series">Edgeworth expansions</a>, and of course for <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Hermite_quadrature">Gauss-Hermite quadrature</a>.</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Loucas Pillaud-Vivien and Alessandro Rudi for proofreading this blog post and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] George Neville Watson. <a href="https://academic.oup.com/jlms/article-pdf/s1-8/3/194/2363185/s1-8-3-194.pdf">Notes on Generating Functions of Polynomials: (2) Hermite Polynomials</a>. <em>Journal of the London Mathematical Soc</em>iety, 8, 194-199, 1933.<br/>[2] Huaiyu Zhu, Christopher K. I. Williams, Richard Rohwer, and Michal Morciniec. <a href="https://publications.aston.ac.uk/id/eprint/38366/1/NCRG_97_011.pdf">Gaussian regression and optimal finite dimensional linear models</a>. In <em>Neural Networks and Machine Learning</em>. Springer-Verlag, 1998.<br/>[3] A. Daniely, R. Frostig, and Y. Singer. <a href="https://papers.nips.cc/paper/6427-toward-deeper-understanding-of-neural-networks-the-power-of-initialization-and-a-dual-view-on-expressivity.pdf">Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity</a>. In Advances In Neural Information Processing Systems, 2016.<br/>[4] Gabor Szegö. <em>Orthogonal polynomials</em>. American Mathematical Society, 1939.<br/>[5] Grace Wahba. <a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970128">Spline models for observational data</a>. Society for Industrial and Applied Mathematics, 1990.<br/>[6] Bernhard Schölkopf, Alexander J. Smola. <a href="https://mitpress.mit.edu/books/learning-kernels">Learning with kernels: support vector machines, regularization, optimization, and beyond</a>. MIT Press, 2002.<br/>[7] John Shawe-Taylor, Nello Cristianini. <em>Kernel methods for pattern analysis</em>. Cambridge University Press, 2004.<br/>[8] Andrea Caponnetto, Ernesto De Vito. <a href="https://link.springer.com/content/pdf/10.1007/s10208-006-0196-8.pdf">Optimal rates for the regularized least-squares algorithm</a>. Foundations of Computational Mathematics 7.3: 331-368, 2007.<br/>[9] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. <a href="http://web.stanford.edu/~hastie/Papers/ESLII.pdf">The elements of statistical learning</a>. Vol. 1. No. 10. Springer series in statistics, 2001.<br/>[10] Trevor Hastie and Robert Tibshirani. <em>Generalized Additive Models</em>. Chapman &amp; Hall, 1990.<br/>[11] Ingo Steinwart, Don Hush, and Clint Scovel. <a href="http://[PDF] ieee.org">An explicit description of the reproducing kernel Hilbert spaces of Gaussian RBF kernels</a>. <em>IEEE Transactions on Information Theory</em>, 52.10:4635-4643, 2006.<br/>[12] Jason Altschuler, Francis Bach, Alessandro Rudi, Jonathan Niles-Weed. <a href="https://arxiv.org/pdf/1810.10046">Approximating the quadratic transportation metric in near-linear time</a>. Technical report arXiv:1810.10046, 2018.<br/>[13] Jason Altschuler, Francis Bach, Alessandro Rudi, Jonathan Niles-Weed. <a href="https://papers.nips.cc/paper/8693-massively-scalable-sinkhorn-distances-via-the-nystrom-method.pdf">Massively scalable Sinkhorn distances via the Nyström method</a>. <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2019.<br/>[14] Loucas Pillaud-Vivien, Francis Bach, Tony Lelièvre, Alessandro Rudi, Gabriel Stoltz. <a href="https://hal.archives-ouvertes.fr/hal-02327453v1/document">Statistical Estimation of the Poincaré constant and Application to Sampling Multimodal Distributions</a>. <em>Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS),</em> 2020.<br/>[15] Nachman Aronszajn. <a href="https://www.ams.org/journals/tran/1950-068-03/S0002-9947-1950-0051437-7/S0002-9947-1950-0051437-7.pdf">Theory of Reproducing Kernels</a>. <em>Transactions of the American Mathematical Society</em>, 68(3): 337–404, 1950.</p>



<h2>Proof of properties of Hermite polynomials</h2>



<p class="justify-text">In this small appendix, I give “simple” proofs (that sometimes require knowledge of <a href="https://en.wikipedia.org/wiki/Complex_analysis">complex analysis</a>) to the properties presented above.</p>



<p class="justify-text"><strong>Generating function.</strong> We have, using <a href="https://en.wikipedia.org/wiki/Residue_theorem">residue theory</a>, $$H_k(x)=(-1)^k e^{x^2} \frac{d^k}{d x^k}\big[ e^{-x^2} \big] = (-1)^k \frac{k!}{2i\pi} e^{x^2} \oint_\gamma \frac{e^{-z^2}}{(z-x)^{k+1}}dz, $$ where \(\gamma\) is a contour in the complex plane around \(x\). This leads to, for any \(t\) (here, we ignore on purpose the summability issues, for more details, see [4, Section 5.5]): $$ \sum_{k=0}^\infty \frac{t^k}{k!} H_k(x) =  \frac{1}{2i\pi} e^{x^2} \oint_\gamma \frac{e^{-z^2}}{z-x} \sum_{k=0}^\infty \frac{t^k} {(x-z)^{k}}dz, $$ which can be simplified using the sum of the geometric series, leading to $$\frac{1}{2i\pi} e^{x^2} \oint_\gamma \frac{e^{-z^2}}{z-x} \frac{z-x}{z-x- t} dz =  \frac{1}{2i\pi} e^{x^2} \oint_\gamma \frac{e^{-z^2}} {z-x- t} dz.$$ Using the first-order residue at \(x+t\). This is thus equal to \(e^{x^2-(t+x)^2} = e^{-t^2 + 2tx}\), which is exactly the generating function statement from Eq. (2).</p>



<p class="justify-text"><strong>Orthogonality for Gaussian distribution.</strong> We can prove through integration by parts, but there is a nicer proof through the generating function. Indeed, with $$ a_{k \ell} = \int_{-\infty}^{+\infty} e^{-x^2} H_k(x) H_\ell(x) dx, $$ for \(k, \ell \geqslant 0\), we get $$\sum_{k,\ell = 0}^\infty a_{k \ell} \frac{t^k u^\ell}{k! \ell!} = \int_{-\infty}^{+\infty}e^{-x^2}\Big(  \sum_{k,\ell = 0}^\infty a_{k \ell} \frac{t^k u^\ell}{k! \ell!}  H_k(x) H_\ell(x) \Big) dx.$$ Using the generating function, this leads to $$\sum_{k,\ell = 0}^\infty a_{k \ell} \frac{t^k u^\ell}{k! \ell!} =  \int_{-\infty}^{+\infty} e^{-x^2 + 2xu-u^2 + 2xt – t^2} dx= e^{2uv} \int_{-\infty}^{+\infty} e^{-(x-u-v)^2}dx, $$ which can be computed explicitly using normalization constants of the Gaussian distribution, as \( \sqrt{\pi} e^{2uv} = \sqrt{\pi} \sum_{k=0}^\infty \frac{ (2  u v)^k}{k!},\) leading to all desired orthogonality relationships using the uniqueness of all coefficients for factors \(t^k u^\ell\).</p>



<p class="justify-text"><strong>Recurrence relationship.</strong> Taking the derivative of the generating function with respect to \(t\), one gets \( \displaystyle (2x-2t) e^{2tx-t^2} = \sum_{k=0}^\infty \frac{t^{k-1}}{(k-1)!} H_k(x),\) which is equal to (using again the generating function) \(\displaystyle \sum_{k=0}^\infty \frac{t^{k}}{k!} 2x H_k(x) \ – \sum_{n=0}^\infty \frac{t^{k+1}}{k!} 2 H_k(x).\) By equating the coefficients for all powers of \(t\), this leads to the desired recursion in Eq. (1).</p>



<p class="justify-text"><strong>Fourier transform.</strong> Again using the generating function, written $$ e^{-x^2/2 + 2xt – t^2} = \sum_{k=0}^\infty \frac{t^k}{k!} e^{-x^2/2} H_k(x), $$ we can take Fourier transforms and use the fact that the Fourier transform of \(e^{-x^2/2}\) is itself (for the chosen normalization), and then equate coefficients for all powers of \(t\) to conclude (see more details <a href="https://en.wikipedia.org/wiki/Hermite_polynomials#Hermite_functions_as_eigenfunctions_of_the_Fourier_transform">here</a>).</p>



<p class="justify-text"><strong>Expectation for Gaussian distributions.</strong> We finish the appendix by proving Eq. (3). We consider computing for any \(t\), $$\sum_{k=0}^\infty \rho^k \frac{t^k}{k!} H_k (y) = e^{2\rho t y – \rho^2 t^2},$$ using the generating function from Eq. (2). We then compute $$A=\int_{-\infty}^\infty \exp\Big( – \frac{(x-\rho y)^2}{1-\rho^2} \Big) \sum_{k=0}^\infty \frac{t^k}{k!} H_k(x) dx = \int_{-\infty}^\infty \exp\Big( – \frac{(x-\rho y)^2}{1-\rho^2} \Big) \exp( 2tx – t^2) dx.$$ We then use \( \frac{(x-\rho y)^2}{1-\rho^2} – 2tx + t^2 = \frac{x^2}{1-\rho^2}  – \frac{2x[ t(1-\rho^2) + \rho y]}{1-\rho^2}  + t^2 + \frac{\rho^2 y^2}{1-\rho^2}\), leading to $$A = \sqrt{\pi} \sqrt{1-\rho^2} \exp\Big( -t^2 – \frac{\rho^2 y^2}{1-\rho^2} +(1-\rho^2) \big( t + \frac{\rho y}{1-\rho^2} \big)^2 \Big) = \sqrt{\pi} \sqrt{1-\rho^2} e^{2\rho t y – \rho^2 t^2}.$$ By equating powers of \(t\), this leads to Eq. (3).</p></div>
    </content>
    <updated>2020-10-08T19:33:36Z</updated>
    <published>2020-10-08T19:33:36Z</published>
    <category term="Tools"/>
    <author>
      <name>Francis Bach</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2020-10-20T03:22:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/151</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/151" rel="alternate" type="text/html"/>
    <title>TR20-151 |  Pseudobinomiality of the Sticky Random Walk | 

	Venkatesan Guruswami, 

	Vinayak Kumar</title>
    <summary>Random walks on expanders are a central and versatile tool in pseudorandomness.  If an arbitrary half of the vertices of an expander graph are marked, known Chernoff bounds for expander walks imply that the number $M$ of marked vertices visited in a long $n$-step random walk strongly concentrates around the expected $n/2$ value. Surprisingly, it was recently shown that the parity of $M$ also has exponentially small bias.

Is there a common unification of these results? What other statistics about $M$ resemble the binomial distribution (the Hamming weight of a random $n$-bit string)? To gain insight into such questions, we analyze a simpler model called the sticky random walk. This model is a natural stepping stone towards understanding expander random walks, and we also show that it is a necessary step. The sticky random walk starts with a random bit and then each subsequent bit independently equals the previous bit with probability $(1+\lambda)/2$. Here $\lambda$ is the proxy for the expander's (second largest) eigenvalue.
    
Using Krawtchouk expansion of functions, we derive several probabilistic results about the sticky random walk. We show an asymptotically tight $\Theta(\lambda)$ bound on the total variation distance between the (Hamming weight of the) sticky walk and the binomial distribution. We prove that the correlation between the majority and parity bit of the sticky walk is bounded by $O(n^{-1/4})$. This lends hope to unifying Chernoff bounds and parity concentration, as well as establishing other interesting statistical properties, of expander random walks.</summary>
    <updated>2020-10-08T14:03:25Z</updated>
    <published>2020-10-08T14:03:25Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-10-20T03:20:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/150</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/150" rel="alternate" type="text/html"/>
    <title>TR20-150 |  Almost-Everywhere Circuit Lower Bounds from Non-Trivial Derandomization | 

	Lijie Chen, 

	Xin Lyu, 

	Ryan Williams</title>
    <summary>In certain complexity-theoretic settings, it is notoriously difficult to prove complexity separations which hold almost everywhere, i.e., for all but finitely many input lengths. For example, a classical open question is whether $\mathrm{NEXP} \subset \mathrm{i.o.-}\mathrm{NP}$; that is, it is open whether nondeterministic exponential time computations can be simulated on infinitely many input lengths by $\mathrm{NP}$ algorithms. This difficulty also applies to Williams' algorithmic method for circuit lower bounds [Williams, J. ACM 2014]. In particular, although [Murray and Williams, STOC 2018] proved $\mathrm{NTIME}[2^{\mathrm{polylog}(n)}] \not\subset \mathrm{ACC}^0$, it has remained an open problem to show that $\mathrm{E}^{\mathrm{NP}}$ ($2^{O(n)}$ time with an $\mathrm{NP}$ oracle) is not contained in $\mathrm{i.o.-}\mathrm{ACC}^0$. 
	
In this paper, we show how many infinitely-often circuit lower bounds proved by the algorithmic method can be adapted to establish almost-everywhere lower bounds. 

- We show there is a function $f \in \mathrm{E}^{\mathrm{NP}}$ such that for all sufficiently large input lengths $n$ and $\varepsilon \leq o(1)$, $f$ cannot be $(1/2+2^{-n^{\varepsilon}})$-approximated by $2^{n^\varepsilon}$-size $\mathrm{ACC}^0$ circuits on inputs of length $n$, improving lower bounds in [Chen and Ren, STOC 2020] and [Viola, ECCC 2020]. 

- We construct rigid matrices in $\mathrm{P}^{\mathrm{NP}}$ for all but finitely many inputs, rather than infinitely often as in [Alman and Chen, FOCS 2019] and [Bhangale et al., FOCS 2020]. 

- We show there are functions in $\mathrm{E}^{\mathrm{NP}}$ requiring constant-error probabilistic degree at least $\Omega(n/\log^2 n)$ for all large enough $n$, improving an infinitely-often separation of [Viola, ECCC 2020].
	
Our key to proving almost-everywhere worst-case lower bounds is a new ``constructive'' proof of an NTIME hierarchy theorem proved by [Fortnow and Santhanam, CCC 2016], where we show for every ``weak'' nondeterminstic algorithm (with smaller running-time and short witness), a ``refuter algorithm'' exists that can construct ``bad'' inputs for the hard language. We use this refuter algorithm to construct an almost-everywhere hard function. To extend our lower bounds to the average case, we prove a new XOR Lemma based on approximate linear sums, and combine it with the PCP-of-proximity applications developed in [Chen and Williams, CCC 2019] and [Chen and Ren, STOC 2020]. As a byproduct of our new XOR Lemma, we obtain a nondeterministic pseudorandom generator for poly-size $\mathrm{ACC}^0$ circuits with seed length $\mathrm{polylog}(n)$, which resolves an open question in [Chen and Ren, STOC 2020].</summary>
    <updated>2020-10-08T13:53:53Z</updated>
    <published>2020-10-08T13:53:53Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-10-20T03:20:30Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-7346788885933681031</id>
    <link href="https://blog.computationalcomplexity.org/feeds/7346788885933681031/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/revisiting-continuum-hypothesis.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7346788885933681031" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7346788885933681031" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/10/revisiting-continuum-hypothesis.html" rel="alternate" type="text/html"/>
    <title>Revisiting the Continuum Hypothesis</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I have been thinking about CH lately for two reasons</p><p>1) I reread the article</p><p>Hilbert's First Problem: The Continuum Hypothesis by Donald Martin from <b>Proceedings of Symposia </b> i<b>n Pure Mathematics: Mathematical developments arising from Hilbert Problems</b>. 1976. (For a book review of the symposia and, <b>The Honor Class</b>, also about Hilbert's problems, see <a href="https://www.cs.umd.edu/users/gasarch/bookrev/44-4.pdf">here</a>.)</p><p>The article takes the point of view that CH CAN have an answer. He discusses large cardinals (why assuming they exist is plausible, but alas, that assumption does not seem to resolve CH) and Projective Det.  (why assuming it is true is plausible, but alas, that assumption does not seem to resolve CH).</p><p>(A set A \subseteq {0,1}^omega is DETERMINED if either Alice or Bob has a winning strategy in the following non-fun game: they alternate picking bits a_1, b_1, a_2, b_2, ... with Alice going first. If a_1 b_1 a_2 b_2... IS IN A then Alice wins, IF NOT then Bob wins. Martin showed that all Borel sets are determined. Proj Det is the statement that all projections of Borel sets are determined. AD is the axiom that ALL sets A are determined. It contradicts AC.)</p><p>But what really inspired this post is the last paragraph:</p><p><i>Throughout the latter part of my discussion, I have been assuming a naive and uncritical attitude towards CH. While this <b>is</b> in fact my attitude, I by no means wish to dismiss the opposite viewpoint.  Those that argue that the concept of set is not sufficiently clear to fix the truth-value of CH have a position that is at present difficult to assail. As long as no new axiom is found which decides CH, their case will continue to grow stronger, and our assertions that the meaning of CH is clear will sound more and more empty.</i></p><p>2) Scott Aaronson mentioned in a blog post (see <a href="https://www.scottaaronson.com/blog/?p=4962">here</a>) that  he has read and understood the proof that CH is independent of set theory.</p><p>SO, this seemed like a good time to revisit thoughts on CH.</p><p> I took a very short poll, just two people, about CH: Stephen Fenner (in a perfect world he would be a set theorists) and Scott Aaronson (having JUST read the proof that CH is ind.  he has thought about it recently).</p><p>Here are some thoughts of theirs and mine</p><p>1) All three of us are Platonists with regard to the Naturals (I was surprised to find recently that there are people who are not!) but not with regard to the reals.  So we would be OKAY with having CH have no answer.</p><p>2) All three of us  agree that it would be nice if SOME axiom was both</p><p>a) Intuitively appealing or aesthetically appealing ,  and</p><p>b) resolved CH.</p><p>I always thought that (a) would be the hard part-- or at least getting everyone (not sure who we are talking about) to AGREE on a new axiom. But even getting an axiom to resolve CH seems hard.  Large cardinals don't seem to do it, and various forms of Determinacy don't seem to do it.</p><p>Scott reminded me of Freiling's Axiom of Symmetry (see <a href="https://en.wikipedia.org/wiki/Freiling%27s_axiom_of_symmetry">here</a>) which IS intuitive and DOES resolve CH (its false) though there are problems with it--- a minor variant   of it contradicts AC (I am QUITE FINE with that since AC implies Banach-Tarski which Darling says shows `Math is broken'.)</p><p>Stephen recalled some of Hugh Woodin's opinions of CH, but Hugh seems to have changed his mind from NOT(CH): 2^{aleph_0} = aleph_2, to CH:  2^{aleph_0} = aleph_1.(See <a href="https://www.jstor.org/stable/24569622?seq=1#metadata_info_tab_contents">here</a>.)</p><p>3) All three of would be okay with V=L, though note that this would put many set theorists out of work. All the math that applies to the real world would still be intact.  I wonder if in an alternative history the reaction to Russell's paradox would be a formulation of set theory where V=L. We would KNOW that CH is true, KNOW that AC is true. We would know a lot about L but less about forcing.</p><p>4) Which Geometry is true: Euclidian, Riemannian, others? This is now regarded as a silly question: Right Tool, Right Job! If you build a bridge use Euclid. If you are doing astronomy use Riemann. Might Set Theory go the same way? It would be AWESOME if Scott Aaronson found some quantum thing where assuming 2^{aleph_0} = aleph_2 was the right way to model it.</p><p>5) If I was more plugged into the set theory community I might do a poll of set theorists, about CH. Actually, someone sort-of already has. Penelope Maddy has two excellent and readable articles where she studies what set theorists believe and why.</p><p><b>Believing The Axioms I</b>: <a href="http://www.cs.umd.edu/~gasarch/BLOGPAPERS/belaxioms1.pdf">here</a></p><p><b>Believing The Axioms II</b>: <a href="http://www.cs.umd.edu/~gasarch/BLOGPAPERS/belaxioms2.pdf">here</a></p><p>Those articles were written in 1988. I wonder if they need an update.</p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2020-10-08T13:52:00Z</updated>
    <published>2020-10-08T13:52:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-10-20T02:07:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://ptreview.sublinear.info/?p=1420</id>
    <link href="https://ptreview.sublinear.info/?p=1420" rel="alternate" type="text/html"/>
    <title>News for September 2020</title>
    <summary>Apologies dear readers for the late posting. The beginning of the school year is always frenzied, and the pandemic has only added to that frenzy. We have an exciting September, with four papers on graph property testing, one two papers on distribution testing, and one paper that connects both topics. (Ed: we normally scan through […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Apologies dear readers for the late posting. The beginning of the school year is always frenzied, and the pandemic has only added to that frenzy. We have an exciting September, with four papers on graph property testing, <s>one</s> two papers on distribution testing, and one paper that connects both topics.</p>



<p><em>(Ed: we normally scan through ECCC and arXiv, but are happy to post about papers that appear elsewhere. Thanks to the reader who pointed out a relevant COLT 2020 paper.)</em></p>



<p><strong>Estimation of Graph Isomorphism Distance in the Query World</strong> by Sourav Chakraborty, Arijit Ghosh, Gopinath Mishra, and Sayantan Sen (<a href="https://eccc.weizmann.ac.il/report/2020/135/">ECCC</a>). Graph isomorphism is about as fundamental as it gets, and this papers studies approximating the graph isomorphism distance for dense graphs. There is a known graph \(G_k\) (with \(n\) vertices). The algorithm is given query access to an input graph \(G_u\) and needs to approximate the number of edge inserts/deletes required to make the graphs isomorphic. This is the tolerant testing version; the property testing version is known to be doable in \(\widetilde{O}(\sqrt{n})\) queries (<a href="https://epubs.siam.org/doi/abs/10.1137/070680795?journalCode=smjcat">Matsliah-Fischer</a>). The main insight of this paper is to relate the tolerant testing complexity to a distribution testing problem. Consider distributions over the \(\{0,1\}^n\) defined by multisets of \(n\) hypercube points. Our aim is to estimate the earthmover distance between a known distribution and an unknown distribution. Interestingly, the query model is different: one can sample the underlying multisets <em>without</em> replacement. It turns out that the optimal complexity of this problem is (upto polylog factors) is the same as the optimal complexity of tolerant testing of graph isomorphism. A direct corollary is that the isomorphism distance can be approximated upto additive \(\epsilon n^2\) using \(\widetilde{O}(n)\) samples. This equivalence also gives an alternate proof for lower bounds for property testing graph isomorphism.</p>



<p><strong>Robustly Self-Ordered Graphs: Constructions and Applications to Property Testing </strong>by Oded Goldreich and Avi Wigderson (<a href="https://eccc.weizmann.ac.il/report/2020/149/">ECCC</a>). Let’s start from the application. The aim is to prove the following property testing lower bounds for the bounded-degree graph setting: an exponential separation between tolerant and vanilla testing, and finding an efficiently decidable property (in polynomial time) that cannot be property tested in sublinear time. For binary strings, results of this form are known. Can these be “ported” to the bounded-degree graph world? Can we construct graphs such that adjacency queries reduce to bit queries in strings? Naturally, one can simply represent the adjacency list as a string and treat graph queries as bit queries. But the problem is that of isomorphisms: different bit strings could represent the same graph and therefore, the different bit strings must have the same status with respect to the underlying property. The key insight in this paper is to introduce <em>robustly self-ordered graph</em>s, as a tool to port bit string property testing lower bounds to bounded-degree graphs. Such graphs essentially have a unique (identity) automorphism, even after a few edge insert/deletes. The actual definition is more technical, but that is the essence. The main result is an explicit construction of such graphs, from which the lower bound can be ported directly through a convenient lemma.</p>



<p><strong>Modifying a Graph’s Degree Sequence and the Testablity of Degree Sequence Properties </strong>by Lior Gishboliner (<a href="https://arxiv.org/pdf/2009.12697.pdf">arXiv</a>). A sequence of numbers \(D = (d_1, d_2, \ldots, d_n)\) is graphic if there exists an undirected graph on \(n\) vertices whose degrees are precisely the numbers of the sequence. Graphical sequences have been characterized by classic results of <a href="https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93Gallai_theorem">Erdös-Gállai</a> and <a href="https://en.wikipedia.org/wiki/Havel%E2%80%93Hakimi_algorithm">Havel-Hakimi</a>. This paper first proves the following theorem. Suppose a graphic sequence \(D’\) has \(l_1\)-distance at most \(\delta\) from the degree sequence \(D\) of a graph \(G\). Then, there exists a graph \(G’\) with degree sequence \(D’\) such that the (dense graph) distance between \(G\) and \(G’\) is \(O(\sqrt{\delta})\). This theorem is used to prove an interesting property testing result. Let \(\mathcal{D}\) be a subset of graphic sequences that are closed under permutation. Let \(\mathcal{G}\) be the set of graphs that have a degree sequence in \(\mathcal{D}\). Then \(\mathcal{G}\) can be tested in \(poly(1/\epsilon)\) queries.</p>



<p><strong>Sampling an Edge Uniformly in Sublinear Time</strong> by Jakub Têtek (<a href="https://arxiv.org/pdf/2009.11178.pdf">arXiv</a>). In the general model for sublinear algorithms on graphs, an important choice is whether one allows uniform random edge queries. A natural question is whether such queries can simulated efficiently, using only random vertex, degree, and neighbor queries. This problem appears somewhat implicitly in previous sublinear subgraph counting algorithms, and <a href="http://drops.dagstuhl.de/opus/volltexte/2019/10628/pdf/LIPIcs-ICALP-2019-52.pdf">Eden-Ron-Rosenbaum</a> study it explicitly. They prove that one can sample from an \(\epsilon\)-approximate uniform distribution (over edges) using \(O(n/\sqrt{\epsilon m})\) samples. The problem of sampling from exactly the uniform distribution is left open. Until this paper. The main result shows that by modifying the Eden-Ron-Rosenbaum algorithm parameters, one can generate edge samples from an \(\epsilon\)-approximate uniform distribution using \(O((n/\sqrt{m})\log \epsilon^{-1})\) samples. The exact uniform distribution is achieved by setting \(\epsilon = 1/n\), to get a sample complexity of \(O((n\log n)/\sqrt{m})\).</p>



<p><strong>Faster Property Testers in a Variation of the Bounded Degree Model </strong>by Isolde Adler and Polly Fahey (<a href="https://arxiv.org/pdf/2009.07770.pdf">arXiv</a>). The setting of bounded-degree graph property testing naturally extends to bounded-degree relational databases, which can be thought of as “directed” hypergraphs. This is an interesting new direction of research, that combines property testing with database theory (see <a href="https://core.ac.uk/download/pdf/157699299.pdf">Adler-Harwath</a> and <a href="https://dl.acm.org/doi/10.1145/3294052.3319679">Chen-Yoshida</a>). One of the main contributions of this work is to consider another notion of distance: edge and vertex inserts/deletes. This is a natural extension, and we can now compare distances between graphs/databases with different numbers of vertices. The main result is that, under this notion of distance, a large class of properties can be tested in constant running time on databases with bounded degree and treewidth. Specifically, any property expressible in Counting Monadic Second-Order Logic (CMSO) can be tested in constant time. Previous results by Alder-Harwath showed that such properties can be tested (under the standard distance notion) in constant queries, but polylogarithmic time. </p>



<p><strong>Optimal Testing of Discrete Distributions with High Probability</strong> by Ilias Diakonikolas, Themis Gouleakis, Daniel M. Kane, John Peebles, and Eric Price (<a href="https://arxiv.org/abs/2009.06540">arXiv</a>, <a href="https://eccc.weizmann.ac.il/report/2020/140/">ECCC</a>). The focus of this paper is distribution testing in the “high probability” regime, where we wish the error of the tester to be \(&lt; \delta\). Typically, most results just get an error of at most \(1/3\), from which standard probabilistic boosting would tack on an extra \(O(\log 1/\delta)\) factor. In standard TCS settings, one doesn’t focus on optimizing this dependence, but in statistics, there is significant focus on the optimal sample complexity. And indeed, for practical applications, it is crucial to have sharp bounds on the right number of samples required for hypothesis testing. The paper also argues that getting the optimal sample complexity requires new algorithms, even for uniformity testing. There are optimal results given for closeness and independence testing. The optimal sample complexity only pays a multiplicative factor of \(\log^{1/3} (1/\delta)\) or \(\log^{1/2}(1/\delta)\) over the optimal bound for constant error (with other additive terms depending on \(\log(1/\delta)\)).</p>



<p><strong>Bessel Smoothing and Multi-Distribution Property Estimation</strong> by Yi Hao and Ping Li (<a href="http://proceedings.mlr.press/v125/hao20a.html">COLT 2020</a>). Let us consider some standard (tolerant) distribution testing questions, phrases as approximation algorithms. Given sample access to two distributions \(p\) and \(q\) over \([n]\), we may wish to estimate the \(l_1\)-distance, \(l_2\)-distance, relative entropy, etc. between these distributions. One can phrases this problem abstractly as estimating \(\sum_{i \in [n]} f(p_i, q_i)\), where \(f\) is some explicit function. This papers shows that for any 1-Lipschitz function \(f\) that satisfies some “regularity” property, the sum \(\sum_{i \in [n]} f(p_i, q_i)\) can be \(\epsilon\)-approximated with \(O(\epsilon^{-3}n/\sqrt{\log n})\) samples (apologies to the authors to replacing their \(k\) with the more familiar \(n\) for our readers). Thus, we can get sublinear sampling complexity for a very general class of estimation problems. Moreover, this was actually the simplest setting consider in the paper. One can deal with such functions of \(d\) distributions, not just two distributions. One of the corollaries of the theorems is a sublinear tolerant tester for the property of being a mixture of distributions.</p></div>
    </content>
    <updated>2020-10-07T23:20:27Z</updated>
    <published>2020-10-07T23:20:27Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Seshadhri</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2020-10-19T23:34:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=488</id>
    <link href="https://tcsplus.wordpress.com/2020/10/07/tcs-talk-wednesday-october-14-jayadev-acharya-cornell-university/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, October 14 — Jayadev Acharya, Cornell University</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, October 14th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Jayadev Acharya from Cornell University will speak about “Distributed Statistical Inference under Local Information Constraints ” (abstract below). You can reserve a spot as an individual or a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, October 14th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Jayadev Acharya</strong> from Cornell University will speak about  “<em>Distributed Statistical Inference under Local Information Constraints</em> ” (abstract below).</p>



<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our  website</a> on the day of the talk, so people who did not sign up will still be able to  watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>



<blockquote class="wp-block-quote"><p>Abstract: We consider statistical inference tasks in a distributed setting where access to data samples is subjected to strict “local constraints,” through a unified framework that captures communication limitations and (local) privacy constraints as special cases. We study estimation (learning) and goodness-of-fit (testing) for both discrete and high-dimensional distributions. Our goal is to understand how the sample complexity increases under the information constraints.<br/><br/>In this talk we will provide an overview of this field and a sample of some of our results. We will discuss the role of (public) randomness  and interactivity in information-constrained inference, and make a case for thinking about randomness and interactivity as resources.<br/><br/>The work is part of a long-term ongoing collaboration with Clément Canonne (IBM Research) and Himanshu Tyagi (IISc), and includes works done with Cody Freitag (Cornell), Yanjun Han (Stanford), Yuhan Liu (Cornell), and Ziteng Sun (Cornell). </p></blockquote></div>
    </content>
    <updated>2020-10-07T20:11:36Z</updated>
    <published>2020-10-07T20:11:36Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-10-20T03:21:28Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/neurips2020/</id>
    <link href="https://differentialprivacy.org/neurips2020/" rel="alternate" type="text/html"/>
    <title>Conference Digest - NeurIPS 2020</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://neurips.cc/Conferences/2020">NeurIPS 2020</a> is the biggest conference on machine learning, with tons of content on differential privacy in many different forms.
We were able to find two workshops, a competition, and 31 papers. 
This was just going off the preliminary <a href="https://nips.cc/Conferences/2020/AcceptedPapersInitial">accepted papers list</a>, so it’s possible that we might have missed some papers on differential privacy – please let us know!
We will update this post later, once all the conference material (papers and videos) are publicly available.</p>

<h2 id="workshops">Workshops</h2>

<ul>
  <li>
    <p><a href="https://ppml-workshop.github.io/">Privacy Preserving Machine Learning - PriML and PPML Joint Edition</a></p>
  </li>
  <li>
    <p><a href="http://icfl.cc/SpicyFL/2020">International Workshop on Scalability, Privacy, and Security in Federated Learning (SpicyFL 2020)</a></p>
  </li>
</ul>

<h2 id="competitions">Competitions</h2>

<ul>
  <li><a href="https://www.vanderschaar-lab.com/privacy-challenge/">Hide-and-Seek Privacy Challenge: Synthetic Data Generation vs. Patient Re-identification with Clinical Time-series Data</a></li>
</ul>

<h2 id="papers">Papers</h2>

<ul>
  <li>
    <p><a href="https://arxiv.org/abs/2007.05665">A Computational Separation between Private Learning and Online Learning</a><br/>
<a href="https://cs-people.bu.edu/mbun/">Mark Bun</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2004.05975">Adversarially Robust Streaming Algorithms via Differential Privacy</a><br/>
<a href="http://u.cs.biu.ac.il/~avinatan/">Avinatan Hasidim</a>, <a href="http://www.cs.tau.ac.il/~haimk/">Haim Kaplan</a>, <a href="https://www.tau.ac.il/~mansour/">Yishay Mansour</a>, <a href="https://research.google/people/YossiMatias/">Yossi Matias</a>, <a href="https://www.uri.co.il/">Uri Stemmer</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.07709">Auditing Differentially Private Machine Learning: How Private is Private SGD?</a><br/>
<a href="https://www.ccis.northeastern.edu/home/jagielski/">Matthew Jagielski</a>, <a href="https://www.ccs.neu.edu/home/jullman/">Jonathan Ullman</a>, <a href="https://www.ccs.neu.edu/home/alina/">Alina Oprea</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2007.11707">Breaking the Communication-Privacy-Accuracy Trilemma</a><br/>
<a href="https://web.stanford.edu/~wnchen/index.html">Wei-Ning Chen</a>, <a href="https://kairouzp.github.io/">Peter Kairouz</a>, <a href="https://web.stanford.edu/~aozgur/">Ayfer Ozgur</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.06618">CoinPress: Practical Private Mean and Covariance Estimation</a><br/>
<a href="https://sravb.github.io/">Sourav Biswas</a>, <a href="https://yihedong.me/">Yihe Dong</a>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, <a href="https://www.ccs.neu.edu/home/jullman/">Jonathan Ullman</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2008.08007">Differentially Private Clustering: Tight Approximation Ratios</a><br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>, <a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>, <a href="https://pasin30055.github.io/">Pasin Manurangsi</a></p>
  </li>
  <li>
    <p><a href="http://web.mit.edu/dubeya/www/files/dp_linucb_20.pdf">Differentially-Private Federated Contextual Bandits</a><br/>
<a href="http://web.mit.edu/dubeya/www/">Abhimanyu Dubey</a>, <a href="https://www.media.mit.edu/people/sandy/overview/">Alex Pentland</a></p>
  </li>
  <li>
    <p>Faster Differentially Private Samplers via Rényi Divergence Analysis of Discretized Langevin MCMC<br/>
<a href="https://people.eecs.berkeley.edu/~arunganesh/">Arun Ganesh</a>, <a href="http://kunaltalwar.org/">Kunal Talwar</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.08265">GS-WGAN: A Gradient-Sanitized Approach for Learning Differentially Private Generators</a><br/>
<a href="https://cispa.de/en/people/dingfan.chen">Dingfan Chen</a>, <a href="https://tribhuvanesh.github.io/">Tribhuvanesh Orekondy</a>, <a href="https://cispa.saarland/group/fritz/">Mario Fritz</a></p>
  </li>
  <li>
    <p>Improving Sparse Vector Technique with Renyi Differential Privacy<br/>
<a href="https://jeremy43.github.io/">Yuqing Zhu</a>, <a href="https://sites.cs.ucsb.edu/~yuxiangw/">Yu-Xiang Wang</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2005.10630">Instance-optimality in differential privacy via approximate inverse sensitivity mechanisms</a><br/>
<a href="http://web.stanford.edu/~asi/">Hilal Asi</a>, <a href="https://web.stanford.edu/~jduchi/">John Duchi</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2007.13660">Learning discrete distributions: user vs item-level privacy</a><br/>
<a href="https://www.ece.cornell.edu/research/grad-students/yuhan-liu">Yuhan Liu</a>, <a href="http://theertha.info/">Ananda Theertha Suresh</a>, <a href="http://felixyu.org/">Felix Xinnan Yu</a>, <a href="https://research.google/people/author11555/">Sanjiv Kumar</a>, <a href="https://research.google/people/author125/">Michael D Riley</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2008.00331">Learning from Mixtures of Private and Public Populations</a><br/>
<a href="https://sites.google.com/view/rbassily">Raef Bassily</a>, <a href="http://www.cs.technion.ac.il/~shaymrn/">Shay Moran</a>, <a href="http://web.cse.ohio-state.edu/~nandi.10/">Anupama Nandi</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.00701">Locally Differentially Private (Contextual) Bandits Learning</a><br/>
<a href="https://scholar.google.com/citations?user=Bw-WdyUAAAAJ">Kai Zheng</a>, <a href="https://tianle.website/">Tianle Cai</a>, <a href="https://www.weiranhuang.com/">Weiran Huang</a>, <a href="http://www.ee.columbia.edu/~zgli/">Zhenguo Li</a>, <a href="http://www.liweiwang-pku.com/">Liwei Wang</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2005.12601">Locally private non-asymptotic testing of discrete distributions is faster using interactive mechanisms</a><br/>
<a href="https://warwick.ac.uk/fac/sci/statistics/staff/academic-research/berrett/">Thomas Berrett</a>, <a href="http://cbutucea.perso.math.cnrs.fr/">Cristina Butucea</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.01980">On the Equivalence between Online and Private Learnability beyond Binary Classification</a><br/>
<a href="https://scholar.google.com/citations?user=ajqlbHUAAAAJ">Young Hun Jung</a>, <a href="https://scholar.google.com/citations?user=5xt0ba0AAAAJ&amp;hl=en">Baekjin Kim</a>, <a href="https://ambujtewari.github.io/">Ambuj Tewari</a></p>
  </li>
  <li>
    <p>Optimal Private Median Estimation under Minimal Distributional Assumptions<br/>
<a href="https://tzamos.com/">Christos Tzamos</a>, <a href="http://www.cs.columbia.edu/~emvlatakis/">Emmanouil-Vasileios Vlatakis-Gkaragkounis</a>, <a href="http://www.mit.edu/~izadik/">Ilias Zadik</a></p>
  </li>
  <li>
    <p>Permute-and-Flip: A new mechanism for differentially-private selection<br/>
<a href="https://people.cs.umass.edu/~rmckenna/">Ryan McKenna</a>, <a href="https://people.cs.umass.edu/~sheldon/">Daniel Sheldon</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2007.06605">Privacy Amplification via Random Check-Ins</a><br/>
<a href="https://borjaballe.github.io/">Borja Balle</a>, <a href="https://kairouzp.github.io/">Peter Kairouz</a>, <a href="https://scholar.google.com/citations?user=iKPWydkAAAAJ">Brendan McMahan</a>, <a href="https://scholar.google.com/citations?user=iKPWydkAAAAJ">Om Thakkar</a>, <a href="https://athakurta.squarespace.com/">Abhradeep Thakurta</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1905.11947">Private Identity Testing for High-Dimensional Distributions</a><br/>
<a href="http://www.cs.columbia.edu/~ccanonne/">Clement Canonne</a>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, <a href="https://audramarymcmillan.wixsite.com/mysite">Audra McMillan</a>, <a href="https://www.ccs.neu.edu/home/jullman/">Jonathan Ullman</a>, <a href="https://www.ccs.neu.edu/home/lydiazak/">Lydia Zakynthinou</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2004.07839">Private Learning of Halfspaces: Simplifying the Construction and Reducing the Sample Complexity</a><br/>
<a href="http://www.cs.tau.ac.il/~haimk/">Haim Kaplan</a>, <a href="https://www.tau.ac.il/~mansour/">Yishay Mansour</a>, <a href="https://www.uri.co.il/">Uri Stemmer</a>, <a href="https://dblp.org/pid/146/9658.html">Eliad Tsfadia</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.10129">Smoothed Analysis of Online and Differentially Private Learning</a><br/>
<a href="https://www.cs.cornell.edu/~nika/">Nika Haghtalab</a>, <a href="http://timroughgarden.org/">Tim Roughgarden</a>, <a href="https://ashettyv.github.io/">Abhishek Shetty</a></p>
  </li>
  <li>
    <p>Smoothly Bounding User Contributions in Differential Privacy<br/>
<a href="https://www.epasto.org/">Alessandro Epasto</a>, <a href="https://research.google/people/MohammadMahdian/">Mohammad Mahdian</a>, <a href="https://sites.google.com/view/jieming-mao">Jieming Mao</a>, <a href="https://people.csail.mit.edu/mirrokni/Welcome.html">Vahab Mirrokni</a>, <a href="https://www.linkedin.com/in/lijie-ren-57162633/">Lijie Ren</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.06914">Stability of Stochastic Gradient Descent on Nonsmooth Convex Losses</a><br/>
<a href="https://sites.google.com/view/rbassily">Raef Bassily</a>, <a href="http://vtaly.net/">Vitaly Feldman</a>, <a href="https://sites.google.com/view/cguzman/">Cristobal Guzman</a>, <a href="http://kunaltalwar.org/">Kunal Talwar</a></p>
  </li>
  <li>
    <p>Synthetic Data Generators – Sequential and Private<br/>
<a href="https://research.google/people/OlivierBousquet/">Olivier Bousquet</a>, <a href="https://www.tau.ac.il/~rlivni/">Roi Livni</a>, <a href="http://www.cs.technion.ac.il/~shaymrn/">Shay Moran</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2004.00010">The Discrete Gaussian for Differential Privacy</a><br/>
<a href="http://www.cs.columbia.edu/~ccanonne/">Clement Canonne</a>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, <a href="http://www.thomas-steinke.net/">Thomas Steinke</a></p>
  </li>
  <li>
    <p>The Flajolet-Martin Sketch Itself Preserves Differential Privacy: Private Counting with Minimal Space<br/>
<a href="https://cs-people.bu.edu/ads22/">Adam Smith</a>, <a href="https://shs037.github.io/">Shuang Song</a>, <a href="https://athakurta.squarespace.com/">Abhradeep Thakurta</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.08598">Towards practical differentially private causal graph discovery</a><br/>
<a href="https://wanglun1996.github.io/">Lun Wang</a>, Qi Pang, <a href="https://people.eecs.berkeley.edu/~dawnsong/">Dawn Song</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2006.15429">Understanding Gradient Clipping in Private SGD: A Geometric Perspective</a><br/>
<a href="https://scholar.google.com/citations?user=M0ki5ZgAAAAJ">Xiangyi Chen</a>, <a href="https://zstevenwu.com/">Steven Wu</a>, <a href="https://people.ece.umn.edu/~mhong/mingyi.html">Mingyi Hong</a></p>
  </li>
</ul></div>
    </summary>
    <updated>2020-10-07T16:30:00Z</updated>
    <published>2020-10-07T16:30:00Z</published>
    <author>
      <name>Gautam Kamath</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2020-10-19T23:35:08Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://dstheory.wordpress.com/?p=63</id>
    <link href="https://dstheory.wordpress.com/2020/10/07/friday-oct-09-alexandr-andoni-from-columbia-university/" rel="alternate" type="text/html"/>
    <title>Friday, Oct 09 — Alexandr Andoni from Columbia University</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The next Foundations of Data Science virtual talk will take place on Friday, Oct 09th at 10:00 AM Pacific Time (1:00 pm Eastern Time, 18:00 Central European Time, 17:00 UTC).  Alexandr Andoni from Columbia University will speak about “Approximating Edit Distance in Near-Linear Time”. Abstract: Edit distance is a classic measure of similarity between strings, with<a class="more-link" href="https://dstheory.wordpress.com/2020/10/07/friday-oct-09-alexandr-andoni-from-columbia-university/">Continue reading <span class="screen-reader-text">"Friday, Oct 09 — Alexandr Andoni from Columbia University"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next Foundations of Data Science virtual talk will take place on Friday, Oct 09th at 10:00 AM Pacific Time (1:00 pm Eastern Time, 18:00 Central European Time, 17:00 UTC).  <strong>Alexandr Andoni </strong>from Columbia University will speak about “<em><strong>Approximating Edit Distance in Near-Linear Time</strong></em>”.</p>



<p><strong>Abstract</strong>: Edit distance is a classic measure of similarity between strings, with applications ranging from computational biology to coding. Computing edit distance is also a classic dynamic programming problem, with a quadratic run-time solution, often taught in the “Intro to Algorithms” classes. Improving this runtime has been a decades-old challenge, now ruled likely-impossible using tools from the modern area of fine-grained complexity. We show how to approximate the edit distance between two strings in near-linear time, up to a constant factor. Our result completes a research direction set forth in the breakthrough paper of [Chakraborty, Das, Goldenberg, Koucky, Saks; FOCS’18], which showed the first constant-factor approximation algorithm with a (strongly) sub-quadratic running time.</p>



<p>Joint work with Negev Shekel Nosatzki, available at<a href="https://arxiv.org/abs/2005.07678"> https://arxiv.org/abs/2005.07678</a>.</p>



<p><a href="https://sites.google.com/view/dstheory" rel="noreferrer noopener" target="_blank">Please register here to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>
    </content>
    <updated>2020-10-07T15:44:56Z</updated>
    <published>2020-10-07T15:44:56Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>dstheory</name>
    </author>
    <source>
      <id>https://dstheory.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://dstheory.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://dstheory.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://dstheory.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://dstheory.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Foundation of Data Science – Virtual Talk Series</title>
      <updated>2020-10-20T03:22:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17368</id>
    <link href="https://rjlipton.wordpress.com/2020/10/06/knowledge-is-good/" rel="alternate" type="text/html"/>
    <title>Knowledge is Good</title>
    <summary>Science is good too Emil Faber is the pretend founder of the pretend Faber College. The 1978 movie Animal House starts with a close-up of Faber’s statue, which has the inscription, Knowledge Is Good. Today, Ken and I thought we might talk about knowledge, science, mathematics, proofs, and more. The phrase on Faber’s pedestal is […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Science is good too</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.files.wordpress.com/2020/08/seal.png"><img alt="" class="alignright size-thumbnail wp-image-17660" height="150" src="https://rjlipton.files.wordpress.com/2020/08/seal.png?w=150" width="150"/></a></p>
<p>
Emil Faber is the pretend founder of the pretend Faber College. The 1978 movie <a href="https://en.wikipedia.org/wiki/Animal_House">Animal House</a> starts with a close-up of Faber’s statue, which has the inscription, <b>Knowledge Is Good</b>.</p>
<p>
Today, Ken and I thought we might talk about knowledge, science, mathematics, proofs, and more.<br/>
<span id="more-17368"/></p>
<p>
The phrase on Faber’s pedestal is meant to be a joke, as is the subtitle we added saying the same about science. But there is some truth to both of them. From the cause of climate change to the best response to the current pandemic to sports predictions there is much interest in science. Science is good, indeed.</p>
<p><a href="https://rjlipton.files.wordpress.com/2020/08/faberpedestal.jpg"><img alt="" class="aligncenter size-thumbnail wp-image-17661" height="228" src="https://rjlipton.files.wordpress.com/2020/08/faberpedestal.jpg?w=300" width="300"/></a></p>
<p>
</p><p/><h2> Science </h2><p/>
<p/><p>
What is science and what are methods of creating knowledge via science? There is a whole world on the philosophy of <a href="https://en.wikipedia.org/wiki/Science">science</a>. The central questions are: What is science? What methods are used to create new science? Is science good?—just kidding. </p>
<p>
We are not experts on the philosophy of science. But there seem to be three main ways to create scientific knowledge.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\bullet }"/> <i>Experiments:</i> This is the classic one. Think about the testing of a candidate vaccine to stop the pandemic. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\bullet }"/> <i>Computational Experiments:</i> This is relatively new. Think computer simulations of how climate change is effected by the methods of creating energy—for example. wind vs. coal. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\bullet }"/> <i>Mathematical Proofs:</i> This is the one we focus on here at GLL. Think proofs that some algorithm works or that there is no algorithm that can work unless… </p>
<p>
</p><p/><h2> Mathematical Proofs </h2><p/>
<p/><p>
We are interested in creating knowledge via proving new theorems. This is how we try to create knowledge. Our science is based not on experiments and not on simulations but mostly on the theorem-proof method. Well not exactly. We do use experiments and simulations. For example, the field of quantum algorithms uses both of these.</p>
<p>
However, math proofs are the basis of complexity theory. This means that we need to create proofs and then check that they are correct. The difficulty of checking a proof is based on who created them: </p>
<ul>
<li>
You did—checking your own work. <p/>
</li><li>
Someone else did—refereeing for a journal. <p/>
</li><li>
Someone in your class did—grading exams. <p/>
</li><li>
Some graduate student did—mentoring. <p/>
</li><li>
Someone on the web who claims a major result like <img alt="{\mathsf{P &lt; NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathsf{P &lt; NP}}"/> did—debugging. <p/>
</li><li>
And so on.
</li></ul>
<p>
</p><p/><h2> My Favorite Checking Method </h2><p/>
<p/><p>
My favorite tool for checking is this trick: Suppose that we have a proof <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{P}"/> that demonstrates <img alt="{A \implies X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+%5Cimplies+X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{A \implies X}"/> is true. Sometimes it is possible to show that there is a proof <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{Q}"/> that proves <img alt="{A \implies Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+%5Cimplies+Y%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{A \implies Y}"/> where: </p>
<ol>
<li>
The proof <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{Q}"/> is based on changing the claimed proof <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{P}"/>. <p/>
</li><li>
The proof <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{Q}"/> demonstrates <img alt="{A \implies Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+%5Cimplies+Y%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{A \implies Y}"/>, and; <p/>
</li><li>
The statement <img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{Y}"/> does not follow from <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{A}"/>.
</li></ol>
<p>
One way this commonly arises is when <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{P}"/> as a proof did not use all of the assumptions in <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{A}"/>. Thus <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{P}"/> really proves more that <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{X}"/> and it proves <img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{Y}"/>. But we note that <img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{Y}"/> is not a consequence of <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{A}"/>.</p>
<p>
For example, consider the Riemann hypothesis. Suppose that we claim that we have a proof that 	</p>
<p align="center"><img alt="\displaystyle  \sum_{n=1}^{\infty} \frac{1}{n^{s}} \neq 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bn%3D1%7D%5E%7B%5Cinfty%7D+%5Cfrac%7B1%7D%7Bn%5E%7Bs%7D%7D+%5Cneq+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  \sum_{n=1}^{\infty} \frac{1}{n^{s}} \neq 0 "/></p>
<p>follows from the usual axioms of math plus <img alt="{\Re(s) &gt; 1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CRe%28s%29+%3E+1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\Re(s) &gt; 1/2}"/>. Sounds great. But suppose this is based on an argument that assumes that 	</p>
<p align="center"><img alt="\displaystyle  \sum_{n=1}^{\infty} \frac{1}{n^{s}} = 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bn%3D1%7D%5E%7B%5Cinfty%7D+%5Cfrac%7B1%7D%7Bn%5E%7Bs%7D%7D+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  \sum_{n=1}^{\infty} \frac{1}{n^{s}} = 0 "/></p>
<p>and manipulates the summation, eventually yielding a contradiction, without using the condition <img alt="{\Re(s) &gt; 1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CRe%28s%29+%3E+1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\Re(s) &gt; 1/2}"/>. This is a problem, since there are <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{s}"/> with <img alt="{\Re(s) = 1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CRe%28s%29+%3D+1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\Re(s) = 1/2}"/> so that the sum is zero. This is an example of the above method of checking. </p>
<p>
</p><p/><h2> A New Checking Method </h2><p/>
<p/><p>
From time to time claims are made of resolutions to famous conjectures. Think <img alt="{\mathsf{P = NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathsf{P = NP}}"/>. These claims have all been wrong to date. So most researchers are reluctant to take time to check any new claims. Why would you take the effort to try and find the bug that is likely there? </p>
<p>
I wonder if there could be a method that is based on competition. For concreteness, suppose Alice and Bob are two researchers who both claim a resolution to the <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathsf{P}}"/> versus <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathsf{NP}}"/> problem. Alice has a lower bound argument that <img alt="{\mathsf{P &lt; NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathsf{P &lt; NP}}"/> and Bob has an upper bound that <img alt="{\mathsf{P = NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathsf{P = NP}}"/>. Could we have them play a “game”? </p>
<blockquote><p><b> </b> <em> Give their papers to each other. Have them try to find a flaw in each other’s paper. </em>
</p></blockquote>
<p/><p>
They are highly motivated. Could we argue that if they cannot find any flaw then we would be slightly more motivated to look at the papers? </p>
<p>
This might work even if they both claim <img alt="{\mathsf{P = NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathsf{P = NP}}"/>. Ken and I, personally, have had more claims of <img alt="{\mathsf{P = NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathsf{P = NP}}"/> brought to our attention. Even in this case they would be highly motivated: the awards, the prizes, the praise will go to the one who is correct. </p>
<p>
</p><p/><h2> Possible Extensions </h2><p/>
<p/><p>
One difference in our situation from classic empirical science is the nature of gaps in knowledge. For example, one of the big current controversies in physics is over the existence of <a href="https://en.wikipedia.org/wiki/Dark_matter">dark matter</a>. The Wikipedia article we just linked seems to date mostly to years around 2012 when dark matter was more widely accepted than <a href="https://medium.com/futuresin/doubting-dark-matter-f3e400c7dcd7">strikes</a> us <a href="https://www.scientificamerican.com/article/is-dark-matter-real/">today</a> (see also <a href="http://backreaction.blogspot.com/2019/10/dark-matter-nightmare-what-if-we-just.html">this</a> and <a href="http://backreaction.blogspot.com/2020/03/are-dark-energy-and-dark-matter.html">this</a>). There are cases where two competing theories are incompatible yet the available data do not suffice to find a fault in either. </p>
<p>
Whereas, with claimed proofs of incompatible statements, such as <img alt="{\mathsf{P &lt; NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathsf{P &lt; NP}}"/> and <img alt="{\mathsf{P = NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathsf{P = NP}}"/>, at least one must have a demonstrable error. The statements themselves may have barriers all the way up to undecidability, but that does not matter to judging the proffered proofs.</p>
<p>
The method may be more applicable in life sciences where the gap is gathering sufficient field or lab observations. For a topical example, consider claims about the risk or safety of human gatherings amid the pandemic. One extreme is represented by the extraordinary <a href="http://ftp.iza.org/dp13670.pdf">claim</a>, which is <a href="https://www.wired.com/story/how-much-do-crowds-contribute-to-covid-its-complicated/">evidently</a> quite <a href="https://bgr.com/2020/09/14/sturgis-bike-rally-paper-impact-on-covid-19-coronavirus-cases/">excessive</a>, that the Sturgis motorcycle rally in August led to over 250,000 Covid-19 cases. The other extreme would be analyses used to justify gatherings with minimal precautions. The extremes cannot coexist. The means to arbitrate between them are available in principle but require costly social effort for contact tracing and testing as well as resolving mathematical issues between epidemiological models.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
What do you think of our new checking method? Should it be more widely employed for evaluating claims and hypotheses?</p>
<p/></font></font></div>
    </content>
    <updated>2020-10-07T04:54:16Z</updated>
    <published>2020-10-07T04:54:16Z</published>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Open Problems"/>
    <category term="P=NP"/>
    <category term="Proofs"/>
    <category term="Results"/>
    <category term="Animal House"/>
    <category term="claims"/>
    <category term="Emil Faber"/>
    <category term="Logic"/>
    <category term="science"/>
    <category term="science and society"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-10-20T03:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://minimizingregret.wordpress.com/?p=389</id>
    <link href="https://minimizingregret.wordpress.com/2020/10/06/blackwell-approachability-meets-online-convex-optimization/" rel="alternate" type="text/html"/>
    <title>Blackwell approachability meets Online Convex Optimization</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">David Blackwell was still roaming the corridors of UC Berkeley’s stat department during my postdoc years, circa 2009. Jake Abernethy, Sasha Rakhlin, Peter Bartlett and myself were discussing his results, and his seminal contributions to prediction theory were already well known. At that time, still the early days of online convex optimization, we were contemplating … <a class="more-link" href="https://minimizingregret.wordpress.com/2020/10/06/blackwell-approachability-meets-online-convex-optimization/">Continue reading <span class="screen-reader-text">Blackwell approachability meets Online Convex Optimization</span> <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>David Blackwell was still roaming the corridors of UC Berkeley’s stat department during my postdoc years, circa 2009. Jake Abernethy, Sasha Rakhlin, Peter Bartlett and myself were discussing his results, and his seminal contributions to prediction theory were already well known.</p>



<figure class="wp-block-image size-large is-resized"><img alt="" class="wp-image-395" height="183" src="https://minimizingregret.files.wordpress.com/2020/10/download.jpeg?w=275" width="275"/>David Blackwell</figure>



<figure class="wp-block-image size-large is-resized"><img alt="" class="wp-image-397" height="200" src="https://minimizingregret.files.wordpress.com/2020/10/16-figure11-1-1.png?w=662" width="204"/>James Hannan</figure>



<p>At that time, still the early days of online convex optimization, we were contemplating everything from adaptive gradient methods to bandit convex optimization. Blackwell’s famed approachability theorem was looming in the background, considered to be one of the strongest theorems in the ML-theorists toolkit. </p>



<p>I’ve recently added a new draft chapter to to V2.0 of <a href="http://ocobook.cs.princeton.edu">Introduction to Online Convex Optimization</a>, about the connection between OCO and Blackwell approachability: they are algorithmically equivalent! More precisely, an efficient algorithm for approachability gives rise to an efficient algorithm for OCO with sublinear regret, and vice versa. </p>



<p>Details are in the chapter draft, and  I recommend this music for reading it: (the song is called “together we won despite everything”, dedicated to music and science)</p>



<figure class="wp-block-embed is-type-rich is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<div class="jetpack-video-wrapper"/>
</div></figure>



<p>.</p>



<p/></div>
    </content>
    <updated>2020-10-06T00:11:46Z</updated>
    <published>2020-10-06T00:11:46Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Elad Hazan</name>
    </author>
    <source>
      <id>https://minimizingregret.wordpress.com</id>
      <logo>https://minimizingregret.files.wordpress.com/2017/08/cropped-pu1.png?w=32</logo>
      <link href="https://minimizingregret.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://minimizingregret.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://minimizingregret.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://minimizingregret.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Google Princeton AI and Hazan Lab @ Princeton University</subtitle>
      <title>Minimizing Regret</title>
      <updated>2020-10-20T03:21:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1794</id>
    <link href="https://theorydish.blog/2020/10/05/forc-2021-is-on-its-way/" rel="alternate" type="text/html"/>
    <title>FORC 2021 Is on Its Way</title>
    <summary>After a powerful launch at 2021, the second meeting of The Symposium on Foundations of Responsible Computing (FORC) is on its way. The call for papers for FORC 2021 is out, with a wonderful PC, headed by Katrina Ligett. Please consider sending your strong submissions down our way.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>After <a href="https://theorydish.blog/2020/03/30/forc-2020-going-strong-going-virtual/">a powerful launch</a> at 2021, the second meeting of  <a href="https://responsiblecomputing.org/">The Symposium on Foundations of Responsible Computing (FORC)</a> is on its way. The <a href="https://responsiblecomputing.org/forc-2021-call-for-papers/">call for papers</a> for FORC 2021 is out, with a wonderful PC, headed by <a href="https://www.cs.huji.ac.il/~katrina/">Katrina Ligett</a>. Please consider sending your strong submissions down our way.</p></div>
    </content>
    <updated>2020-10-05T15:00:47Z</updated>
    <published>2020-10-05T15:00:47Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2020-10-20T03:21:43Z</updated>
    </source>
  </entry>
</feed>
