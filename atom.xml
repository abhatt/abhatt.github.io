<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2022-04-01T06:39:24Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=19829</id>
    <link href="https://rjlipton.wpcomstaging.com/2022/03/31/the-2021-turing-award/" rel="alternate" type="text/html"/>
    <title>The 2021 Turing Award</title>
    <summary>If a machine is expected to be infallible, it cannot also be intelligent.—Alan Turing Jack Dongarra has just won the 2021 Turing Award. Congrats to him. Dongarra is a Distinguished Professor of Computer Science in the Electrical Engineering and Computer Science Department at the University of Tennessee. He has connections to the nearby Oak Ridge […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<i>If a machine is expected to be infallible, it cannot also be intelligent.—Alan Turing</i></p>
<p>
Jack Dongarra has just won the 2021 <a href="https://amturing.acm.org">Turing Award</a>. Congrats to him. </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/03/31/the-2021-turing-award/jd/" rel="attachment wp-att-19835"><img alt="" class="aligncenter size-full wp-image-19835" height="168" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/03/jd.png?resize=300%2C168&amp;ssl=1" width="300"/></a></p>
<p/><p><br/>
Dongarra is a Distinguished Professor of Computer Science in the Electrical Engineering and Computer Science Department at the University of Tennessee. He has connections to the nearby Oak Ridge National Laboratory, to Rice University, and in England to the University of Manchester—Turing’s academic home.  The nub of ACM’s <a href="https://awards.acm.org/about/2021-turing">article</a> says the following, with <i>linear algebra</i> emphasized:</p>
<blockquote><p>
Dongarra’s major contribution was in creating open-source software libraries and standards which employ linear algebra as an intermediate language that can be used by a wide variety of applications. These libraries have been written for single processors, parallel computers, multicore nodes, and multiple GPUs per node. Dongarra’s libraries also introduced many important innovations including autotuning, mixed precision arithmetic, and batch computations.
</p></blockquote>
<p>Dongarra’s doctoral advisor was Cleve Moler decades ago in 1980.  I always think we should thank our advisors—having been one myself.  But they continued on parallel tracks in the 1980s.  Moler had invented <a href="https://en.wikipedia.org/wiki/MATLAB">MATLAB</a> in the 1970s, but with the advent of IBM PCs in the 1980s, realized their promise for bringing advanced numerical computations into wide use.  Moler and John Little and Steve Bangert co-founded <a href="https://en.wikipedia.org/wiki/MathWorks">MathWorks</a> in 1984 to market their rewrite of MATLAB in C.  Dongarra went on to co-write a host of mathematical tools: <a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms">BLAS</a>, <a href="https://en.wikipedia.org/wiki/LAPACK">LAPACK</a> (which succeeded his work with Moler and others on <a href="https://en.wikipedia.org/wiki/EISPACK">EISPACK</a> and <a href="https://en.wikipedia.org/wiki/LINPACK">LINPACK</a>), <a href="https://en.wikipedia.org/wiki/Automatically_Tuned_Linear_Algebra_Software">ATLAS</a>, <a href="https://en.wikipedia.org/wiki/HPCG_benchmark">HPCG</a>, and much more.</p>
<p>
</p><p/><h2> Why The Turing Award? </h2><p/>
<p/><p>
Dongarra received the Turing for his software—code that has had significant impact in many areas of computational science from data analytics, healthcare, renewable energy, weather prediction, genomics, and economics, to name a few. His work was over four decades of writing code that can solve linear systems. Such systems occur in just about all of engineering and science. They challenge us because we constantly wish to solve bigger and more difficult systems. And we are able to solve bigger systems because computers continue to get faster—able to do more operations per second. </p>
<p>
However, the challenge was and is that while computers get faster every year, <em>how</em> they get faster remains involved. They cannot just compute faster but how they get faster continues to be complex. If a computer just got faster every year, then Dongarra would <b>not</b> have been able to win a Turing award. That how they get faster is more complex, more involved, that made his Turing award possible. </p>
<p>
</p><p/><h2> Why So Hard? </h2><p/>
<p/><p>
What are some of the challenges that Dongarra had to fight, and continues to fight?  We can build off the following <a href="https://web.archive.org/web/20111021183914/http://www.csbruce.com/~csbruce/quotes/craig-cut.html">quip</a> by the Canadian software developer Craig Bruce:</p>
<blockquote><p>“It’s hardware that makes a machine fast. It’s software that makes a fast machine slow.”</p></blockquote>
<p>
And in 1980 we could add: what could make an IBM PC fast?</p>
<p>
The goal of Dongarra’s software was to run fast on the current hardware systems. The challenge over the decades is simple:  As hardware systems were improved in performance they did not simply get faster. They needed to change how they worked in order to get faster.  The systems changed from single processors, to parallel computers, multicore nodes, and multiple GPUs per node. These changes made the ability to exploit the potential performance harder and harder. </p>
<p>
Let’s assume, for example, that the hardware could not go 100X faster but could execute 100 operations at the same time. Then provided we can exploit this parallelism we can make the hardware seem to be 100 times faster. But if we cannot exploit this, we are in trouble. This is the key issue that Dongarra faced. This challenge is exactly what made his work so difficult and important.</p>
<p/><h2>The Test of Time</h2><p/>
<p>
The solutions found by Dongarra and his co-workers were definitive enough, and the core of linear algebra in LINPACK pure enough, that it could yield a salient benchmark of hardware power.  The <a href="https://en.wikipedia.org/wiki/LINPACK_benchmarks">LINPACK benchmark suite</a> won <a href="https://www.nytimes.com/1991/09/22/business/technology-measuring-how-fast-computers-really-are.html">agreement</a> by the early 1990s as giving the definitive <a href="https://en.wikipedia.org/wiki/TOP500">measure</a> of hardware power.  A decade ago, they began <a href="https://news.utk.edu/2013/07/10/professor-jack-dongarra-announces-supercomputer-benchmark/">updating</a> it to involve HPCG.</p>
<p>
Thus it is not enough to say that this work has stood the test of time: it <b>is</b> the test of time.  And this is carrying into the future.  A high LINPACK score in 2013 was an early <a href="https://newatlas.com/d-wave-quantum-computer-supercomputer-ranking/27476/">indicator</a> of the power achievable by D-Wave’s quantum adiabatic hardware.  An <a href="https://www.bcg.com/publications/2022/value-of-quantum-computing-benchmarks">article</a> last month titled “The Race to Quantum Advantage Depends on Benchmarking” includes LINPACK as a contender.  There is recent work toward a more-tailored <a href="https://math.berkeley.edu/~linlin/presentations/202007_GoogleQuantum_LinLin_handout.pdf">quantum LINPACK</a> benchmark.</p>
<p/><h2> Open Problems </h2><p/>
<p/><p>
Some interesting milestones about the Turing awards are:  The first recipient of the award, in 1966, was Alan Perlis, an American computer scientist who wrote the compiler for the ALGOL computer programming language. The first woman to win the prize was Frances Allen, in 2006, for her work in compiler optimization, which contributed to the development of parallel execution in multiprocessing. The youngest recipient was Donald Knuth, who was 36 when he received the award in 1974 for his work on algorithms and computer programming. The oldest recipient was Alfred Aho, who was 79 when he received the award in 2020 for his work on algorithms and the theory of programming language implementation.  I knew all the above winners pretty well. Rich DeMillo and I wrote a paper with Perlis in <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/social.pdf">1979</a>.  </p>
<p>
Turing himself was also a pioneer in AI, of course.  He thought about how computers could reason, how they could think.  What directions will the Turing Awards take in the future—and will they reflect this side of Turing’s vision more?</p></div>
    </content>
    <updated>2022-04-01T03:08:34Z</updated>
    <published>2022-04-01T03:08:34Z</published>
    <category term="News"/>
    <category term="benchmarks"/>
    <category term="Jack Dongarra"/>
    <category term="LAPACK"/>
    <category term="LINPACK"/>
    <category term="numerical computing"/>
    <category term="software"/>
    <category term="Turing Award"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2022-04-01T06:37:55Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2022/03/31/linkage</id>
    <link href="https://11011110.github.io/blog/2022/03/31/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Columbia University mathematician Michael Thaddeus is dubious of the data Columbia used for its #2 ranking from US News &amp; World Report (\(\mathbb{M}\), summary, via). Conclusion: the whole ranking system is irredeemably flawed and casts a pernicious influence on priorities (e.g. Columbia would be discouraged from keeping a Nobelist who happens not to have a Ph.D., because the ranking counts faculty with Ph.D.s but not Nobelists).</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="http://www.math.columbia.edu/~thaddeus/ranking/investigation.html">Columbia University mathematician Michael Thaddeus is dubious of the data Columbia used for its #2 ranking from US News &amp; World Report</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107970703826205607">\(\mathbb{M}\)</a>,</span> <a href="http://www.math.columbia.edu/~thaddeus/ranking/executivesummary.pdf">summary</a>, <a href="https://www.chronicle.com/article/columbia-is-ranked-no-2-by-u-s-news-a-professor-says-its-spot-is-based-on-false-data">via</a>). Conclusion: the whole ranking system is irredeemably flawed and casts a pernicious influence on priorities (e.g. Columbia would be discouraged from keeping a Nobelist who happens not to have a Ph.D., because the ranking counts faculty with Ph.D.s but not Nobelists).</p>
  </li>
  <li>
    <p><a href="https://www.theguardian.com/commentisfree/2022/mar/15/ukrainian-heritage-under-threat-truth-soviet-era-russia">Ukrainian archivists rush to digitize and digitally export the information in their documents</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107975640992420745">\(\mathbb{M}\)</a>),</span> both in case the Russian invaders act to destroy unflattering material and as a precaution against more indiscriminate destruction: 
Their physical artworks and architecture are harder to protect, though: “In the midst of the unfolding human tragedy is the appalling cultural loss the war may wreak.”</p>
  </li>
  <li>
    <p>I saw the scene below while returning from my final exam this morning <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107980104426629314">\(\mathbb{M}\)</a>),</span> the first in-person exam I’ve given in two years. Tomorrow we stop requiring masks indoors. Spring has sprung? On the other hand, I don’t have any hard data to support this, but my TAs think that the prevalence of suspected academic dishonesty is much higher than it was before we locked down, perhaps because the students have become used to doing homeworks and exams in an environment where it’s harder to detect.</p>

    <p style="text-align: center;"><img alt="Two trees in Aldrich Park, UC Irvine" src="https://www.ics.uci.edu/~eppstein/pix/2trees/2trees-m.jpg" style="border-style: solid; border-color: black;"/></p>
  </li>
  <li>
    <p><a href="https://vimeo.com/286360639">Sudanese Möbius Band</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107987571486022171">\(\mathbb{M}\)</a>).</span> This is a minimal surface within a hypersphere, with a great circle as its boundary. It can then be stereographically projected into 3d Euclidean space, preserving the circular shape of the boundary. There was apparently a SIGGRAPH’84 video on it by Dan Asimov, one of its discoverers and namesakes (with Sue Goodman: Sue-Dan-ese), but it appears to exist only on old VHS tapes, so instead this more recent video by Charlie Gunn will have to make do.</p>
  </li>
  <li>
    <p><a href="https://gilkalai.wordpress.com/2022/03/21/combinatorial-convexity-a-wonderful-new-book-by-imre-barany/">Gil Kalai provides a micro-review of Imre Bárány’s new book <em>Combinatorial Convexity</em></a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107995544907177318">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p><a href="https://www.theverge.com/2022/3/21/22988994/russia-science-publication-database-conferences">Russian government bans Russian-based scientists from participating in international conferences and discourages them from publishing in internationally-indexed journals</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107998691420226306">\(\mathbb{M}\)</a>).</span></p>
  </li>
  <li>
    <p>Two recent deaths <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108002049810796984">\(\mathbb{M}\)</a>):</span> <a href="https://www.itsoc.org/news/alexander-vardy-distinguished-coding-theorist-passed-away">Alexander Vardy, UCSD coding theorist</a>, and <a href="https://www.ukrinform.net/rubric-society/3435402-ukrainian-mathematician-commits-suicide-in-russia-after-not-being-allowed-to-leave-country.html">Konstantin Olmezov, Ukrainian combinatorist, after being prevented from leaving Russia</a>.</p>
  </li>
  <li>
    <p><a href="https://lthmath.tumblr.com/post/189129742584/a-little-mobius-strip-art-by-adam-p%C4%99kalski-for">A little Möbius strip art by Adam Pękalski</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108009843844704577">\(\mathbb{M}\)</a>).</span></p>

    <p style="text-align: center;"><img alt="M&#xF6;bius strip art by Adam P&#x119;kalski" src="https://11011110.github.io/blog/assets/2022/pekalsky.png"/></p>
  </li>
  <li>
    <p><a href="http://www.ponticulus.hu/rovatok/hidverok/vierling-models-of-surfaces.html">Models of surfaces and abstract art in the early 20th century</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108016004847866791">\(\mathbb{M}\)</a>),</span> Angela Vierling-Claassen, <em>Ponticulus Hungaricus</em>. A brief survey how physical models of mathematical surfaces, made for teaching and research, came to be influential in modern art. This whole site looks like it would be interesting, if you read Hungarian. A Hungarian version of bridgesmathart.org, if you like (where the linked paper originally appeared). They have occasional other pieces in English, as well.</p>
  </li>
  <li>
    <p><a href="https://www.seas.harvard.edu/news/2022/02/bubbles-bubbles-everywhere">Simulating “tens of thousands of bubbles in foamy flows”</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108021804418963231">\(\mathbb{M}\)</a>,</span> <a href="https://doi.org/10.1126/sciadv.abm0590">research article</a>,
<a href="https://technews.acm.org/archives.cfm?fo=2022-02-feb/feb-09-2022.html">via</a>). The issue is that preventing spurious bubble coalescence by tracking the shapes of individual bubbles is slow and expensive. As far as I can tell the new solution involves a grid-based heuristic for coloring the graph of bubbles that are dangerously near each other, and tracking the shape of an entire color class at once.</p>
  </li>
  <li>
    <p><a href="https://sinews.siam.org/Details-Page/siam-honors-womens-history-month">SIAM spotlights the accomplishments of four women mathematicians for Women’s History Month</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108025979413339894">\(\mathbb{M}\)</a>).</span> They are mathematical systems biologist and drug developer Carolyn Cho, image reconstruction researcher Carola-Bibiane Schönlieb, fluid mechanics expert and numerical analyst María González Taboada, and computational flow software developer Carolyn Woodward.</p>
  </li>
  <li>
    <p><a href="https://mathematicalstamps.eu/index">Mathematical postage stamps</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108032725203711460">\(\mathbb{M}\)</a>).</span> The entry page is rather bare and not obvious in its navigation: use the “stamps” menu at the top of the page to find the actual content.</p>
  </li>
  <li>
    <p><a href="https://www.nature.com/articles/d41586-022-00793-1">The rise of citational justice: how scholars are making references fairer</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108038364210238562">\(\mathbb{M}\)</a>,</span> <a href="https://retractionwatch.com/2022/03/26/weekend-reads-concussion-researcher-faces-more-scrutiny-mendel-the-fraud-seeking-redemption-after-misconduct-finding/">via</a>). The title of this <em>Nature</em> article is not really accurate: it’s mostly about how scholars are making more efforts to discover how biased against disadvantaged groups their citation patterns are, not so much what to do about it. But knowledge is power.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/in-music-and-math-lillian-pierce-builds-landscapes-20220330">Nice profile and interview of number theorist and musician Lillian Pierce</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108046674227966771">\(\mathbb{M}\)</a>,</span> <a href="https://en.wikipedia.org/wiki/Lillian_Pierce">see also</a>), in part on how those two interests tie together in unexpected ways.</p>
  </li>
  <li>
    <p><a href="https://www.acm.org/media-center/2022/march/turing-award-2021">Jack Dongarra has won the 2021 Turing Award</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108048639276194672">\(\mathbb{M}\)</a>,</span> <a href="https://www.nytimes.com/2022/03/30/technology/turing-award-jack-dongarra.html">see also</a>), for his “pioneering contributions to numerical algorithms and libraries that enabled high performance computational software to keep pace with exponential hardware improvements for over four decades”.</p>
  </li>
  <li>
    <p><a href="https://desystemize.substack.com/p/desystemize-9">How changing your knowledge representation can lead to new insights in sudoku solving</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108052563128328185">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=30863073">via</a>). In particular, the post describes a notion of equivalent sets (pairs of sets of sudoku cells that must contain the same values of each other), how to recognize sets as equivalent, and how to use the idea in puzzle solving and puzzle construction, that was all new to me.</p>
  </li>
</ul></div>
    </content>
    <updated>2022-03-31T17:37:00Z</updated>
    <published>2022-03-31T17:37:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2022-04-01T01:03:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/042</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/042" rel="alternate" type="text/html"/>
    <title>TR22-042 |  Matrix Polynomial Factorization via Higman Linearization | 

	Vikraman Arvind, 

	Pushkar Joglekar</title>
    <summary>In continuation to our recent work on noncommutative
 polynomial factorization, we consider the factorization problem for
 matrices of polynomials and show the following results.
\begin{itemize}  
\item Given as input a full rank $d\times d$ matrix $M$ whose entries
  $M_{ij}$ are polynomials in the free noncommutative ring
  $\mathbb{F}_q\langle x_1,x_2,\ldots,x_n \rangle$, where each $M_{ij}$ is given by a
  noncommutative arithmetic formula of size at most $s$, we give a
  randomized algorithm that runs in time polynomial in $d,s, n$ and
  $\log_2q$ that computes a factorization of $M$ as a matrix product
  $M=M_1M_2\cdots M_r$, where each $d\times d$ matrix factor $M_i$ is
  irreducible (in a well-defined sense) and the entries of each $M_i$
  are polynomials in $\mathbb{F}_q \langle x_1,x_2,\ldots,x_n \rangle$ that are output
  as algebraic branching programs. We also obtain a deterministic
  algorithm for the problem that runs in $poly(d,n,s,q)$.
\item A special case is the efficient factorization of matrices whose
  entries are univariate polynomials in $\mathbb{F}[x]$. When $\mathbb{F}$ is a finite
  field the above result applies. When $\mathbb{F}$ is the field of rationals
  we obtain a deterministic polynomial-time algorithm for the problem.
  \end{itemize}</summary>
    <updated>2022-03-31T12:29:44Z</updated>
    <published>2022-03-31T12:29:44Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-04-01T06:37:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/03/31/phd-studentship-at-university-of-liverpool-apply-by-april-18-2022/</id>
    <link href="https://cstheory-jobs.org/2022/03/31/phd-studentship-at-university-of-liverpool-apply-by-april-18-2022/" rel="alternate" type="text/html"/>
    <title>PhD Studentship at University of Liverpool (apply by April 18, 2022)</title>
    <summary>Applications are invited for a 4-year PhD studentship on Game Theory and Computational Social Choice on Blockchain at the University of Liverpool, in collaboration with the blockchain technology company IOHK. The selected candidate will be advised by Aris Filos-Ratsikas and Rahul Savani from the University of Liverpool, and Philip Lazos from IOHK. Website: https://www.findaphd.com/phds/project/game-theory-and-computational-social-choice-on-blockchain-epsrc-cdt-in-distributed-algorithms/?p142348 Email: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Applications are invited for a 4-year PhD studentship on Game Theory and Computational Social Choice on Blockchain at the University of Liverpool, in collaboration with the blockchain technology company IOHK. The selected candidate will be advised by Aris Filos-Ratsikas and Rahul Savani from the University of Liverpool, and Philip Lazos from IOHK.</p>
<p>Website: <a href="https://www.findaphd.com/phds/project/game-theory-and-computational-social-choice-on-blockchain-epsrc-cdt-in-distributed-algorithms/?p142348">https://www.findaphd.com/phds/project/game-theory-and-computational-social-choice-on-blockchain-epsrc-cdt-in-distributed-algorithms/?p142348</a><br/>
Email: Aris.Filos-Ratsikas@liverpool.ac.uk</p></div>
    </content>
    <updated>2022-03-31T12:26:27Z</updated>
    <published>2022-03-31T12:26:27Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-04-01T06:37:59Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://benjamin-recht.github.io/2022/03/31/external-evaluations/</id>
    <link href="http://benjamin-recht.github.io/2022/03/31/external-evaluations/" rel="alternate" type="text/html"/>
    <title>There’s more to data than distributions.</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>This is first guest post by Deb. More to come!</em></p>

<p>In “<a href="https://www.nejm.org/doi/full/10.1056/NEJMc2104626">The clinician and dataset shift in artificial intelligence</a>,” published in the New England Journal of Medicine, a set of physician-scientists describe how a popular sepsis-prediction system developed by the company Epic needed to be deactivated. “Changes in patients’ demographic characteristics associated with the coronavirus disease 2019 pandemic” supposedly caused spurious alerting arising from the system, rendering it of little value to clinicians. For the authors, this is a clear illustration of distribution shift, a change in training and test data that, in this case, made it difficult to distinguish between fevers and bacterial sepsis. They go into detail about what this means: distribution shift is a fundamental challenge in machine learning, and whenever we attempt to deploy machine learning in the real world without considering the way in which that real world environment can change (whether its changes in technology (e.g., software vendors), changes in population and setting (e.g., new demographics), or changes in behavior (e.g., new reimbursement incentives), then we fail to properly consider the ways in which the data can change or shift between train and test environments. If not considered, the model will inevitably fail.</p>

<p>And why not? If the underlying test data diverges from the data used in the development of the model, we should expect disappointing results. But the distribution shift problem is so common that ML researchers and practitioners have started seeing it everywhere they look. In fact, in many cases, they will inappropriately characterize any failure of deployed ML models as a distribution shift. This both muddles our understanding of what exactly distribution shift means, and limits our vocabulary for the range of failures that can show up in deployment. In this blog post, I’ll use Epic’s sepsis-detector to illustrate some of the current confusion about distribution shift, and why the notion of “external validity”, a description of generalization problems used widely in other fields, is perhaps more relevant.</p>

<p>The terminology of distribution shift is both too specific and not specific enough. A “change in distribution” could be characterized as anything from a variation in source to a re-sampling. <a href="https://rtg.cis.upenn.edu/cis700-2019/papers/dataset-shift/dataset-shift-terminology.pdf">These changes could involve changes to the input features (ie. covariate shift), changes to the labels (ie. prior probability shift) or both (ie. concept drift).</a></p>

<p>The notion of “data distributions” themselves assumes data comes from an imagined data generating function. In that world of infinitely abundant independent data points samples from a bespoke probability distribution (ie. the “independent and identically distributed” i.i.d. assumption), describing data in terms of how it’s distributed makes a lot of sense. But as Breiman describes in <a href="http://www2.math.uu.se/~thulin/mm/breiman.pdf">“Statistical Modeling: The Two Cultures”</a>, that assumption doesn’t often hold for real world data. Very rarely does one actually know the data generating function, or even a reasonable proxy - real world data is disorganized, inconsistent, and unpredictable. As a result, the term “distribution” is vague enough to not address the additional specificity necessary to direct actions and interventions. When we talk about a hypothetical distribution shift, we talk about data changes but are not specific about which data changes happen and why they happen. We’re also constraining our discourse by just looking at changes in the data in the first place, when in fact, many other changes occur between development and deployment (such as changes in interactions with the model, changes in the interpretation of model results, etc.). Specifying the type of distribution shift is one solution, but more importantly, we need to understand specific distribution shifts as part of a broader phenomenon of external validity that we need to begin to articulate as a field.</p>

<p>The most significant consequence of the myopic obsession with distributions is how it constrains ML evaluations. The benchmarking paradigm that dominates ML at the moment is a by-product of its obsession with detecting shifts in data - the evaluation of models on static data test sets are tied to assumptions about failures being due to shifts in data distribution and not much else. A myopic view on distribution shift confuses the discourse on how to evaluate models for deployment. <a href="https://www.nature.com/articles/s41591-021-01312-x">Several</a> <a href="https://www.bmj.com/content/374/bmj.n1872">studies</a> on <a href="https://www.nature.com/articles/s41746-020-00324-0">regulatory approvals</a> of ML-based tools in healthcare already demonstrate how over-emphasis on data distribution shift failures has led ML practitioners and even regulators within the healthcare space to inappropriately prioritize the use of <em>retrospective studies</em> (ie. evaluations on static collections of past data) rather than <em>prospective studies</em> (ie. examinations of the system within its context of use). Things like multi-site assessment, median evaluation sample size, demographic subgroup performance and
“side-by-side comparison of clinicians’ performances with and without AI” are also exceedingly rare in the evaluation of ML-based healthcare tools, as they don’t fit our current narrow perception of what can go wrong when you throw an ML model into the real world. Of course distribution shift matters but the nature in which we focus on it to the exclusion of everything else is regrettable. For better regulation and evaluation methodology for machine learning deployments, we need to expand our thinking and align ourselves with the other fields attempting to understand performance gaps between the theory and practice of interventions.</p>

<p>This broader notion of validity characterizes the accuracy of the claims being made in a specific context. The related notion of reliability has to do with reproducibility and the consistency of results (think of measurement precision), but validity is concerned with some notion of truthfulness and how close claims get to the target of describing the real relationship between inputs and outputs. There are various notions of validity discussed in measurement theory, evaluation science, program evaluation and experiment design literature, but there are common core concepts. For example, internal validity is about assessing a consistent causal relationship between the inputs and outputs within the experiment and construct validity is related to the evaluation of how well experimental variables represent the real world phenomena being observed. When discussing generalization issues, we are most interested in external validity, which analyzes if  the causal relationship between inputs and outputs observed in experiments holds for inputs and outputs outside the experimental setting.</p>

<p>To understand how external validity differs from the current discourse on distribution shift, let’s go back to the sepsis monitoring example. <a href="https://jamanetwork.com/journals/jamainternalmedicine/article-abstract/2781307">“External Validation of a Widely Implemented Proprietary Sepsis Prediction Model in Hospitalized Patients,”</a>, published in JAMA, describes a retrospective study on the use of the sepsis tool between December 2018 and October 2019 (notably well before the pandemic began). They examined 27,697 patients undergoing 38,455 hospitalizations and found that the Epic Sepsis Model predicted the onset of sepsis with an area under the curve of 0.63, “which is substantially worse than the performance reported by its developer”. Furthermore, the tool “did not identify 1,709 patients with sepsis (67%) despite generating alerts… for 6,971 of all 38 455 hospitalized patients (18%), thus creating a large burden of alert fatigue.” These researchers rightfully describe these issues as “external validity” issues, and go into detail examining a range of problems far beyond the data-related “shifts” described in the “Clinician and Dataset Shift” oped. They don’t pretend that this doesn’t have to do with changes in the data - of course it does. Epic’s system evaluation was on data from 3 US health systems from 2013 to 2015, and that’s certainly a different dataset than University of Michigan’s 2018-2019 patient records. But they also comment on changes to the interactions doctors had with the model and how that modified outcomes, as well as other external validity factors that had very little to do with data at all, much less “data distribution shift.” Even when discussing substantive data changes, they are specific in characterizing what it is and breaking down the differences that occurred upon deployment at their hospital.</p>

<p>As this study shows, machine learning needs some clean guidelines for evaluating external validity.
To begin scaffolding such frameworks, we can learn from the social sciences. For example, Erin Hartman, a UC Berkeley colleague in political science, and her co-author Naoki Egami <a href="https://erinhartman.com/publication/elements/">propose a taxonomy that provides an interesting start to this discussion</a>. Their interest is in assessing external validity in scenarios where a population is given a policy treatment (eg. sending out voting reminders, updating the tax code, giving out free vaccines, etc.) and the impact of this treatment as measured within the experiment and also once implemented in the real world. If we consider the treatment to be an ML model’s integration into a broader system, we can begin to articulate what external validity could mean in the algorithmic context.  In my next blog post, I’ll try to work through Hartman and Egami’s framework and other specific proposals from other fields on how we could begin to taxonomize external validity issues, and see which of the external validity problems they describe are actually quite relevant to machine learning.</p></div>
    </summary>
    <updated>2022-03-31T00:00:00Z</updated>
    <published>2022-03-31T00:00:00Z</published>
    <source>
      <id>http://benjamin-recht.github.io/</id>
      <author>
        <name>Ben Recht</name>
      </author>
      <link href="http://benjamin-recht.github.io/" rel="alternate" type="text/html"/>
      <link href="http://benjamin-recht.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Musings on systems, information, learning, and optimization.</subtitle>
      <title>arg min blog</title>
      <updated>2022-03-31T22:42:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2203.16476</id>
    <link href="http://arxiv.org/abs/2203.16476" rel="alternate" type="text/html"/>
    <title>Differentially Private All-Pairs Shortest Path Distances: Improved Algorithms and Lower Bounds</title>
    <feedworld_mtime>1648684800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghazi:Badih.html">Badih Ghazi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Ravi.html">Ravi Kumar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Manurangsi:Pasin.html">Pasin Manurangsi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nelson:Jelani.html">Jelani Nelson</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2203.16476">PDF</a><br/><b>Abstract: </b>We study the problem of releasing the weights of all-pair shortest paths in a
weighted undirected graph with differential privacy (DP). In this setting, the
underlying graph is fixed and two graphs are neighbors if their edge weights
differ by at most $1$ in the $\ell_1$-distance. We give an $\epsilon$-DP
algorithm with additive error $\tilde{O}(n^{2/3} / \epsilon)$ and an
$(\epsilon, \delta)$-DP algorithm with additive error $\tilde{O}(\sqrt{n} /
\epsilon)$ where $n$ denotes the number of vertices. This positively answers a
question of Sealfon (PODS'16), who asked whether a $o(n)$-error algorithm
exists. We also show that an additive error of $\Omega(n^{1/6})$ is necessary
for any sufficiently small $\epsilon, \delta &gt; 0$.
</p>
<p>Finally, we consider a relaxed setting where a multiplicative approximation
is allowed. We show that, with a multiplicative approximation factor $k$, %$2k
- 1$, the additive error can be reduced to $\tilde{O}\left(n^{1/2 + O(1/k)} /
\epsilon\right)$ in the $\epsilon$-DP case and $\tilde{O}(n^{1/3 + O(1/k)} /
\epsilon)$ in the $(\epsilon, \delta)$-DP case, respectively.
</p></div>
    </summary>
    <updated>2022-03-31T22:37:33Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-03-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2203.16264</id>
    <link href="http://arxiv.org/abs/2203.16264" rel="alternate" type="text/html"/>
    <title>Optimally Tracking Labels on an Evolving Tree</title>
    <feedworld_mtime>1648684800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Acharya:Aditya.html">Aditya Acharya</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mount:David_M=.html">David M. Mount</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2203.16264">PDF</a><br/><b>Abstract: </b>Motivated by the problem of maintaining data structures for a large sets of
points that are evolving over the course of time, we consider the problem of
maintaining a set of labels assigned to the vertices of a tree, where the
locations of these labels change over time. We study the problem in the
evolving data framework, where labels change over time due to the action of an
agent called the evolver. The algorithm can only track these changes by
explicitly probing individual nodes of the tree. This framework captures the
tradeoff between the complexity of maintaining an up-to-date view of the
structure and the quality of results computed with the available view. Our
results allow for both randomized and adversarial evolution of the data,
subject to allowing different speedup factors between the algorithm and the
evolver. We show that in the limit, our algorithm maintains labels to within an
average distance of $O(1)$ of their actual locations. We also present nearly
matching lower bounds.
</p></div>
    </summary>
    <updated>2022-03-31T22:39:20Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-03-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2203.16151</id>
    <link href="http://arxiv.org/abs/2203.16151" rel="alternate" type="text/html"/>
    <title>Manipulative Attacks and Group Identification</title>
    <feedworld_mtime>1648684800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Emil Junker <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2203.16151">PDF</a><br/><b>Abstract: </b>The group identification problem asks to identify a socially qualified
subgroup among a group of individuals based on their pairwise valuations. There
are several different rules that can be used to determine the social
qualification status. In this work, we consider the consent rules, the
consensus-start-respecting rule, and the liberal-start-respecting rule.
</p>
<p>In the context of group identification, a manipulative attack is the attempt
by an outsider to influence the outcome of the selection process through
certain means of manipulation. These means include adding, removing, or
partitioning individuals, as well as bribing individuals to change their
opinion.
</p>
<p>In this work, we provide an overview of manipulative attacks in group
identification as well as group identification with partial profiles. In
particular, we study the computational complexity of the corresponding
problems. Most results presented in this work are aggregated from the
literature, but we also show results for previously unstudied problems; these
include general and exact group control in binary profiles and in ternary
profiles, as well as constructive group control in $r$-profiles. For many
considered problems, we also study the parameterized complexity.
</p></div>
    </summary>
    <updated>2022-03-31T22:37:26Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2022-03-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2203.15862</id>
    <link href="http://arxiv.org/abs/2203.15862" rel="alternate" type="text/html"/>
    <title>Restless Temporal Path Parameterized Above Lower Bounds</title>
    <feedworld_mtime>1648684800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zschoche:Philipp.html">Philipp Zschoche</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2203.15862">PDF</a><br/><b>Abstract: </b>Reachability questions are one of the most fundamental algorithmic primitives
in temporal graphs -- graphs whose edge set changes over discrete time steps. A
core problem here is the NP-hard Short Restless Temporal Path: given a temporal
graph $\mathcal G$, two distinct vertices $s$ and $z$, and two numbers $\delta$
and $k$, is there a $\delta$-restless temporal $s$-$z$ path of length at most
$k$? A temporal path is a path whose edges appear in chronological order and a
temporal path is $\delta$-restless if two consecutive path edges appear at most
$\delta$ time steps apart from each other. Among others, this problem has
applications in neuroscience and epidemiology. While Short Restless Temporal
Path is known to be computationally hard, e.g., it is NP-hard for only three
time steps and W[1]-hard when parameterized by the feedback vertex number of
the underlying graph, it is fixed-parameter tractable when parameterized by the
path length $k$. We improve on this by showing that Short Restless Temporal
Path can be solved in (randomized) $4^{k-d}|\mathcal G|^{O(1)}$ time, where $d$
is the minimum length of a temporal $s$-$z$ path.
</p></div>
    </summary>
    <updated>2022-03-31T22:40:26Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-03-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=613</id>
    <link href="https://tcsplus.wordpress.com/2022/03/30/tcs-talk-wednesday-april-6-jessica-sorrell-ucsd/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, April 6 — Jessica Sorrell, UCSD</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, April 6th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Jessica Sorrell from UCSD will speak about “Reproducibility in Learning” (abstract below). You can reserve a spot as an individual or a group to join us live by […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, April 6th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="https://cseweb.ucsd.edu/~jlsorrel/"><strong>Jessica Sorrell</strong></a> from UCSD will speak about “<em>Reproducibility in Learning</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Registration is <em>not</em> required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: Reproducibility is vital to ensuring scientific conclusions are reliable, but failures of reproducibility have been a major issue in nearly all scientific areas of study in recent decades. A key issue underlying the reproducibility crisis is the explosion of methods for data generation, screening, testing, and analysis, where, crucially, only the combinations producing the most significant results are reported. Such practices (also known as p-hacking, data dredging, and researcher degrees of freedom) can lead to erroneous findings that appear to be significant, but that don’t hold up when other researchers attempt to replicate them.</p>
<p>In this talk, we introduce a new notion of reproducibility for randomized algorithms. This notion ensures that with high probability, an algorithm returns exactly the same output when run with two samples from the same distribution. Despite the exceedingly strong demand of reproducibility, there are efficient reproducible algorithms for several fundamental problems in statistics and learning. We show that any statistical query algorithm can be made reproducible with a modest increase in sample complexity, and we use this to construct reproducible algorithms for finding approximate heavy-hitters and medians. Using these ideas, we give the first reproducible algorithm for learning halfspaces via a reproducible weak learner and a reproducible boosting algorithm. We initiate the study of lower bounds and inherent tradeoffs for reproducible algorithms, giving nearly tight sample complexity upper and lower bounds for reproducible versus non-reproducible SQ algorithms. Finally, we discuss connections to other well-studied notions of algorithmic stability, such as differential privacy.</p>
<p>Joint work with Russell Impagliazzo (UCSD), Rex Lei (UCSD), and Toniann Pitassi (Columbia University), to appear in STOC 2022.</p></blockquote></div>
    </content>
    <updated>2022-03-30T22:35:16Z</updated>
    <published>2022-03-30T22:35:16Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2022-04-01T06:38:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://blog.simons.berkeley.edu/?p=636</id>
    <link href="https://blog.simons.berkeley.edu/2022/03/where-do-q-functions-come-from/" rel="alternate" type="text/html"/>
    <title>Where Do Q-Functions Come From?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">by Sean Meyn (University of Florida) and Gergely Neu (Pompeu Fabra University) One theoretical foundation of reinforcement learning is optimal control, usually rooted in the Markovian variety known as Markov decision processes (MDPs). The MDP model consists of a state process, … <a href="https://blog.simons.berkeley.edu/2022/03/where-do-q-functions-come-from/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>by <a href="https://meyn.ece.ufl.edu/" rel="noreferrer noopener" target="_blank">Sean Meyn</a> (University of Florida) and <a href="http://cs.bme.hu/~gergo/" rel="noreferrer noopener" target="_blank">Gergely Neu</a> (Pompeu Fabra University)</p>



<p>One theoretical foundation of reinforcement learning is optimal control, usually rooted in the Markovian variety known as Markov decision processes (MDPs). The MDP model consists of a state process, an action (or input) process, and a one-step reward that is a function of state and action. The goal is to obtain a policy (function from states to actions) that is optimal in some predefined sense. Chris Watkins introduced the Q-function in the 1980s as part of a methodology for reinforcement learning. Given its importance for over three decades, it is not surprising that the question of the true meaning of <em>Q</em> was a hot topic for discussion during the Simons Institute’s Fall 2020 program on <a href="https://simons.berkeley.edu/programs/rl20" rel="noreferrer noopener" target="_blank">Theory of Reinforcement Learning</a>.</p>



<p>This short note focuses on interactions at the start of the program, and research directions inspired in part by these interactions. To start with, <em>who is Q</em>? Was this code for one of Watkins’ friends at Cambridge? The question was posed early on, which led to an online investigation. The mystery was shattered through a response from Chris: we now know that the letter <em>Q</em> stands for <em>quality</em>, not Quinlyn or Quincy. To discuss further questions and potential answers requires some technicalities.</p>



<p>The discounted-cost optimality criterion is a favorite metric for performance in computer science and operations research, and is the setting of the original Q-function formulation. The definition requires a state process <span class="math inline">\(\{X_k : k\ge 0\}\)</span> and an action (or input) process <span class="math inline">\(\{A_k : k\ge 0\}\)</span>, evolving on respective spaces (which are assumed discrete in this note). There is a controlled transition matrix <span class="math inline">\(P\)</span> that describes dynamics: <span class="math inline">\(X_{k+1}\)</span> is distributed according to <span class="math inline">\(P(\cdot|x,a)\)</span> when <span class="math inline">\(X_k=x\)</span> and <span class="math inline">\(A_k=a\)</span>, for any action sequence that is adapted to the state sequence.</p>



<p>With <span class="math inline">\(\gamma\)</span> denoting the discount factor, the Q-function is the solution to a nonlinear fixed-point equation <span class="math inline">\(T^*Q = Q\)</span> in which <span class="math inline">\(T^*\)</span> is the Bellman operator: <span class="math display">\[\left(T^*Q\right)(x,a) = r(x,a) + \gamma \mathbb{E}_{X’\sim P(\cdot|x,a)}\left[\max_{a’} Q(X’,a’)\right]\]</span> This must hold for each state-action pair <span class="math inline">\((x,a)\)</span>, with the maximum over all possible actions. This is a version of the dynamic programming (DP) equation that has been with us for about seven decades.</p>



<p>The magic of Q-learning, which is based on this DP equation, is that the maximum appears within an expectation. This makes possible the application of Monte Carlo methods to obtain an approximate solution based solely on observations of the actual system to be controlled, or through simulations.</p>



<p>One core idea of modern reinforcement learning (RL) is to find approximate solutions of the DP equation within a function class (e.g., neural networks, as popularized by the deep Q-learning approach of Mnih et al., 2015). While success stories are well-known, useful theory is scarce: we don’t know if a solution exists to an approximate DP equation except in very special settings, and we don’t know if a good approximation will lead to good performance for the resulting policy. We don’t even know if the recursive algorithms that define Q-learning will be stable — estimates may diverge to infinity.</p>



<p>There are many ways to read these negative results, and indeed many articles have been written around this subject. Our own reading is probably among the most radical: without understanding the issues around the existence of solutions to these DP equation approximations or their interpretation, we should search for alternative approximations of dynamic programming suitable for application in RL.</p>



<p>These concerns were raised at Sean Meyn’s <a href="https://www.youtube.com/watch?v=nUaFu3WzW7o" rel="noreferrer noopener" target="_blank">boot camp lecture</a>, where he called on listeners to revisit an alternate foundation of optimal control: the linear programming (LP) approach introduced by Manne (1960) and further developed by Denardo (1970) and d’Epenoux (1963). The message was greeted with enthusiasm from some attendees, including Gergely Neu, who responded, “You have blown my mind!” He had been working on his own formulation of this idea, which became logistic Q-learning (more on this below).</p>



<span id="more-636"/>



<p>The least-squares approach that is central to most traditional RL algorithms (such as DQN) is replaced by an LP, so that we move from <span class="math inline">\(L_2\)</span> to <span class="math inline">\(L_\infty\)</span> approximation. There is ample motivation for this point of view:</p>



<ul><li>
		<p>Lyapunov theory and the theory of inverse dynamic programming provide firm motivation: a good approximation of the DP equation in an <span class="math inline">\(L_\infty\)</span> sense implies strict bounds on closed-loop performance of the resulting policy. You can learn more about this theory in standard RL texts, and Meyn’s new RL monograph to appear this year.</p>
	</li><li>
		<p>The LP approach reframes the DP approximation as a numerical optimization problem rather than a root-finding problem. Existence of a solution is guaranteed under minimal assumptions, even when working with a restricted set of candidate solutions.</p>
	</li></ul>



<p>Despite these advantages, LP-based approaches were left out of the mainstream RL tool kit until recently. One reason may be a glaring gap in the framework: the classical LPs are phrased in terms of state-value functions and not Q-functions! This means it was not obvious how to bring in Monte Carlo methods for algorithm design.</p>



<p>In 2020, we independently discovered the following LP that can be used to cast Q-learning as a constrained optimization problem — the so-called DPLP: <span class="math display">\[\begin{aligned} \mbox{minimize } \qquad &amp; \mathbb{E}_{X\sim\nu_0}\left[\max_{a} Q(X,a)\right] \\ \mbox{subject to } \qquad &amp; Q(x,a) \ge r(x,a) + \gamma \mathbb{E}_{X’\sim P(\cdot|x,a)}\left[\max_{a’} Q(X’,a’)\right]\end{aligned}\]</span> Without any constraints on the function class, the optimizer is the solution to the DP equation. The constraints amount to the DP inequality <span class="math inline">\(Q \ge T^* Q\)</span>, which tells us only <em>negative</em> deviations should be penalized! This is to be contrasted with the commonly used squared Bellman error criterion that penalizes deviations of both signs. In seeking an approximation within a function class <span class="math inline">\(\mathcal{F}\)</span>, it is not at all difficult to ensure a feasible solution exists, even when there is no theory to ensure that an approximate Bellman optimality operator has a fixed point.</p>



<p>There are a variety of possible ways of turning <span> this</span> insight into practical algorithms. One path (taken by Bas-Serrano et al., 2021) is to replace the hard constraint in the LP with a smooth upper bound on the expected constraint violations, resulting in the following unconstrained minimization problem: <span class="math display">\[%  \mbox{minimize } \mathbb{E}_{X\sim\nu_0}[\max_a Q(X,a)] + \frac 1\eta \log 
% \mathbb{E}_{(X,A)\sim\mu}[e^{\eta(r(X,A) + \gamma \mathbb{E}_{X’\sim P(\cdot|X,A)}[\max_{a’} Q(X’,a’)] – Q(X,A))}].
\mbox{minimize} \;\;\;\;\;\;\,\, \mathbb{E}_{X\sim\nu_0}\left[\max_a Q(X,a)\right] + \frac 1\eta \log 
\mathbb{E}_{(X,A)\sim\mu}\left[e^{\eta((T^*Q)(X,A) – Q(X,A))}\right].\]</span> Bas-Serrano et al. (2021) dubbed the above objective function the “logistic Bellman error” and have shown that it enjoys several favorable properties that make it an ideal objective for stochastic optimization. Interactions at the Simons Institute led to the development of another related algorithmic framework called “convex Q-learning,” in which the hard constraints in the LP are replaced with a different continuous approximation (Lu et al., 2021).</p>



<p>It is straightforward to derive practical algorithms from both approaches by replacing the expectations with sample averages. This approach led to good empirical performance, along with the beginnings of theory for convergence and performance bounds.</p>



<p>The LP approach had been simmering in the background over the prior decade: de Farias and Van Roy (2003) proposed an LP approach for approximate dynamic programming, and Mehta and Meyn (2009) contains foundations for convex Q-learning, including a variant of the DPLP for deterministic systems in continuous time.</p>



<p>The new techniques bring challenges that must be overcome before these LP-based approaches can take on the world. One is that these optimization problems can be very poorly conditioned. Another is that the approximation may greatly underestimate the true Q-function when using a poor choice of function class for approximation. We remain optimistic: just like the RL community has successfully developed numerical recipes for addressing the much more severe issue of nonexistence of fixed points to the approximate Bellman operators, we believe that similar know-how can be developed for LP-based methods if theorists and practitioners join forces. Now that we understand where Q comes from, let us decide together where it will go next!</p>



<p><em>Acknowledgment</em>. The authors owe a great debt to the Simons Institute for providing inspiration and interactions throughout the fall program. In particular, our joint work (Lu et al., 2021) was in large part an outcome of discussions at the meeting. The LP approach is one theme of the new textbook (Meyn, 2022), and the impact of the program is seen throughout the book.</p>



<h2 class="unnumbered" id="references">References</h2>



<ul><li> <p>A. S. Manne. Linear programming and sequential decisions. <em>Management Science</em> 6(3), pages 259–267, 1960.</p> </li><li> <p>E. V. Denardo. On linear programming in a Markov decision problem. <em>Management Science</em> 16(5), pages 281-288, 1970.</p> </li><li> <p>F. d’Epenoux. A probabilistic production and inventory problem. <em>Management Science</em> 10(1), pages 98-108, 1963.</p> </li><li> <p>D. P. de Farias and B. Van Roy. The linear programming approach to approximate dynamic programming. <em>Operations Research</em>, 51(6), pages 850–865, 2003.</p> </li><li> <p>J. Bas-Serrano, S. Curi, A. Krause, and G. Neu. Logistic Q-learning. In International Conference on Artificial Intelligence and Statistics, pages 3610-3618, 2021.</p> </li><li> <p>P. G. Mehta and S. P. Meyn. Q-learning and Pontryagin’s minimum principle. In Conference on Decision and Control, pages 3598–3605, Dec. 2009.</p> </li><li> <p>F. Lu, P. G. Mehta, S. P. Meyn, and G. Neu. Convex Q-learning. In American Control Conference, pages 4749–4756. IEEE, 2021.</p> </li><li> <p>S. Meyn. <em>Control Systems and Reinforcement Learning</em>. Cambridge University Press (to appear), Cambridge, 2022. Draft manuscript available at <a href="https://meyn.ece.ufl.edu/2021/08/01/control-systems-and-reinforcement-learning/" rel="noreferrer noopener" target="_blank">https://meyn.ece.ufl.edu/2021/08/01/control-systems-and-reinforcement-learning/</a></p> </li><li> <p>V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. <em>Nature</em> 518 (7540), pages 529-533, 2015.</p> </li></ul></div>
    </content>
    <updated>2022-03-30T16:00:39Z</updated>
    <published>2022-03-30T16:00:39Z</published>
    <category term="General"/>
    <author>
      <name>Preeti Aroon</name>
    </author>
    <source>
      <id>https://blog.simons.berkeley.edu</id>
      <link href="https://blog.simons.berkeley.edu/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://blog.simons.berkeley.edu" rel="alternate" type="text/html"/>
      <subtitle>What's New at the Simons Institute for the Theory of Computing.</subtitle>
      <title>Calvin Café: The Simons Institute Blog</title>
      <updated>2022-04-01T06:38:45Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2022-03-30-asynchronous-agreement-part-three-a-modern-version-of-ben-ors-protocol/</id>
    <link href="https://decentralizedthoughts.github.io/2022-03-30-asynchronous-agreement-part-three-a-modern-version-of-ben-ors-protocol/" rel="alternate" type="text/html"/>
    <title>Asynchronous Agreement Part Three: a Modern version of Ben-Or's protocol</title>
    <summary>In this series of posts, we explore the marvelous world of consensus in the Asynchronous model. In this third post, we present a modern version of Ben-Or’s classic protocol that is part of our new work on Asynchronous Agreement. In the first post we defined the problem and in the...</summary>
    <updated>2022-03-30T11:21:00Z</updated>
    <published>2022-03-30T11:21:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2022-03-31T22:43:04Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2022-03-30-asynchronous-agreement-part-two-ben-ors-protocol/</id>
    <link href="https://decentralizedthoughts.github.io/2022-03-30-asynchronous-agreement-part-two-ben-ors-protocol/" rel="alternate" type="text/html"/>
    <title>Asynchronous Agreement Part Two: Ben-Or's protocol</title>
    <summary>In this series of posts, we explore the marvelous world of consensus in the Asynchronous model. In this post, we present Ben-Or’s classic protocol from 1983. In the next post, we will present a more modern version. In the previous post we defined the problem of Asynchronous Agreement, so without...</summary>
    <updated>2022-03-30T11:16:00Z</updated>
    <published>2022-03-30T11:16:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2022-03-31T22:43:04Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2022-03-30-asynchronous-agreement-part-one-defining-the-problem/</id>
    <link href="https://decentralizedthoughts.github.io/2022-03-30-asynchronous-agreement-part-one-defining-the-problem/" rel="alternate" type="text/html"/>
    <title>Asynchronous Agreement Part One: Defining the problem</title>
    <summary>In this series of posts, we explore the marvelous world of consensus in the Asynchronous model. In this post, we start by simply defining the problem. Recall the FLP theorem: FLP theorem 1985: Any protocol where no two non-faulty parties decide different values in the asynchronous model that is resilient...</summary>
    <updated>2022-03-30T11:11:00Z</updated>
    <published>2022-03-30T11:11:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2022-03-31T22:43:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=19785</id>
    <link href="https://rjlipton.wpcomstaging.com/2022/03/26/waiting-for-self-deriving-cars/" rel="alternate" type="text/html"/>
    <title>Waiting For Self-Deriving Cars</title>
    <summary>Once you trust a self-driving car with your life, you pretty much will trust Artificial Intelligence with anything—Dave Waters. ITProToday src Keith Kirkpatrick is the author of an interesting CACM article on self-driving cars. It is titled “Still Waiting For Self-Driving Cars” and appears in the news section of this month’s issue. Today we discuss […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Once you trust a self-driving car with your life, you pretty much will trust Artificial Intelligence with anything—Dave Waters.</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2022/03/26/waiting-for-self-deriving-cars/kk/" rel="attachment wp-att-19787"><img alt="" class="alignright wp-image-19787" height="110" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/03/kk.jpg?resize=110%2C110&amp;ssl=1" width="110"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">ITProToday <a href="https://www.itprotoday.com/author/Keith-Kirkpatrick">src</a></font></td>
</tr>
</tbody>
</table>
<p>
Keith Kirkpatrick is the author of an interesting CACM <a href="https://cacm.acm.org/magazines/2022/4/259392-still-waiting-for-self-driving-cars/fulltext">article</a> on self-driving cars. It is titled “Still Waiting For Self-Driving Cars” and appears in the news section of this month’s issue. </p>
<p>
Today we discuss why it has been so difficult to get self-driving cars started.</p>
<p>
Kirkpatrick’s article starts:</p>
<blockquote><p><b> </b> <em> <i>Over the past decade, technology and automotive pundits have predicted the “imminent” arrival of fully autonomous vehicles that can drive on public roads without any active monitoring or input from a human driver. Elon Musk has predicted his company Tesla would deliver fully autonomous vehicles by the end of 2021, but he made similar predictions in 2020, 2019, and 2017. Each prediction has fallen flat, largely due to real-world safety concerns, particularly related to how self-driving cars perform in adverse conditions or situations.</i> </em>
</p></blockquote>
<p>
</p><p/><h2> An Issue </h2><p/>
<p/><p>
As printed in the current CACM issue, his article says the following on page 13:</p>
<blockquote><p><b> </b> <em> A potential intermediate solution currently being tested in Germany is to utilize remote drivers to control vehicles. Vay, a Berlin-based startup, has been testing a fleet of remote-controlled electric vehicles in that city and plans to roll out a mobility service in Europe and potentially the U.S. this year. The service will allow customers to order a remote-controlled car and have it drive them to their desired destination; on arrival, they get out if the vehicle and leave it to a human teledriver miles away to either park the vehicle or steer it to the next client. </em>
</p></blockquote>
<p/><p>
I claim this shows the issue that we have with automatic driving systems. There is a typo in the above paragraph. A typo that shows what we are up against in the attempt to make automatic driving systems. </p>
<p>
<b>DO YOU SEE IT?</b></p>
<p/><p><br/>
<a href="https://rjlipton.wpcomstaging.com/2022/03/26/waiting-for-self-deriving-cars/cacmifoftypo/" rel="attachment wp-att-19792"><img alt="" class="aligncenter wp-image-19792" height="36" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/03/CACMifoftypo.jpg?resize=253%2C36&amp;ssl=1" width="253"/></a></p>
<p/><p><br/>
We claim that a typo like this in a CACM article is part of the reason it is so hard to make self-driving cars. Do you agree? Or is the typo not critical? While we’re at it, does the title of this post have a typo?  Read on…</p>
<p>
Ken contributes a more difficult example of this kind, one he used while addressing the same issue of AI advances in his department’s Freshman Seminar. It relates to a later section in Kirkpatrick’s article, where he discusses the issue of</p>
<blockquote><p><b> </b> <em> …testing to ensure vehicle navigation systems understand the complex social interactions that often occur between oncoming and adjacent drivers, or drivers and pedestrians. Generally, if a pedestrian is about to cross or is crossing a street, the driver and pedestrian will make eye contact, and will use nonverbal cues to indicate the direction and speed of their movement. </em>
</p></blockquote>
<p/><p>
Over to Ken:</p>
<p>
</p><p/><h2> Ken’s Example </h2><p/>
<p/><p>
Here is the relevant portion of the White House <a href="https://obamawhitehouse.archives.gov/the-press-office/2012/07/13/remarks-president-campaign-event-roanoke-virginia">transcript</a> of Barack Obama’s July 2012 campaign speech in Roanoke, Virginia—the one with the notorious phrase “you didn’t build that”:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/03/26/waiting-for-self-deriving-cars/youdidntbuildthat/" rel="attachment wp-att-19798"><img alt="" class="aligncenter wp-image-19798" height="229" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/03/YouDidntBuildThat.jpg?resize=363%2C229&amp;ssl=1" width="363"/></a></p>
<p>
The question we could submit to Google—whose own automated-driving efforts have had <a href="https://www.nytimes.com/2015/09/02/technology/personaltech/google-says-its-not-the-driverless-cars-fault-its-other-drivers.html">difficulties</a> with interpreting rules—is:</p>
<blockquote><p><b> </b> <em> What does the word “that” in “you didn’t build that” refer to? </em>
</p></blockquote>
<p/><p>
By rules of linguistic derivation, the answer seems clear:</p>
<ul>
<li>
The word “that” refers to the most recent noun, which is “business.” <p/>
</li><li>
The dashes ‘- -‘ connect two parts that refer to each other. <p/>
</li><li>
The word cannot anyway refer to “roads and bridges” because that is plural whereas “that” is singular, like “business.” <p/>
</li><li>
(The preceding point is not a self-contradiction because in quotes, “roads and bridges” is a phrase—singular.)
</li></ul>
<p>
Now please take a minute-plus to listen to the actual delivery of this part of the speech (you may need to click back from 0:00:08 to 0:00:00 to get the start):</p>
<p/><center><br/>
<br/>
</center><p/>
<p/><p><br/>
First, the transcript is missing a word: Obama actually said “<em>that</em>—you didn’t build <em>that</em>.” Perhaps having a rhetorically emphasized <em>that</em> right after “business” makes the semantic designation even clearer? But then notice that the part “if you’ve got a business” was delivered in a quick and parenthetical manner inside the full phrase “Somebody invested in roads and bridges … you didn’t build that.” </p>
<p>
The interpretation that the “that” refers to <em>infrastructure</em> is supported by the speech’s immediate segue to the Internet, sandwiched around “somebody else made that happen.” Going out to sources, Obama was <a href="https://www.washingtonpost.com/blogs/fact-checker/post/an-unoriginal-obama-quote-taken-out-of-context/2012/07/20/gJQAdG7hyW_blog.html">channeling</a> a point already notoriously made by Elizabeth Warren in a 2011 <a href="https://www.nationalreview.com/the-agenda/elizabeth-warrens-quote-reihan-salam/">speech</a>. But without going out to sources—something we cannot expect an AI to do in the moment—there is Obama’s next sentence (also included in the speech clip):</p>
<blockquote><p><b> </b> <em> “The point is, is that when we succeed, we succeed because of our individual initiative, but also because we do things together.” </em>
</p></blockquote>
<p/><p>
Whether Obama intended to say that people did not build their businesses can still be argued, but that’s not the real point. My first point is that AIs based on current art for ascribing meanings not only would make that ascription, but <em>should</em> do so for overall consistency. And the second, larger, point at the end of my <a href="https://cse.buffalo.edu/~regan/cse199/InternetAndData199np.pdf">slides</a>, where I get to hard AI problems, is this:</p>
<blockquote><p><b> </b> <em> We base our confidence in developing AI systems on Alan Turing’s principle that whatever the human mind can resolve, a computer can be programmed to apprehend and execute. But what if important data, on fine scales, concern matters that our human minds cannot resolve? </em>
</p></blockquote>
<p/><p>
CACM puts in a bubble Kirkpatrick’s quoting another expert that “it will take years and massive compute power to train self-driving systems to understand the nonverbal cues that pass between drivers and pedestrians.”  We believe the task and need of getting cars to <em>self-derive</em> meanings is greater still.  </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Do these issues say anything about the difficulty of getting code right? We think so, but what do you think?</p>
<p/><p><br/>
[some word changes; I seem unable to fix the endpoints of my clip on the C-SPAN website.]</p></font></font></div>
    </content>
    <updated>2022-03-27T02:05:26Z</updated>
    <published>2022-03-27T02:05:26Z</published>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="&quot;you didn't build that&quot;"/>
    <category term="AI"/>
    <category term="CACM"/>
    <category term="hard problems of AI"/>
    <category term="Keith Kirkpatrick"/>
    <category term="linguistics"/>
    <category term="self-driving cars"/>
    <category term="translation"/>
    <category term="typos"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2022-04-01T06:37:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://thmatters.wordpress.com/?p=1365</id>
    <link href="https://thmatters.wordpress.com/2022/03/26/call-for-nominations-stoc-test-of-time-award-deadline-apr-30/" rel="alternate" type="text/html"/>
    <title>Call for Nominations: STOC Test of Time Award (Deadline: Apr 30)</title>
    <summary>The 2022 STOC Test of Time Award recognizes papers published in the Proceedings of the Annual ACM Symposium on Theory of Computing. This is the second year of this annual award. There are three awards, targeting the STOC conferences 10, 20, and 30 years prior to the year in which the award is given. While […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>The 2022 STOC Test of Time Award</strong> recognizes papers published in the Proceedings of the Annual ACM Symposium on Theory of Computing. This is the second year of this annual award. There are three awards, targeting the STOC conferences 10, 20, and 30 years prior to the year in which the award is given. While there is a preference for papers in the target years (and nominations from those years are encouraged), in each of these award categories it is also possible to nominate STOC conference papers published up to four conferences earlier than the targeted conference. Thus, the 2022 STOC Test of Time Awards will be for papers presented at the STOC conferences in 2008-2012, 1998-2002, and 1988-1992. The awards, which will be presented at STOC 2022, include a prize of US $500 per author as well as complimentary registration for all authors who attend the conference at which the award is given.</p>



<h2>Nomination Procedure</h2>



<p>Nominations should be sent to <a rel="noreferrer noopener" target="_blank"><strong>stoc22.tot.award@gmail.com</strong></a><strong> </strong>with a subject line of <strong>“STOC Test of Time Award” </strong>no later than <strong>April 30, 2022</strong>. Nominations should contain an explanation of the impact of the nominated paper(s), including references to follow-on work. A nomination may be accompanied by up to three additional endorsement letters, which may be sent by the endorsers directly to the same email address with the same subject line. Self-nominations are disallowed. </p>



<h2>Selection</h2>



<p>The winners will be selected by a committee appointed by the SIGACT Executive Committee. For 2022 the selection committee consists of Toniann Pitassi (Columbia), Satish Rao (Berkeley), Salil Vadhan (Harvard, chair), Avi Wigderson (Institute for Advanced Study). </p>



<p>In selecting the Test of Time Award winners, the Committee will pay particular attention to long-term impact. This impact can come in many forms, including but not limited to:</p>



<ol><li>Opening up a new area of research</li><li>Introducing new techniques</li><li>Solving a problem of lasting importance</li><li>Stimulating advances in other areas of computer science or in other disciplines.</li></ol>



<p>The committee expects to select exactly one paper for each award. However, when circumstances justify it, up to three may be selected. The committee may consider papers that were not explicitly nominated and gather additional input from experts, but formal nominations are extremely helpful in the committee’s deliberations and strongly encouraged.</p></div>
    </content>
    <updated>2022-03-27T01:53:14Z</updated>
    <published>2022-03-27T01:53:14Z</published>
    <category term="awards"/>
    <category term="Deadlines"/>
    <category term="TCS Community"/>
    <author>
      <name>shuchic</name>
    </author>
    <source>
      <id>https://thmatters.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://thmatters.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://thmatters.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://thmatters.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://thmatters.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theory Matters</title>
      <updated>2022-04-01T06:38:09Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-7417921219645055549</id>
    <link href="http://blog.computationalcomplexity.org/feeds/7417921219645055549/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/03/i-dont-care-about-ketanji-brown.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/7417921219645055549" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/7417921219645055549" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/03/i-dont-care-about-ketanji-brown.html" rel="alternate" type="text/html"/>
    <title>I don't care about Ketanji Brown Jackson's LSAT scores and she does not care about my GRE scores</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Tucker Carlson has asked to see Ketanji Brown Jacksons's LSATs. </p><p>When I applied to College they (not sure who <i>they</i> are) wanted to see my SAT scores. Putting aside the issue of whether the test means anything, they viewed the SATs (and my HS grades and letters from teachers) as a sign of my </p><p>                                                       <i>potential. </i></p><p>When I applied to Grad school they (a different <i>they</i>) wanted to see my GRE scores. Putting aside the issue of whether the test means anything, they viewed the GREs (and my college grades and letters from professors) as a sign of my</p><p>                                                       <i>potential. </i></p><p>When I applied for jobs as a professor they (another <i>they</i>) wanted to see my resume (papers I wrote) and letters from my advisor and others (I think). They did not look at my grades (just as well- I got a B in both compiler design and operating systems. Darling is amazed I even took operating systems). This was probably the oddest of the application processes since they were looking for both</p><p>                                                   <i> potential and achievement.</i></p><p>That is, the evidence that I could do research was that I had done some research. This was before the current  era where grad students had to have x papers in prestige conferences to get a job at a top y school. The letter from my advisor may well have spoken of my potential. </p><p>When I went up for tenure ALL they cared about was PAPERS (and letters saying they were good papers), and some teaching and service. It was based just  on </p><p>                                                          <i>achievement.</i></p><p>A wise man named Lance Fortnow once told me:</p><p><i>The worst thing a letter of recommendation for a tenure case can say is `this person has great potential'</i></p><p><br/></p><p>It would have been rather odd for Tucker Carlson to ask to see my SAT scores or GRE scores or by HS, College, or Grad School grades as a criteria for Tenure. Those tests and those grades are there to measure potential to DO something, whereas if you are going up for tenure or a Supreme Court seat, you've already DONE stuff. </p><p>After I got into grad school one of my first thoughts was</p><p><i>Nobody will ever want to see my GRE's again. ( I was right.) </i></p><p>After KBJ got into Law School she might have thought</p><p><i>Nobody will ever want to see my LSAT scores again. (She was wrong.)</i></p><p><i><br/></i></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2022-03-27T00:10:00Z</updated>
    <published>2022-03-27T00:10:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2022-03-29T23:22:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://dstheory.wordpress.com/?p=122</id>
    <link href="https://dstheory.wordpress.com/2022/03/25/wednesday-april-6th-2022-shuchi-chawla-from-ut-austin/" rel="alternate" type="text/html"/>
    <title>Wednesday April 6th 2022 — Shuchi Chawla from UT Austin</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The third Foundations of Data Science virtual talk of this year will take place on Wednesday, March 6th at 1:00 PM Pacific Time (16:00 Eastern Time, 22:00 Central European Time, 20:00 UTC). Shuchi Chawla from UT Austin will speak about “Pandora’s Box with Correlations: Learning and Approximation.” Please register here to join the virtual talk. Abstract:<a class="more-link" href="https://dstheory.wordpress.com/2022/03/25/wednesday-april-6th-2022-shuchi-chawla-from-ut-austin/">Continue reading <span class="screen-reader-text">"Wednesday April 6th 2022 — Shuchi Chawla from UT Austin"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The third <a href="https://sites.google.com/view/dstheory/home" rel="noreferrer noopener" target="_blank">Foundations of Data Science</a> virtual talk of this year will take place on <strong>Wednesday, March 6th</strong> at <strong>1:00 PM Pacific Time</strong> (16:00 Eastern Time, 22:00 Central European Time, 20:00 UTC). <a href="https://www.cs.utexas.edu/people/faculty-researchers/shuchi-chawla">Shuchi Chawla</a> from<strong> UT Austin</strong> will speak about “Pandora’s Box with Correlations: Learning and Approximation.<em>”</em></p>



<p><a href="https://sites.google.com/view/dstheory" rel="noreferrer noopener" target="_blank">Please register here to join the virtual talk.</a></p>



<p><strong>Abstract</strong>:  In the Pandora’s Box problem, the algorithm is provided with a number of boxes with unknown (stochastic) rewards contained inside them. The algorithm can open any box at some cost, discover the reward inside, and based on these observations can choose one box and keep the reward contained in it. Given the distributions from which the rewards are drawn, the algorithm must determine an order in which to open the boxes as well as when to stop and accept the best reward found so far. In general, an optimal algorithm may make both decisions adaptively based on instantiations observed previously. The Pandora’s Box problem and its extensions capture many kinds of optimization problems with stochastic input where the algorithm can obtain instantiations of input random variables at some cost. Previous work on these problems assumes that different random variables in the input are distributed independently. As such it does not capture many real-world settings. In this work, we provide the first algorithms for Pandora’s Box-type problems with correlations. In the independent setting, optimal algorithms are non-adaptive and based on the notion of the Gittins index. These techniques fail to extend to the correlated case. We assume that the algorithm has access to samples drawn from the joint distribution on input and provide solutions that require few samples; are computationally efficient; and guarantee approximate optimality. <br/>This is joint work with Evangelia Gergatsouli, Yifeng Teng, Christos Tzamos, and Ruimin Zhang and appeared in FOCS’20.</p>



<p> The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>
    </content>
    <updated>2022-03-25T23:14:50Z</updated>
    <published>2022-03-25T23:14:50Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>dstheory</name>
    </author>
    <source>
      <id>https://dstheory.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://dstheory.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://dstheory.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://dstheory.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://dstheory.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Foundation of Data Science – Virtual Talk Series</title>
      <updated>2022-04-01T06:39:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/03/23/lecturer-in-theoretical-computer-science-at-university-of-auckland-apply-by-may-15-2022/</id>
    <link href="https://cstheory-jobs.org/2022/03/23/lecturer-in-theoretical-computer-science-at-university-of-auckland-apply-by-may-15-2022/" rel="alternate" type="text/html"/>
    <title>Lecturer in Theoretical Computer Science at University of Auckland (apply by May 15, 2022)</title>
    <summary>We seek two early-career top candidates working in any subfield of TCS, preferably in algorithms and data structures, probabilistic computation, quantum and algorithmic information theory, computational biology, and computational complexity. Website: https://jobs.smartrecruiters.com/TheUniversityOfAuckland/743999813405096-lecturer-in-theoretical-computer-science Email: g.russello@auckland.ac.nz</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We seek two early-career top candidates working in any subfield of TCS, preferably in algorithms and data structures, probabilistic computation, quantum and algorithmic information theory, computational biology, and computational complexity.</p>
<p>Website: <a href="https://jobs.smartrecruiters.com/TheUniversityOfAuckland/743999813405096-lecturer-in-theoretical-computer-science">https://jobs.smartrecruiters.com/TheUniversityOfAuckland/743999813405096-lecturer-in-theoretical-computer-science</a><br/>
Email: g.russello@auckland.ac.nz</p></div>
    </content>
    <updated>2022-03-23T23:12:08Z</updated>
    <published>2022-03-23T23:12:08Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-04-01T06:37:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/041</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/041" rel="alternate" type="text/html"/>
    <title>TR22-041 |  Boolean functions with small approximate spectral norm | 

	Hamed Hatami, 

	TsunMing Cheung, 

	Rosie Zhao, 

	Itai Zilberstein</title>
    <summary>The sum of the absolute values of the Fourier coefficients of a function $f:\mathbb{F}_2^n \to \mathbb{R}$ is called the spectral norm of $f$.    Green and Sanders' quantitative version of  Cohen's idempotent theorem states that if the spectral norm of $f:\mathbb{F}_2^n \to \{0,1\}$ is at most $M$, then the support of $f$ belongs to the ring of sets generated by at most $\ell(M)$ cosets,  where $\ell(M)$ is a constant that only depends on $M$.
    
    We prove that the above statement can be generalized to approximate spectral norms if and only if the support of $f$ and its complement satisfy a certain arithmetic connectivity condition. In particular, our theorem provides a new proof of the quantitative Cohen's theorem for $\mathbb{F}_2^n$.</summary>
    <updated>2022-03-23T08:19:23Z</updated>
    <published>2022-03-23T08:19:23Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-04-01T06:37:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/040</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/040" rel="alternate" type="text/html"/>
    <title>TR22-040 |  Should decisions in QCDCL follow prefix order? | 

	Benjamin Böhm, 

	Tomáš Peitl, 

	Olaf Beyersdorff</title>
    <summary>Quantified conflict-driven clause learning (QCDCL) is one of the main solving approaches for quantified Boolean formulas (QBF). One of the differences between QCDCL and propositional CDCL is that QCDCL typically follows the prefix order of the QBF for making decisions.
We investigate an alternative model for QCDCL solving where decisions can be made in arbitrary order. The resulting system QCDCL-ANY is still sound and terminating, but does not necessarily allow to always learn asserting clauses or cubes. To address this potential drawback, we additionally introduce two subsystems that guarantee to always learn asserting clauses (QCDCL-UNI-ANY) and asserting cubes (QCDCL-EXI-ANY), respectively.
We model all four approaches by formal proof systems and show that QCDCL-UNI-ANY is exponentially better than QCDCL on false formulas, whereas QCDCL-EXI-ANY is exponentially better than QCDCL on true QBFs. Technically, this involves constructing specific QBF families and showing lower and upper bounds in the respective proof systems.
We complement our theoretical study with some initial experiments that confirm our theoretical findings.</summary>
    <updated>2022-03-22T10:18:30Z</updated>
    <published>2022-03-22T10:18:30Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-04-01T06:37:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=22518</id>
    <link href="https://gilkalai.wordpress.com/2022/03/21/combinatorial-convexity-a-wonderful-new-book-by-imre-barany/" rel="alternate" type="text/html"/>
    <title>Combinatorial Convexity: A Wonderful New Book by Imre Bárány</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">A few days ago I received by mail Imre Bárány’s new book Combinatorial Convexity. The book presents Helly-type theorems and other results in convexity with combinatorial flavour. The choice of material and the choice of proofs is terrific and it … <a href="https://gilkalai.wordpress.com/2022/03/21/combinatorial-convexity-a-wonderful-new-book-by-imre-barany/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A few days ago I received by mail Imre Bárány’s new book <em>Combinatorial Convexity</em>. The book presents Helly-type theorems and other results in convexity with combinatorial flavour. The choice of material and the choice of proofs is terrific and it is an ideal book for a course for graduate students and advanced undergraduates.  Congratulations, Imre!</p>
<p>The <a href="https://bookstore.ams.org/ulect-77/">AMS page</a> gives the following description</p>
<blockquote><p><span style="color: #0000ff;"><em>This book is about the combinatorial properties of convex sets, families of convex sets in finite dimensional Euclidean spaces, and finite points sets related to convexity. This area is classic, with theorems of Helly, Carathéodory, and Radon that go back more than a hundred years. At the same time, it is a modern and active field of research with recent results like Tverberg’s theorem, the colourful versions of Helly and Carathéodory, and the <span class="MathJax" id="MathJax-Element-1-Frame"><span class="math" id="MathJax-Span-1"><span class="mrow" id="MathJax-Span-2"><span class="mo" id="MathJax-Span-3">(</span><span class="mi" id="MathJax-Span-4">p</span><span class="mo" id="MathJax-Span-5">,</span><span class="mi" id="MathJax-Span-6">q</span><span class="mo" id="MathJax-Span-7">)</span></span></span></span> theorem of Alon and Kleitman. As the title indicates, the topic is convexity and geometry, and is close to discrete mathematics. The questions considered are frequently of a combinatorial nature, and the proofs use ideas from geometry and are often combined with graph and hypergraph theory. </em></span></p>
<p><span style="color: #0000ff;"><em>The book is intended for students (graduate and undergraduate alike), but postdocs and research mathematicians will also find it useful. It can be used as a textbook with short chapters, each suitable for a one- or two-hour lecture. Not much background is needed: basic linear algebra and elements of (hyper)graph theory as well as some mathematical maturity should suffice.</em></span></p></blockquote>
<p>Here is also <a href="https://www.amazon.com/Combinatorial-Convexity-University-Lecture-77/dp/1470467097">the Amzon page</a>.</p>
<p><img alt="ib2" class="alignnone size-full wp-image-22521" src="https://gilkalai.files.wordpress.com/2022/03/ib2.png?w=640"/></p></div>
    </content>
    <updated>2022-03-21T07:55:52Z</updated>
    <published>2022-03-21T07:55:52Z</published>
    <category term="Combinatorics"/>
    <category term="Convexity"/>
    <category term="Geometry"/>
    <category term="Imre Barany"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2022-04-01T06:37:50Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-7134833319825279941</id>
    <link href="http://blog.computationalcomplexity.org/feeds/7134833319825279941/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/03/do-you-want-to-be-sigact-news-book.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/7134833319825279941" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/7134833319825279941" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/03/do-you-want-to-be-sigact-news-book.html" rel="alternate" type="text/html"/>
    <title>Do you want to be the SIGACT NEWS book review editor?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I ran the SIGACT Book Review Column from 1997-2015 (18 years). You can find all of my columns, plus reviews I did for Fred, <a href="http://www.cs.umd.edu/~gasarch/bookrev/bookrev.html">here</a>.</p><p>When I handed it off to Fred Green I gave him this sage advice:</p><p>                   <i>Nobody should do this kind of job for more than about 5 years.</i></p><p>He ran the SIGACT Book Review Column since the end of 2015. You can find some of his columns <a href="http://mathcs.clarku.edu/~fgreen/SIGACTReviews/bookrev/bookrev.html">here</a>.</p><p>Fred is taking my advice and looking for a successor.</p><p>SO, this blog is a call to ask</p><p>                 DO YOU WANT TO BE THE SIGACT NEWS BOOK REVIEW EDITOR?</p><p>If so then email</p><p>Fred: fgreen@clarku.edu</p><p><br/></p><p>DO NOT BE SHY! I suspect he won't get many applicants, so if you want the job its probably yours.</p><p><br/></p><p>PROS</p><p>1) You get to skim lots of books and read some of  them.</p><p>2) You get some free books.</p><p>3) You get plugged into the book community (this helped me when I wrote my two books).</p><p>4) You'll have two Veteran Book Review Editors happy to review for you.</p><p>5) You get to decide the direction the column goes in.</p><p>Both Fred and I did mostly CS theory books. However:</p><p>a) I did more combinatorics, educational, history, and Computers &amp; Society books than usual.</p><p>b) Fred did more Number Theory and Physics than usual.</p><p>(Since I did the job 18 years and Fred for 6, its not clear what <i>usual</i> means.) </p><p><br/></p><p>CONS</p><p>1) You have to get out a book review column 4 times a year.</p><p>2) You have to find reviewers for books and then email them when the reviews are due.</p><p>(I think Fred is still waiting for me to review a Biography of Napier. Oh well. On the other hand, I was the one who liked having history books, which may explain why Fred never hassled me about it.) </p><p><br/></p><p>ADVICE</p><p>Prob should be done by someone who already has Tenure. While seeing and skimming thosebooks is GOOD for your research career, and good in the long-termsomeone pre-tenure really needs to get papers out in the short term. Also, when you get a book think about who might be good to review it--- don't take on to many yourself. </p><p><br/></p><p>PARTING GIFT OR WELCOME GIFT</p><p>In a recent column I had a review of a 5-book set from the LESS WRONG blog. I amcurrently working on a review of a 4-book set set from the LESS WRONG blog. This willeither be a parting gift for Fred or a Welcome gift to his successor.</p><p><br/></p></div>
    </content>
    <updated>2022-03-21T03:16:00Z</updated>
    <published>2022-03-21T03:16:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2022-03-29T23:22:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8295</id>
    <link href="https://windowsontheory.org/2022/03/20/cool-projects-from-my-crypto-class/" rel="alternate" type="text/html"/>
    <title>Cool projects from my crypto class</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This fall, I taught my course CS 127: Cryptography, based on my lecture notes: “An intensive introduction to cryptography”. This is a course that starts with no background knowledge, and gets to advanced concepts including lattice-based (aka “post quantum”) encryption, fully homomorphic encryption, zero-knowledge proofs, multiparty secure computation, software obfuscation, quantum computing and crypto, and … <a class="more-link" href="https://windowsontheory.org/2022/03/20/cool-projects-from-my-crypto-class/">Continue reading <span class="screen-reader-text">Cool projects from my crypto class</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This fall, I taught my course <a href="https://cs127.boazbarak.org/">CS 127: Cryptography</a>, based on my lecture notes: <a href="https://intensecrypto.org/">“An intensive introduction to cryptography”</a>. This is a course that starts with no background knowledge, and gets to advanced concepts including lattice-based (aka “post quantum”) encryption, fully homomorphic encryption, zero-knowledge proofs, multiparty secure computation, software obfuscation, quantum computing and crypto, and more. </p>



<p><a href="https://cs127.boazbarak.org/projects/">As in previous years</a>, I had an fantastic group of students, several of whom produced impressive course projects. These include the following: </p>



<p><strong>Gavin Uberti</strong>, <strong>Kevin Luo</strong>, <strong>Oliver Cheng,</strong>  and <strong>Wittmann Goh</strong> <a href="https://arxiv.org/abs/2112.04581">implemented Witness Encryption</a>. Witness encryption is a cool concept whereby you can encrypt a secret X (for example the private key corresponding to a bitcoin wallet) so that people can decrypt X if and only if they can find a solution to some puzzle P. You can do this <em>even if you don’t know a solution yourself!</em> So for example you could use this to offer an automatically paying reward for a formal proof of the Reimann Hypothesis, or as they did, offer 2270  Satoshis to anyone solving this Soduko puzzle:</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2022/03/image.png"><img alt="" class="wp-image-8301" src="https://windowsontheory.files.wordpress.com/2022/03/image.png?w=503"/></a></figure>



<p><strong>Simas Sakenis</strong> wrote a <a href="https://windowsontheory.files.wordpress.com/2022/03/simas_project.pdf">survey of proofs of stake in cryptocurrencies</a>,  providing a uniform formalizaiton of proofs of work and proofs of stake, and explaining the difference. </p>



<p><strong>Michael Kiestra</strong> and <strong>Beatrice Nash</strong> proposed  <a href="https://windowsontheory.files.wordpress.com/2022/03/kiestra_nash.pdf">CLAMBAKE</a>,  a protocol that uses broadcast encryption and Yao’s garbled circuits to achieve a privacy-preserving protocol for facilitating access to controlled resources such as university buildings.</p>



<p>(There were more projects in the course, but some students preferred not to post these publicly since they are still working on them; I will update this post with more projects if appropriate.)</p>



<p/></div>
    </content>
    <updated>2022-03-20T21:11:37Z</updated>
    <published>2022-03-20T21:11:37Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2022-04-01T06:38:13Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/03/20/postdoc-at-hamburg-university-of-technology-apply-by-april-8-2022/</id>
    <link href="https://cstheory-jobs.org/2022/03/20/postdoc-at-hamburg-university-of-technology-apply-by-april-8-2022/" rel="alternate" type="text/html"/>
    <title>Postdoc at Hamburg University of Technology (apply by April 8, 2022)</title>
    <summary>The Institute for Algorithms and Complexity at Hamburg University of Technology is seeking a postdoc to work on algorithms for combinatorial optimization and operations research (pay level TV-L 14). These can be approximation algorithms, parameterized algorithms, dynamic algorithms, streaming algorithms, or related. Join our international team to solve some of the most intractable problems! Website: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Institute for Algorithms and Complexity at Hamburg University of Technology is seeking a postdoc to work on algorithms for combinatorial optimization and operations research (pay level TV-L 14). These can be approximation algorithms, parameterized algorithms, dynamic algorithms, streaming algorithms, or related. Join our international team to solve some of the most intractable problems!</p>
<p>Website: <a href="https://stellenportal.tuhh.de/jobposting/6816c1195abac5e2e9f2c7f1a72506004fd3ead0">https://stellenportal.tuhh.de/jobposting/6816c1195abac5e2e9f2c7f1a72506004fd3ead0</a><br/>
Email: algo@tuhh.de</p></div>
    </content>
    <updated>2022-03-20T20:42:51Z</updated>
    <published>2022-03-20T20:42:51Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-04-01T06:37:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/03/20/ukraine-student-postdoc-senior-research-fellows-at-ben-gurion-university-apply-by-december-6-2022/</id>
    <link href="https://cstheory-jobs.org/2022/03/20/ukraine-student-postdoc-senior-research-fellows-at-ben-gurion-university-apply-by-december-6-2022/" rel="alternate" type="text/html"/>
    <title>Ukraine  Student/ Postdoc/  Senior Research  Fellows at Ben-Gurion University (apply by December 6, 2022)</title>
    <summary>If you are a student/scholar affected by the war in Ukraine, BGU offers an emergency scholarship, furthermore, I also have immediately available funds for those who are interested in doing research in Theoretical computer science or Error-Correcting Codes. Website: https://www.tfaforms.com/399172?fbclid=IwAR1AW_tvfxoC6yA7EXcptO5tr3SjOM0dAgngz6v6WtlMfHr1ghDwXC0MMt4, https://www.cs.bgu.ac.il/~klim/ Email: klimefrem@gmail.com</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>If you are a student/scholar affected by the war in Ukraine, BGU offers an emergency scholarship, furthermore, I also have immediately available funds for those who are interested in doing research in Theoretical computer science or Error-Correcting Codes.</p>
<p>Website: <a href="https://www.tfaforms.com/399172?fbclid=IwAR1AW_tvfxoC6yA7EXcptO5tr3SjOM0dAgngz6v6WtlMfHr1ghDwXC0MMt4">https://www.tfaforms.com/399172?fbclid=IwAR1AW_tvfxoC6yA7EXcptO5tr3SjOM0dAgngz6v6WtlMfHr1ghDwXC0MMt4</a>, <a href="https://www.cs.bgu.ac.il/~klim/">https://www.cs.bgu.ac.il/~klim/</a><br/>
Email: klimefrem@gmail.com</p></div>
    </content>
    <updated>2022-03-20T16:54:04Z</updated>
    <published>2022-03-20T16:54:04Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-04-01T06:37:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=606</id>
    <link href="https://tcsplus.wordpress.com/2022/03/19/tcs-talk-wednesday-march-23-shuichi-hirahara-national-institute-of-informatics-japan/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, March 23 — Shuichi Hirahara, National Institute of Informatics, Japan</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, March 23rd at 6:00 PM Eastern Time (1:00 PM Pacific Time, 13:00 Central European Time, 22:00 UTC [note the unusual time!]). Shuichi Hirahara from the National Institute of Informatics, Japan will speak about “Excluding PH Pessiland” (abstract below). You can reserve a spot as an […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, March 23rd at 6:00 PM Eastern Time<strong> (1:00 PM Pacific Time, 13:00 Central European Time, 22:00 UTC [note the unusual time!]).</strong> <a href="https://researchmap.jp/shuichi.hirahara/?lang=english"><strong>Shuichi Hirahara</strong></a> from the National Institute of Informatics, Japan will speak about “<em>Excluding PH Pessiland</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Registration is <em>not</em> required to attend the interactive talk, and the link will be posted on the website the day prior to the talk; however, by registering in the form, you will receive a reminder, along with the link. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: Pessiland is the worst of Impagliazzo’s five possible worlds: it is a world where NP is hard on average and pseudorandom generators do not exist. Excluding Pessiland (i.e., showing the equivalence between average-case hardness of NP and the existence of pseudorandom generators) is one of the most important open questions in theoretical computer science. In this talk, we propose to consider PH (Polynomial Hierarchy) variants of Impagliazzo’s five possible worlds. Our main result is to unconditionally rule out PH variants of Pessiland. I will also mention recent progress on excluding PH Heuristica: average-case hardness of PH follows from exponential worst-case hardness of PH.</p>
<p>Based on joint works with Rahul Santhanam.</p></blockquote></div>
    </content>
    <updated>2022-03-19T05:22:43Z</updated>
    <published>2022-03-19T05:22:43Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2022-04-01T06:38:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=998</id>
    <link href="https://emanueleviola.wordpress.com/2022/03/17/focs-2022/" rel="alternate" type="text/html"/>
    <title>FOCS 2022</title>
    <summary>I am on the FOCS 2022 Program Committee, and I am overjoyed that the PC meeting will be virtual. Hopefully, the days are over when the program chair can brush aside all cost-benefit considerations, impose their backyard on far-away scholars who need service items, and then splash their offspring at the welcome party. I am […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I am on the <a href="https://focs2022.eecs.berkeley.edu/cfp.html">FOCS 2022 </a>Program Committee, and I am overjoyed that the PC meeting will be virtual.  Hopefully, the days are over when the program chair can brush aside all cost-benefit considerations, impose their backyard on far-away scholars who need service items, and then splash their offspring at the welcome party.</p>



<p>I am also overjoyed that we will be implementing double-blind reviews.  This issue has been discussed and ridiculed at length.  Admittedly, it makes it harder to adhere to Leonid Levin’s 1995 influential <em><a href="https://dl.acm.org/doi/10.1145/202840.606487">STOC Criteria</a></em>.  For example, if a reviewer wanted to trash a paper based on the fact that the authors are not in the position to judge their own work, now they’ll have to check online for talks or preprints to know who the authors are.  Given the volume of reviews, it’s reasonable to expect that in some cases the reviewer won’t be able to conclusively exclude that a letter-writer is among the authors.  In such a situation they can resort to writing a very long, thorough, and competent review whose most significant digit is the STOC/FOCS death sentence: <em>weak accept</em>.</p>



<p>No, I actually do have something more constructive to say about this.  I was — as they say — privileged to serve on many NSF panels.  As an aside, it’s interesting that there the track-record of the investigators <em>is </em>a key factor in the decision; in fact, according to many including myself, it should carry even more weight, rather than forcing investigators to fill pages with made-up directions most of which won’t pan out.  But that’s another story; what is relevant for this post is that each panel begins with a quick “de-biasing” briefing, which I actually enjoy and from which I learnt something.  For example, there’s a classic experiment where the ratio of females hired as musicians increases if auditions hide the performer behind a tent and make them walk in on carpet so you can’t tell what shoes they are wearing.  Similar experiments exist that hide names from the folders of applicants, etc.  What I propose is to do a similar thing when reviewing papers.  That is, start with a de-biasing briefing: tell reviewers to ask themselves whether their attitude towards a paper would be different if:</p>



<ol><li>The authors of this paper/previous relevant work were ultra-big shots, or</li><li>The authors of this paper/previous relevant work were hapless nobodies, or</li><li>The proofs in this paper could be simplified dramatically to the point that even I understand them, or</li><li>This result came with a super-complicated proof which I can’t even begin to follow, or</li></ol>



<p>What other questions would be good to ask?</p></div>
    </content>
    <updated>2022-03-17T14:27:29Z</updated>
    <published>2022-03-17T14:27:29Z</published>
    <category term="Uncategorized"/>
    <category term="utopia-tcs"/>
    <author>
      <name>Manu</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2022-04-01T06:38:24Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3213642318918472638</id>
    <link href="http://blog.computationalcomplexity.org/feeds/3213642318918472638/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/03/the-war-and-math.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/3213642318918472638" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/3213642318918472638" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/03/the-war-and-math.html" rel="alternate" type="text/html"/>
    <title>The War and Math</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>During the early parts of the cold war of the 20th century, we saw two almost independent developments of computational complexity, in the west and in the then USSR. There was little communication between the two groups, and countless theorems proven twice, most notably the seminal NP-complete papers of Cook and Levin. To understand more, I recommend the two articles about the early days of complexity by <a href="https://doi.org/10.1109/MAHC.1981.10005">Juris Hartmanis</a> and by <a href="https://doi.ieeecomputersociety.org/10.1109/MAHC.1984.10036">Boris Trakhtenbrot</a>.</p><p>Russia's invasion and relentless bombing in Ukraine have quickly separated the east and the west again. </p><p>Our first concern needs to be with Ukraine and its citizens. We hope for a quick end to this aggression and Ukraine remaining a free and democratic country. Ukrainian cities have undergone massive damage, and even in the best possible outcome it will take years if not decades to fully rebuild the country. </p><p>Terry Tao has been <a href="https://terrytao.wordpress.com/2022/03/02/resources-for-displaced-mathematicians/">collecting resources</a> for displaced mathematicians due to the crisis.</p><p>We've cut off ties with Russia institutions. In our world, major events to be held in Russia, including the <a href="https://www.mathunion.org/">International Congress of Mathematics</a> and the <a href="https://logic.pdmi.ras.ru/csr2022/">Computer Science in Russia</a> conference are being moved online. I was invited to workshops in St Petersburg in 2020 and 2021, both cancelled due to Covid, and was looking forward to one in 2022, which if it happens, will now happen without me. </p><p>The music world has has cancelled some stars, most notably <a href="https://www.nytimes.com/2022/03/02/arts/music/ukraine-putin-valery-gergiev-anna-netrebko.html">Valery Gergiev and Anna Netrebko</a>, due to their close ties to Putin. It's rare that we do the same to mathematicians for political reasons though <a href="https://blog.computationalcomplexity.org/2019/06/imus-non-controversial-changing-name-of.html">not unheard of</a>. I suspect most of our colleagues in Russia oppose the war in Ukraine, or would if they had accurate information of what was going on. I have several Russian friends and colleagues including <a href="https://blog.computationalcomplexity.org/2019/06/compressing-in-moscow.html">two I travelled to Moscow in 2019 to honor</a> and would hate to be disconnected from them.</p><p>It's way too early to know how this will all play out. Will we see a quick Russian retreat? Not likely. Will we see a situation that sees a mass migration of Ukranian and Russian mathematicians and computer scientists to Europe and North America, like in the 1990's? Possibly. We will see a repeat of the cold war, disconnected internets and science on both sides happening in isolation? I hope not but we can't rule it out.</p></div>
    </content>
    <updated>2022-03-17T13:28:00Z</updated>
    <published>2022-03-17T13:28:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2022-03-29T23:22:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=19755</id>
    <link href="https://rjlipton.wpcomstaging.com/2022/03/17/meta-wishes/" rel="alternate" type="text/html"/>
    <title>Meta Wishes</title>
    <summary>When faced with two choices, simply toss a coin. It works because in that brief moment when the coin is in the air, you suddenly know what you are hoping for. Neil L. is a leprechaun. He has been visiting me or Ken once every year since we started GLL. We had never seen a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>When faced with two choices, simply toss a coin. It works because in that brief moment when the coin is in the air, you suddenly know what you are hoping for.</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2022/03/17/meta-wishes/neill-2/" rel="attachment wp-att-19757"><img alt="" class="alignright wp-image-19757" height="153" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/03/NeilL.jpg?resize=146%2C153&amp;ssl=1" width="146"/></a></p>
<p><font color="darkgreen"/></p><font color="darkgreen">
<p>
Neil L. is a leprechaun. He has been visiting me or Ken once every year since we started GLL. We had never seen a leprechaun before we began the blog—there must be some connection. </p>
<p>
Today we want to share the experience we each had with him this morning of St. Patrick’s Day.<br/>
<span id="more-19755"/></p>
<p>
That’s right—Neil visited both of us separately. Four years ago, when I had major heart surgery on St. Patrick’s Eve, Neil <a href="https://rjlipton.wpcomstaging.com/2018/03/17/leprechauns-know-what-it-feels/">came</a> to my previous New York apartment, grabbed a page of my notes, and took it to Ken. The past two years, Ken was on Zoom with me when Neil appeared. This time, Neil had a bone to pick with Ken—after posing a problem to me.</p>
<p>
</p><p/><h2> Neil’s Problem </h2><p/>
<p/><p>
Kathryn, my dear wife, and I are still in our new midtown Manhattan apartment after my procedure two weeks ago. She went to sleep in the bedroom, but I stayed up awaiting Neil’s arrival. I must have dozed off, but jolted awake to his laugh and the waft of his pipe’s green smoke.</p>
<p>
Neil said hi and explained right away that he had a problem. “A classic problem that leprechauns have faced forever.” I nodded to him and rubbed my eyes to be awake enough to listen. Neil continued:</p>
<blockquote><p><b> </b> <em> The other day I was minding me business and I fell into a trap. Nasty fall. The trapper was an old foe of mine. She told me she had three wishes. And I was bound to grant them as usual. Sae much, sae normal. But then she breached the rules. </em>
</p></blockquote>
<p>
</p><p/><h2> No More Wishes </h2><p/>
<p/><p>
Now I knew the rules of wishes, so Neil did not have to tell me: Must be about something contingent, not love or death, no self-transmogrification, and most of all—well, let’s hear it from the genie in Disney’s <a href="https://en.wikipedia.org/wiki/Aladdin_(1992_Disney_film)">Aladdin</a>:</p>
<blockquote><p><b> </b> <em> “Three wishes, three. Uno, dos, tres, no substitutions, exchanges, or refunds, and ixnay on the wishing for more wishes.” </em>
</p></blockquote>
<p/></font><p><font color="darkgreen">
Neil picked up his story: “Her first two wishes were:<br/>
</font> </p>
<ol>
<li>
A pot of gold coins. <p/>
</li><li>
Another pot of gold.
</li></ol>
<p><font color="darkgreen"/></p><font color="darkgreen">
<p>
Those were fair and I showed I could handle them forthwith. But then she asked for: </p>
<blockquote><p><b> </b> <em> ‘Please ten chirag oil lamps, each with a genie who can grant three wishes.’ </em>
</p></blockquote>
<p/><p>
This was terrible. It sent me to the lore. Ye cannot ask your servant for more wishes. Ye cannot involve another leprechaun. But lamps of themselves are just objects. That a <em>chirag</em> lamp has a genie is like an oyster has a pearl. Even if half the lamps be duds, that still  compounds the wishes—and she wished for ten that were not duds.”</p>
<p>
</p><p/><h2> Math and Myth </h2><p/>
<p/><p>
Neil puffed a few more times, as if really expecting an answer from me. He even prompted, “What do ye think?” </p>
<p>
That sent me into befuddlement. Usually I try to engage Neil on a <em>math</em> problem, to trick him into telling the answer to Riemann or factoring or P=NP. But despite the “<img alt="{&gt; 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3E+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>” aspect, this wasn’t math—it was more about <em>myth</em>. Math I can take seriously, but with weighty matters personal and worldwide, I did not want to play “meta” games. </p>
<p>
Neil read my mind: “Nay, I assure ye—it be really about math. Even the Riemann—”</p>
<p>
That made me think: if Neil knew that it had real math content, he must know the answer already. No sense groping for it groggily. I just retorted: “I don’t want to guess the answer. Please just tell me.”</p>
<p>
“Ye know there be only one way I ever tell ye answers…”</p>
<p>
“Sure. OK.” I really wanted to join Kathryn for some sleep.</p>
<p>
I blinked as Neil vanished in green light. Only his pipe smoke remained, and it curled around into a shape I couldn’t place at first. It took awhile to become sharp enough that I could tell what it was:</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/03/17/meta-wishes/greenzeta/" rel="attachment wp-att-19758"><img alt="" class="aligncenter wp-image-19758" height="123" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/03/GreenZeta.png?resize=60%2C123&amp;ssl=1" width="60"/></a></p>
<p/><h2> Myth and Math </h2><p/>
<p/><p>
I, Ken, shall tell the rest. Ordinarily, I would have been eager to engage Neil again about the NCAA basketball tournaments. But I too am too touched by the same weighty matters, and quite forgot the day.</p>
<p>
My unawareness did not matter because Neil apparated and instantly started confronting me: “What gave ye the mickey to <a href="https://rjlipton.wpcomstaging.com/2022/03/14/are-these-the-last-digits-of-pi/">write</a> of mathematics being ’emergent’? Ye dinna even define it.”</p>
<p>
I had to concede I’d not only been vague but had analogized it too to two senses of “emergent” in philosophy. I started to explain: “It’s like Albert Einstein’s famous question, ‘Did God have a choice?’ among physical laws. Now about math…”</p>
<p>
Neil cut me off: “When Einstein said ‘God,’ he meant nature—but when <em>you</em> say it, you mean leprechauns. You are asking: ‘Do we leprechauns have the power to change mathematical truths in advance of your learning them?’ That’s our turf—!”</p>
<p>
I stammered that I had a right to pose the question and had not meant to insinuate. But Neil kept on: “If we really could change outcomes then what would <em>maths</em> rest on? On <em>myths</em> just as well. But where ye speak of <em>law</em>, we have <em>lore</em>. Established rules. And they suffice.”</p>
<p>
This was waxing cryptic and I thought bringing up Kurt Gödel would only make it more so. Neil sensed I was adrift and went on: “I shall inform ye via the same story I told Dick.”</p>
<p>
</p><p/><h2> Neil’s Solution </h2><p/>
<p/><p>
Neil unfurled his story. At his same pause, one perception dawned on me: “Neil, did the lady’s wish imply a recursion—meaning the genies would be asked for a wish generator in the same proportion?”</p>
<p>
Neil nodded: “The lore says yes—the lore on hearing what is said. That is what I consulted it for. The rest I could work out on paper.” </p>
<p>
The paper part was easy up to a point. The wishes implied an infinite summation: the original <img alt="{3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, then <img alt="{30}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B30%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> more of the ten genies, with the third wishes to each bringing <img alt="{30}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B30%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> at the next level, for <img alt="{30 \cdot 10 = 300}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B30+%5Ccdot+10+%3D+300%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> more wishes there, and so on. In sum, </p>
<p align="center"><img alt="\displaystyle  3 + 30 + 300 + 3000 + \cdots = 3\cdot (1 + 10 + 100 + 1000 + \cdots) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++3+%2B+30+%2B+300+%2B+3000+%2B+%5Ccdots+%3D+3%5Ccdot+%281+%2B+10+%2B+100+%2B+1000+%2B+%5Ccdots%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>wishes. Clearly the number of wishes is bounded by the series </p>
<p align="center"><img alt="\displaystyle  3\cdot(1 + 10 + 3^{\log_2 10} + 100 + 5^{\log_2 10} + 6^{\log_2 10} + 7^{\log_2 10} + 1000 + \cdots) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++3%5Ccdot%281+%2B+10+%2B+3%5E%7B%5Clog_2+10%7D+%2B+100+%2B+5%5E%7B%5Clog_2+10%7D+%2B+6%5E%7B%5Clog_2+10%7D+%2B+7%5E%7B%5Clog_2+10%7D+%2B+1000+%2B+%5Ccdots%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>Which equals <img alt="{3\cdot \zeta(-\log_2 10)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%5Ccdot+%5Czeta%28-%5Clog_2+10%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Without asking Neil how he’d computed that, I went to an <a href="https://solvemymath.com/online_math_calculator/number_theory/riemann_function/index.php">online</a> zeta <a href="https://keisan.casio.com/exec/system/1180573439">calculator</a> and obtained a bounding total of </p>
<p align="center"><img alt="\displaystyle  3\cdot 0.006023525392866159581193... \;\;= \;\;+0.018070576178598478743579... " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++3%5Ccdot+0.006023525392866159581193...+%5C%3B%5C%3B%3D+%5C%3B%5C%3B%2B0.018070576178598478743579...+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>wishes. Neil puffed and chortled and finished his story:</p>
<blockquote><p><b> </b> <em> “So the unlucky lass had wished herself into wishing a grand total of less than one-fiftieth of a wish. Since only whole wishes can be honoured, this sprang me from the trap and kept me both me pots o’ gold to boot. I gave her one coin as a peace offering.” </em>
</p></blockquote>
<p/><p>
I joined the laughter for just a moment. This followed mathematical rules but outlandishly, and I groped for the point. But Neil supplied it directly:</p>
<blockquote><p><b> </b> <em> “Some of your fellow travelers have felt reaching answers by non-constructive methods to be less outlandish only in degree not kind. If ye later apprehend the answer by calculation—which is what your post styled as “emergent” knowledge—then it must be exactly the same object previously described non-constructively. Thus in the realms ye ascribed to us wee folk, we are the guardians and gatekeepers of truth, not the forgers of it.” </em>
</p></blockquote>
<p/><p>
Neil tipped his hat and simply faded out—no green smoke for me.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Can Neil’s zeta-function calculations insulate against any attempt at recursively gaining infinite wishes? The zeta function whips up and down in tune with the Bernoulli numbers, but the leprechauns do have freedom to choose a bounding series. Or is there an infinite series scheme that rises above 3 wishes even so? We hope this has given some St. Patrick’s Day diversion.</p>
<p/></font></font></font></div>
    </content>
    <updated>2022-03-17T05:58:57Z</updated>
    <published>2022-03-17T05:58:57Z</published>
    <category term="leprechauns"/>
    <category term="Neil L."/>
    <category term="philosophy"/>
    <category term="St. Patrick's Day"/>
    <category term="zeta function"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2022-04-01T06:37:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/03/16/postdoc-at-sandia-national-labs-apply-by-march-31-2022/</id>
    <link href="https://cstheory-jobs.org/2022/03/16/postdoc-at-sandia-national-labs-apply-by-march-31-2022/" rel="alternate" type="text/html"/>
    <title>Postdoc at Sandia National Labs (apply by March 31, 2022)</title>
    <summary>Sandia Labs is seeking a postdoc to work on quantum or quantum-inspired classical approximation, sublinear, or streaming algorithms, as part of a DOE-funded collaboration among several national labs and universities. We encourage theoretical computer scientists interested in quantum information but without prior expertise to apply. Website: https://far-qc.sandia.gov/job-opportunities/ Email: odparek@sandia.gov</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Sandia Labs is seeking a postdoc to work on quantum or quantum-inspired classical approximation, sublinear, or streaming algorithms, as part of a DOE-funded collaboration among several national labs and universities. We encourage theoretical computer scientists interested in quantum information but without prior expertise to apply.</p>
<p>Website: <a href="https://far-qc.sandia.gov/job-opportunities/">https://far-qc.sandia.gov/job-opportunities/</a><br/>
Email: odparek@sandia.gov</p></div>
    </content>
    <updated>2022-03-16T23:26:53Z</updated>
    <published>2022-03-16T23:26:53Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-04-01T06:37:59Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2022/03/15/linkage</id>
    <link href="https://11011110.github.io/blog/2022/03/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Here’s a silly but probably new proof that the harmonic series diverges (\(\mathbb{M}\)). The expected number of comparisons used by randomized quicksort on an input of size \(n\) is at most \(2nH_n\), where \(H_n\) is the \(n\)th partial sum of the harmonic series (see Cormen et al, Introduction to Algorithms, Chapter 7). However, every comparison sorting algorithm requires at least \(\log_2n!=n\log_2n-O(n)\) comparisons, by the standard decision tree argument (Cormen et al, Section 8.1). Therefore, \(H_n=\Omega(\log n)\).</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p>Here’s a silly but probably new proof that the harmonic series diverges <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107883892455172986">\(\mathbb{M}\)</a>).</span> The expected number of comparisons used by randomized quicksort on an input of size \(n\) is at most <span style="white-space: nowrap;">\(2nH_n\),</span> where \(H_n\) is the <span style="white-space: nowrap;">\(n\)th</span> partial sum of the harmonic series (see Cormen et al, <em>Introduction to Algorithms</em>, Chapter 7). However, every comparison sorting algorithm requires at least \(\log_2n!=n\log_2n-O(n)\) comparisons, by the standard decision tree argument (Cormen et al, Section 8.1). Therefore, <span style="white-space: nowrap;">\(H_n=\Omega(\log n)\).</span></p>
  </li>
  <li>
    <p>To be fair, the lecture hall I teach in this term doesn’t look quite so much like a prison if you enter by the main door at the top of the hall instead of the back door by the stage <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107890151210168412">\(\mathbb{M}\)</a>).</span></p>

    <p style="text-align: center;"><img alt="Humanities Lecture Hall, UC Irvine" src="https://www.ics.uci.edu/~eppstein/pix/hlh/hlh-m.jpg" style="border-style: solid; border-color: black;"/></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2203.00671">Maximum flow and minimum-cost flow in almost-linear time</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@FreddyR/107890263250219998">\(\mathbb{M}\)</a>).</span> New arXiv preprint by Li Chen, Rasmus Kyng, Yang P. Liu, Richard Peng, Maximilian Probst Gutenberg, Sushant Sachdeva. It assumes integer capacities but that’s enough to get near-linear bipartite maximum matching, itself a breakthrough.</p>
  </li>
  <li>
    <p>In early March, UC Berkeley was <a href="https://www.latimes.com/california/story/2022-03-04/how-much-will-uc-berkeley-have-to-cut-admissions-after-supreme-court-loss-what-we-know">ordered to drastically cut enrollment under California’s strict environmental impact review laws</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107899108816738487">\(\mathbb{M}\)</a>).</span> In practice these laws  are often used as a pretext for lawsuits to shake down or stop public developments for reasons unrelated to environmental impact (this is a heavily built-up area already; the impact is that it would have more students living in it and the people suing wanted to shake down UC for non-university-related low-income housing expansion). By mid-March, the state legislature had passed <a href="https://www.latimes.com/california/story/2022-03-14/california-legislature-passes-bill-berkeley-enrollment">emergency legislation to temporarily sidestep the issue</a>.</p>
  </li>
  <li>
    <p>Another batch of Wikipedia Good Articles <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107905159574583760">\(\mathbb{M}\)</a>):</span></p>

    <ul>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Fibonacci_nim">Fibonacci nim</a>: subtraction game with a Fibonacci number based strategy.</p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Kepler_triangle">Kepler triangle</a>: not the shape of the great pyramid of Giza, but one of its other properties inspired me to make the illustration below.</p>

        <p style="text-align: center;"><img alt="Isosceles triangle is formed from two Kepler triangles, reflected across their short sides, and its inscribed circle, having the maximum radius possible among all isosceles triangles with the same side length" src="https://11011110.github.io/blog/assets/2022/kepler.svg"/></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/Component_(graph_theory)">Connected components of undirected graphs</a>: saving this batch from complete frivolity.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>New book in discrete geometry <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107914084986329325">\(\mathbb{M}\)</a>):</span> <em>Polynomial Methods and Incidence Geometry</em>, Adam Sheffer, Cambridge University Press. See <a href="https://adamsheffer.wordpress.com/2022/03/03/new-book-polynomial-methods-and-incidence-theory/">Adam’s announcement</a> and <a href="http://faculty.baruch.cuny.edu/ASheffer/000book.pdf">an early draft with missing chapters</a>.</p>
  </li>
  <li>
    <p>When I’ve been thinking recently about who I might know who is Ukrainian or Ukrainian-American, the first to mind was Andrea Danyluk, with whom I went to grad school. We lost touch later, but she had a long distinguished career at Williams College. <a href="https://president.williams.edu/in-memoriam/the-passing-of-andrea-danyluk/">Sadly, she died a few days ago</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107916240235667375">\(\mathbb{M}\)</a>).</span> The Computing Research Association chose her as the <a href="https://cra.org/about/awards/a-nico-habermann-award/#2022">2022 winner of their A. Nico Habermann Award for increasing diversity in computing research</a>, shortly before her death.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@jsiehler/107927992555262577">The SET card game is not accessible to the color-impaired</a>, its manufacturer shows no interest in fixing it or providing accessible alternatives, and is actively blocking any attempts by others to do the same. Sadly, this makes it unusable as a classroom activity.</p>
  </li>
  <li>
    <p><a href="https://www.asbmb.org/asbmb-today/careers/030822/what-s-with-wikipedia-and-women">What’s with Wikipedia and women?
Things are changing, little by little, at the open-source encyclopedia</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107933443665051198">\(\mathbb{M}\)</a>).</span> New article from the American Society for Biochemistry and Molecular Biology mentions in passing my efforts creating articles on women in STEM and patrolling deletion discussions on them.</p>
  </li>
  <li>
    <p><a href="https://sinews.siam.org/Details-Page/in-pursuit-of-perfect-pinnacles">In pursuit of perfect pinnacles</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107945184119710916">\(\mathbb{M}\)</a>).</span> Why do spiky shapes form in nature, for instance in limestone and ice? Leif Ristroph, Jinzi Mac Huang, and Michael Shelley survey recent research in this <em>SIAM News</em> column.</p>
  </li>
  <li>
    <p>Another Wikipedia Good Article, on an important rather than recreational topic: <a href="https://en.wikipedia.org/wiki/Harmonic_series_(mathematics)">harmonic series</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107950806929679318">\(\mathbb{M}\)</a>)</span>  on the divergent series</p>

\[\sum_{n=1}^\infty\frac{1}{n} = 1 + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \frac{1}{5} + \cdots.\]

    <p>While cleaning it up I learned that the term “harmonic number” and notation for its partial sums comes from Knuth, and also that the “crossing the desert” puzzle, one of the standard examples for harmonic series, dates to long before the harmonic series itself.</p>
  </li>
  <li>
    <p><a href="https://blog.plover.com/math/se/notation.html">Bad but interesting mathematical notation</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@mjd/107927925348652783">\(\mathbb{M}\)</a>).</span> Minimal subsystems of arithmetic aside, Mark-Jason Dominus wrestles with the problem of finding an intuitive visual representation for expressions that combine a single associative operation with two mutually inverse unary operations.</p>
  </li>
  <li>
    <p>Wikimedia foundation VP Maggie Dennis warns Wikipedia editors writing about the Russian invasion of Ukraine that <a href="https://lists.wikimedia.org/hyperkitty/list/wikimedia-l@lists.wikimedia.org/message/KIMZHJMWMKXFRCMWIE5WL3YIJNFMSNVH/">they are likely to be doxxed</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107956409088287040">\(\mathbb{M}\)</a>,</span> <a href="https://en.wikipedia.org/wiki/Wikipedia_talk:Did_you_know">via</a>), especially when their “activities are seen as opposing the Russian narrative of the war”. One assumes by the Russians, although she does not say that explicitly.</p>
  </li>
  <li>
    <p><a href="http://reconf.wikidot.com/">A wiki on combinatorial reconfiguration problems</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/107963773811594015">\(\mathbb{M}\)</a>).</span> The main content at this point appears to be <a href="http://reconf.wikidot.com/papers/">their extensive bibliography of papers on the topic</a>, available both on-wiki and at a linked overleaf site. I can’t tell whether the wiki or overleaf version of the .bib file is supposed to be primary, though.</p>
  </li>
</ul></div>
    </content>
    <updated>2022-03-15T22:26:00Z</updated>
    <published>2022-03-15T22:26:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2022-04-01T01:03:27Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/dp-fine-tuning/</id>
    <link href="https://differentialprivacy.org/dp-fine-tuning/" rel="alternate" type="text/html"/>
    <title>Differentially private deep learning can be effective with self-supervised models</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Differential Privacy (DP) is a formal definition of privacy which guarantees that the outcome of a statistical procedure does not vary much regardless of whether an individual input is included or removed from the training dataset. 
This guarantee is desirable when we are tasked to train machine learning models on private datasets that should not memorize individual inputs. 
Past works have shown that differentially private models can be resilient to strong membership inference [<a href="https://proceedings.mlr.press/v37/kairouz15.html">1</a>, <a href="https://ieeexplore.ieee.org/abstract/document/9519424">34</a>, <a href="https://proceedings.neurips.cc/paper/2020/hash/fc4ddc15f9f4b4b06ef7844d6bb53abf-Abstract.html">35</a>] and data reconstruction attacks [<a href="https://www.usenix.org/conference/usenixsecurity19/presentation/carlini">2</a>, <a href="https://arxiv.org/abs/2201.12383">3</a>] when the privacy parameter is set to be sufficiently small. 
See a <a href="https://differentialprivacy.org/how-to-deploy-ml-with-dp/">prior post</a> for more background on differentially private machine learning.</p>

<p>Yet, in practice, most attempts at training differentially private deep learning models on moderately-sized datasets have resulted in large performance drops compared to when training without privacy-protection baked in. 
These performance drops are oftentimes large enough to discourage the adoption of differential privacy protection into machine learning pipelines altogether.</p>

<p>To provide a reference of the potential performance hit, the authors of [<a href="https://arxiv.org/abs/2102.12677">5</a>] trained a ResNet-20 from scratch on CIFAR-10 with a privacy budget of \(\epsilon=8\) that has test accuracy barely over 62% (see their Table 1). 
Contrast this with the 8.75% error rate (91.25% accuracy) reported for training the same architecture without enforcing differential privacy [<a href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">6</a>]. 
While some works report private learning results better than the above, absent additional data, pre-training, or external knowledge, most improvements have been incremental, and the test accuracy for CIFAR-10 models trained under modest privacy leakage (\(\epsilon=3\)) has roughly settled to ~70% in the literature [<a href="https://arxiv.org/abs/2011.11660">4</a>].</p>

<p>One reason behind the performance drop lies in sample efficiency — differentially private learning generally requires much more data than non-private learning to reach an acceptable level of performance. 
This also means that learning the high-level features (e.g., syntactic structure in text, edge detectors for images) necessary to perform specific tasks with private data can be much more sample-costly.</p>

<p>This blog post surveys results that leverage public self-supervised pre-training to obtain high-performing models through differentially private fine-tuning.
The pre-train-fine-tune paradigm is straightforward to execute and results in high-performing models under modest privacy budgets for many standard computer vision and natural language processing tasks. 
Moreover, existing results have shown that private fine-tuning consistently benefits from improvements in public pre-training.</p>

<h2 id="self-supervised-pre-training">Self-Supervised Pre-Training</h2>

<p>Self-supervised learning is a paradigm which leverages unlabeled data to learn representations that can be useful for a range of downstream tasks.
Since self-supervised learning doesn’t target specific tasks itself, 
the (pre-)training procedure doesn’t require labeled data — in many cases, mildly curated unlabeled data is sufficient for self-supervised pre-training to produce models for subsequent fine-tuning. 
So far, there have been two broadly successful instantiations of this learning paradigm in computer vision [<a href="http://proceedings.mlr.press/v119/chen20j.html">9</a>] and natural language processing [<a href="https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf">7</a>, <a href="https://arxiv.org/abs/1810.04805">8</a>]. 
We recap the two approaches below.<sup id="fnref:1"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:1" rel="footnote">1</a></sup></p>

<p><strong>Contrastive pre-training for vision:</strong> 
One class of self-supervised methods in computer vision (SimCLR, [<a href="http://proceedings.mlr.press/v119/chen20j.html">9</a>]) performs pre-training through contrastive learning. 
Algorithms of this type produce embeddings for images with the goal of creating different embeddings for semantically different images and similar embeddings for similar ones. 
Concretely, the algorithm used in SimCLR forces models to produce similar embeddings for an image and its augmented siblings (e.g., image rotated by some degrees), 
and different embeddings for separate images (and their augmentations). 
The SimCLR framework with large scale models and compute led to state-of-the-art (non-private) ImageNet fine-tuning results at the time of its writing.</p>

<p><strong>Masked language modeling and autoregressive language modeling for text:</strong> 
Masked Language Modeling (MLM) and Auto-regressive Language Modeling (ALM) are two self-supervised pre-training approaches. 
While the former asks models to predict deliberately masked out tokens from a piece of text, the latter asks models to simply predict the next token in a sequence. 
With large amounts of unlabeled text data, large and expressive Transformer models [<a href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">24</a>], and lots of compute, both approaches produce powerful models that are good starting points for downstream fine-tuning. 
For instance, Bidirectional Encoder Representations from Transformers (BERT, [<a href="https://arxiv.org/abs/1810.04805">8</a>]), produced state-of-the-art (non-private) results (at the time) for a large collection of language understanding tasks when fine-tuned on each.</p>

<h2 id="fine-tuning-self-supervised-models-with-dp-optimization">Fine-Tuning Self-Supervised Models With DP-Optimization</h2>
<p>Self-supervised pre-training is appealing in the context of differentially private machine learning. 
This is because (i) the mildly curated data needed for pre-training can usually be obtained cheaply from the public domain, and (ii) pre-trained models may contain useful domain knowledge that can reduce the sample complexity of subsequent private learning. 
A paradigm for private learning that leverages self-supervised pre-training could follow two steps:</p>

<ul>
  <li>collect cheap and public (unlabeled) data from the task domain (e.g., vision, language, etc.) to pre-train a model with self-supervised learning, and</li>
  <li>collect moderate amounts of task-specific private (labeled) data and fine-tune the pre-trained model under differential privacy to perform the task.<sup id="fnref:2"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:2" rel="footnote">2</a></sup></li>
</ul>

<p>To date, some of the best differentially private deep learning results in the literature have resulted from instantiating this paradigm [<a href="https://arxiv.org/abs/2011.11660">4</a>, <a href="https://arxiv.org/abs/2110.05679">11</a>, <a href="https://arxiv.org/abs/2110.06500">12</a>].
Below, we review works which capitalize on self-supervised pre-training by differentially privately fine-tuning pre-trained models with an iterative gradient method like DP-SGD [<a href="https://dl.acm.org/doi/abs/10.1145/2976749.2978318">19</a>, <a href="https://ieeexplore.ieee.org/abstract/document/6736861">20</a>].<sup id="fnref:3"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:3" rel="footnote">3</a></sup>
<img alt="" src="https://differentialprivacy.org/images/fine-tuning-paradigm.png"/></p>

<p><strong>Private fine-tuning with SimCLR features:</strong> 
The authors of [<a href="https://arxiv.org/abs/2011.11660">4</a>] fine-tuned a linear model on top of the embedding vectors produced by SimCLRv2 from the CIFAR-10 dataset. Under a privacy budget of \(\epsilon=2\), 
these models reached an average test accuracy of 92.7%. This number can be further improved to ~94% with the use of larger and wider pre-trained models in the SimCLRv2 family.<sup id="fnref:4"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:4" rel="footnote">4</a></sup> 
These test accuracies are very close to some standard non-private results attained by an off-the-shelf ResNet architecture [<a href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">6</a>].</p>

<p><strong>Privately fine-tuning BERT variants and GPT-2:</strong> 
The authors of [<a href="https://arxiv.org/abs/2110.05679">11</a>, <a href="https://arxiv.org/abs/2110.06500">12</a>, <a href="http://proceedings.mlr.press/v139/yu21f.html">16</a>] showed that with appropriate hyper-parameters, fine-tuning BERT variants and GPT-2 with DP-optimization results in high-performing private models for text classification and language generation — even on datasets of modest sizes and under modest privacy budgets. 
Notably, some of these models attain a task performance close to non-private models from previous years in the literature. 
These results also exceed many non-private learning results from the pre-BERT and pre-GPT years.<sup id="fnref:5"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:5" rel="footnote">5</a></sup></p>

<p>More interestingly, the authors showed that the larger (and thus better) the pre-trained model, the better the private fine-tuning performance gets. 
This empirical observation in private fine-tuning of large Transformers is qualitatively different from what’s implied by the usual minimax optimal rates derived for vanilla private learning with convex loss functions under approximate differential privacy [<a href="https://ieeexplore.ieee.org/abstract/document/6979031">14</a>, <a href="https://proceedings.neurips.cc/paper/2019/hash/3bd8fdb090f1f5eb66a00c84dbc5ad51-Abstract.html">15</a>]. 
This discrepancy between experimental results for training large models and the theory for learning with convex losses suggests there is more to be understood.<sup id="fnref:6"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:6" rel="footnote">6</a></sup></p>

<p>Overall, for both vision and language tasks, private learning performance has consistently improved with the improvement in the quality of pre-training, 
where the latter is measured by the non-private fine-tuning performance.<sup id="fnref:7"><a class="footnote" href="https://differentialprivacy.org/feed.xml#fn:7" rel="footnote">7</a></sup></p>

<p>
  <img src="https://differentialprivacy.org/../images/figure1_classification.png" width="48%"/>
  <img src="https://differentialprivacy.org/../images/figure1_generation.png" width="48%"/> 
  Figure 1: Privately fine-tuning better (and larger) pre-trained models lead to consistently improving performance for text classification and language generation. 
Left: text classification on MNLI [<a href="https://arxiv.org/abs/1704.05426">25</a>]. Right: language generation on E2E [<a href="https://arxiv.org/abs/1706.09254">26</a>].
</p>

<h2 id="conclusion-and-outlook">Conclusion and Outlook</h2>

<p>We surveyed recent works in the literature that obtained highly performant private machine learning models leveraging self-supervised pre-training. 
Common to these results is the trend that the performance of private learning consistently improved with the quality of public pre-training. 
We therefore anticipate that the general paradigm may be useful in additional settings (e.g., federated learning) and tasks (e.g., private synthetic image generation), and lead to better private learning results.</p>

<p>We have thus far assumed that the data for public pre-training can be cheaply obtained.
This, however, does not imply that determining whether a particular source of data is appropriate for public pre-training is an easy problem.
Using publicly available data is not necessarily risk-free in terms of privacy.
For instance, the authors of [<a href="https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting">33</a>] were able to extract personally identifiable information from a GPT-2 model pre-trained on data scraped from the public internet.</p>

<p>Self-supervised pre-training has led to progress in private deep learning, but leveraging pre-trained models alone will not address several fundamental challenges to differentially private learning.
First and foremost, the datasets of machine learning tasks may be sampled from long-tailed distributions [<a href="https://proceedings.neurips.cc/paper/2020/hash/1e14bfe2714193e7af5abc64ecbd6b46-Abstract.html">21</a>]. 
When privately trained on such datasets, a machine learning model may fail to acquire the learning signal necessary to perform accurate predictions for examples on the tail [<a href="https://dl.acm.org/doi/abs/10.1145/3442188.3445934">28</a>] or from underrepresented (sub)populations [<a href="https://proceedings.neurips.cc/paper/2019/hash/fc0de4e0396fff257ea362983c2dda5a-Abstract.html">29</a>]. 
Second, many machine learning problems are in a domain where public data (even unlabeled data) may be sparse, e.g., medical imaging. 
Developing refined versions of the pre-train-fine-tune approach for problems from these domains is an interesting avenue for future work.</p>

<p>Lastly, differential privacy as one specific definition of privacy may not capture all that’s desired for privacy in reality. 
For instance, while differentially private algorithms naturally give machine unlearning guarantees [<a href="https://ieeexplore.ieee.org/abstract/document/9519428">30</a>, <a href="https://ieeexplore.ieee.org/abstract/document/7163042">32</a>], tailored unlearning algorithms tend to have higher capacities of unlearning [<a href="https://proceedings.neurips.cc/paper/2021/hash/9627c45df543c816a3ddf2d8ea686a99-Abstract.html">31</a>].
In addition, what constitutes a record in the differential privacy framework can oftentimes be unclear. 
Inappropriately defined example boundaries can create correlated records which cause differential privacy guarantees to degrade [<a href="https://arxiv.org/abs/1603.01508">22</a>].
Moreover, differential privacy guarantees won’t directly prevent the inference of private data outside the original context [<a href="https://heinonline.org/hol-cgi-bin/get_pdf.cgi?handle=hein.journals/washlr79&amp;section=16">23</a>]. 
These are fundamental limitations of differential privacy which improvements to differentially private learning won’t touch on.</p>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>The authors thank Nicolas Papernot and Gautam Kamath for detailed feedback and edit suggestions.</p>

<hr/>

<h2 id="references">References</h2>
<p>[1] Rahman MA, Rahman T, Laganière R, Mohammed N, Wang Y. Membership Inference Attack against Differentially Private Deep Learning Model. Trans. Data Priv.. 2018 Apr 1;11(1):61-79.</p>

<p>[2] Carlini N, Liu C, Erlingsson Ú, Kos J, Song D. The secret sharer: Evaluating and testing unintended memorization in neural networks. In 28th USENIX Security Symposium (USENIX Security 19) 2019 (pp. 267-284).</p>

<p>[3] Guo C, Karrer B, Chaudhuri K, van der Maaten L. Bounding Training Data Reconstruction in Private (Deep) Learning. arXiv preprint arXiv:2201.12383. 2022 Jan 28.</p>

<p>[4] Tramer F, Boneh D. Differentially private learning needs better features (or much more data). arXiv preprint arXiv:2011.11660. 2020 Nov 23.</p>

<p>[5] Yu D, Zhang H, Chen W, Liu TY. Do not let privacy overbill utility: Gradient embedding perturbation for private learning. arXiv preprint arXiv:2102.12677. 2021 Feb 25.</p>

<p>[6] He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. InProceedings of the IEEE conference on computer vision and pattern recognition 2016 (pp. 770-778).</p>

<p>[7] Radford A, Narasimhan K, Salimans T, Sutskever I. Improving language understanding by generative pre-training.</p>

<p>[8] Devlin J, Chang MW, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. 2018 Oct 11.</p>

<p>[9] Chen T, Kornblith S, Norouzi M, Hinton G. A simple framework for contrastive learning of visual representations. InInternational conference on machine learning 2020 Nov 21 (pp. 1597-1607). PMLR.</p>

<p>[10] Li XL, Liang P. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190. 2021 Jan 1.</p>

<p>[11] Li X, Tramer F, Liang P, Hashimoto T. Large language models can be strong differentially private learners. arXiv preprint arXiv:2110.05679. 2021 Oct 12.</p>

<p>[12] Yu D, Naik S, Backurs A, Gopi S, Inan HA, Kamath G, Kulkarni J, Lee YT, Manoel A, Wutschitz L, Yekhanin S. Differentially private fine-tuning of language models. arXiv preprint arXiv:2110.06500. 2021 Oct 13.</p>

<p>[13] Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis M, Zettlemoyer L, Stoyanov V. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. 2019 Jul 26.</p>

<p>[14] Bassily R, Smith A, Thakurta A. Private empirical risk minimization: Efficient algorithms and tight error bounds. In2014 IEEE 55th Annual Symposium on Foundations of Computer Science 2014 Oct 18 (pp. 464-473). IEEE.</p>

<p>[15] Bassily R, Feldman V, Talwar K, Guha Thakurta A. Private stochastic convex optimization with optimal rates. Advances in Neural Information Processing Systems. 2019;32.</p>

<p>[16] Yu D, Zhang H, Chen W, Yin J, Liu TY. Large scale private learning via low-rank reparametrization. InInternational Conference on Machine Learning 2021 Jul 1 (pp. 12208-12218). PMLR.</p>

<p>[17] Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I. Language models are unsupervised multitask learners. OpenAI blog. 2019 Feb 24;1(8):9.</p>

<p>[18] Bommasani R, Hudson DA, Adeli E, Altman R, Arora S, von Arx S, Bernstein MS, Bohg J, Bosselut A, Brunskill E, Brynjolfsson E, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258. 2021 Aug 16.</p>

<p>[19] Abadi M, Chu A, Goodfellow I, McMahan HB, Mironov I, Talwar K, Zhang L. Deep learning with differential privacy. InProceedings of the 2016 ACM SIGSAC conference on computer and communications security 2016 Oct 24 (pp. 308-318).</p>

<p>[20] Song S, Chaudhuri K, Sarwate AD. Stochastic gradient descent with differentially private updates. In2013 IEEE Global Conference on Signal and Information Processing 2013 Dec 3 (pp. 245-248). IEEE.</p>

<p>[21] Feldman V, Zhang C. What neural networks memorize and why: Discovering the long tail via influence estimation. Advances in Neural Information Processing Systems. 2020;33:2881-91.</p>

<p>[22] Ghosh A, Kleinberg R. Inferential privacy guarantees for differentially private mechanisms. arXiv preprint arXiv:1603.01508. 2016 Mar 4.</p>

<p>[23] Nissenbaum H. Privacy as contextual integrity. Wash. L. Rev.. 2004;79:119.</p>

<p>[24] Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I. Attention is all you need. Advances in neural information processing systems. 2017;30.</p>

<p>[25] Williams A, Nangia N, Bowman SR. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426. 2017 Apr 18.</p>

<p>[26] Novikova J, Dušek O, Rieser V. The E2E dataset: New challenges for end-to-end generation. arXiv preprint arXiv:1706.09254. 2017 Jun 28.</p>

<p>[27] Papernot N, Chien S, Song S, Thakurta A, Erlingsson U. Making the shoe fit: Architectures, initializations, and tuning for learning with privacy.</p>

<p>[28] Suriyakumar VM, Papernot N, Goldenberg A, Ghassemi M. Chasing your long tails: Differentially private prediction in health care settings. InProceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency 2021 Mar 3 (pp. 723-734).</p>

<p>[29] Bagdasaryan E, Poursaeed O, Shmatikov V. Differential privacy has disparate impact on model accuracy. Advances in Neural Information Processing Systems. 2019;32.</p>

<p>[30] Bourtoule L, Chandrasekaran V, Choquette-Choo CA, Jia H, Travers A, Zhang B, Lie D, Papernot N. Machine unlearning. In2021 IEEE Symposium on Security and Privacy (SP) 2021 May 24 (pp. 141-159). IEEE.</p>

<p>[31] Sekhari A, Acharya J, Kamath G, Suresh AT. Remember what you want to forget: Algorithms for machine unlearning. Advances in Neural Information Processing Systems. 2021 Dec 6;34.</p>

<p>[32] Cao Y, Yang J. Towards making systems forget with machine unlearning. In2015 IEEE Symposium on Security and Privacy 2015 May 17 (pp. 463-480). IEEE.</p>

<p>[33] Carlini N, Tramer F, Wallace E, Jagielski M, Herbert-Voss A, Lee K, Roberts A, Brown T, Song D, Erlingsson U, Oprea A. Extracting training data from large language models. In30th USENIX Security Symposium (USENIX Security 21) 2021 (pp. 2633-2650).</p>

<p>[34] Nasr M, Songi S, Thakurta A, Papemoti N, Carlin N. Adversary instantiation: Lower bounds for differentially private machine learning. In2021 IEEE Symposium on Security and Privacy (SP) 2021 May 24 (pp. 866-882). IEEE.</p>

<p>[35] Jagielski M, Ullman J, Oprea A. Auditing differentially private machine learning: How private is private sgd?. Advances in Neural Information Processing Systems. 2020;33:22205-16.</p>
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Authors of [<a href="https://arxiv.org/abs/2108.07258">18</a>] framed these self-supervised models which are trained on broad data at scale that are adaptable to a wide range of downstream tasks as “foundation models.” <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:1">↩</a></p>
    </li>
    <li id="fn:2">
      <p>The idea of privately fine-tuning a publicly pre-trained model certainly isn’t new. One of the first differentially private deep learning papers [<a href="https://arxiv.org/abs/1607.00133">19</a>] considered an experiment which fine-tuned convolutional nets on CIFAR-10 which were pre-trained on CIFAR-100. Results on privately fine-tuning <em>self-supervised</em> models are, on the other hand, more recent. Covering these results is our main focus here. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:2">↩</a></p>
    </li>
    <li id="fn:3">
      <p>Blue and pink sphere avatars taken from [<a href="https://arxiv.org/abs/2108.07258">18</a>]. Credit to <a href="https://cs.stanford.edu/~dorarad/">Drew A. Hudson</a> for making these. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:3">↩</a></p>
    </li>
    <li id="fn:4">
      <p>Unpublished result. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:4">↩</a></p>
    </li>
    <li id="fn:5">
      <p>Hyper-parameters that work well for non-private learning typically aren’t those that work best for differentially private learning [<a href="https://openreview.net/pdf?id=rJg851rYwH">27</a>]. It’s crucial to use a large batch size, a small clipping norm, an appropriate learning rate, and a reasonably large number of training epochs to obtain the mentioned private learning results [<a href="https://arxiv.org/abs/2110.05679">11</a>]. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:5">↩</a></p>
    </li>
    <li id="fn:6">
      <p>In practice, past works have presented mixed results on whether larger models would yield better performance. While some showed that using more filters in a convolutional network can degrade the performance of private learning after some threshold [<a href="https://openreview.net/pdf?id=rJg851rYwH">27</a>], others showed that a larger model can outperform a smaller model from a different model family [<a href="https://arxiv.org/abs/2011.11660">4</a>]. Note these results are conditioned on their particular hyperparameter choices. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:6">↩</a></p>
    </li>
    <li id="fn:7">
      <p>Since the pre-training data for large language models are oftentimes collected through large scale web scraping (e.g., WebText), a common concern is that some training and test instances for downstream tasks may already appear in the pre-training data. Self-supervised pre-training therefore can give models an opportunity to “see” this data even before they are privately fine-tuned. Authors of [<a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">17</a>] confirmed that there is a 1-6% overlap between the test set of many natural language processing tasks and the pre-training data they collected (WebText); these common tasks, however, don’t include those studied by authors of [<a href="https://arxiv.org/abs/2110.05679">11</a>]. The numbers suggest a possibility that existing private fine-tuning results in the literature could be slightly inflated compared to when the pre-training data didn’t contain any instance for any downstream task for which evaluation was performed. <a class="reversefootnote" href="https://differentialprivacy.org/feed.xml#fnref:7">↩</a></p>
    </li>
  </ol>
</div></div>
    </summary>
    <updated>2022-03-15T19:00:00Z</updated>
    <published>2022-03-15T19:00:00Z</published>
    <author>
      <name>Tatsunori Hashimoto</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2022-03-31T22:43:10Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3121871012533153752</id>
    <link href="http://blog.computationalcomplexity.org/feeds/3121871012533153752/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/03/problem-x-wont-be-solved-in-my-lifetime.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/3121871012533153752" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/3121871012533153752" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/03/problem-x-wont-be-solved-in-my-lifetime.html" rel="alternate" type="text/html"/>
    <title>Problem X won't be solved in MY lifetime- but what about...</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>1) In 1989 on the episde The Royale of Star Trek: The Next Generation (which takes place in the far future)  Captain Picard is working on Fermat's last theorem which he says quite explicitly is still open.</p><p>When I saw the episode I asked Larry Washington, a Number Theorist at Univ of MD, when he thought FLT would be solved. He said</p><p>                                      <i>It will be solved within the next 10 years.</i></p><p>And indeed- Wiles solved it in 1993-sort of. There was a flaw in the proof which he fixed in 1994 with the help of his former student Richard Taylor. Wiles published the correction to the flaw in 1995, so we will date it as having been solved in 1995. Larry Washington was correct.  And in an episode of Star Trek: Deep Space Nine in 1995 (episode name:Facets) Dax says that a previous host, Tobin Dax, had done the most creative work on FLT since Wiles. Maybe Tobin wrote this limerick:</p><p>A challenge for many long ages</p><p>Had baffled the savants and sages</p><p>Yet at last came the light</p><p>Seems that Fermat was right</p><p>To the margin add 200 pages.</p><p><br/></p><p>I asked Larry W when he thought Riemann would be solved. He said  </p><p>                   <i> In your lifetime but not in mine.</i></p><p>He is about 10 years older than I am and I think we are both in good health. This seems like a rather precise prediction so I am skeptical. But he did get FLT right...</p><p>2) In class I sometimes say things like </p><p><i>I do not think Quantum Computers will factor faster than classical in my lifetime. </i></p><p><i>I do not think P vs NP will be solved in my lifetime.</i></p><p><i>I can imagine P=BPP will be proven in my lifetime. (I said that 10 years ago. I am less imaginative now.) </i></p><p><i>I hope the muffin problem is solved in my lifetime (it was, see <a href="https://arxiv.org/abs/1907.08726">here</a>).</i></p><p>I didn't quite think about the difference in my age and the students until recently when I was working with Ilya Hajiaghayi (Mohammd H's 9 year old son) on cryptography and he said </p><p><i>In your recorded lecture you said you don't think quantum computers will be a threat to cryptography  in your lifetime. What about in my lifetime?</i></p><p>Indeed- his lifetime and mine are rather far apart. </p><p>I am reminded that one of the answers to my P vs NP poll made the point that while we have some sense of what will happen in the next 10 years, maybe even 20, math and life can change so much that conjectures beyond that are guesswork. Any  prediction for x years from now you should have confidence &lt; 1/ln(x) of it being true.</p><p><i><br/></i></p><p><i><br/></i></p><p><i><br/></i></p><p><br/></p></div>
    </content>
    <updated>2022-03-15T14:30:00Z</updated>
    <published>2022-03-15T14:30:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2022-03-29T23:22:06Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://benjamin-recht.github.io/2022/03/15/external-validity/</id>
    <link href="http://benjamin-recht.github.io/2022/03/15/external-validity/" rel="alternate" type="text/html"/>
    <title>Machine Learning has a validity problem.</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>One of the central tenets of machine learning warns the more times you run experiments with the same test set, the more you overfit to that test set. This conventional wisdom is mostly wrong and prevents machine learning from reconciling its inductive nihilism with the rest of the empirical sciences.</p>

<p>Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar led an passionate quest to test the overfitting hypothesis, devoting countless hours to reproducing machine learning benchmarks. In particular, they painstakingly recreated a test set of the famous <a href="https://www.image-net.org/">ImageNet benchmark</a>, which itself is responsible for bringing about the latest AI feeding frenzy. Out of the many surprises in my research career, what <a href="https://arxiv.org/abs/1902.10811">they found surprised me the most.</a></p>

<p class="center"><img alt="The scatterplot of nightmares" src="http://www.argmin.net/assets/RSS_Scatter.png" width="90%"/></p>

<p>In this graph, the x-axis is the accuracy on the original ImageNet benchmark, which has been used millions of times by individual researchers at Google alone. On the y-axis is the accuracy evaluated on “ImageNet v2” set, which was made by closely trying to replicate the data creation method for the benchmark. Each blue dot represents a single machine learning model trained on the original ImageNet data. The red line is a linear fit to these models, and the dashed line is what we would see if the accuracy was the same on both test sets. What do we see? The models which perform the best on the original test set perform the best on the new test set. That is, there is no evidence of overfitting.</p>

<p>What is clear, however, is a noticeable drop in performance on the new test set. Despite their best efforts in reproducing the ImageNet protocol, there is evidence of a <em>distribution shift</em>. Distribution shift is a far reaching term describing whenever the data on which a machine learning algorithm is deployed is different from the data on which it is trained. The Mechanical Turk workers who labeled the images were different from those originally employed. The API used for the labeling was slightly different. The selection mechanism to aggregate differences in opinions between labelers is slightly different. The small differences add up to around a 10% drop in accuracy, equivalent to five years of progress on the benchmark.</p>

<p>Folks in my research group have reproduced this phenomenon several times. In <a href="https://papers.nips.cc/paper/9117-a-meta-analysis-of-overfitting-in-machine-learning">Kaggle competitions</a>, where the held out set and validation set were <em>identically</em> distributed, we saw no overfitting <em>and</em> no distribution shift. We found sensitivity to distribution shifts in CIFAR10, in <a href="https://arxiv.org/abs/1906.02168">video</a>, and in <a href="https://arxiv.org/abs/2004.14444">question answering</a> benchmarks. And Chhavi Yadav and Leon Bottou showed that we have not yet overfit to the <a href="https://arxiv.org/abs/1905.10498">venerable MNIST data set</a>, but distribution shift remains a challenge.</p>

<p>The marked sensitivity to distribution shift is a huge issue. If small ambiguities in reproductions lead to large losses in predictive performance, what happens when we take ML systems designed on static benchmarks and deploy them in important applications? A decade of AI fever has delivered piles of evidence that distribution shift is machine learning’s achilles heel. Algorithms run inside the big tech companies need to be constantly retrained with their huge computing resources. <a href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002683">Data-driven algorithms for radiology often fail if one changes the X-ray machine</a>. <a href="https://jamanetwork.com/journals/jamainternalmedicine/article-abstract/2781307">AI algorithms for sepsis fail if you change hospitals</a>. And self-driving car systems are readily confused in new environments (No citation needed. Keep your Tesla away from me.).</p>

<p>The only way forward is for machine learning to engage more broadly with other scientists who have been tackling similar issues for centuries. My first proposal is simple: let’s change our terminology to align with the rest of the sciences. The study of distribution shift in machine learning has always been insular and, while machine learning is particularly sensitive, all empirical science must deal with the jump from experiment to reality.</p>

<p>With this in mind, <a href="https://twitter.com/rajiinio">Deb Raji</a> and I have been digging through the scientific literature for a while now hoping to find some answers. In most other parts of science, “robustness to distribution shift” is called external validity. External validity quantifies how well a finding generalizes beyond a specific experiment. For example, a significant result on a particular cohort may not generalize to a broader population.</p>

<p>Predictive algorithms and experimental science both rely on repeatability. “The sun has always risen in the east.” “The apple always falls straight to the ground.” We expect that given the same contexts, the natural world more or less repeats itself. There is unfortunately a big leap from the sun rising in the morning, to an experimental finding in machine learning or biomedicine being reproducible. Why?</p>

<p>The experimental contexts under which predictions and inferences are designed are often far too narrow. The results of a study performed on young male college students in Maine may not help us understand properties of a retirement community in Arizona. These populations are different! However, it may give us insights into other cohorts of male college students: a study at Bates may generalize to Colby or Bowdoin.</p>

<p>Contexts can change in a myriad of ways. Some examples include the following:</p>

<ol>
  <li>The context can just be too narrow in the experiment. Do studies on adults generalize to children? Do studies on medications with only men generalize to women?</li>
  <li>The measured quantity may itself change. It is often easier to measure, detect, and control for exogenous disturbances in a lab setting than in the real world.</li>
  <li>Populations can change over time. For example, medical recommendations from the 1980s may no longer apply to the current population. Recent developments have led to <a href="https://www.npr.org/2021/10/13/1045746669/task-force-says-most-people-should-not-take-daily-aspirin-to-prevent-a-heart-att">not recommending aspirin to prevent heart attacks</a>.  Machine Learners like to call this <em>covariate shift</em>.</li>
  <li>Even more nefariously, the population can change in response to the intervention. A classic example of this is <a href="https://en.wikipedia.org/wiki/Goodhart%27s_law">Goodhart’s Law</a> which states “Any observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes.”</li>
</ol>

<p>How can we grapple with these external validity challenges? Verifying external validity is daunting and the set of potential solutions remains quite limited. As I mentioned, Deb and I have been chatting about this for a year now, and we’ve now dragged the rest of the group into our investigations. So I’m going to share the blog with Deb for a few posts now, and we’ll both expand on what we’ve been reading and thinking about. In the next few posts, we’ll explore some of the intricacies of when external validity can fail and will also try to spell out some of the research directions that might help bridge the gaps between experiments and reality.</p></div>
    </summary>
    <updated>2022-03-15T00:00:00Z</updated>
    <published>2022-03-15T00:00:00Z</published>
    <source>
      <id>http://benjamin-recht.github.io/</id>
      <author>
        <name>Ben Recht</name>
      </author>
      <link href="http://benjamin-recht.github.io/" rel="alternate" type="text/html"/>
      <link href="http://benjamin-recht.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Musings on systems, information, learning, and optimization.</subtitle>
      <title>arg min blog</title>
      <updated>2022-03-31T22:42:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=19736</id>
    <link href="https://rjlipton.wpcomstaging.com/2022/03/14/are-these-the-last-digits-of-pi/" rel="alternate" type="text/html"/>
    <title>Are These the Last Digits of Pi?</title>
    <summary>Ghoulish reflections on whether mathematics is emergent Composite of src1, src2 Thomas Keller and Heiko Rölke led a team at the University of Applied Sciences in Graubünden, Switzerland, that set a new record for the computation of last August. They computed 62.8 trillion digits of . The last ten digits they obtained are 7817924264. Today, […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Ghoulish reflections on whether mathematics is emergent</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2022/03/14/are-these-the-last-digits-of-pi/kellerrolkepi/" rel="attachment wp-att-19738"><img alt="" class="alignright wp-image-19738" height="95" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/03/KellerRolkePi.png?resize=222%2C95&amp;ssl=1" width="222"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite of <a href="https://www.fhgr.ch/en/specialist-areas/applied-future-technologies/davis-centre/pi-challenge/">src1</a>, <a href="https://www.welt.de/wissenschaft/plus233700070/Zahl-Pi-Forscher-haben-62-8-Billionen-Stellen-ermittelt.html">src2</a></font></td>
</tr>
</tbody>
</table>
<p>
Thomas Keller and Heiko Rölke led a team at the University of Applied Sciences in Graubünden, Switzerland, that set a new record for the computation of <img alt="{\pi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> last August. They computed 62.8 trillion digits of <img alt="{\pi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The last ten digits they obtained are 7817924264.</p>
<p>
Today, we wish people Happy Pi Day amid wishes for happier days overall.<br/>
<span id="more-19736"/></p>
<p>
Pi Day needs the day to be written American-style as 3/14/22. In international style it would be 31/4/22, but April does not have 31 days. This year involves the numerator of the simplest serviceable approximation to pi: </p>
<p align="center"><img alt="\displaystyle  \pi \approx \frac{22}{7} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cpi+%5Capprox+%5Cfrac%7B22%7D%7B7%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>Because <img alt="{\pi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is irrational, any finite fraction or number of digits is an approximation. Because <img alt="{\pi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is computable—indeed highly efficiently computable in <a href="https://rjlipton.wpcomstaging.com/2009/03/15/cooks-class-contains-pi/">senses</a> we have <a href="https://rjlipton.wpcomstaging.com/2010/07/14/making-an-algorithm-an-algorithm-bbp/">covered</a>—we can adduce that the code for doing so represents <img alt="{\pi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> exactly. Furthermore, the symbol <img alt="{\pi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> lends itself to many other calculations that yield exact finite results. </p>
<p>
The digits, however, have their own mystique. We still do not know whether they are <a href="http://pi314.at/math/normal.html">normal</a> in any base, let alone base ten. Speaking as mathematical Platonists, we regard the infinite sequence as a completed, unchanging entity—one for which assertions like “it is normal” have currently-definite truth values. </p>
<p>
This is what events of the past few weeks have prompted Dick and me to question. If our world presently stops existing, 7817924264 will be the last digits of <img alt="{\pi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that we know.</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2022/03/14/are-these-the-last-digits-of-pi/pimug/" rel="attachment wp-att-19739"><img alt="" class="alignright wp-image-19739" height="168" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/03/PiMug.jpg?resize=150%2C168&amp;ssl=1" width="150"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Etsy Math Mug <a href="https://www.etsy.com/listing/497368583/math-mug-my-pin-is-the-last-4-digits-of">source</a></font>
</td>
</tr>
</tbody></table>
<p/><h2> Not Just WW III </h2><p/>
<p/><p>
I already have a story of impermanent truth rooted in Russia. On my phone I have a free app for the <a href="https://chessok.com/?page_id=27966">Lomonosov Tablebases</a>, which give the perfect result of all chess positions with 7 or fewer pieces. Those tables took years to compile and exist only as 100+ terabytes on a machine at the Lomonosov Moscow State University computer center. </p>
<p>
Sometime over the new year, I noticed that the server stopped working. It was <a href="http://talkchess.com/forum3/viewtopic.php?f=2&amp;t=74046">reportedly</a> the victim of a ransomware attack. Is it meaningful to say that the tables currently exist? Mathematically, yes, and physically likely also yes—assuming the bits were merely blocked and not altered on the storage plates. </p>
<p>
Happily, there is a second 7-piece table called <a href="https://lichess.org/blog/W3WeMyQAACQAdfAL/7-piece-syzygy-tablebases-are-complete">Syzygy</a>, which uses only about 18 terabytes and exists in multiple dowloaded copies. However, this leads us to another question about correctness. The Syzygy computation reproduced some key extremal features of the Lomonosov compilation, such as the position with the longest number of moves needed to win. Not all have been verifiable, because Syzygy counts the time needed to make concrete progress in the form of a capture or pawn advance rather than the time to give checkmate. What I don’t know—and maybe now won’t know—is:</p>
<blockquote><p><b> </b> <em> Has Syzygy been used to verify every win/draw/loss (WDL) verdict computed by Lomonosov, and vice-versa? </em>
</p></blockquote>
<p/><p>
Doing so would require cross-checking many trillions of chess positions. Of course, we should expect that the algorithms used to produce these tables have been verified as completely correct. </p>
<p>
That is worth saying again: The chess algorithms have been verified in themselves. This is not a case of a shock I had before Christmas, when a module in my own chess software threw an error for the first time since I wrote it in 2014, having worked perfectly on over 100 million moves in several million games. The function in question records not only the exact source and destination squares of a move but also the minimum information required by short-form notation systems to disambiguate it from other pieces that could move to the square. There was one game where the players horsed around until one side had 5 queens that could all move to the same square, and that was 1 more than my scheme had presumed possible. My code is thus not-quite correct.</p>
<p>
But even analytical correctness fails in cases of hardware error. A cosmic ray temporarily <a href="https://www.independent.co.uk/news/science/subatomic-particles-cosmic-rays-computers-change-elections-planes-autopilot-a7584616.html">changed</a> the outcome of an election in Belgium. Fortunately, the systems have cross-checks to catch these events. When the subject is <em>mathematical truths</em>, however, how are we checking? On what basis can we be satisfied by such checks?</p>
<p>
</p><p/><h2> Emergence </h2><p/>
<p/><p>
Despite our Platonist convictions, as practitioners of mathematics we experience its truths as <a href="https://en.wikipedia.org/wiki/Emergence">emergent</a>. By “emergent” we mean more than saying theorems are unknown until the point in time where they are clearly proved. <em>Pace</em> our <a href="https://rjlipton.wpcomstaging.com/2019/04/21/pnp-proofs/">claimers</a>, P versus NP has not been proved either way, and we live in a world where even those who strongly believe <img alt="{\mathsf{P &lt; NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3C+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> will aver it is unknown. We <a href="https://rjlipton.wpcomstaging.com/2021/12/31/make-a-trillion-dollars/">covered</a> this recently.</p>
<p>
Our sense of <em>emergent</em> meets at least the “weak” criterion enunciated <a href="http://consc.net/papers/emergence.pdf">here</a> by David Chalmers, a sense of unexpectedness <a href="https://rjlipton.wpcomstaging.com/2015/10/29/guessing-conjectures/">that</a> we <a href="https://rjlipton.wpcomstaging.com/2011/04/13/even-great-mathematicians-guess-wrong/">have</a> often <a href="https://rjlipton.wpcomstaging.com/2010/06/19/guessing-the-truth/">discussed</a> going <a href="https://rjlipton.wpcomstaging.com/2009/09/27/surprises-in-mathematics-and-theory/">back</a> to the <a href="https://rjlipton.wpcomstaging.com/2009/02/19/we-all-guessed-wrong/">beginning</a> of the blog. </p>
<p>
Chalmers’s strong sense, when applied to mathematics, leads into independence results of the kind effected by Kurt Gödel, this blog’s eponym. We realize that none of our 1,000+ posts has yet attempted a deep appraisal of what these independence results <em>mean</em>. We will not do so now because we are questioning something more basic: Discussion of Gödelian independence is with regard to a truth value that is presumed to exist. What if it doesn’t exist?</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/03/14/are-these-the-last-digits-of-pi/provemewrong/" rel="attachment wp-att-19741"><img alt="" class="aligncenter wp-image-19741" height="218" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/03/ProveMeWrong.jpg?resize=277%2C218&amp;ssl=1" width="277"/></a></p>
<p>
We are not just catching the tension of Platonism with <a href="https://en.wikipedia.org/wiki/Logical_positivism">logical positivism</a> and related paths to asking, what is science? We are asking whether reality aligns with the position that we already feel is best for <em>practice</em>. </p>
<p>
“Emergent Mathematics” is a teaching movement that, in the words of one prominent <a href="https://www.jstor.org/stable/23434871">paper</a>, gets away from mathematics courses that “are focused on completed results that often hide the messiness and complication that led to their production.” In trying to explain much data showing that the most experienced mathematicians are not the most accomplished teachers, the paper’s two authors seem to identify the former with the position of putting emphasis on completed results. They seek the best attitude for childhood <em>learning</em> of mathematics, and believe it to be orthogonal to that of presenting finished mathematics.  But going another 90 degrees around the dial, perhaps their compass needle’s other end points to the best philosophical position for <em>creating</em> mathematics.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
I think we would all still agree that the next trillion digits of <img alt="{\pi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> currently exist. The tougher question is whether it is scientifically meaningful to postulate knowledge of them, without knowing them. We may get an opinion on that from a regular friend late Wednesday into Thursday.</p></font></font></div>
    </content>
    <updated>2022-03-14T20:34:36Z</updated>
    <published>2022-03-14T20:34:36Z</published>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="P=NP"/>
    <category term="Proofs"/>
    <category term="emergence"/>
    <category term="Heiko R&#xF6;lke"/>
    <category term="Pi"/>
    <category term="Pi Day"/>
    <category term="program correctness"/>
    <category term="records"/>
    <category term="Thomas Keller"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2022-04-01T06:37:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://www.solipsistslog.com/?p=551</id>
    <link href="http://www.solipsistslog.com/a-mysterious-constant-called-pi-arising-from-the-gaussian-integral-with-a-minor-application-to-circles/" rel="alternate" type="text/html"/>
    <title>A mysterious constant called pi, arising from the Gaussian integral (with a minor application to circles)</title>
    <summary>Hi, nerd blog! (This is a post that I wrote a long time ago and then never published. I figured it would be nice to publish it on March 14th.) Today, we’re interested in the Gaussian integral     for . This integral of course has lots of very serious practical applications, as it arises […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Hi, nerd blog! (This is a post that I wrote a long time ago and then never published. I figured it would be nice to publish it on March 14th.)</p>



<p>Today, we’re interested in the Gaussian integral </p><p class="ql-center-displayed-equation" style="line-height: 41px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[f(C) := \int_{-\infty}^\infty e^{-Cx^2} {\rm d} x\]" class="ql-img-displayed-equation quicklatex-auto-format" height="41" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-096cdf537e620c218a57694e9aca2bf4_l3.png" title="Rendered by QuickLaTeX.com" width="170"/></p> for <img alt="C &gt; 0" class="ql-img-inline-formula quicklatex-auto-format" height="12" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-87ddd944257c5bf57009a80226e5c414_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="47"/>. This integral of course has lots of very serious practical applications, as it arises naturally in the study of the Gaussian/normal distribution. But, more importantly, it’s a lot of fun to play with and is simply beautiful. We’ll see a bit about what it makes it so pretty below. We start by simply trying to figure out the value of this thing, which isn’t super easy.<p/>



<p>By a change of variables, we immediately see that <img alt="f(C)" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-051ceeda9119a5fc9be66053f9dd40a3_l3.png" title="Rendered by QuickLaTeX.com" width="37"/> is proportional to <img alt="1/\sqrt{C}" class="ql-img-inline-formula quicklatex-auto-format" height="21" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-94fd71739495e33d6c86fe0b70cdbcf1_l3.png" title="Rendered by QuickLaTeX.com" width="46"/>. But, what is the constant of proportionality? It’s actually nicer to ask a slightly different question: what is the unique value of <img alt="C" class="ql-img-inline-formula quicklatex-auto-format" height="12" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-f34f74d98915e33f37a086f8cbfb996a_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="14"/> such that <img alt="f(C) = 1" class="ql-img-inline-formula quicklatex-auto-format" height="18" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-a4141e7514a6f611842effecf51c1199_l3.png" title="Rendered by QuickLaTeX.com" width="70"/>. A quick numerical computation shows that <img alt="f(3.14159) \approx 1" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-24b068172655b4e41a85afb20c734882_l3.png" title="Rendered by QuickLaTeX.com" width="114"/>. E.g., here’s some Mathematica code to find this value:<br/><img alt="" class="wp-image-552" height="110" src="http://www.solipsistslog.com/wp-content/uploads/2021/03/Screen-Shot-2021-03-27-at-3.25.48-PM.png" style="width: 400px;" width="1066"/>.</p>



<p>This constant <img alt="C \approx 3.14159" class="ql-img-inline-formula quicklatex-auto-format" height="13" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-3351e9436eef51bd249c328e212df088_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="97"/> is so important for this blog post that it is worth giving it a name. So, I looked through the Greek alphabet for a nice letter that doesn’t get used much and chose the obscure lowercase letter <img alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/>—spelled <em>pi</em> in English, and pronounced like “pie”. In other words, by definition <img alt="f(\pi) = 1" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-64cecd17d70571e9f6e5d0e701cda533_l3.png" title="Rendered by QuickLaTeX.com" width="67"/>. (If this implicit definition bothers you, we can equivalently just define <img alt="\pi := f(1)^{2}" class="ql-img-inline-formula quicklatex-auto-format" height="20" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-fd24a2e8eab1e07787b7429db8502015_l3.png" title="Rendered by QuickLaTeX.com" width="80"/>. But, I find the implicit definition to be more elegant.)</p>



<p>So, we have this brand new mysterious constant <img alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/>. What should we do with it? It is of course natural to try to find different expressions for it (though our integral expression can already be used to compute it to quite high precision). A first idea is to apply the change of variables <img alt="u = \pi x^2" class="ql-img-inline-formula quicklatex-auto-format" height="15" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-fcb6cf4e246632c794949355411b046b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="62"/> to obtain </p><p class="ql-center-displayed-equation" style="line-height: 41px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[1 = 2\int_{0}^{\infty} e^{-\pi x^2}{\rm d} x = \pi^{-1/2} \int_0^{\infty} e^{-u}/u^{1/2} {\rm d} u\; .\]" class="ql-img-displayed-equation quicklatex-auto-format" height="41" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-f10d9cf7ba99c79b8b6410cfbffbcb0d_l3.png" title="Rendered by QuickLaTeX.com" width="343"/></p> So, <p class="ql-center-displayed-equation" style="line-height: 41px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[\pi =\Big( \int_0^\infty e^{-u}/u^{1/2} {\rm d} u\Big)^2\; ,\]" class="ql-img-displayed-equation quicklatex-auto-format" height="41" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-a70346615d75d7bd8aad3e871d66f99c_l3.png" title="Rendered by QuickLaTeX.com" width="199"/></p> which you might recognize as the square of the <a href="https://en.wikipedia.org/wiki/Gamma_function" rel="noreferrer noopener" target="_blank">Gamma function</a> evaluated at <img alt="1/2" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-872abde626b8cdf8e983a3345ee98925_l3.png" title="Rendered by QuickLaTeX.com" width="25"/>, i.e., <img alt="\pi = \Gamma(1/2)^2" class="ql-img-inline-formula quicklatex-auto-format" height="20" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-f89e7a589afd5b47baf442a3c38013ee_l3.png" title="Rendered by QuickLaTeX.com" width="93"/>. (Recalling that <img alt="\Gamma(n) = (n-1)!" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-59278ef14f8421f91a9c38456784aa70_l3.png" title="Rendered by QuickLaTeX.com" width="118"/> for integer <img alt="n" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/>, one might interpret this as saying that <img alt="\sqrt{\pi}" class="ql-img-inline-formula quicklatex-auto-format" height="18" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-a0df3b7b3c88b80bd1b77220c28fb5ec_l3.png" title="Rendered by QuickLaTeX.com" width="25"/> is “the factorial of <img alt="-1/2" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b5205e45813b524be8d323d4b2d0fade_l3.png" title="Rendered by QuickLaTeX.com" width="39"/>.”) <p/>



<p>This mysterious identity will play a key role later. We could, of course, find other identities involving this new constant <img alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/>. But, I thought instead I’d jump ahead to a rather obscure fact about the relationship between <img alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/> and a circle.</p>



<h2>Our constant’s relationship with circles</h2>



<p>In my opinion, the Gaussian distribution is far more interesting in dimensions larger than one. In particular, consider the distribution on <img alt="\mathbb{R}^n" class="ql-img-inline-formula quicklatex-auto-format" height="12" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-f9868b4451c5811a288f7fdd10be5558_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="21"/> given by the probability density function </p><p class="ql-center-displayed-equation" style="line-height: 24px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[\Pr[\mathbf{x}] = e^{-\pi \|\mathbf{x}\|^2}\; .\]" class="ql-img-displayed-equation quicklatex-auto-format" height="24" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-cbeee2e0bc6e4a6f5288818cb4aa2a7a_l3.png" title="Rendered by QuickLaTeX.com" width="129"/></p> Notice that <p class="ql-center-displayed-equation" style="line-height: 41px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[\int_{\mathbb{R}^n}e^{-\pi \|\mathbf{x}\|^2} {\rm d} \mathbf{x} = \int_{-\infty}^{\infty} \cdots \int_{-\infty}^{\infty} e^{-\pi x_1^2 -\cdots - \pi x_n^2} {\rm d}x_1 \ldots {\rm d} x_n = 1\; ,\]" class="ql-img-displayed-equation quicklatex-auto-format" height="41" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-8c88f8ef8350b5bf5e3924147dbed5de_l3.png" title="Rendered by QuickLaTeX.com" width="457"/></p> so that this is in fact a distribution. <p/>



<p>In fact, up to scaling, this distribution is the <em>unique</em> continuous radial product distribution—i.e., the unique distribution such that <img alt="\Pr[\mathbf{x}]" class="ql-img-inline-formula quicklatex-auto-format" height="18" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-17a91f072153c1ba54d02a280dccc979_l3.png" title="Rendered by QuickLaTeX.com" width="38"/> can be written both as a function only of the norm of <img alt="\mathbf{x}" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-bcda923e732ff6e429d93d0fa7ea8a47_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/>, <img alt="\Pr[\mathbf{x}] = f^*(\|\mathbf{x}\|)" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-2c1674e9adcb304d86d51e6e3e89bd93_l3.png" title="Rendered by QuickLaTeX.com" width="124"/> for some continuous function <img alt="f^*" class="ql-img-inline-formula quicklatex-auto-format" height="16" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-d1bc0c1e9f63254ef68d09d8ea3e316d_l3.png" title="Rendered by QuickLaTeX.com" width="17"/>, <em>and</em> as a product of functions of its coordinates, <img alt="\Pr[\mathbf{x}] = f_1(x_1)\cdots f_n(x_n)" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-724a32e8712eb42c68f40510a4461d33_l3.png" title="Rendered by QuickLaTeX.com" width="188"/>. This makes the Gaussian a uniquely powerful tool for reducing complicated multi-dimensional problems to one-dimensional problems. </p>



<p>For example, suppose that for some strange reason we wish to know the circumference of a circle with radius one. (If we were less civilized mathematicians, we might instead set the diameter to be equal to <img alt="1" class="ql-img-inline-formula quicklatex-auto-format" height="13" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-4868771cbc422b5818f85500909ce433_l3.png" title="Rendered by QuickLaTeX.com" width="7"/>, so that the radius would be <img alt="1/2" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-872abde626b8cdf8e983a3345ee98925_l3.png" title="Rendered by QuickLaTeX.com" width="25"/>.) We can try to write this as some kind of path integral or something—and suffer quite a bit in the process—or we can use the following beautiful trick. We can write<br/></p><p class="ql-center-displayed-equation" style="line-height: 41px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[1 = \int_{\mathbb{R}^2} e^{-\pi \|x\|^2} {\rm d} x = \int_0^\infty e^{-\pi r^2} \sigma_r {\rm d} r= \sigma_1 \int_0^\infty e^{-\pi r^2} r {\rm d} r\;,\]" class="ql-img-displayed-equation quicklatex-auto-format" height="41" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-7d71e3575dad757294955cdf449254fa_l3.png" title="Rendered by QuickLaTeX.com" width="430"/></p><br/>where <img alt="\sigma_r" class="ql-img-inline-formula quicklatex-auto-format" height="11" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-192e8dc0de46182a65dec93ebf8b5081_l3.png" title="Rendered by QuickLaTeX.com" width="16"/> is the circumference of a circle of with radius <img alt="r" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-c409433a9e2dfcdb83360a974d243f18_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="8"/>. (The <em>only</em> facts that we have used here are our definition of <img alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/> together with the fact that <img alt="\sigma_r = r \sigma_1" class="ql-img-inline-formula quicklatex-auto-format" height="11" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-c8ca93d60deca29a9a89487ef2eddf50_l3.png" title="Rendered by QuickLaTeX.com" width="66"/>.) Fortunately, the last integral is easy to compute as <br/><p class="ql-center-displayed-equation" style="line-height: 41px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[\int_0^\infty e^{-\pi r^2} r {\rm d} r = \frac{1}{2\pi} \cdot \int_0^\infty e^{-u} {\rm d} u = \frac{1}{2\pi} \;.\]" class="ql-img-displayed-equation quicklatex-auto-format" height="41" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-f7b0b0433407f63b0c9f6682c331ab63_l3.png" title="Rendered by QuickLaTeX.com" width="303"/></p> Rearranging, we see that <img alt="\sigma_1 = 2\pi" class="ql-img-inline-formula quicklatex-auto-format" height="15" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-bceb0469014bc513c98176ca8e9baf1d_l3.png" title="Rendered by QuickLaTeX.com" width="61"/>!<p/>



<p>So, surprisingly, our mysterious constant <img alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/> is actually intimately related with the circumference of a circle. (If we were less civilized mathematicians, we might even have simply defined <img alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/> to be the circumference of a circle with radius <img alt="1/2" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-872abde626b8cdf8e983a3345ee98925_l3.png" title="Rendered by QuickLaTeX.com" width="25"/>.)</p>



<h2>What’s so special about two dimensions? Surface area of n-spheres.</h2>



<p>But, why stop in dimension <img alt="2" class="ql-img-inline-formula quicklatex-auto-format" height="12" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-e584dd0bab4e6c8efc164939c28db757_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="8"/>? This same <em>one neat trick</em> is just as useful in higher dimensions. E.g., what is the surface area <img alt="\sigma_1^{(n-1)}" class="ql-img-inline-formula quicklatex-auto-format" height="24" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-e14b74f2c92c5d34a31da4576bfaecc9_l3.png" title="Rendered by QuickLaTeX.com" width="46"/> of a unit sphere in <img alt="n" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/> dimensions? (Conventionally, we write the <img alt="n" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/>-sphere as <img alt="S^{n-1}" class="ql-img-inline-formula quicklatex-auto-format" height="15" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-8897f8b728befe1f7421e387094e6fd4_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="37"/> because it as an <img alt="(n-1)" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-8c10e83257454bc711811cc71f964a7e_l3.png" title="Rendered by QuickLaTeX.com" width="53"/>-dimensional object that happens to be embedded in <img alt="n" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/>-dimensional space. This is why I write <img alt="\sigma^{(n-1)}_1" class="ql-img-inline-formula quicklatex-auto-format" height="24" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-1529a8cd27bcb6dcd64708282b0f0f0f_l3.png" title="Rendered by QuickLaTeX.com" width="46"/> for its surface area.) Well, we have </p><p class="ql-center-displayed-equation" style="line-height: 41px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[1 = \int_{-\mathbb{R}^n} e^{-\pi \|x\|^2} {\rm d} x = \int_0^\infty e^{-\pi r^2} \sigma^{(n-1)}_r {\rm d} r= \sigma^{(n-1)}_1 \int_0^\infty e^{-\pi r^2} r^{n-1} {\rm d} r\; .\]" class="ql-img-displayed-equation quicklatex-auto-format" height="41" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-f260e2a2d6410051fb74cc87df3c7792_l3.png" title="Rendered by QuickLaTeX.com" width="531"/></p> (Again, the only property that I have used here is that <img alt="\sigma_r^{(n-1)} = r^{n-1} \sigma_1^{(n-1)}" class="ql-img-inline-formula quicklatex-auto-format" height="24" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b61ebafad59fad72671514841700ebab_l3.png" title="Rendered by QuickLaTeX.com" width="153"/>.) This integral is a bit less pretty, but using the same approach, we see that  <p class="ql-center-displayed-equation" style="line-height: 42px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[\int_0^\infty e^{-\pi r^2} r^{n-1} {\rm d} r = \frac{1}{2\pi^{n/2}} \cdot \int_0^{\infty} e^{-u} u^{n/2-1} {\rm d} u = \frac{\Gamma(n/2)}{2\pi^{n/2}}\; ,\]" class="ql-img-displayed-equation quicklatex-auto-format" height="42" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-65c828363c086f1b820b730be79a4c39_l3.png" title="Rendered by QuickLaTeX.com" width="437"/></p> where the last step is literally just plugging in the definition of the Gamma function. Rearranging, we see that the surface area of the unit sphere in <img alt="n" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/> dimensions is exactly <img alt="\frac{2\pi^{n/2}}{\Gamma(n/2)}" class="ql-img-inline-formula quicklatex-auto-format" height="30" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-50d143fa8519caff967068ce2d9de43f_l3.png" title="Rendered by QuickLaTeX.com" width="42"/>.<p/>



<p>If the Gamma function intimidates you, that’s fine. (It certainly intimidates me.) We can go a bit further by remembering that for integers <img alt="m" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-6b41df788161942c6f98604d37de8098_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="15"/>, <img alt="\Gamma(m) = (m-1)!" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-d226e3a5659b079c31cbf51beb7d6f69_l3.png" title="Rendered by QuickLaTeX.com" width="128"/>, while <img alt="\Gamma(m+1/2) = \sqrt{\pi}(2m)!/(4^m m!)" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-5d2b39f9e1062dcc5fa00ec58d82eb91_l3.png" title="Rendered by QuickLaTeX.com" width="246"/>. (Both of these identities follow from the relation <img alt="\Gamma(x) = (x-1) \Gamma(x-1)" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-fb2599738079fdbd25e5b92aa6f75617_l3.png" title="Rendered by QuickLaTeX.com" width="178"/>, which follows from integration by parts, together with the values <img alt="\Gamma(1) = 1" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-53c41acb4ea9695fbe3fbed4d494e73a_l3.png" title="Rendered by QuickLaTeX.com" width="65"/> and <img alt="\Gamma(1/2) = \sqrt{\pi}" class="ql-img-inline-formula quicklatex-auto-format" height="19" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-119c76244632be47b3cddb44bcce032b_l3.png" title="Rendered by QuickLaTeX.com" width="101"/>.) </p>



<p>Then, we see that the surface area of the unit sphere in <img alt="n" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/> dimensions is </p><p class="ql-center-displayed-equation" style="line-height: 55px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[\sigma_1^{(n-1)} = \begin{cases}\pi^{n/2} \cdot \frac{2}{(n/2-1)!} &amp;n\text{ even}\\\pi^{(n-1)/2} \cdot \frac{2^n \cdot ((n-1)/2)!}{(n-1)!} &amp;n\text{ odd}.\end{cases}\]" class="ql-img-displayed-equation quicklatex-auto-format" height="55" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-7093395acfb9701717087f97c5b73145_l3.png" title="Rendered by QuickLaTeX.com" width="318"/></p> In particular, from this formula, we immediately see the perhaps surprising fact that the surface area of the sphere in <img alt="n" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/> dimensions rapidly approaches <img alt="0" class="ql-img-inline-formula quicklatex-auto-format" height="12" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-a5e437be25f29374d30f66cd46adf81c_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="9"/> as <img alt="n \to \infty" class="ql-img-inline-formula quicklatex-auto-format" height="10" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-032ad3d6327a419ab33b269b23c31b7c_l3.png" title="Rendered by QuickLaTeX.com" width="55"/>. (I.e., “<img alt="n" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/>-dimensional unit spheres are tiny.”) We also see the rather strange fact that <img alt="\sigma_1^{(n-1)}" class="ql-img-inline-formula quicklatex-auto-format" height="24" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-e14b74f2c92c5d34a31da4576bfaecc9_l3.png" title="Rendered by QuickLaTeX.com" width="46"/> is a rational multiple of <img alt="\pi^{\lfloor n/2 \rfloor}" class="ql-img-inline-formula quicklatex-auto-format" height="17" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-362edd7540e9e43db2e65a0ddc45e684_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="43"/>.<p/>



<p>We can also plug in low values of <img alt="n" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/> to see what we get. E.g., I have heard that some people are interested in the case when <img alt="n = 2" class="ql-img-inline-formula quicklatex-auto-format" height="12" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-a47a3377f14fefc160d10b089a4aab45_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="42"/> and <img alt="n = 3" class="ql-img-inline-formula quicklatex-auto-format" height="12" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-8eda9fc1983d40517cca42fa671a0f51_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="43"/>. Plugging in, one sees that the circumference of a circle with radius one is <img alt="2\pi" class="ql-img-inline-formula quicklatex-auto-format" height="12" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-5bfa2124624f767670227d1aeab8d85c_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="20"/> (which, ok, we already saw before), and that the surface area of a sphere with radius one is <img alt="4\pi" class="ql-img-inline-formula quicklatex-auto-format" height="12" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-a64f86508ea52835b7fd42736282275d_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="20"/>. But, we can easily go farther: the surface area in four dimensions is <img alt="2\pi^2" class="ql-img-inline-formula quicklatex-auto-format" height="15" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-7249ec3f5fb52ad8207e0f9d873c7a4f_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="27"/>, and in five dimensions, it is <img alt="8\pi^{2}/3" class="ql-img-inline-formula quicklatex-auto-format" height="20" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-5b263bf1209b153f5824b853138f0510_l3.png" title="Rendered by QuickLaTeX.com" width="45"/>.</p>



<p>And, we can of course find the volume of the unit <img alt="n" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-b170995d512c659d8668b4e42e1fef6b_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/>-ball by computing a simple integral </p><p class="ql-center-displayed-equation" style="line-height: 44px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[V^{(n)} = \int_0^{1}\sigma_r^{(n)} {\rm} d r = \sigma_1^{(n)} \cdot \int_0^{1} r^{n-1} {\rm d} r = \sigma_1^{(n)}/n \; .\]" class="ql-img-displayed-equation quicklatex-auto-format" height="44" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-08b8932dcb424209e24a111106e869b2_l3.png" title="Rendered by QuickLaTeX.com" width="366"/></p><p/>



<p>In short, I think this mysterious constant <img alt="\pi" class="ql-img-inline-formula quicklatex-auto-format" height="8" src="http://www.solipsistslog.com/wp-content/ql-cache/quicklatex.com-26d6788550ffd50fe94542bb3e8ee615_l3.png" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/> is rather nice. Perhaps it will find other applications.</p></div>
    </content>
    <updated>2022-03-14T14:34:24Z</updated>
    <published>2022-03-14T14:34:24Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Noah Stephens-Davidowitz</name>
    </author>
    <source>
      <id>http://www.solipsistslog.com</id>
      <link href="http://www.solipsistslog.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="http://www.solipsistslog.com" rel="alternate" type="text/html"/>
      <subtitle>Life inside the confused and curious mind of a nerd.</subtitle>
      <title>Solipsist's Log</title>
      <updated>2022-03-31T22:42:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/039</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/039" rel="alternate" type="text/html"/>
    <title>TR22-039 |  Parallel Repetition For All 3-Player Games Over Binary Alphabet | 

	Kunal Mittal, 

	Uma Girish, 

	Justin Holmgren, 

	Wei Zhan, 

	Ran Raz</title>
    <summary>We prove that for every 3-player (3-prover) game, with binary questions and answers and value less than $1$, the value of the $n$-fold parallel repetition of the game decays polynomially fast to $0$. That is, for every such game, there exists a constant $c&gt;0$, such that the value of the $n$-fold parallel repetition of the game is at most $n^{-c}$.

Along the way to proving this theorem, we prove two additional parallel repetition theorems for multiplayer (multiprover) games, that may be of independent interest:

$\textbf{Playerwise Connected Games (with any number of players and any Alphabet size):}$ We identify a large class of multiplayer games and prove that for every game with value less than $1$ in that class, the value of the $n$-fold parallel repetition of the game decays polynomially fast to $0$.

More precisely, our result applies for $\textit{playerwise connected games}$, with any number of players and any alphabet size:
For each player $i$, we define the graph $G_i$, whose vertices are the possible questions for that player and two questions $x,x'$ are connected by an edge if there exists a vector $y$ of questions for all other players, such that both $(x,y)$ and $(x',y)$ are asked by the referee with non-zero probability. We say that the game is $\textit{playerwise connected}$ if for every $i$, the graph $G_i$ is connected.

Our class of playerwise connected games is strictly larger than the class of connected games that was defined in [DHVY17] and for which exponentially fast decay bounds are known [DHVY17]. For playerwise connected games that are not connected, only inverse Ackermann decay bounds were previously known [Ver96].

$\textbf{Exponential Bounds for the Anti-Correlation Game:}$ In the 3-player $\textit{anti-correlation game}$, two out of three players are given $1$ as input, and the remaining player is given $0$. The two players who were given $1$ must produce different outputs in $\{0,1\}$. We prove that the value of the $n$-fold parallel repetition of that game decays exponentially fast to $0$. That is, there exists a constant $c&gt;0$, such that the value of the $n$-fold parallel repetition of the game is at most $2^{-cn}$. Only inverse Ackermann decay bounds were previously known [Ver96].

The 3-player anti-correlation game was studied and motivated in several previous works. In particular, Holmgren and Yang gave it as an example for a 3-player game whose non-signaling value (is smaller than $1$ and yet) does not decrease at all under parallel repetition [HY19].</summary>
    <updated>2022-03-14T05:14:56Z</updated>
    <published>2022-03-14T05:14:56Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-04-01T06:37:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/038</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/038" rel="alternate" type="text/html"/>
    <title>TR22-038 |  Lower bounds for Polynomial Calculus with extension variables over finite fields | 

	Sasank Mouli, 

	Russell Impagliazzo, 

	Toniann Pitassi</title>
    <summary>For every prime p &gt; 0, every n &gt; 0 and ? = O(logn), we show the existence
of an unsatisfiable system of polynomial equations over O(n log n) variables of degree O(log n) such that any Polynomial Calculus refutation over F_p with M extension variables, each depending on at most ? original variables requires size exp(?????(n2/(?^2*2^?*(M + ????nlog(n))))) .</summary>
    <updated>2022-03-13T20:03:44Z</updated>
    <published>2022-03-13T20:03:44Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-04-01T06:37:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/03/13/ukraine-student-senior-research-fellows-at-tel-aviv-university-apply-by-june-1-2022/</id>
    <link href="https://cstheory-jobs.org/2022/03/13/ukraine-student-senior-research-fellows-at-tel-aviv-university-apply-by-june-1-2022/" rel="alternate" type="text/html"/>
    <title>Ukraine student / senior research fellows at Tel Aviv University (apply by June 1, 2022)</title>
    <summary>Tel Aviv University offers emergency scholarship for research students from Ukraine (see attached). Further, I (Gil Cohen) have immediately available senior / students research fellows in theoretical computer science, coding theory, spectral graph theory, and adjacent mathematical branches. Website: https://c1f423b8-ee8e-41b1-a3a7-2cfc865115ec.filesusr.com/ugd/d112fa_4de343bf5b3a410eae40b3853dcef087.pdf Email: coheng@gmail.com</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Tel Aviv University offers emergency scholarship for research students from Ukraine (see attached). Further, I (Gil Cohen) have immediately available senior / students research fellows in theoretical computer science, coding theory, spectral graph theory, and adjacent mathematical branches.</p>
<p>Website: <a href="https://c1f423b8-ee8e-41b1-a3a7-2cfc865115ec.filesusr.com/ugd/d112fa_4de343bf5b3a410eae40b3853dcef087.pdf">https://c1f423b8-ee8e-41b1-a3a7-2cfc865115ec.filesusr.com/ugd/d112fa_4de343bf5b3a410eae40b3853dcef087.pdf</a><br/>
Email: coheng@gmail.com</p></div>
    </content>
    <updated>2022-03-13T11:10:32Z</updated>
    <published>2022-03-13T11:10:32Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-04-01T06:37:59Z</updated>
    </source>
  </entry>
</feed>
