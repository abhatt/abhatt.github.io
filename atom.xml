<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-05-17T22:50:31Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7732</id>
    <link href="https://windowsontheory.org/2020/05/17/foundations-of-responsible-computing/" rel="alternate" type="text/html"/>
    <title>Foundations of responsible computing</title>
    <summary>[Hat tip: Aaron Roth] The inaugural conference on the foundations of responsible computing will take place in less than two weeks (June 1-2). Registration is FREE but you need to register by May 28. The program looks fantastic, and includes keynotes by Adrian Weller, Rakesh Vohra, Patricia Williams, and Jon Kleinberg, as well as a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>[Hat tip: Aaron Roth]</p>



<p>The inaugural <a href="https://responsiblecomputing.org/">conference on the foundations of responsible computing</a> will take place in less than two weeks (June 1-2). Registration is FREE but you need to register by May 28.</p>



<p/>



<p>The <a href="https://responsiblecomputing.org/program/">program</a> looks fantastic, and includes keynotes by <strong>Adrian Weller</strong>, <strong>Rakesh Vohra</strong>, <strong>Patricia Williams</strong>, and <strong>Jon Kleinberg</strong>,  as well as a set of (very interesting, judging by the titles) contributed talks.</p>



<p/>



<p/>



<p/></div>
    </content>
    <updated>2020-05-17T13:21:41Z</updated>
    <published>2020-05-17T13:21:41Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-05-17T22:35:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.06827</id>
    <link href="http://arxiv.org/abs/2005.06827" rel="alternate" type="text/html"/>
    <title>Shortest Distances as Enumeration Problem</title>
    <feedworld_mtime>1589673600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Casel:Katrin.html">Katrin Casel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Friedrich:Tobias.html">Tobias Friedrich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Neubert:Stefan.html">Stefan Neubert</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schmid:Markus_L=.html">Markus L. Schmid</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.06827">PDF</a><br/><b>Abstract: </b>We investigate the single source shortest distance (SSSD) and all pairs
shortest distance (APSD) problems as enumeration problems (on unweighted and
integer weighted graphs), meaning that the shortest distances are produced and
listed one by one without repetition. The performance is measured in the RAM
model of computation with respect to preprocessing time and delay, i.e., the
maximum time that elapses between two consecutive outputs. This point of view
reveals that specific types of output (e.g., excluding non-reachability
information, excluding the self-distances) and the order of enumeration (e.g.,
sorted by distance, row-wise w.r.t. the distance matrix) have a huge impact on
the complexity of APSD while they appear to have no effect on SSSD.
</p>
<p>In particular, we show for APSD that enumeration without output restrictions
is possible with delay in the order of the average degree. Excluding
non-reachability information, or requesting the output to be sorted by
distance, increases this delay to the order of the maximum degree. Further, for
weighted graphs, a delay in the order of the average degree is also not
possible without preprocessing or considering self-distances as output. In
contrast, for SSSD we find that a delay in the order of the maximum degree
without preprocessing is attainable and unavoidable for any of these
requirements.
</p></div>
    </summary>
    <updated>2020-05-17T22:33:25Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-15T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17042</id>
    <link href="https://rjlipton.wordpress.com/2020/05/16/the-2020-y-prize/" rel="alternate" type="text/html"/>
    <title>The 2020 Y Prize</title>
    <summary>Can one maintain privacy while publicly congratulating? Cropped from X Facemask src X, who shall go nameless here, recently won a prize for research. Today we congratulate X and wish to talk a bit about their work while safeguarding X’s identity. X also won another prize Z—but we will not mention which one. In both […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Can one maintain privacy while publicly congratulating?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2020/05/x-facemaskman.jpg"><img alt="" class="alignright size-thumbnail wp-image-17044" height="129" src="https://rjlipton.files.wordpress.com/2020/05/x-facemaskman.jpg?w=150&amp;h=129" width="150"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from X Facemask <a href="https://www.anime-remix.com/product/face-mask-cotton-cloth-designer-graphic-x/">src</a></font></td>
</tr>
</tbody>
</table>
<p>
X, who shall go nameless here, recently won a prize for research. </p>
<p>
Today we congratulate X and wish to talk a bit about their work while safeguarding X’s identity.</p>
<p>
X also won another prize Z—but we will not mention which one. In both cases the honor is about the creation of a new field rather than the solution of a long-standing open problem. Her—OK, X is a she—brilliant work has allowed many others to write papers in these areas. And it has led to countless conferences, meetings, and talks. </p>
<p>
Oops, we used the word “brilliant.” That has already leaked her identity, given where you are reading this. Just type it into our search box.</p>
<p>
We assume you get the point. One can leak information about people without mentioning them directly. </p>
<p/><p>
<br/><br/>
<br/><br/>
</p><h2> The Real Start of the Post</h2><p/>
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2020/05/dworkaf.jpg"><img alt="" class="alignright wp-image-17045" height="138" src="https://rjlipton.files.wordpress.com/2020/05/dworkaf.jpg?w=180&amp;h=138" width="180"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Her <a href="https://royalsociety.org/science-events-and-lectures/2018/06/you-and-ai-fairness/">work</a> in Algorithmic Fairness too</font><br/>
</td></tr></tbody>
</table>
<p>
Cynthia Dwork is the <a href="http://www.sigact.org/prizes/knuth/citation2020.pdf">winner</a> of the 2020 Donald E. Knuth Prize. The prize is awarded jointly by ACM SIGACT and the IEEE Computer Society Technical Committee. The ceremony will include a lecture by Cynthia at <a href="https://focs2020.cs.duke.edu/">FOCS 2020</a>, which is still projected to meet in-person at Duke next November 16–19.</p>
<p>
Today we congratulate Cynthia and talk a little about her work.</p>
<p>
I, Dick, have known her for many years. I am excited to see that she has won the Knuth prize. </p>
<p>
She has always been a top researcher and a terrific speaker. I fondly recall some of her earlier talks at Princeton and elsewhere. There is something about her ability to explain complex ideas in a lucid manner. I have often thought that this ability must be one of the reasons she has been so successful in her research. </p>
<p>
We will talk a little about <em>differential privacy</em> (DP) and then her other joint work which led to another prize. We should be quick to <a href="https://rjlipton.wordpress.com/2015/02/06/cynthia-dwork-and-a-brilliant-idea/">mention</a> that <a href="https://rjlipton.wordpress.com/2015/02/07/still-a-brilliant-idea/">others</a> have been instrumental in both developments.</p>
<p>
</p><p/><h2> Quantifying an Idea </h2><p/>
<p/><p>
DP might have been appreciated long before there were large databases and computer systems. The genius is not just the simplicity of the idea but the development of mathematical tools to govern its application.</p>
<p>
For a simple example, suppose a professor has a general policy of posting exam average scores in her class of 100. Suppose she posts that the average is 73.3 after 98 students have taken the exam but two need to make it up a week later. Then suppose she re-posts the average after they take it and it is 72.5. The difference may not seem like much. But the class learns two things: </p>
<ul>
<li>
Both students had their makeup scores included in the average. (If the 73.3 is exact then it is not even possible to get a rounded 72.5 with just one score of zero, pending the other.) <p/>
</li><li>
The two students collectively bombed the exam. (Just possibly one got a zero while the other did OK, but even then the other’s score was below average.)
</li></ul>
<p>
Your first thought might be that the fault is in the excessive precision in the grade reporting. If she had rounded to the nearest integer then both averages would have been reported as “73” and there would be no problem. But rounding is accidental. The new average could have been 72.49, which would be rounded down to “72,” when the set of possible interpretations is worse than before. </p>
<p>
In wide-scale situations where queries for multiple averages at different times are allowed, effects of rounding may cancel so that a large statistical analysis can subtract them out. Thus passively reducing precision is not enough. The key is to affect the precision actively by adding a random <img alt="{\pm \epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\pm \epsilon}"/> to the reported results, where <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/> is judiciously chosen. The point is:</p>
<blockquote><p><b> </b> <em> Adding <img alt="{\pm \epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+%5Cepsilon%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\pm \epsilon}"/>, better than rounding, makes inference much harder. </em>
</p></blockquote>
<p>In the exam-score example, randomly adding or subtracting a number between 0 and 2 changes the calculus so that seeing “73.3” and then “72.5” does not allow a high-confidence inference about the makeup students’ scores. One could reason that the first number could easily have been 72.3 and the second 73.5, in line with interpretations where the makeup students upped the average. Actually, a jump from 72.3 to 73.5 in the <em>true</em> averages is impossible from just two new scores, so the class could deduce that the reported averages are fudged. But that is OK. Two basic purposes of reporting the averages are met despite the fudging: </p>
<ul>
<li>
to show the exam was consistent with past years—especially good to know this year; <p/>
</li><li>
to give you some idea (knowing your own score exactly) where you stand relative to the class.
</li></ul>
<p>
In related situations, one could argue that a mean of <img alt="{\pm 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\pm 1}"/> for <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/> is not enough. If the exam has 100 one-point questions each with a 73.3% expectation of getting it right for everyone, then the standard deviation about the mean is almost <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>. Using a mean change of <img alt="{\pm 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\pm 2}"/> seems also fine in this case, whereas rounding to the nearest 05 (i.e., reporting “70” or “75” as the average) might be too coarse.</p>
<p>
We do not intend our example to be definitive, only that it conveys relevant issues. The example is enough to convey that DP leads into deep matters in statistics and <em>what can be inferred by statistical methods</em>. The latter melds the complexity-theoretic notion of knowledge gain from interactive protocols with statistical (machine) learning. And as with quantum computing, theorems and methods from the subfield have come back to inform fundamental questions about complexity (of course, this is true of crypto in general).</p>
<p>
</p><p/><h2> Privacy and Online Chess </h2><p/>
<p/><p>
I, Ken, am the lead on this post, and have had data-privacy affect me personally. The pandemic has forced the cancellation of all over-the-board (OTB) tournaments, some as far ahead as August. Chess has all moved online, as heralded by <a href="https://www.nytimes.com/2020/05/08/sports/coronavirus-chess-online-tournament.html">two</a> recent <a href="https://www.nytimes.com/2020/04/23/sports/chess-cheating-south-dakota-races.html">articles</a> in the New York Times and one in the Wall Street Journal <a href="https://www.wsj.com/articles/forget-sourdough-everyone-is-playing-chess-now-11588845643">proclaiming</a>, “Sports: Chess Is the New King of the Pandemic.” </p>
<p>
As these articles also note, cheating has long been a major concern for online chess. I have been a recourse for second-opinions in online cheating cases but have not tried to be <em>primary</em> in this domain. A great conversation in November 2006 with Daniel Sleator (who—besides being famous for splay trees and amortized analysis—<a href="https://en.wikipedia.org/wiki/Internet_Chess_Club#History">co-founded</a> the Internet Chess Club) told me how online cheating detection avails much information about the manner of play that is not applicable or reliably available in OTB chess. Also for academic reasons, I chose a minimal approach of using no information besides the moves of the games and the <a href="https://en.wikipedia.org/wiki/Elo_rating_system">Elo ratings</a> of the players. An unexpected advantage of that choice is exemplified by an e-mail I received—fifteen minutes before a conference call on Wednesday with representatives of the International Chess Federation (FIDE) and other online platforms—from a different chess organizer requesting a statement that:</p>
<blockquote><p><b> </b> <em> …you will only use the data of the games, and not any personal data that might be obtained from the [tournaments] for your research, such as player name etc. … in writing as a standard precaution … to make sure that there isn’t third party usage of the data or usage outside of its intent. </em>
</p></blockquote>
<p/><p>
Some recent legal decisions have confirmed that the moves of the games are public data and cannot be copyrighted. The e-mail did not mention the players’ ratings. As used by FIDE and the US and most online platforms (<a href="https://fivethirtyeight.com/features/how-we-calculate-nba-elo-ratings/">and</a> <a href="https://en.wikipedia.org/wiki/World_Football_Elo_Ratings">much</a> <a href="https://dotesports.com/general/news/elo-ratings-explained-20565">more</a> <a href="https://fivethirtyeight.com/methodology/how-our-nfl-predictions-work/">widely</a>), those are 4-digit numbers, and they are public. Knowing all four digits might be enough to pinpoint the identity of a player. </p>
<p>
Thus I suggested rounding the rating to the nearest 10 or even 20. This makes little difference to my statistical cheating tests, which already give 25 Elo points benefit-of-doubt for uncertainty about the rating. Tournament officials I’ve served have done this in a couple of individual cases but now the need is <em>en masse</em>. </p>
<p>
The question based on the previous section above is whether I should instead (or also) suggest that they implement full DP by adding <img alt="{\pm \epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\pm \epsilon}"/> with a mean of <img alt="{\pm 10}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+10%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\pm 10}"/> or <img alt="{\pm 20}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+20%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\pm 20}"/> or so, with-or-without rounding. A caveat is that it is easy for someone with a tournament spreadsheet to round a column of figures before exporting text to me, but applying a random function is less familiar and prone to major mess-ups.</p>
<p>
</p><p/><h2> The Other Prize </h2><p/>
<p/><p>
The Knuth Prize is not Cynthia’s only win for 2020. In December she was awarded the 2020 IEEE Richard W. Hamming Medal. A <a href="https://ieeetv.ieee.org/ieeetv-specials/honors-2020-cynthia-dwork-wins-the-ieee-richard-w-hamming-medal">video</a> gives a one-minute summary of differential privacy and her work on <em>non-malleable</em> cryptosystems via lattice bases. The latter stems from her 2000 <a href="https://www.cs.huji.ac.il/~dolev/pubs/nmc.pdf">paper</a> with Danny Dolev and Moni Naor (DDN).</p>
<p>
We have not covered this crypto topic before. It involves chosen-ciphertext attacks where Eve, the evesdropper, first gets hold of a transmitted ciphertext <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/>. The cryptosystem is <em>malleable</em> if Eve can know enough about the structure of <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> to make a legal ciphertext <img alt="{y'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y'}"/>, one that may decode to a plaintext <img alt="{x'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x'}"/> that advances her interests compared to the intended <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>. Wikipedia’s <a href="https://en.wikipedia.org/wiki/Malleability_(cryptography)">article</a> gives an example where <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> embeds a numerical field <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> that Eve can recognize and alter en-route. The paper gives a similar example where Eve cannot disrupt the transmission of <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> but can separately send her own <img alt="{y'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y'}"/> with a value <img alt="{r'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r'}"/> that gives her advantage. This may be so even without being able to decode or gain any information about the plain value of <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/>, except that <img alt="{r'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r'}"/> is different in a meaningful way. For instance, <img alt="{r'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r'}"/> could be a lower bid on a contract or higher bid in a secure auction.</p>
<p>
One would think that <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> should just be encoded via an error-correcting code whose range is secure so that an attacker would be overwhelmingly unlikely to find another value <img alt="{r'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r'}"/> in the legal range of the code. However, this clashes with the goal of <em>homomorphic encryption</em> which allows numerical operations on encoded values. Or the technology used may be too simple, as in the paper’s mention of submitting bids to a fax number. </p>
<p>
Note: both fax machines and the paper date to the previous millennium. But the paper next gives an example that is staying with us in this millennium, where <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> is a proof of <img alt="{\mathsf{P \neq NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%5Cneq+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{P \neq NP}}"/>. Suppose Alice arranges to transmit <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> to a referee Ralph using a zero-knowledge protocol, but Eve is able to intercept their messages. Eve can impersonate Ralph to Alice and impersonate Alice to Ralph as Ralph asks questions in the protocol. It may seem impossible to forestall such a “man-in-the-middle” <a href="https://en.wikipedia.org/wiki/Man-in-the-middle_attack">attack</a> without extrinsic considerations of hardware and timing, but DDN design an ingenious scheme for <em>non-malleable zero-knowledge proofs</em>. </p>
<p>
</p><p/><h2> A Video </h2><p/>
<p/><p>
<em>En-passant</em> of this, we want to note a great <a href="https://www.youtube.com/watch?v=4Wl-3kadvgw">video</a> that was made for next month’s Women in Theory <a href="https://womenintheory.wordpress.com/">workshop</a>. It has an all-female cast and lyrics by Avi Wigderson parodying the song “I Will Survive.” Cynthia is not in the video, but she likewise has been a great spokesperson for the experiences of women in our professional life.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Our congratulations again to Cynthia—by name—on the Knuth and Hamming prizes.</p>
<p/></font></font></div>
    </content>
    <updated>2020-05-16T20:58:14Z</updated>
    <published>2020-05-16T20:58:14Z</published>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-05-17T22:31:19Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3483</id>
    <link href="https://agtb.wordpress.com/2020/05/16/foundations-of-responsible-computing-is-virtual/" rel="alternate" type="text/html"/>
    <title>The First Symposium on the Foundations of Responsible Computing Is Virtual [June 1st-2nd]</title>
    <summary>The First Symposium on Foundations of Responsible Computing (FORC 2020) is a forum for mathematical research in computation and society.  The Symposium aims to catalyze the formation of a community supportive of the application of theoretical computer science, statistics, economics and other relevant analytical fields to problems of pressing and anticipated societal concern. Lead by […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The <a href="https://responsiblecomputing.org/">First Symposium on Foundations of Responsible Computing (FORC 2020)</a> is a forum for mathematical research in computation and society.  The Symposium aims to catalyze the formation of a community supportive of the application of theoretical computer science, statistics, economics and other relevant analytical fields to problems of pressing and anticipated societal concern.</p>
<p>Lead by program chairs <a href="https://www.microsoft.com/en-us/research/people/dwork/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fpeople%2Fdwork%2F">Cynthia Dwork</a>, <a href="https://engineering.stanford.edu/people/omer-reingold">Omer Reingold</a>, and <a href="https://www.cis.upenn.edu/~aaroth/">Aaron</a><a href="https://www.cis.upenn.edu/~aaroth/"> Roth</a>; the virtual symposium is on June 1st and 2nd, 2020.  The program consists of keynote talks by <a href="http://mlg.eng.cam.ac.uk/adrian/">Adrian Weller</a>, <a href="https://sites.google.com/site/quaerereverum9/">Rakesh Vohra</a>, <a href="https://www.law.columbia.edu/faculty/patricia-j-williams">Patricia Williams</a>, and <a href="http://www.cs.cornell.edu/home/kleinber/">Jon Kleinberg</a>; and 21 contributed papers.  The conference is free but <a href="https://forms.gle/vgTzkMS6jLN9xPud8">registration is required</a>.  The <a href="https://responsiblecomputing.org/program/">full program</a> is available on the <a href="https://responsiblecomputing.org/">conference webpage</a>.</p>
<p> </p>
<p> </p></div>
    </content>
    <updated>2020-05-16T19:03:36Z</updated>
    <published>2020-05-16T19:03:36Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Jason Hartline</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2020-05-17T22:29:27Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/05/15/linkage</id>
    <link href="https://11011110.github.io/blog/2020/05/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Famous people’s bookshelves, visible as they teleconference from home (, via). The story calls this inadvertent, but that seems dubious. Much of my teleconferencing has books behind me, deliberately, mostly as a clean background in a part of the house where it’s convenient to sit and lighting is good, but also because very few views in my house avoid books. On the other hand, at least one colleague has substituted a fake background from a library…</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://www.nytimes.com/2020/04/30/books/celebrity-bookshelves-tv-coronavirus.html">Famous people’s bookshelves, visible as they teleconference from home</a> (<a href="https://mathstodon.xyz/@11011110/104097353853629567"/>, <a href="https://www.metafilter.com/186820/Famous-Peoples-Bookshelves">via</a>). The story calls this inadvertent, but that seems dubious. Much of my teleconferencing has books behind me, deliberately, mostly as a clean background in a part of the house where it’s convenient to sit and lighting is good, but also because very few views in my house avoid books. On the other hand, at least one colleague has substituted a fake background from a library…</p>
  </li>
  <li>
    <p><a href="http://faculty.smcm.edu/sgoldstine/pinecones.html">Susan Goldstine paints numbers onto the scales of pinecones</a> (<a href="https://mathstodon.xyz/@11011110/104102680813443034"/>) to show how phyllotaxis causes the Fibonacci numbers to line up.</p>
  </li>
  <li>
    <p>In a post on <a href="https://www.flyingcoloursmaths.co.uk/dictionary-of-mathematical-eponymy-the-quine-mccluskey-algorithm/">the Quine–McCluskey algorithm for minimizing Boolean functions</a> (<a href="https://mathstodon.xyz/@11011110/104114018052115751"/>), Colin Beveridge suggests that all mathematical Q-eponyms are named for Dan Quillen or Willard Quine. But even if you discount Quasi-abelian categories, Quasi-Hopf algebras, and Quasi-Newton methods (named after Quasi-Abel, Quasi-Hopf, and Quasi-Newton) there’s also <a href="https://en.wikipedia.org/wiki/Qvist%27s_theorem">Qvist’s theorem</a> in finite geometry, from Bertil Qvist.</p>
  </li>
  <li>
    <p><a href="https://www.math.columbia.edu/~woit/wordpress/?p=11723">Why the Szpiro conjecture is still a conjecture</a> (<a href="https://mathstodon.xyz/@11011110/104120078920669171"/>). See especially the linked collection of comments from the preceding post (with significant contributions from Fields medalist Peter Scholze) for details of why Scholze thinks Mochizuki’s claimed proof not only doesn’t work, but can’t work.</p>
  </li>
  <li>
    <p>Colin (not to be confused with Colin, above) <a href="https://mathstodon.xyz/@ColinTheMathmo/104123191826114771">asks for self-intersecting polyhedra with only hexagonal faces</a>. I described <a href="https://11011110.github.io/blog/2009/09/18/not-nauru-graph.html">a non-self-intersecting toroidal polyhedron with L-shaped hexagonal faces</a> in an earlier post, but I think the <a href="https://en.wikipedia.org/wiki/Small_triambic_icosahedron">small triambic icosahedron</a> (<a href="https://digital.lib.washington.edu/researchworks/handle/1773/4593">conjectured by Grünbaum to be the only face-symmetric polyhedron with more than five sides per face</a>) is closer to what he is asking for.</p>
  </li>
  <li>
    <p><a href="https://blog.computationalcomplexity.org/2020/05/why-is-there-no-dn-grid-for-hilberts.html">Bill Gasarch asks: why is there no grid for Hilbert’s 10th?</a> (<a href="https://mathstodon.xyz/@11011110/104131498905150965"/>). What he wants to know is, for which pairs  can we algorithmically find integer solutions to degree- -variable polynomial equations, and for which pairs is it undecidable? The answer seems to be: we can solve them when , we can’t solve them for some pairs of larger numbers, and there’s a big gap of unknown pairs.</p>
  </li>
  <li>
    <p><a href="https://euro-math-soc.eu/news/20/05/8/prize-winners-announced">EMS Prize, Klein Prize, and Neugebauer Prize</a> (<a href="https://mathstodon.xyz/@11011110/104133715419191450"/>). Winners: Karim Adiprasito, Ana Caraiani, Alexander Efimov, Simion Filip, Aleksandr Logunov, Kaisa Matomäki, Phan Thành Nam, Joaquim Serra, Jack Thorne, and Maryna Viazovska; Arnulf Jentzen; Karine Chemla.</p>
  </li>
  <li>
    <p><a href="https://cstheory.stackexchange.com/q/46746/95">On the complexity of a “list” datastructure in the RAM model</a> (<a href="https://mathstodon.xyz/@11011110/104142162759402806"/>). Linked lists can insert and delete at arbitrary positions. Arrays can find the value at position . Binary trees can do both, but with log time per operation. Combining methods for maintaining order in a list and integer ranking/unranking instead gives . This showed up just in time for me to add some of the ideas to the syllabus for my ongoing online graduate data structures class.</p>
  </li>
  <li>
    <p>Some reading on “git pull –rebase”: <a href="https://medium.com/@DGabeau/git-pull-rebase-vs-git-pull-c2b352fe53aa">Gabeau/medium</a>, <a href="https://coderwall.com/p/7aymfa/please-oh-please-use-git-pull-rebase">Hasiński/coderwall</a>, <a href="https://stackoverflow.com/questions/2472254/when-should-i-use-git-pull-rebase">Shved &amp; Mortensen/stackoverflow</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/104145964667479373"/>).</span> I have been using this in situations with multiple authors simultaneously working on a paper, to keep my contributions to the edit history linear (avoiding edit conflicts and merge bubbles in the history). It’s been working well for me but I realize it might be controversial…</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Klaus_Roth">Klaus Roth (1925–2015)</a>, Fields medalist who made important contributions to Diophantine approximation, arithmetic combinatorics, and discrepancy theory (<a href="https://mathstodon.xyz/@11011110/104151822927869863"/>). Now a Good Article on Wikipedia.</p>
  </li>
  <li>
    <p><a href="https://www.scottaaronson.com/blog/?p=4794">Scott Aaronson discusses four new preprints in quantum computing</a> (<a href="https://mathstodon.xyz/@11011110/104164952895881843"/>).</p>
  </li>
  <li>
    <p><a href="https://thmatters.wordpress.com/2020/05/14/call-for-nominations-for-talg-new-editor-in-chief/">Call for nominations for new editor-in-chief of <em>ACM Transactions on Algorithms</em></a> (<a href="https://mathstodon.xyz/@11011110/104168138945005653"/>). Nominations are due <span style="white-space: nowrap;">June 8.</span></p>
  </li>
  <li>
    <p>My campus computing support people have decided that the middle of a very work-intensive term is the time to make me choose between stopping making illustrations or giving up access to the scientific literature (<a href="https://mathstodon.xyz/@11011110/104174466523993151"/>). The connection is the VPN I use to access campus journal and database subscriptions. They want to upgrade to a version incompatible with OS X 10.12. Upgrading OS X would be incompatible with my purchased Adobe software and make me pay money I don’t have for a subscription instead.</p>
  </li>
</ul></div>
    </content>
    <updated>2020-05-15T16:42:00Z</updated>
    <published>2020-05-15T16:42:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-05-16T00:07:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=435</id>
    <link href="https://tcsplus.wordpress.com/2020/05/14/tcs-talk-wednesday-may-20-mark-bun-boston-university/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, May 20 — Mark Bun, Boston University</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, May 20th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Mark Bun from Boston University will speak about “An Equivalence between Private Classification and Online Predictability” (abstract below). You can reserve a spot as an individual or a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, May 20th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Mark Bun</strong> from Boston University will speak about “<em>An Equivalence between Private Classification and Online Predictability</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our website</a> on the day of the talk, so people who did not sign up will still be able to watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: We prove that every concept class with finite Littlestone dimension can be learned by an (approximate) differentially-private algorithm. The converse direction was shown in recent work of Alon, Livni, Malliaris, and Moran, STOC ’19. Together these two results show that a class of functions is privately learnable if and only if it is learnable in the mistake-bound model of online learning. To establish our result, we introduce “global stability,” a new notion of algorithmic stability for learning algorithms that we show can always be satisfied when learning classes of finite Littlestone dimension.</p></blockquote>
<p> </p></div>
    </content>
    <updated>2020-05-14T18:04:41Z</updated>
    <published>2020-05-14T18:04:41Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-05-17T22:39:11Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2020-05-14-streamlet/</id>
    <link href="https://decentralizedthoughts.github.io/2020-05-14-streamlet/" rel="alternate" type="text/html"/>
    <title>Streamlet: A Simple Textbook Blockchain Protocol</title>
    <summary>Guest post by Benjamin Chan and Elaine Shi In this post, we describe an extraordinarily simple blockchain protocol called Streamlet. Consensus is a complex problem and has been studied since the 1980s. More recently, blockchain research has spawned many new works aiming for performance and ease-of-implementation. However, simple, understandable protocols...</summary>
    <updated>2020-05-14T17:48:00Z</updated>
    <published>2020-05-14T17:48:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2020-05-17T22:48:07Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1688</id>
    <link href="https://theorydish.blog/2020/05/14/pride-and-prejudice-from-research-to-practice/" rel="alternate" type="text/html"/>
    <title>Pride and Prejudice: From Research to Practice</title>
    <summary>Despite (or because of) being a devoted theoretician, I truly enjoy every occasion when theory influences practice. I therefore felt quite a bit of satisfaction when our rather recent work, in the context of algorithmic fairness (with Úrsula Hébert-Johnson, Michael P. Kim and Guy N. Rothblum), found an application for predicting COVID-19 complications. Researchers in Israel, in collaboration with Israel’s biggest health-care provider, adapted a refined model for predicting flu complications to a model for predicting COVID-19 complications.  At the time, only very limited data from China were available (marginal statistics).  This is where our work came in (following several past empiric studies of the method):  the team applied our algorithm to improve the accuracy of predictions across various subpopulations (as part of an immense research and engineering effort). Now that there is (unfortunately) more data, it seems that the predictor exhibited surprisingly good performance (surprising, due to the poor training data).  See a manuscript here, an interview here and a more technical talk here (starting at minute 36 roughly). The predictor was applied with the appropriate cautiousness to inform and advise patients. But this is also an example of the gravity of decisions by researchers and software developers. Taking [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Despite (or because of) being a devoted theoretician, I truly enjoy every occasion when theory influences practice. I therefore felt quite a bit of satisfaction when our <a href="https://arxiv.org/abs/1711.08513">rather recent work</a>, in the context of algorithmic fairness (with Úrsula Hébert-Johnson, Michael P. Kim and Guy N. Rothblum), found an application for predicting COVID-19 complications. Researchers in Israel, in collaboration with Israel’s biggest health-care provider, adapted a refined model for predicting flu complications to a model for predicting COVID-19 complications.  At the time, only very limited data from China were available (marginal statistics).  This is where our work came in (following several past empiric studies of the method):  the team applied our algorithm to improve the accuracy of predictions across various subpopulations (as part of an immense research and engineering effort). Now that there is (unfortunately) more data, it seems that the predictor exhibited surprisingly good performance (surprising, due to the poor training data).  See a <a href="https://www.medrxiv.org/content/10.1101/2020.04.23.20076976v1">manuscript</a> here, an interview <a href="https://www.youtube.com/watch?v=r_alyuZULYI">here</a> and a more technical talk <a href="https://www.youtube.com/watch?v=weaRmSVA3yM">here</a> (starting at minute 36 roughly). The predictor was applied with the appropriate cautiousness to inform and advise patients.</p>
<p>But this is also an example of the gravity of decisions by researchers and software developers. Taking it to extreme, imagine a predictor that is used to determine which patients are denied treatment in an overwhelmed hospital. The booming research area of algorithmic fairness sees a very short turnover from research ideas (in many areas) to deployment. In an ideal world, it would have been much better to first have a couple of decades to develop the computational foundations of algorithmic fairness, before the practical need arose. But in the real world, the huge scale of algorithmic decision making creates immense demand for solutions. Industry, as well as policy and law makers are unlikely to wait decades or even years, nor is it clear that they should. From my perspective, this reality underscores the urgency for <em>principled</em> and <em>deliberate</em> research – rather than <em>hasty</em> research – continuously developing the foundations of algorithmic fairness and offering answers to real-world challenges.</p></div>
    </content>
    <updated>2020-05-14T17:32:17Z</updated>
    <published>2020-05-14T17:32:17Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2020-05-17T22:40:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://thmatters.wordpress.com/?p=1307</id>
    <link href="https://thmatters.wordpress.com/2020/05/14/cra-and-ccc-announce-computing-innovation-fellows-2020/" rel="alternate" type="text/html"/>
    <title>CRA and CCC announce Computing Innovation Fellows 2020</title>
    <summary>The Computing Research Association (CRA) and Computing Community Consortium (CCC) have announced a new CI Fellows program that will offer 2 year postdoctoral opportunities in computing starting Fall’20. The deadline for application is yet to be announced but will be around mid-June 2020, with decisions being made around mid-July 2020 for positions beginning this fall […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Computing Research Association (CRA) and Computing Community Consortium (CCC) have announced a new CI Fellows program that will offer 2 year postdoctoral opportunities in computing starting Fall’20.</p>
<p>The deadline for application is yet to be announced but will be around <strong>mid-June 2020</strong>, with decisions being made around mid-July 2020 for positions beginning this fall or winter. We will update this post once a deadline is announced. In the meantime, further details can be found at <a href="https://cifellows2020.org/" rel="nofollow">https://cifellows2020.org/</a>.</p></div>
    </content>
    <updated>2020-05-14T16:39:50Z</updated>
    <published>2020-05-14T16:39:50Z</published>
    <category term="Deadlines"/>
    <category term="for PhD students"/>
    <category term="Funding opportunity"/>
    <category term="postdocs"/>
    <author>
      <name>shuchic</name>
    </author>
    <source>
      <id>https://thmatters.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://thmatters.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://thmatters.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://thmatters.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://thmatters.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theory Matters</title>
      <updated>2020-05-17T22:35:14Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://thmatters.wordpress.com/?p=1304</id>
    <link href="https://thmatters.wordpress.com/2020/05/14/call-for-nominations-for-talg-new-editor-in-chief/" rel="alternate" type="text/html"/>
    <title>Call for Nominations for TALG new Editor-in-Chief</title>
    <summary>Prof. David Shmoys is chairing the committee to identify a new Editor-in-Chief for ACM Trans. on Algorithms.  The deadline for nominations is Jun 8th. Please see details below. ACM TALG (webpage: http://talg.acm.org/) publishes original research of the highest quality dealing with algorithms. It is a peer-reviewed journal, appearing quarterly. Specific areas of computation covered by the journal are […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Prof. <span class="il">David</span> <span class="il">Shmoys</span> is chairing the committee to identify a new Editor-in-Chief for ACM Trans. on Algorithms.  The deadline for nominations is <strong>Jun 8th</strong>. Please see details below.</p>
<p>ACM TALG (webpage: <a href="http://talg.acm.org/" rel="nofollow">http://talg.acm.org/</a>) publishes original research of the highest quality dealing with algorithms. It is a peer-reviewed journal, appearing quarterly. Specific areas of computation covered by the journal are listed at <a href="http://talg.acm.org/Aims.html" rel="nofollow">http://talg.acm.org/Aims.html</a>.</p>
<p>We are looking for a well-established person with a strong record of research achievements and service, and with a vision for the future of the field. The term of appointment is three years, to begin late summer 2020, with the possibility of renewal for a second term. The editor-in-chief is responsible for faithfully executing the editorial charter of the journal yet should be proactive in adapting the journal and its charter to changes in the field. A description of the duties of the EiC and evaluation criteria can be found at <a href="http://www.acm.org/publications/policies/evaluation" rel="nofollow">http://www.acm.org/publications/policies/evaluation</a>.</p>
<p>The Search Committee members are:<br/>
David Eppstein – University of California, Irvine<br/>
Anna Karlin – University of Washington<br/>
Chris Hankin – Imperial College London – ACM Publications Board Liaison<br/>
Dana Randall – Georgia Institute of Technology<br/>
David Shmoys – Cornell University – Committee Chair.</p>
<p>All nominees, including self-nominees, should send a CV and a Vision Statement for TALG (at least one page), with subject header “EiC nomination” to the committee chair – david.shmoys@cornell.edu .</p>
<p>The deadline for nominations is Monday, June 8, 2020 at 11:59 p.m. (EST).</p></div>
    </content>
    <updated>2020-05-14T16:34:59Z</updated>
    <published>2020-05-14T16:34:59Z</published>
    <category term="Deadlines"/>
    <author>
      <name>shuchic</name>
    </author>
    <source>
      <id>https://thmatters.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://thmatters.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://thmatters.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://thmatters.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://thmatters.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theory Matters</title>
      <updated>2020-05-17T22:35:13Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4389</id>
    <link href="https://lucatrevisan.wordpress.com/2020/05/14/spectral-sparsification-of-hypergraphs/" rel="alternate" type="text/html"/>
    <title>Spectral Sparsification of Hypergraphs</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">In this post we will construct a “spectral sparsifier” of a given hypergraph in a way that is similar to how Spielman and Srivastava construct spectral graph sparsifiers. We will assign a probability to each hyperedge, we will sample each … <a href="https://lucatrevisan.wordpress.com/2020/05/14/spectral-sparsification-of-hypergraphs/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
 In this post we will construct a “spectral sparsifier” of a given hypergraph in a way that is similar to how Spielman and Srivastava construct spectral graph sparsifiers. We will assign a probability <img alt="{p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e}"/> to each hyperedge, we will sample each hyperedge <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> with probability <img alt="{p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e}"/>, and we will weigh it by <img alt="{1/p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2Fp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/p_e}"/> if selected. We will then bound the “spectral error” of this construction in terms of the supremum of a Gaussian process using Talagrand’s comparison inequality and finally bound the supremum of the Gaussian process (which will involve matrices) using matrix Chernoff bounds. This is <a href="https://arxiv.org/abs/1905.01495">joint work with Nikhil Bansal and Ola Svensson</a>.</p>
<p>
<span id="more-4389"/></p>
<p>
</p><p><b>1. Hypergraph Sparsifiers </b></p>
<p/><p>
An (undirected) hypergraph <img alt="{H= (V,E)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%3D+%28V%2CE%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H= (V,E)}"/> is a collection <img alt="{E}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E}"/> of subsets <img alt="{e \subseteq V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be+%5Csubseteq+V%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e \subseteq V}"/>, called hyperedges. A graph is the special case in which each set <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> has cardinality 2. For simplicity we will talk only about <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/>-uniform hypergraphs, that is hypergraphs in which all hyperedges have the same cardinality <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> (the arguments below would work also in the non-uniform case in which all hyperedges have cardinality <em>at most</em> <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/>). Hyperedges may have weights.</p>
<p>
If <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> is a subset of vertices, a hyperedge <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> is <em>cut</em> by <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> if <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> has non-empty intersection with both <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> and <img alt="{V-S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV-S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V-S}"/>. We call <img alt="{cut_H(S)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bcut_H%28S%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{cut_H(S)}"/> the number of hyperedges of <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> cut by <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>, or the total weight of such hyperedges if <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> is a weighted hypergraph.</p>
<p>
We can then generalize the notion of Benczur-Karger sparsification, and say that <img alt="{H'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H'}"/> is a cut sparsifier of <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> with error <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/> if <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> and <img alt="{H'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H'}"/> have the same vertex set <img alt="{V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V}"/> and</p>
<p/><p align="center"><img alt="\displaystyle  \forall S\subseteq V: \ \ \ 1-\epsilon \leq \frac{cut_{H'}(S)}{cut_{H}(S)} \leq 1 + \epsilon " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+S%5Csubseteq+V%3A+%5C+%5C+%5C+1-%5Cepsilon+%5Cleq+%5Cfrac%7Bcut_%7BH%27%7D%28S%29%7D%7Bcut_%7BH%7D%28S%29%7D+%5Cleq+1+%2B+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall S\subseteq V: \ \ \ 1-\epsilon \leq \frac{cut_{H'}(S)}{cut_{H}(S)} \leq 1 + \epsilon "/></p>
<p>
Kogan and Krauthgamer show that every <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/>-uniform hypergraph <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> admits a cut sparsifier of error <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/> with only <img alt="{O(\epsilon^{-2} n (r + \log n))}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Cepsilon%5E%7B-2%7D+n+%28r+%2B+%5Clog+n%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\epsilon^{-2} n (r + \log n))}"/> weighted hyperedges. They are able to extend the argument of Benczur and Karger to hypergraphs, including arguing that there are few sparse cuts. They assign a probability <img alt="{p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e}"/> to each hyperedge, sample it with that probability, weighing it <img alt="{1/p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2Fp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/p_e}"/> if selected, and then use a union bound and Chernoff bounds to argue that all cuts are preserved.</p>
<p>
Anand Louis has introduced a <a href="https://dl.acm.org/doi/abs/10.1145/2746539.2746555">notion of hypergraph Laplacian</a> in the following way. The Laplacian quadratic form of a hypergraph <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> is a function <img alt="{Q_H : {\mathbb R}^V \rightarrow {\mathbb R}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_H+%3A+%7B%5Cmathbb+R%7D%5EV+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_H : {\mathbb R}^V \rightarrow {\mathbb R}}"/> defined as </p>
<p align="center"><img alt="\displaystyle  Q_H(x) = \sum_{e\in E} w(e) \ \max_{a,b\in e} \ (x_a - x_b)^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q_H%28x%29+%3D+%5Csum_%7Be%5Cin+E%7D+w%28e%29+%5C+%5Cmax_%7Ba%2Cb%5Cin+e%7D+%5C+%28x_a+-+x_b%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  Q_H(x) = \sum_{e\in E} w(e) \ \max_{a,b\in e} \ (x_a - x_b)^2 "/></p>
<p> where <img alt="{w(e)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%28e%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w(e)}"/> is the weight of the hyperedge <img alt="{E}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E}"/>, or 1 if the hypergraph is unweighted.</p>
<p>
This definition has the motivation that it recovers the Laplacian quadratic form <img alt="{Q_G(x) = x^T L_Gx}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_G%28x%29+%3D+x%5ET+L_Gx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_G(x) = x^T L_Gx}"/> in the case of graphs, and that if <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> is the indicator of a set <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> then <img alt="{Q_H(x) = cut_H(S)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_H%28x%29+%3D+cut_H%28S%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_H(x) = cut_H(S)}"/>. Furthermore, one can define “eigenvalues” and “eigenvectors” of this hypergraph Laplacian and recover a Cheeger inequality and even <a href="https://dl.acm.org/doi/abs/10.1145/3178123">higher-order Cheeger inequalities</a>.</p>
<p>
So it seems interesting to consider the following notion of sparsification: a hypergraph <img alt="{H'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H'}"/> is a spectral sparsifier of <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> with error <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/> if <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> and <img alt="{H'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H'}"/> have the same vertex set and</p>
<p/><p align="center"><img alt="\displaystyle  \forall x\in {\mathbb R}^V \ \ \ 1-\epsilon \leq \frac{Q_{H'}(x)}{Q_{H}(x)} \leq 1 + \epsilon " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+x%5Cin+%7B%5Cmathbb+R%7D%5EV+%5C+%5C+%5C+1-%5Cepsilon+%5Cleq+%5Cfrac%7BQ_%7BH%27%7D%28x%29%7D%7BQ_%7BH%7D%28x%29%7D+%5Cleq+1+%2B+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall x\in {\mathbb R}^V \ \ \ 1-\epsilon \leq \frac{Q_{H'}(x)}{Q_{H}(x)} \leq 1 + \epsilon "/></p>
<p> where, as before, the convention is that <img alt="{0/0 = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%2F0+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0/0 = 1}"/>.</p>
<p>
Soma and Yoshida <a href="https://dl.acm.org/doi/10.5555/3310435.3310594">studied this question</a> and gave a construction with <img alt="{O(\epsilon^{-2} n^3\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Cepsilon%5E%7B-2%7D+n%5E3%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\epsilon^{-2} n^3\log n)}"/> hyperedges. In the rest of this post we will discuss the construction by Nikhil Bansal, Ola Svensson and me, which uses <img alt="{O(\epsilon^{-2} r^3 n \log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Cepsilon%5E%7B-2%7D+r%5E3+n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\epsilon^{-2} r^3 n \log n)}"/> hyperedges.</p>
<p>
</p><p><b>2. Choosing Probabilities </b></p>
<p/><p>
Given a hypergraph <img alt="{H=(V,E)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%3D%28V%2CE%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H=(V,E)}"/>, we can construct a multigraph <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> by taking each hyperedge <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> of <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> and then constructing, in <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>, a clique between the vertices of <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/>. Thus, in <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>, the edge <img alt="{(a,b)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28a%2Cb%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(a,b)}"/> is repeated as many times (possibly, zero times) as the number of hyperedges <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> of <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> that contain both <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a}"/> and <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/>. Another way to think about it is that the Laplacian of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> is given by</p>
<p/><p align="center"><img alt="\displaystyle  L_G = \sum_{e\in E} \sum_{a,b\in e} L_{a,b} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++L_G+%3D+%5Csum_%7Be%5Cin+E%7D+%5Csum_%7Ba%2Cb%5Cin+e%7D+L_%7Ba%2Cb%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  L_G = \sum_{e\in E} \sum_{a,b\in e} L_{a,b} "/></p>
<p>
where the inner sum is over the <img alt="{{r \choose 2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Br+%5Cchoose+2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{r \choose 2}}"/> unordered pairs <img alt="{\{a,b\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7Ba%2Cb%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{a,b\}}"/>. This graph relates in several interesting ways with <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/>. For example, if <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> is a subset of vertices and <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> is a hyperedge that is cut by <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>, then between <img alt="{r-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r-1}"/> and <img alt="{{r \choose 2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Br+%5Cchoose+2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{r \choose 2}}"/> of the edges of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> derived from <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> are cut by <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>. If <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> is not cut by <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>, then none of the edges derived from <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> is cut by <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>, so we have</p>
<p/><p align="center"><img alt="\displaystyle  \forall S\subseteq V: \ \ \ (r-1) \ cut_H(S) \leq cut_G(S) \leq {r \choose 2} cut_H(S) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+S%5Csubseteq+V%3A+%5C+%5C+%5C+%28r-1%29+%5C+cut_H%28S%29+%5Cleq+cut_G%28S%29+%5Cleq+%7Br+%5Cchoose+2%7D+cut_H%28S%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall S\subseteq V: \ \ \ (r-1) \ cut_H(S) \leq cut_G(S) \leq {r \choose 2} cut_H(S) "/></p>
<p>
and, with a bit more work, it is possible to prove similar bounds for the Laplacian quadratic forms</p>
<p/><p align="center"><img alt="\displaystyle  \forall x\in {\mathbb R}^ V: \ \ \ \frac r2 \ Q_H(x) \leq x^T L_G x \leq {r \choose 2} Q_H(x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+x%5Cin+%7B%5Cmathbb+R%7D%5E+V%3A+%5C+%5C+%5C+%5Cfrac+r2+%5C+Q_H%28x%29+%5Cleq+x%5ET+L_G+x+%5Cleq+%7Br+%5Cchoose+2%7D+Q_H%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall x\in {\mathbb R}^ V: \ \ \ \frac r2 \ Q_H(x) \leq x^T L_G x \leq {r \choose 2} Q_H(x) "/></p>
<p> This means that if we sparsify <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>, say with the Spielman-Srivastava construction, we obtain information about <img alt="{Q_H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_H}"/>, up to multiplicative error <img alt="{O(r)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28r%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(r)}"/>. Now suppose that, as we sample edges of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> to sparsify it in the Spielman-Srivastava way, we will also pick hyperedges of <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> (for example we pick <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> if at least one of its corresponding edges in <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> is picked), and we weigh them by the inverse of the probability of being selected. Then we may hope that if the Spielman-Srivastava sparsification of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> is tuned to achieve error <img alt="{\epsilon/r^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%2Fr%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon/r^2}"/>, the hypergraph that we obtain will have error at most <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>. Indeed, this is roughly what happens, and we will be able to prove it by showing that the error in the hypergraph sparsification is dominated by the error in the “Gaussian version” of Spielman-Srivastava described in the previous post.</p>
<p>
So we are going to assign to each hyperedge <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> a probability </p>
<p align="center"><img alt="\displaystyle  p_e = \min \left\{ 1 , B \cdot \sum_{a,b \in e} || L^{-1/2}_G L_{a,b} L^{-1/2} || \right\} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++p_e+%3D+%5Cmin+%5Cleft%5C%7B+1+%2C+B+%5Ccdot+%5Csum_%7Ba%2Cb+%5Cin+e%7D+%7C%7C+L%5E%7B-1%2F2%7D_G+L_%7Ba%2Cb%7D+L%5E%7B-1%2F2%7D+%7C%7C+%5Cright%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  p_e = \min \left\{ 1 , B \cdot \sum_{a,b \in e} || L^{-1/2}_G L_{a,b} L^{-1/2} || \right\} "/></p>
<p> where the factor <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> will be chosen later to get the construction to work and <img alt="{R_{a,b} := || L^{-1/2}_G L_{a,b} L^{-1/2} ||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR_%7Ba%2Cb%7D+%3A%3D+%7C%7C+L%5E%7B-1%2F2%7D_G+L_%7Ba%2Cb%7D+L%5E%7B-1%2F2%7D+%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R_{a,b} := || L^{-1/2}_G L_{a,b} L^{-1/2} ||}"/> is the effective resistance of the edge <img alt="{(a,b)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28a%2Cb%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(a,b)}"/> of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>.</p>
<p>
 In fact, as before, it will be helpful to have probabilities that are non-positive powers of two, so we will choose <img alt="{p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e}"/> to be a power of two such that </p>
<p align="center"><img alt="\displaystyle  \min \left\{ 1 , B \cdot \sum_{a,b \in e} R_{a,b} \right\} \leq p_e \leq \min \left\{ 1 , 2B \cdot \sum_{a,b \in e} R_{a,b} \right\} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin+%5Cleft%5C%7B+1+%2C+B+%5Ccdot+%5Csum_%7Ba%2Cb+%5Cin+e%7D+R_%7Ba%2Cb%7D+%5Cright%5C%7D+%5Cleq+p_e+%5Cleq+%5Cmin+%5Cleft%5C%7B+1+%2C+2B+%5Ccdot+%5Csum_%7Ba%2Cb+%5Cin+e%7D+R_%7Ba%2Cb%7D+%5Cright%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \min \left\{ 1 , B \cdot \sum_{a,b \in e} R_{a,b} \right\} \leq p_e \leq \min \left\{ 1 , 2B \cdot \sum_{a,b \in e} R_{a,b} \right\} "/></p>
<p>
We have </p>
<p align="center"><img alt="\displaystyle  \sum_e p_e \leq 2 B \sum_{e\in E} \sum_{a,b\in e} R_{a,b} \leq 2B \cdot (n-1) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_e+p_e+%5Cleq+2+B+%5Csum_%7Be%5Cin+E%7D+%5Csum_%7Ba%2Cb%5Cin+e%7D+R_%7Ba%2Cb%7D+%5Cleq+2B+%5Ccdot+%28n-1%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_e p_e \leq 2 B \sum_{e\in E} \sum_{a,b\in e} R_{a,b} \leq 2B \cdot (n-1) "/></p>
<p>
Another fact that will become useful later (it will save us a factor of the order of <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> in the number of hyperedges in the construction) is that</p>
<p/><p align="center"><img alt="\displaystyle  \max_{a,b \in e} R_{a,b} \leq \frac 2r \sum_{a,b \in e} R_{a,b} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmax_%7Ba%2Cb+%5Cin+e%7D+R_%7Ba%2Cb%7D+%5Cleq+%5Cfrac+2r+%5Csum_%7Ba%2Cb+%5Cin+e%7D+R_%7Ba%2Cb%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \max_{a,b \in e} R_{a,b} \leq \frac 2r \sum_{a,b \in e} R_{a,b} "/></p>
<p>
</p><p><b>3. A Discrete Random Process </b></p>
<p/><p>
Our construction of a hypergraph sparsifier <img alt="{H'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H'}"/> will be to select each <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> independently with probability <img alt="{p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e}"/> and, if selected, weigh it by <img alt="{1/p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2Fp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/p_e}"/>. Our goal is to find an upper bound in probability, or even in expectation, on </p>
<p align="center"><img alt="\displaystyle  \sup_{x \in {\mathbb R}^n} \ \ \left | 1 - \frac{Q_{H'}(x)}{Q_H(x)} \right | " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx+%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+%5C+%5Cleft+%7C+1+-+%5Cfrac%7BQ_%7BH%27%7D%28x%29%7D%7BQ_H%28x%29%7D+%5Cright+%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sup_{x \in {\mathbb R}^n} \ \ \left | 1 - \frac{Q_{H'}(x)}{Q_H(x)} \right | "/></p>
<p> Recalling that <img alt="{x^T L_G x \leq \frac{r^2}2 Q_H(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5ET+L_G+x+%5Cleq+%5Cfrac%7Br%5E2%7D2+Q_H%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^T L_G x \leq \frac{r^2}2 Q_H(x)}"/>, we will study</p>
<p/><p align="center"><img alt="\displaystyle  \sup_{x : x^T L_G x = 1} | Q_H(x) - Q_{H'} (x) | " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx+%3A+x%5ET+L_G+x+%3D+1%7D+%7C+Q_H%28x%29+-+Q_%7BH%27%7D+%28x%29+%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sup_{x : x^T L_G x = 1} | Q_H(x) - Q_{H'} (x) | "/></p>
<p> Because if we can show that the above quantity is at most <img alt="{2\epsilon /r^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Cepsilon+%2Fr%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2\epsilon /r^2}"/>, then, for every <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>, </p>
<p align="center"><img alt="\displaystyle  | Q_H(x) - Q_{H'} (x) | \leq \frac{2\epsilon}{r^2} \ x^T L_G x \leq \epsilon Q_H(x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C+Q_H%28x%29+-+Q_%7BH%27%7D+%28x%29+%7C+%5Cleq+%5Cfrac%7B2%5Cepsilon%7D%7Br%5E2%7D+%5C+x%5ET+L_G+x+%5Cleq+%5Cepsilon+Q_H%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  | Q_H(x) - Q_{H'} (x) | \leq \frac{2\epsilon}{r^2} \ x^T L_G x \leq \epsilon Q_H(x) "/></p>
<p> as desired.</p>
<p>
If we define </p>
<p align="center"><img alt="\displaystyle  Q_e(x) = \max_{a,b \in E} x^T L_{a,b} x " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q_e%28x%29+%3D+%5Cmax_%7Ba%2Cb+%5Cin+E%7D+x%5ET+L_%7Ba%2Cb%7D+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  Q_e(x) = \max_{a,b \in E} x^T L_{a,b} x "/></p>
<p> then we are interested in the supremum in <img alt="{T = \{ x : x^T L_G x = 1 \}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT+%3D+%5C%7B+x+%3A+x%5ET+L_G+x+%3D+1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T = \{ x : x^T L_G x = 1 \}}"/> of </p>
<p align="center"><img alt="\displaystyle  F(x) = Q_H(x) - Q_{H'} (x) = \sum_e z_e Q_e (x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28x%29+%3D+Q_H%28x%29+-+Q_%7BH%27%7D+%28x%29+%3D+%5Csum_e+z_e+Q_e+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F(x) = Q_H(x) - Q_{H'} (x) = \sum_e z_e Q_e (x) "/></p>
<p> where <img alt="{z_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z_e}"/> is a random variable that is equal to <img alt="{1/p_e-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2Fp_e-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/p_e-1}"/> with probability <img alt="{p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e}"/> and is equal to <img alt="{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-1}"/> with probability <img alt="{1-p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1-p_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1-p_e}"/>.</p>
<p>
As before, we will do the construction in rounds. If the smallest probability <img alt="{p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e}"/> is <img alt="{2^{-\ell}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B-%5Cell%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{-\ell}}"/>, then we will have <img alt="{\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell}"/> rounds. We start with <img alt="{H^{(0)} = H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%5E%7B%280%29%7D+%3D+H%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H^{(0)} = H}"/> and then, at round <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>, we take all hyperedges <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> such that <img alt="{p_e \leq 2^{k-1-\ell}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e \leq 2^{k-1-\ell}}"/> and, independently for each such hyperedge, we either delete it or we double its weight (with probability 1/2 in each case). The final hypergraph <img alt="{H^{(\ell)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%5E%7B%28%5Cell%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H^{(\ell)}}"/> is distributed like <img alt="{H'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H'}"/>. We have the processes</p>
<p/><p align="center"><img alt="\displaystyle  F^{(k)}(x) = Q_{H^{(k-1)}}(x) - Q_{H^{(k)}} (x) = \sum_{e\in E_k} r^{(k)}_e w^{(k-1)}_e Q_e (x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%5E%7B%28k%29%7D%28x%29+%3D+Q_%7BH%5E%7B%28k-1%29%7D%7D%28x%29+-+Q_%7BH%5E%7B%28k%29%7D%7D+%28x%29+%3D+%5Csum_%7Be%5Cin+E_k%7D+r%5E%7B%28k%29%7D_e+w%5E%7B%28k-1%29%7D_e+Q_e+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F^{(k)}(x) = Q_{H^{(k-1)}}(x) - Q_{H^{(k)}} (x) = \sum_{e\in E_k} r^{(k)}_e w^{(k-1)}_e Q_e (x) "/></p>
<p> where the random variables <img alt="{r^{(k)}_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%5E%7B%28k%29%7D_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r^{(k)}_e}"/> are Rademacher, the weights <img alt="{w^{(k-1)}_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%5E%7B%28k-1%29%7D_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w^{(k-1)}_e}"/> are either 0 or <img alt="{2^{k-1-\ell} / p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bk-1-%5Cell%7D+%2F+p_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{k-1-\ell} / p_e}"/>, and <img alt="{E_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_k}"/> is the set of edges that are “active” in round <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>, that is, the set of hyperedges <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> such that <img alt="{p_e \leq 2^{k-1-\ell}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e \leq 2^{k-1-\ell}}"/>.</p>
<p>
For each hypergraph <img alt="{H^{(k)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%5E%7B%28k%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H^{(k)}}"/>, we will also consider its associated graph <img alt="{G^{(k)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%5E%7B%28k%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G^{(k)}}"/>, obtained by replacing each hyperedge with a clique. The Laplacian of <img alt="{G^{(k)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%5E%7B%28k%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G^{(k)}}"/> is </p>
<p align="center"><img alt="\displaystyle  L_{G^{(k)}} = \sum_e w^{(k)}_e \sum_{a,b\in e} L_{a,b} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++L_%7BG%5E%7B%28k%29%7D%7D+%3D+%5Csum_e+w%5E%7B%28k%29%7D_e+%5Csum_%7Ba%2Cb%5Cin+e%7D+L_%7Ba%2Cb%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  L_{G^{(k)}} = \sum_e w^{(k)}_e \sum_{a,b\in e} L_{a,b} "/></p>
<p>
We have the following lemma</p>
<blockquote><p><b>Lemma 1</b> <em> For every outcome of the first <img alt="{k-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k-1}"/> rounds such that <img alt="{L_{G^{(k-1)}} \preceq 2 L_G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7BG%5E%7B%28k-1%29%7D%7D+%5Cpreceq+2+L_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{G^{(k-1)}} \preceq 2 L_G}"/>, there is a probability at least <img alt="{1-1/n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1-1%2Fn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1-1/n}"/> over the randomness of the <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>-th round that </em></p><em>
<p align="center"><img alt="\displaystyle  \sup_{x: x^T L_G x = 1} | F^{(k)} (x) | \leq O \left( \sqrt{2^{k-\ell} \frac { \log n}{Br} } \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%3A+x%5ET+L_G+x+%3D+1%7D+%7C+F%5E%7B%28k%29%7D+%28x%29+%7C+%5Cleq+O+%5Cleft%28+%5Csqrt%7B2%5E%7Bk-%5Cell%7D+%5Cfrac+%7B+%5Clog+n%7D%7BBr%7D+%7D+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sup_{x: x^T L_G x = 1} | F^{(k)} (x) | \leq O \left( \sqrt{2^{k-\ell} \frac { \log n}{Br} } \right) "/></p>
<p> and that </p>
<p align="center"><img alt="\displaystyle  L_{G^{(k)}} - L_{G^{(k-1)} } \preceq O \left( \sqrt{2^{k-\ell} \frac { \log n}{B} } \ \right) \cdot L_G " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++L_%7BG%5E%7B%28k%29%7D%7D+-+L_%7BG%5E%7B%28k-1%29%7D+%7D+%5Cpreceq+O+%5Cleft%28+%5Csqrt%7B2%5E%7Bk-%5Cell%7D+%5Cfrac+%7B+%5Clog+n%7D%7BB%7D+%7D+%5C+%5Cright%29+%5Ccdot+L_G+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  L_{G^{(k)}} - L_{G^{(k-1)} } \preceq O \left( \sqrt{2^{k-\ell} \frac { \log n}{B} } \ \right) \cdot L_G "/></p>
</em><p><em> </em></p></blockquote>
<p> This means that we can take <img alt="{B = O(\epsilon^{-2} r^3\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB+%3D+O%28%5Cepsilon%5E%7B-2%7D+r%5E3%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B = O(\epsilon^{-2} r^3\log n)}"/> and apply the above lemma inductively to argue that we have a high probability that <img alt="{\sup_{x\in T} |F(x) | \leq 2\epsilon /r^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csup_%7Bx%5Cin+T%7D+%7CF%28x%29+%7C+%5Cleq+2%5Cepsilon+%2Fr%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sup_{x\in T} |F(x) | \leq 2\epsilon /r^2}"/>, and so we get a hypergraph spectral sparsifier with error <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/> and <img alt="{O(\epsilon^{-2} r^3 n \log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Cepsilon%5E%7B-2%7D+r%5E3+n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\epsilon^{-2} r^3 n \log n)}"/> hyperedges. </p>
<p>
To prove the Lemma we will define the Gaussian process</p>
<p/><p align="center"><img alt="\displaystyle  \hat F^{(k)} (x) = \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b\in e} g^{(k)}_{e,a,b} \ x^T L_{a,b} x " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Chat+F%5E%7B%28k%29%7D+%28x%29+%3D+%5Csum_%7Be%5Cin+E_k%7D+w%5E%7B%28k-1%29%7D_e+%5Csum_%7Ba%2Cb%5Cin+e%7D+g%5E%7B%28k%29%7D_%7Be%2Ca%2Cb%7D+%5C+x%5ET+L_%7Ba%2Cb%7D+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \hat F^{(k)} (x) = \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b\in e} g^{(k)}_{e,a,b} \ x^T L_{a,b} x "/></p>
<p> where the <img alt="{g^{(k)}_{e,a,b}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%5E%7B%28k%29%7D_%7Be%2Ca%2Cb%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g^{(k)}_{e,a,b}}"/> are Gaussian. Notice the two differences between <img alt="{F^{(k)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%5E%7B%28k%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F^{(k)}}"/> and <img alt="{\hat F^{(k)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Chat+F%5E%7B%28k%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\hat F^{(k)}}"/>: we replaced the Rademacher choices with Gaussian choices, and we replaced </p>
<p align="center"><img alt="\displaystyle  Q_e(x) = \max _{a,b\in e} \ (x_a - x_b)^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q_e%28x%29+%3D+%5Cmax+_%7Ba%2Cb%5Cin+e%7D+%5C+%28x_a+-+x_b%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  Q_e(x) = \max _{a,b\in e} \ (x_a - x_b)^2 "/></p>
<p> with </p>
<p align="center"><img alt="\displaystyle  \sum_{a,b\in e} x^T L_{a,b} x = \sum_{a,b \in e} (x_a - x_b)^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Ba%2Cb%5Cin+e%7D+x%5ET+L_%7Ba%2Cb%7D+x+%3D+%5Csum_%7Ba%2Cb+%5Cin+e%7D+%28x_a+-+x_b%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{a,b\in e} x^T L_{a,b} x = \sum_{a,b \in e} (x_a - x_b)^2 "/></p>
<p> Furthermore, we are doing a random choice for each pair within each hyperedge instead of just one random choice per hyperedge.</p>
<blockquote><p><b>Fact 2</b> <em> The random processes <img alt="{F^{(k)} (x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%5E%7B%28k%29%7D+%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F^{(k)} (x)}"/> and <img alt="{-F^{(k)} (x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-F%5E%7B%28k%29%7D+%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-F^{(k)} (x)}"/> are <img alt="{O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(1)}"/>-dominated by <img alt="{\hat F(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Chat+F%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\hat F(x)}"/>. </em></p></blockquote>
<p/><p>
<em>Proof:</em>  For every <img alt="{x,y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x,y}"/> we have </p>
<p align="center"><img alt="\displaystyle  || F^{(k)} (x) - F^{(k)}(y) ||^2_{\Psi_2} = O(1) \cdot \sum_{e\in E_k} ( w^{(k-1)}_e Q_e (x) )^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+F%5E%7B%28k%29%7D+%28x%29+-+F%5E%7B%28k%29%7D%28y%29+%7C%7C%5E2_%7B%5CPsi_2%7D+%3D+O%281%29+%5Ccdot+%5Csum_%7Be%5Cin+E_k%7D+%28+w%5E%7B%28k-1%29%7D_e+Q_e+%28x%29+%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  || F^{(k)} (x) - F^{(k)}(y) ||^2_{\Psi_2} = O(1) \cdot \sum_{e\in E_k} ( w^{(k-1)}_e Q_e (x) )^2 "/></p>
<p> and </p>
<p align="center"><img alt="\displaystyle  (d(x,y))^2 = \mathop{\mathbb E} (\hat F^{(k)} (x) - \hat F^{(k)} (y))^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28d%28x%2Cy%29%29%5E2+%3D+%5Cmathop%7B%5Cmathbb+E%7D+%28%5Chat+F%5E%7B%28k%29%7D+%28x%29+-+%5Chat+F%5E%7B%28k%29%7D+%28y%29%29%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  (d(x,y))^2 = \mathop{\mathbb E} (\hat F^{(k)} (x) - \hat F^{(k)} (y))^2"/></p>
<p align="center"><img alt="\displaystyle  = \sum_{e\in E_k} (w^{(k-1)}_e)^2 \sum_{a,b\in e} ((x_a-x_b)^2 - (y_a-y_b)^2)^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_%7Be%5Cin+E_k%7D+%28w%5E%7B%28k-1%29%7D_e%29%5E2+%5Csum_%7Ba%2Cb%5Cin+e%7D+%28%28x_a-x_b%29%5E2+-+%28y_a-y_b%29%5E2%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \sum_{e\in E_k} (w^{(k-1)}_e)^2 \sum_{a,b\in e} ((x_a-x_b)^2 - (y_a-y_b)^2)^2 "/></p>
<p> To complete the proof, we argue that </p>
<p align="center"><img alt="\displaystyle  (Q_e (x) - Q_e (y))^2 \leq \sum_{a,b\in e} ((x_a-x_b)^2 - (y_a-y_b)^2)^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28Q_e+%28x%29+-+Q_e+%28y%29%29%5E2+%5Cleq+%5Csum_%7Ba%2Cb%5Cin+e%7D+%28%28x_a-x_b%29%5E2+-+%28y_a-y_b%29%5E2%29%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  (Q_e (x) - Q_e (y))^2 \leq \sum_{a,b\in e} ((x_a-x_b)^2 - (y_a-y_b)^2)^2"/></p>
<p> To verify the above inequality, assume <img alt="{Q_e(x) \geq Q_e(y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_e%28x%29+%5Cgeq+Q_e%28y%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_e(x) \geq Q_e(y)}"/>, otherwise the argument will be symmetric, and call <img alt="{a',b'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%27%2Cb%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a',b'}"/> the maximizer for <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> and <img alt="{a'',b''}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%27%27%2Cb%27%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a'',b''}"/> the maximizer for <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/>. Then </p>
<p align="center"><img alt="\displaystyle  (Q_e (x) - Q_e (y))^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28Q_e+%28x%29+-+Q_e+%28y%29%29%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  (Q_e (x) - Q_e (y))^2"/></p>
<p align="center"><img alt="\displaystyle  = ((x_{a'} - x_{b'})^2 - (y_{a''} - y_{b''})^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%28%28x_%7Ba%27%7D+-+x_%7Bb%27%7D%29%5E2+-+%28y_%7Ba%27%27%7D+-+y_%7Bb%27%27%7D%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = ((x_{a'} - x_{b'})^2 - (y_{a''} - y_{b''})^2 "/></p>
<p align="center"><img alt="\displaystyle  \leq ((x_{a'} - x_{b'})^2 - (y_{a'} - y_{b'})^2)^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+%28%28x_%7Ba%27%7D+-+x_%7Bb%27%7D%29%5E2+-+%28y_%7Ba%27%7D+-+y_%7Bb%27%7D%29%5E2%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \leq ((x_{a'} - x_{b'})^2 - (y_{a'} - y_{b'})^2)^2 "/></p>
<p align="center"><img alt="\displaystyle  \leq \sum_{a,b\in e} ((x_a-x_b)^2 - (y_a-y_b)^2)^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+%5Csum_%7Ba%2Cb%5Cin+e%7D+%28%28x_a-x_b%29%5E2+-+%28y_a-y_b%29%5E2%29%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \leq \sum_{a,b\in e} ((x_a-x_b)^2 - (y_a-y_b)^2)^2"/></p>
<p> <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
It remains to estimate the expected sup </p>
<p/><p align="center"><img alt="\displaystyle  \mathop{\mathbb E} \ \sup_{x\in T} \ \hat F^{(k)} (x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5C+%5Csup_%7Bx%5Cin+T%7D+%5C+%5Chat+F%5E%7B%28k%29%7D+%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E} \ \sup_{x\in T} \ \hat F^{(k)} (x) "/></p>
<p> and the diameter </p>
<p align="center"><img alt="\displaystyle  \sup_{x,y \in T} \sqrt{ \mathop{\mathbb E} ( \hat F^{(k)} (x) - \hat F^{(k)} (y))^2 } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%2Cy+%5Cin+T%7D+%5Csqrt%7B+%5Cmathop%7B%5Cmathbb+E%7D+%28+%5Chat+F%5E%7B%28k%29%7D+%28x%29+-+%5Chat+F%5E%7B%28k%29%7D+%28y%29%29%5E2+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sup_{x,y \in T} \sqrt{ \mathop{\mathbb E} ( \hat F^{(k)} (x) - \hat F^{(k)} (y))^2 } "/></p>
<p> where, recall, <img alt="{T = \{ x : x^T L_G x = 1\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT+%3D+%5C%7B+x+%3A+x%5ET+L_G+x+%3D+1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T = \{ x : x^T L_G x = 1\}}"/>.</p>
<p>
With the usual change of variable we have </p>
<p align="center"><img alt="\displaystyle  \sup_{x \in T} \ \hat F^{(k)} (x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx+%5Cin+T%7D+%5C+%5Chat+F%5E%7B%28k%29%7D+%28x%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sup_{x \in T} \ \hat F^{(k)} (x)"/></p>
<p align="center"><img alt="\displaystyle  = \sup_{y: ||y||= 1} \ \hat F^{(k)} (L_G^{-1/2} y) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csup_%7By%3A+%7C%7Cy%7C%7C%3D+1%7D+%5C+%5Chat+F%5E%7B%28k%29%7D+%28L_G%5E%7B-1%2F2%7D+y%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \sup_{y: ||y||= 1} \ \hat F^{(k)} (L_G^{-1/2} y) "/></p>
<p align="center"><img alt="\displaystyle  = \left\| \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b\in e} g^{(k)}_{e,a,b} \ L_{G}^{-1/2} L_{a,b} L_G^{-1/2} \right \| " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Cleft%5C%7C+%5Csum_%7Be%5Cin+E_k%7D+w%5E%7B%28k-1%29%7D_e+%5Csum_%7Ba%2Cb%5Cin+e%7D+g%5E%7B%28k%29%7D_%7Be%2Ca%2Cb%7D+%5C+L_%7BG%7D%5E%7B-1%2F2%7D+L_%7Ba%2Cb%7D+L_G%5E%7B-1%2F2%7D+%5Cright+%5C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \left\| \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b\in e} g^{(k)}_{e,a,b} \ L_{G}^{-1/2} L_{a,b} L_G^{-1/2} \right \| "/></p>
<p align="center"><img alt="\displaystyle  = \left\| \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b\in e} g^{(k)}_{e,a,b} \ M_{a,b} \right \| " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Cleft%5C%7C+%5Csum_%7Be%5Cin+E_k%7D+w%5E%7B%28k-1%29%7D_e+%5Csum_%7Ba%2Cb%5Cin+e%7D+g%5E%7B%28k%29%7D_%7Be%2Ca%2Cb%7D+%5C+M_%7Ba%2Cb%7D+%5Cright+%5C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \left\| \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b\in e} g^{(k)}_{e,a,b} \ M_{a,b} \right \| "/></p>
<p> where, as before, we use the notation <img alt="{M_{a,b} := L_{G}^{-1/2} L_{a,b} L_G^{-1/2} }" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM_%7Ba%2Cb%7D+%3A%3D+L_%7BG%7D%5E%7B-1%2F2%7D+L_%7Ba%2Cb%7D+L_G%5E%7B-1%2F2%7D+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M_{a,b} := L_{G}^{-1/2} L_{a,b} L_G^{-1/2} }"/>, <img alt="{M^{(k)} = L_{G}^{-1/2} L_{G^{(k)} } L_G^{-1/2} }" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%5E%7B%28k%29%7D+%3D+L_%7BG%7D%5E%7B-1%2F2%7D+L_%7BG%5E%7B%28k%29%7D+%7D+L_G%5E%7B-1%2F2%7D+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M^{(k)} = L_{G}^{-1/2} L_{G^{(k)} } L_G^{-1/2} }"/> and <img alt="{M := L_{G}^{-1/2} L_G L_G^{-1/2} }" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM+%3A%3D+L_%7BG%7D%5E%7B-1%2F2%7D+L_G+L_G%5E%7B-1%2F2%7D+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M := L_{G}^{-1/2} L_G L_G^{-1/2} }"/>.</p>
<p>
By matrix Chernoff bounds for Gaussian sums of matrices,</p>
<p/><p align="center"><img alt="\displaystyle  \mathop{\mathbb E} \left\| \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b\in e} g^{(k)}_{e,a,b} \ M_{a,b} \right \| " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Cleft%5C%7C+%5Csum_%7Be%5Cin+E_k%7D+w%5E%7B%28k-1%29%7D_e+%5Csum_%7Ba%2Cb%5Cin+e%7D+g%5E%7B%28k%29%7D_%7Be%2Ca%2Cb%7D+%5C+M_%7Ba%2Cb%7D+%5Cright+%5C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E} \left\| \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b\in e} g^{(k)}_{e,a,b} \ M_{a,b} \right \| "/></p>
<p> <a name="eq.after.chernoff"/></p><a name="eq.after.chernoff">
<p align="center"><img alt="\displaystyle   \leq O(\sqrt{\log n}) \cdot \sqrt{\max_{e\in E_k, a,b\in e} \left\| w^{(k-1)}_e M_{a,b} \right\|} \sqrt{ \left\| \sum_{e\in E_k} \sum_{a,b\in e} w^{(k-1)}_e \ M_{a,b} \right \| } \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Cleq+O%28%5Csqrt%7B%5Clog+n%7D%29+%5Ccdot+%5Csqrt%7B%5Cmax_%7Be%5Cin+E_k%2C+a%2Cb%5Cin+e%7D+%5Cleft%5C%7C+w%5E%7B%28k-1%29%7D_e+M_%7Ba%2Cb%7D+%5Cright%5C%7C%7D+%5Csqrt%7B+%5Cleft%5C%7C+%5Csum_%7Be%5Cin+E_k%7D+%5Csum_%7Ba%2Cb%5Cin+e%7D+w%5E%7B%28k-1%29%7D_e+%5C+M_%7Ba%2Cb%7D+%5Cright+%5C%7C+%7D+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle   \leq O(\sqrt{\log n}) \cdot \sqrt{\max_{e\in E_k, a,b\in e} \left\| w^{(k-1)}_e M_{a,b} \right\|} \sqrt{ \left\| \sum_{e\in E_k} \sum_{a,b\in e} w^{(k-1)}_e \ M_{a,b} \right \| } \ \ \ \ \ (1)"/></p>
</a><p><a name="eq.after.chernoff"/></p>
<p>
Recall that if <img alt="{e\in E_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%5Cin+E_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e\in E_k}"/> is a hyperedge that is active at round <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>, then <img alt="{p_e \leq 2^{k-1-\ell}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e \leq 2^{k-1-\ell}}"/> and <img alt="{w^{(k-1)}_e \leq 2^{k-1-\ell} / p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%5E%7B%28k-1%29%7D_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D+%2F+p_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w^{(k-1)}_e \leq 2^{k-1-\ell} / p_e}"/>, and we also have </p>
<p align="center"><img alt="\displaystyle  p_e \geq B \sum_{a,b\in e} ||M_{a,b}|| \geq B \frac r2 \max_{a,b\in e} ||M_{a,b}||" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++p_e+%5Cgeq+B+%5Csum_%7Ba%2Cb%5Cin+e%7D+%7C%7CM_%7Ba%2Cb%7D%7C%7C+%5Cgeq+B+%5Cfrac+r2+%5Cmax_%7Ba%2Cb%5Cin+e%7D+%7C%7CM_%7Ba%2Cb%7D%7C%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  p_e \geq B \sum_{a,b\in e} ||M_{a,b}|| \geq B \frac r2 \max_{a,b\in e} ||M_{a,b}||"/></p>
<p> so that we have </p>
<p align="center"><img alt="\displaystyle  \max_{e\in E_k, a,b\in e} w^{(k-1)}_e ||M_{a,b} || \leq \frac{2^{k-1-\ell}} {Br}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmax_%7Be%5Cin+E_k%2C+a%2Cb%5Cin+e%7D+w%5E%7B%28k-1%29%7D_e+%7C%7CM_%7Ba%2Cb%7D+%7C%7C+%5Cleq+%5Cfrac%7B2%5E%7Bk-1-%5Cell%7D%7D+%7BBr%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \max_{e\in E_k, a,b\in e} w^{(k-1)}_e ||M_{a,b} || \leq \frac{2^{k-1-\ell}} {Br}"/></p>
<p> The last term of <a href="https://lucatrevisan.wordpress.com/feed/#eq.after.chernoff">(1)</a> is the spectral norm of <img alt="{M^{(k-1)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%5E%7B%28k-1%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M^{(k-1)}}"/>, which is at most 2 by the assumption of the Lemma.</p>
<p>
Collecting all the pieces, we have proved that </p>
<p align="center"><img alt="\displaystyle  \mathop{\mathbb E} \ \sup_{x\in T} \ \hat F^{(k)} (x) \leq O \left( \sqrt{\log n \cdot \frac {2^{k-\ell}}{B r}}\right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5C+%5Csup_%7Bx%5Cin+T%7D+%5C+%5Chat+F%5E%7B%28k%29%7D+%28x%29+%5Cleq+O+%5Cleft%28+%5Csqrt%7B%5Clog+n+%5Ccdot+%5Cfrac+%7B2%5E%7Bk-%5Cell%7D%7D%7BB+r%7D%7D%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E} \ \sup_{x\in T} \ \hat F^{(k)} (x) \leq O \left( \sqrt{\log n \cdot \frac {2^{k-\ell}}{B r}}\right) "/></p>
<p>
We also need to bound the diameter of <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>, that is</p>
<p/><p align="center"><img alt="\displaystyle  \sup_{x,y \in T} \sqrt{\mathop{\mathbb E} (\hat F^{(k)}(x) -\hat F^{(k)} (y) )^2 } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%2Cy+%5Cin+T%7D+%5Csqrt%7B%5Cmathop%7B%5Cmathbb+E%7D+%28%5Chat+F%5E%7B%28k%29%7D%28x%29+-%5Chat+F%5E%7B%28k%29%7D+%28y%29+%29%5E2+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sup_{x,y \in T} \sqrt{\mathop{\mathbb E} (\hat F^{(k)}(x) -\hat F^{(k)} (y) )^2 } "/></p>
<p> under the usual change of basis, for every <img alt="{x,y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x,y}"/> of length 1 we want to bound the square root of </p>
<p align="center"><img alt="\displaystyle  \mathop{\mathbb E}\left ( \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b} g_{a,e,b} x^T (M_{a,b} x - y^T M_{a,b} y)\right)^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D%5Cleft+%28+%5Csum_%7Be%5Cin+E_k%7D+w%5E%7B%28k-1%29%7D_e+%5Csum_%7Ba%2Cb%7D+g_%7Ba%2Ce%2Cb%7D+x%5ET+%28M_%7Ba%2Cb%7D+x+-+y%5ET+M_%7Ba%2Cb%7D+y%29%5Cright%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E}\left ( \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b} g_{a,e,b} x^T (M_{a,b} x - y^T M_{a,b} y)\right)^2 "/></p>
<p align="center"><img alt="\displaystyle  = \sum_{e\in E_k} \left( w^{(k-1)}_e\right)^2 \sum_{a,b} (x^T M_{a,b} x - y^T M_{a,b} y)^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_%7Be%5Cin+E_k%7D+%5Cleft%28+w%5E%7B%28k-1%29%7D_e%5Cright%29%5E2+%5Csum_%7Ba%2Cb%7D+%28x%5ET+M_%7Ba%2Cb%7D+x+-+y%5ET+M_%7Ba%2Cb%7D+y%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \sum_{e\in E_k} \left( w^{(k-1)}_e\right)^2 \sum_{a,b} (x^T M_{a,b} x - y^T M_{a,b} y)^2 "/></p>
<p align="center"><img alt="\displaystyle  \leq \sum_{e\in E_k} \left( w^{(k-1)}_e\right)^2 \sum_{a,b} (x^T M_{a,b} x)^2 + (y^T M_{a,b} y)^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+%5Csum_%7Be%5Cin+E_k%7D+%5Cleft%28+w%5E%7B%28k-1%29%7D_e%5Cright%29%5E2+%5Csum_%7Ba%2Cb%7D+%28x%5ET+M_%7Ba%2Cb%7D+x%29%5E2+%2B+%28y%5ET+M_%7Ba%2Cb%7D+y%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \leq \sum_{e\in E_k} \left( w^{(k-1)}_e\right)^2 \sum_{a,b} (x^T M_{a,b} x)^2 + (y^T M_{a,b} y)^2 "/></p>
<p align="center"><img alt="\displaystyle  \leq \sum_{e\in E_k} \left( w^{(k-1)}_e\right)^2 \max_{a,b \in e} ||M_{a,b} || \sum_{a,b} (x^T M_{a,b} x + y^T M_{a,b} y) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+%5Csum_%7Be%5Cin+E_k%7D+%5Cleft%28+w%5E%7B%28k-1%29%7D_e%5Cright%29%5E2+%5Cmax_%7Ba%2Cb+%5Cin+e%7D+%7C%7CM_%7Ba%2Cb%7D+%7C%7C+%5Csum_%7Ba%2Cb%7D+%28x%5ET+M_%7Ba%2Cb%7D+x+%2B+y%5ET+M_%7Ba%2Cb%7D+y%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \leq \sum_{e\in E_k} \left( w^{(k-1)}_e\right)^2 \max_{a,b \in e} ||M_{a,b} || \sum_{a,b} (x^T M_{a,b} x + y^T M_{a,b} y) "/></p>
<p> where <img alt="{w^{(k-1)}_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%5E%7B%28k-1%29%7D_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w^{(k-1)}_e}"/> is either zero or <img alt="{\frac{2^{k-\ell-1}}{B \sum_{a,b\in e} ||M_e||}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B2%5E%7Bk-%5Cell-1%7D%7D%7BB+%5Csum_%7Ba%2Cb%5Cin+e%7D+%7C%7CM_e%7C%7C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{2^{k-\ell-1}}{B \sum_{a,b\in e} ||M_e||}}"/> and we can continue our chain of inequalities with </p>
<p align="center"><img alt="\displaystyle  \leq \frac{2^{k-\ell}}{Br} \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b} (x^T M_{a,b} x + y^T M_{a,b} y )" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+%5Cfrac%7B2%5E%7Bk-%5Cell%7D%7D%7BBr%7D+%5Csum_%7Be%5Cin+E_k%7D+w%5E%7B%28k-1%29%7D_e+%5Csum_%7Ba%2Cb%7D+%28x%5ET+M_%7Ba%2Cb%7D+x+%2B+y%5ET+M_%7Ba%2Cb%7D+y+%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \leq \frac{2^{k-\ell}}{Br} \sum_{e\in E_k} w^{(k-1)}_e \sum_{a,b} (x^T M_{a,b} x + y^T M_{a,b} y )"/></p>
<p align="center"><img alt="\displaystyle  = \frac{2^{k-\ell}}{Br} (x^T M^{(k-1)} x + y^T M^{(k-1)} y ) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Cfrac%7B2%5E%7Bk-%5Cell%7D%7D%7BBr%7D+%28x%5ET+M%5E%7B%28k-1%29%7D+x+%2B+y%5ET+M%5E%7B%28k-1%29%7D+y+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \frac{2^{k-\ell}}{Br} (x^T M^{(k-1)} x + y^T M^{(k-1)} y ) "/></p>
<p align="center"><img alt="\displaystyle  \leq 4 \cdot \frac{2^{k-\ell}}{Br} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+4+%5Ccdot+%5Cfrac%7B2%5E%7Bk-%5Cell%7D%7D%7BBr%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \leq 4 \cdot \frac{2^{k-\ell}}{Br} "/></p>
<p> where we used again the assumption of the Lemma <img alt="{|| M^{(k-1)} || \leq 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C+M%5E%7B%28k-1%29%7D+%7C%7C+%5Cleq+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|| M^{(k-1)} || \leq 2}"/>.</p>
<p>
To prove the last part of the lemma, we have to prove that with high probability</p>
<p/><p align="center"><img alt="\displaystyle  || M^{(k)} - M^{(k-1)} || \leq O\left( \sqrt{ \frac {2^{k-\ell} \log n}{B}} \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+M%5E%7B%28k%29%7D+-+M%5E%7B%28k-1%29%7D+%7C%7C+%5Cleq+O%5Cleft%28+%5Csqrt%7B+%5Cfrac+%7B2%5E%7Bk-%5Cell%7D+%5Clog+n%7D%7BB%7D%7D+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  || M^{(k)} - M^{(k-1)} || \leq O\left( \sqrt{ \frac {2^{k-\ell} \log n}{B}} \right) "/></p>
<p> We see that </p>
<p align="center"><img alt="\displaystyle  M^{(k)} - M^{(k-1)} = \sum_{e\in E_k} w^{(k-1)}_e r^{(k)}_e \sum_{a,b \in e} M_{a,b} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++M%5E%7B%28k%29%7D+-+M%5E%7B%28k-1%29%7D+%3D+%5Csum_%7Be%5Cin+E_k%7D+w%5E%7B%28k-1%29%7D_e+r%5E%7B%28k%29%7D_e+%5Csum_%7Ba%2Cb+%5Cin+e%7D+M_%7Ba%2Cb%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  M^{(k)} - M^{(k-1)} = \sum_{e\in E_k} w^{(k-1)}_e r^{(k)}_e \sum_{a,b \in e} M_{a,b} "/></p>
<p> as noted before, each <img alt="{ w^{(k-1)}_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+w%5E%7B%28k-1%29%7D_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{ w^{(k-1)}_e}"/> is either zero or <img alt="{\frac {2^{k-1-\ell}}{B\sum_{a,b\in e} ||M_{a,b}||}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+%7B2%5E%7Bk-1-%5Cell%7D%7D%7BB%5Csum_%7Ba%2Cb%5Cin+e%7D+%7C%7CM_%7Ba%2Cb%7D%7C%7C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac {2^{k-1-\ell}}{B\sum_{a,b\in e} ||M_{a,b}||}}"/>, so </p>
<p align="center"><img alt="\displaystyle  \left \| w^{(k-1)}_e \sum_{a,b \in e} M_{a,b} \right \| \leq \frac {2^{k-1-\ell}}{B} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft+%5C%7C+w%5E%7B%28k-1%29%7D_e+%5Csum_%7Ba%2Cb+%5Cin+e%7D+M_%7Ba%2Cb%7D+%5Cright+%5C%7C+%5Cleq+%5Cfrac+%7B2%5E%7Bk-1-%5Cell%7D%7D%7BB%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \left \| w^{(k-1)}_e \sum_{a,b \in e} M_{a,b} \right \| \leq \frac {2^{k-1-\ell}}{B} "/></p>
<p> and the final claim follows from matrix Chernoff bounds.</p>
<p>
Now we can put everything together. Applying our lemma inductively, we can say that with high probability</p>
<p/><p align="center"><img alt="\displaystyle  \sup_{x: x^T L_G x = 1} |F(x)| \leq \sum_{k=1}^\ell \sup_{x: x^T L_G x = 1} |F^{(k)} (x)| \leq \sum_{k=1}^\ell O\left( \sqrt{ 2^{k-\ell} \frac{\log n}{Br} } \right) \leq O \left( \sqrt{ \frac{\log n}{Br} } \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%3A+x%5ET+L_G+x+%3D+1%7D+%7CF%28x%29%7C+%5Cleq+%5Csum_%7Bk%3D1%7D%5E%5Cell+%5Csup_%7Bx%3A+x%5ET+L_G+x+%3D+1%7D+%7CF%5E%7B%28k%29%7D+%28x%29%7C+%5Cleq+%5Csum_%7Bk%3D1%7D%5E%5Cell+O%5Cleft%28+%5Csqrt%7B+2%5E%7Bk-%5Cell%7D+%5Cfrac%7B%5Clog+n%7D%7BBr%7D+%7D+%5Cright%29+%5Cleq+O+%5Cleft%28+%5Csqrt%7B+%5Cfrac%7B%5Clog+n%7D%7BBr%7D+%7D+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sup_{x: x^T L_G x = 1} |F(x)| \leq \sum_{k=1}^\ell \sup_{x: x^T L_G x = 1} |F^{(k)} (x)| \leq \sum_{k=1}^\ell O\left( \sqrt{ 2^{k-\ell} \frac{\log n}{Br} } \right) \leq O \left( \sqrt{ \frac{\log n}{Br} } \right) "/></p>
<p> and </p>
<p align="center"><img alt="\displaystyle  \forall k: L^{(k)} _G \preceq O \left( \sum_{i=1}^k \sqrt{ 2^{i-\ell} \frac{\log n}{B} } \right) \cdot L_G \preceq 2 L_G " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+k%3A+L%5E%7B%28k%29%7D+_G+%5Cpreceq+O+%5Cleft%28+%5Csum_%7Bi%3D1%7D%5Ek+%5Csqrt%7B+2%5E%7Bi-%5Cell%7D+%5Cfrac%7B%5Clog+n%7D%7BB%7D+%7D+%5Cright%29+%5Ccdot+L_G+%5Cpreceq+2+L_G+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall k: L^{(k)} _G \preceq O \left( \sum_{i=1}^k \sqrt{ 2^{i-\ell} \frac{\log n}{B} } \right) \cdot L_G \preceq 2 L_G "/></p>
<p> provided that we choose <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> at least at absolute constant times <img alt="{\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log n}"/>.</p>
<p>
In particular, we can choose <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> to be an absolute constant times <img alt="{\epsilon^{-2} r^3 \log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%5E%7B-2%7D+r%5E3+%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon^{-2} r^3 \log n}"/> and have</p>
<p/><p align="center"><img alt="\displaystyle  \sup_{x: x^T L_G x = 1} |F(x)| \leq \frac{2}{r^2} \epsilon " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%3A+x%5ET+L_G+x+%3D+1%7D+%7CF%28x%29%7C+%5Cleq+%5Cfrac%7B2%7D%7Br%5E2%7D+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sup_{x: x^T L_G x = 1} |F(x)| \leq \frac{2}{r^2} \epsilon "/></p>
<p> which is the same as </p>
<p align="center"><img alt="\displaystyle  \forall x: \ \ |Q_{H} (x) - Q_{H'} (x) | \leq \frac{2}{r^2} \epsilon \cdot x^T L_G x \leq \epsilon\ Q_H(x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+x%3A+%5C+%5C+%7CQ_%7BH%7D+%28x%29+-+Q_%7BH%27%7D+%28x%29+%7C+%5Cleq+%5Cfrac%7B2%7D%7Br%5E2%7D+%5Cepsilon+%5Ccdot+x%5ET+L_G+x+%5Cleq+%5Cepsilon%5C+Q_H%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall x: \ \ |Q_{H} (x) - Q_{H'} (x) | \leq \frac{2}{r^2} \epsilon \cdot x^T L_G x \leq \epsilon\ Q_H(x) "/></p>
<p>
So, with high probability, <img alt="{H'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H'}"/> is a spectral sparsifier of <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> with error <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>, and it has <img alt="{O(Bn) = O(\epsilon^{-2} r^3 n \log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28Bn%29+%3D+O%28%5Cepsilon%5E%7B-2%7D+r%5E3+n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(Bn) = O(\epsilon^{-2} r^3 n \log n)}"/> hyperedges</p>
<p>
</p><p><b>4. Some Additional Thoughts </b></p>
<p/><p>
The dependence on <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> is definitely not optimal, particularly because when <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> is order of <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> we know from the work of Soma and Yoshida that we can do better. One place where we seem to lose is that, although we know</p>
<p/><p align="center"><img alt="\displaystyle  Q_e(x) \leq O \left( \frac 1r \right) \ \sum_{a,b\in } x^T L_{a,b} x " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q_e%28x%29+%5Cleq+O+%5Cleft%28+%5Cfrac+1r+%5Cright%29+%5C+%5Csum_%7Ba%2Cb%5Cin+%7D+x%5ET+L_%7Ba%2Cb%7D+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  Q_e(x) \leq O \left( \frac 1r \right) \ \sum_{a,b\in } x^T L_{a,b} x "/></p>
<p> we are only able to show that </p>
<p align="center"><img alt="\displaystyle  \sum_{e\in E_k} r^{(k)}_e w^{(k)}_e Q_e(x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Be%5Cin+E_k%7D+r%5E%7B%28k%29%7D_e+w%5E%7B%28k%29%7D_e+Q_e%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{e\in E_k} r^{(k)}_e w^{(k)}_e Q_e(x) "/></p>
<p> is <img alt="{O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(1)}"/>-dominated by </p>
<p align="center"><img alt="\displaystyle  \sum_{e\in E_k} g^{(k)}_e w^{(k)}_e \sum_{a,b \in e } x^T L_{ab} x " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Be%5Cin+E_k%7D+g%5E%7B%28k%29%7D_e+w%5E%7B%28k%29%7D_e+%5Csum_%7Ba%2Cb+%5Cin+e+%7D+x%5ET+L_%7Bab%7D+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{e\in E_k} g^{(k)}_e w^{(k)}_e \sum_{a,b \in e } x^T L_{ab} x "/></p>
<p> rather than, as we could have hoped, <img alt="{O(1/r)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%281%2Fr%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(1/r)}"/>-dominated. The difficulty is that the bound</p>
<p/><p align="center"><img alt="\displaystyle  (Q_e(x) - Q_e(y))^2 \leq \sum_{a,b\in e} (x^T L_{ab} x - y^T L_{ab} y)^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28Q_e%28x%29+-+Q_e%28y%29%29%5E2+%5Cleq+%5Csum_%7Ba%2Cb%5Cin+e%7D+%28x%5ET+L_%7Bab%7D+x+-+y%5ET+L_%7Bab%7D+y%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  (Q_e(x) - Q_e(y))^2 \leq \sum_{a,b\in e} (x^T L_{ab} x - y^T L_{ab} y)^2 "/></p>
<p> is sometimes tight. In order to do better, it seems necessary to do something a bit differently from what we do here. </p>
<p>
The effective resistance of an edge <img alt="{(a,b)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28a%2Cb%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(a,b)}"/> in a graph <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> can be written as</p>
<p/><p align="center"><img alt="\displaystyle  \sup_{x\in {\mathbb R}^n} \ \ \frac{x^T L_{a,b} x}{x^T L_G x} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+%5C+%5Cfrac%7Bx%5ET+L_%7Ba%2Cb%7D+x%7D%7Bx%5ET+L_G+x%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sup_{x\in {\mathbb R}^n} \ \ \frac{x^T L_{a,b} x}{x^T L_G x} "/></p>
<p>
with the convention that <img alt="{0/0 = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%2F0+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0/0 = 0}"/>. So it seems reasonable that a good definition of effective resistance for an hyperedge <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> in a hypergraph <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> would be</p>
<p/><p align="center"><img alt="\displaystyle  \sup_{x\in {\mathbb R}^n} \ \ \frac{Q_e(x)}{Q_H(x) } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%5Cin+%7B%5Cmathbb+R%7D%5En%7D+%5C+%5C+%5Cfrac%7BQ_e%28x%29%7D%7BQ_H%28x%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sup_{x\in {\mathbb R}^n} \ \ \frac{Q_e(x)}{Q_H(x) } "/></p>
<p> One can argue that these “effective resistances” add up to <img alt="{O(nr)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28nr%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(nr)}"/>, but perhaps they add up to <img alt="{O(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n)}"/>? If we sample according to those “effective resitances,” can we apply generic chaining directly to <img alt="{\sum_{e\in E_k} w^{(k)}_e g^{(k)}_e Q_e(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Be%5Cin+E_k%7D+w%5E%7B%28k%29%7D_e+g%5E%7B%28k%29%7D_e+Q_e%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_{e\in E_k} w^{(k)}_e g^{(k)}_e Q_e(x)}"/> without having to rely on a Gaussian process on matrices, for which we have Chernoff bounds?</p>
<p/></div>
    </content>
    <updated>2020-05-14T16:13:04Z</updated>
    <published>2020-05-14T16:13:04Z</published>
    <category term="theory"/>
    <category term="sparsification"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2020-05-17T22:24:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/05/14/6-year-postdoc-at-tu-wien-vienna-austria-apply-by-may-28-2020/</id>
    <link href="https://cstheory-jobs.org/2020/05/14/6-year-postdoc-at-tu-wien-vienna-austria-apply-by-may-28-2020/" rel="alternate" type="text/html"/>
    <title>6-year postdoc at TU Wien, Vienna, Austria (apply by May 28, 2020)</title>
    <summary>A 6-Year Postdoc Position in Algorithms is available at TU Wien, Vienna, Austria. Research experience in one of the following areas is of advantage: parameterized complexity, algorithmic applications of graph decompositions, SAT and CSP. Website: https://www.ac.tuwien.ac.at/jobs/#junprof Email: sz@ac.tuwien.ac.at</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A 6-Year Postdoc Position in Algorithms is available at TU Wien, Vienna, Austria. Research experience in one of the following areas is of advantage: parameterized complexity, algorithmic applications of graph decompositions, SAT and CSP.</p>
<p>Website: <a href="https://www.ac.tuwien.ac.at/jobs/#junprof">https://www.ac.tuwien.ac.at/jobs/#junprof</a><br/>
Email: sz@ac.tuwien.ac.at</p></div>
    </content>
    <updated>2020-05-14T10:16:15Z</updated>
    <published>2020-05-14T10:16:15Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-05-17T22:33:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/05/14/postdoc-at-university-of-haifa-israel-apply-by-december-30-2020/</id>
    <link href="https://cstheory-jobs.org/2020/05/14/postdoc-at-university-of-haifa-israel-apply-by-december-30-2020/" rel="alternate" type="text/html"/>
    <title>Postdoc at University of Haifa (Israel) (apply by December 30, 2020)</title>
    <summary>We invite applications for a postdoc position, hosted by Or Meir, at the university of Haifa in Israel. We are especially looking for candidates who are interested in working on communication complexity and circuit complexity, but we will also consider candidates who are interested in other areas of complexity theory and algorithms. For details, please […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We invite applications for a postdoc position, hosted by Or Meir, at the university of Haifa in Israel. We are especially looking for candidates who are interested in working on communication complexity and circuit complexity, but we will also consider candidates who are interested in other areas of complexity theory and algorithms. For details, please e-mail Or Meir directly.</p>
<p>Website: <a href="https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxub2dhcm9uemV3aTF8Z3g6Nzc2ZjU5ZjIwY2QwNzJhMg&amp;urp=gmail_link">https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxub2dhcm9uemV3aTF8Z3g6Nzc2ZjU5ZjIwY2QwNzJhMg&amp;urp=gmail_link</a><br/>
Email: ormeir@cs.haifa.ac.il</p></div>
    </content>
    <updated>2020-05-14T09:57:34Z</updated>
    <published>2020-05-14T09:57:34Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-05-17T22:32:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/05/13/lecturer-at-university-of-illinois-chicago-apply-by-may-20-2020/</id>
    <link href="https://cstheory-jobs.org/2020/05/13/lecturer-at-university-of-illinois-chicago-apply-by-may-20-2020/" rel="alternate" type="text/html"/>
    <title>Lecturer at University of Illinois Chicago (apply by May 20, 2020)</title>
    <summary>The Department of Mathematics, Statistics, and Computer Science at UIC is accepting applications for Lecturer positions beginning in Fall 2020 to teach computer science and statistics courses. Please see the full ad at the link for more details. Website: https://jobs.uic.edu/job-board/job-details?jobID=131035&amp;job=lecturer-mathematics-statistics-and-computer-science Email: willp@uic.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Mathematics, Statistics, and Computer Science at UIC is accepting applications for Lecturer positions beginning in Fall 2020 to teach computer science and statistics courses. Please see the full ad at the link for more details.</p>
<p>Website: <a href="https://jobs.uic.edu/job-board/job-details?jobID=131035&amp;job=lecturer-mathematics-statistics-and-computer-science">https://jobs.uic.edu/job-board/job-details?jobID=131035&amp;job=lecturer-mathematics-statistics-and-computer-science</a><br/>
Email: willp@uic.edu</p></div>
    </content>
    <updated>2020-05-13T23:22:28Z</updated>
    <published>2020-05-13T23:22:28Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-05-17T22:32:57Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7728</id>
    <link href="https://windowsontheory.org/2020/05/13/resources-for-the-upcoming-job-market-crunch/" rel="alternate" type="text/html"/>
    <title>Resources for the upcoming job market crunch</title>
    <summary>Aside from its devastating death toll, the COVID-19 pandemic has had severe economic implications. The impact on universities is particularly substantial, including disruptions to our physical campuses and student residences, as well as to the sources of income for private and public universities such as endowments and state budgets. All this means that the academic […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Aside from its devastating death toll, the COVID-19 pandemic has had severe economic implications. The impact on universities is particularly substantial, including disruptions to our physical campuses and student residences, as well as to the sources of income for private and public universities such as endowments and state budgets.</p>



<p>All this means that the academic job market is likely going to be tough in the near future, and computer  science will not be immune. During the last recession, the CCC started a <a href="https://cra.org/ccc/leadership-development/cifellows/">computing innovation fellows</a> program which was very successful, and I hope that something similar will occur this time as well. But it won’t be enough.</p>



<p>If you are aware of any postdoc positions (or better yet, can create one) please do make sure to post it on the <a href="https://cstheory-jobs.org/">CS theory job board</a>. If you know of any teaching position that could potentially be applicable for theorists, please post it there too.  This crisis can also be an opportunity to get fantastic people for such positions. If you have any ideas on how we as the theoretical CS community can support graduating students  and postdocs, please do share these in the comments or on <a href="https://twitter.com/boazbaraktcs">Twitter</a>.</p>



<p>If anything, this crisis has taught us that the world needs more science, not less. Moreover, computer science has been and will continue to be a crucial component in fighting this epidemic, including not just modeling but also tracing applications using crypto, load balancing that ensures the Internet doesn’t crush, and more. I am thus hopeful that within a couple of years, the academic job market for theoretical computer scientists will recover. However we should try to do all we can to help our junior colleagues get through this period.</p></div>
    </content>
    <updated>2020-05-13T16:22:27Z</updated>
    <published>2020-05-13T16:22:27Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-05-17T22:35:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/05/13/postdoc-at-university-of-salzburg-apply-by-june-14-2020/</id>
    <link href="https://cstheory-jobs.org/2020/05/13/postdoc-at-university-of-salzburg-apply-by-june-14-2020/" rel="alternate" type="text/html"/>
    <title>Postdoc at University of Salzburg (apply by June 14, 2020)</title>
    <summary>A postdoc position is available in the project “Distributed Algorithms for Fundamental Graph Problems” led by Sebastian Forster at the University of Salzburg, Austria. Research experience in efficient graph algorithms is required, but not specifically in the distributed setting. The position is for one year with the possibility of extension by another year. Website: https://www.cs.sbg.ac.at/~forster/openings.html […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A postdoc position is available in the project “Distributed Algorithms for Fundamental Graph Problems” led by Sebastian Forster at the University of Salzburg, Austria. Research experience in efficient graph algorithms is required, but not specifically in the distributed setting. The position is for one year with the possibility of extension by another year.</p>
<p>Website: <a href="https://www.cs.sbg.ac.at/~forster/openings.html">https://www.cs.sbg.ac.at/~forster/openings.html</a><br/>
Email: sebastian.forster@sbg.ac.at</p></div>
    </content>
    <updated>2020-05-13T14:54:41Z</updated>
    <published>2020-05-13T14:54:41Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-05-17T22:32:58Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4385</id>
    <link href="https://lucatrevisan.wordpress.com/2020/05/12/spielman-srivastava-sparsification-a-la-talagrand/" rel="alternate" type="text/html"/>
    <title>Spielman-Srivastava Sparsification à la Talagrand</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This is the second in a series of posts explaining a result on hypergraph sparsification that uses Talagrand’s work on Gaussian processes. In the previous post we talked about Gaussian and sub-Gaussian processes and generic chaining. In this post we … <a href="https://lucatrevisan.wordpress.com/2020/05/12/spielman-srivastava-sparsification-a-la-talagrand/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
 This is the second in a series of posts explaining a <a href="https://arxiv.org/abs/1905.01495">result on hypergraph sparsification</a> that uses Talagrand’s work on Gaussian processes. </p>
<p>
In the <a href="https://lucatrevisan.wordpress.com/2020/05/03/talagrands-generic-chaining/">previous post</a> we talked about Gaussian and sub-Gaussian processes and generic chaining. </p>
<p>
In this post we talk about the Spielman-Srivastava probabilistic construction of graph sparsifiers. Their analysis requires a bound on the largest eigenvalue of a certain random matrix, that can be derived from matrix Chernoff bounds. </p>
<p>
We will then make our life harder and we will also derive an analysis of the Spielman-Srivastava construction by casting the largest eigenvalue of that random matrix as the sup of a sub-Gaussian process, and then we will apply the machinery from the previous post.</p>
<p>
This will be more complicated than it needs to be, but the payoff will be that, as will be shown in the next post, this more complicated proof will also apply, with some changes, to the setting of hypergraphs.</p>
<p>
<span id="more-4385"/></p>
<p>
</p><p><b>1. Graph Sparsifiers </b></p>
<p/><p>
For a graph <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> having vertex set <img alt="{V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V}"/>, if <img alt="{S\subseteq V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%5Csubseteq+V%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S\subseteq V}"/> is a subset of vertices, we denote by <img alt="{cut_G(S)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bcut_G%28S%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{cut_G(S)}"/> the number of edges that cross the cut <img alt="{(S,V-S)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28S%2CV-S%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(S,V-S)}"/>, that is, that have one endpoint in <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> and one endpoint outside <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>. If <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> is a weighted graph, <img alt="{cut_G(S)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bcut_G%28S%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{cut_G(S)}"/> is the sum of the weights of such edges.</p>
<p>
A graph <img alt="{G'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G'}"/> is a <em>cut sparsifier</em> of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> with error at most <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/> if <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> and <img alt="{G'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G'}"/> have the same vertex set <img alt="{V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V}"/> and, for every <img alt="{S\subseteq V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%5Csubseteq+V%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S\subseteq V}"/>, we have</p>
<p/><p align="center"><img alt="\displaystyle  1-\epsilon \leq \frac{cut_G(S)}{cut_{G'} (S) } \leq 1+\epsilon " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1-%5Cepsilon+%5Cleq+%5Cfrac%7Bcut_G%28S%29%7D%7Bcut_%7BG%27%7D+%28S%29+%7D+%5Cleq+1%2B%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  1-\epsilon \leq \frac{cut_G(S)}{cut_{G'} (S) } \leq 1+\epsilon "/></p>
<p>
that is, if all cuts in <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> and <img alt="{G'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G'}"/> are the same, up to a multiplicative error <img alt="{1\pm \epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%5Cpm+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1\pm \epsilon}"/>. (We use the convention that <img alt="{0/0 = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%2F0+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0/0 = 1}"/>.) This definition was introduced by Benczur and Karger who showed that every graph has a cut sparsifier with at most <img alt="{O(\epsilon^{-2} n \log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Cepsilon%5E%7B-2%7D+n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\epsilon^{-2} n \log n)}"/> edges, where <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> is the number of vertices, and that such a sparsifier can be constructed by an efficient probabilistic algorithm.</p>
<p>
This is a useful construction because if one wants to run on <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> an algorithm that approximately solves a problem that depends on the cut structure of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> (for example min cut, min st-cut, max flow, or an algorithm for a clustering problem), then one may alternatively run such algorithm on <img alt="{G'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G'}"/>, and an approximate solution for <img alt="{G'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G'}"/> will also be an approximate solution for <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>. The advantage is that one runs the algorithm on a graph that has fewer edges, and so one needs less time and less memory to run the algorithm.</p>
<p>
The approach of Benczur and Karger is to assign to each edge <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> a probability <img alt="{p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e}"/>, in a certain careful way that we will not describe. Then they generate <img alt="{G'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G'}"/> by doing the following independently for every edge <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/>: with probability <img alt="{p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e}"/> we put the edge <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> in <img alt="{G'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G'}"/>, and give it weight <img alt="{1/p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2Fp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/p_e}"/>, and with probability <img alt="{1-p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1-p_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1-p_e}"/> we do not put the edge <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> in <img alt="{G'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G'}"/>. For every cut <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>, we have that <img alt="{cut_{G'}(S)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bcut_%7BG%27%7D%28S%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{cut_{G'}(S)}"/> is a random variable with expectation <img alt="{cut_G(S)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bcut_G%28S%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{cut_G(S)}"/>, and Benczur and Karger are able to use Chernoff bounds and a union bound to show that there is a setting of those probabilities such that it is likely that all cuts will be preserved with multiplicative error at <img alt="{1\pm \epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%5Cpm+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1\pm \epsilon}"/> and such that <img alt="{\sum_e p_e = O(\epsilon^{-2} n \log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_e+p_e+%3D+O%28%5Cepsilon%5E%7B-2%7D+n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_e p_e = O(\epsilon^{-2} n \log n)}"/>. The union bound has to be done very carefully and, in particular, one has to use the fact that there can be few sparse cuts.</p>
<p>
Spielman and Teng introduced the stronger definition of <em>spectral sparsifier</em>: according to their definition, a graph <img alt="{G'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G'}"/> is a spectral sparsifier of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> with error at most <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/> if, for every vector <img alt="{x\in {\mathbb R}^V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+%7B%5Cmathbb+R%7D%5EV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x\in {\mathbb R}^V}"/> we have </p>
<p align="center"><img alt="\displaystyle  1-\epsilon \leq \frac{ x^T L_G x}{x^T L_{G'} x} \leq 1+ \epsilon " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1-%5Cepsilon+%5Cleq+%5Cfrac%7B+x%5ET+L_G+x%7D%7Bx%5ET+L_%7BG%27%7D+x%7D+%5Cleq+1%2B+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  1-\epsilon \leq \frac{ x^T L_G x}{x^T L_{G'} x} \leq 1+ \epsilon "/></p>
<p> where <img alt="{L_G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_G}"/> is the Laplacian matrix of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> and, as before, we take the convention that <img alt="{0/0 = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%2F0+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0/0 = 1}"/>. This is a stronger condition because, if <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> is the indicator vector of a set <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>, then <img alt="{x^T L_G x = cut_G(S)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5ET+L_G+x+%3D+cut_G%28S%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^T L_G x = cut_G(S)}"/>, so the Benczur-Karger condition is equivalent to the special case of the Spielman-Teng condition in which we only quantify over Boolean vectors <img alt="{x\in \{ 0,1\}^V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+%5C%7B+0%2C1%5C%7D%5EV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x\in \{ 0,1\}^V}"/>.</p>
<p>
Spielman and Teng gave an efficient construction of spectral sparsifiers with <img alt="{O(\epsilon^{-2} n \log^{O(1)} n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Cepsilon%5E%7B-2%7D+n+%5Clog%5E%7BO%281%29%7D+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\epsilon^{-2} n \log^{O(1)} n)}"/> edges. </p>
<p>
Later, Spielman and Srivastava reduced the number of edges to <img alt="{O(\epsilon^{-2} n \log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Cepsilon%5E%7B-2%7D+n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\epsilon^{-2} n \log n)}"/>, with a proof similar to that of Benczur and Karger’s: they attribute a probability <img alt="{p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e}"/> to each edge, and then sample each edge <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> with probability <img alt="{p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e}"/>, weighing it <img alt="{1/p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2Fp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/p_e}"/> if selected. The Laplacian <img alt="{L_{G'}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7BG%27%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{G'}}"/> of the resulting weighted graph satisfies <img alt="{\mathop{\mathbb E} L_{G'} = L_G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D+L_%7BG%27%7D+%3D+L_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathop{\mathbb E} L_{G'} = L_G}"/>, and so one has to study the concentration of the random matrix <img alt="{L_{G'}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7BG%27%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{G'}}"/> around its average, which is doable using matrix Chernoff bounds.</p>
<p>
More recently, Batson, Spielman and Srivastava gave a construction of spectral sparsifiers with <img alt="{O(\epsilon^{-2} n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Cepsilon%5E%7B-2%7D+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\epsilon^{-2} n)}"/> edges. Their construction is deterministic, and it proceeds by choosing one edge at a time, in a process that is driven by a certain potential function. Allen-Zhu, Liao and Orecchia have presented an interpretation of Batson-Spielman-Srivastava as an online optimization game played using a Follow-the-Regularized-Leader strategy. As my sequence of posts on online optimization will continue after the current hiatus, I plan to present the results of Allen-Zhu, Liao and Orecchia.</p>
<p>
</p><p><b>2. The Spielman-Srivastava Construction </b></p>
<p/><p>
We will assume that the graph <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> is connected, otherwise we can apply the construction below to each connected component.</p>
<p>
In the Spielman-Srivastava construction, we assign a probability <img alt="{p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e}"/> to each edge, and then we want to say that the event</p>
<p>
<a name="eq.ss"/></p><a name="eq.ss">
<p align="center"><img alt="\displaystyle   \forall x \in {\mathbb R}^n: \ \ \ - \epsilon x^T L_G x \leq x^T L_{G'} x - x^T L_G x \leq \epsilon x^T L_G x \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Cforall+x+%5Cin+%7B%5Cmathbb+R%7D%5En%3A+%5C+%5C+%5C+-+%5Cepsilon+x%5ET+L_G+x+%5Cleq+x%5ET+L_%7BG%27%7D+x+-+x%5ET+L_G+x+%5Cleq+%5Cepsilon+x%5ET+L_G+x+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle   \forall x \in {\mathbb R}^n: \ \ \ - \epsilon x^T L_G x \leq x^T L_{G'} x - x^T L_G x \leq \epsilon x^T L_G x \ \ \ \ \ (1)"/></p>
</a><p><a name="eq.ss"/> holds with high probability, where <img alt="{G'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G'}"/> is the random graph obtained by sampling each edge <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> independently with probability <img alt="{p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e}"/>, and weighing it <img alt="{1/p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2Fp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/p_e}"/> if selected, and <img alt="{L_{G'}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7BG%27%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{G'}}"/> is the Laplacian of <img alt="{G'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G'}"/>. </p>
<p>
Actually, Spielman and Srivastava sample from a slightly different distribution. Namely they repeatedly sample from a distribution on all edges, in which edge <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> has probability proportional to <img alt="{p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e}"/>, because this distribution is easier to analyze with matrix Chernoff bounds, but we will proceed to analyze the distribution described in the above paragraph.</p>
<p>
<a href="https://en.wikipedia.org/wiki/Matrix_Chernoff_bound">Matrix Chernoff bounds</a> give upper bounds to the probability that a sum of independent random matrices deviates in operator norm from its average, and take the form of</p>
<p>
<a name="eq.matrixc"/></p><a name="eq.matrixc">
<p align="center"><img alt="\displaystyle   \Pr \left[ \left\| \sum_e (M_e - \mathop{\mathbb E} M_e ) \right\| \geq \ell \right] \leq \cdots \ \ \ \ \ (2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5CPr+%5Cleft%5B+%5Cleft%5C%7C+%5Csum_e+%28M_e+-+%5Cmathop%7B%5Cmathbb+E%7D+M_e+%29+%5Cright%5C%7C+%5Cgeq+%5Cell+%5Cright%5D+%5Cleq+%5Ccdots+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle   \Pr \left[ \left\| \sum_e (M_e - \mathop{\mathbb E} M_e ) \right\| \geq \ell \right] \leq \cdots \ \ \ \ \ (2)"/></p>
</a><p><a name="eq.matrixc"/> where the <img alt="{M_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M_e}"/> are independent random matrices. If the matrices are <img alt="{n\times n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n\times n}"/> Hermitian, the most general case is given by the <em>matrix Bernstein</em> bound</p>
<p>
<a name="eq.matrix.bern"/></p><a name="eq.matrix.bern">
<p align="center"><img alt="\displaystyle   \Pr \left[ \left\| \sum_e (M_e - \mathop{\mathbb E} M_e ) \right\| \geq \ell \right] \leq 2n \cdot \exp \left( \frac{-\ell^2}{2\sigma^2 + \frac 23 B\ell} \right) \ \ \ \ \ (3)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5CPr+%5Cleft%5B+%5Cleft%5C%7C+%5Csum_e+%28M_e+-+%5Cmathop%7B%5Cmathbb+E%7D+M_e+%29+%5Cright%5C%7C+%5Cgeq+%5Cell+%5Cright%5D+%5Cleq+2n+%5Ccdot+%5Cexp+%5Cleft%28+%5Cfrac%7B-%5Cell%5E2%7D%7B2%5Csigma%5E2+%2B+%5Cfrac+23+B%5Cell%7D+%5Cright%29+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle   \Pr \left[ \left\| \sum_e (M_e - \mathop{\mathbb E} M_e ) \right\| \geq \ell \right] \leq 2n \cdot \exp \left( \frac{-\ell^2}{2\sigma^2 + \frac 23 B\ell} \right) \ \ \ \ \ (3)"/></p>
</a><p><a name="eq.matrix.bern"/> where </p>
<p align="center"><img alt="\displaystyle  \sigma^2 := \left\| \sum_e \mathop{\mathbb E} (M_e - \mathop{\mathbb E} M_e )^2 \right\| " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csigma%5E2+%3A%3D+%5Cleft%5C%7C+%5Csum_e+%5Cmathop%7B%5Cmathbb+E%7D+%28M_e+-+%5Cmathop%7B%5Cmathbb+E%7D+M_e+%29%5E2+%5Cright%5C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sigma^2 := \left\| \sum_e \mathop{\mathbb E} (M_e - \mathop{\mathbb E} M_e )^2 \right\| "/></p>
<p> is the “variance” of <img alt="{\sum_e M_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_e+M_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_e M_e}"/>, and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> is an upper bound such that </p>
<p align="center"><img alt="\displaystyle  \forall e\ \ ||M_e|| \leq B" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+e%5C+%5C+%7C%7CM_e%7C%7C+%5Cleq+B&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall e\ \ ||M_e|| \leq B"/></p>
<p> holds with probability one.</p>
<p>
It remains to reformulate <a href="https://lucatrevisan.wordpress.com/feed/#eq.ss">(1)</a> in a form like <a href="https://lucatrevisan.wordpress.com/feed/#eq.matrixc">(2)</a>. We first note that </p>
<p align="center"><img alt="\displaystyle  L_G = \sum_{e \in E} L_e " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++L_G+%3D+%5Csum_%7Be+%5Cin+E%7D+L_e+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  L_G = \sum_{e \in E} L_e "/></p>
<p> where <img alt="{L_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_e}"/> is the Laplacian matrix of the edge <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/>. That is, if <img alt="{e = (u,v)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be+%3D+%28u%2Cv%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e = (u,v)}"/>, then <img alt="{L_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_e}"/> is the rank-1 matrix whose quadratic form is <img alt="{x^T L_e x = (x_u - x_v)^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5ET+L_e+x+%3D+%28x_u+-+x_v%29%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^T L_e x = (x_u - x_v)^2}"/>.</p>
<p>
So we can write </p>
<p/><p align="center"><img alt="\displaystyle  x^T L_{G'} x - x^T L_G x = \sum_e z_e L_e " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5ET+L_%7BG%27%7D+x+-+x%5ET+L_G+x+%3D+%5Csum_e+z_e+L_e+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x^T L_{G'} x - x^T L_G x = \sum_e z_e L_e "/></p>
<p> where <img alt="{z_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z_e}"/> is a random variable that is equal to <img alt="{1/p_e - 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2Fp_e+-+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/p_e - 1}"/> with probability <img alt="{p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e}"/> and to <img alt="{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-1}"/> with probability <img alt="{1-p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1-p_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1-p_e}"/>. </p>
<p>
We also note that all the terms in <a href="https://lucatrevisan.wordpress.com/feed/#eq.ss">(1)</a> are invariant under shifts in <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>, so we can rewrite the event <a href="https://lucatrevisan.wordpress.com/feed/#eq.ss">(1)</a> as</p>
<p/><p align="center"><img alt="\displaystyle  \forall x\in {\mathbb R}^n_\perp : \ \ - \epsilon x^T L_G x \leq \sum_e z_e x^T L_e x \leq \epsilon x^T L_G x \ \ \ \ \ (4)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+x%5Cin+%7B%5Cmathbb+R%7D%5En_%5Cperp+%3A+%5C+%5C+-+%5Cepsilon+x%5ET+L_G+x+%5Cleq+%5Csum_e+z_e+x%5ET+L_e+x+%5Cleq+%5Cepsilon+x%5ET+L_G+x+%5C+%5C+%5C+%5C+%5C+%284%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall x\in {\mathbb R}^n_\perp : \ \ - \epsilon x^T L_G x \leq \sum_e z_e x^T L_e x \leq \epsilon x^T L_G x \ \ \ \ \ (4)"/></p>
<p>
Where <img alt="{{\mathbb R}^n _\perp}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+R%7D%5En+_%5Cperp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\mathbb R}^n _\perp}"/> is the set of vectors <img alt="{x\in {\mathbb R}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x\in {\mathbb R}^n}"/> that are orthogonal to <img alt="{(1\cdots 1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281%5Ccdots+1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(1\cdots 1)}"/>. If we apply the change of variable <img alt="{y = L_G^{1/2} x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By+%3D+L_G%5E%7B1%2F2%7D+x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y = L_G^{1/2} x}"/>, which is bijective on <img alt="{{\mathbb R}^n_\perp}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+R%7D%5En_%5Cperp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\mathbb R}^n_\perp}"/>, the previous event becomes</p>
<p/><p align="center"><img alt="\displaystyle  \forall y\in {\mathbb R}^n_\perp : \ \ - \epsilon ||y||^2 \leq \sum_e z_e y^T L_{G}^{-1/2} L_e L^{-1/2}_G y \leq \epsilon||y||^2 \ \ \ \ \ (5)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+y%5Cin+%7B%5Cmathbb+R%7D%5En_%5Cperp+%3A+%5C+%5C+-+%5Cepsilon+%7C%7Cy%7C%7C%5E2+%5Cleq+%5Csum_e+z_e+y%5ET+L_%7BG%7D%5E%7B-1%2F2%7D+L_e+L%5E%7B-1%2F2%7D_G+y+%5Cleq+%5Cepsilon%7C%7Cy%7C%7C%5E2+%5C+%5C+%5C+%5C+%5C+%285%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall y\in {\mathbb R}^n_\perp : \ \ - \epsilon ||y||^2 \leq \sum_e z_e y^T L_{G}^{-1/2} L_e L^{-1/2}_G y \leq \epsilon||y||^2 \ \ \ \ \ (5)"/></p>
<p>
which is implied by (and actually equivalent to):</p>
<p>
<a name="eq.ss.spec"/></p><a name="eq.ss.spec">
<p align="center"><img alt="\displaystyle  \left\| \sum_e z_e L_{G}^{-1/2} L_e L^{-1/2}_G \right \| \leq \epsilon \ \ \ \ \ (6)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%5C%7C+%5Csum_e+z_e+L_%7BG%7D%5E%7B-1%2F2%7D+L_e+L%5E%7B-1%2F2%7D_G+%5Cright+%5C%7C+%5Cleq+%5Cepsilon+%5C+%5C+%5C+%5C+%5C+%286%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \left\| \sum_e z_e L_{G}^{-1/2} L_e L^{-1/2}_G \right \| \leq \epsilon \ \ \ \ \ (6)"/></p>
</a><p><a name="eq.ss.spec"/></p>
<p>
Looking at the matrix Bernstein bound, we want to choose the <img alt="{p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e}"/> so that, say,</p>
<p>
<a name="eq.ss.var"/></p><a name="eq.ss.var">
<p align="center"><img alt="\displaystyle   \left\| \sum_e \mathop{\mathbb E} (z_e L_{G}^{-1/2} L_e L^{-1/2}_G )^2\right \| &lt; \frac {\epsilon^2}{4\log n} \ \ \ \ \ (7)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Cleft%5C%7C+%5Csum_e+%5Cmathop%7B%5Cmathbb+E%7D+%28z_e+L_%7BG%7D%5E%7B-1%2F2%7D+L_e+L%5E%7B-1%2F2%7D_G+%29%5E2%5Cright+%5C%7C+%3C+%5Cfrac+%7B%5Cepsilon%5E2%7D%7B4%5Clog+n%7D+%5C+%5C+%5C+%5C+%5C+%287%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle   \left\| \sum_e \mathop{\mathbb E} (z_e L_{G}^{-1/2} L_e L^{-1/2}_G )^2\right \| &lt; \frac {\epsilon^2}{4\log n} \ \ \ \ \ (7)"/></p>
</a><p><a name="eq.ss.var"/> and also so that <a name="eq.ss.max"/></p><a name="eq.ss.max">
<p align="center"><img alt="\displaystyle   \forall e \ \ \ ||z_e L_{G}^{-1/2} L_e L^{-1/2}_G|| &lt; \frac{\epsilon}{4\log n} \ \ \ \ \ (8)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Cforall+e+%5C+%5C+%5C+%7C%7Cz_e+L_%7BG%7D%5E%7B-1%2F2%7D+L_e+L%5E%7B-1%2F2%7D_G%7C%7C+%3C+%5Cfrac%7B%5Cepsilon%7D%7B4%5Clog+n%7D+%5C+%5C+%5C+%5C+%5C+%288%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle   \forall e \ \ \ ||z_e L_{G}^{-1/2} L_e L^{-1/2}_G|| &lt; \frac{\epsilon}{4\log n} \ \ \ \ \ (8)"/></p>
</a><p><a name="eq.ss.max"/> holds with probability 1. </p>
<p>
The variance term <a href="https://lucatrevisan.wordpress.com/feed/#eq.ss.var">(7)</a> is the spectral norm of </p>
<p/><p align="center"><img alt="\displaystyle  \sum_e ( \mathop{\mathbb E} z_e^2 ) \cdot ( L_{G}^{-1/2} L_e L^{-1/2}_G )^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_e+%28+%5Cmathop%7B%5Cmathbb+E%7D+z_e%5E2+%29+%5Ccdot+%28+L_%7BG%7D%5E%7B-1%2F2%7D+L_e+L%5E%7B-1%2F2%7D_G+%29%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_e ( \mathop{\mathbb E} z_e^2 ) \cdot ( L_{G}^{-1/2} L_e L^{-1/2}_G )^2"/></p>
<p align="center"><img alt="\displaystyle  = \sum_e \left( \frac 1 {p_e} - 1 \right) \cdot ( L_{G}^{-1/2} L_e L^{-1/2}_G )^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_e+%5Cleft%28+%5Cfrac+1+%7Bp_e%7D+-+1+%5Cright%29+%5Ccdot+%28+L_%7BG%7D%5E%7B-1%2F2%7D+L_e+L%5E%7B-1%2F2%7D_G+%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \sum_e \left( \frac 1 {p_e} - 1 \right) \cdot ( L_{G}^{-1/2} L_e L^{-1/2}_G )^2 "/></p>
<p align="center"><img alt="\displaystyle  = \sum_e \left( \frac 1 {p_e} - 1 \right) \cdot || L_{G}^{-1/2} L_e L^{-1/2}_G || \cdot L_{G}^{-1/2} L_e L^{-1/2}_G " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_e+%5Cleft%28+%5Cfrac+1+%7Bp_e%7D+-+1+%5Cright%29+%5Ccdot+%7C%7C+L_%7BG%7D%5E%7B-1%2F2%7D+L_e+L%5E%7B-1%2F2%7D_G+%7C%7C+%5Ccdot+L_%7BG%7D%5E%7B-1%2F2%7D+L_e+L%5E%7B-1%2F2%7D_G+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \sum_e \left( \frac 1 {p_e} - 1 \right) \cdot || L_{G}^{-1/2} L_e L^{-1/2}_G || \cdot L_{G}^{-1/2} L_e L^{-1/2}_G "/></p>
<p> where we computed <img alt="{\mathop{\mathbb E} z_e^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D+z_e%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathop{\mathbb E} z_e^2}"/> and we used the fact that if <img alt="{M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M}"/> is a rank-1 real-valued symmetric matrix then <img alt="{M^2 = ||M|| \cdot M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%5E2+%3D+%7C%7CM%7C%7C+%5Ccdot+M%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{M^2 = ||M|| \cdot M}"/>.</p>
<p>
So we have</p>
<p/><p align="center"><img alt="\displaystyle  \sum_e ( \mathop{\mathbb E} z_e^2 ) \cdot ( L_{G}^{-1/2} L_e L^{-1/2}_G )^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_e+%28+%5Cmathop%7B%5Cmathbb+E%7D+z_e%5E2+%29+%5Ccdot+%28+L_%7BG%7D%5E%7B-1%2F2%7D+L_e+L%5E%7B-1%2F2%7D_G+%29%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_e ( \mathop{\mathbb E} z_e^2 ) \cdot ( L_{G}^{-1/2} L_e L^{-1/2}_G )^2"/></p>
<p align="center"><img alt="\displaystyle  \preceq \left( \max_e \left( \frac 1 {p_e} - 1 \right) \cdot || L_{G}^{-1/2} L_e L^{-1/2}_G || \right) \cdot \sum_e L_{G}^{-1/2} L_e L^{-1/2}_G " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cpreceq+%5Cleft%28+%5Cmax_e+%5Cleft%28+%5Cfrac+1+%7Bp_e%7D+-+1+%5Cright%29+%5Ccdot+%7C%7C+L_%7BG%7D%5E%7B-1%2F2%7D+L_e+L%5E%7B-1%2F2%7D_G+%7C%7C+%5Cright%29+%5Ccdot+%5Csum_e+L_%7BG%7D%5E%7B-1%2F2%7D+L_e+L%5E%7B-1%2F2%7D_G+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \preceq \left( \max_e \left( \frac 1 {p_e} - 1 \right) \cdot || L_{G}^{-1/2} L_e L^{-1/2}_G || \right) \cdot \sum_e L_{G}^{-1/2} L_e L^{-1/2}_G "/></p>
<p align="center"><img alt="\displaystyle  \preceq \left( \max_e \left( \frac 1 {p_e} - 1 \right) \cdot || L_{G}^{-1/2} L_e L^{-1/2}_G || \right) \cdot I " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cpreceq+%5Cleft%28+%5Cmax_e+%5Cleft%28+%5Cfrac+1+%7Bp_e%7D+-+1+%5Cright%29+%5Ccdot+%7C%7C+L_%7BG%7D%5E%7B-1%2F2%7D+L_e+L%5E%7B-1%2F2%7D_G+%7C%7C+%5Cright%29+%5Ccdot+I+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \preceq \left( \max_e \left( \frac 1 {p_e} - 1 \right) \cdot || L_{G}^{-1/2} L_e L^{-1/2}_G || \right) \cdot I "/></p>
<p>
If we set </p>
<p align="center"><img alt="\displaystyle  p_e = \min \left\{ 4 \frac{\log n}{\epsilon^2} ||L_{G}^{-1/2} L_e L^{-1/2}_G || , 1 \right\} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++p_e+%3D+%5Cmin+%5Cleft%5C%7B+4+%5Cfrac%7B%5Clog+n%7D%7B%5Cepsilon%5E2%7D+%7C%7CL_%7BG%7D%5E%7B-1%2F2%7D+L_e+L%5E%7B-1%2F2%7D_G+%7C%7C+%2C+1+%5Cright%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  p_e = \min \left\{ 4 \frac{\log n}{\epsilon^2} ||L_{G}^{-1/2} L_e L^{-1/2}_G || , 1 \right\} "/></p>
<p> then we see that we satisfy both <a href="https://lucatrevisan.wordpress.com/feed/#eq.ss.var">(7)</a> and <a href="https://lucatrevisan.wordpress.com/feed/#eq.ss.max">(8)</a>.</p>
<p>
The term <img alt="{ || L_{G}^{-1/2} L_e L^{-1/2}_G ||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+%7C%7C+L_%7BG%7D%5E%7B-1%2F2%7D+L_e+L%5E%7B-1%2F2%7D_G+%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{ || L_{G}^{-1/2} L_e L^{-1/2}_G ||}"/> is the <em>effective resistance</em> of the edge <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> and it is usually denoted as <img alt="{R_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R_e}"/>. It is known that <img alt="{\sum_eR_e = n-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_eR_e+%3D+n-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_eR_e = n-1}"/> for connected graphs. Thus we have </p>
<p align="center"><img alt="\displaystyle  \sum_e p_e = O(\epsilon^{-2} n \log n) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_e+p_e+%3D+O%28%5Cepsilon%5E%7B-2%7D+n+%5Clog+n%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_e p_e = O(\epsilon^{-2} n \log n) "/></p>
<p> as promised.</p>
<p>
</p><p><b>3. Analysing the Construction as a Sub-Gaussian Process </b></p>
<p/><p>
Now we would like to provide an analysis of the Spielman-Srivastava construction in terms of bounding the sup of a sub-Gaussian process via the Talagrand comparison inequality. Indeed we can think of our goal as showing that the following two random variables are both <img alt="{\leq \epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleq+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\leq \epsilon}"/> with high probability:</p>
<p/><p align="center"><img alt="\displaystyle  \sup_{x: x^T L_G x = 1 } \ \ \sum_e z_e x^T L_e x " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%3A+x%5ET+L_G+x+%3D+1+%7D+%5C+%5C+%5Csum_e+z_e+x%5ET+L_e+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sup_{x: x^T L_G x = 1 } \ \ \sum_e z_e x^T L_e x "/></p>
<p align="center"><img alt="\displaystyle  \sup_{x: x^T L_G x = 1 } \ \ \sum_e -z_e x^T L_e x " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%3A+x%5ET+L_G+x+%3D+1+%7D+%5C+%5C+%5Csum_e+-z_e+x%5ET+L_e+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sup_{x: x^T L_G x = 1 } \ \ \sum_e -z_e x^T L_e x "/></p>
<p>
where the <img alt="{z_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z_e}"/> are the random variables defined in the previous section. Unfortunately, while random processes that involve weighted sums of Rademacher random variables are well suited to such an analysis, random processes that involve highly biased Boolean random variables do not work very well in this framework.</p>
<p>
To avoid this problem, we will think of performing the Spielman-Srivastava sampling in phases, such that each phase involves unbiased Boolean random variables.</p>
<p>
To avoid having to deal with too many constants, we will think of setting <img alt="{p_e = \min \{ 1, \epsilon^{-2} R_e \log n \}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e+%3D+%5Cmin+%5C%7B+1%2C+%5Cepsilon%5E%7B-2%7D+R_e+%5Clog+n+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e = \min \{ 1, \epsilon^{-2} R_e \log n \}}"/>, with a goal of achieving sparsification with error <img alt="{O(\epsilon)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Cepsilon%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\epsilon)}"/>. We will want to think of the process of sampling an edge as a sequence of unbiased Boolean choices, so it will be convenient to round up probabilities to non-positive powers of 2. So we will set edge probabilities such that <img alt="{p_e = 2^{-j_e}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e+%3D+2%5E%7B-j_e%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e = 2^{-j_e}}"/> for some integer <img alt="{j_e \geq 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj_e+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j_e \geq 0}"/> and such that </p>
<p align="center"><img alt="\displaystyle  \min \{ 1, \epsilon^{-2} R_e \log n \} \leq p_e \leq \min \{ 1, 2 \epsilon^{-2} R_e \log n \} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin+%5C%7B+1%2C+%5Cepsilon%5E%7B-2%7D+R_e+%5Clog+n+%5C%7D+%5Cleq+p_e+%5Cleq+%5Cmin+%5C%7B+1%2C+2+%5Cepsilon%5E%7B-2%7D+R_e+%5Clog+n+%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \min \{ 1, \epsilon^{-2} R_e \log n \} \leq p_e \leq \min \{ 1, 2 \epsilon^{-2} R_e \log n \} "/></p>
<p>
If we let <img alt="{\min_e p_e = 2^{-\ell}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmin_e+p_e+%3D+2%5E%7B-%5Cell%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\min_e p_e = 2^{-\ell}}"/>, we will think of the process of sampling <img alt="{G'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G'}"/> as proceeding in <img alt="{\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell}"/> rounds. If <img alt="{p_e = 2^{-j_e}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e+%3D+2%5E%7B-j_e%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e = 2^{-j_e}}"/> then, in each of the last <img alt="{j_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j_e}"/> round (that is, in round <img alt="{\ell -j_e +1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell+-j_e+%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell -j_e +1}"/> through <img alt="{\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell}"/>), we choose with probability 1/2 to delete the edge <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> and with probability 1/2 to double its weight.</p>
<p>
(Why do we do it in the last <img alt="{j_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j_e}"/> rounds and not in the first <img alt="{j_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j_e}"/> rounds? This issue confused me a lot at some point. Hold on to this question until later.)</p>
<p>
Let us call <img alt="{G^{(k)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%5E%7B%28k%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G^{(k)}}"/> the graph obtained after round <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>, so that <img alt="{G^{(0)} = G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%5E%7B%280%29%7D+%3D+G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G^{(0)} = G}"/> and <img alt="{G^{(\ell)} = G'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%5E%7B%28%5Cell%29%7D+%3D+G%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G^{(\ell)} = G'}"/>. We see that </p>
<p align="center"><img alt="\displaystyle  \sup_{x: x^T L_G x = 1 } \ \ \left | \sum_e z_e x^T L_G x \right| \leq \sum_{k=1}^\ell \sup_{x: x^T L_G x = 1 } \ \ \left | x^T (L_{G^{(k)}} - L_{G^{(k-1)}} ) x \right| " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%3A+x%5ET+L_G+x+%3D+1+%7D+%5C+%5C+%5Cleft+%7C+%5Csum_e+z_e+x%5ET+L_G+x+%5Cright%7C+%5Cleq+%5Csum_%7Bk%3D1%7D%5E%5Cell+%5Csup_%7Bx%3A+x%5ET+L_G+x+%3D+1+%7D+%5C+%5C+%5Cleft+%7C+x%5ET+%28L_%7BG%5E%7B%28k%29%7D%7D+-+L_%7BG%5E%7B%28k-1%29%7D%7D+%29+x+%5Cright%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sup_{x: x^T L_G x = 1 } \ \ \left | \sum_e z_e x^T L_G x \right| \leq \sum_{k=1}^\ell \sup_{x: x^T L_G x = 1 } \ \ \left | x^T (L_{G^{(k)}} - L_{G^{(k-1)}} ) x \right| "/></p>
<p>
Let us now understand the quadratic form in each of the above term. If we let <img alt="{w^{(k)}_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%5E%7B%28k%29%7D_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w^{(k)}_e}"/> be the weight of edge <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> in graph <img alt="{G^{(k)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%5E%7B%28k%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G^{(k)}}"/>, we have</p>
<p/><p align="center"><img alt="\displaystyle  x^T (L_{G^{(k)}} - L_{G^{(k-1)}} ) x = \sum_e (w^{(k)}_e - w^{(k-1)}_e ) \ x^T L_e x " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5ET+%28L_%7BG%5E%7B%28k%29%7D%7D+-+L_%7BG%5E%7B%28k-1%29%7D%7D+%29+x+%3D+%5Csum_e+%28w%5E%7B%28k%29%7D_e+-+w%5E%7B%28k-1%29%7D_e+%29+%5C+x%5ET+L_e+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x^T (L_{G^{(k)}} - L_{G^{(k-1)}} ) x = \sum_e (w^{(k)}_e - w^{(k-1)}_e ) \ x^T L_e x "/></p>
<p> Regarding the weights, if we consider an edge <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> such that <img alt="{p_e = 2^{-j_e}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_e+%3D+2%5E%7B-j_e%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_e = 2^{-j_e}}"/>, we have that the edge <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> is left untouched in round <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> if <img alt="{\ell - j_e +1 &gt; k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell+-+j_e+%2B1+%3E+k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell - j_e +1 &gt; k}"/>, that is, if <img alt="{j_e \leq \ell -k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj_e+%5Cleq+%5Cell+-k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j_e \leq \ell -k}"/>. In that case, <img alt="{w^{(k)}_e - w^{(k-1)}_e=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%5E%7B%28k%29%7D_e+-+w%5E%7B%28k-1%29%7D_e%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w^{(k)}_e - w^{(k-1)}_e=0}"/>. If <img alt="{j_e \geq \ell -k+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj_e+%5Cgeq+%5Cell+-k%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j_e \geq \ell -k+1}"/>, then <img alt="{w^{(k)}_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%5E%7B%28k%29%7D_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w^{(k)}_e}"/> is equally likely to be <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> or <img alt="{ 2w^{(k-1)}_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+2w%5E%7B%28k-1%29%7D_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{ 2w^{(k-1)}_e}"/>. In other words, <img alt="{w^{(k)}_e - w^{(k-1)}_e = r_e^{(k)} w^{(k-1)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%5E%7B%28k%29%7D_e+-+w%5E%7B%28k-1%29%7D_e+%3D+r_e%5E%7B%28k%29%7D+w%5E%7B%28k-1%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w^{(k)}_e - w^{(k-1)}_e = r_e^{(k)} w^{(k-1)}}"/> where <img alt="{r_e^{(k)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br_e%5E%7B%28k%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r_e^{(k)}}"/> is a Rademacher random variable. </p>
<p>
Putting everything together, we have</p>
<p/><p align="center"><img alt="\displaystyle  x^T (L_{G^{(k)}} - L_{G^{(k-1)}} ) x = \sum_{e : p_e \leq 2^{k-1-\ell}} \ r_e^{(k)} w^{(k-1)} \ x^T L_e x " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5ET+%28L_%7BG%5E%7B%28k%29%7D%7D+-+L_%7BG%5E%7B%28k-1%29%7D%7D+%29+x+%3D+%5Csum_%7Be+%3A+p_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D%7D+%5C+r_e%5E%7B%28k%29%7D+w%5E%7B%28k-1%29%7D+%5C+x%5ET+L_e+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x^T (L_{G^{(k)}} - L_{G^{(k-1)}} ) x = \sum_{e : p_e \leq 2^{k-1-\ell}} \ r_e^{(k)} w^{(k-1)} \ x^T L_e x "/></p>
<p> and now we are in good shape because Rademacher sums are well suited to be analyzed as sub-Gaussian processes. In particular, we will able to prove the following lemma.</p>
<blockquote><p><b>Lemma 1</b> <em> There are bounds <img alt="{\epsilon_k = O(\epsilon \cdot 2^{(k-\ell)/2})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_k+%3D+O%28%5Cepsilon+%5Ccdot+2%5E%7B%28k-%5Cell%29%2F2%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon_k = O(\epsilon \cdot 2^{(k-\ell)/2})}"/> such that for every outcome of the first <img alt="{k-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k-1}"/> steps that satisfies <img alt="{L_{G^{(k-1)}} \preceq 2 L_G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7BG%5E%7B%28k-1%29%7D%7D+%5Cpreceq+2+L_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{G^{(k-1)}} \preceq 2 L_G}"/>, we have </em></p><em>
<p align="center"><img alt="\displaystyle  \Pr \left[ \sup_{x: x^T L_G x = 1 } \ \ \left | x^T (L_{G^{(k)}} - L_{G^{(k-1)}} ) x \right| &gt; \epsilon_k \right] \leq \frac 1n " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPr+%5Cleft%5B+%5Csup_%7Bx%3A+x%5ET+L_G+x+%3D+1+%7D+%5C+%5C+%5Cleft+%7C+x%5ET+%28L_%7BG%5E%7B%28k%29%7D%7D+-+L_%7BG%5E%7B%28k-1%29%7D%7D+%29+x+%5Cright%7C+%3E+%5Cepsilon_k+%5Cright%5D+%5Cleq+%5Cfrac+1n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \Pr \left[ \sup_{x: x^T L_G x = 1 } \ \ \left | x^T (L_{G^{(k)}} - L_{G^{(k-1)}} ) x \right| &gt; \epsilon_k \right] \leq \frac 1n "/></p>
</em><p><em> </em></p></blockquote>
<p/><p>
Applying the lemma inductively, we see that we have probability at least <img alt="{1-\ell/n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1-%5Cell%2Fn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1-\ell/n}"/> that</p>
<p/><p align="center"><img alt="\displaystyle  \sup_{x: x^T L_G x = 1 } \ \ \left | \sum_e z_e x^T L_G x \right| \leq \sum_{k=1}^\ell \epsilon_k \leq O(\epsilon) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%3A+x%5ET+L_G+x+%3D+1+%7D+%5C+%5C+%5Cleft+%7C+%5Csum_e+z_e+x%5ET+L_G+x+%5Cright%7C+%5Cleq+%5Csum_%7Bk%3D1%7D%5E%5Cell+%5Cepsilon_k+%5Cleq+O%28%5Cepsilon%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sup_{x: x^T L_G x = 1 } \ \ \left | \sum_e z_e x^T L_G x \right| \leq \sum_{k=1}^\ell \epsilon_k \leq O(\epsilon) "/></p>
<p> as desired, and it remains to observe that in every undirected graph every edge has effective resistance at least <img alt="{1/n^{O(1)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2Fn%5E%7BO%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/n^{O(1)}}"/> so that <img alt="{\ell = O(\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell+%3D+O%28%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell = O(\log n)}"/>.</p>
<p>
It will be convenient to do a change of variables and write</p>
<p/><p align="center"><img alt="\displaystyle  \sup_{x: x^T L_G x = 1 } \ \ \left | x^T (L_{G^{(k)}} - L_{G^{(k-1)}} ) x \right| " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%3A+x%5ET+L_G+x+%3D+1+%7D+%5C+%5C+%5Cleft+%7C+x%5ET+%28L_%7BG%5E%7B%28k%29%7D%7D+-+L_%7BG%5E%7B%28k-1%29%7D%7D+%29+x+%5Cright%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sup_{x: x^T L_G x = 1 } \ \ \left | x^T (L_{G^{(k)}} - L_{G^{(k-1)}} ) x \right| "/></p>
<p align="center"><img alt="\displaystyle  = \sup_{x: x^T L_G x = 1 } \left | \sum_{e : p_e \leq 2^{k-1-\ell}} \ r_e^{(k)} w^{(k-1)} \ x^T L_e x \right | " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csup_%7Bx%3A+x%5ET+L_G+x+%3D+1+%7D+%5Cleft+%7C+%5Csum_%7Be+%3A+p_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D%7D+%5C+r_e%5E%7B%28k%29%7D+w%5E%7B%28k-1%29%7D+%5C+x%5ET+L_e+x+%5Cright+%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \sup_{x: x^T L_G x = 1 } \left | \sum_{e : p_e \leq 2^{k-1-\ell}} \ r_e^{(k)} w^{(k-1)} \ x^T L_e x \right | "/></p>
<p> <a name="eq.termk"/></p><a name="eq.termk">
<p align="center"><img alt="\displaystyle   = \sup_{y: ||y||=1 } \left | \sum_{e : p_e \leq 2^{k-1-\ell}} \ r_e^{(k)} w^{(k-1)} \ x^T L_G^{-1/2} L_e L_G^{-1/2} x \right | \ \ \ \ \ (9)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%3D+%5Csup_%7By%3A+%7C%7Cy%7C%7C%3D1+%7D+%5Cleft+%7C+%5Csum_%7Be+%3A+p_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D%7D+%5C+r_e%5E%7B%28k%29%7D+w%5E%7B%28k-1%29%7D+%5C+x%5ET+L_G%5E%7B-1%2F2%7D+L_e+L_G%5E%7B-1%2F2%7D+x+%5Cright+%7C+%5C+%5C+%5C+%5C+%5C+%289%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle   = \sup_{y: ||y||=1 } \left | \sum_{e : p_e \leq 2^{k-1-\ell}} \ r_e^{(k)} w^{(k-1)} \ x^T L_G^{-1/2} L_e L_G^{-1/2} x \right | \ \ \ \ \ (9)"/></p>
</a><p><a name="eq.termk"/> To lighten up the notation in our subsequent arguments, it will be convenient to give names to the matrices that we obtain after this change of basis, and call </p>
<p align="center"><img alt="\displaystyle  M_e := L_G^{-1/2} L_e L_G^{-1/2} \ \ \ \ \ (10)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++M_e+%3A%3D+L_G%5E%7B-1%2F2%7D+L_e+L_G%5E%7B-1%2F2%7D+%5C+%5C+%5C+%5C+%5C+%2810%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  M_e := L_G^{-1/2} L_e L_G^{-1/2} \ \ \ \ \ (10)"/></p>
<p/><p align="center"><img alt="\displaystyle  M := L_G^{-1/2} L_G L_G^{-1/2} = \sum_e M_e \preceq I \ \ \ \ \ (11)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++M+%3A%3D+L_G%5E%7B-1%2F2%7D+L_G+L_G%5E%7B-1%2F2%7D+%3D+%5Csum_e+M_e+%5Cpreceq+I+%5C+%5C+%5C+%5C+%5C+%2811%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  M := L_G^{-1/2} L_G L_G^{-1/2} = \sum_e M_e \preceq I \ \ \ \ \ (11)"/></p>
<p/><p align="center"><img alt="\displaystyle  M^{(k)} := L_G^{-1/2} L_{G^{(k)}} L_G^{-1/2}= \sum_e w^{(k)} _e M_e \ \ \ \ \ (12)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++M%5E%7B%28k%29%7D+%3A%3D+L_G%5E%7B-1%2F2%7D+L_%7BG%5E%7B%28k%29%7D%7D+L_G%5E%7B-1%2F2%7D%3D+%5Csum_e+w%5E%7B%28k%29%7D+_e+M_e+%5C+%5C+%5C+%5C+%5C+%2812%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  M^{(k)} := L_G^{-1/2} L_{G^{(k)}} L_G^{-1/2}= \sum_e w^{(k)} _e M_e \ \ \ \ \ (12)"/></p>
<p> In this notation, we have </p>
<p align="center"><img alt="\displaystyle  \min \{ 1, \epsilon^{-2} ||M_e|| \log n \} \leq p_e \leq \min \{ 1, 2 \epsilon^{-2} ||M_e|| \log n \} \ \ \ \ \ (13)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin+%5C%7B+1%2C+%5Cepsilon%5E%7B-2%7D+%7C%7CM_e%7C%7C+%5Clog+n+%5C%7D+%5Cleq+p_e+%5Cleq+%5Cmin+%5C%7B+1%2C+2+%5Cepsilon%5E%7B-2%7D+%7C%7CM_e%7C%7C+%5Clog+n+%5C%7D+%5C+%5C+%5C+%5C+%5C+%2813%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \min \{ 1, \epsilon^{-2} ||M_e|| \log n \} \leq p_e \leq \min \{ 1, 2 \epsilon^{-2} ||M_e|| \log n \} \ \ \ \ \ (13)"/></p>
<p> and recall that we assumed </p>
<p align="center"><img alt="\displaystyle  M^{(k-1)} \preceq 2 M \preceq 2I \ \ \ \ \ (14)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++M%5E%7B%28k-1%29%7D+%5Cpreceq+2+M+%5Cpreceq+2I+%5C+%5C+%5C+%5C+%5C+%2814%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  M^{(k-1)} \preceq 2 M \preceq 2I \ \ \ \ \ (14)"/></p>
<p>
With this notation, the quantity in <a href="https://lucatrevisan.wordpress.com/feed/#eq.termk">(9)</a> that we want to bound becomes </p>
<p align="center"><img alt="\displaystyle  \sup_{y: ||y||=1 } | F(y) | " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7By%3A+%7C%7Cy%7C%7C%3D1+%7D+%7C+F%28y%29+%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sup_{y: ||y||=1 } | F(y) | "/></p>
<p> where </p>
<p align="center"><img alt="\displaystyle  F(y) = \sum_{e : p_e \leq 2^{k-1-\ell}} \ r_e^{(k)} w^{(k-1)}_e \ y^T M_e y " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28y%29+%3D+%5Csum_%7Be+%3A+p_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D%7D+%5C+r_e%5E%7B%28k%29%7D+w%5E%7B%28k-1%29%7D_e+%5C+y%5ET+M_e+y+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F(y) = \sum_{e : p_e \leq 2^{k-1-\ell}} \ r_e^{(k)} w^{(k-1)}_e \ y^T M_e y "/></p>
<p> is a centered random process.</p>
<p>
Define the centered Gaussian process </p>
<p align="center"><img alt="\displaystyle  \hat F(y) := \sum_{e : p_e \leq 2^{k-1-\ell}} \ g_e^{(k)} w^{(k-1)}_e \ y^T M_e y " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Chat+F%28y%29+%3A%3D+%5Csum_%7Be+%3A+p_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D%7D+%5C+g_e%5E%7B%28k%29%7D+w%5E%7B%28k-1%29%7D_e+%5C+y%5ET+M_e+y+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \hat F(y) := \sum_{e : p_e \leq 2^{k-1-\ell}} \ g_e^{(k)} w^{(k-1)}_e \ y^T M_e y "/></p>
<p> where <img alt="{g_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g_e}"/> are independent standard normal random variables. Then we immediately see that both <img alt="{-F(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-F%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-F(\cdot)}"/> and <img alt="{F(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(\cdot)}"/> are <img alt="{O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(1)}"/>-dominated by <img alt="{\hat F(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Chat+F%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\hat F(\cdot)}"/>, because </p>
<p align="center"><img alt="\displaystyle  || F(x) - F(y) ||^2_{\Psi_2} = O(1) \cdot \sum_{e : p_e \leq 2^{k-1-\ell}} \left( w^{(k-1)} _e\ x^T M_e x \right)^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+F%28x%29+-+F%28y%29+%7C%7C%5E2_%7B%5CPsi_2%7D+%3D+O%281%29+%5Ccdot+%5Csum_%7Be+%3A+p_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D%7D+%5Cleft%28+w%5E%7B%28k-1%29%7D+_e%5C+x%5ET+M_e+x+%5Cright%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  || F(x) - F(y) ||^2_{\Psi_2} = O(1) \cdot \sum_{e : p_e \leq 2^{k-1-\ell}} \left( w^{(k-1)} _e\ x^T M_e x \right)^2 "/></p>
<p> and </p>
<p align="center"><img alt="\displaystyle  (d(x,y))^2 = \mathop{\mathbb E} (\hat F(x) - \hat F(y) )^2 = \sum_{e : p_e \leq 2^{k-1-\ell}} \left( w^{(k-1)}_e \ x^T M_e x \right)^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28d%28x%2Cy%29%29%5E2+%3D+%5Cmathop%7B%5Cmathbb+E%7D+%28%5Chat+F%28x%29+-+%5Chat+F%28y%29+%29%5E2+%3D+%5Csum_%7Be+%3A+p_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D%7D+%5Cleft%28+w%5E%7B%28k-1%29%7D_e+%5C+x%5ET+M_e+x+%5Cright%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  (d(x,y))^2 = \mathop{\mathbb E} (\hat F(x) - \hat F(y) )^2 = \sum_{e : p_e \leq 2^{k-1-\ell}} \left( w^{(k-1)}_e \ x^T M_e x \right)^2 "/></p>
<p>
In order to deduce the lemma from the Talagrand comparison inequality it suffices to show that</p>
<p/><p align="center"><img alt="\displaystyle  \mathop{\mathbb E} \sup_{x: ||x||=1} \hat F(x) \leq O(\epsilon_k) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bx%3A+%7C%7Cx%7C%7C%3D1%7D+%5Chat+F%28x%29+%5Cleq+O%28%5Cepsilon_k%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E} \sup_{x: ||x||=1} \hat F(x) \leq O(\epsilon_k) "/></p>
<p> and to have a bound on the diameter </p>
<p align="center"><img alt="\displaystyle  \sup_{x,y: ||x||=||y||=1} d(x,y) \leq O \left( \frac {\epsilon_k} {\sqrt {\log n}} \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%2Cy%3A+%7C%7Cx%7C%7C%3D%7C%7Cy%7C%7C%3D1%7D+d%28x%2Cy%29+%5Cleq+O+%5Cleft%28+%5Cfrac+%7B%5Cepsilon_k%7D+%7B%5Csqrt+%7B%5Clog+n%7D%7D+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sup_{x,y: ||x||=||y||=1} d(x,y) \leq O \left( \frac {\epsilon_k} {\sqrt {\log n}} \right) "/></p>
<p>
To bound the average supremum of the Gaussian process we could use generic chaining, but fortunately there is a a matrix Chernoff bound that say that if <img alt="{\{ A_e\}_{e\in E}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+A_e%5C%7D_%7Be%5Cin+E%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ A_e\}_{e\in E}}"/> are real-valued symmetric matrices and <img alt="{g_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g_e}"/> are independent standard normal random variables then </p>
<p/><p align="center"><img alt="\displaystyle  \mathop{\mathbb E} \sup _{x : ||x||=1} \sum_e g_e x^T A_e x \leq \sqrt{2\left \| \sum_e A_e^2 \right \| \log 2n} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Csup+_%7Bx+%3A+%7C%7Cx%7C%7C%3D1%7D+%5Csum_e+g_e+x%5ET+A_e+x+%5Cleq+%5Csqrt%7B2%5Cleft+%5C%7C+%5Csum_e+A_e%5E2+%5Cright+%5C%7C+%5Clog+2n%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E} \sup _{x : ||x||=1} \sum_e g_e x^T A_e x \leq \sqrt{2\left \| \sum_e A_e^2 \right \| \log 2n} "/></p>
<p>
Applied to our setting,</p>
<p/><p align="center"><img alt="\displaystyle  \mathop{\mathbb E} \sup_{x: ||x||=1} \hat F(x) \leq O(\sqrt {\log n}) \cdot \sqrt{\left \| \sum_{e : p_e \leq 2^{k-1-\ell}} \left( w^{(k-1)}_e \ M_e \right)^2 \right \|} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bx%3A+%7C%7Cx%7C%7C%3D1%7D+%5Chat+F%28x%29+%5Cleq+O%28%5Csqrt+%7B%5Clog+n%7D%29+%5Ccdot+%5Csqrt%7B%5Cleft+%5C%7C+%5Csum_%7Be+%3A+p_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D%7D+%5Cleft%28+w%5E%7B%28k-1%29%7D_e+%5C+M_e+%5Cright%29%5E2+%5Cright+%5C%7C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E} \sup_{x: ||x||=1} \hat F(x) \leq O(\sqrt {\log n}) \cdot \sqrt{\left \| \sum_{e : p_e \leq 2^{k-1-\ell}} \left( w^{(k-1)}_e \ M_e \right)^2 \right \|} "/></p>
<p> where </p>
<p align="center"><img alt="\displaystyle  \sum_{e : p_e \leq 2^{k-1-\ell}} \left( w^{(k-1)}_e \ M_e \right)^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Be+%3A+p_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D%7D+%5Cleft%28+w%5E%7B%28k-1%29%7D_e+%5C+M_e+%5Cright%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{e : p_e \leq 2^{k-1-\ell}} \left( w^{(k-1)}_e \ M_e \right)^2 "/></p>
<p align="center"><img alt="\displaystyle  \preceq \max_{e : p_e \leq 2^{k-1-\ell}} w^{(k-1)}_e \| M_e \| \cdot \sum_{e : p_e \leq 2^{k-1-\ell}} w^{(k-1)}_e \ M_e " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cpreceq+%5Cmax_%7Be+%3A+p_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D%7D+w%5E%7B%28k-1%29%7D_e+%5C%7C+M_e+%5C%7C+%5Ccdot+%5Csum_%7Be+%3A+p_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D%7D+w%5E%7B%28k-1%29%7D_e+%5C+M_e+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \preceq \max_{e : p_e \leq 2^{k-1-\ell}} w^{(k-1)}_e \| M_e \| \cdot \sum_{e : p_e \leq 2^{k-1-\ell}} w^{(k-1)}_e \ M_e "/></p>
<p align="center"><img alt="\displaystyle  \preceq 2^{k-1-\ell} \frac 1 {p_e} \| M_e \| \cdot M^{(k-1)} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cpreceq+2%5E%7Bk-1-%5Cell%7D+%5Cfrac+1+%7Bp_e%7D+%5C%7C+M_e+%5C%7C+%5Ccdot+M%5E%7B%28k-1%29%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \preceq 2^{k-1-\ell} \frac 1 {p_e} \| M_e \| \cdot M^{(k-1)} "/></p>
<p align="center"><img alt="\displaystyle  \preceq 2^{k-1-\ell} \cdot \frac{\epsilon^2}{||M_e|| \log n} \cdot ||M_e|| \cdot M^{(k-1)} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cpreceq+2%5E%7Bk-1-%5Cell%7D+%5Ccdot+%5Cfrac%7B%5Cepsilon%5E2%7D%7B%7C%7CM_e%7C%7C+%5Clog+n%7D+%5Ccdot+%7C%7CM_e%7C%7C+%5Ccdot+M%5E%7B%28k-1%29%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \preceq 2^{k-1-\ell} \cdot \frac{\epsilon^2}{||M_e|| \log n} \cdot ||M_e|| \cdot M^{(k-1)} "/></p>
<p align="center"><img alt="\displaystyle  \preceq2^{k-1-\ell} \cdot \frac{\epsilon^2}{\log n} \cdot 2 \ I " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cpreceq2%5E%7Bk-1-%5Cell%7D+%5Ccdot+%5Cfrac%7B%5Cepsilon%5E2%7D%7B%5Clog+n%7D+%5Ccdot+2+%5C+I+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \preceq2^{k-1-\ell} \cdot \frac{\epsilon^2}{\log n} \cdot 2 \ I "/></p>
<p> so indeed </p>
<p align="center"><img alt="\displaystyle  \mathop{\mathbb E} \sup_{x: ||x||=1} \hat F(x) \leq O(\epsilon_k) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bx%3A+%7C%7Cx%7C%7C%3D1%7D+%5Chat+F%28x%29+%5Cleq+O%28%5Cepsilon_k%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E} \sup_{x: ||x||=1} \hat F(x) \leq O(\epsilon_k) "/></p>
<p>
Now we can return to the question about the order in which we process the edges. By starting from the lowest-probability edges, we are also starting from the edges for which <img alt="{||M_e||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7CM_e%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{||M_e||}"/> is smallest. When we bound </p>
<p align="center"><img alt="\displaystyle  || \sum_e (w_e^{(k)} M_e)^2 || \leq \left( \max_e w_e^{(k)} ||M_e|| \right) \cdot ||\sum_e w_e^{(k)} M_e|| " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+%5Csum_e+%28w_e%5E%7B%28k%29%7D+M_e%29%5E2+%7C%7C+%5Cleq+%5Cleft%28+%5Cmax_e+w_e%5E%7B%28k%29%7D+%7C%7CM_e%7C%7C+%5Cright%29+%5Ccdot+%7C%7C%5Csum_e+w_e%5E%7B%28k%29%7D+M_e%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  || \sum_e (w_e^{(k)} M_e)^2 || \leq \left( \max_e w_e^{(k)} ||M_e|| \right) \cdot ||\sum_e w_e^{(k)} M_e|| "/></p>
<p> and the sum is over the edges <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> that are processed at round <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>, it is convenient to be able to say that <img alt="{w_e^{(k)} ||M_e||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_e%5E%7B%28k%29%7D+%7C%7CM_e%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w_e^{(k)} ||M_e||}"/> has a round-dependent upper bound. Indeed, if <img alt="{e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e}"/> is processed at round <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>, then <img alt="{w_e^{(k)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_e%5E%7B%28k%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w_e^{(k)}}"/> is either zero or <img alt="{2^{k-1-\ell} /p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bk-1-%5Cell%7D+%2Fp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{k-1-\ell} /p_e}"/>, so that <img alt="{w_e^{(k)} ||M_e|| \leq 2^{k-1-\ell} \cdot ||M_e||/p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_e%5E%7B%28k%29%7D+%7C%7CM_e%7C%7C+%5Cleq+2%5E%7Bk-1-%5Cell%7D+%5Ccdot+%7C%7CM_e%7C%7C%2Fp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w_e^{(k)} ||M_e|| \leq 2^{k-1-\ell} \cdot ||M_e||/p_e}"/>, and the terms <img alt="{2^{k-1-\ell}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bk-1-%5Cell%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{k-1-\ell}}"/> add up to <img alt="{O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(1)}"/> when summed over rounds. If we had proceeded in the opposite direction, we would have only been able to bound <img alt="{w_e^{(k)} ||M_e||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_e%5E%7B%28k%29%7D+%7C%7CM_e%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w_e^{(k)} ||M_e||}"/> as an absolute constant times <img alt="{||M_e||/p_e}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7CM_e%7C%7C%2Fp_e%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{||M_e||/p_e}"/>, and we would have lost a factor of the order of the number of rounds in the analysis.</p>
<p>
The diameter is bounded by analogous considerations. The square of the diameter </p>
<p/><p align="center"><img alt="\displaystyle  \sup_{x,y : ||x||=||y||=1} (d(x,y))^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%2Cy+%3A+%7C%7Cx%7C%7C%3D%7C%7Cy%7C%7C%3D1%7D+%28d%28x%2Cy%29%29%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sup_{x,y : ||x||=||y||=1} (d(x,y))^2"/></p>
<p align="center"><img alt="\displaystyle  = \sum_{e: p_e \leq 2^{k-1-\ell}} \left(w^{(k-1)}_e \right)^2 \left( x^T M_e x - y^TM_ey \right)^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_%7Be%3A+p_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D%7D+%5Cleft%28w%5E%7B%28k-1%29%7D_e+%5Cright%29%5E2+%5Cleft%28+x%5ET+M_e+x+-+y%5ETM_ey+%5Cright%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \sum_{e: p_e \leq 2^{k-1-\ell}} \left(w^{(k-1)}_e \right)^2 \left( x^T M_e x - y^TM_ey \right)^2 "/></p>
<p align="center"><img alt="\displaystyle  \leq \left( \sup_{x,y,e : ||u||=||v||=1,p_e \leq 2^{k-1-\ell} } w^{(k-1)}_e | x^T M_e x - y^TM_ey| \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+%5Cleft%28+%5Csup_%7Bx%2Cy%2Ce+%3A+%7C%7Cu%7C%7C%3D%7C%7Cv%7C%7C%3D1%2Cp_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D+%7D+w%5E%7B%28k-1%29%7D_e+%7C+x%5ET+M_e+x+-+y%5ETM_ey%7C+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \leq \left( \sup_{x,y,e : ||u||=||v||=1,p_e \leq 2^{k-1-\ell} } w^{(k-1)}_e | x^T M_e x - y^TM_ey| \right) "/></p>
<p align="center"><img alt="\displaystyle \cdot \left( \sup_{x,y : ||x||=||y||=1} \sum_{e: p_e \leq 2^{k-1-\ell}} w^{(k-1)}_e | x^T M_e x - y^TM_ey | \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Ccdot+%5Cleft%28+%5Csup_%7Bx%2Cy+%3A+%7C%7Cx%7C%7C%3D%7C%7Cy%7C%7C%3D1%7D+%5Csum_%7Be%3A+p_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D%7D+w%5E%7B%28k-1%29%7D_e+%7C+x%5ET+M_e+x+-+y%5ETM_ey+%7C+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \cdot \left( \sup_{x,y : ||x||=||y||=1} \sum_{e: p_e \leq 2^{k-1-\ell}} w^{(k-1)}_e | x^T M_e x - y^TM_ey | \right) "/></p>
<p> and we have </p>
<p align="center"><img alt="\displaystyle  \sup_{x,y,e : ||u||=||v||=1,p_e \leq 2^{k-1-\ell} } w^{(k-1)}_e | x^T M_e x - y^TM_ey | " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%2Cy%2Ce+%3A+%7C%7Cu%7C%7C%3D%7C%7Cv%7C%7C%3D1%2Cp_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D+%7D+w%5E%7B%28k-1%29%7D_e+%7C+x%5ET+M_e+x+-+y%5ETM_ey+%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sup_{x,y,e : ||u||=||v||=1,p_e \leq 2^{k-1-\ell} } w^{(k-1)}_e | x^T M_e x - y^TM_ey | "/></p>
<p align="center"><img alt="\displaystyle  \leq 2^{k-\ell-1} \frac{\epsilon^2}{||M_e|| \log n} 2 ||M_e|| " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+2%5E%7Bk-%5Cell-1%7D+%5Cfrac%7B%5Cepsilon%5E2%7D%7B%7C%7CM_e%7C%7C+%5Clog+n%7D+2+%7C%7CM_e%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \leq 2^{k-\ell-1} \frac{\epsilon^2}{||M_e|| \log n} 2 ||M_e|| "/></p>
<p align="center"><img alt="\displaystyle  \leq O\left ( \frac {\epsilon_k^2}{\log n} \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+O%5Cleft+%28+%5Cfrac+%7B%5Cepsilon_k%5E2%7D%7B%5Clog+n%7D+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \leq O\left ( \frac {\epsilon_k^2}{\log n} \right) "/></p>
<p> and </p>
<p align="center"><img alt="\displaystyle  \sup_{x,y : ||x||=||y||=1} \sum_{e: p_e \leq 2^{k-1-\ell}} w^{(k-1)}_e | x^T M_e x - y^TM_ey |" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%2Cy+%3A+%7C%7Cx%7C%7C%3D%7C%7Cy%7C%7C%3D1%7D+%5Csum_%7Be%3A+p_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D%7D+w%5E%7B%28k-1%29%7D_e+%7C+x%5ET+M_e+x+-+y%5ETM_ey+%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sup_{x,y : ||x||=||y||=1} \sum_{e: p_e \leq 2^{k-1-\ell}} w^{(k-1)}_e | x^T M_e x - y^TM_ey |"/></p>
<p align="center"><img alt="\displaystyle  \sup_{x,y : ||x||=||y||=1} \sum_{e: p_e \leq 2^{k-1-\ell}} w^{(k-1)}_e (x^T M_e x + y^TM_ey ) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%2Cy+%3A+%7C%7Cx%7C%7C%3D%7C%7Cy%7C%7C%3D1%7D+%5Csum_%7Be%3A+p_e+%5Cleq+2%5E%7Bk-1-%5Cell%7D%7D+w%5E%7B%28k-1%29%7D_e+%28x%5ET+M_e+x+%2B+y%5ETM_ey+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sup_{x,y : ||x||=||y||=1} \sum_{e: p_e \leq 2^{k-1-\ell}} w^{(k-1)}_e (x^T M_e x + y^TM_ey ) "/></p>
<p align="center"><img alt="\displaystyle  \leq \sup_{x,y: ||x||=||y||=1} x^T M^{(k-1)} x + y^T M^{(k-1)} y " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+%5Csup_%7Bx%2Cy%3A+%7C%7Cx%7C%7C%3D%7C%7Cy%7C%7C%3D1%7D+x%5ET+M%5E%7B%28k-1%29%7D+x+%2B+y%5ET+M%5E%7B%28k-1%29%7D+y+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \leq \sup_{x,y: ||x||=||y||=1} x^T M^{(k-1)} x + y^T M^{(k-1)} y "/></p>
<p align="center"><img alt="\displaystyle  \leq 4 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+4+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \leq 4 "/></p>
<p>
Which gives the desired bound </p>
<p align="center"><img alt="\displaystyle  \sup_{x,y : ||x||=||y||=1} d(x,y) = O(\epsilon_k / \sqrt{\log n} )" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx%2Cy+%3A+%7C%7Cx%7C%7C%3D%7C%7Cy%7C%7C%3D1%7D+d%28x%2Cy%29+%3D+O%28%5Cepsilon_k+%2F+%5Csqrt%7B%5Clog+n%7D+%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sup_{x,y : ||x||=||y||=1} d(x,y) = O(\epsilon_k / \sqrt{\log n} )"/></p>
<p>
Now the Talagrand comparison inequality gives us the lemma, and hence the analysis of the Spielman-Srivastava construction.</p>
<p>
</p><p><b>4. A Final Comment </b></p>
<p/><p>
There was a point that confused me for a while about this argument. Namely, we are not able to study </p>
<p/><p align="center"><img alt="\displaystyle  \sup_{x : x^T L_G x=1} x^T(L_G - L_{G'}) x " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csup_%7Bx+%3A+x%5ET+L_G+x%3D1%7D+x%5ET%28L_G+-+L_%7BG%27%7D%29+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sup_{x : x^T L_G x=1} x^T(L_G - L_{G'}) x "/></p>
<p> by showing that </p>
<p align="center"><img alt="\displaystyle  x^T(L_G - L_{G'}) x " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5ET%28L_G+-+L_%7BG%27%7D%29+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x^T(L_G - L_{G'}) x "/></p>
<p> is dominated by a Gaussian process, because <img alt="{L_{G'}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7BG%27%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{G'}}"/> involves biased Boolean random variables which yield poor bounds when we try to dominate them by a Gaussian distribution. Instead we write</p>
<p/><p align="center"><img alt="\displaystyle  x^T(L_G - L_{G'}) x = \sum_k x^T(L_{G^{(k-1)}} - L_{G^{(k)}}) x " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5ET%28L_G+-+L_%7BG%27%7D%29+x+%3D+%5Csum_k+x%5ET%28L_%7BG%5E%7B%28k-1%29%7D%7D+-+L_%7BG%5E%7B%28k%29%7D%7D%29+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x^T(L_G - L_{G'}) x = \sum_k x^T(L_{G^{(k-1)}} - L_{G^{(k)}}) x "/></p>
<p> and then we show how to dominate each term on the right-hand side by a Gaussian process. But then won’t the sum of those Gaussian processes dominate <img alt="{x^T(L_G - L_{G'}) x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5ET%28L_G+-+L_%7BG%27%7D%29+x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^T(L_G - L_{G'}) x}"/>, which was not supposed to be possible?</p>
<p>
But the point is that in the analysis of <img alt="{ x^T(L_{G^{(k-1)}} - L_{G^{(k)}}) x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+x%5ET%28L_%7BG%5E%7B%28k-1%29%7D%7D+-+L_%7BG%5E%7B%28k%29%7D%7D%29+x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{ x^T(L_{G^{(k-1)}} - L_{G^{(k)}}) x}"/> we throw away the low-probability case that, in the previous processes, we ended with <img alt="{L_{G^{(k-1)}} \not\preceq 2 L_G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7BG%5E%7B%28k-1%29%7D%7D+%5Cnot%5Cpreceq+2+L_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{G^{(k-1)}} \not\preceq 2 L_G}"/>. These low-probability events that we remove from consideration are enough to cut off the problematic tail of the discrete distribution that does not have a good Gaussian domination.</p>
<p>
To get a better sense of what is happening, suppose that we are looking at the real-valued random variable </p>
<p align="center"><img alt="\displaystyle  S = \sum_{i=1}^n s_i " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S+%3D+%5Csum_%7Bi%3D1%7D%5En+s_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  S = \sum_{i=1}^n s_i "/></p>
<p> where each <img alt="{s_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_i}"/> is a independent random variable that is equal to 1 with probability <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> and equal to zero with probability <img alt="{1-p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1-p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1-p}"/>. Suppose that <img alt="{p = 2^{-\ell}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp+%3D+2%5E%7B-%5Cell%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p = 2^{-\ell}}"/> is a negative power of two and that <img alt="{pn &gt; n^{-\Omega(1)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bpn+%3E+n%5E%7B-%5COmega%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{pn &gt; n^{-\Omega(1)}}"/>, let’s say <img alt="{pn &gt; \sqrt n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bpn+%3E+%5Csqrt+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{pn &gt; \sqrt n}"/>.</p>
<p>
We would like to say that there is a <img alt="{t = O(\sqrt{np \log n})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt+%3D+O%28%5Csqrt%7Bnp+%5Clog+n%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t = O(\sqrt{np \log n})}"/> such that </p>
<p align="center"><img alt="\displaystyle  \Pr [ S -\mathop{\mathbb E} S &gt; t ] \leq \frac 1{n^{\Omega(1)}} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPr+%5B+S+-%5Cmathop%7B%5Cmathbb+E%7D+S+%3E+t+%5D+%5Cleq+%5Cfrac+1%7Bn%5E%7B%5COmega%281%29%7D%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \Pr [ S -\mathop{\mathbb E} S &gt; t ] \leq \frac 1{n^{\Omega(1)}} "/></p>
<p>
Also, we have made a vow to only bound the tail of discrete random variables by showing that they are sub-Gaussian and then using the tail of the dominating Gaussian.</p>
<p>
If we try to argue about the sub-Gaussianity of <img alt="{S-\mathop{\mathbb E} S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS-%5Cmathop%7B%5Cmathbb+E%7D+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S-\mathop{\mathbb E} S}"/>, we are in trouble, because there is probability <img alt="{p^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p^n}"/> that <img alt="{S=pn}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%3Dpn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S=pn}"/>, so that <img alt="{S-\mathop{\mathbb E} S \geq \frac n2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS-%5Cmathop%7B%5Cmathbb+E%7D+S+%5Cgeq+%5Cfrac+n2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S-\mathop{\mathbb E} S \geq \frac n2}"/>. This is problematic because, in a Gaussian distribution, a deviation that occurs with probability <img alt="{p^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p^n}"/> can only be <img alt="{O(\sqrt {n \log 1/p})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt+%7Bn+%5Clog+1%2Fp%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\sqrt {n \log 1/p})}"/> times the standard deviation, so the standard deviation has to be <img alt="{\Omega (\sqrt{ n / \log 1/p})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega+%28%5Csqrt%7B+n+%2F+%5Clog+1%2Fp%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Omega (\sqrt{ n / \log 1/p})}"/> and a deviation that is achieved with probability <img alt="{1/n^{\Theta(1)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2Fn%5E%7B%5CTheta%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/n^{\Theta(1)}}"/> is of the order at least <img alt="{\Omega \left( \sqrt {\frac{n\log n} {\log 1/p}} \right)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega+%5Cleft%28+%5Csqrt+%7B%5Cfrac%7Bn%5Clog+n%7D+%7B%5Clog+1%2Fp%7D%7D+%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Omega \left( \sqrt {\frac{n\log n} {\log 1/p}} \right)}"/> which is much more than the <img alt="{O(\sqrt{np \log n})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt%7Bnp+%5Clog+n%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\sqrt{np \log n})}"/> that we were hoping for. </p>
<p>
The problem is that a Gaussian distribution with standard deviation of the order of <img alt="{\sqrt {pn}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt+%7Bpn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sqrt {pn}}"/> dominates our distribution in the regime we are interested in, but not in all regimes.</p>
<p>
To overcome this problem, we write</p>
<p/><p align="center"><img alt="\displaystyle  S - \mathop{\mathbb E} S = \sum_{k=1}^\ell S^{(k)} - S^{(k-1)} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S+-+%5Cmathop%7B%5Cmathbb+E%7D+S+%3D+%5Csum_%7Bk%3D1%7D%5E%5Cell+S%5E%7B%28k%29%7D+-+S%5E%7B%28k-1%29%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  S - \mathop{\mathbb E} S = \sum_{k=1}^\ell S^{(k)} - S^{(k-1)} "/></p>
<p> where </p>
<p align="center"><img alt="\displaystyle  S^{(k)} = \sum_i s^{(k)}_i " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S%5E%7B%28k%29%7D+%3D+%5Csum_i+s%5E%7B%28k%29%7D_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  S^{(k)} = \sum_i s^{(k)}_i "/></p>
<p> and the random variables <img alt="{s^{(k)}_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%5E%7B%28k%29%7D_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s^{(k)}_i}"/> are constructed in the following way: <img alt="{s_i^{(0)} = p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_i%5E%7B%280%29%7D+%3D+p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_i^{(0)} = p}"/>, and <img alt="{s^{(k)}_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%5E%7B%28k%29%7D_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s^{(k)}_i}"/> is equally likely to be either 0 or <img alt="{2 s_i^{(k-1)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2+s_i%5E%7B%28k-1%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2 s_i^{(k-1)}}"/>. In this way, <img alt="{S^{(0)} = \mathop{\mathbb E} S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%5E%7B%280%29%7D+%3D+%5Cmathop%7B%5Cmathbb+E%7D+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S^{(0)} = \mathop{\mathbb E} S}"/> and <img alt="{S^{(\ell)} = S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%5E%7B%28%5Cell%29%7D+%3D+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S^{(\ell)} = S}"/>.</p>
<p>
For every choice of the <img alt="{s^{(k-1)}_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%5E%7B%28k-1%29%7D_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s^{(k-1)}_i}"/>, we can write </p>
<p align="center"><img alt="\displaystyle  S^{(k)} - S^{(k-1)} = \sum_i r^{(k)}_i s^{(k-1)}_i " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S%5E%7B%28k%29%7D+-+S%5E%7B%28k-1%29%7D+%3D+%5Csum_i+r%5E%7B%28k%29%7D_i+s%5E%7B%28k-1%29%7D_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  S^{(k)} - S^{(k-1)} = \sum_i r^{(k)}_i s^{(k-1)}_i "/></p>
<p> where the <img alt="{r^{(k)}_i }" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%5E%7B%28k%29%7D_i+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r^{(k)}_i }"/> are independent Rademacher random variable. Over the randomness of the <img alt="{r^{(k)}_i }" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%5E%7B%28k%29%7D_i+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r^{(k)}_i }"/>, the random variable <img alt="{S^{(k)} - S^{(k-1)} }" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%5E%7B%28k%29%7D+-+S%5E%7B%28k-1%29%7D+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S^{(k)} - S^{(k-1)} }"/> has a sub-Gaussian distribution dominated by a Gaussian of variance </p>
<p align="center"><img alt="\displaystyle  \sum_i \left( s^{(k-1)}_i \right)^2 \leq 2^k p \cdot \sum_i s^{(k-1)}_i " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_i+%5Cleft%28+s%5E%7B%28k-1%29%7D_i+%5Cright%29%5E2+%5Cleq+2%5Ek+p+%5Ccdot+%5Csum_i+s%5E%7B%28k-1%29%7D_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_i \left( s^{(k-1)}_i \right)^2 \leq 2^k p \cdot \sum_i s^{(k-1)}_i "/></p>
<p> Now, without breaking our vow, we can inductively get a high-probability estimate that for each <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> </p>
<p align="center"><img alt="\displaystyle  S^{(k)} - S^{(k-1)} \leq O(\sqrt{ 2^k p^2 n \log n}) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S%5E%7B%28k%29%7D+-+S%5E%7B%28k-1%29%7D+%5Cleq+O%28%5Csqrt%7B+2%5Ek+p%5E2+n+%5Clog+n%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  S^{(k)} - S^{(k-1)} \leq O(\sqrt{ 2^k p^2 n \log n}) "/></p>
<p> while maintaining the invariant that, say, <img alt="{ \sum_i s^{(k-1)}_i \leq 2pn}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+%5Csum_i+s%5E%7B%28k-1%29%7D_i+%5Cleq+2pn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{ \sum_i s^{(k-1)}_i \leq 2pn}"/> for each <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>. When we sum up the error bounds, the whole sum is of the order of the last term, which is <img alt="{O(\sqrt {pn \log n})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt+%7Bpn+%5Clog+n%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\sqrt {pn \log n})}"/>.</p>
<p/></div>
    </content>
    <updated>2020-05-12T15:33:04Z</updated>
    <published>2020-05-12T15:33:04Z</published>
    <category term="theory"/>
    <category term="matrix Chernoff bounds"/>
    <category term="sparsification"/>
    <category term="spectral graph theory"/>
    <category term="Talagrand"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2020-05-17T22:24:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2020/05/12/ideal-workshop-on-estimation-of-network-processes-and-information-diffusion/</id>
    <link href="https://cstheory-events.org/2020/05/12/ideal-workshop-on-estimation-of-network-processes-and-information-diffusion/" rel="alternate" type="text/html"/>
    <title>IDEAL Workshop on Estimation of Network Processes and Information Diffusion</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">May 14, 2020 Virtual https://www.ideal.northwestern.edu/events/estimation-of-network-processes-and-information-diffusion/?fbclid=IwAR3HRiqCz8zTQfHt46hF-VF9PHE2F1v4uvQ6V_ggxmzKqFvromZXhVOOygY Many important dynamic processes are determined by an underlying network structure. Examples include the spread of epidemics, the dynamics of public opinions, the diffusion of information about social programs, and biological processes such as neural spike trains. Data about these processes is becoming increasingly available which has lead to a … <a class="more-link" href="https://cstheory-events.org/2020/05/12/ideal-workshop-on-estimation-of-network-processes-and-information-diffusion/">Continue reading <span class="screen-reader-text">IDEAL Workshop on Estimation of Network Processes and Information Diffusion</span></a></div>
    </summary>
    <updated>2020-05-12T03:47:26Z</updated>
    <published>2020-05-12T03:47:26Z</published>
    <category term="workshop"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2020-05-17T22:39:32Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/05/12/inbox-of-triangle</id>
    <link href="https://11011110.github.io/blog/2020/05/12/inbox-of-triangle.html" rel="alternate" type="text/html"/>
    <title>The inbox of a triangle</title>
    <summary>In affine geometry, the minimum-area ellipse surrounding a given triangle and the maximum-area ellipse within it (its two Steiner ellipses) are concentric and similar. This can be seen easily by performing an affine transformation to an equilateral triangle, observing that in this case these ellipses are concentric circles (the circumcircle and incircle), and that the extreme ellipses of the transformed shape are the transforms of the extreme ellipses of the original shape. In Cartesian geometry, where the Cartesian coordinates can be independently linearly transformed or swapped, something similar turns out to happen. In this case, the minimum-area axis-parallel rectangle surrounding a given triangle (its bounding box) and the maximum-area rectangle within it (let’s call it the inbox) are always similar, though not concentric.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In affine geometry, the minimum-area ellipse surrounding a given triangle and the maximum-area ellipse within it (its two <a href="https://en.wikipedia.org/wiki/Steiner_inellipse">Steiner ellipses</a>) are concentric and similar. This can be seen easily by performing an affine transformation to an equilateral triangle, observing that in this case these ellipses are concentric circles (the circumcircle and incircle), and that the extreme ellipses of the transformed shape are the transforms of the extreme ellipses of the original shape. In <a href="https://11011110.github.io/blog/2020/04/28/cartesian-triangle-centers.html">Cartesian geometry</a>, where the Cartesian coordinates can be independently linearly transformed or swapped, something similar turns out to happen. In this case, the minimum-area axis-parallel rectangle surrounding a given triangle (its bounding box) and the maximum-area rectangle within it (let’s call it the <em>inbox</em>) are always similar, though not concentric.</p>

<p style="text-align: center;"><img alt="Bounding boxes and inboxes of two triangles" src="https://11011110.github.io/blog/assets/2020/inbox.svg"/></p>

<p>The inbox always touches all three sides of its triangle. For if it missed any one of the sides, it could be made larger, while staying within the triangle, by a dilation centered at the opposite vertex. It turns out that for triangles in general position (no two vertex coordinates equal) there are two cases, shown above. If the two median coordinates belong to different triangle vertices (as is true for all acute triangles) then the inbox touches all three sides at separate points, and has a fourth vertex interior to the triangle, as shown on the left. If there is one triangle vertex with both median coordinates (necessarily an obtuse vertex), then the inbox touches that vertex and the opposite side, with two other vertices free, as shown on the right.
Let’s call these the acute and obtuse cases, even though the triangle in the acute case might actually be obtuse.</p>

<p>In the obtuse case, we can apply a Cartesian transformation to make the bounding box square. The inbox lies within an isosceles right triangle formed by axis-parallel lines through the obtuse vertex and by the opposite side of the triangle. By the symmetry of this isosceles right triangle, the inbox is also square. Just as in the Steiner ellipse argument, this implies that the inbox of the untransformed triangle is similar to its bounding box.</p>

<p>In the acute case, let’s again draw axis-parallel lines through the median coordinates. Each line crosses one triangle side, and the line segment between these two crossings cuts the triangle into a smaller triangle (light blue below) and an <a href="https://en.wikipedia.org/wiki/Orthodiagonal_quadrilateral">orthodiagonal quadrilateral</a> (dark blue), with the property that any rectangle that touches the three sides of the triangle also touches the fourth side of the quadrilateral. In this case, the rectangle formed by the four midpoints of the quadrilateral (its <a href="https://en.wikipedia.org/wiki/Varignon%27s_theorem">Varignon rectangle</a>) must be the one with maximum area. For, this rectangle has exactly half the area of the orthodiagonal quadrilateral, as the four triangular flaps surrounding it can be folded over to exactly cover the rectangle. Any other rectangle would have two longer flaps and two shorter flaps, which when folded would overlap near the middle of the rectangle, showing that the total flap area is larger than the rectangle area.</p>

<p style="text-align: center;"><img alt="Construction of an orthodiagonal quadrilateral in which the inbox is inscribed in the acute case" src="https://11011110.github.io/blog/assets/2020/envelope.svg"/></p>

<p>The inbox has half the width and half the height of the bounding box of the orthodiagonal quadrilateral, so they both are similar. I have a coordinate-based proof that they are also both similar to the bounding box of the original triangle, but not a nice synthetic proof. Suppose we perform a Cartesian transformation of the triangle so that its cut-off vertex (the one not part of the orthodiagonal quadrilateral) is at the origin and so that its bounding box is the unit square. Let the median coordinates of the triangle vertices be  and . Then the vertex of the bounding box of the orthodiagonal quadrilateral that is closest to the origin has coordinates , on the diagonal of the unit square, and its farthest vertex has coordinates , so it is a square too.</p>

<p>The center of the Steiner ellipses is the centroid of the triangle, the only affine-equivariant triangle center. Both the center of the inbox and the center of its similarity with the bounding box appear to be Cartesian triangle centers in the sense of <a href="https://11011110.github.io/blog/2020/04/28/cartesian-triangle-centers.html">my previous post</a>: they are continuous Cartesian-equivariant functionals from triangles to points. Unlike the centroid or the bounding-box center, they cannot be calculated separately in the two coordinates, as can be seen from the first example, where the median -coordinate is halfway between the other two but the inbox center and center of similarity -coordinates are not. However, they differ from the equal-box-area center of my previous post, which is outside the triangle in the obtuse case while these centers are always inside.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/104156395479629486">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-05-12T00:24:00Z</updated>
    <published>2020-05-12T00:24:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-05-16T00:07:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17029</id>
    <link href="https://rjlipton.wordpress.com/2020/05/11/consistency-and-pnp/" rel="alternate" type="text/html"/>
    <title>Consistency and P=NP</title>
    <summary>Can we at least show that is consistent? [ From personal page ] Jan Krajicek is an expert on linking computational complexity and mathematical logic. He has authored four books on this area including the often cited text Bounded Arithmetic and Complexity Theory. Moreover, he has studied the consistency of various of theories and whether […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Can we at least show that <img alt="{P\neq NP}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%5Cneq+NP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P\neq NP}"/> is consistent?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/11/consistency-and-pnp/unknown-140/" rel="attachment wp-att-17032"><img alt="" class="alignright size-full wp-image-17032" src="https://rjlipton.files.wordpress.com/2020/05/unknown.jpeg?w=600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ From personal page ]</font></td>
</tr>
</tbody>
</table>
<p>
Jan Krajicek is an expert on linking computational complexity and mathematical logic. He has authored four books on this area including the often cited <a href="https://www.cambridge.org/core/books/bounded-arithmetic-propositional-logic-and-complexity-theory/899C8FF084B3EB2430EDEBC27921D635">text</a> <em>Bounded Arithmetic and Complexity Theory</em>. Moreover, he has studied the consistency of various of theories and whether they can prove <img alt="{\mathsf{P \neq NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%5Cneq+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{P \neq NP}}"/> and related questions. </p>
<p>
Today I thought we would discuss consistency proofs in general.</p>
<p><span id="more-17029"/></p>
<p>
Lately I have been thinking about <img alt="{\mathsf{P=NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%3DNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{P=NP}}"/> and logic, with emphasis on consistency results. Thanks to Kurt Gödel’s famous Second Incompleteness Theorem we know that consistency theorems are difficult to prove. In particular he showed that</p>
<blockquote><p><b>Theorem 1 (Informally)</b> <em> Any sufficiently powerful consistent theory cannot prove that it is consistent. </em>
</p></blockquote>
<p/><p>
In order to prove that some theory <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> is consistent we need a stronger theory say <img alt="{T^{*}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%5E%7B%2A%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T^{*}}"/>. But to prove this theory is itself consistent we need an even stronger theory <img alt="{T^{**}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%5E%7B%2A%2A%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T^{**}}"/> and so on. As they <a href="https://en.wikipedia.org/wiki/Turtles_all_the_way_down">say</a>:</p>
<blockquote><p><b> </b> <em> Its turtles all the way down. </em>
</p></blockquote>
<p/><p>
It is not clear who first said this, but an 1854 <a href="https://books.google.com/books?id=SGtYAAAAMAAJ&amp;pg=PA48#v=onepage&amp;q&amp;f=false">transcript</a> of debates between a mainline and maverick minister records the former (Joseph Berg) saying of the latter (Joseph Barker):</p>
<blockquote><p><b> </b> <em> My opponent’s reasoning reminds me of the heathen, who, being asked on what the world stood, replied, “On a tortoise.” But on what does the tortoise stand? “On another tortoise.” With Mr. Barker, too, there are tortoises all the way down. </em>
</p></blockquote>
<p/><p>
Recall a logical theory is consistent provided it never proves <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> and <img alt="{\neg A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cneg+A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\neg A}"/> where as usual <img alt="{\neg A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cneg+A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\neg A}"/> means “not” <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/>. Thus a theory is consistent provided it will never prove both <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> and <img alt="{\neg A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cneg+A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\neg A}"/>. This means that once a statement is proved we are done. There will be no surprise in the future. </p>
<p>
Bluntly: I would hate if my paper showing that <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> is true, was followed by your paper showing that <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> is also false. Besides pride I would hate this because, then any statement is true. This would certainly reduce any interest in my theorem proving <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> and also in your theorem proving <img alt="{\neg X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cneg+X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\neg X}"/>. This is “ex contradictione quodlibet”.</p>
<p>
The rule 	</p>
<p align="center"><img alt="\displaystyle  X \text{ and } \neg X \implies \text{``Pigs can fly''} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X+%5Ctext%7B+and+%7D+%5Cneg+X+%5Cimplies+%5Ctext%7B%60%60Pigs+can+fly%27%27%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  X \text{ and } \neg X \implies \text{``Pigs can fly''} "/></p>
<p>must be true. Right. Or must it? In a moment, but first some examples of consistency issues.</p>
<p>
</p><p/><h2> Failure of Consistency </h2><p/>
<p/><p>
Since David Hilbert explicitly, and before implicitly, there has been interest in showing that our math theories are consistent. It may seem strange that consistency could ever be an issue, but read on.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>A theory that was thought to be inconsistent:</i><br/>
The creation of non-Euclidean geometries asked could these geometries be consistent? The answer is yes. It is possible to show that they are as consistent as Euclidean geometries. This done by building models of their new geometry by using Eucildean geometries in clever ways.</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/11/consistency-and-pnp/models2/" rel="attachment wp-att-17033"><img alt="" class="aligncenter size-medium wp-image-17033" height="282" src="https://rjlipton.files.wordpress.com/2020/05/models2.jpg?w=300&amp;h=282" width="300"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2"/>
</td>
</tr>
</tbody></table>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>A theory that was thought to be consistent:</i> <br/>
The famous Russell paradox destroyed early formulations of set theory. Bertrand Russell noted that the formulations allowed defining the following as a set: 	</p>
<p align="center"><img alt="\displaystyle  S = \{ x \mid x \not\in x \}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S+%3D+%5C%7B+x+%5Cmid+x+%5Cnot%5Cin+x+%5C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  S = \{ x \mid x \not\in x \}. "/></p>
<p>But this is a problem, since it creates logical statements that are equivalent to their negations: 	</p>
<p align="center"><img alt="\displaystyle  x \in S \iff x \not\in S. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x+%5Cin+S+%5Ciff+x+%5Cnot%5Cin+S.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x \in S \iff x \not\in S. "/></p>
<p>Fixing this led to the creation of modern set theory <a href="https://plato.stanford.edu/entries/set-theory/#AxiZFC">ZF</a>. It is named Zermelo-Fraenkel set theory, after Ernst Zermelo and Abraham Fraenkel.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>A theory that was thought to be—still unclear:</i> <br/>
Willard Quine’s theory “New Foundations” (<a href="https://en.wikipedia.org/wiki/New_Foundations">NF</a>) of set theory remains a problem. There are some possible relative proofs that show that it is as consistent as normal set theory. See this for the <a href="https://arxiv.org/abs/1503.01406">claim</a> by Randall Holmes that NF is as consistent as ZF—see <a href="https://math.boisestate.edu/~holmes/nfproof/sorted_at_last_maybe.pdf">this</a>.</p>
<p>
</p><p/><h2> Length-based Immunity </h2><p/>
<p/><p>
The sentences used by Krajicek and his co-authors Jan Bydzovsky and Igor Oliveira in a recent <a href="https://arxiv.org/abs/1905.12935">paper</a> deal with forms of <em>length-based immunity</em>. The complexity-theoretic definition is this:</p>
<blockquote><p><b>Definition 2</b> <em><a name="lbimmune"/> A language <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{B}"/> is <b>length-based immune</b> (LBI) to <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathsf{P}}"/> if for every polynomial-time program <img alt="{P_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP_i%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{P_i}"/>, there are only finitely many <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/> such that <img alt="{P_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP_i%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{P_i}"/> is correct on strings of length <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/>. That is, </em></p><em>
<p align="center"><img alt="\displaystyle  (\forall P_i)(\exists \ell)(\forall n &gt; \ell)(\exists x \in \Sigma^n): P_i(x) \neq B(x). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28%5Cforall+P_i%29%28%5Cexists+%5Cell%29%28%5Cforall+n+%3E+%5Cell%29%28%5Cexists+x+%5Cin+%5CSigma%5En%29%3A+P_i%28x%29+%5Cneq+B%28x%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  (\forall P_i)(\exists \ell)(\forall n &gt; \ell)(\exists x \in \Sigma^n): P_i(x) \neq B(x). "/></p>
</em><p><em/>
</p></blockquote>
<p/><p>
This property is akin to <em>immunity</em>: not having an infinite easy subset. The known <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{NP}}"/>-complete sets are <em>not</em> immune: for instance there are many easy cases of SAT. Under the <a href="https://en.wikipedia.org/wiki/Berman-Hartmanis_conjecture">Isomorphism Conjecture</a>, no <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{NP}}"/>-complete set is immune. Whether SAT can be length-based immune might depend on details of how formulas are encoded, but under a “natural” encoding one might expect length-based immunity to hold.</p>
<p>
Of course, if <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{NP}}"/> has a length-based immune set then <img alt="{\mathsf{P \neq NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%5Cneq+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{P \neq NP}}"/>. Say a theory <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> is “adequate” if it is consistent and proves basic facts about complexity classes such as that one. Then for an adequate theory <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>:</p>
<blockquote><p><b> </b> <em> Suppose we prove it is consistent with <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{T}"/> that <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathsf{NP}}"/> has an LBI set. Then <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{T}"/> cannot prove that <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathsf{NP}}"/> does not have an LBI set. Since <img alt="{\mathsf{P = NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathsf{P = NP}}"/> imples that <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathsf{NP}}"/> does not have an LBI set, this yields the simple and notable conclusion that <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{T}"/> cannot prove <img alt="{\mathsf{P = NP}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D.%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathsf{P = NP}.}"/> That is, <img alt="{\mathsf{P \neq NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%5Cneq+NP%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathsf{P \neq NP}}"/> is consistent with <img alt="{T.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT.%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{T.}"/></em>
</p></blockquote>
<p/><p>
Krajicek, as we said before, has tried to prove that complexity statements like <img alt="{\mathsf{P \neq NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%5Cneq+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{P \neq NP}}"/> are consistent. The trouble is that such questions are difficult. So he and others have worked on showing that these questions are at least consistent with theories <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> of arithmetic. It seems too hard to handle Peano arithmetic yet, so they work with theories <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> that are weaker than Peano arithmetic—but still adequate. For these theories, Krajicek and company have results for some variations of Definition~<a href="https://rjlipton.wordpress.com/feed/#lbimmune">2</a>, albeit ones that seem not to have a direct relation to <img alt="{\mathsf{P = NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{P = NP}}"/>. The two main ways to vary the Definition~<a href="https://rjlipton.wordpress.com/feed/#lbimmune">2</a> are:</p>
<ul>
<li>
Change the class <img alt="{\mathcal{C}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BC%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{C}}"/> that has—or may not have—a length-based immune set to something else, for instance <img alt="{\mathcal{C} = \mathsf{P^{NP}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BC%7D+%3D+%5Cmathsf%7BP%5E%7BNP%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{C} = \mathsf{P^{NP}}}"/>. <p/>
</li><li>
Change the class <img alt="{\mathcal{Q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BQ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{Q}}"/> of programs the sets are immune to from polynomial-time programs to something else.
</li></ul>
<p>
</p><p/><h2> Fixed Polynomial Size Circuits </h2><p/>
<p/><p>
They fix a constant exponent <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> and take <img alt="{\mathcal{Q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BQ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{Q}}"/> to be families of <em>circuits</em> <img alt="{C_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_n}"/> of fixed polynomial size <img alt="{n^k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Ek%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^k}"/>. The circuits can be non-uniform, and indeed the unprovable statements are defined by a process that is not constructive. </p>
<p>
Circuits of linear or other fixed polynomial size are a fascinating and important topic in themselves. We have <a href="https://rjlipton.wordpress.com/2010/03/31/a-problem-with-proving-problems-are-hard/">posted</a> about Ravi Kannan’s famous theorem that for every fixed <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>, the second level of the polynomial hierarchy has languages without <img alt="{n^k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Ek%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^k}"/>-sized circuits. The upper bound has since been pushed lower but not all the way to <img alt="{\mathsf{P^{NP}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%5E%7BNP%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{P^{NP}}}"/>. It is of course believed that <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{NP}}"/> has languages without <img alt="{n^k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Ek%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^k}"/>-sized circuits, since SAT is believed to require super-polynomial size. But the question of whether <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{NP}}"/> has sets that lack such circuits—let alone being immune—is currently not known to have implications about <img alt="{\mathsf{P = NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%3D+NP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{P = NP}}"/>. </p>
<p>
Note that the easy instances of SAT that we alluded to above are easy for linear-size circuits. Moreover, whether <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{P}}"/> itself has—or doesn’t have—sets that are length-based immune to <img alt="{n^k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Ek%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^k}"/>-size circuits is a statement that can be plausibly argued either way. Krajicek and Oliveira had earlier <a href="https://arxiv.org/abs/1605.00263">proved</a> that a particular theory called PV—which we will discuss next—cannot prove for any <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> that every language in <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{P}}"/> has <img alt="{n^k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Ek%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^k}"/>-sized circuits. This unprovable statement, without the LBI condition, strikes us as less plausible—hence less surprising that a weaker theory like PV cannot prove it. What they prove now is:</p>
<blockquote><p><b>Theorem 3</b> <em><a name="thm1a"/> For any fixed <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{k}"/>, PV cannot prove that every language in <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathsf{P}}"/> allows circuits of size <img alt="{n^k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Ek%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n^k}"/> to get the right answers on infinitely many input lengths. That is, it is consistent with PV that <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathsf{P}}"/> has sets that are LBI to <img alt="{n^k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Ek%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n^k}"/>-size circuits.</em></p><em>
</em><p><em>
Moreover, for some particular theories called <img alt="{S^1_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%5E1_2%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{S^1_2}"/> and <img alt="{T^1_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%5E1_2%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{T^1_2}"/> that extend PV, it is consistent that <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathsf{NP}}"/>, respectively <img alt="{\mathsf{p^{NP}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7Bp%5E%7BNP%7D%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathsf{p^{NP}}}"/> has sets that are LBI to <img alt="{n^k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Ek%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n^k}"/>-sized circuits. </em>
</p></blockquote>
<p/><p>
We will talk a little more about these theories.</p>
<p>
</p><p/><h2> PV and Related Theories </h2><p/>
<p/><p>
What are the candidate theories for <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>? One is PV, a theory created by Steve Cook. For every polynomial-time program <img alt="{P_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P_i}"/>, PV has a symbol <img alt="{\varphi_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvarphi_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\varphi_i}"/> for the function that <img alt="{P_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P_i}"/> computes. Then add all the equations that these functions satisfy and you have PV. For example, 	</p>
<p align="center"><img alt="\displaystyle  2^{x+1} = 2 \times 2^{x}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%5E%7Bx%2B1%7D+%3D+2+%5Ctimes+2%5E%7Bx%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  2^{x+1} = 2 \times 2^{x}, "/></p>
<p>for the exponential function <img alt="{x \rightarrow 2^{x}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%5Crightarrow+2%5E%7Bx%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x \rightarrow 2^{x}}"/>. </p>
<p>
PV can formalize and prove properties of program-building constructs like composition and bounded for-loops that preserve running in polynomial time. It can formalize the concept of polynomial-size Boolean circuits and establish basic facts about them. It can prove deep results like the PCP theorem and more. </p>
<p>
Thus showing that your favorite statement is at least consistent with PV would be neat. It is critical that PV only allows equations as axioms. Thus PV does not axiomatize the statement 	</p>
<p align="center"><img alt="\displaystyle  \forall x \exists y&gt;x \ 2^{y}-1 \text{ is a prime} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+x+%5Cexists+y%3Ex+%5C+2%5E%7By%7D-1+%5Ctext%7B+is+a+prime%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall x \exists y&gt;x \ 2^{y}-1 \text{ is a prime} "/></p>
<p>even if it is true. The more-formal statement of their theorem for PV is:</p>
<blockquote><p><b>Theorem 4</b> <em><a name="thm1b"/> For every <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{k}"/>, there is a formula <img alt="{\varphi(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvarphi%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\varphi(x)}"/> in the language of PV such that the statement </em></p><em>
<p align="center"><img alt="\displaystyle  \Upsilon_{\varphi} = (\forall \ell)(\exists n &gt; \ell)(\exists C_n \text{ of size } \leq n^k)(\forall x\in \Sigma^n): \varphi(x) = C_n(x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CUpsilon_%7B%5Cvarphi%7D+%3D+%28%5Cforall+%5Cell%29%28%5Cexists+n+%3E+%5Cell%29%28%5Cexists+C_n+%5Ctext%7B+of+size+%7D+%5Cleq+n%5Ek%29%28%5Cforall+x%5Cin+%5CSigma%5En%29%3A+%5Cvarphi%28x%29+%3D+C_n%28x%29+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  \Upsilon_{\varphi} = (\forall \ell)(\exists n &gt; \ell)(\exists C_n \text{ of size } \leq n^k)(\forall x\in \Sigma^n): \varphi(x) = C_n(x) "/></p>
</em><p><em>is not provable in PV, and such that the function <img alt="{\varphi(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvarphi%28x%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\varphi(x)}"/> is polynomial-time computable. </em>
</p></blockquote>
<p/><p>
The theorem is not diminished if we restrict attention to yes/no formulas and circuit outputs. Thus it is not provable that every language in <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{P}}"/> has infinitely many input lengths at which it is easy for <img alt="{n^k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Ek%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^k}"/>-sized circuits. Put another way, it is consistent with PV to assert (for any <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>) that <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{P}}"/> has a language that lacks size-<img alt="{n^k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Ek%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^k}"/> circuits <img alt="{C_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_n}"/> in the strong sense of <em>every</em> <img alt="{C_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_n}"/> making an error on some string of length <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>, beyond a finite number of <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>. This is stronger than saying the language lacks <img alt="{n^k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Ek%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^k}"/>-size circuits, which makes the consistency more notable.</p>
<p>
Where PV lacks power is with the unrestricted induction enjoyed by Peano Arithmetic. The other theories <img alt="{S_2^1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_2%5E1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S_2^1}"/> and <img alt="{T_2^1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_2%5E1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_2^1}"/>, which were named and studied by Sam Buss in the 1980s, add more induction—by limiting the kind of formulas on which it is allowed.</p>
<p>
Here is where we point to the papers for details. The devil in those details is that the exceptional formulas <img alt="{\varphi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvarphi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\varphi}"/> are not expressly constructed. For one, the proofs of their theorems use a disjunction about whether the polynomial hierarchy collapses at a certain level. Note that something similar is already at work in our <a href="https://rjlipton.wordpress.com/2010/03/31/a-problem-with-proving-problems-are-hard/">post</a> on Kannan’s theorem. </p>
<p>
Second, their proofs exploit a quirk of PV and the other theories that they have symbols for all polynomial-time programs without having commensurate command of the semantics of what these programs represent. The induction and axioms available to <img alt="{S_2^1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_2%5E1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S_2^1}"/> and <img alt="{T_2^1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_2%5E1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_2^1}"/> bridge some but not all of these gaps. </p>
<p>
Third and most particular, their theorem uses a self-referential strategy. It starts building <img alt="{\varphi'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvarphi%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\varphi'}"/> but branches according to whether the theory <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> can prove <img alt="{\Upsilon_{\varphi'}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CUpsilon_%7B%5Cvarphi%27%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Upsilon_{\varphi'}}"/>. If not it keeps going, if yes it does something else. Thus the proof does not tell what <img alt="{\varphi(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvarphi%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\varphi(x)}"/> is, nor identify a particular language in <img alt="{\mathcal{C}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BC%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{C}}"/> for which the LBI property is consistent.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Another way of saying “Statement <img alt="{\Upsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CUpsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Upsilon}"/> is consistent with <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>” is that one can build a <em>model</em> of <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> in which <img alt="{\Upsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CUpsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Upsilon}"/> is true. What do those models look like? Do they relate to the notion of <em>oracles</em> <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> relative to which certain statements—such as <img alt="{\mathsf{NP}^A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%5EA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{NP}^A}"/> having languages that are (length-based-) immune to <img alt="{\mathsf{P}^A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%5EA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{P}^A}"/>—are true?</p>
<p>
[William–&gt;Willard Quine]</p></font></font></div>
    </content>
    <updated>2020-05-11T12:51:06Z</updated>
    <published>2020-05-11T12:51:06Z</published>
    <category term="Ideas"/>
    <category term="P=NP"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="consistent"/>
    <category term="models"/>
    <category term="Peano"/>
    <category term="SAT"/>
    <category term="Theory"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-05-17T22:31:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3481</id>
    <link href="https://agtb.wordpress.com/2020/05/11/special-issue-of-jaamas-on-fair-division/" rel="alternate" type="text/html"/>
    <title>Special Issue of JAAMAS on Fair Division</title>
    <summary>Dear all, We are happy to announce that the Journal of Autonomous Agents and Multiagent Systems will host a special issue on Fair Division. We welcome full versions of papers on fair division that have appeared in recent editions of AAMAS, IJCAI, AAAI, ECAI, ACM EC, WINE, SAGT, and other relevant conferences, as well as […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Dear all,</p>
<p>We are happy to announce that the Journal of Autonomous Agents and Multiagent Systems will host a special issue on Fair Division. We welcome full versions of papers on fair division that have appeared in recent editions of AAMAS, IJCAI, AAAI, ECAI, ACM EC, WINE, SAGT, and other relevant conferences, as well as papers that have not been published in conference proceedings.</p>
<p><a href="https://www.springer.com/journal/10458/updates/17851836">Call for Papers</a></p>
<p>Please note that there will be a rolling review process: submissions accepted before the completion of the issue will be available on the journal website shortly after acceptance. Deadline: March 1, 2021.</p>
<p>Guest Editors<br/>
Edith Elkind<br/>
Nicolas Maudet<br/>
Warut Suksompong</p></div>
    </content>
    <updated>2020-05-11T11:05:41Z</updated>
    <published>2020-05-11T11:05:41Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>felixbrandt</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2020-05-17T22:29:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1693</id>
    <link href="https://theorydish.blog/2020/05/10/weve-got-wit-and-they-got-talent/" rel="alternate" type="text/html"/>
    <title>We’ve got WIT and they got Talent</title>
    <summary>The wonderful Women in Theory is, like so many other events this year, virtual in 2020 (physical meeting postponed to 2021). And it is happening today! The virtual meeting lets our WIT show that their talents go beyond science. Highly recommended clip! I’ve been watching on loop 🙂</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The wonderful <a href="https://theorydish.blog/2020/01/09/women-in-theory-2018-call-for-application-2/">Women in Theory</a> is, like so many other events this year, virtual in 2020 (physical meeting <a href="https://womenintheory.wordpress.com/">postponed to 2021)</a>. And it is happening today! The virtual meeting lets our WIT show that their talents go beyond science. <a href="https://www.youtube.com/watch?v=4Wl-3kadvgw">Highly recommended clip</a>! I’ve been watching on loop <img alt="&#x1F642;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;"/></p></div>
    </content>
    <updated>2020-05-10T17:41:07Z</updated>
    <published>2020-05-10T17:41:07Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2020-05-17T22:40:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/05/10/visiting-professor-at-university-of-tyumen-school-of-advanced-studies-apply-by-june-30-2020/</id>
    <link href="https://cstheory-jobs.org/2020/05/10/visiting-professor-at-university-of-tyumen-school-of-advanced-studies-apply-by-june-30-2020/" rel="alternate" type="text/html"/>
    <title>Visiting Professor at University of Tyumen, School of Advanced Studies (apply by June 30, 2020)</title>
    <summary>Visiting Professors of Computer Science, 2-10 months appointments during 2020-2021 academic year. No knowledge of Russian is required. Please send a CV, two letters of reference (to be sent separately), and a cover letter describing your professional background, including the description of courses you have taught or would like to teach, to job.sas@utmn.ru. Website: https://sas.utmn.ru/en/ […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Visiting Professors of Computer Science, 2-10 months appointments during 2020-2021 academic year. No knowledge of Russian is required.</p>
<p>Please send a CV, two letters of reference (to be sent separately), and a cover letter describing your professional background, including the description of courses you have taught or would like to teach, to job.sas@utmn.ru.</p>
<p>Website: <a href="https://sas.utmn.ru/en/">https://sas.utmn.ru/en/</a><br/>
Email: d.kontowski@utmn.ru</p></div>
    </content>
    <updated>2020-05-10T06:35:29Z</updated>
    <published>2020-05-10T06:35:29Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-05-17T22:32:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=430</id>
    <link href="https://tcsplus.wordpress.com/2020/05/08/430/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, May 13 — Sahil Singla, Princeton University and IAS</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, May 13th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Sahil Singla from Princeton University and IAS will speak about “Online Vector Balancing and Geometric Discrepancy” (abstract below). You can reserve a spot as an individual or a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, May 13th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Sahil Singla</strong> from Princeton University and IAS will speak about “<em>Online Vector Balancing and Geometric Discrepancy</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our website</a> on the day of the talk, so people who did not sign up will still be able to watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: We consider an online vector balancing question where <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=fff&amp;fg=444444&amp;s=0" title="T"/> vectors, chosen from an arbitrary distribution over <img alt="[-1,1]^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5B-1%2C1%5D%5En&amp;bg=fff&amp;fg=444444&amp;s=0" title="[-1,1]^n"/>, arrive one-by-one and must be immediately given a <img alt="\{+1,-1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%2B1%2C-1%5C%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="\{+1,-1\}"/> sign. The goal is to keep the discrepancy—the <img alt="\ell_{\infty}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell_%7B%5Cinfty%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="\ell_{\infty}"/>-norm of any signed prefix-sum—as small as possible. A concrete example of this question is the online interval discrepancy problem where <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=fff&amp;fg=444444&amp;s=0" title="T"/> points are sampled one-by-one uniformly in the unit interval <img alt="[0,1]" class="latex" src="https://s0.wp.com/latex.php?latex=%5B0%2C1%5D&amp;bg=fff&amp;fg=444444&amp;s=0" title="[0,1]"/>, and the goal is to immediately color them <img alt="\{+1,-1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%2B1%2C-1%5C%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="\{+1,-1\}"/> such that every sub-interval remains always nearly balanced. As random coloring incurs <img alt="\Omega(T^{1/2})" class="latex" src="https://s0.wp.com/latex.php?latex=%5COmega%28T%5E%7B1%2F2%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\Omega(T^{1/2})"/> discrepancy, while the worst-case offline bounds are <img alt="\Theta(\sqrt{n \log (T/n)})" class="latex" src="https://s0.wp.com/latex.php?latex=%5CTheta%28%5Csqrt%7Bn+%5Clog+%28T%2Fn%29%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\Theta(\sqrt{n \log (T/n)})"/> for vector balancing and 1 for interval balancing, a natural question is whether one can (nearly) match the offline bounds in the online setting for these problems. One must utilize the stochasticity as in the worst-case scenario it is known that discrepancy is <img alt="\Omega(T^{1/2})" class="latex" src="https://s0.wp.com/latex.php?latex=%5COmega%28T%5E%7B1%2F2%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\Omega(T^{1/2})"/> for any online algorithm.</p>
<p>In this work, we introduce a new framework that allows us to handle online vector balancing even when the input distribution has dependencies across coordinates. In particular, this lets us obtain a <img alt="\textrm{poly}(n, \log T)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7Bpoly%7D%28n%2C+%5Clog+T%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\textrm{poly}(n, \log T)"/> bound for online vector balancing under arbitrary input distributions, and a <img alt="\textrm{poly}\log (T)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7Bpoly%7D%5Clog+%28T%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\textrm{poly}\log (T)"/> bound for online interval discrepancy. Our framework is powerful enough to capture other well-studied geometric discrepancy problems; e.g., we obtain a <img alt="\textrm{poly}(\log^d (T))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7Bpoly%7D%28%5Clog%5Ed+%28T%29%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\textrm{poly}(\log^d (T))"/> bound for the online <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=fff&amp;fg=444444&amp;s=0" title="d"/>-dimensional Tusnády’s problem. All our bounds are tight up to polynomial factors.</p>
<p>A key new technical ingredient in our work is an anti-concentration inequality for sums of pairwise uncorrelated random variables, which might also be of independent interest.</p>
<p>Based on joint works with Nikhil Bansal, Haotian Jiang, Janardhan Kulkarni, and Makrand Sinha. Part of this work appears in STOC 2020 and is available at <a href="https://arxiv.org/abs/1912.03350" rel="nofollow">https://arxiv.org/abs/1912.03350</a></p></blockquote>
<p> </p></div>
    </content>
    <updated>2020-05-09T01:20:02Z</updated>
    <published>2020-05-09T01:20:02Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-05-17T22:39:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/05/08/14-phd-positions-in-stochastics-and-algorithmics-cofund-at-networks-apply-by-may-31-2020/</id>
    <link href="https://cstheory-jobs.org/2020/05/08/14-phd-positions-in-stochastics-and-algorithmics-cofund-at-networks-apply-by-may-31-2020/" rel="alternate" type="text/html"/>
    <title>14 PhD Positions in Stochastics and Algorithmics (COFUND) at NETWORKS (apply by May 31, 2020)</title>
    <summary>The research project NETWORKS is looking for 14 international PhD students in mathematics and computer science. Are you interested in the stochastics and algorithmics behind network problems? And would you like to be part of this project with its many activities? Then we invite you to apply for one of these positions. Website: https://www.thenetworkcenter.nl/Open-Positions/openposition/29/14-PhD-Positions-in-Stochastics-and-Algorithmics-COFUND- Email: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The research project NETWORKS is looking for 14 international PhD students in mathematics and computer science. Are you interested in the stochastics and algorithmics behind network problems? And would you like to be part of this project with its many activities? Then we invite you to apply for one of these positions.</p>
<p>Website: <a href="https://www.thenetworkcenter.nl/Open-Positions/openposition/29/14-PhD-Positions-in-Stochastics-and-Algorithmics-COFUND-">https://www.thenetworkcenter.nl/Open-Positions/openposition/29/14-PhD-Positions-in-Stochastics-and-Algorithmics-COFUND-</a><br/>
Email: info@thenetworkcenter.nl</p></div>
    </content>
    <updated>2020-05-08T13:12:32Z</updated>
    <published>2020-05-08T13:12:32Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-05-17T22:33:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=19821</id>
    <link href="https://gilkalai.wordpress.com/2020/05/08/to-cheer-you-up-in-difficult-times-3-a-guest-post-by-noam-lifshitz-on-the-new-hypercontractivity-inequality-of-peter-keevash-noam-lifshitz-eoin-long-and-dor-minzer/" rel="alternate" type="text/html"/>
    <title>To cheer you up in difficult times 3: A guest post by Noam Lifshitz on the new hypercontractivity inequality of Peter Keevash, Noam Lifshitz, Eoin Long and Dor Minzer</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This is a guest post kindly contributed by Noam Lifshitz. My short introduction: There is nothing like a new hypercontractivity inequality to cheer you up in difficult times and this post describes an amazing new hypercontractivity inequality.  The post describes … <a href="https://gilkalai.wordpress.com/2020/05/08/to-cheer-you-up-in-difficult-times-3-a-guest-post-by-noam-lifshitz-on-the-new-hypercontractivity-inequality-of-peter-keevash-noam-lifshitz-eoin-long-and-dor-minzer/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>This is a guest post kindly contributed by Noam Lifshitz</em>.</p>
<p>My short introduction: There is nothing like a new hypercontractivity inequality to cheer you up in difficult times and this post describes an amazing new hypercontractivity inequality.  The post describes a recent hypercontractive inequality by Peter Keevash, Noam Lifshitz, Eoin Long and Dor Minzer (KLLM) from their paper: <a href="https://arxiv.org/abs/1906.05568">Hypercontractivity for global functions and sharp thresholds</a>. (We reported on this development in <a href="https://gilkalai.wordpress.com/2018/10/30/exciting-beginning-of-the-year-activities-and-seminars/">this post</a>. By now, there are quite a few important applications.) And for Talagrand’s generic chaining inequality, see this <a href="https://lucatrevisan.wordpress.com/2020/05/03/talagrands-generic-chaining/">beautiful blog post</a> by Luca Trevisan.</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/04/barrysimon.jpg"><img alt="" class="alignnone size-full wp-image-19825" src="https://gilkalai.files.wordpress.com/2020/04/barrysimon.jpg?w=640"/></a></p>
<p><span style="color: #ff0000;">Barry Simon coined the term “hypercontractivity” in the 70s.  (We asked about it here and Nick Read was the first to answer.) A few months ago Barry told us about the early history of hypercontractivity inequalities, and, in particular, the very entertaining story on William Beckner’s Ph. D. qualifying exam.</span></p>
<p>And now to Noam Lifshitz’s guest post.</p>
<h2>Hypercontractivity on product spaces</h2>
<p>Analysis of Boolean functions (ABS) is a very rich subject. There are many works whose concern is generalising some of the results on analysis of Boolean functions to other (product) settings, such as functions on the multicube <img alt="{\left[m\right]^{n},}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%5Bm%5Cright%5D%5E%7Bn%7D%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\left[m\right]^{n},}"/> where <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> is very large. However, in some of these cases the fundemental tools of AOBF seem to be false for functions on the multicube <img alt="{f\colon\left[m\right]^{n}\rightarrow\mathbb{R}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Ccolon%5Cleft%5Bm%5Cright%5D%5E%7Bn%7D%5Crightarrow%5Cmathbb%7BR%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f\colon\left[m\right]^{n}\rightarrow\mathbb{R}.}"/> However, in the recent work of Keevash, Long, Minzer, and I. We introduce the notion of global functions. These are functions that do not strongly depend on a small set of coordinates. We then show that most of the rich classical theory of AOBF can in fact be generalised to these global functions. Using our machinery we were able to strengthen an isoperimetric stability result of Bourgain, and to make progress on some Erdos-Ko-Rado type open problem.</p>
<p>We now discuss some background on the Fourier analysis on functions on the multicube <img alt="{f\colon\left\{ 1,\ldots,m\right\} ^{n}\rightarrow\mathbb{R}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Ccolon%5Cleft%5C%7B+1%2C%5Cldots%2Cm%5Cright%5C%7D+%5E%7Bn%7D%5Crightarrow%5Cmathbb%7BR%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f\colon\left\{ 1,\ldots,m\right\} ^{n}\rightarrow\mathbb{R}.}"/></p>
<h3>Derivatives and Laplacians</h3>
<p>There are two fundemental types of operators on Boolean functions <img alt="{f\colon\left\{ 0,1\right\} ^{n}\rightarrow\mathbb{R}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Ccolon%5Cleft%5C%7B+0%2C1%5Cright%5C%7D+%5E%7Bn%7D%5Crightarrow%5Cmathbb%7BR%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f\colon\left\{ 0,1\right\} ^{n}\rightarrow\mathbb{R}.}"/> The first ones are the discrete derivatives, defined by <img alt="{D_{i}[f]=\frac{f_{i\rightarrow1}-f_{i\rightarrow0}}{2},}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_%7Bi%7D%5Bf%5D%3D%5Cfrac%7Bf_%7Bi%5Crightarrow1%7D-f_%7Bi%5Crightarrow0%7D%7D%7B2%7D%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_{i}[f]=\frac{f_{i\rightarrow1}-f_{i\rightarrow0}}{2},}"/> where <img alt="{f_{i\rightarrow x}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_%7Bi%5Crightarrow+x%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_{i\rightarrow x}}"/> denotes the we plug in the value <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> for the <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>th coordinate. The other closely related ones are the laplacians defined by <img alt="{L_{i}f\left(x\right):=f\left(x\right)-\mathbb{E}f\left(\mathbf{y}\right),}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7Bi%7Df%5Cleft%28x%5Cright%29%3A%3Df%5Cleft%28x%5Cright%29-%5Cmathbb%7BE%7Df%5Cleft%28%5Cmathbf%7By%7D%5Cright%29%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{i}f\left(x\right):=f\left(x\right)-\mathbb{E}f\left(\mathbf{y}\right),}"/> where <img alt="{\mathbf{y}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7By%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{y}}"/> is obtained from <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> by resampling its <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>th coordinate.</p>
<p>The laplacians and the derivatives are closely related. In fact, when we plug in <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> in the <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>th coordinate, we obtain <img alt="{L_{i}[f]_{i\rightarrow1}=D_{i}[f]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7Bi%7D%5Bf%5D_%7Bi%5Crightarrow1%7D%3DD_%7Bi%7D%5Bf%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{i}[f]_{i\rightarrow1}=D_{i}[f]}"/>, and when we plug in <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> in it, we obtain <img alt="{L_{i}[f]_{i\rightarrow0}=-D_{i}[f].}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7Bi%7D%5Bf%5D_%7Bi%5Crightarrow0%7D%3D-D_%7Bi%7D%5Bf%5D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{i}[f]_{i\rightarrow0}=-D_{i}[f].}"/></p>
<p>The 2-norm of the <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>th derivative is called the <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>th influence of <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> as it measures the impact of the <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>th coordinate on the value of <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/>. It’s usually denoted by <img alt="{\mathrm{Inf}_{i}[f]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BInf%7D_%7Bi%7D%5Bf%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathrm{Inf}_{i}[f]}"/>.</p>
<h3>Generalisation to functions on the multicube</h3>
<p>For functions on the multicube we don’t have a very good notion of a discrete derivative, but it turns out that it will be enough to talk about the laplacians and their restrictions. The Laplacians are again defined via <img alt="{L_{i}f\left(x\right):=f\left(x\right)-\mathbb{E}f\left(\mathbf{y}\right),}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7Bi%7Df%5Cleft%28x%5Cright%29%3A%3Df%5Cleft%28x%5Cright%29-%5Cmathbb%7BE%7Df%5Cleft%28%5Cmathbf%7By%7D%5Cright%29%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{i}f\left(x\right):=f\left(x\right)-\mathbb{E}f\left(\mathbf{y}\right),}"/> where <img alt="{\mathbf{y}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7By%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{y}}"/> is obtained from <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> by resampling its <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>th coordinate. It turns out that in the continuous cube it’s not enough to talk about Laplacians of coordinate, and we will also have to concern ourselves with Laplacians of sets. We define the generalised Laplacians of a set <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> by composing the laplacians corresponding to each coordinate in <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> <img alt="{L_{\left\{ i_{1},i_{2},\ldots,i_{r}\right\} }\left[f\right]:=L_{i_{1}}\circ\cdots\circ L_{i_{r}}\left[f\right].}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7B%5Cleft%5C%7B+i_%7B1%7D%2Ci_%7B2%7D%2C%5Cldots%2Ci_%7Br%7D%5Cright%5C%7D+%7D%5Cleft%5Bf%5Cright%5D%3A%3DL_%7Bi_%7B1%7D%7D%5Ccirc%5Ccdots%5Ccirc+L_%7Bi_%7Br%7D%7D%5Cleft%5Bf%5Cright%5D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{\left\{ i_{1},i_{2},\ldots,i_{r}\right\} }\left[f\right]:=L_{i_{1}}\circ\cdots\circ L_{i_{r}}\left[f\right].}"/></p>
<p>We now need to convince ourselves that these laplacians have something to do with the impact of <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> on the outcome of <img alt="{f.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f.}"/> In fact, the following notions are equivalent</p>
<ol>
<li>For each <img alt="{x,y\in\left[m\right]^{S}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%5Cin%5Cleft%5Bm%5Cright%5D%5E%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x,y\in\left[m\right]^{S}}"/>we have <img alt="{\|f_{S\rightarrow x}-f_{S\rightarrow y}\|_{2}&lt;\delta_{1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7Cf_%7BS%5Crightarrow+x%7D-f_%7BS%5Crightarrow+y%7D%5C%7C_%7B2%7D%3C%5Cdelta_%7B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\|f_{S\rightarrow x}-f_{S\rightarrow y}\|_{2}&lt;\delta_{1}}"/></li>
<li>For each <img alt="{x\in\left[m\right]^{S}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin%5Cleft%5Bm%5Cright%5D%5E%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x\in\left[m\right]^{S}}"/> we have <img alt="{\|L_{S}[f]_{S\rightarrow x}\|_{2}&lt;\delta_{2},}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7CL_%7BS%7D%5Bf%5D_%7BS%5Crightarrow+x%7D%5C%7C_%7B2%7D%3C%5Cdelta_%7B2%7D%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\|L_{S}[f]_{S\rightarrow x}\|_{2}&lt;\delta_{2},}"/></li>
</ol>
<p>in the sense that if (1) holds then (2) holds with <img alt="{\delta_{2}=C^{\left|S\right|}\delta_{1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta_%7B2%7D%3DC%5E%7B%5Cleft%7CS%5Cright%7C%7D%5Cdelta_%7B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta_{2}=C^{\left|S\right|}\delta_{1}}"/> and conversely if (2) holds, then (1) holds with <img alt="{\delta_{1}=C^{\left|S\right|}\delta_{2}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta_%7B1%7D%3DC%5E%7B%5Cleft%7CS%5Cright%7C%7D%5Cdelta_%7B2%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta_{1}=C^{\left|S\right|}\delta_{2}.}"/></p>
<p>The main theme of our work is that one can understand <strong>global</strong> function on the continuous cube, and these are functions that satisfy the above equivalent notions for all small sets <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>.</p>
<h3>Noise operator, hypercontractivity, and small set expansion</h3>
<p>For <img alt="{\rho\in\left(0,1\right),}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crho%5Cin%5Cleft%280%2C1%5Cright%29%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\rho\in\left(0,1\right),}"/> the noise operator is given by <img alt="{\mathrm{T}_{\rho}\left[f\right]\left(x\right)=\mathbb{E}f\left(\mathbf{y}\right)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BT%7D_%7B%5Crho%7D%5Cleft%5Bf%5Cright%5D%5Cleft%28x%5Cright%29%3D%5Cmathbb%7BE%7Df%5Cleft%28%5Cmathbf%7By%7D%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathrm{T}_{\rho}\left[f\right]\left(x\right)=\mathbb{E}f\left(\mathbf{y}\right)}"/> when <img alt="{\mathbf{y}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7By%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{y}}"/> is obtained from <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> by independently setting each coordinate <img alt="{\mathbf{y}_{i}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7By%7D_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{y}_{i}}"/> to be <img alt="{\mathbf{x}_{i}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7Bx%7D_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{x}_{i}}"/> with probability <img alt="{\rho}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crho%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\rho}"/> and resampling it with uniformly out of <img alt="{\left\{ -1,1\right\} }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%5C%7B+-1%2C1%5Cright%5C%7D+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\left\{ -1,1\right\} }"/> otherwise. The process which given <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> outputs <img alt="{\mathbf{y}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7By%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{y}}"/> is called the <img alt="{\rho}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crho%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\rho}"/>-noisy process, and we write <img alt="{\mathbf{y}\sim N_{\rho}\left(x\right).}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7By%7D%5Csim+N_%7B%5Crho%7D%5Cleft%28x%5Cright%29.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{y}\sim N_{\rho}\left(x\right).}"/></p>
<p>The Bonami hypercontractivity theorem, which was then generalised by Gross and Beckner states that the noise operator <img alt="{T_{\frac{1}{\sqrt{3}}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_%7B%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_{\frac{1}{\sqrt{3}}}}"/> is a contraction from <img alt="{L^{2}\left(\left\{ 0,1\right\} ^{n}\right)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%5E%7B2%7D%5Cleft%28%5Cleft%5C%7B+0%2C1%5Cright%5C%7D+%5E%7Bn%7D%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L^{2}\left(\left\{ 0,1\right\} ^{n}\right)}"/> to <img alt="{L^{4}\left(\left\{ 0,1\right\} ^{n}\right),}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%5E%7B4%7D%5Cleft%28%5Cleft%5C%7B+0%2C1%5Cright%5C%7D+%5E%7Bn%7D%5Cright%29%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L^{4}\left(\left\{ 0,1\right\} ^{n}\right),}"/> i.e.</p>
<blockquote><p><img alt="\displaystyle \|\mathrm{T}_{\frac{1}{\sqrt{3}}}f\|_{4}\le\|f\|_{2} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5C%7C%5Cmathrm%7BT%7D_%7B%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%7Df%5C%7C_%7B4%7D%5Cle%5C%7Cf%5C%7C_%7B2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \|\mathrm{T}_{\frac{1}{\sqrt{3}}}f\|_{4}\le\|f\|_{2} "/><br/>
for any function <img alt="{f.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f.}"/></p></blockquote>
<p>One consequence of the hypercontractivity theorem is the small set expansion theorem of KKL. It concerns fixed <img alt="{\rho\in\left(0,1\right)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crho%5Cin%5Cleft%280%2C1%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\rho\in\left(0,1\right)}"/> and a sequence of sets <img alt="{A_{n}\subseteq\{0,1\}^{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7Bn%7D%5Csubseteq%5C%7B0%2C1%5C%7D%5E%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_{n}\subseteq\{0,1\}^{n}}"/> with <img alt="{\left|A_{n}\right|=o\left(2^{n}\right).}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%7CA_%7Bn%7D%5Cright%7C%3Do%5Cleft%282%5E%7Bn%7D%5Cright%29.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\left|A_{n}\right|=o\left(2^{n}\right).}"/> The small set expansion theorem states that if we choose <img alt="{\mathbf{x}\sim A_{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7Bx%7D%5Csim+A_%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{x}\sim A_{n}}"/> uniformly and a noisy <img alt="{\mathbf{y}\sim N_{\rho}\left(\mathbf{x}\right),}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7By%7D%5Csim+N_%7B%5Crho%7D%5Cleft%28%5Cmathbf%7Bx%7D%5Cright%29%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{y}\sim N_{\rho}\left(\mathbf{x}\right),}"/> then <img alt="{\mathbf{y}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7By%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{y}}"/> will reside outside of <img alt="{A_{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_{n}}"/> almost surely.</p>
<h3>The Generalisation to the multicube:</h3>
<p>The small set expansion theorem and the hypercontractivity theorem both fail for function on the multicube that are of a very local nature. For instance, let <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> be the set of all <img alt="{x\in\left\{ 1,\ldots,m\right\} ^{n},}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin%5Cleft%5C%7B+1%2C%5Cldots%2Cm%5Cright%5C%7D+%5E%7Bn%7D%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x\in\left\{ 1,\ldots,m\right\} ^{n},}"/> such that <img alt="{x_{1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_%7B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_{1}}"/> is <img alt="{m.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m.}"/> Then <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> is of size <img alt="{m^{n-1},}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%5E%7Bn-1%7D%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m^{n-1},}"/> which is <img alt="{o\left(m^{n}\right)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bo%5Cleft%28m%5E%7Bn%7D%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{o\left(m^{n}\right)}"/> if we allow <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> to be a growing function of <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>. However, the <img alt="{\rho}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crho%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\rho}"/>-noisy process from the set stays within the set with probability <img alt="{\rho.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crho.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\rho.}"/> For a similar reason the hypercontractivity theorem fails as is for functions on <img alt="{\left\{ 1,\ldots,m\right\} ^{n}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%5C%7B+1%2C%5Cldots%2Cm%5Cright%5C%7D+%5E%7Bn%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\left\{ 1,\ldots,m\right\} ^{n}.}"/> However we were able to generalise the hypercontractivity theorem by taking the globalness of <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> into consideration.</p>
<p>Our main hypercontractive inequality is the following</p>
<p><strong>Theorem 1.</strong></p>
<blockquote><p><img alt="\displaystyle \|\mathrm{T}_{\frac{1}{100}}f\|_{4}^{4}\le\sum_{S\subseteq\left[n\right]}\mathbb{E}_{\mathbf{x}\sim\left\{ 1,\ldots,m\right\} ^{m}}\left(\|L_{S}\left[f\right]_{S\rightarrow\mathbf{x}}\|_{2}^{4}\right). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5C%7C%5Cmathrm%7BT%7D_%7B%5Cfrac%7B1%7D%7B100%7D%7Df%5C%7C_%7B4%7D%5E%7B4%7D%5Cle%5Csum_%7BS%5Csubseteq%5Cleft%5Bn%5Cright%5D%7D%5Cmathbb%7BE%7D_%7B%5Cmathbf%7Bx%7D%5Csim%5Cleft%5C%7B+1%2C%5Cldots%2Cm%5Cright%5C%7D+%5E%7Bm%7D%7D%5Cleft%28%5C%7CL_%7BS%7D%5Cleft%5Bf%5Cright%5D_%7BS%5Crightarrow%5Cmathbf%7Bx%7D%7D%5C%7C_%7B2%7D%5E%7B4%7D%5Cright%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \|\mathrm{T}_{\frac{1}{100}}f\|_{4}^{4}\le\sum_{S\subseteq\left[n\right]}\mathbb{E}_{\mathbf{x}\sim\left\{ 1,\ldots,m\right\} ^{m}}\left(\|L_{S}\left[f\right]_{S\rightarrow\mathbf{x}}\|_{2}^{4}\right). "/></p></blockquote>
<p>The terms <img alt="{\|L_{S}\left[f\right]_{S\rightarrow x}\|_{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7CL_%7BS%7D%5Cleft%5Bf%5Cright%5D_%7BS%5Crightarrow+x%7D%5C%7C_%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\|L_{S}\left[f\right]_{S\rightarrow x}\|_{2}}"/> appearing on the right hand side are small whenever <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> has a small dependency on <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> and it turns out that you have the following corrolary of it, which looks a bit more similar to the hypercontractive intequality.</p>
<p> </p>
<p><strong>Corollary 2.</strong></p>
<p>Let <img alt="{f\colon\left\{ 1,\ldots,m\right\} ^{n}\rightarrow\mathbb{R}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Ccolon%5Cleft%5C%7B+1%2C%5Cldots%2Cm%5Cright%5C%7D+%5E%7Bn%7D%5Crightarrow%5Cmathbb%7BR%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f\colon\left\{ 1,\ldots,m\right\} ^{n}\rightarrow\mathbb{R}}"/>, and uppose that <img alt="{\|L_{S}[f]_{S\rightarrow x}\|_{2}\le4^{\left|S\right|}\|f\|_{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7CL_%7BS%7D%5Bf%5D_%7BS%5Crightarrow+x%7D%5C%7C_%7B2%7D%5Cle4%5E%7B%5Cleft%7CS%5Cright%7C%7D%5C%7Cf%5C%7C_%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\|L_{S}[f]_{S\rightarrow x}\|_{2}\le4^{\left|S\right|}\|f\|_{2}}"/> for all sets <img alt="{S.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S.}"/></p>
<p>Then <img alt="{\mathrm{\|\mathrm{T}_{\frac{1}{1000}}f\|_{4}\le\|f\|_{2}.}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7B%5C%7C%5Cmathrm%7BT%7D_%7B%5Cfrac%7B1%7D%7B1000%7D%7Df%5C%7C_%7B4%7D%5Cle%5C%7Cf%5C%7C_%7B2%7D.%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathrm{\|\mathrm{T}_{\frac{1}{1000}}f\|_{4}\le\|f\|_{2}.}}"/></p>
<p>Finally, one might ask wonder why this globalness notion appears only when we look at large values of <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> and not when <img alt="{m=2.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%3D2.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m=2.}"/> I think the corollary is a good explanation for that as <img alt="{\|f\|_{2}^{2}\ge\left(\frac{1}{2}\right)^{\left|S\right|}\|f_{S\rightarrow x}\|_{2}^{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7Cf%5C%7C_%7B2%7D%5E%7B2%7D%5Cge%5Cleft%28%5Cfrac%7B1%7D%7B2%7D%5Cright%29%5E%7B%5Cleft%7CS%5Cright%7C%7D%5C%7Cf_%7BS%5Crightarrow+x%7D%5C%7C_%7B2%7D%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\|f\|_{2}^{2}\ge\left(\frac{1}{2}\right)^{\left|S\right|}\|f_{S\rightarrow x}\|_{2}^{2}}"/> holds trivially for any Boolean function <img alt="{f\colon\left\{ 0,1\right\} ^{n}\rightarrow\mathbb{R}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Ccolon%5Cleft%5C%7B+0%2C1%5Cright%5C%7D+%5E%7Bn%7D%5Crightarrow%5Cmathbb%7BR%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f\colon\left\{ 0,1\right\} ^{n}\rightarrow\mathbb{R}.}"/></p></div>
    </content>
    <updated>2020-05-07T21:01:42Z</updated>
    <published>2020-05-07T21:01:42Z</published>
    <category term="Analysis"/>
    <category term="Combinatorics"/>
    <category term="Computer Science and Optimization"/>
    <category term="Guest post"/>
    <category term="Poetry"/>
    <category term="Probability"/>
    <category term="Barry Simon"/>
    <category term="Dor Minzer"/>
    <category term="Eoin Long"/>
    <category term="Noam Lifshitz"/>
    <category term="Peter Keevash"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2020-05-17T22:28:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://dstheory.wordpress.com/?p=48</id>
    <link href="https://dstheory.wordpress.com/2020/05/05/friday-may-15-amin-karbasi-from-yale-university/" rel="alternate" type="text/html"/>
    <title>Friday, May 15 — Amin Karbasi from Yale University</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The fourth Foundations of Data Science virtual talk will take place on Friday, May 15th at 11:00 AM Pacific Time (2:00 pm Eastern Time, 20:00 Central European Time, 19:00 UTC).  Amin Karbasi from Yale University will speak about “User-Friendly Submodular Maximization”. Abstract: Submodular functions model the intuitive notion of diminishing returns. Due to their far-reaching applications, they<a class="more-link" href="https://dstheory.wordpress.com/2020/05/05/friday-may-15-amin-karbasi-from-yale-university/">Continue reading <span class="screen-reader-text">"Friday, May 15 — Amin Karbasi from Yale University"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The fourth Foundations of Data Science virtual talk will take place on Friday, May 15th at 11:00 AM Pacific Time (2:00 pm Eastern Time, 20:00 Central European Time, 19:00 UTC).  <strong>Amin Karbasi </strong>from Yale University will speak about “<em>User-Friendly Submodular Maximization</em>”.</p>



<p class="has-text-align-left"><strong>Abstract</strong>: Submodular functions model the intuitive notion of diminishing returns. Due to their far-reaching applications, they have been rediscovered in many fields such as information theory, operations research, statistical physics, economics, and machine learning. They also enjoy computational tractability as they can be minimized exactly or maximized approximately.</p>



<p>The goal of this talk is simple. We see how a little bit of randomness, a little bit of greediness, and the right combination can lead to pretty good methods for offline, streaming, and distributed solutions. I do not assume any background on submodularity and try to explain all the required details during the talk.</p>



<p><a href="https://sites.google.com/view/dstheory" rel="noreferrer noopener" target="_blank">Please register here to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>
    </content>
    <updated>2020-05-05T01:30:02Z</updated>
    <published>2020-05-05T01:30:02Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>dstheory</name>
    </author>
    <source>
      <id>https://dstheory.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://dstheory.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://dstheory.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://dstheory.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://dstheory.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Foundation of Data Science – Virtual Talk Series</title>
      <updated>2020-05-17T22:48:14Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/05/04/postdoc-position-at-university-of-alberta-apply-by-december-31-2020/</id>
    <link href="https://cstheory-jobs.org/2020/05/04/postdoc-position-at-university-of-alberta-apply-by-december-31-2020/" rel="alternate" type="text/html"/>
    <title>postdoc position at University of Alberta (apply by December 31, 2020)</title>
    <summary>The Theory Group in the Dept. of Computing Science at U. of Alberta invites applications for TWO postdoc positions. The successful applicants are expected to work closely with Zachary Friggstad and Mohammad Salavatipour in the areas of: approx algorithms, hardness of approximation, combinatorial optimization. For details see https://webdocs.cs.ualberta.ca/~mreza/pdf-ad7.pdf Email: mrs@ualberta.ca Website: http://www.cs.ualberta.ca Email: mrs@ualberta.ca</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Theory Group in the Dept. of Computing Science at U. of Alberta invites applications for TWO postdoc positions. The successful applicants are expected to work closely with Zachary Friggstad and Mohammad Salavatipour in the areas of: approx algorithms, hardness of approximation, combinatorial optimization. For details see <a href="https://webdocs.cs.ualberta.ca/~mreza/pdf-ad7.pdf">https://webdocs.cs.ualberta.ca/~mreza/pdf-ad7.pdf</a><br/>
Email: mrs@ualberta.ca</p>
<p>Website: <a href="http://www.cs.ualberta.ca">http://www.cs.ualberta.ca</a><br/>
Email: mrs@ualberta.ca</p></div>
    </content>
    <updated>2020-05-04T18:43:57Z</updated>
    <published>2020-05-04T18:43:57Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-05-17T22:32:58Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/05/03/hanoi-vs-sierpinski</id>
    <link href="https://11011110.github.io/blog/2020/05/03/hanoi-vs-sierpinski.html" rel="alternate" type="text/html"/>
    <title>Hanoi vs Sierpiński</title>
    <summary>The Hanoi graphs and Sierpiński graphs both look like the Sierpiński triangle, and have a very similar recursive construction from triples of smaller graphs of the same type, but they are not quite the same graphs as each other. The Sierpiński graphs (left, below) are the graphs of the vertices and boundary edges of partially-constructed Sierpiński triangles; they can also be formed from three smaller Sierpiński graphs by identifying pairs of extreme vertices (the vertices of degree two at the three corners of the triangular layout). The Hanoi graphs (right, below) are the state spaces of the tower of Hanoi puzzle, in which rings of different size are moved one at a time between three pegs, only allowing moves that keep the rings sorted on each peg. They also have a construction from three smaller Hanoi graphs, but where the pairs of extreme vertices are connected by an edge rather than identified.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Hanoi graphs and Sierpiński graphs both look like the <a href="https://en.wikipedia.org/wiki/Sierpi%C5%84ski_triangle">Sierpiński triangle</a>, and have a very similar recursive construction from triples of smaller graphs of the same type, but they are not quite the same graphs as each other. The Sierpiński graphs (left, below) are the graphs of the vertices and boundary edges of partially-constructed Sierpiński triangles; they can also be formed from three smaller Sierpiński graphs by identifying pairs of extreme vertices (the vertices of degree two at the three corners of the triangular layout). The <a href="https://en.wikipedia.org/wiki/Hanoi_graph">Hanoi graphs</a> (right, below) are the state spaces of the tower of Hanoi puzzle, in which rings of different size are moved one at a time between three pegs, only allowing moves that keep the rings sorted on each peg. They also have a construction from three smaller Hanoi graphs, but where the pairs of extreme vertices are connected by an edge rather than identified.</p>

<p style="text-align: center;"><img alt="Hanoi graphs and Sierpi&#x144;ski graphs" src="https://11011110.github.io/blog/assets/2020/hanoi-vs-sierpinski.svg"/></p>

<p>The difference between them comes out much more strongly when you generalize them to higher dimensions. The Sierpiński triangle generalizes to tetrahedra (a popular shape for kites) and higher-dimensional simplices; <a href="https://commons.wikimedia.org/wiki/File:Alexander_Graham_Bell_facing_his_wife,_Mabel_Hubbard_Gardiner_Bell,_who_is_standing_in_a_tetrahedral_kite,_Baddeck,_Nova_Scotia.tif">the photo below</a> shows <a href="https://en.wikipedia.org/wiki/Mabel_Gardiner_Hubbard">Mabel Bell</a> and <a href="https://en.wikipedia.org/wiki/Alexander_Graham_Bell">Alexander Graham Bell</a>, seemingly about to kiss, in a three-dimensional Sierpiński graph, the framework for a kite.</p>

<p style="text-align: center;"><img alt="Mabel Bell and Alexander Graham Bell kissing in a Sierpi&#x144;ski tetrahedron kite frame, from https://commons.wikimedia.org/wiki/File:Alexander_Graham_Bell_facing_his_wife,_Mabel_Hubbard_Gardiner_Bell,_who_is_standing_in_a_tetrahedral_kite,_Baddeck,_Nova_Scotia.tif" src="https://11011110.github.io/blog/assets/2020/bell-kite-kiss.jpg"/></p>

<p>Again, the -dimensional Sierpiński graph has a recursive construction from  smaller graphs of the same type, identified at extreme vertices (the vertices of degree  at the  corners of the layout). Because the number of vertices separating the subgraphs at each level of the recursion is so small, these graphs have bounded <a href="https://en.wikipedia.org/wiki/Treewidth">treewidth</a>, and a few years ago on the TCS stackexchange <a href="https://cstheory.stackexchange.com/q/36542">I calculated the treewidth of the Sierpiński triangle graphs explicitly as being four</a>. The same bound transfers easily enough to the three-peg Hanoi graphs.</p>

<p>The analogue of higher dimensions for the Hanoi graphs is to use more pegs. The Hanoi graph with  pegs and  rings has  states, more or less the same as the Sierpiński graph for -dimensional Sierpiński fractals with  levels of recursion. Here’s the one with two rings; each state is described by a pair of letters, using a capital letter for the peg holding the larger ring and a lowercase letter for the peg holding the smaller ring.</p>

<p style="text-align: center;"><img alt="Hanoi graph for two rings on four pegs" src="https://11011110.github.io/blog/assets/2020/Hanoi-4-2.svg"/></p>

<p>The recursive construction for these graphs combines  copies of a smaller graph of the same type: one copy for each position where the largest ring can be placed, and a smaller graph describing the placements of the smaller rings once the largest ring has been placed. These copies of the smaller graph are connected together by edges describing the movements of the largest ring. But I’ve only drawn an example for two rings because these graphs get messy and hard to draw very quickly. The reason is not the exponential number of total vertices, but the large number of connections from one recursive subgraph to another. Two recursive subgraphs are connected whenever the largest ring can move from its peg in one subgraph to its peg in the other, and this is allowed whenever these two pegs have no smaller rings on them. So in a Hanoi graph with  pegs,  rings, and  vertices, each pair of recursive subgraphs has  edges between them, one for each placement of the smaller rings on the  remaining pegs.</p>

<p>The recursive subdivision with  edges between subgraphs leads to a tree-decomposition with treewidth , and this naturally raises the question of whether this is tight or whether some other less-intuitive recursive decomposition has smaller cuts between its recursive subgraphs. This is the question studied in my newest preprint, “On the treewidth of Hanoi graphs” (<a href="https://arxiv.org/abs/2005.00179">arXiv:2005.00179</a>), with UCI student Daniel Frishberg and Oregon State student Will Maxwell, to appear at <a href="https://sites.google.com/view/fun2020/home">FUN 2020</a> (supposedly to be held in person in September in Italy after being rescheduled from June, but I’m not holding my breath). We don’t get a precise answer, but we succeed in proving bounds on the treewidth of the form <span style="white-space: nowrap;">.</span> This is nearly tight for fixed  and variable : we get the same exponential function of  as the upper bound, and are smaller than the upper bound by only a much lower-order polynomial factor. But the exact treewidth remains elusive.</p>

<p>To put it in a possibly more familiar form, when one of these graphs (for a fixed number of pegs and variable number of rings) has  vertices, it has separators of size , where . For the four-peg Hanoi graphs, this means separators of size , more or less the same as for planar graphs (although these graphs seem far from planar). But that nice exponent is just a coincidence caused by the fact that  is a power of . For other choices of , that doesn’t happen and we get a transcendental exponent . So these graphs don’t even act like -dimensional graphs, for which a reasonable separator exponent might be the rational number . And they certainly don’t act like the Sierpiński graphs, for which the exponent is zero.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/104108481482094736">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-05-03T22:03:00Z</updated>
    <published>2020-05-03T22:03:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-05-16T00:07:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://adamsheffer.wordpress.com/?p=5494</id>
    <link href="https://adamsheffer.wordpress.com/2020/05/03/math-summer-programs/" rel="alternate" type="text/html"/>
    <title>Math Summer Programs</title>
    <summary>The virus is causing some math summer programs to cancel. Surprisingly, this led to something wonderful. Unusually strong undergrads are starting to run their own online summer math programs for high school students. 1. The MORPH program is run by the Harvard math club. It offers a variety of mathematical topics at different levels of […]</summary>
    <updated>2020-05-03T01:11:48Z</updated>
    <published>2020-05-03T01:11:48Z</published>
    <category term="Math events"/>
    <author>
      <name>Adam Sheffer</name>
    </author>
    <source>
      <id>https://adamsheffer.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://adamsheffer.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://adamsheffer.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://adamsheffer.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://adamsheffer.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Discrete geometry and other typos</subtitle>
      <title>Some Plane Truths</title>
      <updated>2020-05-17T22:38:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17007</id>
    <link href="https://rjlipton.wordpress.com/2020/05/01/mathematics-of-covid-19/" rel="alternate" type="text/html"/>
    <title>Mathematics of COVID-19</title>
    <summary>Its not just [ Sir Francis Galton by Charles Wellington Furse ] Francis Galton is a perfect example of a Victorian era scientist. Sir Galton, he was knighted in 1909, had many roles: a statistician, a sociologist, a psychologist, an anthropologist, a tropical explorer, a geographer, a meteorologist, a geneticist, and an inventor. He coined […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Its not just <img alt="{R_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R_0}"/></em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/01/mathematics-of-covid-19/220px-sir_francis_galton_by_charles_wellington_furse/" rel="attachment wp-att-17012"><img alt="" class="alignright size-full wp-image-17012" src="https://rjlipton.files.wordpress.com/2020/05/220px-sir_francis_galton_by_charles_wellington_furse.jpg?w=600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ Sir Francis Galton by Charles Wellington Furse ]</font></td>
</tr>
</tbody>
</table>
<p>
Francis Galton is a perfect example of a Victorian era scientist. Sir Galton, he was knighted in 1909, had many roles: a statistician, a sociologist, a psychologist, an anthropologist, a tropical explorer, a geographer, a meteorologist, a geneticist, and an inventor. He coined the phrase “nature versus nurture” and <a href="https://en.wikipedia.org/wiki/Francis_Galton">more</a>.</p>
<p>
Today we trade in jokes for some mathematics of the virus.</p>
<p><span id="more-17007"/></p>
<p>
We wish there was something clever we can say about the spread of the virus. But the statistics of the spread are complex. We wish there was something theory can contribute to the fight against the virus. But the front-line is clearly dominated by medicine and biology.</p>
<p>
However there are two areas that are relevant. The first is the math of how fast the virus spreads and the second is how valid are the claims about the virus. The latter is an area where theory could play a role in the future.</p>
<p>
In preparing this discussion we noted that Galton was indeed an inventor. He invented a device to demonstrate the central limit theorem. You probably have seen some <a href="http://www.karlsims.com/marbles/">version</a>. Sometimes called the bean machine, it gives a visual demonstration of the central limit theorem. Other times it is called the Galton board. Perhaps if Galton were alive today he would be on cable news explaining how the virus spreads. </p>
<p>
<a href="https://rjlipton.wordpress.com/2020/05/01/mathematics-of-covid-19/marble-run/" rel="attachment wp-att-17009"><img alt="" class="aligncenter size-medium wp-image-17009" height="300" src="https://rjlipton.files.wordpress.com/2020/05/marble-run.jpg?w=168&amp;h=300" width="168"/></a></p>
<p>
Galton also had views that are troubling. See <a href="https://www.statisticsviews.com/details/news/11158556/June-2019-issue-of-Significance-just-published.html">this</a> for example. He lived over one hundred years ago, but his views on eugenics are still upsetting. Should we not have featured him? What do you think?</p>
<p/><h2> Extinction </h2><p/>
<p/><p>
The issue is will the terrible virus stop infecting people? Will it become extinct? Or will it at least stop infecting more and more people. The part of math that studies such questions was invented by Galton in 1889 as a model to track family names. We wish we were talking today about family names and not a killer virus. The area he invented is now called <a href="https://en.wikipedia.org/wiki/Branching_process">branching process</a>. </p>
<blockquote><p><b> </b> <em> There was concern amongst the Victorians that aristocratic surnames were becoming extinct. Galton originally posed the question regarding the probability of such an event in an 1873 issue of The Educational Times, and the Reverend Henry Watson replied with a solution. Together, they then wrote an 1874 paper entitled <a href="https://www.jstor.org/stable/2841222?origin=crossref&amp;seq=1#metadata_info_tab_contents">On the probability of the extinction of families</a>. </em>
</p></blockquote>
<p/><p>
We are interested in branching processes and when they are likely to become extinct. We want the virus to stop infecting people, and become extinct. Or at least stop its explosive growth that is so terrible. See these <a href="https://theconversation.com/how-to-flatten-the-curve-of-coronavirus-a-mathematician-explains-133514">comments</a>, for more information. </p>
<p>
<a href="https://rjlipton.wordpress.com/2020/05/01/mathematics-of-covid-19/fig/" rel="attachment wp-att-17010"><img alt="" class="aligncenter size-medium wp-image-17010" height="225" src="https://rjlipton.files.wordpress.com/2020/05/fig.png?w=300&amp;h=225" width="300"/></a></p>
<p>
</p><p/><h2> On Average </h2><p/>
<p/><p>
The contagiousness of a disease is described by its “reproduction rate” or the average number of people infected by one infectious person in a population without immunity. You might also hear this number referred to as the <img alt="{R_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R_0}"/> value. When it is less than <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>, the disease does not become an pandemic. This process is called a <i>branching process</i>. In order to tell if a branching process will eventually become extinct we need more than <img alt="{R_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R_0}"/>. That is we need to understand more than the average number of descendants. Let’s see why.</p>
<p>
Consider a process <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> that creates <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> descendants with probability <img alt="{a_{k}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba_%7Bk%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a_{k}}"/> and a process <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> that creates <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> descendants with probability <img alt="{b_{k}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb_%7Bk%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b_{k}}"/>. The number of average descendants is for <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> is 	</p>
<p align="center"><img alt="\displaystyle  \mu_{A} = a_{1} + 2a_{2} + \dots " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmu_%7BA%7D+%3D+a_%7B1%7D+%2B+2a_%7B2%7D+%2B+%5Cdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mu_{A} = a_{1} + 2a_{2} + \dots "/></p>
<p>and for <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> is 	</p>
<p align="center"><img alt="\displaystyle  \mu_{B} = b_{1} + 2b_{2} + \dots " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmu_%7BB%7D+%3D+b_%7B1%7D+%2B+2b_%7B2%7D+%2B+%5Cdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mu_{B} = b_{1} + 2b_{2} + \dots "/></p>
<p>Is it always better to have the process with the smaller average? The answer is no.</p>
<p>
Consider the process <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> so that 	</p>
<p align="center"><img alt="\displaystyle  a_{0} = 0, a_{1} = 1, a_{2} = \epsilon, a_{3} = 0, \dots " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++a_%7B0%7D+%3D+0%2C+a_%7B1%7D+%3D+1%2C+a_%7B2%7D+%3D+%5Cepsilon%2C+a_%7B3%7D+%3D+0%2C+%5Cdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  a_{0} = 0, a_{1} = 1, a_{2} = \epsilon, a_{3} = 0, \dots "/></p>
<p>And consider the process <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> so that 	</p>
<p align="center"><img alt="\displaystyle  b_{0} = 1/n, b_{1} = 1/n, \dots, b_{n} = 1/n, b_{n+1} = 0, \dots " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++b_%7B0%7D+%3D+1%2Fn%2C+b_%7B1%7D+%3D+1%2Fn%2C+%5Cdots%2C+b_%7Bn%7D+%3D+1%2Fn%2C+b_%7Bn%2B1%7D+%3D+0%2C+%5Cdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  b_{0} = 1/n, b_{1} = 1/n, \dots, b_{n} = 1/n, b_{n+1} = 0, \dots "/></p>
<p>The first average <img alt="{\mu_{A} = 1 + 2\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu_%7BA%7D+%3D+1+%2B+2%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mu_{A} = 1 + 2\epsilon}"/> and the second average is 	</p>
<p align="center"><img alt="\displaystyle  1/n + 2/n + \cdots + n/n = (n+1)/2. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1%2Fn+%2B+2%2Fn+%2B+%5Ccdots+%2B+n%2Fn+%3D+%28n%2B1%29%2F2.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  1/n + 2/n + \cdots + n/n = (n+1)/2. "/></p>
<p>Clearly the second has a much larger value for <img alt="{n \ge 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cge+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n \ge 2}"/>. But the first will never go extinct and the second can become extinct.</p>
<p>
</p><p/><h2> It’s In the Variance </h2><p/>
<p/><p>
The key difference is that the second process has higher <em>variance</em>. The importance of the variance—as opposed to the mean—is remembered in some ways but seems to be forgotten in others. It is the nub of one of the <a href="https://rjlipton.wordpress.com/2020/04/26/time-for-some-jokes/">jokes</a> we included in the previous post:</p>
<blockquote><p><b> </b> <em> There was a statistician who drowned crossing a river—that was only 3 feet deep on average. </em>
</p></blockquote>
<p/><p>
For an example closer to our point, suppose a third-party candidate <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> entering a race expects to take more votes away from candidate <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> than candidate <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> so as to double the margin that <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> expects to lose by. But suppose <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> alters the dynamics of the race so that the standard deviation is quadrupled. Then <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> generally has a better chance of winning under that scenario. If the distribution is normal and the original standard deviation equaled the expected margin, then <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/>‘s chances of winning improve from 16% to over 30%.</p>
<p>
In our case, “winning” means outcomes where the virus dies out, locally and ultimately globally. Such outcomes are needed for opening up. It is not enough to reduce the number of active cases to “<img alt="{O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(1)}"/>” because the branching started from such a state. Whatever active cases there are must be known and contained as well as <img alt="{O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(1)}"/>. This is the situation currently <a href="https://www.bbc.com/news/world-asia-52436658">claimed</a> in New Zealand.</p>
<p>
</p><p/><h2> Branching and Uncertainty </h2><p/>
<p/><p>
At the other end is the state where the virus is not contained but branching stops because of saturation. When the proportion of targeted descendants who have already had the virus and are (we hope) immune is greater than the branching factor, then the expectation takes over from the variance as a determinant of stoppage. This proportion is what is meant by “herd immunity” and is estimated to be 60–70% for this virus.</p>
<p>
What we feel is the central mystery is whether there are enough undetected cases to bring that point even possibly in range. Randomized testing in the New York area has found nearly a 25% rate of antibodies. Analogous <a href="https://www.nytimes.com/2020/04/21/health/coronavirus-antibodies-california.html">tests</a> in less-affected California and in other countries have found under 10% positive rates, however. Those results are subject to uncertainty in the representativeness of the samples.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
We had planned on saying something about checking if reported data about the virus is correct, or is it faked. With so much at stake it seems smart to insist on data being verified. More on that in the future.</p>
<p/></font></font></div>
    </content>
    <updated>2020-05-01T21:12:58Z</updated>
    <published>2020-05-01T21:12:58Z</published>
    <category term="Ideas"/>
    <category term="P=NP"/>
    <category term="People"/>
    <category term="Results"/>
    <category term="branching process"/>
    <category term="coronavirus"/>
    <category term="extinct"/>
    <category term="R0"/>
    <category term="virus"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-05-17T22:30:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=3460</id>
    <link href="https://francisbach.com/gradient-flows/" rel="alternate" type="text/html"/>
    <title>Effortless optimization through gradient flows</title>
    <summary>Optimization algorithms often rely on simple intuitive principles, but their analysis quickly leads to a lot of algebra, where the original idea is not transparent. In last month post, Adrien Taylor explained how convergence proofs could be automated. This month, I will show how proof sketches can be obtained easily for algorithms based on gradient...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">Optimization algorithms often rely on simple intuitive principles, but their analysis quickly leads to a lot of algebra, where the original idea is not transparent. In last month <a href="https://francisbach.com/computer-aided-analyses/">post</a>, <a href="https://www.di.ens.fr/~ataylor/">Adrien Taylor</a> explained how convergence proofs could be automated. This month, I will show how proof sketches can be obtained easily for algorithms based on gradient descent. This will be done using vanishing step-sizes that lead to <em>gradient flows</em>.</p>



<h2>Gradient as local information</h2>



<p class="justify-text">The intuitive principle behind gradient descent is the quest for <em>local</em> descent. We thus need to characterize the local behavior of the function we aim to optimize. This is what gradients are for.</p>



<p class="justify-text">In this blog post, I will consider minimizing a function \(f\) over \(\mathbb{R}^d\). Assuming \(f\) is differentiable, a first order Taylor expansion of \(f\) around a point \(x\) leads to $$f(x+\delta) = f(x) + \nabla f(x) ^\top \delta + o(\| \delta\|),$$ for any norm \(\| \cdot \|\) on \(\mathbb{R}^d\), where \(\nabla f(x) \in \mathbb{R}^d\) is  the gradient of \(f\) at \(x\), composed of partial derivatives of \(f\). Therefore, around \(x\), \(f\) is approximately affine.</p>



<p class="justify-text">Since we have a local affine approximation around \(x\), we can look for the direction of steepest descent, that is, the unit norm vector \(u \in \mathbb{R}^d\) such that \(f\) decays the most along \(u\), that is such that $$  u^\top \nabla f(x)$$ is minimized. This steepest descent direction depends on the choice of norm (assuming that the gradient is not zero at \(x\)).</p>



<p class="justify-text">For the \(\ell_2\)-norm, then minimizing \(u^\top \nabla f(x)\) such that \(\|u\|_2 = 1\), leads to $$ \displaystyle u = \ – \frac{\nabla f(x)}{ \| \nabla f(x) \|_2},$$ that is the steepest descent is along the negative gradient (see an illustration below). In this blog post I will only focus on this steepest descent direction. </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3542" height="149" src="https://francisbach.com/wp-content/uploads/2020/04/gradient_contours-1024x358.png" width="428"/>Function \(f\) represented through its contour lines for values 1, 2, 3, 4 and 5. Negative gradient \(– \nabla f(x)\) as the steepest descent direction at point \(x\), which is orthogonal to the contour lines.</figure></div>



<p class="justify-text">As as side note, for the \(\ell_1\)-norm, then minimizing \(u^\top \nabla f(x)\) such that \(\|u\|_1 = 1\), leads to $$u \in\  – \arg\max_{ v \in \{-e_1,\, e_1,\, -e_2,\, e_2,\dots,\, -e_d,\, e_d \}} v^\top \nabla f(x),$$ where \(e_i\) is the \(i\)-th canonical basis vector of \(\mathbb{R}^d\). Here the steepest descent is along a coordinate axis (along the positive or negative side), and this leads to various forms of <a href="https://en.wikipedia.org/wiki/Coordinate_descent">coordinate descent</a> (this will probably be a topic for another post). </p>



<p class="justify-text">Given that the negative gradient leads to the steepest descent direction (for the Euclidean norm), it is natural to use this as a direction for an iterative algorithm, an idea that dates back to <a href="https://en.wikipedia.org/wiki/Augustin-Louis_Cauchy">Cauchy</a> in 1847 [<a href="http://gallica.bnf.fr/ark:/12148/bpt6k90190w/f406">1</a>] (see the nice summary by Claude Le Maréchal [<a href="https://www.math.uni-bielefeld.de/documenta/vol-ismp/40_lemarechal-claude.pdf">2</a>]).</p>



<h2>From gradient descent to gradient flows</h2>



<p class="justify-text"><a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient descent</a> is the most classical iterative algorithm to minimize differentiable functions. It takes the form $$x_{n+1} = x_{n} \, – \gamma \nabla f(x_{n})$$ at iteration \(n\), where \(\gamma &gt; 0 \) is a step-size.  </p>



<p class="justify-text">Gradient descent comes in many flavors, steepest, stochastic, pre-conditioned, conjugate, proximal, projected, accelerated, etc. There are lots of papers and books [e.g., 3, 4, 5] analyzing it in various settings.</p>



<p class="justify-text">In this post, to simplify its analysis and setting the stage for later posts, I will present the gradient flow, which is essentially the limit of gradient descent when the step-size \(\gamma\) tends to zero.</p>



<p class="justify-text">More precisely, this is obtained by considering that our iterates \(x_n\) are sampled at each multiple of \(\gamma\), from a function \(X: \mathbb{R}_+ \to \mathbb{R}^d\), as $$x_n = X(n\gamma).$$ We can then use a piecewise affine interpolation to define a function defined on all points. We then have for \(t = n\gamma\), $$X(t + \gamma) = x_{n+1} =x_{n} \, – \gamma \nabla f(x_{n}) = X(t)\, – \gamma \nabla f(X(t)).$$ Dividing by \(\gamma\), we get $$ \frac{1}{\gamma} \big[ X(t + \gamma) \, – X(t) \big] = \, – \nabla f(X(t)).$$</p>



<p class="justify-text">When \(\gamma\) tends to zero (and with simple additional regularity assumptions), the left hand side tends to the derivative of \(X\) at \(t\), and thus the function \(X\) tends to the solution of the following <a href="https://en.wikipedia.org/wiki/Ordinary_differential_equation">ordinary differential equation</a> $$ \dot{X}(t) = \ – \nabla f (X(t)).$$ See an illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-3479" height="271" src="https://francisbach.com/wp-content/uploads/2020/04/logistic_2d_flow.gif" width="348"/>Gradient descent (with piece-wise affine interpolation between iterates) vs. gradient flow on the same time scale for a logistic regression problem.</figure></div>



<p class="justify-text">Studying the gradient flow in lieu of the gradient descent recursions comes with pros and cons.</p>



<p class="justify-text"><strong>Simplified analyses</strong>. The gradient flow has no step-size, so all the traditional annoying issues regarding the choice of step-size, with line-search, constant, decreasing or with a weird schedule are unnecessary. Moreover, the use of differential calculus makes proving properties really simple (see examples below). We can thus focus on the essence of the algorithm rather than on technicalities.</p>



<p class="justify-text"><strong>From (continuous) flow to actual (discrete) algorithms</strong>. A flow cannot be run on a computer as it is a continuous-time object. The traditional discretization is the <a href="https://en.wikipedia.org/wiki/Euler_method">Euler method</a>, that exactly replaces the flow by a piecewise-affine interpolation of the gradient descent iterates, where as shown above, we see \(x_n\) as \(X(n\gamma)\), where \(\gamma\) is the time increment between two samples. Four interesting observations:</p>



<ul class="justify-text"><li><em>No direct proof transfer</em> : While Euler discretization always provides an algorithm, the generic convergence proofs do not allow to transfer immediately continuous-time proofs to convergence results for the discrete analysis. A key difficulty is to set-up the step-size \(\gamma\). However, the analysis can often be mimicked, i.e., similar <a href="https://en.wikipedia.org/wiki/Lyapunov_function">Lyapunov functions</a> can be used (see examples below).</li><li><em>Proximal algorithms</em> : Faced with non-continuous gradient functions, the <em>forward</em> version of Euler discretization \(x_{n+1} = x_{n} – \gamma \nabla f(x_{n})\) can be replaced by the <em>backward</em> version $$x_{n+1} = x_{n} \, –  \gamma \nabla f(x_{n+1}),$$ which is only implicit as it can be solved by minimizing $$ f(x) + \frac{1}{2\gamma}\|x-x_{n}\|_2^2,$$ thus leading to the <a href="https://fr.wikipedia.org/wiki/Algorithme_proximal_(optimisation)">proximal point algorithm</a>. Forward-backward schemes can also be recovered when \(f\) is the sum of a smooth and a non-smooth term.</li><li><em>Stochastic gradient descent</em> : There are two ways to deal with stochastic gradient descent, leading to two very different continuous limits. Adding independent and identically distributed (for simplicity) zero-mean noise \(\varepsilon_n\) to the gradient leads to the recursion $$x_{n+1} = x_{n} – \gamma \big[ \nabla f(x_{n}) + \varepsilon_n\big] = x_{n}\, – \gamma \nabla f(x_{n}) \,- \gamma \varepsilon_n,$$ where the noise is multiplied by the step-size \(\gamma\). Surprisingly, taking the limit when \(\gamma\) tends to zero leads to the deterministic gradient flow equation. A more detailed argument is presented at the end of post, but the main hand-waving reason is that the noise contribution vanishes because it is multiplied by the step-size. Note that this limiting behavior is consistent with a convergence to a minimizer of \(f\).</li><li><em>Convergence to a Langevin diffusion</em> : When instead the noise is added with magnitude proportional to the square root \(\sqrt{2 \gamma}\) of the step-size (which is asymptotically larger than \(\gamma\)), when \(\gamma\) tends to zero, and if the covariance of the noise is identity, we converge to a <a href="https://en.wikipedia.org/wiki/Diffusion_process">diffusion process</a> which is the solution of a <a href="https://en.wikipedia.org/wiki/Stochastic_differential_equation">stochastic differential equation</a>: $$ dX(t) = \ – \nabla f(X(t)) + \sqrt{2} dB(t),$$ where \(B\) is a standard <a href="https://en.wikipedia.org/wiki/Brownian_motion">Brownian motion</a>. Moreover, as \(t\) tends to infinity, \(X(t)\) happens to tend in distribution to a random variable with density proportional to \(\exp( – f(x) )\). See more details at the end of the post and in [6]. The difference in behavior is illustrated below.</li></ul>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-3527" height="318" src="https://francisbach.com/wp-content/uploads/2020/04/logistic_2d_flow_SGD-3.gif" width="359"/> Comparison of flow and diffusion, for the same small \(\gamma\). The flow is deterministic and converges to a stationary point of \(f\) (here the global minimum), while the diffusion is stochastic and converges to a distribution (which is typically not a point mass)</figure></div>



<h2>Properties of gradient flows</h2>



<p class="justify-text">The gradient flow $$ \dot{X}(t) = \ – \nabla f (X(t)) $$ is well-defined for a wide variety of conditions on the function \(f\). The most classical ones are <a href="https://en.wikipedia.org/wiki/Picard%E2%80%93Lindel%C3%B6f_theorem">Lipschitz-continuity</a> or semi-convexity [<a href="https://link.springer.com/content/pdf/10.1007/s13373-017-0101-1.pdf">7</a>].</p>



<p class="justify-text">The most obvious property is that the function decreases along the flow; in other words, \(f(X(t))\) is decreasing, which is a simple consequence of $$ \frac{d}{dt} f(X(t)) =  \nabla f(X(t))^\top \frac{dX(t)}{dt} =\  – \| \nabla f (X(t) )\|_2^2 \leqslant 0.$$</p>



<p class="justify-text">If \(f\) is bounded from below, then \(f(X(t))\) will always converge (as a non-increasing function which is bounded from below, see <a href="https://en.wikipedia.org/wiki/Monotone_convergence_theorem">here</a>). However, in general, \(X(t)\) may not always converge without any further assumptions, e.g., it may oscillate forever. This is however rare and there are a variety of sufficient conditions for convergence of gradient flows, that date back to Lojasiewicz [8], and are based on “Lojasiewicz inequalities” that state that for \(y\) and \(x\) close enough, \(|f(x) – f(y)|^{1-\theta} \leqslant C \| \nabla f(x)\|\) for some \(C &gt; 0 \) and \(\theta \in (0,1)\). These are satisfied for “sub-analytical functions”, that include most functions one can imagine [<a href="https://www.sciencedirect.com/sdfe/reader/pii/S0022247X05006864/pdf">9</a>].</p>



<p class="justify-text">Once \(X(t)\) converges to some \(X(\infty) \in \mathbb{R}^d\), assuming \(\nabla f\) is continuous, we must have \(\nabla f(X(\infty))=0\), that is, \(X(\infty)\) is a stationary point of \(f\). Among all stationary points (that can be local minima, local maxima, or saddle-points), the one to which \(X(t)\) converges to depends on \(X(0)\).</p>



<p class="justify-text">Given any stationary point, one can look at the set of initializations that lead to it. Typically, only local minima are stable, that is, the attraction basins of other stationary points has typically zero Lebesgue measure (see, e.g., [<a href="http://www.jmlr.org/proceedings/papers/v49/lee16.pdf">10</a>]). See examples below. </p>



<p class="justify-text">We start with a simple function defined on the two-dimensional plane, with several local minima and saddle-points.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3490" height="401" src="https://francisbach.com/wp-content/uploads/2020/04/plot_non_convex-1.png" width="511"/>Various gradient flows trajectories, starting from green points and ending in black points. Note the proximity of the three top starting points, all ending in different local minima. See the motion below.</figure></div>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-3492" height="500" src="https://francisbach.com/wp-content/uploads/2020/04/logistic_2d_flow_noncvx-1.gif" width="519"/>Various gradient flows trajectories, in motion! All flows share the same time scale. Some seem “slower” than others (because the gradient norm is small).</figure></div>



<p class="justify-text">Before moving on, I cannot resist presenting a “real” two-dimensional example that probably all skiers, hikers, and cyclists with some form of mathematical abilities have thought of, the topographic map. Here is an example below:</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3751" height="458" src="https://francisbach.com/wp-content/uploads/2020/04/glandon_croix_de_fer-1-1024x1024.jpg" width="458"/>Extract from <a href="https://www.geoportail.gouv.fr/">IGN</a> topographic map, around <a href="https://en.wikipedia.org/wiki/Col_du_Glandon">Col du Glandon</a> and <a href="https://en.wikipedia.org/wiki/Col_de_la_Croix_de_Fer">Col de la Croix de Fer</a> (French Alps).</figure></div>



<p class="justify-text">Given the topographic map, how would gradient descent or gradient flow perform? Clearly, this corresponds to a non convex function, but it is quite well-behaved, as following water flows will typically lead to sea level. I chose two starting points famous to cyclists, <a href="https://en.wikipedia.org/wiki/Col_du_Glandon">Col du Glandon</a> and <a href="https://en.wikipedia.org/wiki/Col_de_la_Croix_de_Fer">Col de la Croix de Fer</a>, and ran gradient descent with a small step-size (to approximate the gradient flow), without noise (left) and with noise (right), on the topographic map (thanks to <a href="http://recherche.ign.fr/labos/matis/cv.php?nom=Landrieu">Loïc Landrieu</a> for the data extraction).</p>



<figure class="wp-block-image size-large"><img alt="" class="wp-image-3753" src="https://francisbach.com/wp-content/uploads/2020/04/flows_final_square_small-1024x394.png"/></figure>



<p class="justify-text">Without noise, the descent from la Croix de Fer ends up getting stuck quickly in a local minimum, while the one from Glandon goes down to the valley, but then is not able to follow the almost flat slope. When noise is added, the two flows go a bit lower, highlighting the benefits of noise to escape local minima.</p>



<h2>Gradient flows for optimization and machine learning</h2>



<p class="justify-text">There are (at least) two key questions in optimization and machine learning related to gradient flows: </p>



<ul class="justify-text"><li>When can we have global guarantees for convergence? That is, can we make sure that we choose an initialization point well enough to get the the global optimum <em>without knowing where the global optimum is</em>. A key difficulty is that the volume of the attraction basin of the global optimum can be made arbitrarily small, even for infinitely differentiable functions (imagine a function equal to zero everywhere except on a small ball where it is negative).</li><li>How fast can we get there? “there” can be a stationary point or a global optimum. This is an important question as mere convergence in the limit may be arbitrarily slow [<a href="https://papers.nips.cc/paper/6707-gradient-descent-can-take-exponential-time-to-escape-saddle-points.pdf">11</a>].</li></ul>



<p class="justify-text">An important class of function is <a href="https://en.wikipedia.org/wiki/Convex_function">convex functions</a>, where everything works out very well. We will study them below. Other functions will be studied in future posts.</p>



<h2>Convex functions</h2>



<p class="justify-text">We now assume that the function \(f\) is <a href="https://en.wikipedia.org/wiki/Convex_function">convex</a> and differentiable. Within machine learning, this corresponds to objective functions encountered for supervised learning which are based on empirical risk minimization with a prediction function which is linearly parameterized, such as <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a>.</p>



<p class="justify-text">There are various definitions of convexity, which are based on global properties (the function is always “below its chords”, or it is always “above its tangents”) or local properties (the Hessian is always positive semi-definite). The one which we need here is to be above its tangents, that is, for any \(x, y \in \mathbb{R}^d\), $$f(x) \geqslant f(y)  + \nabla f(y)^\top ( x \, – y).$$ Applying this to any stationary point \(y\) such that \(\nabla f(y)=0\) shows that for all \(x\), \(f(x) \geqslant f(y)\), that is, \(y\) is a global minimizer of \(f\). This is the classical benefit of convexity: no need to worry about local minima.</p>



<p class="justify-text">Another property we will need is the Lojasiewicz inequality, which is in particular satisfied when \(f\) is \(\mu\)-<a href="https://en.wikipedia.org/wiki/Convex_function#Strongly_convex_functions">strongly convex</a> (that is, \(f – \frac{\mu}{2} \| \cdot \|_2^2\) is convex): $$ f(x) \ – f(x_\ast) \leqslant \frac{1}{2 \mu} \| \nabla f (x)\|^2$$ for any minimizer \(x_\ast\) of \(f\) and any \(x\). This property allows to go from a bound on the gradient norm to a bound on function values.</p>



<p class="justify-text">We then obtain the convergence rate <em>in one line</em> as follows (see more details in [<a href="http://papers.nips.cc/paper/6711-integration-methods-and-optimization-algorithms.pdf">12</a>]): $$ \frac{d}{dt} \big[ f(X(t))\ – f(x_\ast) \big] =\  \nabla f(X(t))^\top \dot{X}(t) =  \ – \| \nabla f(X(t))\|_2^2 \leqslant \ – 2\mu  \big[ f(X(t)) \ – f(x_\ast) \big]$$ using the Lojasiewicz inequality above, leading to by simple integration of the derivative of \(\log \big[ f(X(t)) \ – f(x_\ast) \big]\): $$f(X(t)) \ – f(x_\ast) \leqslant \exp( – 2\mu t ) \big[ f(X(0))\  – f(x_\ast) \big], $$ that is, the convergence is exponential and the characteristic time is proportional to \(1/\mu\).</p>



<p class="justify-text">The gradient flow gives the main insight (exponential convergence); and applying the result above to \(t = \gamma n\), we seem to recover the traditional rate proportional to \(\exp( – \gamma \mu n)\); HOWEVER, this is only true asymptotically for \(\gamma\) tending to zero, and proving a result for gradient descent requires extra steps to deal with having a constant step-size. This requires typically \(\gamma \leqslant 1/L\), where \(L\) is the smoothness constant of \(f\), and the simplest proof happens to use the same structure (see [<a href="https://arxiv.org/pdf/1608.04636">13</a>] and references therein, as well as [<a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=zvmmf&amp;paperid=7813&amp;what=fullt&amp;option_lang=eng">14</a>]).</p>



<p class="justify-text">Without strong convexity, we have, using the tangent property at \(X(t)\) and \(x_\ast\): $$ \frac{d}{dt}\big[   \| X(t)\ – x_\ast \|^2 \big] = \ –   2 ( X(t) \ – x_\ast )^\top \nabla f(X(t)) \leqslant \ – 2 \big[ f(X(t)) \ – f(x_\ast) \big],$$  leading to, by integrating from \(0\) to \(t\), and using the monotonicity of \(f(X(t))\): $$  f(X(t)) \ – f(x_\ast) \leqslant \frac{1}{t} \int_0^t \big[ f(X(u)) \ – f(x_\ast) \big] du \leqslant \frac{1}{2t} \| X(0) \ – x_\ast \|^2 \ – \frac{1}{2t} \| X(t) \ – x_\ast \|^2.$$ We recover the usual rates in \(O(1/n)\), with \(t = \gamma n\), with the same caveat as above (the step-size needs to be bounded).</p>



<h2>Conclusion</h2>



<p class="justify-text">In this blog post, I covered the basic aspects of gradient flows, in particular their relationships with various forms of gradient descent, and their use in obtaining simple convergence justifications. Next months, I will cover extensions of the analyses above, in particular in terms of (1) acceleration for convex functions, where several flows and discretizations are interesting beyond the gradient flow and Euler method [12, 15], and (2) another class of functions which includes non-convex functions as encountered when learning with neural networks [16].</p>



<h2>References</h2>



<p class="justify-text">[1] Augustin Louis Cauchy. <a href="http://gallica.bnf.fr/ark:/12148/bpt6k90190w/f406">Méthode générale pour la résolution des systèmes d’équations simultanées</a>. Compte Rendu à l’Académie des Sciences, 25:536–538, 1847.<br/>[2] Claude Lemaréchal. <a href="https://www.math.uni-bielefeld.de/documenta/vol-ismp/40_lemarechal-claude.pdf">Cauchy and the Gradient Method</a>. <em>Documenta Mathematica</em>, <a href="https://www.math.uni-bielefeld.de/documenta/vol-ismp/vol-ismp.html">Extra Volume: Optimization Stories</a>, 251–254, 2012.<br/>[3] Yurii Nesterov. <em>Introductory lectures on convex optimization: A basic course</em> (Vol. 87). Springer Science &amp; Business Media, 2013.<br/>[4] Dimitri P. Bertsekas, <em>Nonlinear programming</em>. Athena Scientific, 1999.<br/>[5] Jorge Nocedal and Stephen Wright. <em>Numerical optimization</em>. Springer Science &amp; Business Media, 2006.<br/>[6] Arnak S. Dalalyan. <a href="https://rss.onlinelibrary.wiley.com/doi/epdf/10.1111/rssb.12183">Theoretical guarantees for approximate sampling from smooth and log‐concave densities</a>. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 79(3), 651-676, 2017.<br/>[7] Filippo Santambrogio. <a href="https://link.springer.com/content/pdf/10.1007/s13373-017-0101-1.pdf">{Euclidean, metric, and Wasserstein} gradient flows: an overview</a>. <em>Bulletin of Mathematical Sciences</em>, <em>7</em>(1), 87-154, 2017.<br/>[8] Stanislaw Lojasiewicz. Sur les trajectoires du gradient d’une fonction analytique. <em>Seminari di Geometria</em>, 1983:115–117, 1982.<br/>[9] Jérôme Bolte, Aris Daniilidis, and Adrian Lewis. <a href="https://www.sciencedirect.com/sdfe/reader/pii/S0022247X05006864/pdf">A nonsmooth Morse–Sard theorem for subanalytic functions</a>. <em>Journal of Mathematical Analysis and Applications</em>, 321(2):729–740, 2006.<br/>[10] Jason D. Lee, Max Simchowitz, Michael I. Jordan, Benjamin Recht. <a href="http://www.jmlr.org/proceedings/papers/v49/lee16.pdf">Gradient descent only converges to minimizers</a>. <em>Conference on learning theory</em>, 1246-1257, 2016.<br/>[11] Simon S. Du, Chi Jin, Jason D. Lee, Michael I. Jordan, Barnabas Poczos, Aarti Singh. <a href="https://papers.nips.cc/paper/6707-gradient-descent-can-take-exponential-time-to-escape-saddle-points.pdf">Gradient descent can take exponential time to escape saddle points</a>. <em>Advances in neural information processing systems</em>, 1067-1077, 2017.<br/>[12] Damien Scieur, Vincent Roulet, Francis Bach, Alexandre d’Aspremont,. <a href="http://papers.nips.cc/paper/6711-integration-methods-and-optimization-algorithms.pdf">Integration methods and optimization algorithms</a>. <em>Advances in Neural Information Processing Systems</em>, 1109-1118, 2017.<br/>[13] Hamed Karimi, Julie Nutini, Mark Schmidt. <a href="https://arxiv.org/pdf/1608.04636">Linear convergence of gradient and proximal-gradient methods under the Polyak-Lojasiewicz condition</a>. <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</em>, 795-811, 2016.<br/>[14] Boris T. Polyak. <a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=zvmmf&amp;paperid=7813&amp;what=fullt&amp;option_lang=eng">Gradient methods for minimizing functionals</a>. <em>Zh. Vychisl. Mat. Mat. Fiz.</em>, 3(4):643–653, 1963. <br/>[15] Weijie Su, Stephen Boyd, Emmanuel J. Candès. <a href="http://www.jmlr.org/papers/volume17/15-084/15-084.pdf">A differential equation for modeling Nesterov’s accelerated gradient method: theory and insights</a>. <em>Journal of Machine Learning Research</em>, 17(1), 5312-5354, 2017.<br/>[16] Lénaïc Chizat, Francis Bach. <a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">On the global convergence of gradient descent for over-parameterized models using optimal transport</a>. <em>Advances in Neural Information Processing Systems</em>, 3036-3046, 2018.</p>



<h2>Limits of stochastic gradient descent for vanishing step-sizes</h2>



<p class="justify-text"><strong>Convergence to gradient flow. </strong>We consider fixed times \(t = n \gamma \) and \(s = m \gamma\), and we let \(\gamma\) tend to zero, with thus \(m\) and \(n\) tending to infinity. Starting from the recursion $$x_{n+1} = x_{n}\, – \gamma \nabla f(x_{n})\  – \gamma \varepsilon_n,$$ we get the following by applying it \(m\) times: $$X(t+s) \ – X(t) = x_{n+m}-x_n = \ – \gamma \sum_{k=0}^{m-1} \nabla f\Big(X\Big(t+\frac{sk}{m}\Big)\Big)\  – \gamma \sum_{k=0}^{m-1} \varepsilon_{k+n}.$$ The term \(\displaystyle \gamma \sum_{k=0}^{m-1} \nabla f\Big(X\Big(t+\frac{sk}{m}\Big)\Big)\) converges to \(\displaystyle \int_{t}^{t+s}\!\!\! \nabla f(X(u)) du\), while the term \(\gamma \sum_{k=0}^{m-1} \varepsilon_{k+n}\) has zero expectation and variance equal to \(\gamma^2 m = \gamma s \) times the variance of each \(\varepsilon_{k+n}\), and thus it tends to zero (since \(\gamma\) tends to zero). Thus, in the limit, $$X(t+s)\  – X(t) = \ – \int_{t}^{t+s} \!\!\! \nabla f(X(u)) du,$$ which is equivalent to the gradient flow equation.</p>



<p class="justify-text"><strong>Convergence to diffusion.</strong> We consider the recursion $$x_{n+1} = x_{n}\,  – \gamma \nabla f(x_{n}) + \sqrt{2\gamma} \varepsilon_n.$$ With the same argument as above, we now get $$X(t+s) \ – X(t) = x_{n+m}-x_n =\ – \gamma \sum_{k=0}^{m-1} \nabla f\Big(X\Big(t+\frac{sk}{m}\Big)\Big)\ – \sqrt{2\gamma} \sum_{k=0}^{m-1} \varepsilon_{k+n}.$$ Now the second term has zero mean but a variance proportional to \(2s\) (<em>which does not go to zero when \(\gamma\) goes to zero</em>). We can then use when \(m\) tends to infinity the <a href="https://en.wikipedia.org/wiki/Wiener_process#Wiener_process_as_a_limit_of_random_walk">limit of the sum of independent variables as a Wiener process</a>, to get $$X(t+s)\ – X(t) =\  – \int_{t}^{t+s} \!\!\! \nabla f(X(u)) du + \sqrt{2} \big[ B(t+s)-B(t) \big].$$ The <a href="https://en.wikipedia.org/wiki/It%C3%B4_diffusion#Invariant_measures">limiting distribution</a> of \(X(t)\) happens to be the so-called <a href="https://en.wikipedia.org/wiki/Gibbs_measure">Gibbs</a> distribution, with density \(\exp(-f(x))\) (the factor of \(\sqrt{2}\) was added to avoid an extra constant factor in the Gibbs distribution). More on this in a future post.</p></div>
    </content>
    <updated>2020-05-01T05:15:04Z</updated>
    <published>2020-05-01T05:15:04Z</published>
    <category term="Machine learning"/>
    <category term="Tools"/>
    <author>
      <name>Francis Bach</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2020-05-17T22:50:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=428</id>
    <link href="https://tcsplus.wordpress.com/2020/04/30/tcs-talk-wednesday-may-6-nathan-klein-university-of-washington/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, May 6 — Nathan Klein, University of Washington</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, May 6th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Nathan Klein from the University of Washington will speak about “An improved approximation algorithm for TSP in the half integral case” (abstract below). You can reserve a spot […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, May 6th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Nathan Klein</strong> from the University of Washington will speak about “<em>An improved approximation algorithm for TSP in the half integral case” (abstract below).</em></p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our<br/>
website</a> on the day of the talk, so people who did not sign up will still be able to watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a>suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: A classic result from Christofides in the 70s tells us that a fast algorithm for the traveling salesperson problem (TSP) exists which returns a solution at most 3/2 times worse than the optimal. Since then, however, no better approximation algorithm has been found. In this talk, I will give an overview of research towards the goal of beating 3/2 and will present the first sub-3/2 approximation algorithm for the special case of “half integral” TSP instances. These instances have received significant attention in part due to a conjecture from Schalekamp, Williamson and van Zuylen that they attain the integrality gap of the subtour polytope. If this conjecture is true, our work shows that the integrality gap of the polytope is bounded away from 3/2, giving hope for an improved approximation for the general case. This presentation is of joint work with Anna Karlin and Shayan Oveis Gharan.</p></blockquote></div>
    </content>
    <updated>2020-05-01T03:10:05Z</updated>
    <published>2020-05-01T03:10:05Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-05-17T22:39:16Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/04/30/linkage</id>
    <link href="https://11011110.github.io/blog/2020/04/30/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Goodbye Insta (). I don’t have an Instagram account. I don’t want to join yet another closed system for capturing my data and sending it to a corporation. But there were a few people whose Instagram accounts I would check out semi-regularly. No longer. Now Instagram won’t show me any posts not-logged-in. If you’re going to fence yourself off from the Internet, then you’re fencing yourself off from me. If you think this is going to encourage me to make an account, the opposite is true.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p>Goodbye Insta (<a href="https://mathstodon.xyz/@11011110/104009624456399772"/>). I don’t have an Instagram account. I don’t want to join yet another closed system for capturing my data and sending it to a corporation. But there were a few people whose Instagram accounts I would check out semi-regularly. No longer. Now Instagram won’t show me any posts not-logged-in. If you’re going to fence yourself off from the Internet, then you’re fencing yourself off from me. If you think this is going to encourage me to make an account, the opposite is true.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2004.07630">Four pages are indeed necessary for planar graphs</a> (<a href="https://mathstodon.xyz/@11011110/104016197893535979"/>). At STOC 1986, Yannakakis proved that planar graphs have 4-page <a href="https://en.wikipedia.org/wiki/Book_embedding">book embeddings</a>, and announced an example requiring 4 pages, but never published the example. Finally now Bekos et al. have provided detailed constructions for planar graphs requiring 4 pages. Still lost in limbo: Unger’s claim from 1992 that testing 3-page embeddability with fixed vertex ordering is polynomial.</p>
  </li>
  <li>
    <p><a href="https://www.latimes.com/california/story/2020-04-16/uc-reeling-under-staggering-coronavirus-costs-the-worst-impacts-all-at-once">Universities are starting to see the costs of the lockdown</a> (<a href="https://mathstodon.xyz/@11011110/104023169399231809"/>), in lost revenue from students and medical centers and extra expenses from the transition to remote learning — the linked story is on the University of California, but other universities are likely in similar or worse shape. So far my campus has not announced any specific cuts but colleagues predict that a hiring freeze, at least, is likely to come.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2003.13777">Subgraph densities in a surface</a> (<a href="https://mathstodon.xyz/@11011110/104035284995793475"/>). In 1993 I published a paper “<a href="https://www.ics.uci.edu/~eppstein/pubs/Epp-JGT-93.pdf">Connectivity, graph minors, and subgraph multiplicity</a>” showing that a planar graph  can have at most linearly many copies in larger planar graphs if and only if  is 3-connected. I thought it was long-forgotten, but now Huynh, Joret and Wood have generalized it in two ways: other surfaces than the plane, and other exponents than one in the number of copies.</p>
  </li>
  <li>
    <p><a href="https://github.com/microsoft/STL/pull/724">Fix boyer_moore_searcher with the Rytter correction</a> (<a href="https://mathstodon.xyz/@11011110/104040417854587667"/>, <a href="https://news.ycombinator.com/item?id=22895932">via</a>). A 40-year-old bugfix to an even older linear-time string matching algorithm finally makes it to production code, with an admonishment that this should have been mentioned in more recent explanations of the algorithm such as <a href="https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string-search_algorithm">Wikipedia’s</a> (since added).</p>
  </li>
  <li>
    <p><a href="https://gilkalai.wordpress.com/2020/04/19/to-cheer-you-up-in-difficult-times-ii-mysterious-matching-news-by-gal-beniamini-naom-nisan-vijay-vazirani-and-thorben-trobst/">To cheer you up in difficult times II: Mysterious matching news</a> (<a href="https://mathstodon.xyz/@11011110/104046659532923035"/>). Gil Kalai blogs about two recent papers: one by Gal Beniamini and Noam Nisan on <a href="https://arxiv.org/abs/2001.07642">a polynomial that is one for bipartite graphs with perfect matchings and zero otherwise</a> and one by UCI colleagues Vijay Vazirani and Thorben Tröbst <a href="https://arxiv.org/abs/2003.08917">extending it to test whether a subgraph of a weighted complete bipartite graph contains a minimum-weight perfect matching</a>.</p>
  </li>
  <li>
    <p><a href="https://inkscape.org/release/inkscape-1.0rc1/">Inkscape 1.0 (release candidate) now runs natively on OS X</a> (<a href="https://mathstodon.xyz/@11011110/104052350948366314"/>, <a href="https://news.ycombinator.com/item?id=22855357">via</a>). I still haven’t tried it, and the discussion suggests it still needs some tuning. But it seems pretty popular in free-software circles, and it’s good to know that there are free vector drawing programs out there that can compete with the (expensive) one I use, Adobe Illustrator.</p>
  </li>
  <li>
    <p>Antoine Chambert-Loir <a href="https://mathstodon.xyz/@antoinechambertloir/104018917625130123">asks for an explanation of the Schläfli graph’s Hamiltonicity</a>, or more specifically how <a href="https://en.wikipedia.org/wiki/Schl%C3%A4fli_graph">the Wikipedia article’s</a> infobox illustration can be related to more standard constructions of the graph.</p>
  </li>
  <li>
    <p><a href="https://aperiodical.com/2020/04/the-big-lock-down-math-off-match-5/">The eggbox puzzle</a> (<a href="https://mathstodon.xyz/@11011110/104063021972727214"/>). <em>The Aperiodical</em> has been posting “Big Lock-Down Math-Off” posts, double-headers with a vote for which “made you say Aha! the loudest”. I chose this one with James Munro’s eggbox puzzle because the illustrations made me say Aha! This state space is a <a href="https://en.wikipedia.org/wiki/Median_graph">median graph</a>!</p>

    <p>To find the median of three -egg placements, put the th egg in the median of its three positions. In fact, it’s a distributive lattice where the meet of two placements is to move each egg to the rightmost of its two positions and the join is to move each egg to the leftmost of the two. Of course that doesn’t help much in solving the actual puzzle…</p>
  </li>
  <li>
    <p><a href="https://listserv.umd.edu/cgi-bin/wa?A2=ind2004&amp;L=CAST10&amp;P=3191">Another MDPI journal editorial board resigns</a> (<a href="https://mathstodon.xyz/@11011110/104068456940158118"/>; 
“another” because of the 2018 <em>Nutrients</em> mass resignation). The topic appears to be related to chemical engineering. The resigning editor-in-chief describes the reason: “MDPI has stated that it will not modify the current plan for rapid, quota-driven growth, while the Editorial Board will not compromise its overarching goals of publication quality and scholarly contribution.”</p>
  </li>
  <li>
    <p>I had been using Wunderlist for to-do lists of review/submit deadlines, a shared family grocery list, etc but <a href="https://11011110.github.io/blog/2020/03/15/stay-home-linkage.html">as I posted earlier</a>, it is being strangled by new owner Microsoft to push you to their other thing. So after comparing other cross-platform shareable to-do-list apps I chose <a href="https://todoist.com/">todoist</a> because of its similar workflow and <a href="https://todoist.com/import/wunderlist">Wunderlist import feature</a> (<a href="https://mathstodon.xyz/@11011110/104074471851344995"/>). Close second was Tom Hull’s suggestion, Trello. Not as close: Microsoft.</p>
  </li>
  <li>
    <p><a href="http://statnet.org/COVID-JustOneFriend/">Can’t I please just visit one friend?</a> (<a href="https://mathstodon.xyz/@11011110/104085917246234853"/>) Or, how graph drawing helps us understand the importance of maintaining strict isolation.</p>
  </li>
  <li>
    <p><a href="https://www.icann.org/news/blog/icann-board-withholds-consent-for-a-change-of-control-of-the-public-interest-registry-pir">ICANN blocks .org sale</a> (<a href="https://mathstodon.xyz/@11011110/104091255847507444"/>, <a href="https://news.ycombinator.com/item?id=23038637">via</a>). This is very good news, and follows onto their <a href="https://www.theregister.co.uk/2020/04/17/icann_california_org_sale_delay/">delay of the sale a couple of weeks ago after the intervention of the California attorney general</a>.</p>
  </li>
</ul></div>
    </content>
    <updated>2020-04-30T21:37:00Z</updated>
    <published>2020-04-30T21:37:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-05-16T00:07:04Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/04/28/cartesian-triangle-centers</id>
    <link href="https://11011110.github.io/blog/2020/04/28/cartesian-triangle-centers.html" rel="alternate" type="text/html"/>
    <title>Cartesian triangle centers</title>
    <summary>The usual definition of a Euclidean triangle center is a point defined from a triangle in a way that is equivariant under similarity transformations, meaning that applying the transformation to a triangle and then constructing its center produces the same point as constructing the center first and then transforming it. These include familiar points defined from triangles like the orthocenter (where perpendiculars to the sides meet), incenter (the center of the inscribed circle), and circumcenter (the center of the circumscribed circle). What I have in mind in this post is a similarly-defined concept, but with a different group of transformations than similarity: the combination of any two separate linear transformations to the two coordinate axes of the Cartesian plane, or transformations that swap the (transformed) axes. Another way of describing these transformations is that they preserve axis-parallel lines and relative area. They also preserve other lines and parallelism of lines.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The usual definition of a Euclidean <a href="https://en.wikipedia.org/wiki/Triangle_center">triangle center</a> is a point defined from a triangle in a way that is <a href="https://en.wikipedia.org/wiki/Equivariant_map">equivariant</a> under <a href="https://en.wikipedia.org/wiki/Similarity_(geometry)">similarity transformations</a>, meaning that applying the transformation to a triangle and then constructing its center produces the same point as constructing the center first and then transforming it. These include familiar points defined from triangles like the <a href="https://en.wikipedia.org/wiki/Orthocenter">orthocenter</a> (where perpendiculars to the sides meet), <a href="https://en.wikipedia.org/wiki/Incenter">incenter</a> (the center of the inscribed circle), and <a href="https://en.wikipedia.org/wiki/Circumcenter">circumcenter</a> (the center of the circumscribed circle). What I have in mind in this post is a similarly-defined concept, but with a different group of transformations than similarity: the combination of any two separate linear transformations to the two coordinate axes of the <a href="https://en.wikipedia.org/wiki/Cartesian_plane">Cartesian plane</a>, or transformations that swap the (transformed) axes. Another way of describing these transformations is that they preserve axis-parallel lines and relative area. They also preserve other lines and parallelism of lines.</p>

<p>One of the standard Euclidean triangle centers turns out to be a Cartesian center as well: the centroid has as its coordinates the average of the three triangle vertex coordinates, and this coordinatewise calculation (independent of the ordering of the three vertices) shows it to be equivariant under Cartesian transformations. It is also the point where the three lines through each vertex and opposite side midpoint meet.</p>

<p style="text-align: center;"><img alt="Centroid of a triangle" src="https://11011110.github.io/blog/assets/2020/centroid.svg"/></p>

<p>Similarly, we can find a Cartesian triangle center from the coordinatewise median of the three vertices.</p>

<p style="text-align: center;"><img alt="Median of a triangle" src="https://11011110.github.io/blog/assets/2020/median.svg"/></p>

<p>The center of the axis-parallel bounding box is also a Cartesian triangle center, again computed coordinatewise as the average of the minimum and maximum coordinates.</p>

<p style="text-align: center;"><img alt="Bounding-box center of a triangle" src="https://11011110.github.io/blog/assets/2020/bbcenter.svg"/></p>

<p>But a Cartesian triangle center does not have to be determined coordinatewise in this way. As an example, denote the three triangle vertices by  and consider the point  with the property that the axis-parallel bounding boxes of the three line segments  all have equal area. For two points  and , the locus of points for which the two bounding boxes have equal area is a line through the other two corners of the bounding box of segment . (To see this, consider a linear transformation that takes this bounding box to a square, and apply symmetry.) The three lines defined in this way from the three pairs of vertices of a non-degenerate triangle always meet in a point, the center we are seeking. (This follows from the shared equality of the three rectangles, but it is also an instance of the dual of <a href="https://en.wikipedia.org/wiki/Pappus%27s_hexagon_theorem">Pappus’s theorem</a>.) And this point is equivariant under Cartesian transformations because these transformations preserve the ratios of areas of bounding rectangles.</p>

<p style="text-align: center;"><img alt="Point of equal bounding boxes of a triangle" src="https://11011110.github.io/blog/assets/2020/equalbb.svg"/></p>

<p>You can tell it’s not a coordinatewise center because, in the figure above, the triangle vertices have equally spaced -coordinates, from which it follows that all coordinatewise centers lie on the horizontal line with median -coordinate, but this center doesn’t.</p>

<p>In general, the geometry of the plane with Cartesian instead of Euclidean transformations does not seem to have been very thoroughly explored. These examples show, I think, that there are interesting things to find there.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/104079992540248342">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-04-28T21:24:00Z</updated>
    <published>2020-04-28T21:24:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-05-16T00:07:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4369</id>
    <link href="https://lucatrevisan.wordpress.com/2020/04/28/phase-2-ready-or-not-here-it-comes/" rel="alternate" type="text/html"/>
    <title>Phase 2, ready or not, here it comes</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Yesterday the Italian prime minister announced the timeline of the loosening of the lockdown. Some manufacturing restarted yesterday, and some customer-facing businesses will gradually reopen between next Monday and the beginning of June. Since the start of the lockdown 51 … <a href="https://lucatrevisan.wordpress.com/2020/04/28/phase-2-ready-or-not-here-it-comes/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Yesterday the Italian prime minister announced the timeline of the loosening of the lockdown. Some manufacturing restarted yesterday, and some customer-facing businesses will gradually reopen between next Monday and the beginning of June.</p>
<p><span id="more-4369"/></p>
<p>Since the start of the lockdown 51 days ago, it was clear that the condition to “reopen” was to have in place a “test-trace-isolate” plan to find infected people as soon as possible after their contagion, trace their contacts, and isolate them. For this, one needs an infrastructure for large-scale testing with quick turnaround, a well-staffed agency to do manual tracing or an app to do it automatically, and facilities to isolate people who are infected and not in need of hospitalization.</p>
<p>None of this has been done. There hasn’t been a sufficient ramp-up of testing capacity; as far as I know no additional people have been hired and trained for manual contact-tracing; there is an app for digital contact-tracing, but the plan to adopt it appears to have been shelved; if people test positive and are well they are asked to stay home, potentially with their family members, who are free to leave as they please.</p>
<p>All our eggs are in the basket of social distancing. The humor site Kotiomkin posted “Basically, phase 2 will rely on everybody’s common sense. We are fucked”.</p>
<p>The problem is that social distancing requires a common-sense avoidance of close contacts with other people, and the government can give guidelines on how to achieve it, but eventually it has to rely on everybody’s sense of responsibility. Unfortunately, the national mood around regulations is to immediately look for loopholes. </p>
<p>For example the initial lockdown measures stipulated that one could leave home to go buy groceries. Then when the police would stop people tens of miles away from their home, people would say “I drove here to buy groceries”, “but we are thirty miles away from where you live”, “yes but here the groceries are better”. So a subsequent amendment stipulated that one could buy groceries only in the town of residence, making it hard for people living next to a town border, for whom the closest grocery store was across the town line. In fact, every time I have encountered a crazy Italian law or regulation, and asked around for the likely reason it was instituted, it was usually to close a loophole in a previous regulation, which in turn had been put in place to close a loophole in a third regulation, and basically it’s loophole-closing all the way down to <a href="https://en.wikipedia.org/wiki/Roman_law">Roman law</a>.</p>
<p>I think that, from this point on, the story of the Italian covid-19 epidemic will not show the future of the rest of the Western world, but will evolve in its own timeline. Meanwhile, there are a couple of lessons that are still relevant, particularly in the comparison between Lombardy and NYC, which continue to track each other remarkably well.</p>
<p><img alt="2020-05-28-nyc-daily" class="alignnone size-full wp-image-4375" src="https://lucatrevisan.files.wordpress.com/2020/04/2020-05-28-nyc-daily.png?w=584"/><img alt="2020-05-28-nyc" class="alignnone size-full wp-image-4376" src="https://lucatrevisan.files.wordpress.com/2020/04/2020-05-28-nyc.png?w=584"/></p>
<p>One is that, at one point, it was decided to move older people with mild cases of covid-19 from hospital to nursing homes, to open up beds in hospitals. Since the personnel of nursing homes are not trained in the safety procedures for infectious diseases, and since nursing homes host older, frail people who are the highest-risk category for this illness, the result was a huge number of deaths in these nursing homes. Now nursing homes in New York are <a href="https://www.nytimes.com/2020/04/24/us/nursing-homes-coronavirus.html">being asked to take covid-19 patients from hospitals</a>.</p>
<p>The other is that an analysis of all-cause mortality shows a spike in March such that the difference between the typical March all-cause mortality and the March 2020 all-cause mortality is much bigger than the number of confirmed covid-19 deaths. Now the same phenomenon is being <a href="https://www.nytimes.com/interactive/2020/04/27/upshot/coronavirus-deaths-new-york-city.html"> observed in New York City</a>, with numbers similar to Lombardy’s. Notably, the baseline all-case mortality rate in Lombardy is much higher than in New York City (because  population growth has stalled and the demographic skews older), so while the absolute number of additional deaths is similar, the relative increase is a much more dramatic 6x in NYC versus roughly 4x in Lombardy. </p></div>
    </content>
    <updated>2020-04-28T16:40:10Z</updated>
    <published>2020-04-28T16:40:10Z</published>
    <category term="Milan"/>
    <category term="New York"/>
    <category term="covid-19"/>
    <category term="phase 2"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2020-05-17T22:24:17Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/04/28/tenure-track-assistant-associate-or-full-professor-theory-of-computation-at-university-of-groningen-the-netherlands-apply-by-may-5-2020/</id>
    <link href="https://cstheory-jobs.org/2020/04/28/tenure-track-assistant-associate-or-full-professor-theory-of-computation-at-university-of-groningen-the-netherlands-apply-by-may-5-2020/" rel="alternate" type="text/html"/>
    <title>Tenure Track Assistant, Associate or Full Professor Theory of Computation at University of Groningen, the Netherlands  (apply by May 5, 2020)</title>
    <summary>The University of Groningen seeks for an outward looking researcher in Computer Science who will perform research on theory of computation, broadly construed, in relation to new (neuromorphic) computing systems and architectures. We offer a challenging position in a unique world-class research environment, where close collaborations between research groups with different expertise are encouraged. Website: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The University of Groningen seeks for an outward looking researcher in Computer Science who will perform research on theory of computation, broadly construed, in relation to new (neuromorphic) computing systems and architectures. We offer a challenging position in a unique world-class research environment, where close collaborations between research groups with different expertise are encouraged.</p>
<p>Website: <a href="https://www.rug.nl/about-ug/work-with-us/job-opportunities/?details=00347-02S0007KLP">https://www.rug.nl/about-ug/work-with-us/job-opportunities/?details=00347-02S0007KLP</a><br/>
Email: b.noheda@rug.nl, j.b.t.m.roerdink@rug.nl and/or j.h.m.van.der.velde@rug.nl</p></div>
    </content>
    <updated>2020-04-28T08:50:14Z</updated>
    <published>2020-04-28T08:50:14Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-05-17T22:32:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7716</id>
    <link href="https://windowsontheory.org/2020/04/27/lessons-from-covid-19-what-works-online-and-what-doesnt/" rel="alternate" type="text/html"/>
    <title>Lessons from COVID-19: What works online and what doesn’t</title>
    <summary>(I am now on Twitter , so you can follow this blog there too if you prefer it. –Boaz) Between Zoom meetings and deadlines, I thought I’d jot down a few of my impressions so far on what lessons we can draw from this period on how well research and education can work online. I’ve […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>(I am now on <a href="https://twitter.com/boazbaraktcs">Twitter</a> ,  so you can follow this blog there too if you prefer it. –Boaz)</p>



<p>Between Zoom meetings and deadlines, I thought I’d jot down a few of my impressions so far on what lessons we can draw from this period on how well research and education can work online.  I’ve had a few surprises in both directions – things that worked better than I would have expected, and aspects that were more problematic than I realized. These are personal impressions – please do comment on your own experiences.</p>



<p>As a rule of thumb, the interactions that most successfully replicate online are those that are relatively short and focused (an hour or so – e.g., a focused research meeting, seminar talk, or a lecture in a course).  Other interactions (e.g., faculty meetings) are also fairly easily to port online, perhaps because the original wasn’t that great to begin with.</p>



<p>The things that are harder to replicate are sustained interactions over longer periods. These include more extended and less directed research collaborations, informal workshops, as well as support for students outside lectures in education.</p>



<p><strong>Works well: Research seminars</strong></p>



<p>I’ve been pleasantly surprised by how effective research seminars such as our <a href="https://mltheory.org/">machine learning theory seminar</a> are over Zoom. In particular these were no less interactive than physical seminars – in fact people are offten <em>more</em> comfortable asking questions on chat than they would during in-person seminars. I hope such seminars become common practice even after this period ends- flying a speaker across the country or the world to give an hour talk doesn’t makes much sense given that there is a perfectly satisfactory alternative. </p>



<p><strong>Works well: Lectures</strong></p>



<p>This term I am teaching <a href="https://cs127.boazbarak.org/schedule/">cryptography</a>, and online lectures on Zoom have gone surprisingly well (after  working out some <a href="https://windowsontheory.org/2020/03/26/technology-for-theory-covid-19-edition/">technical issues</a>). Students participate on chat and ask questions, and seem to be following the lecture quite well. The important caveat is that lectures only work well for the students that attend and can follow them. For students who need extra support, it’s become much harder to access it.  It’s also much easier for students to (literally) “fall off the screen” and fall behind in a course, which brings me to the next point.</p>



<p><strong>Works less well: Support outside lectures</strong></p>



<p>Lectures are just one component of a course. Most of students’ learning occurs outside the classroom, where students meet together and work on problem sets, or discuss course material. These interactions between students (both related and unrelated to course) are where much of their intellectual growth happens. </p>



<p>All these interactions are greatly diminished online, and I did not yet see a good alternative. I’ve seen reduced attendance in office hours and sections, and reports are that students find it much harder to have the sort of chance discussions and opportunities to find study partners that they value so much.  If anything, this experience had made me <em>less</em> positive about the possibility of online education replacing physical colleges (though there are interesting <a href="https://en.wikipedia.org/wiki/Minerva_Schools_at_KGI">hybrid models</a>, where the students are co-located but lecturers are online).</p>



<p><strong>Works less well: unstructured research collaborations</strong></p>



<p>A focused meeting reporting on results or deciding on work allocation works pretty well over Zoom. So far it seems that extended brainstorming meetings, such as talking to someone over several hours in a coffeeshop, are much harder to replicate. In particular, a good part of such meetings is often spent with people staring in silence into their notebooks. As I <a href="https://twitter.com/boazbaraktcs/status/1253330145789673473">wrote</a>,  mutual silence seems to be very hard to do over Zoom.</p>



<p>Generally, informal week-long workshops, where much time is devoted to unstructured discussions, are ones that are most important to hold in person, and are hard (or maybe impossible) to replicate online. I have still not attended an online conference, but I suspect that these aspects of the conference would also be the ones hardest to replicate.</p>



<p><strong>Works well: faculty meetings</strong></p>



<p>I’ve always found it hard to bring a laptop to a faculty meeting and get work done, while listening with one ear to what’s going on. This is so much easier over Zoom <img alt="&#x1F642;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;"/></p></div>
    </content>
    <updated>2020-04-27T21:01:00Z</updated>
    <published>2020-04-27T21:01:00Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-05-17T22:36:03Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1676</id>
    <link href="https://theorydish.blog/2020/04/27/whats-your-story/" rel="alternate" type="text/html"/>
    <title>What’s Your Story?</title>
    <summary>Last quarter, I taught a course on research methods in TOC, which gave me an opportunity to think through many aspects of research. I was promoting a human-centric perspective on research: how to facilitate better research by addressing the conditions needed for an individual researcher and groups of researchers to succeed. As science is a communal effort, the communication of science is critical, and thus one of the topics we covered is oral presentations. There are plenty of resources about research talks, and mostly they emphasize form over matter. How many words in a slide? How many slides in a talk? how to and how not to use font colors? How to and how not to use animation? and so on. While all of these are important, I find that the failing of many research talks is on a much more basic level. Think back to a research talk you heard recently, or to one you heard a few months ago. You may remember how you felt and what you thought of the talk but what do you remember of this talk in terms of content? Most of us will find that we don’t remember much, I rarely do. Yet [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Last quarter, I taught <a href="https://omereingold.wordpress.com/cs-353-the-practice-of-theory-research/">a course on research methods</a> in TOC, which gave me an opportunity to think through many aspects of research. I was promoting a human-centric perspective on research: how to facilitate better research by addressing the conditions needed for an individual researcher and groups of researchers to succeed. As science is a communal effort, the communication of science is critical, and thus one of the topics we covered is oral presentations.</p>
<p>There are plenty of resources about research talks, and mostly they emphasize form over matter. How many words in a slide? How many slides in a talk? how to and how not to use font colors? How to and how not to use animation? and so on. While all of these are important, I find that the failing of many research talks is on a much more basic level.</p>
<p>Think back to a research talk you heard recently, or to one you heard a few months ago. You may remember how you felt and what you thought of the talk but what do you remember of this talk in terms of content? Most of us will find that we don’t remember much, I rarely do. Yet in our presentations, we often follow a research-paper-like mold and squeeze in many little details that are somehow important to us, forgetting that they will all vanish in our audience’s memory soon after (or completely missed in the first place). Giving a talk (writing a paper, writing a blog post etc.) is about communication: who is your audience? what are the limitation of the medium? what is the message you want to convey? Since so little stays with the audience long term, it makes sense to make sure that this little will be what seems most important for you to convey.</p>
<p>The idea I am promoting here is not new, and there are various techniques towards this goal. One (which I think Oded Goldreich shared with me), is to think of audience’s attention as a limited currency. Whenever you share a big idea you spend a big token and other ideas cost a smaller token. Imagine you have one or two big tokens and a few smaller tokens. <a href="https://www.youtube.com/watch?v=5OFAhBw0OXs">Another approach</a>, emphasizes the notion of <strong>a premise</strong>. The idea promoted here is that a talk needs a premise and this should be the title of the talk. Furthermore, every slide needs a premise and it should be the title of the slide. A premise is a main idea and is a complete sentence. It is not unusual to find a slide titled “Analysis” or “Efficiency” but neither of these is a premise. “Problem X has an efficient algorithm” could be. The talk’s premise could help you distill what you want the audience to take out of your talk. It also helps shape the talk, as everything that doesn’t serve the premise shouldn’t be there. Note that each paper can provoke many different premises and thus many different talks.</p>
<p>Here I want to play with a different idea, that I find intriguing, even if it may seem a bit extreme. It will not be controversial that a good talk (and paper) tells a story. After all, humans understand and remember narratives. But could we take inspiration from the form of storytelling in fiction writing? A vast literature, classifies different kinds of stories and explores their templates (see for example <a href="http://storybistro.com/7-story-frameworks/">this short discussion</a>).  Can we find analogues to these types in scientific research talks?</p>
<p>The type of story that is easiest to relate to is the <strong>Quest/Hero’s Journey</strong> (think Lord of the Rings). These have several distinct ingredients: a call to adventure, tests, allies, enemies, ordeal, reward, victorious return. Some research talks that follow this template do it well and preserve a sense of suspense and excitement, others seem like a long list of problems and the tricks that the work uses to handle them.</p>
<p>I believe that many other story templates can find analogues is research talks as well. Here are my initial attempts:</p>
<ul>
<li><strong>Coming of age</strong> stories – this area of research previously only had naive ideas but this works brings significant depth.</li>
<li><strong>The Under<span style="color: #000000;">dog</span></strong><span style="color: #000000;"> (think David and Goliath): a modest technique that concurred a great challenge.</span></li>
<li><strong>Rags to Riches</strong> (think the Ugly Duckling): an area or technique that were not successful prove powerful.
<ul>
<li>Similarly: <strong>Rebirth</strong> (reinvention, renewal).</li>
</ul>
</li>
<li><strong>Comedy</strong> (or the Clarity Tale) – conceptual works shedding a new perspective.</li>
<li><strong>Tragedy</strong> (or the Cautionary Tale) – Some impossibility results come to mind (couldn’t we view Arrow’s impossibility theorem as being tragic?)</li>
<li><strong>Redemption stories</strong>: the field so far has missed the point, was misleading or harmful, but this work makes amends.</li>
</ul>
<p>Can you suggest papers and a story type that could fit them?</p></div>
    </content>
    <updated>2020-04-27T16:26:33Z</updated>
    <published>2020-04-27T16:26:33Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2020-05-17T22:40:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16982</id>
    <link href="https://rjlipton.wordpress.com/2020/04/26/time-for-some-jokes/" rel="alternate" type="text/html"/>
    <title>Time For Some Jokes</title>
    <summary>Can we still smile? [ Hardy and Littlewood] John Littlewood lived through the 1918–1919 flu pandemic, yet he appears not to have remarked on it in print. Nor can we find mention of it by Godfrey Hardy in A Mathematician’s Apology—though Hardy did write about the ravages of WW I. Today, Ken and I thought […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Can we still smile?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/04/26/time-for-some-jokes/unknown-139/" rel="attachment wp-att-16991"><img alt="" class="alignright size-full wp-image-16991" src="https://rjlipton.files.wordpress.com/2020/04/unknown-2.jpeg?w=600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ Hardy and Littlewood]</font></td>
</tr>
</tbody>
</table>
<p>
John Littlewood lived through the 1918–1919 flu pandemic, yet he appears not to have remarked on it in print. Nor can we find mention of it by Godfrey Hardy in <em>A Mathematician’s Apology</em>—though Hardy did write about the ravages of WW I.</p>
<p>
Today, Ken and I thought you might like some fun comments that are not about the current pandemic. </p>
<p>
This is not to say we are ignoring it. We are all fighting the virus in one way or another. Our hearts go out to those of you fighting it directly. We are all worried about ourselves and others. We are stuck at home, at least most of us. We are all in this terrible time together. We hope you all are safe and well. </p>
<p>
We thought we would list a few jokes and stories that you might enjoy. We wrote recently about one kind of mathematical <a href="https://rjlipton.wordpress.com/2020/02/28/reductions-and-jokes/">joke</a> that can be given various proportions of pure levity and mathematical content. Our friends Lance Fortnow and Bill Gasarch, plus commenters in their <a href="https://blog.computationalcomplexity.org/2006/06/funniest-computer-science-joke-ever.html">item</a>, collected some jokes on the computer science side.</p>
<p>
Littlewood’s notion of “mathematical joke” leaned more on mathematical content, though his <a href="https://en.wikipedia.org/wiki/A_Mathematician's_Miscellany">memoir</a> <em>A Mathematician’s Miscellany</em> includes many funny stories as well. At the end of his introduction to the book, he wrote:</p>
<blockquote><p><b> </b> <em> A good mathematical joke is better, and better mathematics, than a dozen mediocre papers. </em>
</p></blockquote>
<p/><p>
We will start at the levity end. This is almost a math <a href="http://web.sonoma.edu/Math/faculty/falbo/jokes.html">joke</a>:</p>
<blockquote><p><b> </b> <em> The Daily News published a story saying that one-half of the MP (Members of Parliament) were crooks.<br/>
The Government took great exception to that and demanded a retraction and an apology.<br/>
The newspaper responded the next day with an apology and reported that one-half of the MPs were not crooks. </em>
</p></blockquote>
<p/><p>
We like this one, even if it is not really a hardcore math one. It does rely on the fact that <img alt="{\frac{1}{2} + \frac{1}{2} = 1.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%7D+%2B+%5Cfrac%7B1%7D%7B2%7D+%3D+1.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{1}{2} + \frac{1}{2} = 1.}"/></p>
<p>
</p><p/><h2> Jokes and More </h2><p/>
<p/><p>
The following are some examples that we hope you all like. They are from a variety of sources: </p>
<ul>
<li>
Jokes that mathematicians think are <a href="https://www.businessinsider.com/13-math-jokes-that-every-mathematician-finds-absolutely-hilarious-2013-5">funny</a>. <p/>
</li><li>
Some are from <a href="https://cstheory.stackexchange.com/questions/3111/funny-tcs-related-papers-etc">StackExchange</a>. <p/>
</li><li>
Others are from Andrej Cherkaev’s <a href="https://www.math.utah.edu/~cherk/mathjokes.html">page</a>.
</li></ul>
<p>We have lightly edited a few.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> “My age is two billion years old,” said Paul Erdös. The point is: </p>
<blockquote><p><b> </b> <em> When I was seventeen years old it was said the earth was two billion years old. Now they say it is four billion years old. So my age is about two billion years old. </em>
</p></blockquote>
<p/><p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> There was a statistician that drowned crossing a river <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\dots}"/> It was 3 feet deep on average. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> An infinite number of mathematicians walk into a bar. The first one orders a beer. The second orders half a beer. The third orders a third of a beer. The bartender bellows, “Get the heck out of here, are you trying to ruin me?”</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> An chemist, a physicist, and a mathematician are stranded on an island when a can of food rolls ashore. The chemist and the physicist comes up with many ingenious ways to open the can. Then suddenly the mathematician gets a bright idea: “Assume we have a can opener <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\dots}"/>” </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> A theorist decides she wants to learn more about practical problems. She sees a seminar with the title: “The Theory of Gears.” So she goes. The speaker stands up and begins, “The theory of gears with a finite number of teeth is well known <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\dots}"/></p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> The reason that every major university maintains a department of mathematics is that it is cheaper to do this than to institutionalize all those people.</p>
<p>
Regarding the last one, Littlewood did after all write in his book:</p>
<blockquote><p><b> </b> <em> Mathematics is a dangerous profession; an appreciable proportion of us go mad. </em>
</p></blockquote>
<p/><p>
This appears to have been a playful swipe at Hardy’s decision to leave Cambridge for Oxford. It was couched in a discussion of events that would seem to have had tiny probabilities before they happened. </p>
<p>
The last two we’ve picked out from the above sites verge into philosophy:</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> The cherry theorem: Question: What is a small, red, round thing that has a cherry pit inside? <br/>
Answer: A cherry.</p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> René Descartes went into his favorite bar and the bartender asked, “would you like your usual drink tonight, Monsieur Descartes?” Descartes replied “I think not.” Then he promptly ceased to exist.</p>
<p>
</p><p/><h2> Wrong Derivations, Right Results </h2><p/>
<p/><p>
Littlewood’s standards for a “mathematical joke” were higher than ours, but we will start by adapting an example from this MathOverflow <a href="https://mathoverflow.net/questions/38856/jokes-in-the-sense-of-littlewood-examples">discussion</a> of Littlewood-style jokes. Sometimes we can play a joke on ourselves by deriving a result we know is right but with an incorrect proof. Here is the example:</p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> <a href="https://www.reddit.com/r/math/comments/1bntfg/are_there_or_can_there_be_jokes_or_puns_in/">Casting out 6’s</a>. Suppose we want to simplify the fraction <img alt="{\frac{166}{664}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B166%7D%7B664%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{166}{664}}"/>. We can use the rule of casting out 6’s to get </p>
<p align="center"><img alt="\displaystyle  \frac{166}{664} = \frac{16}{64} = \frac{1}{4}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B166%7D%7B664%7D+%3D+%5Cfrac%7B16%7D%7B64%7D+%3D+%5Cfrac%7B1%7D%7B4%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac{166}{664} = \frac{16}{64} = \frac{1}{4}. "/></p>
<p>
The rule works quite generally:</p>
<p align="center"><img alt="\displaystyle  \frac{1666}{6664} = \frac{16666}{66664} = \frac{166666}{666664} = \cdots " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B1666%7D%7B6664%7D+%3D+%5Cfrac%7B16666%7D%7B66664%7D+%3D+%5Cfrac%7B166666%7D%7B666664%7D+%3D+%5Ccdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac{1666}{6664} = \frac{16666}{66664} = \frac{166666}{666664} = \cdots "/></p>
<p/><p/>
<p align="center"><img alt="\displaystyle  \frac{26}{65} = \frac{266}{665} = \frac{2666}{6665} = \frac{26666}{66665} = \cdots " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B26%7D%7B65%7D+%3D+%5Cfrac%7B266%7D%7B665%7D+%3D+%5Cfrac%7B2666%7D%7B6665%7D+%3D+%5Cfrac%7B26666%7D%7B66665%7D+%3D+%5Ccdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac{26}{65} = \frac{266}{665} = \frac{2666}{6665} = \frac{26666}{66665} = \cdots "/></p>
<p>You can even turn the paper upside down and cast out the <img alt="{6}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B6%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{6}"/>‘s that you see then:</p>
<p/><p align="center"><img alt="\displaystyle  \frac{19}{95} = \frac{199}{995} = \frac{1999}{9995} = \frac{19999}{99995} = \cdots " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B19%7D%7B95%7D+%3D+%5Cfrac%7B199%7D%7B995%7D+%3D+%5Cfrac%7B1999%7D%7B9995%7D+%3D+%5Cfrac%7B19999%7D%7B99995%7D+%3D+%5Ccdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac{19}{95} = \frac{199}{995} = \frac{1999}{9995} = \frac{19999}{99995} = \cdots "/></p>
<p/><p/>
<p align="center"><img alt="\displaystyle  \frac{49}{98} = \frac{499}{998} = \frac{4999}{9998} = \frac{49999}{99998} = \cdots " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B49%7D%7B98%7D+%3D+%5Cfrac%7B499%7D%7B998%7D+%3D+%5Cfrac%7B4999%7D%7B9998%7D+%3D+%5Cfrac%7B49999%7D%7B99998%7D+%3D+%5Ccdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac{49}{98} = \frac{499}{998} = \frac{4999}{9998} = \frac{49999}{99998} = \cdots "/></p>
<p>
Note, this is a joke: The rule of course does not actually work all the time: 	</p>
<p align="center"><img alt="\displaystyle  \frac{56}{65} = \frac{5}{5} = 1. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B56%7D%7B65%7D+%3D+%5Cfrac%7B5%7D%7B5%7D+%3D+1.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac{56}{65} = \frac{5}{5} = 1. "/></p>
<p/><p><br/>
We thought to try to come up with our own examples, or at least blend in other sources. It once struck me (Ken), on reading a column by Martin Gardner on difference equations, that they give a “convincing proof” of <img alt="{0^0 = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%5E0+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0^0 = 1}"/>. Consider the powers of a natural number <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>, say <img alt="{k = 5}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%3D+5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k = 5}"/>. Take differences like so: </p>
<p align="center"><img alt="\displaystyle  \begin{array}{ccccccccccc} 1 &amp; &amp; 5 &amp; &amp; 25 &amp; &amp; 125 &amp; &amp; 625 &amp; &amp; \dots\\ &amp; 4 &amp; &amp; 20 &amp; &amp; 100 &amp; &amp; 500 &amp; &amp; \dots\\ &amp; &amp; 16 &amp; &amp; 80 &amp; &amp; 400 &amp; &amp; \dots &amp; &amp; \\ &amp; &amp; &amp; 64 &amp; &amp; 320 &amp; &amp; \dots &amp; &amp; &amp;\\ &amp; &amp; &amp; &amp; 256 &amp; &amp; &amp; &amp; \\ &amp; &amp; &amp; &amp; &amp; \ddots &amp; &amp; &amp; \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Bccccccccccc%7D+1+%26+%26+5+%26+%26+25+%26+%26+125+%26+%26+625+%26+%26+%5Cdots%5C%5C+%26+4+%26+%26+20+%26+%26+100+%26+%26+500+%26+%26+%5Cdots%5C%5C+%26+%26+16+%26+%26+80+%26+%26+400+%26+%26+%5Cdots+%26+%26+%5C%5C+%26+%26+%26+64+%26+%26+320+%26+%26+%5Cdots+%26+%26+%26%5C%5C+%26+%26+%26+%26+256+%26+%26+%26+%26+%5C%5C+%26+%26+%26+%26+%26+%5Cddots+%26+%26+%26+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{ccccccccccc} 1 &amp; &amp; 5 &amp; &amp; 25 &amp; &amp; 125 &amp; &amp; 625 &amp; &amp; \dots\\ &amp; 4 &amp; &amp; 20 &amp; &amp; 100 &amp; &amp; 500 &amp; &amp; \dots\\ &amp; &amp; 16 &amp; &amp; 80 &amp; &amp; 400 &amp; &amp; \dots &amp; &amp; \\ &amp; &amp; &amp; 64 &amp; &amp; 320 &amp; &amp; \dots &amp; &amp; &amp;\\ &amp; &amp; &amp; &amp; 256 &amp; &amp; &amp; &amp; \\ &amp; &amp; &amp; &amp; &amp; \ddots &amp; &amp; &amp; \end{array} "/></p>
<p>
The powers of <img alt="{k-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k-1}"/> always appear on the bottom diagonal. Thus we have: <img alt="{(k-1)^0 = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28k-1%29%5E0+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(k-1)^0 = 1}"/>, <img alt="{(k-1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28k-1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(k-1)}"/>, <img alt="{(k-1)^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28k-1%29%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(k-1)^2}"/>, and so on. Now do this for <img alt="{k = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k = 1}"/>:</p>
<p align="center"><img alt="\displaystyle \begin{array}{ccccccccccc} 1 &amp; &amp; 1 &amp; &amp; 1 &amp; &amp; 1 &amp; &amp; 1 &amp; &amp; \dots\\ &amp; 0 &amp; &amp; 0 &amp; &amp; 0 &amp; &amp; 0 &amp; &amp; \dots\\ &amp; &amp; 0 &amp; &amp; 0 &amp; &amp; 0 &amp; &amp; \dots &amp; &amp; \\ &amp; &amp; &amp; 0 &amp; &amp; 0 &amp; &amp; \dots &amp; &amp; &amp;\\ &amp; &amp; &amp; &amp; 0 &amp; &amp; &amp; &amp; \\ &amp; &amp; &amp; &amp; &amp; \ddots &amp; &amp; &amp; \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cbegin%7Barray%7D%7Bccccccccccc%7D+1+%26+%26+1+%26+%26+1+%26+%26+1+%26+%26+1+%26+%26+%5Cdots%5C%5C+%26+0+%26+%26+0+%26+%26+0+%26+%26+0+%26+%26+%5Cdots%5C%5C+%26+%26+0+%26+%26+0+%26+%26+0+%26+%26+%5Cdots+%26+%26+%5C%5C+%26+%26+%26+0+%26+%26+0+%26+%26+%5Cdots+%26+%26+%26%5C%5C+%26+%26+%26+%26+0+%26+%26+%26+%26+%5C%5C+%26+%26+%26+%26+%26+%5Cddots+%26+%26+%26+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \begin{array}{ccccccccccc} 1 &amp; &amp; 1 &amp; &amp; 1 &amp; &amp; 1 &amp; &amp; 1 &amp; &amp; \dots\\ &amp; 0 &amp; &amp; 0 &amp; &amp; 0 &amp; &amp; 0 &amp; &amp; \dots\\ &amp; &amp; 0 &amp; &amp; 0 &amp; &amp; 0 &amp; &amp; \dots &amp; &amp; \\ &amp; &amp; &amp; 0 &amp; &amp; 0 &amp; &amp; \dots &amp; &amp; &amp;\\ &amp; &amp; &amp; &amp; 0 &amp; &amp; &amp; &amp; \\ &amp; &amp; &amp; &amp; &amp; \ddots &amp; &amp; &amp; \end{array} "/></p>
<p>The diagonal now holds the powers of <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>. It thus follows that <img alt="{0^0 = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%5E0+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0^0 = 1}"/>.</p>
<p/><h2> Open Problems </h2><p/>
<p>What are your favorite mathematical jokes? Please send them to us. Be safe. Be well.</p>
<p><a href="https://rjlipton.wordpress.com/2020/04/26/time-for-some-jokes/sign-2/" rel="attachment wp-att-16987"><img alt="" class="aligncenter size-medium wp-image-16987" height="111" src="https://rjlipton.files.wordpress.com/2020/04/sign-1.png?w=300&amp;h=111" width="300"/></a></p>
<p>
[fixed equations in last main section]</p></font></font></div>
    </content>
    <updated>2020-04-27T00:37:51Z</updated>
    <published>2020-04-27T00:37:51Z</published>
    <category term="Oldies"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="jokes"/>
    <category term="math jokes"/>
    <category term="stories"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-05-17T22:32:11Z</updated>
    </source>
  </entry>
</feed>
