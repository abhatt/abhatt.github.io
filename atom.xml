<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-06-05T00:01:50Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/083</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/083" rel="alternate" type="text/html"/>
    <title>TR19-083 |  Testing Graphs against an Unknown Distribution | 

	Lior Gishboliner, 

	Asaf Shapira</title>
    <summary>The area of graph property testing seeks to understand the relation between the global properties of a graph and its local statistics. In the classical model, the local statistics of a graph is defined relative to a uniform distribution over the graph’s vertex set. A graph property $\mathcal{P}$ is said to be testable if the local statistics of a graph can allow one to distinguish between graphs satisfying $\mathcal{P}$ and those that are far from satisfying it.

Goldreich recently introduced a generalization of this model in which one endows the vertex set of the input graph with an arbitrary and unknown distribution, and asked which of the properties that can be tested in the classical model can also be tested in this more general setting. We completely resolve this problem by giving a (surprisingly ``clean'') characterization of these properties. To this end, we prove a removal lemma for vertex weighted graphs which is of independent interest.</summary>
    <updated>2019-06-04T21:17:16Z</updated>
    <published>2019-06-04T21:17:16Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-06-04T23:57:13Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7512</id>
    <link href="https://windowsontheory.org/2019/06/04/itcs-20-call-for-papers-guest-post-by-thomas-vidick/" rel="alternate" type="text/html"/>
    <title>ITCS 20 call for papers (guest post by Thomas Vidick)</title>
    <summary>We invite you to submit your papers to the 11th Innovations inTheoretical Computer Science (ITCS). The conference will be held atthe University of Washington in Seattle, Washington from January 12-14,2020. ITCS seeks to promote research that carries a strong conceptual message(e.g., introducing a new concept, model or understanding, opening a newline of inquiry within traditional […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>

We invite you to submit your papers to the 11th Innovations in<br/>Theoretical Computer Science (ITCS). The conference will be held at<br/>the University of Washington in Seattle, Washington from January 12-14,<br/>2020.</p>



<p>ITCS seeks to promote research that carries a strong conceptual message<br/>(e.g., introducing a new concept, model or understanding, opening a new<br/>line of inquiry within traditional or interdisciplinary areas,<br/>introducing new mathematical techniques and methodologies, or new<br/>applications of known techniques). ITCS welcomes both conceptual and<br/>technical contributions whose contents will advance and inspire the<br/>greater theory community.</p>



<p>Submission deadline: September 9, 2019 (05:59pm PDT)<br/>Notification to authors: October 31, 2019<br/>Conference dates: January 12-14, 2020</p>



<p>See the website at <a href="http://itcs-conf.org/itcs20/itcs20-cfp.html" rel="noreferrer noopener" target="_blank">http://itcs-conf.org/itcs20/itcs20-cfp.html</a> for<br/>detailed information regarding submissions.</p>



<p>Program committee</p>



<p>Nikhil Bansal, CWI + TU Eindhoven<br/>Nir Bitansky, Tel-Aviv University<br/>Clement Canonne, Stanford<br/>Timothy Chan, University of Ilinois at Urbana-Champaign<br/>Edith Cohen, Google and Tel-Aviv University<br/>Shaddin Dughmi, University of Southern California<br/>Sumegha Garg, Princeton<br/>Ankit Garg, Microsoft research<br/>Ran Gelles, Bar-Ilan University<br/>Elena Grigorescu, Purdue<br/>Tom Gur, University of Warwick<br/>Sandy Irani, UC Irvine<br/>Dakshita Khurana, University of Illinois at Urbana-Champaign<br/>Antonina Kolokolova, Memorial University of Newfoundland.<br/>Pravesh Kothari, Carnegie Mellon University<br/>Rasmus Kyng, Harvard<br/>Katrina Ligett, Hebrew University<br/>Nutan Limaye, IIT Bombay<br/>Pasin Manurangsi, UC Berkeley<br/>Tamara Mchedlidze, Karlsruhe Institute of Technology<br/>Dana Moshkovitz, UT Austin<br/>Jelani Nelson, UC Berkeley<br/>Merav Parter, Weizmann Institute<br/>Krzysztof Pietrzak, IST Austria<br/>Elaine Shi, Cornell<br/>Piyush Srivastava, Tata Institute of Fundamental Research, Mumbai<br/>Li-Yang Tan, Stanford<br/>Madhur Tulsiani, TTIC<br/>Gregory Valiant, Stanford<br/>Thomas Vidick, California Institute of Technology (chair)<br/>Virginia Vassilevska Williams, MIT<br/>Ronald de Wolf, CWI and University of Amsterdam<br/>David Woodruff, Carnegie Mellon University

</p></div>
    </content>
    <updated>2019-06-04T21:16:57Z</updated>
    <published>2019-06-04T21:16:57Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>windowsontheory</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2019-06-04T23:59:07Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15923</id>
    <link href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/" rel="alternate" type="text/html"/>
    <title>A Quantum Connection For Matrix Rank</title>
    <summary>A new paper with Chaowen Guan Chaowen Guan is a PhD student at Buffalo. After a busy end to the Spring 2019 term at UB, we are getting time to write about our paper, “Stabilizer Circuits, Quadratic Forms, and Computing Matrix Rank.” Today we emphasize new connections we have found between simulating special quantum circuits […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>A new paper with Chaowen Guan</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/chaowenwhiteboard/" rel="attachment wp-att-15934"><img alt="" class="alignright wp-image-15934" height="130" src="https://rjlipton.files.wordpress.com/2019/06/chaowenwhiteboard.jpg?w=180&amp;h=130" width="180"/></a></p>
<p>
Chaowen Guan is a PhD student at Buffalo. After a busy end to the Spring 2019 term at UB, we are getting time to write about our <a href="https://arxiv.org/abs/1904.00101">paper</a>, “Stabilizer Circuits, Quadratic Forms, and Computing Matrix Rank.”</p>
<p>
Today we emphasize new connections we have found between simulating special quantum circuits and computing matrix rank over the field <img alt="{\mathbb{F}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{F}_2}"/>.</p>
<p>
The quantum circuits involved have been known as polynomial-time solvable <a href="https://en.wikipedia.org/wiki/Gottesman-Knill_theorem">since</a> <a href="https://arxiv.org/abs/quant-ph/9807006v1">1998</a>. They are not universal but form important building blocks of quantum systems people intend to build. They impact the problem of showing quantum circuits are more powerful than classical circuits—the <i>quantum advantage problem</i>—in terms of how much harder quantum stuff must be added to them. </p>
<p>
The question is: How efficiently can we simulate these special circuits? Our answer improves the bound from order-<img alt="{n^3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^3}"/> to <img alt="{n^{\omega}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B%5Comega%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^{\omega}}"/>, where <img alt="{\omega}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\omega}"/> here means the current best-known exponent for multiplying <img alt="{n \times n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n \times n}"/> matrices (over <img alt="{\mathbb{F}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{F}_2}"/> or any field). Today <img alt="{\omega}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\omega}"/> stands at <img alt="{2.3728\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2.3728%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2.3728\dots}"/>. The non-quantum problem of counting solutions to a quadratic polynomial <img alt="{f(x_1,\dots,x_n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x_1,\dots,x_n)}"/> modulo 2 is likewise improved from the <img alt="{O(n^3)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^3)}"/> <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.9881">shown</a> by Andrzej Ehrenfeucht and Marek Karpinski to <img alt="{O(n^\omega)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^\omega)}"/>.</p>
<p>
This comes at a price, however, because the matrix multiplication algorithms that optimize the exponent are <a href="https://en.wikipedia.org/wiki/Galactic_algorithm">galactic</a>. In this post we’ll emphasize what is <em>not</em> galactic: reductions to and from the problem of computing matrix rank that run in linear time—meaning <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2)}"/> time for dense matrices—except for the need to check a yes/no condition in one of them. All this builds on the algebraic methods in our <a href="https://link.springer.com/chapter/10.1007/978-3-662-56499-8_4">paper</a> last year with Amlan Chakrabarti of the University of Calcutta.</p>
<p>
Chaowen has contributed a <a href="https://rjlipton.wordpress.com/2018/07/02/local-hams-in-la-jolla/">post</a> and some other materials for this blog. His work first came up in a <a href="https://rjlipton.wordpress.com/2016/06/29/getting-to-the-roots-of-factoring/">post</a> three years ago that saluted Dick and Kathryn’s wedding. Today is their third anniversary—so this post also comes with happy anniversary wishes.</p>
<p>
</p><p/><h2> Strong Simulation Problems </h2><p/>
<p/><p>
We have <a href="https://rjlipton.wordpress.com/2010/08/02/quantum-algorithms-via-linear-algebra/">covered</a> <a href="https://rjlipton.wordpress.com/2010/08/25/quantum-algorithms-a-different-view-again/">quantum</a> <a href="https://rjlipton.wordpress.com/2011/10/26/quantum-chocolate-boxes/">algorithms</a> <a href="https://rjlipton.wordpress.com/2011/11/14/more-quantum-chocolate-boxes/">several</a> <a href="https://rjlipton.wordpress.com/2015/04/08/a-quantum-two-finger-exercise/">times</a>. We discussed <em>stabilizer circuits</em> in an early <a href="https://rjlipton.wordpress.com/2012/07/08/grilling-quantum-circuits/">post</a> on the work with Amlan and <a href="https://rjlipton.wordpress.com/2017/11/20/a-magic-madison-visit">covered</a> them more recently in connection with the work of Jin-Yi Cai’s group. Suffice it to say that stabilizer circuits—which extend Clifford circuits by allowing intermediate measurement gates—form the most salient case that classical computers can simulate in polynomial time.</p>
<p>
The simulation time is sometimes cited as <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2)}"/> going back to a 2004 <a href="https://arxiv.org/abs/quant-ph/0406196">paper</a> by Scott Aaronson and Daniel Gottesman, but there is a catch: this is only for one measurement of one qubit. For general (non-sparse) instances, all of <a href="https://arxiv.org/abs/quant-ph/0504117">various</a> <a href="https://arxiv.org/abs/1305.6190">other</a> <a href="https://web.eecs.umich.edu/~imarkov/pubs/conf/iccd13-quipu.pdf">algorithms</a> need order-<img alt="{n^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^2}"/> time to re-organize their data structures after each single-qubit measurement. This is so even if one merely wants to measure all <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> qubits in one shot: the time becomes <img alt="{O(n^3)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^3)}"/>. This is one case of what is generally called a <em>strong</em> simulation. It is precisely this time that Chaowen and I improved to <img alt="{O(n^\omega)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^\omega)}"/>.</p>
<p>
In wider contexts, strong simulation of a quantum circuit <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> means the ability to compute the probability of a given output to high precision. When the input and output are both in <img alt="{\{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{0,1\}^n}"/> we may suppose both are <img alt="{0^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0^n}"/> since we can prepend and append <img alt="{\mathsf{NOT}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNOT%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{NOT}}"/> gates to <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. Then strong simulation means computing the amplitude <img alt="{\langle 0^n |C| 0^n \rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\langle 0^n |C| 0^n \rangle}"/> (or computing <img alt="{|\langle 0^n |C| 0^n \rangle|^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|\langle 0^n |C| 0^n \rangle|^2}"/> which is the output probability) to <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-place precision. It doesn’t take much for this to be <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{NP}}"/>-hard, often <img alt="{\mathsf{\#P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{\#P}}"/>-complete. If we take the Clifford generating set </p>
<p align="center"><img alt="\displaystyle  \mathsf{H} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{bmatrix},\quad \mathsf{CZ} = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; -1 \end{bmatrix},\quad \mathsf{S} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; i \end{bmatrix}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BH%7D+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Cbegin%7Bbmatrix%7D+1+%26+1+%5C%5C+1+%26+-1+%5Cend%7Bbmatrix%7D%2C%5Cquad+%5Cmathsf%7BCZ%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%26+0+%5C%5C+0+%26+1+%26+0+%26+0+%5C%5C+0+%26+0+%26+1+%26+0+%5C%5C+0+%26+0+%26+0+%26+-1+%5Cend%7Bbmatrix%7D%2C%5Cquad+%5Cmathsf%7BS%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%5C%5C+0+%26+i+%5Cend%7Bbmatrix%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathsf{H} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{bmatrix},\quad \mathsf{CZ} = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; -1 \end{bmatrix},\quad \mathsf{S} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; i \end{bmatrix}, "/></p>
<p>then we can get universal circuits by adding any any one of the following gates: </p>
<p align="center"><img alt="\displaystyle  \mathsf{T} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; \sqrt{i} \end{bmatrix},\quad \mathsf{CS} = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; i \end{bmatrix},\quad \mathsf{Tof} = \mathit{diag}(1,1,1,1,1,1,\begin{bmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{bmatrix}). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BT%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%5C%5C+0+%26+%5Csqrt%7Bi%7D+%5Cend%7Bbmatrix%7D%2C%5Cquad+%5Cmathsf%7BCS%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%26+0+%5C%5C+0+%26+1+%26+0+%26+0+%5C%5C+0+%26+0+%26+1+%26+0+%5C%5C+0+%26+0+%26+0+%26+i+%5Cend%7Bbmatrix%7D%2C%5Cquad+%5Cmathsf%7BTof%7D+%3D+%5Cmathit%7Bdiag%7D%281%2C1%2C1%2C1%2C1%2C1%2C%5Cbegin%7Bbmatrix%7D+0+%26+1+%5C%5C+1+%26+0+%5Cend%7Bbmatrix%7D%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathsf{T} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; \sqrt{i} \end{bmatrix},\quad \mathsf{CS} = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; i \end{bmatrix},\quad \mathsf{Tof} = \mathit{diag}(1,1,1,1,1,1,\begin{bmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{bmatrix}). "/></p>
<p>In the last one we’ve portrayed the <img alt="{8 \times 8}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B8+%5Ctimes+8%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{8 \times 8}"/> matrix of the <em>Toffoli gate</em> as being <em>block-diagonal</em>. We will later consider block-diagonal matrices permuted so that all <img alt="{2 \times 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2+%5Ctimes+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2 \times 2}"/> “blocks” are at upper left.</p>
<p>
There is <a href="https://arxiv.org/abs/1601.07601">much</a> <a href="https://arxiv.org/pdf/1808.00128.pdf">recent</a> <a href="https://arxiv.org/pdf/1712.03554.pdf">literature</a> on trying to simulate circuits with limited numbers of non-Clifford gates, and on how many such gates may be needed for <a href="https://arxiv.org/pdf/1902.04764.pdf">exponential</a> lower bounds—even just to tell whether <img alt="{\langle 0^n |C| 0^n \rangle \neq 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle+%5Cneq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\langle 0^n |C| 0^n \rangle \neq 0}"/>. This plays against a wider context of <a href="https://arxiv.org/abs/1608.00263">efforts</a> <a href="https://arxiv.org/abs/1807.10749">toward</a> <a href="https://arxiv.org/pdf/1905.00444.pdf">quantum</a> <a href="https://arxiv.org/abs/1203.5813">advantage</a>. Chaowen and I have been trying to apply algebraic-geometric techniques for new lower bounds at the high end, but this time we found new upper bounds at the low end.</p>
<p>
</p><p/><h2> From Matrix Rank to Quantum </h2><p/>
<p/><p>
It is not known how to compute the rank <img alt="{rk(A)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Brk%28A%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{rk(A)}"/> of a dense matrix <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> in better than matrix-multiplication time, even over <img alt="{\mathbb{F}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{F}_2}"/>. We may suppose <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> is square and symmetric, since we can always form the block matrix </p>
<p align="center"><img alt="\displaystyle  A' = \begin{bmatrix} 0 &amp; A^\top \\ A &amp; 0 \end{bmatrix} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A%27+%3D+%5Cbegin%7Bbmatrix%7D+0+%26+A%5E%5Ctop+%5C%5C+A+%26+0+%5Cend%7Bbmatrix%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  A' = \begin{bmatrix} 0 &amp; A^\top \\ A &amp; 0 \end{bmatrix} "/></p>
<p>and then <img alt="{rk(A) = \frac{1}{2}rk(A')}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Brk%28A%29+%3D+%5Cfrac%7B1%7D%7B2%7Drk%28A%27%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{rk(A) = \frac{1}{2}rk(A')}"/>. In the case of <img alt="{\mathbb{F}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{F}_2}"/>, <img alt="{A'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A'}"/> is the adjacency matrix <img alt="{A_G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_G}"/> of an undirected bipartite graph <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>. The rank of <img alt="{A_G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_G}"/> for any undirected graph <img alt="{G = (V,E)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG+%3D+%28V%2CE%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G = (V,E)}"/> must be even. Whereas the rank of the <img alt="{|V| \times |E|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CV%7C+%5Ctimes+%7CE%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|V| \times |E|}"/> vertex-edge incidence matrix always equals <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> minus the number of connected components of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>, less is <a href="http://web.cs.elte.hu/~lovasz/kurzusok/adjrank16.pdf">known</a> about characterizing <img alt="{rk(A_G)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Brk%28A_G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{rk(A_G)}"/>. Our first main theorem brings quantum strong simulation into the picture. Let <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> stand for <img alt="{n^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^2}"/>.</p>
<blockquote><p><b>Theorem 1</b> <em><a name="rank2QC"/> Given any <img alt="{A \in \mathbb{F}_2^{n \times n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+%5Cin+%5Cmathbb%7BF%7D_2%5E%7Bn+%5Ctimes+n%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A \in \mathbb{F}_2^{n \times n}}"/> we can construct in <img alt="{O(N)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{O(N)}"/> time a stabilizer circuit <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{C}"/> on <img alt="{2n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{2n}"/> qubits such that </em></p><em>
<p align="center"><img alt="\displaystyle  rk(A) = \log_2(|\langle 0^{2n} |C| 0^{2n} \rangle|). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++rk%28A%29+%3D+%5Clog_2%28%7C%5Clangle+0%5E%7B2n%7D+%7CC%7C+0%5E%7B2n%7D+%5Crangle%7C%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  rk(A) = \log_2(|\langle 0^{2n} |C| 0^{2n} \rangle|). "/></p>
</em><p><em/>
</p></blockquote>
<p/><p>
One interpretation is that if you believe matrix rank is a “mildly hard” function (with regard to <img alt="{O(N)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(N)}"/>-time computability) then predicting the result of measuring all the qubits in a stabilizer circuit is also “mildly hard.” Such mild hardness would represent a <em>gap</em> between the <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2)}"/> time for weak simulation and the time for strong simulation. Such gaps have been noted and proved for extensions of stabilizer circuits but those are between “polynomial” and an intractable hardness notion.</p>
<p>
One can also view Theorem <a href="https://rjlipton.wordpress.com/feed/#rank2QC">3</a> as a possible avenue toward computing matrix rank without doing either matrix multiplication or Gaussian elimination. This is the view Chaowen and I have had all along. </p>
<p>
</p><p/><h2> From Quantum to Rank </h2><p/>
<p/><p>
The distinguishing point of our converse reduction to the rank <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> is <em>knowledge of normal forms that depend on <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/></em> where one can use the knowledge <em>to delay or avoid computing them explicitly</em>. The normal forms are for polynomials <img alt="{f_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_C}"/> associated to quantum circuits <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> in our <a href="https://rjlipton.wordpress.com/2012/07/08/grilling-quantum-circuits/">earlier</a> <a href="https://link.springer.com/chapter/10.1007/978-3-662-56499-8_4">work</a>. Stabilizer circuits yield <img alt="{f_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_C}"/> as a <em>classical quadratic form</em> over <img alt="{\mathbb{Z}_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_4}"/>, the integers modulo <img alt="{4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{4}"/>. That is, all cross terms <img alt="{x_i x_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_i+x_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_i x_j}"/> in <img alt="{f_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_C}"/> have even coefficients—here, <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> or <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>. Thus quantum computing enters a debate that occupied Carl Gauss and others over two hundred years ago:</p>
<blockquote><p><b> </b> <em> Should every homogeneous quadratic polynomial <img alt="{f(x_1,\dots,x_n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f(x_1,\dots,x_n)}"/> with integer coefficients be called a <b>quadratic form</b>, or only those whose cross terms <img alt="{c_{i,j}x_i x_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_%7Bi%2Cj%7Dx_i+x_j%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{c_{i,j}x_i x_j}"/> all have even coefficients <img alt="{c_{i,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_%7Bi%2Cj%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{c_{i,j}}"/>? </em>
</p></blockquote>
<p/><p>
The point of even coefficients is that they enable having a symmetric <img alt="{n \times n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n \times n}"/> <em>integer</em> matrix <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> such that </p>
<p align="center"><img alt="\displaystyle  f(x) = x^\top S x " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%29+%3D+x%5E%5Ctop+S+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f(x) = x^\top S x "/></p>
<p>for all <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>. Without that condition, <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> might only be half-integral. This old difference turns out to mirror that between universal quantum computing and classical, because the non-Clifford <img alt="{\mathsf{CS}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BCS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{CS}}"/>-gate noted above yields circuits <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> whose <img alt="{f_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_C}"/> over <img alt="{\mathbb{Z}_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_4}"/> have terms <img alt="{x_i x_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_i+x_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_i x_j}"/> and/or <img alt="{3 x_i x_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3+x_i+x_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3 x_i x_j}"/>. While counting solutions in <img alt="{\mathbb{Z}_4^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_4^n}"/> for those polynomials is in <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{P}}"/>, counting their <em>binary</em> solutions is <img alt="{\mathsf{\#P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{\#P}}"/>-complete—an amazing dichotomy we expounded <a href="https://rjlipton.wordpress.com/2017/11/20/a-magic-madison-visit/">here</a>.</p>
<p>
We hasten to add that for <img alt="{k = 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%3D+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k = 2}"/> the classical forms coincide with those over <img alt="{\mathbb{Z}_{2^k}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_%7B2%5Ek%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_{2^k}}"/> whose nonzero cross terms all have coefficient <img alt="{2^{k-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bk-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{k-1}}"/>. Those are called <em>affine</em> in the work by Jin-Yi and others noted above, and our above-mentioned <a href="https://rjlipton.wordpress.com/2017/11/20/a-magic-madison-visit">post</a> noted his 2017 <a href="https://arxiv.org/abs/1705.00942">paper</a> with Heng Guo and Tyson Williams giving another proof of polynomial-time simulation of stabilizer circuits via <img alt="{f_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_C}"/> being affine. Our work improving the polynomial bounds, however, draws on a 2009 <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.730.2154">paper</a> by Kai-Uwe Schmidt and further theory of classical quadratic forms. This paper uses work going back to 1938 that decomposes a classical (affine) quadratic form <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> over <img alt="{\mathsf{Z}_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{Z}_4}"/> further as <a name="repn"/></p><a name="repn">
<p align="center"><img alt="\displaystyle  f(x) = f_0(x) + 2(x \bullet v) \quad\text{with}\quad f_0(x) = x^\top B x, \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%29+%3D+f_0%28x%29+%2B+2%28x+%5Cbullet+v%29+%5Cquad%5Ctext%7Bwith%7D%5Cquad+f_0%28x%29+%3D+x%5E%5Ctop+B+x%2C+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f(x) = f_0(x) + 2(x \bullet v) \quad\text{with}\quad f_0(x) = x^\top B x, \ \ \ \ \ (1)"/></p>
</a><p><a name="repn"/> for binary arguments <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>. Here <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/> is a binary vector with <img alt="{v_i = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_i+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_i = 1}"/> if <img alt="{S[i,i] = 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%5Bi%2Ci%5D+%3D+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S[i,i] = 2}"/> or <img alt="{S[i,i] = 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%5Bi%2Ci%5D+%3D+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S[i,i] = 3}"/>, <img alt="{v_i =0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_i+%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_i =0}"/> otherwise, and the operations including the inner product <img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> are mod-2 except that the final <img alt="{+}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%2B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{+}"/> is in <img alt="{\mathbb{Z}_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_4}"/>. Then <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> is <em>alternating</em> if the diagonal of <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> is all-zero, <em>non-alternating</em> otherwise. Now take <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> to be the rank of <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/>. The key normal-form lemma is:</p>
<blockquote><p><b>Lemma 2</b> <em> There is a change of basis to <img alt="{y_1,\dots,y_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By_1%2C%5Cdots%2Cy_n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{y_1,\dots,y_n}"/> such that if <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f}"/> is non-alternating then <img alt="{f_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f_0}"/> is transformed to </em></p><em>
<p align="center"><img alt="\displaystyle  f'_0(y) = y_1 + y_2 + \cdots + y_r, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%27_0%28y%29+%3D+y_1+%2B+y_2+%2B+%5Ccdots+%2B+y_r%2C+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  f'_0(y) = y_1 + y_2 + \cdots + y_r, "/></p>
<p>whereas if <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f}"/> is alternating then <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{r}"/> is even and <img alt="{f_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f_0}"/> is transformed to </p>
<p align="center"><img alt="\displaystyle  f'_0(y) = 2y_1 y_2 + 2y_3 y_4 + \cdots + 2y_{r-1} y_r. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%27_0%28y%29+%3D+2y_1+y_2+%2B+2y_3+y_4+%2B+%5Ccdots+%2B+2y_%7Br-1%7D+y_r.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  f'_0(y) = 2y_1 y_2 + 2y_3 y_4 + \cdots + 2y_{r-1} y_r. "/></p>
</em><p><em>In either case, there is a binary vector <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{w}"/> so that <img alt="{f(y) = f'_0(y) + 2(y \bullet w)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28y%29+%3D+f%27_0%28y%29+%2B+2%28y+%5Cbullet+w%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f(y) = f'_0(y) + 2(y \bullet w)}"/> for all <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{y}"/>. </em>
</p></blockquote>
<p/><p>
The point is that to evaluate the quantum circuit <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>, we don’t need to evaluate <img alt="{f_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_C}"/>, but can make inferences about the structure of the solution sets to <img alt="{f_C(x) = a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_C%28x%29+%3D+a%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_C(x) = a}"/> for <img alt="{a = 0,1,2,3 \pmod{4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%3D+0%2C1%2C2%2C3+%5Cpmod%7B4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a = 0,1,2,3 \pmod{4}}"/>, where <img alt="{x \in \{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cin+%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x \in \{0,1\}^n}"/>. Given the knowledge of <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/>, the normal form goes a long way to this. The vector <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> is also needed, but the fact of its having only <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> bits gives hope of finding it in <img alt="{O(n^2) = O(N)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29+%3D+O%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2) = O(N)}"/> time. That—plus an analysis of the normal form <img alt="{f'_0,w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%27_0%2Cw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f'_0,w}"/> itself of course—would complete an <img alt="{O(N)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(N)}"/>-time reduction from computing the amplitude to computing <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/>.</p>
<p>
</p><p/><h2> The Needed Piece—For Now </h2><p/>
<p/><p>
Chaowen took the lead all through the Fall 2018 term in trying multiple attacks. In the non-alternating case, the change of basis converts <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> into a diagonal matrix <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> over <img alt="{\mathbb{F}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{F}_2}"/>. In the alternating case, the same process makes <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> a block-diagonal matrix of the kind we mentioned above. The conversion <img alt="{D = Q B Q^\top}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD+%3D+Q+B+Q%5E%5Ctop%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D = Q B Q^\top}"/> in both cases also yields <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/>. Of course <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q}"/> can be computed by Gaussian elimination in <img alt="{O(n^3)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^3)}"/> time, but this is what we wanted to avoid.</p>
<p>
After poring over older literature on <img alt="{n^\omega}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^\omega}"/>-time methods, including a 1974 <a href="https://www.ams.org/journals/mcom/1974-28-125/S0025-5718-1974-0331751-8/">paper</a> by James Bunch and John Hopcroft (see also <a href="http://renatoppl.com/blog/2014/08/12/solving-linear-systems-and-inverting-a-matrix-is-equivalent-to-matrix-multiplication/">this</a>), we found a <a href="https://arxiv.org/abs/1802.10453">paper</a> from last year by Jean-Guillaume Dumas and Clemens Pernet that gives exactly what we needed: an LDU-type decomposition that yields <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> in <img alt="{O(n^\omega)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^\omega)}"/> time. We only needed to apply the change-of-basis analysis in Schmidt’s paper to this decomposition and combine with the normal-form analysis to establish our algorithm for computing the amplitude <img alt="{\langle 0^n |C| 0^n \rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\langle 0^n |C| 0^n \rangle}"/>: </p>
<ol>
<li>
Convert <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> to the classical quadratic form <img alt="{f_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_C}"/> with matrix <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> over <img alt="{\mathbb{Z}_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_4}"/> and associate the <img alt="{n \times n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n \times n}"/> matrix <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> over <img alt="{\mathbb{Z}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_2}"/> as above. This needs only <img alt="{O(N)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(N)}"/> time. <p/>
</li><li>
Compute the Dumas-Pernet decomposition <img alt="{B = PLDL^\top P^\top}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB+%3D+PLDL%5E%5Ctop+P%5E%5Ctop%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B = PLDL^\top P^\top}"/> over <img alt="{\mathbb{Z}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_2}"/> where <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> is a permutation matrix, <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L}"/> is lower-triangular, and <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> is block-diagonal with blocks that are either <img alt="{1 \times 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%5Ctimes+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 \times 1}"/> or <img alt="{2 \times 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2+%5Ctimes+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2 \times 2}"/>. Of course, this involves computing the rank <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> of <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> and takes <img alt="{O(n^\omega)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^\omega)}"/> time. Think of it as <img alt="{D = QBQ^\top}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD+%3D+QBQ%5E%5Ctop%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D = QBQ^\top}"/>. This takes <img alt="{O(n^\omega)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^\omega)}"/> time—indeed, <img alt="{O(n^2 r^{\omega - 2})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2+r%5E%7B%5Comega+-+2%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2 r^{\omega - 2})}"/> time according to Dumas and Pernet. <p/>
</li><li>
Compute <img alt="{D' = Q S Q^\top}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%27+%3D+Q+S+Q%5E%5Ctop%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D' = Q S Q^\top}"/> over <img alt="{\mathbb{Z}_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_4}"/>. This, too, takes <img alt="{O(n^\omega)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^\omega)}"/> time. <p/>
</li><li>
If any diagonal <img alt="{1 \times 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%5Ctimes+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 \times 1}"/> block of the original <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> has become <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/> in <img alt="{D'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D'}"/>, output <img alt="{\langle 0^n |C| 0^n \rangle = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\langle 0^n |C| 0^n \rangle = 0}"/>. Else, <img alt="{\langle 0^n |C| 0^n \rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\langle 0^n |C| 0^n \rangle}"/> is nonzero and we have enough information about <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> and <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> to find it—in only <img alt="{O(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n)}"/> time, in fact.
</li></ol>
<p>
This proves our main theorem:</p>
<blockquote><p><b>Theorem 3</b> <em><a name="rank2QC"/> For stabilizer circuits <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{C}"/>, <img alt="{\langle 0^n |C| 0^n \rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\langle 0^n |C| 0^n \rangle}"/> is computable in <img alt="{O(n^\omega)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{O(n^\omega)}"/> time. So is counting binary solutions to a classical quadratic form over <img alt="{\mathbb{Z}_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_4}"/>, or any quadratic polynomial mod 2. </em>
</p></blockquote>
<p/><p>
Because we use the decomposition, the above is not a clean <img alt="{O(N)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(N)}"/>-time reduction to computing <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/>. It does not make Theorem <a href="https://rjlipton.wordpress.com/feed/#rank2QC">3</a> into a linear-time equivalence. By further analysis, however, we show that the only impediment is needing <img alt="{D'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D'}"/> in step 4 of our algorithm to tell whether <img alt="{\langle 0^n |C| 0^n \rangle = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\langle 0^n |C| 0^n \rangle = 0}"/>. If we are <a href="https://rjlipton.wordpress.com/2010/09/05/promise-problems-and-twopaths/">promised</a> that it is nonzero, then we obtain the probability <img alt="{|\langle 0^n |C| 0^n \rangle|^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|\langle 0^n |C| 0^n \rangle|^2}"/> in <img alt="{O(N)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(N)}"/> time from <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> alone. This is actually where the power of Chaowen’s analysis of the normal forms is brightest and neatest. We will devote further posts to this and to illuminating further connections in graph and matroid theory.</p>
<p>
</p><p/><h2> A Three-Part Example </h2><p/>
<p/><p>
Consider the following quantum circuit <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. OK, this is a very low-tech drawing. Besides the six Hadamard gates it has two <img alt="{\mathsf{CZ}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BCZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{CZ}}"/> gates, which are shown as simple bars since they are symmetric:</p>
<p>
<a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/c1/" rel="attachment wp-att-15927"><img alt="" class="aligncenter wp-image-15927" height="112" src="https://rjlipton.files.wordpress.com/2019/06/c1.png?w=240&amp;h=112" width="240"/></a></p>
<p>
By the rules given <a href="https://rjlipton.wordpress.com/2012/07/08/grilling-quantum-circuits/">here</a>, the three Hadamard gates at left introduce “nondeterministic variables” <img alt="{x_1,x_2,x_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_1%2Cx_2%2Cx_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_1,x_2,x_3}"/>. The three Hadamard gates at right also give nondeterministic variables, but they are immediately equated to the output variables <img alt="{z_1,z_2,z_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz_1%2Cz_2%2Cz_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z_1,z_2,z_3}"/> so we skip them. The polynomial <img alt="{q_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_C}"/> is </p>
<p align="center"><img alt="\displaystyle  2u_1 x_1 + 2u_2 x_2 + 2u_3 x_3 + 2x_1 x_2 + 2 x_2 x_3 + 2 x_1 z_1 + 2 x_2 z_2 + 2 x_3 z_3. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2u_1+x_1+%2B+2u_2+x_2+%2B+2u_3+x_3+%2B+2x_1+x_2+%2B+2+x_2+x_3+%2B+2+x_1+z_1+%2B+2+x_2+z_2+%2B+2+x_3+z_3.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  2u_1 x_1 + 2u_2 x_2 + 2u_3 x_3 + 2x_1 x_2 + 2 x_2 x_3 + 2 x_1 z_1 + 2 x_2 z_2 + 2 x_3 z_3. "/></p>
<p>Upon substituting <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> for all of <img alt="{u_1,u_2,u_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_1%2Cu_2%2Cu_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_1,u_2,u_3}"/> and <img alt="{z_1,z_2,z_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz_1%2Cz_2%2Cz_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z_1,z_2,z_3}"/> this gives simply <img alt="{f(x) = 2x_1 x_2 + 2 x_2 x_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29+%3D+2x_1+x_2+%2B+2+x_2+x_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x) = 2x_1 x_2 + 2 x_2 x_3}"/>. This is an alternating form with </p>
<p align="center"><img alt="\displaystyle  S = B = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S+%3D+B+%3D+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  S = B = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}, "/></p>
<p>which is the adjacency matrix of the path graph of length 2 on <img alt="{n = 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n = 3}"/> vertices. Gaussian elimination does not need any prior swaps, so the permutation matrix <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> in the decomposition is the identity and we get </p>
<p align="center"><img alt="\displaystyle  Q = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix}, \quad\text{giving}\quad D = QBQ^\top = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{bmatrix} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5Cend%7Bbmatrix%7D%2C+%5Cquad%5Ctext%7Bgiving%7D%5Cquad+D+%3D+QBQ%5E%5Ctop+%3D+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+0+%5C%5C+0+%26+0+%26+0+%5Cend%7Bbmatrix%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  Q = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix}, \quad\text{giving}\quad D = QBQ^\top = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{bmatrix} "/></p>
<p>as the block-diagonal matrix over <img alt="{\mathbb{Z}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_2}"/>. Now we re-compute the products over <img alt="{\mathbb{Z}_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_4}"/> to get </p>
<p align="center"><img alt="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^\top \!=\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 2 &amp; 0 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 2 \\ 0 &amp; 2 &amp; 0 \end{bmatrix} \!=\! D'. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q+S+Q%5E%7B%5Ctop%7D+%5C%21%5C%21%3D%5C%21%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot+Q%5E%5Ctop+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+2+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5C%5C+0+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+2+%5C%5C+0+%26+2+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+D%27.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^\top \!=\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 2 &amp; 0 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 2 \\ 0 &amp; 2 &amp; 0 \end{bmatrix} \!=\! D'. "/></p>
<p>Now <img alt="{D'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D'}"/> has entries that are <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/> but they are off-diagonal, and hence cancel when <img alt="{y^\top D' y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%5E%5Ctop+D%27+y%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y^\top D' y}"/> is computed in the <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/>-basis. Since <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> is likewise the zero vector, this gives the transformed form as </p>
<p align="center"><img alt="\displaystyle  f'_0(y_1,y_2,y_3) = 2y_1 y_2. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%27_0%28y_1%2Cy_2%2Cy_3%29+%3D+2y_1+y_2.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f'_0(y_1,y_2,y_3) = 2y_1 y_2. "/></p>
<p>It is easy to compute that <img alt="{f'_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%27_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f'_0}"/> has six values of 0 and two values of 2, which gives the amplitude as the difference <img alt="{6 - 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B6+-+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{6 - 2}"/> divided by the square root of <img alt="{2^6}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E6%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^6}"/>, so <img alt="{\frac{1}{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{1}{2}}"/>, The probability of getting <img alt="{000}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B000%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{000}"/> as the result of the measurement is <img alt="{\frac{1}{4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{1}{4}}"/>.</p>
<p>
Now suppose we insert a <img alt="{\mathsf{Z}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{Z}}"/>-gate <img alt="{\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -1 \end{bmatrix}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbegin%7Bbmatrix%7D+1+%26+0+%5C%5C+0+%26+-1+%5Cend%7Bbmatrix%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -1 \end{bmatrix}}"/> on the first qubit to make a new circuit <img alt="{C_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_2}"/>. Since <img alt="{\mathsf{Z}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{Z}}"/> and <img alt="{\mathsf{CZ}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BCZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{CZ}}"/> are diagonal in the standard basis it does not matter where between the Hadamard gates it goes, say:</p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/c2/" rel="attachment wp-att-15928"><img alt="" class="aligncenter wp-image-15928" height="106" src="https://rjlipton.files.wordpress.com/2019/06/c2.png?w=240&amp;h=106" width="240"/></a></p>
<p/><p><br/>
After substituting zeroes the form over <img alt="{\mathbb{Z}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}}"/> is <img alt="{g = 2x_1 x_2 + 2 x_2 x_3 + 2x_1^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg+%3D+2x_1+x_2+%2B+2+x_2+x_3+%2B+2x_1%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g = 2x_1 x_2 + 2 x_2 x_3 + 2x_1^2}"/>. This gives </p>
<p align="center"><img alt="\displaystyle  S = \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix},\quad v = (1,0,0), \quad B = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S+%3D+%5Cbegin%7Bbmatrix%7D+2+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D%2C%5Cquad+v+%3D+%281%2C0%2C0%29%2C+%5Cquad+B+%3D+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  S = \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix},\quad v = (1,0,0), \quad B = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}. "/></p>
<p>The matrix <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> is the same as in the first example, hence so are the matrices <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q}"/> and <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> and the alternating status of <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/>. The difference made by <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/> and the resulting <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> makes itself felt when we re-compute over <img alt="{\mathbb{Z}_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_4}"/>: </p>
<p align="center"><img alt="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^\top \!=\! \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 2 &amp; 2 &amp; 0 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 2 &amp; 1 &amp; 2 \\ 1 &amp; 0 &amp; 2 \\ 2 &amp; 2 &amp; 2 \end{bmatrix} \!=\! D'. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q+S+Q%5E%7B%5Ctop%7D+%5C%21%5C%21%3D%5C%21%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21+%5Cbegin%7Bbmatrix%7D+2+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot+Q%5E%5Ctop+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+2+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+2+%26+2+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5C%5C+0+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+2+%26+1+%26+2+%5C%5C+1+%26+0+%26+2+%5C%5C+2+%26+2+%26+2+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+D%27.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^\top \!=\! \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 2 &amp; 2 &amp; 0 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 2 &amp; 1 &amp; 2 \\ 1 &amp; 0 &amp; 2 \\ 2 &amp; 2 &amp; 2 \end{bmatrix} \!=\! D'. "/></p>
<p>Well, <img alt="{D'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D'}"/> is far from diagonal—perhaps we shouldn’t use that name—but again the off-diagonal <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>s are innocuous so we really have </p>
<p align="center"><img alt="\displaystyle  D'' = \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 2 \end{bmatrix}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%27%27+%3D+%5Cbegin%7Bbmatrix%7D+2+%26+1+%26+0+%5C%5C+1+%26+0+%26+0+%5C%5C+0+%26+0+%26+2+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  D'' = \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 2 \end{bmatrix}. "/></p>
<p>The <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/> at upper left does not zero out the amplitude, because it is within a <img alt="{2 \times 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2+%5Ctimes+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2 \times 2}"/> block. The <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/> at lower right, however, constitutes a <img alt="{1 \times 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%5Ctimes+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 \times 1}"/> block of <img alt="{D''}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%27%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D''}"/>, so it signifies that <img alt="{000}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B000%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{000}"/> is not a possible measurement outcome. Essentially what has happened is that in the <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/>-basis the form has become </p>
<p align="center"><img alt="\displaystyle  g'(y) = 2y_1^2 + 2y_1 y_2 + 2y_3^2. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%27%28y%29+%3D+2y_1%5E2+%2B+2y_1+y_2+%2B+2y_3%5E2.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  g'(y) = 2y_1^2 + 2y_1 y_2 + 2y_3^2. "/></p>
<p>The isolated term in <img alt="{y_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y_3}"/> contributes <img alt="{+2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%2B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{+2}"/> mod <img alt="{4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{4}"/> to half the <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>–<img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> assignments so as to cancel the other half, leaving a difference of <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> in the numerator of the amplitude.</p>
<p>
For the third example, let us insert a phase gate <img alt="{\mathsf{S}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{S}}"/> after the <img alt="{\mathsf{Z}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{Z}}"/> to make a circuit <img alt="{C_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_3}"/>:</p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/c3/" rel="attachment wp-att-15929"><img alt="" class="aligncenter wp-image-15929" height="90" src="https://rjlipton.files.wordpress.com/2019/06/c3.png?w=240&amp;h=90" width="240"/></a></p>
<p/><p><br/>
The <img alt="{\mathsf{ZS}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{ZS}}"/> combination is the same as <img alt="{\mathsf{S^*}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BS%5E%2A%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{S^*}}"/>, the adjoint (and inverse) of <img alt="{\mathsf{S}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{S}}"/>. Now after substitutions we have <img alt="{h_{C_3}(x) = 2x_1 x_2 + 2 x_2 x_3 + 3x_1^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh_%7BC_3%7D%28x%29+%3D+2x_1+x_2+%2B+2+x_2+x_3+%2B+3x_1%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h_{C_3}(x) = 2x_1 x_2 + 2 x_2 x_3 + 3x_1^2}"/>, giving: </p>
<p align="center"><img alt="\displaystyle  S = \begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix},\quad v = (1,0,0), \quad B = \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S+%3D+%5Cbegin%7Bbmatrix%7D+3+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D%2C%5Cquad+v+%3D+%281%2C0%2C0%29%2C+%5Cquad+B+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  S = \begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix},\quad v = (1,0,0), \quad B = \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}. "/></p>
<p>Note that <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> is still a 0-1 matrix. This <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> has full rank. Again it helps our exposition that <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> is diagonalizable without swaps (and that the inverse of an invertible lower-triangular matrix is lower-triangular), so we can find <img alt="{QBQ^\top = D = I}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQBQ%5E%5Ctop+%3D+D+%3D+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{QBQ^\top = D = I}"/> with </p>
<p align="center"><img alt="\displaystyle  Q = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \end{bmatrix}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+1+%26+1+%26+0+%5C%5C+1+%26+1+%26+1+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  Q = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \end{bmatrix}. "/></p>
<p>In the <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/>-basis we get <img alt="{h'(y) = y_1 + y_2 + y_3 + 2(y \bullet w)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%27%28y%29+%3D+y_1+%2B+y_2+%2B+y_3+%2B+2%28y+%5Cbullet+w%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h'(y) = y_1 + y_2 + y_3 + 2(y \bullet w)}"/> for some <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/>. To test for zero amplitude—before we know what <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> is—we compute in <img alt="{\mathbb{Z}_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_4}"/>: </p>
<p align="center"><img alt="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \end{bmatrix} \!\cdot\!\begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^{\top} \!=\! \begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 2 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 3 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 2 \\ 0 &amp; 2 &amp; 3 \end{bmatrix} \!=\! D'. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q+S+Q%5E%7B%5Ctop%7D+%5C%21%5C%21%3D%5C%21%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+1+%26+1+%26+0+%5C%5C+0+%26+1+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21%5Cbegin%7Bbmatrix%7D+3+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot+Q%5E%7B%5Ctop%7D+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+3+%26+1+%26+0+%5C%5C+0+%26+1+%26+1+%5C%5C+0+%26+2+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+1+%26+1+%5C%5C+0+%26+1+%26+1+%5C%5C+0+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+3+%26+0+%26+0+%5C%5C+0+%26+1+%26+2+%5C%5C+0+%26+2+%26+3+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+D%27.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \end{bmatrix} \!\cdot\!\begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^{\top} \!=\! \begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 2 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 3 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 2 \\ 0 &amp; 2 &amp; 3 \end{bmatrix} \!=\! D'. "/></p>
<p>Again we can ignore the off-diagonal <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>‘s. There is no <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/> on the main diagonal, so we know the amplitude is non-zero. To compute it, we only need the information on the diagonal, which tells us <img alt="{h'_0(y) = y_1 + y_2 + y_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%27_0%28y%29+%3D+y_1+%2B+y_2+%2B+y_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h'_0(y) = y_1 + y_2 + y_3}"/> and <img alt="{w = (1,0,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw+%3D+%281%2C0%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w = (1,0,1)}"/> in the transformed basis. Note that we could have written <img alt="{h'_0(y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%27_0%28y%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h'_0(y)}"/> down the moment we learned that <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> has rank <img alt="{3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3}"/> over <img alt="{\mathbb{F}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{F}_2}"/>, so <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> is the only rigmarole. The final analysis—using a recursion detailed in the appendix of our paper—gives the amplitude as </p>
<p align="center"><img alt="\displaystyle  \frac{2 - 2i}{8} = \frac{1 - i}{4}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B2+-+2i%7D%7B8%7D+%3D+%5Cfrac%7B1+-+i%7D%7B4%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac{2 - 2i}{8} = \frac{1 - i}{4}, "/></p>
<p>and so the probability of the output <img alt="{000}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B000%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{000}"/> is <img alt="{\frac{1}{8}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B8%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{1}{8}}"/>. </p>
<p>
We remark finally that <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> is generally not the same as <img alt="{Qv}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Qv}"/>. To see where it comes from, let us now compute <img alt="{Q B Q^{\top}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ+B+Q%5E%7B%5Ctop%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q B Q^{\top}}"/> (not <img alt="{QSQ^\top}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQSQ%5E%5Ctop%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{QSQ^\top}"/>) over <img alt="{\mathbb{Z}_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_4}"/> to get <img alt="{QBQ^\top = D + 2U}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQBQ%5E%5Ctop+%3D+D+%2B+2U%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{QBQ^\top = D + 2U}"/>. Then </p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}  f(x) &amp;=&amp; x^\top B x + 2x^\top v = x^\top Q^{-1} (D+2U) (Q^\top)^{-1} x + 2x^\top (Q^{-1} Q) v\\ &amp;=&amp; y^\top (D + 2U) y + 2 y^\top Qv, \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++f%28x%29+%26%3D%26+x%5E%5Ctop+B+x+%2B+2x%5E%5Ctop+v+%3D+x%5E%5Ctop+Q%5E%7B-1%7D+%28D%2B2U%29+%28Q%5E%5Ctop%29%5E%7B-1%7D+x+%2B+2x%5E%5Ctop+%28Q%5E%7B-1%7D+Q%29+v%5C%5C+%26%3D%26+y%5E%5Ctop+%28D+%2B+2U%29+y+%2B+2+y%5E%5Ctop+Qv%2C+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}  f(x) &amp;=&amp; x^\top B x + 2x^\top v = x^\top Q^{-1} (D+2U) (Q^\top)^{-1} x + 2x^\top (Q^{-1} Q) v\\ &amp;=&amp; y^\top (D + 2U) y + 2 y^\top Qv, \end{array} "/></p>
<p>where <img alt="{y = (Q^\top)^{-1} x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By+%3D+%28Q%5E%5Ctop%29%5E%7B-1%7D+x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y = (Q^\top)^{-1} x}"/>. Now off-diagonal elements in <img alt="{2U}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2U%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2U}"/> will cancel when taking <img alt="{2 y^\top U y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2+y%5E%5Ctop+U+y%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2 y^\top U y}"/> modulo 4, so we need only retain the diagonal <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u}"/> of <img alt="{U}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{U}"/> as a binary vector. Since <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> is binary, <img alt="{y^\top \mathit{diag}(u) y = y^\top u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%5E%5Ctop+%5Cmathit%7Bdiag%7D%28u%29+y+%3D+y%5E%5Ctop+u%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y^\top \mathit{diag}(u) y = y^\top u}"/>. This finally gives </p>
<p align="center"><img alt="\displaystyle  f(x) = y^\top D y + 2y^\top (u + Qv) = y^\top D y + 2(y \bullet w) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%29+%3D+y%5E%5Ctop+D+y+%2B+2y%5E%5Ctop+%28u+%2B+Qv%29+%3D+y%5E%5Ctop+D+y+%2B+2%28y+%5Cbullet+w%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f(x) = y^\top D y + 2y^\top (u + Qv) = y^\top D y + 2(y \bullet w) "/></p>
<p>with <img alt="{w = u + Qv}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw+%3D+u+%2B+Qv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w = u + Qv}"/>. In the third example we have <img alt="{Qv = (1,1,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQv+%3D+%281%2C1%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Qv = (1,1,1)}"/> and </p>
<p align="center"><img alt="\displaystyle  QBQ^{\top} = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \cdot Q^\top = \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 2 &amp; 1 &amp; 1 \\ 2 &amp; 2 &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} = \begin{bmatrix} 1 &amp; 2 &amp; 2 \\ 2 &amp; 3 &amp; 0 \\ 2 &amp; 0 &amp; 1 \end{bmatrix}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++QBQ%5E%7B%5Ctop%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+1+%26+1+%26+0+%5C%5C+0+%26+1+%26+1+%5Cend%7Bbmatrix%7D+%5Ccdot+%5Cbegin%7Bbmatrix%7D+1+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D+%5Ccdot+Q%5E%5Ctop+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+1+%26+0+%5C%5C+2+%26+1+%26+1+%5C%5C+2+%26+2+%26+1+%5Cend%7Bbmatrix%7D+%5Ccdot+%5Cbegin%7Bbmatrix%7D+1+%26+1+%26+1+%5C%5C+0+%26+1+%26+1+%5C%5C+0+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+2+%26+2+%5C%5C+2+%26+3+%26+0+%5C%5C+2+%26+0+%26+1+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  QBQ^{\top} = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \cdot Q^\top = \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 2 &amp; 1 &amp; 1 \\ 2 &amp; 2 &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} = \begin{bmatrix} 1 &amp; 2 &amp; 2 \\ 2 &amp; 3 &amp; 0 \\ 2 &amp; 0 &amp; 1 \end{bmatrix}. "/></p>
<p>The diagonal gives <img alt="{u = (0,1,0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu+%3D+%280%2C1%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u = (0,1,0)}"/> and so <img alt="{w = u + Qv \pmod{2} = (1,0,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw+%3D+u+%2B+Qv+%5Cpmod%7B2%7D+%3D+%281%2C0%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w = u + Qv \pmod{2} = (1,0,1)}"/>. This agrees with what we read off above by comparing <img alt="{D'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D'}"/> with <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/>. There is a different worked-out example for the triangle graph on three vertices in the paper.</p>
<p>
</p><p/><h2> Looking Ahead </h2><p/>
<p/><p>
Chaowen and I continue to be interested in shortcuts to computing the amplitude and/or probability. Here we take a cue from how Volker Strassen titled his famous 1969 <a href="https://eudml.org/doc/131927">paper</a> on matrix multiplication:</p>
<blockquote><p><b> </b> <em> “Gaussian Elimination is not Optimal.” </em>
</p></blockquote>
<p/><p>
We would like to find cases where we can say, “Matrix Multiplication is not Optimal.” In view of recent papers blunting efforts to show <img alt="{\omega = 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Comega+%3D+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\omega = 2}"/>—see this <a href="https://rjlipton.wordpress.com/2018/08/30/limits-on-matrix-multiplication/">post</a>—the question may shift to which computations may not need the full power of matrix multiplication and be achievable in <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2)}"/> time after all. This applies to computing the rank (over <img alt="{\mathbb{Z}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_2}"/>) itself, and the question extends to sparse cases like those considered in the <a href="http://www-scf.usc.edu/~hoyeeche/papers/matrix-rank.pdf">paper</a>, “Fast Matrix Rank Algorithms and Applications,” by Ho Yee Cheung, Tsz Chiu Kwok, and Lap Chi Lau.</p>
<p>
The second circuit in the above example corresponds to a graph with a self-loop at node 1—or, depending on how one counts incidence of self-loops in undirected graphs, one could call it a double self-loop. It exemplifies circuits used to create quantum <a href="https://en.wikipedia.org/wiki/Graph_state">graph states</a>, and those circuits are representative of stabilizer circuits in general. The third circuit can be said to have a “triple loop,” or maybe better, a “3/2-loop”—while if the original <img alt="{\mathsf{Z}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{Z}}"/>-gate were a single <img alt="{\mathsf{S}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{S}}"/>-gate giving the form <img alt="{x_1^2 + 2x_1 x_2 + 2 x_2 x_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_1%5E2+%2B+2x_1+x_2+%2B+2+x_2+x_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_1^2 + 2x_1 x_2 + 2 x_2 x_3}"/>, we would face the ambiguity of calling it a “loop” or a “half-loop.” Sorting this out properly needs going beyond graph theory. In upcoming posts, Chaowen and I will say more about how all this yields new problems in graph theory and new connections between quantum computing and <em>matroid theory</em>.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
What do our results say about the problem of computing the rank of a matrix, and possibly separating it from dependence on matrix multiplication?</p>
<p>
We hope that we have begun to convey how our paper uncovers a lot of fun computational mathematics. We are grateful for communications from people we’ve approached (some acknowledged in our paper) about possible known connections, but there may be more we don’t know. Our next posts will say more about combinatorial aspects of quantum circuits.</p>
<p/></font></font></div>
    </content>
    <updated>2019-06-04T19:55:03Z</updated>
    <published>2019-06-04T19:55:03Z</published>
    <category term="algorithms"/>
    <category term="All Posts"/>
    <category term="News"/>
    <category term="Proofs"/>
    <category term="Results"/>
    <category term="algebra"/>
    <category term="Chaowen Guan"/>
    <category term="concrete complexity"/>
    <category term="counting"/>
    <category term="linear algebra"/>
    <category term="polynomial simulation"/>
    <category term="quadratic forms"/>
    <category term="quantum"/>
    <category term="solution sets"/>
    <category term="stabilizer circuits"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-06-04T23:58:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1500</id>
    <link href="https://theorydish.blog/2019/06/04/itcs20-call-for-papers/" rel="alternate" type="text/html"/>
    <title>ITCS’20 Call for Papers</title>
    <summary>ITCS is one of my favorite (if not my favorite) conferences, with not only great and insightful papers but also a friendly atmosphere. This year should be no exception! tl;dr: the ITCS’20 CFP has been posted. Read it, and submit your work there!   We invite you to submit your papers to the 11th Innovations in Theoretical Computer Science (ITCS). The conference will be held at the University of Washington in Seattle, Washington from January 12-14, 2020. ITCS seeks to promote research that carries a strong conceptual message (e.g., introducing a new concept, model or understanding, opening a new line of inquiry within traditional or interdisciplinary areas, introducing new mathematical techniques and methodologies, or new applications of known techniques). ITCS welcomes both conceptual and technical contributions whose contents will advance and inspire the greater theory community. Important dates Submission deadline: September 9, 2019 (05:59pm PDT) Notification to authors: October 31, 2019 Conference dates: January 12-14, 2020 See the website at http://itcs-conf.org/itcs20/itcs20-cfp.html for detailed information regarding submissions. Program committee Nikhil Bansal, CWI + TU Eindhoven Nir Bitansky, Tel-Aviv University Clement Canonne, Stanford Timothy Chan, University of Ilinois at Urbana-Champaign Edith Cohen, Google and Tel-Aviv University Shaddin Dughmi, University of Southern California Sumegha Garg, [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p style="text-align: justify;"><em>ITCS is one of my favorite (if not my favorite) conferences, with not only great and insightful papers but also a friendly atmosphere. This year should be no exception!</em></p>
<p><em><strong>tl;dr:</strong> the ITCS’20 CFP has been <a href="http://itcs-conf.org/itcs20/itcs20-cfp.html" rel="noopener" target="_blank">posted</a>. Read it, and submit your work there!</em></p>
<hr/>
<p> </p>
<p style="text-align: justify;">We invite you to submit your papers to the <a href="http://itcs-conf.org/" rel="noopener" target="_blank">11th Innovations in</a> <a href="http://itcs-conf.org/" rel="noopener" target="_blank">Theoretical Computer Science</a> (ITCS). The conference will be held at the University of Washington in Seattle, Washington from January 12-14, 2020.</p>
<p style="text-align: justify;">ITCS seeks to promote research that carries a strong conceptual message (e.g., introducing a new concept, model or understanding, opening a new line of inquiry within traditional or interdisciplinary areas, introducing new mathematical techniques and methodologies, or new applications of known techniques). ITCS welcomes both conceptual and technical contributions whose contents will advance and inspire the<br/>
greater theory community.</p>
<p><strong>Important dates</strong></p>
<ul>
<li><em>Submission deadline:</em> September 9, 2019 (05:59pm PDT)</li>
<li><em>Notification to authors:</em> October 31, 2019</li>
<li><em>Conference dates:</em> January 12-14, 2020</li>
</ul>
<p>See the website at <a href="http://itcs-conf.org/itcs20/itcs20-cfp.html">http://itcs-conf.org/itcs20/itcs20-cfp.html</a> for detailed information regarding submissions.</p>
<p><strong>Program committee</strong></p>
<p>Nikhil Bansal, CWI + TU Eindhoven<br/>
Nir Bitansky, Tel-Aviv University<br/>
Clement Canonne, Stanford<br/>
Timothy Chan, University of Ilinois at Urbana-Champaign<br/>
Edith Cohen, Google and Tel-Aviv University<br/>
Shaddin Dughmi, University of Southern California<br/>
Sumegha Garg, Princeton<br/>
Ankit Garg, Microsoft research<br/>
Ran Gelles, Bar-Ilan University<br/>
Elena Grigorescu, Purdue<br/>
Tom Gur, University of Warwick<br/>
Sandy Irani, UC Irvine<br/>
Dakshita Khurana, University of Illinois at Urbana-Champaign<br/>
Antonina Kolokolova, Memorial University of Newfoundland.<br/>
Pravesh Kothari, Carnegie Mellon University<br/>
Rasmus Kyng, Harvard<br/>
Katrina Ligett, Hebrew University<br/>
Nutan Limaye, IIT Bombay<br/>
Pasin Manurangsi, UC Berkeley<br/>
Tamara Mchedlidze, Karlsruhe Institute of Technology<br/>
Dana Moshkovitz, UT Austin<br/>
Jelani Nelson, UC Berkeley<br/>
Merav Parter, Weizmann Institute<br/>
Krzysztof Pietrzak, IST Austria<br/>
Elaine Shi, Cornell<br/>
Piyush Srivastava, Tata Institute of Fundamental Research, Mumbai<br/>
Li-Yang Tan, Stanford<br/>
Madhur Tulsiani, TTIC<br/>
Gregory Valiant, Stanford<br/>
Thomas Vidick, California Institute of Technology (chair)<br/>
Virginia Vassilevska Williams, MIT<br/>
Ronald de Wolf, CWI and University of Amsterdam<br/>
David Woodruff, Carnegie Mellon University</p></div>
    </content>
    <updated>2019-06-04T18:58:43Z</updated>
    <published>2019-06-04T18:58:43Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>ccanonne</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2019-06-05T00:00:46Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3116995407343145604</id>
    <link href="https://blog.computationalcomplexity.org/feeds/3116995407343145604/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/06/imus-non-controversial-changing-name-of.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3116995407343145604" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3116995407343145604" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/06/imus-non-controversial-changing-name-of.html" rel="alternate" type="text/html"/>
    <title>IMU's non-controversial changing the name of the Nevanlinna Prize</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">(I want to thank Alexander Soifer for supplying me with some of the documents I point to in this post. We should all thank him for getting the ball rolling on changing the name of the Nevanlinna Prize.)<br/>
<br/>
The <i>Nevanlinna Prize </i>was essentially a Fields Medal for Theoretical Computer Science.  I do not know why it is a<i> Prize </i>instead of a <i>Medal.</i><br/>
<div>
<br/></div>
<div>
It has been renamed <i>The Abacus Medal. </i>If you want to know why the IMU (International Mathematics Union) thinks the new name is good <i>but do not </i><i>care even a little about why the original name was bad</i> then see this article: <a href="https://www.heidelberg-laureate-forum.org/blog/imu-abacus-medal/">here</a>.</div>
<div>
<br/></div>
<div>
So why is <i>The Nevanlinna Prize</i> a bad name? In brief, Rolf Nevanlinna was an enthusiastic Nazi sympathizer. How enthused? He served as the chair of the Finish SS recruitment committee.<br/>
<br/>
That would seem like enough to get the name changed. In fact, it makes one wonder why the prize originally had the name.<br/>
<br/>
1) Why the change now?  It began when Alexander Soifer came across this information about Nevanlinna while working on his book<br/>
<br/>
<i>The Scholar and the State: In Search of Van der Waerdan</i> (see <a href="https://amzn.to/2WnfDYh">here</a> to buy it, see <a href="https://mathcs.clarku.edu/~fgreen/SIGACTReviews/bookrev/47-1.pdf">here</a> for a book review column that includes my review of it).<br/>
<br/>
He then wrote a letter to the IMU which sponsors the <i>Nevanlinna Prize</i>. The letter is <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/letterToImu.pdf">here</a>. Note that Alexander offered to pay for the prize ($15,000 every four years) if that will help get the name changed.<br/>
<br/>
After a response that lamely said (I paraphrase): <i>Gee, we didn't know. Oh well</i>. Alex wrote another letter which is <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/letterToImu2.pdf">here</a>.<br/>
<br/>
The story has a happy ending: the name was changed.  (No, Alexander is not paying for the award.)<br/>
<br/>
2) For a full summary of why the award was originally named Nevanlinna  and why it was changed see the article, <i>Yes We Can,  </i>by Alexander Soifer,<i> </i>in an issue of the journal <i>Mathematical Competition</i>s, see <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/yeswecan.pdf">here</a>.</div>
<div>
<br/></div>
<div>
3) When is change possible?<br/>
<br/></div>
<div>
 Assume Y did X and X is awful (e.g., I assume for most of my readers believing and spreading Nazi propaganda). Assume there is a Y-prize. What does it take to have the name changed?<br/>
<br/></div>
<div>
<br/></div>
<div>
a) You need someone pushing hard for it. Kudos to Alexander Soifer who started this.</div>
<div>
<br/></div>
<div>
b) There is no really good reason to use that name in the first place. </div>
<div>
<br/></div>
<div>
What was Nevanlinna's contribution to mathematical aspects of computer science? The IMU (International Mathematics Union) internet page answers:</div>
<div>
<br/></div>
<div>
<i>The prize was named in honors of Rolf Nevanlinna ... who in the 1950's had taken the initiative to the computer organization at Finnish Universities. </i></div>
<div>
<i><br/></i></div>
<div>
That's all. If there was a Gauss Prize (actually there IS a Gauss Prize) and we later found out that Gauss was X, I doubt we would change the name of the award. Gauss's name is on it since he is a great mathematician. </div>
<div>
<br/></div>
<div>
c) The person on the award is not the one giving the money. If we found out that Nobel was an X,  I doubt the name would change since he is paid for it. </div>
<div>
<br/></div>
<div>
d) If the award name is well known then it might not change. Nobel is a good example. I think the Nevanlinna prize is mostly unknown to the public. The Field's medal is better known, though still not that well known. The general public became briefly aware of the Field's medal twice: when it was mentioned in the movie <i>Good Will Hunting,</i> and when Perelman turned it down. Fame is fleeting for both prizes and people.</div>
<div>
<br/></div>
<div>
e) Organizations don't like to change things. Hence X would need to be particularly bad to warrant a name change. </div>
<div>
<br/></div>
<div>
OTHER THOUGHTS</div>
<div>
<br/></div>
<div>
1) Why <i>The Abacus Medal</i>? Perhaps they are worried that if they name it after someone and that someone turns out to be an X they'll have to change it again. I find the explanation given <a href="https://www.heidelberg-laureate-forum.org/blog/imu-abacus-medal/">here</a> to be unsatisfying. I find the fact that they make <b>NO MENTION</b> of why they are no longer naming it <i>The</i> <i>Nevanlinna prize </i>appalling and insulting.</div>
<div>
<br/></div>
<div>
2) Lets turn to people who get the awards. If someone solved two Millennium problems and clearly deserved a Field's Medal, but was an X, should they be denied the prize on that basis. I would tend to think no (that is, they should get the prize) but it does trouble me. What would happen?  I honestly don't know.  </div>
<div>
<br/></div>
<div>
3) X will change over time.</div>
<div>
<br/></div></div>
    </content>
    <updated>2019-06-04T15:53:00Z</updated>
    <published>2019-06-04T15:53:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-06-04T20:14:20Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-32902056.post-2757759457141834790</id>
    <link href="http://paulwgoldberg.blogspot.com/feeds/2757759457141834790/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=32902056&amp;postID=2757759457141834790" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/32902056/posts/default/2757759457141834790" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/32902056/posts/default/2757759457141834790" rel="self" type="application/atom+xml"/>
    <link href="http://paulwgoldberg.blogspot.com/2019/06/plans-for-wine-conferences.html" rel="alternate" type="text/html"/>
    <title>plans for WINE conferences</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div dir="ltr" style="text-align: left;">Update on the annual Conference on Web and Internet Economics (I am on the steering committee).<br/><br/><a href="http://wine2019.cs.columbia.edu/">WINE 2019</a> (the 15th) will be at Columbia University, December 10-12. We could not avoid the clash with NeurIPS, due to Columbia’s exam schedule. Submission deadline is July 15th.<br/><br/>The plan is for WINE 2020 to take place at Peking University, following the tradition to rotate between Europe, USA, and Asia.<br/><br/>WINE 2021 is under discussion; one idea it to hold it in Addis Ababa, Ethiopia, which is not as novel as it may seem at first sight, it would be following ICLR 2020 (see <a href="https://venturebeat.com/2018/11/19/major-ai-conference-is-moving-to-africa-in-2020-due-to-visa-issues/">this link</a> (noting the visa issues) and others). The rationale is that Africa has a burgeoning AI community including people who are interested in algorithmic game theory, and for them, Ethiopia is an easy destination (administratively, in particular). WINE 2018 (at Oxford, UK) had (I think) 5 African participants, and about 10 more would have liked to come but were denied visas. These participants brought home to me the point that there is this developing AI community in Africa. At WINE 2018, Eric Sodomka (from Facebook) gave a well-received presentation on the idea of holding a future WINE in Africa. Are there other places in Africa we should be thinking of? I welcome feedback and comments!<br/><br/></div></div>
    </content>
    <updated>2019-06-04T10:46:00Z</updated>
    <published>2019-06-04T10:46:00Z</published>
    <category scheme="http://www.blogger.com/atom/ns#" term="aggregator"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="conferences"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="game theory"/>
    <author>
      <name>Paul Goldberg</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/10952445127830395305</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-32902056</id>
      <category term="aggregator"/>
      <category term="UK academia"/>
      <category term="politics"/>
      <category term="meetings"/>
      <category term="research"/>
      <category term="research directions"/>
      <category term="conferences"/>
      <category term="funding"/>
      <category term="people"/>
      <category term="rant"/>
      <category term="economics"/>
      <category term="academia"/>
      <category term="forecasts"/>
      <category term="internet"/>
      <category term="advertisement"/>
      <category term="game theory"/>
      <category term="trips"/>
      <category term="University of Liverpool"/>
      <category term="editorial"/>
      <category term="times higher"/>
      <category term="announcements"/>
      <category term="publications"/>
      <category term="teaching"/>
      <category term="technical"/>
      <category term="work"/>
      <category term="postgraduate research"/>
      <category term="technology"/>
      <category term="books"/>
      <category term="social choice"/>
      <category term="talks"/>
      <category term="CACM"/>
      <category term="liverpool"/>
      <category term="open problems"/>
      <category term="problems"/>
      <category term="research assessment"/>
      <category term="tongue in cheek"/>
      <category term="administration"/>
      <category term="architecture"/>
      <category term="email"/>
      <category term="environment"/>
      <category term="games"/>
      <category term="mechanism design"/>
      <category term="puzzles"/>
      <category term="web sites"/>
      <category term="URLs"/>
      <category term="USS"/>
      <category term="education"/>
      <category term="higher education"/>
      <category term="oxford"/>
      <category term="schools"/>
      <category term="UCU"/>
      <category term="go"/>
      <category term="intellectual property"/>
      <category term="jobs"/>
      <category term="league table"/>
      <category term="math education"/>
      <category term="money"/>
      <category term="products"/>
      <category term="science"/>
      <category term="students"/>
      <category term="Liberal democrats"/>
      <category term="UK"/>
      <category term="XJTLU"/>
      <category term="behavioural economics"/>
      <category term="china"/>
      <category term="current affairs"/>
      <category term="diversity"/>
      <category term="epsrc"/>
      <category term="family"/>
      <category term="geography"/>
      <category term="holidays"/>
      <category term="joke"/>
      <category term="misc"/>
      <category term="nerd humour"/>
      <category term="pensions"/>
      <category term="predictions"/>
      <category term="proposals"/>
      <category term="psephology"/>
      <category term="region"/>
      <category term="student finance"/>
      <category term="visits"/>
      <category term="warwick"/>
      <category term="web"/>
      <category term="weekends"/>
      <category term="wikipedia"/>
      <category term="writing"/>
      <author>
        <name>Paul Goldberg</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/10952445127830395305</uri>
      </author>
      <link href="http://paulwgoldberg.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/32902056/posts/default/-/aggregator" rel="self" type="application/atom+xml"/>
      <link href="http://paulwgoldberg.blogspot.com/search/label/aggregator" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/32902056/posts/default/-/aggregator/-/aggregator?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>theoretical computer science, economics, and academic life in general. Writing in personal capacity, not representing my employer or other colleagues</subtitle>
      <title>Paul Goldberg</title>
      <updated>2019-06-04T13:05:01Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.00908</id>
    <link href="http://arxiv.org/abs/1906.00908" rel="alternate" type="text/html"/>
    <title>Phase-based Minimalist Parsing and complexity in non-local dependencies</title>
    <feedworld_mtime>1559606400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chesi:Cristiano.html">Cristiano Chesi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00908">PDF</a><br/><b>Abstract: </b>A cognitively plausible parsing algorithm should perform like the human
parser in critical contexts. Here I propose an adaptation of Earley's parsing
algorithm, suitable for Phase-based Minimalist Grammars (PMG, Chesi 2012), that
is able to predict complexity effects in performance. Focusing on self-paced
reading experiments of object clefts sentences (Warren &amp; Gibson 2005) I will
associate to parsing a complexity metric based on cued features to be retrieved
at the verb segment (Feature Retrieval &amp; Encoding Cost, FREC). FREC is
crucially based on the usage of memory predicted by the discussed parsing
algorithm and it correctly fits with the reading time revealed.
</p></div>
    </summary>
    <updated>2019-06-04T23:23:59Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-06-04T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.00809</id>
    <link href="http://arxiv.org/abs/1906.00809" rel="alternate" type="text/html"/>
    <title>Rpair: Rescaling RePair with Rsync</title>
    <feedworld_mtime>1559606400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gagie:Travis.html">Travis Gagie</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/I:Tomohiro.html">Tomohiro I</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Manzini:Giovanni.html">Giovanni Manzini</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Navarro:Gonzalo.html">Gonzalo Navarro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sakamoto:Hiroshi.html">Hiroshi Sakamoto</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Takabatake:Yoshimasa.html">Yoshimasa Takabatake</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00809">PDF</a><br/><b>Abstract: </b>Data compression is a powerful tool for managing massive but repetitive
datasets, especially schemes such as grammar-based compression that support
computation over the data without decompressing it. In the best case such a
scheme takes a dataset so big that it must be stored on disk and shrinks it
enough that it can be stored and processed in internal memory. Even then,
however, the scheme is essentially useless unless it can be built on the
original dataset reasonably quickly while keeping the dataset on disk. In this
paper we show how we can preprocess such datasets with context-triggered
piecewise hashing such that afterwards we can apply RePair and other
grammar-based compressors more easily. We first give our algorithm, then show
how a variant of it can be used to approximate the LZ77 parse, then leverage
that to prove theoretical bounds on compression, and finally give experimental
evidence that our approach is competitive in practice.
</p></div>
    </summary>
    <updated>2019-06-04T23:38:39Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-04T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.00703</id>
    <link href="http://arxiv.org/abs/1906.00703" rel="alternate" type="text/html"/>
    <title>Parameterised Complexity for Abduction</title>
    <feedworld_mtime>1559606400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mahmood:Yasir.html">Yasir Mahmood</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meier:Arne.html">Arne Meier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schmidt:Johannes.html">Johannes Schmidt</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00703">PDF</a><br/><b>Abstract: </b>Abductive reasoning is a non-monotonic formalism stemming from the work of
Peirce. It describes the process of deriving the most plausible explanations of
known facts. Considering the positive version asking for sets of variables as
explanations, we study, besides asking for existence of the set of
explanations, two explanation size limited variants of this reasoning problem
(less than or equal to, and equal to). In this paper, we present a thorough
classification regarding the parameterised complexity of these problems under a
wealth of different parameterisations. Furthermore, we analyse all possible
Boolean fragments of these problems in the constraint satisfaction approach
with co-clones. Thereby, we complete the parameterised picture started by
Fellows et al. (AAAI 2012), partially building on results of Nordh and
Zanuttini (Artif. Intell. 2008). In this process, we outline a fine-grained
analysis of the inherent intractability of these problems and pinpoint their
tractable parts.
</p></div>
    </summary>
    <updated>2019-06-04T23:24:14Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-06-04T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.00692</id>
    <link href="http://arxiv.org/abs/1906.00692" rel="alternate" type="text/html"/>
    <title>Betti numbers of unordered configuration spaces of small graphs</title>
    <feedworld_mtime>1559606400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Drummond=Cole:Gabriel_C=.html">Gabriel C. Drummond-Cole</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00692">PDF</a><br/><b>Abstract: </b>The purpose of this document is to provide data about known Betti numbers of
unordered configuration spaces of small graphs in order to guide research and
avoid duplicated effort. It contains information for connected multigraphs
having at most six edges which contain no loops, no bivalent vertices, and no
internal (i.e., non-leaf) bridges.
</p></div>
    </summary>
    <updated>2019-06-04T23:56:57Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-06-04T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.00659</id>
    <link href="http://arxiv.org/abs/1906.00659" rel="alternate" type="text/html"/>
    <title>Multistage Vertex Cover</title>
    <feedworld_mtime>1559606400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fluschnik:Till.html">Till Fluschnik</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Niedermeier:Rolf.html">Rolf Niedermeier</a>, Valentin Rohm, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zschoche:Philipp.html">Philipp Zschoche</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00659">PDF</a><br/><b>Abstract: </b>Covering all edges of a graph by a minimum number of vertices, this is the
NP-hard Vertex Cover problem, is among the most fundamental algorithmic tasks.
Following a recent trend in studying dynamic and temporal graphs, we initiate
the study of Multistage Vertex Cover. Herein, having a series of graphs with
same vertex set but over time changing edge sets (known as temporal graph
consisting of various layers), the goal is to find for each layer of the
temporal graph a small vertex cover and to guarantee that the two vertex cover
sets between two subsequent layers differ not too much (specified by a given
parameter). We show that, different from classic Vertex Cover and some other
dynamic or temporal variants of it, Multistage Vertex Cover is computationally
hard even in fairly restricted settings. On the positive side, however, we also
spot several fixed-parameter tractability results based on some of the most
natural parameterizations.
</p></div>
    </summary>
    <updated>2019-06-04T23:26:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-06-04T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.00618</id>
    <link href="http://arxiv.org/abs/1906.00618" rel="alternate" type="text/html"/>
    <title>A Direct $\tilde{O}(1/\epsilon)$ Iteration Parallel Algorithm for Optimal Transport</title>
    <feedworld_mtime>1559606400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jambulapati:Arun.html">Arun Jambulapati</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sidford:Aaron.html">Aaron Sidford</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tian:Kevin.html">Kevin Tian</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00618">PDF</a><br/><b>Abstract: </b>Optimal transportation, or computing the Wasserstein or ``earth mover's''
distance between two distributions, is a fundamental primitive which arises in
many learning and statistical settings. We give an algorithm which solves this
problem to additive $\epsilon$ with $\tilde{O}(1/\epsilon)$ parallel depth, and
$\tilde{O}\left(n^2/\epsilon\right)$ work. Barring a breakthrough on a
long-standing algorithmic open problem, this is optimal for first-order
methods. Blanchet et. al. '18, Quanrud '19 obtained similar runtimes through
reductions to positive linear programming and matrix scaling. However, these
reduction-based algorithms use complicated subroutines which may be deemed
impractical due to requiring solvers for second-order iterations (matrix
scaling) or non-parallelizability (positive LP). The fastest practical
algorithms run in time $\tilde{O}(\min(n^2 / \epsilon^2, n^{2.5} / \epsilon))$
(Dvurechensky et. al. '18, Lin et. al. '19). We bridge this gap by providing a
parallel, first-order, $\tilde{O}(1/\epsilon)$ iteration algorithm without
worse dependence on dimension, and provide preliminary experimental evidence
that our algorithm may enjoy improved practical performance. We obtain this
runtime via a primal-dual extragradient method, motivated by recent theoretical
improvements to maximum flow (Sherman '17).
</p></div>
    </summary>
    <updated>2019-06-04T23:54:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-04T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.00563</id>
    <link href="http://arxiv.org/abs/1906.00563" rel="alternate" type="text/html"/>
    <title>Direct Linear Time Construction of Parameterized Suffix and LCP Arrays for Constant Alphabets</title>
    <feedworld_mtime>1559606400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fujisato:Noriki.html">Noriki Fujisato</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nakashima:Yuto.html">Yuto Nakashima</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Inenaga:Shunsuke.html">Shunsuke Inenaga</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bannai:Hideo.html">Hideo Bannai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Takeda:Masayuki.html">Masayuki Takeda</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00563">PDF</a><br/><b>Abstract: </b>We present the first worst-case linear time algorithm that directly computes
the parameterized suffix and LCP arrays for constant sized alphabets. Previous
algorithms either required quadratic time or the parameterized suffix tree to
be built first. More formally, for a string over static alphabet $\Sigma$ and
parameterized alphabet $\Pi$, our algorithm runs in $O(n\pi)$ time and $O(n)$
words of space, where $\pi$ is the number of distinct symbols of $\Pi$ in the
string.
</p></div>
    </summary>
    <updated>2019-06-04T23:49:46Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-04T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.00482</id>
    <link href="http://arxiv.org/abs/1906.00482" rel="alternate" type="text/html"/>
    <title>On the Use of Randomness in Local Distributed Graph Algorithms</title>
    <feedworld_mtime>1559606400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghaffari:Mohsen.html">Mohsen Ghaffari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuhn:Fabian.html">Fabian Kuhn</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00482">PDF</a><br/><b>Abstract: </b>We attempt to better understand randomization in local distributed graph
algorithms by exploring how randomness is used and what we can gain from it: -
We first ask the question of how much randomness is needed to obtain efficient
randomized algorithms. We show that for all locally checkable problems for
which polylog $n$-time randomized algorithms exist, there are such algorithms
even if either (I) there is a only a single (private) independent random bit in
each polylog $n$-neighborhood of the graph, (II) the (private) bits of
randomness of different nodes are only polylog $n$-wise independent, or (III)
there are only polylog $n$ bits of global shared randomness (and no private
randomness). - Second, we study how much we can improve the error probability
of randomized algorithms. For all locally checkable problems for which polylog
$n$-time randomized algorithms exist, we show that there are such algorithms
that succeed with probability $1-n^{-2^{\varepsilon(\log\log n)^2}}$ and more
generally $T$-round algorithms, for $T\geq$ polylog $n$, that succeed with
probability $1-n^{-2^{\varepsilon\log^2T}}$. We also show that polylog $n$-time
randomized algorithms with success probability $1-2^{-2^{\log^\varepsilon n}}$
for some $\varepsilon&gt;0$ can be derandomized to polylog $n$-time deterministic
algorithms. Both of the directions mentioned above, reducing the amount of
randomness and improving the success probability, can be seen as partial
derandomization of existing randomized algorithms. In all the above cases, we
also show that any significant improvement of our results would lead to a major
breakthrough, as it would imply significantly more efficient deterministic
distributed algorithms for a wide class of problems.
</p></div>
    </summary>
    <updated>2019-06-04T23:33:19Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-04T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.00476</id>
    <link href="http://arxiv.org/abs/1906.00476" rel="alternate" type="text/html"/>
    <title>Noise reduction using past causal cones in variational quantum algorithms</title>
    <feedworld_mtime>1559606400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shehab:Omar.html">Omar Shehab</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kim:Isaac_H=.html">Isaac H. Kim</a>, Nhung H. Nguyen, Kevin Landsman, Cinthia H. Alderete, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhu:Daiwei.html">Daiwei Zhu</a>, C. Monroe, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Linke:Norbert_M=.html">Norbert M. Linke</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00476">PDF</a><br/><b>Abstract: </b>We introduce an approach to improve the accuracy and reduce the sample
complexity of near term quantum-classical algorithms. We construct a simpler
initial parameterized quantum state, or ansatz, based on the past causal cone
of each observable, generally yielding fewer qubits and gates. We implement
this protocol on a trapped ion quantum computer and demonstrate improvement in
accuracy and time-to-solution at an arbitrary point in the variational search
space. We report a $\sim 27\%$ improvement in the accuracy of the variational
calculation of the deuteron binding energy and $\sim 40\%$ improvement in the
accuracy of the quantum approximate optimization of the MAXCUT problem applied
to the dragon graph $T_{3,2}$. When the time-to-solution is prioritized over
accuracy, the former requires $\sim 71\%$ fewer measurements and the latter
requires $\sim 78\%$ fewer measurements.
</p></div>
    </summary>
    <updated>2019-06-04T23:37:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-04T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.00417</id>
    <link href="http://arxiv.org/abs/1906.00417" rel="alternate" type="text/html"/>
    <title>The Number of Minimum $k$-Cuts: Improving the Karger-Stein Bound</title>
    <feedworld_mtime>1559606400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Anupam.html">Anupam Gupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lee:Euiwoong.html">Euiwoong Lee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jason.html">Jason Li</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00417">PDF</a><br/><b>Abstract: </b>Given an edge-weighted graph, how many minimum $k$-cuts can it have? This is
a fundamental question in the intersection of algorithms, extremal
combinatorics, and graph theory. It is particularly interesting in that the
best known bounds are algorithmic: they stem from algorithms that compute the
minimum $k$-cut.
</p>
<p>In 1994, Karger and Stein obtained a randomized contraction algorithm that
finds a minimum $k$-cut in $O(n^{(2-o(1))k})$ time. It can also enumerate all
such $k$-cuts in the same running time, establishing a corresponding extremal
bound of $O(n^{(2-o(1))k})$. Since then, the algorithmic side of the minimum
$k$-cut problem has seen much progress, leading to a deterministic algorithm
based on a tree packing result of Thorup, which enumerates all minimum $k$-cuts
in the same asymptotic running time, and gives an alternate proof of the
$O(n^{(2-o(1))k})$ bound. However, beating the Karger--Stein bound, even for
computing a single minimum $k$-cut, has remained out of reach.
</p>
<p>In this paper, we give an algorithm to enumerate all minimum $k$-cuts in
$O(n^{(1.981+o(1))k})$ time, breaking the algorithmic and extremal barriers for
enumerating minimum $k$-cuts. To obtain our result, we combine ideas from both
the Karger--Stein and Thorup results, and draw a novel connection between
minimum $k$-cut and extremal set theory. In particular, we give and use tighter
bounds on the size of set systems with bounded dual VC-dimension, which may be
of independent interest.
</p></div>
    </summary>
    <updated>2019-06-04T23:47:33Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-04T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.00339</id>
    <link href="http://arxiv.org/abs/1906.00339" rel="alternate" type="text/html"/>
    <title>Sample-Optimal Low-Rank Approximation of Distance Matrices</title>
    <feedworld_mtime>1559606400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Indyk:Piotr.html">Piotr Indyk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vakilian:Ali.html">Ali Vakilian</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wagner:Tal.html">Tal Wagner</a>, David Woodruff <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00339">PDF</a><br/><b>Abstract: </b>A distance matrix $A \in \mathbb R^{n \times m}$ represents all pairwise
distances, $A_{ij}=\mathrm{d}(x_i,y_j)$, between two point sets $x_1,...,x_n$
and $y_1,...,y_m$ in an arbitrary metric space $(\mathcal Z, \mathrm{d})$. Such
matrices arise in various computational contexts such as learning image
manifolds, handwriting recognition, and multi-dimensional unfolding.
</p>
<p>In this work we study algorithms for low-rank approximation of distance
matrices. Recent work by Bakshi and Woodruff (NeurIPS 2018) showed it is
possible to compute a rank-$k$ approximation of a distance matrix in time
$O((n+m)^{1+\gamma}) \cdot \mathrm{poly}(k,1/\epsilon)$, where $\epsilon&gt;0$ is
an error parameter and $\gamma&gt;0$ is an arbitrarily small constant. Notably,
their bound is sublinear in the matrix size, which is unachievable for general
matrices.
</p>
<p>We present an algorithm that is both simpler and more efficient. It reads
only $O((n+m) k/\epsilon)$ entries of the input matrix, and has a running time
of $O(n+m) \cdot \mathrm{poly}(k,1/\epsilon)$. We complement the sample
complexity of our algorithm with a matching lower bound on the number of
entries that must be read by any algorithm. We provide experimental results to
validate the approximation quality and running time of our algorithm.
</p></div>
    </summary>
    <updated>2019-06-04T23:54:55Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-04T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.00326</id>
    <link href="http://arxiv.org/abs/1906.00326" rel="alternate" type="text/html"/>
    <title>Approximate degree, secret sharing, and concentration phenomena</title>
    <feedworld_mtime>1559606400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bogdanov:Andrej.html">Andrej Bogdanov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mande:Nikhil_S=.html">Nikhil S. Mande</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thaler:Justin.html">Justin Thaler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Williamson:Christopher.html">Christopher Williamson</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00326">PDF</a><br/><b>Abstract: </b>The $\epsilon$-approximate degree $deg_\epsilon(f)$ of a Boolean function $f$
is the least degree of a real-valued polynomial that approximates $f$ pointwise
to error $\epsilon$. The approximate degree of $f$ is at least $k$ iff there
exists a pair of probability distributions, also known as a dual polynomial,
that are perfectly $k$-wise indistinguishable, but are distinguishable by $f$
with advantage $1 - \epsilon$. Our contributions are:
</p>
<p>We give a simple new construction of a dual polynomial for the AND function,
certifying that $deg_\epsilon(f) \geq \Omega(\sqrt{n \log 1/\epsilon})$. This
construction is the first to extend to the notion of weighted degree, and
yields the first explicit certificate that the $1/3$-approximate degree of any
read-once DNF is $\Omega(\sqrt{n})$.
</p>
<p>We show that any pair of symmetric distributions on $n$-bit strings that are
perfectly $k$-wise indistinguishable are also statistically $K$-wise
indistinguishable with error at most $K^{3/2} \cdot \exp(-\Omega(k^2/K))$ for
all $k &lt; K &lt; n/64$. This implies that any symmetric function $f$ is a
reconstruction function with constant advantage for a ramp secret sharing
scheme that is secure against size-$K$ coalitions with statistical error
$K^{3/2} \exp(-\Omega(deg_{1/3}(f)^2/K))$ for all values of $K$ up to $n/64$
simultaneously. Previous secret sharing schemes required that $K$ be determined
in advance, and only worked for $f=$ AND.
</p>
<p>Our analyses draw new connections between approximate degree and
concentration phenomena.
</p>
<p>As a corollary, we show that for any $d &lt; n/64$, any degree $d$ polynomial
approximating a symmetric function $f$ to error $1/3$ must have $\ell_1$-norm
at least $K^{-3/2} \exp({\Omega(deg_{1/3}(f)^2/d)})$, which we also show to be
tight for any $d &gt; deg_{1/3}(f)$. These upper and lower bounds were also
previously only known in the case $f=$ AND.
</p></div>
    </summary>
    <updated>2019-06-04T23:32:13Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-06-04T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.00324</id>
    <link href="http://arxiv.org/abs/1906.00324" rel="alternate" type="text/html"/>
    <title>Ubiquitous Complexity of Entanglement Spectra</title>
    <feedworld_mtime>1559606400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cheng:Bin.html">Bin Cheng</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yung:Man=Hong.html">Man-Hong Yung</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00324">PDF</a><br/><b>Abstract: </b>In recent years, the entanglement spectra of quantum states have been
identified to be highly valuable for improving our understanding on many
problems in quantum physics, such as classification of topological phases,
symmetry-breaking phases, and eigenstate thermalization, etc. However, it
remains a major challenge to fully characterize the entanglement spectrum of a
given quantum state. An outstanding problem is whether the difficulty is
intrinsically technical or fundamental? Here using the tools in computational
complexity, we perform a rigorous analysis to pin down the counting complexity
of entanglement spectra of (i) states generated by polynomial-time quantum
circuits, (ii) ground states of gapped 5-local Hamiltonians, and (iii)
projected entangled-pair states (PEPS). We prove that despite the state
complexity, the problems of counting the number of sizable elements in the
entanglement spectra all belong to the class $\mathsf{\# P}$-complete, which is
as hard as calculating the partition functions of Ising models. Our result
suggests that the absence of an efficient method for solving the problem is
fundamental in nature, from the point of view of computational complexity
theory.
</p></div>
    </summary>
    <updated>2019-06-04T23:32:58Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-06-04T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.00298</id>
    <link href="http://arxiv.org/abs/1906.00298" rel="alternate" type="text/html"/>
    <title>Optimal Register Construction in M&amp;M Systems</title>
    <feedworld_mtime>1559606400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hadzilacos:Vassos.html">Vassos Hadzilacos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hu:Xing.html">Xing Hu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Toueg:Sam.html">Sam Toueg</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00298">PDF</a><br/><b>Abstract: </b>Motivated by recent distributed systems technology, Aguilera et al.
introduced a hybrid model of distributed computing, called message-and-memory
model or m&amp;m model for short. In this model processes can communicate by
message passing and also by accessing some shared memory. We consider the basic
problem of implementing an atomic single-writer multi-reader (SWMR) register
shared by all the processes in m&amp;m systems. Specifically, for every m&amp;m system,
we give an algorithm that implements such a register in this system and show
that it is optimal in the number of process crashes that it can tolerate. This
generalizes the well-known implementation of an atomic SWMR register in a pure
message-passing system.
</p></div>
    </summary>
    <updated>2019-06-04T23:37:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-04T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.00294</id>
    <link href="http://arxiv.org/abs/1906.00294" rel="alternate" type="text/html"/>
    <title>On the computational complexity of the probabilistic label tree algorithms</title>
    <feedworld_mtime>1559606400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Robert Busa-Fekete, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dembczynski:Krzysztof.html">Krzysztof Dembczynski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Golovnev:Alexander.html">Alexander Golovnev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jasinska:Kalina.html">Kalina Jasinska</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuznetsov:Mikhail.html">Mikhail Kuznetsov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sviridenko:Maxim.html">Maxim Sviridenko</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Chao.html">Chao Xu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00294">PDF</a><br/><b>Abstract: </b>Label tree-based algorithms are widely used to tackle multi-class and
multi-label problems with a large number of labels. We focus on a particular
subclass of these algorithms that use probabilistic classifiers in the tree
nodes. Examples of such algorithms are hierarchical softmax (HSM), designed for
multi-class classification, and probabilistic label trees (PLTs) that
generalize HSM to multi-label problems. If the tree structure is given,
learning of PLT can be solved with provable regret guaranties [Wydmuch et.al.
2018]. However, to find a tree structure that results in a PLT with a low
training and prediction computational costs as well as low statistical error
seems to be a very challenging problem, not well-understood yet.
</p>
<p>In this paper, we address the problem of finding a tree structure that has
low computational cost. First, we show that finding a tree with optimal
training cost is NP-complete, nevertheless there are some tractable special
cases with either perfect approximation or exact solution that can be obtained
in linear time in terms of the number of labels $m$. For the general case, we
obtain $O(\log m)$ approximation in linear time too. Moreover, we prove an
upper bound on the expected prediction cost expressed in terms of the expected
training cost. We also show that under additional assumptions the prediction
cost of a PLT is $O(\log m)$.
</p></div>
    </summary>
    <updated>2019-06-04T23:30:18Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-06-04T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.00219</id>
    <link href="http://arxiv.org/abs/1906.00219" rel="alternate" type="text/html"/>
    <title>Probabilistic Top-k Dominating Query Monitoring over Multiple Uncertain IoT Data Streams in Edge Computing Environments</title>
    <feedworld_mtime>1559606400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lai:Chuan=Chi.html">Chuan-Chi Lai</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Tien=Chun.html">Tien-Chun Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Chuan=Ming.html">Chuan-Ming Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Li=Chun.html">Li-Chun Wang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00219">PDF</a><br/><b>Abstract: </b>Extracting the valuable features and information in Big Data has become one
of the important research issues in Data Science. In most Internet of Things
(IoT) applications, the collected data are uncertain and imprecise due to
sensor device variations or transmission errors. In addition, the sensing data
may change as time evolves. We refer an uncertain data stream as a dataset that
has velocity, veracity, and volume properties simultaneously. This paper
employs the parallelism in edge computing environments to facilitate the top-k
dominating query process over multiple uncertain IoT data streams. The
challenges of this problem include how to quickly update the result for
processing uncertainty and reduce the computation cost as well as provide
highly accurate results. By referring to the related existing papers for
certain data, we provide an effective probabilistic top-k dominating query
process on uncertain data streams, which can be parallelized easily. After
discussing the properties of the proposed approach, we validate our methods
through the complexity analysis and extensive simulated experiments. In
comparison with the existing works, the experimental results indicate that our
method can improve almost 60% computation time, reduce nearly 20% communication
cost between servers, and provide highly accurate results in most scenarios.
</p></div>
    </summary>
    <updated>2019-06-04T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-04T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.00211</id>
    <link href="http://arxiv.org/abs/1906.00211" rel="alternate" type="text/html"/>
    <title>Multi-reference factor analysis: low-rank covariance estimation under unknown translations</title>
    <feedworld_mtime>1559606400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Landa:Boris.html">Boris Landa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shkolnisky:Yoel.html">Yoel Shkolnisky</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00211">PDF</a><br/><b>Abstract: </b>We consider the problem of estimating the covariance matrix of a random
signal observed through unknown translations (modeled by cyclic shifts) and
corrupted by noise. Solving this problem allows to discover low-rank structures
masked by the existence of translations (which act as nuisance parameters),
with direct application to Principal Components Analysis (PCA). We assume that
the underlying signal is of length $L$ and follows a standard factor model with
mean zero and $r$ normally-distributed factors. To recover the covariance
matrix in this case, we propose to employ the second- and fourth-order
shift-invariant moments of the signal known as the $\textit{power spectrum}$
and the $\textit{trispectrum}$. We prove that they are sufficient for
recovering the covariance matrix (under a certain technical condition) when
$r&lt;\sqrt{L}$. Correspondingly, we provide a polynomial-time procedure for
estimating the covariance matrix from many (translated and noisy) observations,
where no explicit knowledge of $r$ is required, and prove the procedure's
statistical consistency. While our results establish that covariance estimation
is possible from the power spectrum and the trispectrum for low-rank covariance
matrices, we prove that this is not the case for full-rank covariance matrices.
We conduct numerical experiments that corroborate our theoretical findings, and
demonstrate the favorable performance of our algorithms in various settings,
including in high levels of noise.
</p></div>
    </summary>
    <updated>2019-06-04T23:36:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-04T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.00191</id>
    <link href="http://arxiv.org/abs/1906.00191" rel="alternate" type="text/html"/>
    <title>On problems related to crossing families</title>
    <feedworld_mtime>1559606400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Evans:William.html">William Evans</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saeedi:Noushin.html">Noushin Saeedi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00191">PDF</a><br/><b>Abstract: </b>Given a set of points in the plane, a \emph{crossing family} is a collection
of segments, each joining two of the points, such that every two segments
intersect internally. Aronov et al. [Combinatorica,~14(2):127-134,~1994] proved
that any set of $n$ points contains a crossing family of size
$\Omega(\sqrt{n})$. They also mentioned that there exist point sets whose
maximum crossing family uses at most $\frac{n}{2}$ of the points. We improve
the upper bound on the size of crossing families to $5\lceil \frac{n}{24}
\rceil$. We also introduce a few generalizations of crossing families, and give
several lower and upper bounds on our generalized notions.
</p></div>
    </summary>
    <updated>2019-06-04T23:56:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-06-04T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.00140</id>
    <link href="http://arxiv.org/abs/1906.00140" rel="alternate" type="text/html"/>
    <title>Fast Algorithm for K-Truss Discovery on Public-Private Graphs</title>
    <feedworld_mtime>1559606400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Soroush Ebadian, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Xin.html">Xin Huang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00140">PDF</a><br/><b>Abstract: </b>In public-private graphs, users share one public graph and have their own
private graphs. A private graph consists of personal private contacts that only
can be visible to its owner, e.g., hidden friend lists on Facebook and secret
following on Sina Weibo. However, existing public-private analytic algorithms
have not yet investigated the dense subgraph discovery of k-truss, where each
edge is contained in at least k-2 triangles. This paper aims at finding k-truss
efficiently in public-private graphs. The core of our solution is a novel
algorithm to update k-truss with node insertions. We develop a
classification-based hybrid strategy of node insertions and edge insertions to
incrementally compute k-truss in public-private graphs. Extensive experiments
validate the superiority of our proposed algorithms against state-of-the-art
methods on real-world datasets.
</p></div>
    </summary>
    <updated>2019-06-04T23:53:34Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-04T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.00074</id>
    <link href="http://arxiv.org/abs/1906.00074" rel="alternate" type="text/html"/>
    <title>Balancing spreads of influence in a social network</title>
    <feedworld_mtime>1559606400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Becker:Ruben.html">Ruben Becker</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cor=ograve=:Federico.html">Federico Corò</a>, Gianlorenzo D'Angelo, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gilbert:Hugo.html">Hugo Gilbert</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00074">PDF</a><br/><b>Abstract: </b>The personalization of our news consumption on social media has a tendency to
reinforce our pre-existing beliefs instead of balancing our opinions. This
finding is a concern for the health of our democracies which rely on an access
to information providing diverse viewpoints. To tackle this issue from a
computational perspective, Garimella et al. (NIPS'17) modeled the spread of
these viewpoints, also called campaigns, using the well-known independent
cascade model and studied an optimization problem that aims at balancing
information exposure in a social network when two opposing campaigns propagate
in the network. The objective in their $NP$-hard optimization problem is to
maximize the number of people that are exposed to either both or none of the
viewpoints. For two different settings, one corresponding to a model where
campaigns spread in a correlated manner, and a second one, where the two
campaigns spread in a heterogeneous manner, they provide constant ratio
approximation algorithms. In this paper, we investigate a more general
formulation of this problem. That is, we assume that $\mu$ different campaigns
propagate in a social network and we aim to maximize the number of people that
are exposed to either $\nu$ or none of the campaigns, where $\mu\ge\nu\ge2$. We
provide dedicated approximation algorithms for both the correlated and
heterogeneous settings. Interestingly, for the heterogeneous setting with
$\nu\ge 3$, we give a reduction leading to several approximation hardness
results. Maybe most importantly, we obtain that the problem cannot be
approximated within a factor of $n^{-g(n)}$ for any $g(n)=o(1)$ assuming
Gap-ETH, denoting with $n$ the number of nodes in the social network. For $\nu
\ge 4$, there is no $n^{-\epsilon}$-approximation algorithm if a certain class
of one-way functions exists, where $\epsilon &gt; 0$ is a given constant which
depends on $\nu$.
</p></div>
    </summary>
    <updated>2019-06-04T23:37:09Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-04T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.00029</id>
    <link href="http://arxiv.org/abs/1906.00029" rel="alternate" type="text/html"/>
    <title>Human-Usable Password Schemas: Beyond Information-Theoretic Security</title>
    <feedworld_mtime>1559606400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rosenfeld:Elan.html">Elan Rosenfeld</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vempala:Santosh.html">Santosh Vempala</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blum:Manuel.html">Manuel Blum</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00029">PDF</a><br/><b>Abstract: </b>Password users frequently employ passwords that are too simple, or they just
reuse passwords for multiple websites. A common complaint is that utilizing
secure passwords is too difficult. One possible solution to this problem is to
use a password schema. Password schemas are deterministic functions which map
challenges (typically the website name) to responses (passwords). Previous work
has been done on developing and analyzing publishable schemas, but these
analyses have been information-theoretic, not complexity-theoretic; they
consider an adversary with infinite computing power.
</p>
<p>We perform an analysis with respect to adversaries having currently
achievable computing capabilities, assessing the realistic practical security
of such schemas. We prove for several specific schemas that a computer is no
worse off than an infinite adversary and that it can successfully extract all
information from leaked challenges and their respective responses, known as
challenge-response pairs. We also show that any schema that hopes to be secure
against adversaries with bounded computation should obscure information in a
very specific way, by introducing many possible constraints with each
challenge-response pair. These surprising results put the analyses of password
schemas on a more solid and practical footing.
</p></div>
    </summary>
    <updated>2019-06-04T23:24:37Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-06-04T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.00013</id>
    <link href="http://arxiv.org/abs/1906.00013" rel="alternate" type="text/html"/>
    <title>Parameterization of tensor network contraction</title>
    <feedworld_mtime>1559606400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Bryan O'Gorman <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.00013">PDF</a><br/><b>Abstract: </b>We present a conceptually clear and algorithmically useful framework for
parameterizing the costs of tensor network contraction. Our framework is
completely general, applying to tensor networks with arbitrary bond dimensions,
open legs, and hyperedges. The fundamental objects of our framework are rooted
and unrooted contraction trees, which represent classes of contraction orders.
Properties of a contraction tree correspond directly and precisely to the time
and space costs of tensor network contraction. The properties of rooted
contraction trees give the costs of parallelized contraction algorithms. We
show how contraction trees relate to existing tree-like objects in the graph
theory literature, bringing to bear a wide range of graph algorithms and tools
to tensor network contraction. Independent of tensor networks, we show that the
edge congestion of a graph is almost equal to the branchwidth of its line
graph.
</p></div>
    </summary>
    <updated>2019-06-04T23:49:43Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-04T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://offconvex.github.io/2019/06/03/trajectories/</id>
    <link href="http://offconvex.github.io/2019/06/03/trajectories/" rel="alternate" type="text/html"/>
    <title>Is Optimization a Sufficient Language for Understanding Deep Learning?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In this Deep Learning era, machine learning usually boils down to defining a suitable objective/cost function for the learning task at hand, and then optimizing this function using some variant of gradient descent (implemented via backpropagation).  Little wonder that hundreds of ML papers each year are devoted to various aspects of optimization. Today I will suggest that if our goal is mathematical understanding of deep learning, then  the optimization viewpoint is potentially insufficient —at least in the conventional view:</p>

<blockquote>
  <p><strong>Conventional View (CV) of Optimization</strong>: Find a solution of minimum possible value of the objective, as fast as possible.</p>
</blockquote>

<p>Note that <em>a priori</em> it is not obvious if all learning should involve optimizing a single objective. Whether or not this is true for  learning in the brain is a longstanding open question in neuroscience. Brain components appear to have been repurposed/cobbled together through various accidents of evolution and the whole assemblage may or may not boil down to optimization of an objective. See <a href="https://arxiv.org/pdf/1606.03813.pdf">this survey by Marblestone et al</a>.</p>

<p>I am suggesting that deep learning algorithms also have important properties that are not always reflected in the objective value. Current deep nets, being vastly overparametrized, have multiple optima. They are trained until the objective is almost zero (i.e., close to optimality) and training is said to succeed if the optimum (or near-optimum) model thus found also performs well on unseen/held-out data —i.e., <em>generalizes.</em> The catch here is that the value of the objective may imply nothing about generalization (see <a href="https://arxiv.org/abs/1611.03530">Zhang et al.</a>).</p>

<p>Of course experts will now ask: “Wasn’t generalization theory invented precisely for this reason as the “second leg” of machine learning,  where optimization is the first leg?” For instance this theory shows how to add regularizers to the training objective to ensure the solution generalizes. Or that <em>early stopping</em> (i.e., stopping before reaching the optimum) or even adding noise to the gradient (e.g. by playing with batch sizes and learning rates) can be preferable to perfect optimization, even in simple settings such as regression.</p>

<p>However, in practice explicit regularizers  and noising tricks can’t prevent deep nets from attaining low training objective even on data with random labels; see <a href="https://arxiv.org/abs/1611.03530">Zhang et al.</a>. Current generalization theory is designed to give <em>post hoc</em> explanations for why a particular model generalized. It is agnostic about <em>how</em> the solution was obtained, and thus makes few prescriptions —apart from recommending some regularization— for optimization.   (See my earlier <a href="http://www.offconvex.org/2017/12/08/generalization1/">blog post</a>, which explains the distinction between descriptive and prescriptive methods, and  that generalization theory is primarily descriptive.) The fundamental mystery is:</p>

<blockquote>
  <p>Even vanilla gradient descent (GD) is good at finding models with reasonable generalization. Furthermore, methods to speed up gradient descent (e.g., acceleration or adaptive regularization) can sometimes lead to worse generalization.</p>
</blockquote>

<p>In other words, GD has an innate bias towards finding solutions with good generalization. Magic happens along the GD trajectory and is not captured in the objective value per se. We’re reminded of the old adage.</p>

<blockquote>
  <p>The journey matters more than the destination.</p>
</blockquote>

<p>I will illustrate this viewpoint by sketching new  rigorous analyses of gradient descent in two simple but suggestive settings. I  hope more  detailed writeups will appear in future blog posts.</p>

<p>Acknowledgements: My views on this topic were initially shaped by the excellent papers from TTI Chicago group regarding the implicit bias of gradient descent (<a href="https://arxiv.org/pdf/1709.01953.pdf">Behnam Neyshabur’s thesis</a> is a good starting point), and then of course by  various coauthors.</p>

<h2 id="computing-with-infinitely-wide-deep-nets">Computing with Infinitely Wide Deep Nets</h2>

<p>Since overparametrization does not appear to hurt deep nets too much, researchers have wondered what happens in the infinite limit of overparametrization: use a fixed training set such as CIFAR10 to train a classic deep net architecture like AlexNet or VGG19 whose “width” —namely, number of channels in the convolutional filters, and number of nodes in fully connected internal layers—- is allowed to increase to <strong>infinity</strong>. Note that initialization (using sufficiently small Gaussian weights) and training makes sense for any finite width, no matter how large. We assume $\ell_2$ loss at the output.</p>

<p>Understandably, such questions can seem hopeless and pointless: all the computing in the world is insufficient to train an infinite net, and we theorists already have our hands full trying to figure out finite nets.  But sometimes in math/physics one can derive insight into questions by studying them in the infinite limit.  Here where an infinite net is training on a finite dataset like CIFAR10, the number of optima is infinite and we are trying to understand what GD does.</p>

<p>Thanks to insights in recent papers on provable learning by overparametrized deep nets (some of the key papers are: <a href="https://arxiv.org/abs/1811.04918">Allen-Zhu et al 1</a>, <a href="https://arxiv.org/abs/1811.03962">Allen-Zhu et al 2</a> <a href="https://arxiv.org/abs/1811.03804">Du et al</a>, <a href="https://arxiv.org/abs/1811.08888">Zou et al</a>) researchers have realized that a nice limiting structure emerges:</p>

<blockquote>
  <p>As width $\rightarrow \infty$, trajectory approaches the trajectory of GD for a kernel regression problem, where the (fixed) kernel in question is the so-called  <em>Neural Tangent Kernel</em> (NTK). (For convolutional nets the kernel is <em>Convolutional NTK or CNTK.</em> )</p>
</blockquote>

<p>The kernel was identified and named by <a href="https://arxiv.org/abs/1806.07572">Jacot et al.</a>, and also implicit in some of the above-mentioned papers on overparametrized nets, e.g. <a href="https://arxiv.org/abs/1810.02054">Du et al</a>.</p>

<p>The definition of this fixed kernel uses the infinite net at its random initialization. For  two inputs $x_i$ and $x_j$ the kernel inner product  $K(x_i, x_j)$  is the inner product of the gradient $\nabla_x$ of the output with respect to the input, evaluated at $x=x_i$, and $x= x_j$ respectively. As the net size increases to infinity this kernel inner product can be shown to converge to a limiting value (there is a technicality about how to define the limit, and the series of new papers have improved the formal statement here; eg <a href="https://arxiv.org/abs/1902.04760">Yang2019</a> and our paper below.).</p>

<p>Our <a href="https://arxiv.org/abs/1904.11955">new paper with Simon Du, Wei Hu, Zhiyuan Li, Russ Salakhutdinov and Ruosang Wang</a> shows that the CNTK can be efficiently computed via dynamic programming, giving us a way to efficiently compute the answer of the trained net for any desired input,  <em>even though training the infinite net directly is of course computationally infeasible.</em> (Aside: Please do not confuse these new results with some earlier papers which view infinite nets as kernels or Gaussian Processes —see citations/discussion in our paper—  since they correspond to training only the top layer while freezing the lower layers to a random initialization.) Empirically we find that this infinite net (aka kernel regression with respect to the NTK) yields better performance on CIFAR10 than any previously known kernel —not counting kernels that were  hand-tuned or designed by training on image data. For instance we can compute the kernel corresponding to a 10-layer convolutional net (CNN) and obtain 77.4% success rate on CIFAR10.</p>

<h2 id="deep-matrix-factorization-for-solving-matrix-completion">Deep Matrix Factorization for solving Matrix Completion</h2>

<p><a href="https://en.wikipedia.org/wiki/Matrix_completion">Matrix completion</a>, motivated by design of recommender systems, is well-studied for over a decade: given $K$ random entries of an unknown matrix, we wish to recover the unseen entries. Solution is not unique in general. But if the unknown matrix is low rank or approximately low rank and satisfies some additional technical assumptions (eg <em>incoherence</em>) then various algorithms can recover the unseen entries approximately or even exactly. A famous algorithm  based upon <a href="https://en.wikipedia.org/wiki/Matrix_norm#Schatten_norms">nuclear/trace norm</a>  minimization is as follows: find matrix that fits all the known observations and has minimum nuclear norm. (Note that nuclear norm is a convex relaxation of rank.) It is also possible to rephrase this as a single objective in the form required by the Conventional View as follows where $S$ is the subset of indices of revealed entries,  $\lambda$ is a multiplier:</p>



<p>In case you didn’t know about nuclear norms, you will like the interesting suggestion made by <a href="http://papers.nips.cc/paper/7195-implicit-regularization-in-matrix-factorization">Gunasekar et al. 2017</a>: let us just forget about the nuclear norm penalty term  altogether. Instead try to recover the missing entries by  simply training (via simple gradient descent/backpropagation) a linear net with two layers on the first term in the loss. This linear net is just a multiplication of two $n\times n $ matrices (you can read about linear deep nets in this <a href="http://www.offconvex.org/2018/03/02/acceleration-overparameterization/">earlier blog post by Nadav Cohen</a>) so we obtain the following  where $e_i$ is the vector with all entries $0$ except for $1$ in the $i$th position:</p>



<p>The “data” now corresponds to indices $(i, j) \in S$, and the training loss captures how well the end-to-end model $M_2M_1$ fits the revealed entries.  Since $S$ was chosen randomly among all entries,  “generalization” corresponds exactly to doing well at predicting the remaining entries. Empirically, soving matrix completion this way via deep learning  (i.e., gradient descent to solve for $M_1, M_2$, and entirely forgetting about ensuring low rank) works as well as the classic algorithm, leading to the following conjecture, which if true would imply that the implicit regularization effect of gradient descent in this case is captured exactly by the nuclear norm.</p>

<blockquote>
  <p>(Conjecture by Gunasekar et al.; Rough Statement) When solving matrix completion as above using a depth-$2$ linear net, the solution obtained is exactly the  one obtained by the nuclear norm minimization method.</p>
</blockquote>

<p>But as you may have already guessed, this turns out to be too simplistic. In <a href="https://arxiv.org/abs/1905.13655">a new paper with Nadav Cohen, Wei Hu and Yuping Luo</a>, we report new experiments suggesting that the above conjecture is false. (I hedge by saying “suggest” because some fine print in the conjecture statement makes it pretty hard to refute definitively.) More interesting, we find that if we overparametrize the problem by further increasing the number of layers from two to $3$ or even higher —which we call Deep Matrix Factorization—then this empirically solves matrix completion even better than nuclear norm minimization. (Note that we’re working in the regime where $S$ is slightly smaller than what it needs to be for nuclear norm algorithm to exactly recover the matrix. Inductive bias is most important precisely in such data-poor settings!) We provide partial analysis for this improved performance of depth $N$ nets by analysing —surprise surprise!—the trajectory of gradient descent and showing how it biases strongly toward finding solutions of low rank, and this bias is stronger than simple nuclear norm. Furthermore our analysis suggests that this bias toward low rank  cannot be captured by nuclear norm or any obvious Schatten quasi-norm of the end-to-end matrix.</p>

<p>NB: Empirically we find that Adam, the celebrated  acceleration method for deep learning, speeds up optimization a lot here as well, but slightly hurts generalization. This relates to what I said above about the  Conventional View being insufficient to capture generalization.</p>

<h2 id="conclusionstakeways">Conclusions/Takeways</h2>

<p>Though the above settings are simple, they suggest that to understand deep learning we have to go beyond the Conventional View of optimization, which focuses only on the value of the objective and the rate of convergence.</p>

<p>(1): Different optimization strategies —GD, SGD, Adam, AdaGrad etc. —-lead to different learning algorithms. They induce different trajectories, which may lead to solutions with different generalization properties.</p>

<p>(2) We need to develop a new vocabulary (and mathematics) to reason about trajectories. This goes beyond the usual “landscape view” of stationary points, gradient norms, Hessian norms, smoothness etc. Caution: trajectories depend on initialization!</p>

<p>(3): I wish I had learnt a few tricks about ODEs/PDEs/Dynamical Systems/Lagrangians in college, to be in better shape to reason about trajectories!</p></div>
    </summary>
    <updated>2019-06-03T10:00:00Z</updated>
    <published>2019-06-03T10:00:00Z</published>
    <source>
      <id>http://offconvex.github.io/</id>
      <author>
        <name>Off the Convex Path</name>
      </author>
      <link href="http://offconvex.github.io/" rel="alternate" type="text/html"/>
      <link href="http://offconvex.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Algorithms off the convex path.</subtitle>
      <title>Off the convex path</title>
      <updated>2019-06-05T00:00:08Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://gradientscience.org/robust_reps/</id>
    <link href="http://gradientscience.org/robust_reps/" rel="alternate" type="text/html"/>
    <title>Robustness beyond Security&amp;#58; Representation Learning</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a class="bbutton" href="https://arxiv.org/abs/1906.00945" style="float: left;">
<i class="fas fa-file-pdf"/>
    Read the paper
</a>
<a class="bbutton" href="http://git.io/robust-reps" style="float: right;">
<i class="fab fa-github"/>
   Download the notebooks
</a></p>

<p><i>This post discusses our <a href="https://arxiv.org/abs/1906.00945">latest paper</a>
on deep network representations—while representations of standard
networks are brittle and thus not fully reflective of the input geometry,
we find that the representations of robust networks are amenable to all
sorts of manipulation, and can truly be thought of (and dealt with) as just
high-level feature representations. Our work suggests that robustness might
be more broadly useful than just protection against adversarial examples.
</i></p>

<p>One of the most promising aspects of deep neural networks is their
potential to learn high-level <i>features</i> that are useful beyond the
classification task at hand. Our mental model of deep
learning classifiers is often similar to the following diagram, in which
the networks learns progressively higher-level features until the final
layer, which acts as a linear classifier over these high-level features:</p>

<p><img alt="" src="http://gradientscience.org/assets/rf1_images/visualization.png"/></p>
<div class="footnote">
A conceptual picture of our understanding of modern deep neural networks
(NVIDIA).
</div>

<p>This picture is consistent with the surprising versatility of deep neural
network <i>feature representations</i>—learned representations for one
task are useful for many others
(as in <a href="https://papers.nips.cc/paper/959-learning-many-related-tasks-at-the-same-time-with-backpropagation.pdf">transfer learning</a>), and
distance in representation space has often been proposed as a perceptual
metric on natural images (as in <a href="http://arxiv.org/abs/1801.03924">VGG distance</a>).</p>

<div class="footnote">
<strong>Note:</strong> In <a href="https://arxiv.org/abs/1906.00945">our paper</a> and in this blog post, we refer to the
<i>representation</i> $R(x)$ of an input $x$ for a network as the
values of the penultimate layer in the network for that input.
</div>

<p>But to what extent is this picture accurate? It turns out that it is
rather simple to (consistently) construct images that are <i>completely</i>
different to a human, but share very similar representations:</p>

<p><img alt="Standard representations are brittle" src="http://gradientscience.org/assets/rf1_images/standard_brittleness.png"/></p>
<div class="footnote">
The above two images, despite seeming completely different to humans, share very
similar representations.
</div>

<p>This phenomenon is somewhat troubling for our conceptual picture: if
feature representations actually encode high-level, human-meaningful
features, we should not be able to find two images with totally different
features that the model “sees” as very similar.</p>

<p>The phenomenon at play here turns out to be more fundamental than just
pairs of images with similar representations. Indeed, the
representations of neural networks seem to be <i>pervasively brittle</i>:
they can be manipulated arbitrarily without meaningful change to the input.
(In fact, this brittleness is similar to the phenomenon that we exploit
when making <a href="https://gradientscience.org/intro_adversarial">adversarial examples</a>.)</p>

<p>Clearly, this brittleness precludes standard representations from acting
how we want them to—in particular, distance in representation space is
not fully <i>aligned</i> with our human perception of distance in feature
space. So, how might we go about fixing this issue?</p>

<h3 id="adversarial-robustness-as-a-feature-prior">Adversarial Robustness as a Feature Prior</h3>

<p>Unfortunately, we don’t have a way to explicitly control which features
models learn (or in what way they learn them). We can, however,
disincentivize models from using features that humans <i>definitely</i>
don’t use by imposing a <i>prior</i> during training. In our paper, we
explore a very simple prior: namely, that imperceptible changes in the
input should not cause large changes in the model’s prediction (i.e.,
models should not rely on brittle features):</p>



<p>Note that this stability is a necessary, but not sufficient property: all
features that humans use certainly obey this property (for reasonably small
), but not every feature obeying this property is one that we
want our models to rely on.</p>

<p>How should we enforce this prior? Well, observe that the condition
$\eqref{eq:robustcond}$ above is actually <i>precisely</i> $\ell_2$-<a href="https://gradientscience.org/intro_adversarial">adversarial
robustness</a>! Thus, a natural method to employ is robust optimization,
which, as we discussed in a <a href="https://gradientscience.org/robust_opt_pt1">previous post</a>, provides reasonable
robustness to adversarial perturbations. Concretely, instead of just
minimizing loss, we opt to minimize <i>adversarial loss</i>:</p>



<h3 id="inverting-representations">Inverting representations</h3>

<p>Now, given a network trained in this manner, what happens if we look for
images with the same representations? Concretely, fixing some image $x$,
what happens if we look for an image $x’$ that has a matching
representation:</p>



<p>(Note that we found the image pairs presented earlier for standard networks
by solving exactly the above problem.) It turns out that when our model is
<i>robust</i>, we end up with an image that is remarkably similar to the
original:</p>

<div class="widget">
    <div class="choices_one" id="left2">
	<span class="widgetheading">Choose an Image</span>
    </div>
    <span class="widgetheading">Reconstructed Image</span>
    <div class="beer-slider selected_one" id="inv_slider">
	<img id="selectedinv1"/>
	<div class="beer-reveal" style="border-right: 3px white solid;">
	    <img class="slider_img" id="selectedinv2"/>
	</div>
    </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Interactive demo</strong>: click on any of the images on the left to see its
reconstruction via the representation of a robust network. The top row
contains random images from the test set, and the bottom row has random
<i>out-of-distribution</i> inputs (images without a correct class).
</div>

<p>Indeed, instead of being able to manipulate feature representations
arbitrarily within a small radius, we now find that matching the
representation of an image leads to (approximately) matching the image
itself.</p>

<h2 id="what-can-we-do-with-these-representations">What can we do with these representations?</h2>
<p>We just saw that the learned representation of a robust deep classifier
suffices to reconstruct its input pretty accurately (at least in terms of
human perception). This highlights two crucial properties of these
representations: a) optimizing for closeness in representation space leads
to perceptually similar images, b) representations contain a large amount
of information about the high-level features of the inputs. These
properties are very desirable and prompt us to further explore the
structure and potential of these representations.  <i>What we find is that
the representations of robust networks can truly be thought of as
high-level feature representations, and thus (in stark contrast to standard
networks) are naturally amenable to various types of manipulation.</i></p>

<p>In the following sections, we explore these “robust representations” in
more depth. A crucial theme in our exploration is
<i>model-faithfulness</i>. Though significant work has been done in
manipulating and interpreting standard (non-robust) models, it seems as
though getting anything meaningful from standard networks requires
enforcing <i>priors</i> into the visualization process
(see this excerpt <a href="https://distill.pub/2017/feature-visualization/#enemy-of-feature-vis">“The Enemy of Feature Visualization”</a>
for a discussion and illustration of this). This comes at the cost of either hiding
vital signals the model utilizes or introducing information that was not
already present in the model—thus blurring the line between what
information the model actually has, versus what information we introduced
when interacting with it. In contrast, throughout our exploration we will
rely on only direct optimization over representation space, without
introducing any priors or extra information.</p>

<h3 id="feature-visualization">Feature visualization</h3>

<p>We begin our exploration of robust representations by trying to understand
the features captured by their individual components. We visualize these
components in the simplest possible way: we perform gradient descent to
find inputs that maximally activate individual components of the
representation. This is how a few <em>random</em> visualizations look like:</p>

<p><img src="http://gradientscience.org/assets/rf1_images/features.png" style="margin: 0;"/></p>
<div class="footnote">
   Inputs maximizing various coordinates (separated by column) of a robust network, found via gradient descent starting from the "seed" image on the far left.
</div>

<p>We see a surprising alignment with human concepts. For instance, the last
component above seems to correspond to “anemone” and the second-last component to
“flowers”. In fact, these names are consistent with the test images
maximally activating these neurons—here are the images corresponding to
each component:</p>

<div class="widget">
    <div class="choices_one" id="left_maxact">
	<span class="widgetheading">Choose a Coordinate (Feature)</span>
    </div>
    <span class="widgetheading">Top Images</span>
    <div class="selected_one" id="maxact_selected"/>
</div>
<div style="clear: both;"/>
<div class="footnote">
    <strong>Interactive demo</strong>: On the left are components of the representation of a robust network
    (the thumbnails are a visualization of the components maximized from
    noise). On the right are the images from the test set that maximally activate the corresponding components.
</div>

<p>These visualizations might look familiar. Indeed, similar results have been
produced in prior work using non-robust models (e.g.
<a href="https://distill.pub/2017/feature-visualization/">here</a> or
<a href="https://distill.pub/2018/building-blocks/">here</a>). The difference is that
the images above are generated by directly maximizing representation
components with gradient descent in input space—we do not enforce any
priors or regularization. For standard networks, the same process is
unfruitful—to circumvent this, prior work imposes priors on the
optimization process.</p>

<h3 id="feature-manipulation">Feature Manipulation</h3>

<p>So far, we have seen that matching the representation of an image starting
from random noise, recovers the high-level features of the image itself. At
the same time, we saw that individual representation components correspond
to high-level human-meaningful concepts. These findings suggests an
intriguing possibility: perhaps we can directly modify high-level features
of an image by manipulating the corresponding representation over the input
space.</p>

<p>This turns out to yield remarkably plausible results! Here we visualize the
results of increasing a few select components via gradient descent over the
image space for a few <em>random</em> (not cherry-picked) inputs:</p>

<div class="widget">
    <div class="choices_right" id="right1">
	<span class="widgetheading">Choose a Coordinate</span>
    </div>
    <div class="selected_two">
	<span class="widgetheading">Feature Addition</span>
	<div class="beer-slider" id="manipulation_slider">
	    <img id="man_selected1"/> 
	    <div class="beer-reveal" style="border-right: 3px white solid;">
		<img class="slider_img" id="man_selected2"/> 
	    </div>
	</div>
    </div>
    <div class="choices_left" id="left1">
	<span class="widgetheading">Choose a Source Image</span>
    </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Interactive demo</strong>: On the left are randomly selected source images,
and on the right are components of the representation of a robust network
(the thumbnails are a visualization of the components maximized from
noise). In the middle is the feature from the selected component, "added"
to the selected image.
</div>

<p>These images end up actually exhibiting the relevant features in a way that
is plausible to humans (for example, stripes appear mostly on animals
instead of the background).</p>

<p>This opens up a wide range of fine-grained manipulations that one can
perform by leveraging the learned representations (in fact, stay tuned for
some applications in our next blog post).</p>

<h3 id="input-interpolation">Input interpolation</h3>
<p>In fact, this outlook can be pushed even further—robust models can be
leveraged as a tool for another kind of manipulation: input-to-input
interpolation. That is, if we think of robust representations as encoding
the high-level features of an input in a sensible manner, an intuitive way
to interpolate between any two inputs is to linearly interpolate their
representations. More precisely, given any two inputs, we can try to
construct an interpolation between them by linearly interpolating their
representations and then constructing inputs to match these
representations.</p>

<p>This rather intuitive way of dealing with representations turns out to work
reasonably well—we can interpolate between arbitrary images. Randomly
sampled interpolations are shown below:</p>

<div class="widget">
    <div class="choices_left" id="int_left1">
	<span class="widgetheading">Choose a Source Image</span>
    </div>
    <div class="selected_two">
	<span class="widgetheading">Interpolation</span>
	<video style="width: 100%;">
	    <source id="int_selected" type="video/mp4">
	</source></video>
    </div>
    <div class="choices_right" id="int_right1">
	<span class="widgetheading">Choose a Destination Image</span>
    </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Interactive demo</strong>: On the left are randomly selected source images,
and on the right are randomly selected target images.
In the middle is the feature interpolation from the selected source image
to the selected target.
</div>

<p>As we can see, the interpolations appears perceptually plausible. Note
that, in contrast to approaches based on generative models
(e.g. <a href="https://arxiv.org/abs/1511.06434">here</a> or
<a href="https://arxiv.org/abs/1809.11096">here</a>), this approach can interpolate
between arbitrary inputs and not only between those produced by the
generative model.</p>

<h3 id="insight-into-model-predictions">Insight into model predictions</h3>
<p>Expanding on our view of deep classifiers as simple linear classifiers on
top of the learned representations, there is also a simple way to gain
insight into predictions of (robust) models. In particular, for incorrect
predictions, we can identify the component most heavily contributing to the
incorrect class (in the same way we would for a linear classifier) and then
directly manipulate the input to increase the value of that component (with
input-space gradient descent). Here we perform this visualization for a few
random misclassified inputs:</p>

<p><img src="http://gradientscience.org/assets/rf1_images/misclassification_IN.jpg" style="margin: 0;"/></p>

<p>The resulting images could provide insight into the model’s incorrect
decision. For instance, we see the bug becoming a dog eye or negative space
becoming the face of a dog. At a high level, these inputs demonstrate which
parts of the image the incorrect prediction was most sensitive to.</p>

<p>Still, as a word of caution, it is important to note that just as with all
saliency methods (e.g. heatmaps, occlusion studies, etc.), visualizing
features and studying misclassification only gives insights into a “local”
sense of model behaviour. Deep neural networks are complex, highly
non-linear models and it’s important to keep in mind that <i>local
sensitivity does not necessarily entail causality</i>.</p>

<h2 id="towards-better-learned-representations">Towards better learned representations</h2>
<p>As we discussed, robust feature representations possess properties that
make them desirable from a broader point of view. In particular, we found
these representations to be better aligned with a perceptual notion of
distance, while allowing us to perform direct input manipulations in a
model-faithful way. These are properties that are fundamental to any “truly
human-level” representation. One can thus view adversarial robustness as a
very potent prior for obtaining representations that are more aligned with
human perception beyond the standard goals of security and reliability.</p></div>
    </summary>
    <updated>2019-06-03T00:00:00Z</updated>
    <published>2019-06-03T00:00:00Z</published>
    <source>
      <id>http://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="http://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="http://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2019-06-04T23:58:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4199</id>
    <link href="https://www.scottaaronson.com/blog/?p=4199" rel="alternate" type="text/html"/>
    <link href="http://www.fields.utoronto.ca/video-archive/static/2019/05/2774-20557/mergedvideo.ogv" length="339811750" rel="enclosure" type="video/ogg"/>
    <link href="https://www.scottaaronson.com/blog/?p=4199#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4199" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">NP-complete Problems and Physics: A 2019 View</title>
    <summary xml:lang="en-US">If I want to get back to blogging on a regular basis, given the negative amount of time that I now have for such things, I’ll need to get better at dispensing with pun-filled titles, jokey opening statements, etc. etc., and resigning myself to a less witty, more workmanline blog. So in that spirit: a […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>If I want to get back to blogging on a regular basis, given the negative amount of time that I now have for such things, I’ll need to get better at dispensing with pun-filled titles, jokey opening statements, etc. etc., and resigning myself to a less witty, more workmanline blog.</p>



<p>So in that spirit: a few weeks ago I gave a talk at the Fields Institute in Toronto, at a <a href="http://www.fields.utoronto.ca/activities/18-19/NP50">symposium</a> to celebrate Stephen Cook and the 50th anniversary (or actually more like 48th anniversary) of the discovery of NP-completeness.  Thanks so much to the organizers for making this symposium happen.</p>



<p>You can <a href="http://www.fields.utoronto.ca/video-archive/static/2019/05/2774-20557/mergedvideo.ogv">watch the video of my talk here</a> (or <a href="https://www.scottaaronson.com/talks/npphys-toronto.ppt">read the PowerPoint slides here</a>).  The talk, on whether NP-complete problems can be efficiently solved in the physical universe, covers much the same ground as <a href="https://www.scottaaronson.com/papers/npcomplete.pdf">my 2005 survey article</a> on the same theme (not to mention dozens of earlier talks), but this is an updated version and I’m happier with it than I was with most past iterations.</p>



<p>As I explain at the beginning of the talk, I wasn’t going to fly to Toronto at all, due to severe teaching and family constraints—but my wife Dana uncharacteristically <em>urged me to go</em> (“don’t worry, I’ll watch the kids!”).  Why?  Because in her view, it was the risks that Steve Cook took 50 years ago, as an untenured assistant professor at Berkeley, that gave birth to the field of computational complexity that Dana and I both now work in.</p>



<p>Anyway, be sure to <a href="http://www.fields.utoronto.ca/video-archive//event/2774/2019">check out the other talks as well</a>—they’re by an assortment of random nobodies like Richard Karp, Avi Wigderson, Leslie Valiant, Michael Sipser, Alexander Razborov, Cynthia Dwork, and Jack Edmonds.  I found the talk by Edmonds particularly eye-opening: he explains how he thought about (the objects that we now call) P and NP∩coNP when he first defined them in the early 60s, and how it was similar to and different from the way we think about them today.</p>



<p>Another memorable moment came when Edmonds interrupted Sipser’s talk—about the history of P vs. NP—to deliver a booming diatribe about how what really matters is not mathematical proof, but just how quickly you can solve problems in the real world.  Edmonds added that, from a practical standpoint, P≠NP is “true today but might become false in the future.”  In response, Sipser asked “what does a mathematician like me care about the real world?,” to roars of approval from the audience.  I might’ve picked a different tack—about how for every practical person I meet for whom it’s blindingly obvious that “in real life, P≠NP,” I meet another for whom it’s equally obvious that “in real life, P=NP” (for all the usual reasons: because SAT solvers work so well in practice, because physical systems so easily relax as their ground states, etc).  No wonder it took 25+ years of smart people thinking about operations research and combinatorial optimization before the P vs. NP question was even explicitly posed.</p>



<hr/>

<p><font color="red"><strong>Unrelated Announcement:</strong></font> The Texas Advanced Computing Center (TACC), a leading supercomputing facility in North Austin that’s part of the University of Texas, is seeking to hire a Research Scientist focused on quantum computing.  Such a person would be a full participant in our <a href="https://www.cs.utexas.edu/~qic/">Quantum Information Center</a> at UT Austin, with plenty of opportunities for collaboration.  <a href="https://utaustin.wd1.myworkdayjobs.com/UTstaff/job/PICKLE-RESEARCH-CAMPUS/Research-Scientist_R_00003442">Check out their posting!</a></p>



<p/></div>
    </content>
    <updated>2019-06-02T13:52:31Z</updated>
    <published>2019-06-02T13:52:31Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Adventures in Meatspace"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2019-06-03T13:29:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/082</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/082" rel="alternate" type="text/html"/>
    <title>TR19-082 |  Approximate degree, secret sharing, and concentration phenomena | 

	Andrej Bogdanov, 

	Nikhil Mande, 

	Justin Thaler, 

	Christopher Williamson</title>
    <summary>The $\epsilon$-approximate degree $\widetilde{\text{deg}}_\epsilon(f)$ of a Boolean function $f$ is the least degree of a real-valued polynomial that approximates $f$ pointwise to error $\epsilon$.  The approximate degree of $f$ is at least $k$ iff there exists a pair of probability distributions, also known as a dual polynomial, that are perfectly $k$-wise indistinguishable, but are distinguishable by $f$ with advantage $1 - \epsilon$.  Our contributions are:

We give a simple new construction of a dual polynomial for the AND function, certifying that $\widetilde{\text{deg}}_\epsilon(f) \geq \Omega(\sqrt{n \log 1/\epsilon})$.  This construction is the first to extend to the notion of weighted degree, and yields the first explicit certificate that the $1/3$-approximate degree of any read-once DNF is $\Omega(\sqrt{n})$.

We show that any pair of symmetric distributions on $n$-bit strings that are perfectly $k$-wise indistinguishable are also statistically $K$-wise indistinguishable with error at most $K^{3/2} \cdot \exp(-\Omega(k^2/K))$ for all $k \leq K \leq n/64$.
This implies that any symmetric function $f$ is a reconstruction function with constant advantage for a ramp secret sharing scheme that is secure against size-$K$ coalitions with statistical error $K^{3/2} \exp(-\Omega(\widetilde{\text{deg}}_{1/3}(f)^2/K))$ for all values of $K$ up to $n/64$ simultaneously.
Previous secret sharing schemes required that $K$ be determined in advance, and only worked for $f=$ AND.  

Our analyses draw new connections between approximate degree and concentration phenomena.

As a corollary, we show that for any $d \leq n/64$, any degree $d$ polynomial approximating a symmetric function $f$ to error $1/3$
must have $\ell_1$-norm at least $K^{-3/2} \exp({\Omega(\widetilde{\text{deg}}_{1/3}(f)^2/d)})$, which we also show to be tight for any $d &gt; \widetilde{\text{deg}}_{1/3}(f)$.
These upper and lower bounds were also previously only known in the case $f=$ AND.</summary>
    <updated>2019-06-02T04:15:39Z</updated>
    <published>2019-06-02T04:15:39Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-06-04T23:57:13Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/081</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/081" rel="alternate" type="text/html"/>
    <title>TR19-081 |  Channels of Small Log-Ratio Leakage and Characterization of Two-Party Differentially Private Computation | 

	Iftach Haitner, 

	Noam Mazor, 

	Ronen Shaltiel, 

	Jad Silbak</title>
    <summary>Consider a PPT two-party protocol ?=(A,B) in which the parties get no private inputs and obtain outputs O^A,O^B?{0,1}, and let V^A and V^B denote the parties’ individual views. Protocol ? has ?-agreement if Pr[O^A=O^B]=1/2+?. The leakage of ? is the amount of information a party obtains about the event {O^A=O^B}; that is, the leakage ? is the maximum, over P?{A,B}, of the distance between V^P|OA=OB and V^P|OA!=OB. Typically, this distance is measured in statistical distance, or, in the computational setting, in computational indistinguishability. For this choice, Wullschleger [TCC ’09] showed that if ?&gt;&gt;? then the protocol can be transformed into an OT protocol.

We consider measuring the protocol leakage by the log-ratio distance (which was popularized by its use in the differential privacy framework). The log-ratio distance between X,Y over domain ? is the minimal ??0 for which, for every v??, log(Pr[X=v]/Pr[Y=v])? [??,?]. In the computational setting, we use computational indistinguishability from having log-ratio distance ?. We show that a protocol with (noticeable) accuracy ???(?^2) can be transformed into an OT protocol (note that this allows ?&gt;&gt;?). We complete the picture, in this respect, showing that a protocol with ??o(?^2) does not necessarily imply OT. Our results hold for both the information theoretic and the computational settings, and can be viewed as a “fine grained” approach to “weak OT amplification”.

We then use the above result to fully characterize the complexity of differentially private two-party computation for the XOR function, answering the open question put by Goyal, Khurana, Mironov, Pandey, and Sahai [ICALP ’16] and Haitner, Nissim, Omri, Shaltiel, and Silbak [FOCS ’18]. Specifically, we show that for any (noticeable) ???(?^2), a two-party protocol that computes the XOR function with ?-accuracy and ?-differential privacy can be transformed into an OT protocol. This improves upon Goyal et al. that only handle ???(?), and upon Haitner et al. who showed that such a protocol implies (infinitely-often) key agreement (and not OT). Our characterization is tight since OT does not follow from protocols in which ??o(?^2), and extends to functions (over many bits) that “contain” an “embedded copy” of the XOR function.</summary>
    <updated>2019-06-02T04:13:14Z</updated>
    <published>2019-06-02T04:13:14Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-06-04T23:57:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/080</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/080" rel="alternate" type="text/html"/>
    <title>TR19-080 |  On List Recovery of High-Rate Tensor Codes | 

	Noga Ron-Zewi, 

	Swastik Kopparty, 

	Shubhangi Saraf, 

	Nicolas Resch, 

	Shashwat Silas</title>
    <summary>We continue the study of list recovery properties of high-rate tensor codes, initiated by Hemenway, Ron-Zewi, and Wootters (FOCS'17). In that work it was shown that the tensor product of an efficient (poly-time) high-rate globally list recoverable code is {\em approximately}  locally list recoverable, as well as globally list recoverable in {\em probabilistic} near-linear time. This was used in turn to give the first capacity-achieving list decodable codes with (1) local list decoding algorithms, and with (2)  {\em probabilistic} near-linear time  global list decoding algorithms. This was also yielded constant-rate codes approaching the Gilbert-Varshamov bound  with  {\em probabilistic}  near-linear time global   unique decoding algorithms.

In the current work we obtain the following results:
1. The tensor product of an efficient (poly-time) high-rate globally list recoverable code is globally list recoverable in {\em deterministic} near-linear time. This yields in turn the first capacity-achieving list decodable codes with {\em deterministic} near-linear time global  list decoding algorithms. It also gives constant-rate codes approaching the Gilbert Varshamov bound with {\em deterministic} near-linear time global unique decoding algorithms.

2. If the base code is additionally locally correctable, then the tensor product is (genuinely) locally list recoverable. This yields in turn constant-rate codes approaching the Gilbert-Varshamov bound that are {\em locally correctable} with query complexity and running time $N^{o(1)}$. This improves over prior work by Gopi et. al. (SODA'17; IEEE Transactions on Information Theory'18) that only gave query complexity $N^{\epsilon}$ with rate that is exponentially small in $1/\epsilon$.

3. A nearly-tight combinatorial lower bound on output list size for list recovering high-rate tensor codes. This bound implies in turn a nearly-tight lower bound of $N^{\Omega(1/\log \log N)}$ on the product of  query complexity and output list size  for locally list recovering high-rate tensor codes.</summary>
    <updated>2019-06-01T16:43:20Z</updated>
    <published>2019-06-01T16:43:20Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-06-04T23:57:13Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/079</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/079" rel="alternate" type="text/html"/>
    <title>TR19-079 |  Average Bias and Polynomial Sources | 

	Arnab Bhattacharyya, 

	Philips George John, 

	Suprovat Ghoshal, 

	Raghu Meka</title>
    <summary>We identify a new notion of pseudorandomness for randomness sources, which we call the average bias. Given a distribution $Z$ over $\{0,1\}^n$, its average bias is: $b_{\text{av}}(Z) =2^{-n} \sum_{c \in \{0,1\}^n} |\mathbb{E}_{z \sim Z}(-1)^{\langle c, z\rangle}|$. A source with average bias at most $2^{-k}$ has min-entropy at least $k$, and so low average bias is a stronger condition than high min-entropy. We observe that the inner product function is an extractor for any source with average bias less than $2^{-n/2}$.

  The notion of average bias especially makes sense for polynomial sources, i.e., distributions sampled by low-degree $n$-variate polynomials over $\mathbb{F}_2$. For the well-studied case of affine sources, it is easy to see that min-entropy $k$ is exactly equivalent to average bias of $2^{-k}$. We show that for quadratic sources, min-entropy $k$ implies that the average bias is at most $2^{-\Omega(\sqrt{k})}$. We use this relation to design dispersers for separable quadratic sources with a min-entropy guarantee.</summary>
    <updated>2019-06-01T16:39:21Z</updated>
    <published>2019-06-01T16:39:21Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-06-04T23:57:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/078</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/078" rel="alternate" type="text/html"/>
    <title>TR19-078 |  Pseudo-Mixing Time of Random Walks | 

	Itai Benjamini, 

	Oded Goldreich</title>
    <summary>We introduce the notion of pseudo-mixing time of a graph define as the number of steps in a random walk that suffices for generating a vertex that looks random to any polynomial-time observer, where, in addition to the tested vertex, the observer is also provided with oracle access to the incidence function of the graph. 

Assuming the existence of one-way functions,
we show that the pseudo-mixing time of a graph can be much smaller than its mixing time.
Specifically, we present bounded-degree $N$-vertex Cayley graphs that have pseudo-mixing time $t$ for any $t(N)=\omega(\log\log N)$. 
Furthermore, the vertices of these graphs can be represented by string of length $2\log_2N$, and the incidence function of these graphs can be computed by Boolean circuits of size $poly(\log N)$.</summary>
    <updated>2019-06-01T07:34:30Z</updated>
    <published>2019-06-01T07:34:30Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-06-04T23:57:12Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/05/31/linkage</id>
    <link href="https://11011110.github.io/blog/2019/05/31/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>No maths for Europe (). Sadly, the EU parliament has passed up a chance to find a nice (or even not-so-nice) formula for its apportionment of seats to countries, instead opting for back-room deals and numbers pulled out of a hat.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://plus.maths.org/content/democratic-dilemmas">No maths for Europe</a> (<a href="https://mathstodon.xyz/@11011110/102109693915830408"/>). Sadly, the EU parliament has passed up a chance to find a nice (or even not-so-nice) <a href="https://en.wikipedia.org/wiki/Highest_averages_method">formula for its apportionment</a> of seats to countries, instead opting for back-room deals and numbers pulled out of a hat.</p>
  </li>
  <li>
    <p>Prominent cryptographers <a href="https://en.wikipedia.org/wiki/Adi_Shamir">Adi Shamir</a> and <a href="https://en.wikipedia.org/wiki/Ross_J._Anderson">Ross J. Anderson</a> were both <a href="https://www.schneier.com/blog/archives/2019/05/why_are_cryptog.html">denied visas to travel to the US</a> for a conference and a book awards ceremony respectively (<a href="https://mathstodon.xyz/@11011110/102112360619663485"/>, <a href="https://boingboing.net/2019/05/17/denying-cryptographers-problem.html">see also</a>). Bruce Schneier mentions “two other prominent cryptographers who are in the same boat”. Odd and troubling.</p>
  </li>
  <li>
    <p><a href="https://mathlesstraveled.com/2019/05/09/computing-the-euler-totient-function-part-1/">Three</a> <a href="https://mathlesstraveled.com/2019/05/18/computing-the-euler-totient-function-part-2-seeing-phi-is-multiplicative/">new</a> <a href="https://mathlesstraveled.com/2019/05/27/computing-the-euler-totient-function-part-3-proving-phi-is-multiplicative/">blog posts</a> by Brent Yorgey concern the <a href="https://en.wikipedia.org/wiki/Euler%27s_totient_function">Euler totient function</a> (<a href="https://mathstodon.xyz/@11011110/102118180704402052"/>). Computing it quickly would break RSA; Brent describes using factoring to do better than brute force. The problem is clearly in , and I think it may be a natural candidate for being -intermediate. Igor Pak (who asked me for -intermediate problems when I recently visited UCLA) <a href="https://cstheory.stackexchange.com/q/43954/95">thinks the prime-counting function may be another</a>, but neither function is very combinatorial. <a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html">In a recent blog post I found a couple of combinatorial candidates</a>, but others would be interesting.</p>
  </li>
  <li>
    <p>The image below (<a href="https://commons.wikimedia.org/wiki/File:Gr%C3%BCnbaum-Rigby_configuration,_vector_graphics.svg">as redrawn by Brammers</a>) is the 
<a href="https://en.wikipedia.org/wiki/Gr%C3%BCnbaum%E2%80%93Rigby_configuration">Grünbaum–Rigby configuration</a> (<a href="https://mathstodon.xyz/@11011110/102119858635464298"/>) with 21 points and lines, 4 points per line, and 4 lines per point. Klein studied it in the complex projective plane in 1879, but it wasn’t known to have this nice real heptagonal realization until Grünbaum and Rigby (1990). The new Wikipedia article on it was started by “Tomo” (whose real-world identity Wikipedia’s arcane outing rules bar me from disclosing, but he just turned 70, so if you figure it out wish him a happy birthday).</p>

    <p style="text-align: center;"><img alt="The Gr&#xFC;nbaum&#x2013;Rigby configuration" src="https://11011110.github.io/blog/assets/2019/grunrig.svg" width="60%"/></p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Garden_of_Eden_(cellular_automaton)">Garden of Eden</a> (<a href="https://mathstodon.xyz/@11011110/102129199678568606"/>). Now a Good Article on Wikipedia.</p>
  </li>
  <li>
    <p><a href="https://www.pnas.org/content/early/2019/05/20/1902572116. Via https://mathstodon.xyz/@helger/102138884170343694">Ono et al prove that almost all Jensen-Pólya polynomials have only real roots</a> (<a href="https://mathstodon.xyz/@11011110/102143915068349382"/>, <a href="https://mathstodon.xyz/@helger/102138884170343694">via</a>). The Riemann Hypothesis is equivalent to the statement that they all do. The same thing works for similar families of polynomials associated with partition functions and proves a conjecture of Chen. See also <a href="https://phys.org/news/2019-05-mathematicians-revive-abandoned-approach-riemann.html">a popularized account</a> and <a href="http://people.oregonstate.edu/~petschec/ONTD/Talk1.pdf">Ono’s talk slides</a>.</p>
  </li>
  <li>
    <p><a href="https://scilogs.spektrum.de/hlf/imu-abacus-medal/">The International Mathematical Union is renaming</a> its <a href="https://en.wikipedia.org/wiki/Nevanlinna_Prize">Nevanlinna Prize</a> to be the IMU Abacus Medal (<a href="https://mathstodon.xyz/@11011110/102149453984232922"/>). The prize is given every four years for major accomplishments in theoretical computer science. The article doesn’t say why rename but it’s because Nevanlinna was a Nazi sympathizer and collaborator. The prize was named after him in the early 1980s because its funding came from Finland, but Nevanlinna also never had much to do with TCS.</p>
  </li>
  <li>
    <p><a href="https://blog.computationalcomplexity.org/2019/05/notorious-lah-or-notorious-lah-or-you.html">Gasarch on proofreading</a> (<a href="https://mathstodon.xyz/@11011110/102154856271421940"/>). Just as in programming, there’s always one more bug.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Ellen_Fetter">Ellen Fetter</a> and <a href="https://en.wikipedia.org/wiki/Margaret_Hamilton_(scientist)">Margaret Hamilton</a>: <a href="https://www.quantamagazine.org/hidden-heroines-of-chaos-ellen-fetter-and-margaret-hamilton-20190520/">Uncredited collaborators with Edward Lorenz at the birth of chaos theory</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/102163812229252010"/>).</span></p>
  </li>
  <li>
    <p><a href="https://www.nytimes.com/2019/05/17/science/math-physics-knitting-matsumoto.html">Elisabetta Matsumoto is studying the mathematics of knitting</a> (<a href="https://mathstodon.xyz/@11011110/102177647389031957"/>, <a href="https://twitter.com/Sabetta_">via</a>), with the hope that it can lead to new programmable metamaterials.</p>
  </li>
  <li>
    <p><a href="https://www.sciencemag.org/news/2019/05/ieee-major-science-publisher-bans-huawei-scientists-reviewing-papers">IEEE bans Huawei employees from reviewing submissions to its journals</a> (<a href="https://mathstodon.xyz/@11011110/102183137967208347"/>, <a href="https://news.ycombinator.com/item?id=20046771">via</a>), saying it is forced to do so by US government sanctions.</p>
  </li>
  <li>
    <p>The UCI University Club (which in other places might be called a faculty club) is next door to the building I work in, and has a bustling side business hosting weddings. Here’s the view that greeted me as I left the office this evening, looking across their lawn towards the gazebo (<a href="https://mathstodon.xyz/@11011110/102188200347058355"/>).</p>

    <p style="text-align: center;"><a href="https://www.ics.uci.edu/~eppstein/pix/uclub/index.html"><img alt="UCI University Club lawn" src="https://www.ics.uci.edu/~eppstein/pix/uclub/uclub-m.jpg" style="border-style: solid; border-color: black;"/></a></p>
  </li>
  <li>
    <p>Line arrangements in architecture (<a href="https://mathstodon.xyz/@11011110/102193430755771327"/>): the beams of <a href="https://en.wikipedia.org/wiki/Mathematical_Bridge">Cambridge’s Mathematical Bridge</a> form tangent lines to its arch and then extend through and support its trusswork, while another set of radial lines tie the structure together. The bridge just looks like a wood truss bridge in real life but <a href="https://commons.wikimedia.org/wiki/File:Mathematical_Bridge_tangents.jpg">this artificially-colored image</a> makes the underlying structure clearer.</p>

    <p style="text-align: center;"><img alt="Cambridge's Mathematical Bridge" src="https://11011110.github.io/blog/assets/2019/cambridgebridge.jpg" style="border-style: solid; border-color: black;" width="80%"/></p>
  </li>
</ul></div>
    </content>
    <updated>2019-05-31T21:40:00Z</updated>
    <published>2019-05-31T21:40:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-06-01T05:27:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/077</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/077" rel="alternate" type="text/html"/>
    <title>TR19-077 |  Consistency of circuit lower bounds with bounded theories | 

	Jan Bydzovsky, 

	Jan  Krajicek, 

	Igor Carboni Oliveira</title>
    <summary>Proving that there are problems in $P^{NP}$ that require boolean circuits of super-linear size is a major frontier in complexity theory. While such lower bounds are known for larger complexity classes, existing results only show that the corresponding problems are hard on infinitely many input lengths. For instance, proving almost-everywhere circuit lower bounds is open even for problems in MAEXP. Giving the notorious difficulty of proving lower bounds that hold for all large input lengths, we ask the following question: 

Can we show that a large set of techniques cannot prove that NP is easy infinitely often? 

Motivated by this and related questions about the interaction between mathematical proofs and computations, we investigate circuit complexity from the perspective of logic.

  Among other results, we prove that for any parameter $k \geq 1$ it is consistent with theory $T$ that computational class $C$ is not contained infinitely often in SIZE$(n^k)$, where $(T, C)$ is one of the pairs:

  $T = T^1_2\;$ and $\;C = P^{NP}$, $\quad T = S^1_2\;$ and $\;C = NP$, $\quad T =~ $PV$\;$ and $C = P$.

  In other words, these theories cannot establish infinitely often circuit upper bounds for the corresponding problems. This is of interest because the weaker theory PV already formalizes sophisticated arguments, such as a proof of the PCP Theorem (Pich, 2015). These consistency statements are unconditional and improve on earlier theorems of Krajicek and Oliveira (2017) and Bydzovsky and Muller (2018) on the consistency of lower bounds with PV.</summary>
    <updated>2019-05-30T10:45:15Z</updated>
    <published>2019-05-30T10:45:15Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-06-04T23:57:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://grigory.github.io/blog/theory-jobs-2019</id>
    <link href="http://grigory.github.io/blog/theory-jobs-2019/" rel="alternate" type="text/html"/>
    <title xml:lang="en">Theory Jobs 2019</title>
    <content type="xhtml" xml:lang="en"><div xmlns="http://www.w3.org/1999/xhtml"><p><img src="http://grigory.github.io/blog/pics/theory-jobs-2019.png"/>
Apparently, it’s a busy life being an assistant prof so there were no posts here all year. However, while some of us are decompressing after the NeurIPS deadline, <a href="https://docs.google.com/spreadsheets/d/1Oegc0quwv2PqoR_pzZlUIrPw4rFsZ4FKoKkUvmLBTHM/edit?usp=sharing">here is a link</a> to a crowdsourced spreadsheet created to collect information about theory jobs this year. 
Congratulations to both job seekers and departments/labs who are done with their searches!</p>

<p>In the past my academic uncle Lance Fortnow set this spreadsheet up (check <a href="https://blog.computationalcomplexity.org/2017/06/theory-jobs-2016.html">this link</a> to his post from two years ago which also has links to all the previous years). This year the first entry is Lance himself who is moving back to Chicago to be the Dean of the College of Science at the Illinois Institute of Technology. Did Lance get the idea from his advisor <a href="https://en.wikipedia.org/wiki/Michael_Sipser">Michael Sipser</a> who is also a Dean of Science but at MIT? In any case, great to see theoretical computer scientists stepping up to be the deans of science, congratulations!</p>

<p>Rules about the spreadsheet have been copied from last years and all edits to the document are anonymized. Please, post a comment if you have any suggestions about the rules.</p>
<ul>
 <li>Separate sheets for faculty, industry and postdocs/visitors. </li>
 <li>People should be connected to theoretical computer science, broadly defined.</li>
 <li>Only add jobs that you are absolutely sure have been offered and accepted. This is not the place for speculation and rumors. </li>
 <li>You are welcome to add yourself, or people your department has hired. </li>
</ul>

<p>This document will continue to grow as more jobs settle.</p>




  <p><a href="http://grigory.github.io/blog/theory-jobs-2019/">Theory Jobs 2019</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on May 30, 2019.</p></div>
    </content>
    <updated>2019-05-30T00:00:00Z</updated>
    <published>2019-05-30T00:00:00Z</published>
    <author>
      <name>Grigory Yaroslavtsev</name>
      <email>grigory@grigory.us</email>
      <uri>http://grigory.github.io/blog</uri>
    </author>
    <source>
      <id>http://grigory.github.io/blog/</id>
      <author>
        <name>Grigory Yaroslavtsev</name>
        <email>grigory@grigory.us</email>
        <uri>http://grigory.github.io/blog/</uri>
      </author>
      <link href="http://grigory.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="http://grigory.github.io/blog" rel="alternate" type="text/html"/>
      <title xml:lang="en">The Big Data Theory</title>
      <updated>2019-05-30T19:18:10Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-1040381893569171546</id>
    <link href="https://blog.computationalcomplexity.org/feeds/1040381893569171546/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/nsf-panels.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1040381893569171546" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1040381893569171546" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/nsf-panels.html" rel="alternate" type="text/html"/>
    <title>NSF Panels</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The government shut down in January led to delays at the National Science Foundation and only recently announcing decisions on grants submitted last fall. For those who successfully received awards, congratulations! For those who didn't, don't take it personally, buckle up and try again.<br/>
<br/>
For those who don't know how the process works, for each grant program, the program directors organize one or more panels which typically meets in person at NSF headquarters in Alexandria, Virginia. A typical panel has about a dozen panelists and twenty or so proposals. Before the panels, each proposal gets at least three reviews by the panelists. Discussions ensue over a day or two, proposals get sorted into categories: Highly Competitive, Competitive, Low Competitive and Not Competitive and then ranked ordered in the top categories.<br/>
<br/>
There are tight rules for Conflict-of-Interest and those who are conflicted have to leave the room during the discussions on those papers.<br/>
<br/>
If you do get asked to serve on a panel, you should definitely do so. You get to see how the process works and help influence funding and research directions in your field. You can't reveal when you serve on a particular panel but you can say "Served on NSF Panels" on your CV.<br/>
<br/>
Panels tend to take proposals that will likely make progress and not take ones less risky. Funding risky proposals is specifically mentioned to the panel but when push comes to shove and there is less funding than worthy proposals, panelists gravitate towards proposals that don't take chances.<br/>
<br/>
Panels are not unlike conference program committees. It didn't always work this way, it used to be more like journal publications. I remember when the program director would send out proposals for outside reviews and then make funding decisions. That gave the program director more discretion to fund a wider variety of proposals.<br/>
<br/>
The NSF budget for computing goes up slowly while the number of academic computer scientists grows at a much larger clip. Until this changes, we'll have more and more worthy proposals unfunded, particularly proposals of bold risky projects. That's the saddest part of all.</div>
    </content>
    <updated>2019-05-29T20:09:00Z</updated>
    <published>2019-05-29T20:09:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-06-04T20:14:20Z</updated>
    </source>
  </entry>
</feed>
