<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-06-09T22:25:01Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>http://gradientscience.org/robust_apps/</id>
    <link href="http://gradientscience.org/robust_apps/" rel="alternate" type="text/html"/>
    <title>Robustness beyond Security&amp;#58; Computer Vision Applications</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a class="bbutton" href="https://gradientscience.org/robust-apps.pdf" style="float: left;">
<i class="fas fa-file-pdf"/>
    Paper
</a>
<a class="bbutton" href="http://git.io/robust-apps" style="float: left;">
<i class="fab fa-github"/>
   Notebooks
</a>
<a class="bbutton" href="http://bit.ly/robustness_demo" style="float: left;">
<i class="fas fa-code"/>
   Live Demo
</a></p>

<p><i>We discuss our <a href="https://gradientscience.org/robust-apps.pdf">latest paper</a>
on computer vision applications of robust classifiers.
We are able to leverage the features learned by a single classifier to 
develop a rich toolkit for diverse computer vision applications.
Our results suggest the robust classification framework as a viable alternative
to more complex or task-specific approaches.
</i></p>

<p>In our <a href="https://gradientscience.org/robust_reps/">previous post</a>, we saw how robust models 
capture high-level, human-aligned features that can be directly manipulated through 
gradient descent. In this post, we demonstrate how to leverage these robust models 
to perform a wide range of computer vision tasks. In fact, we perform all these tasks 
by simply optimizing the predicted class scores of a <i>single</i> robustly trained 
classifier (per dataset). The resulting toolkit is simple, versatile and 
reliable—to highlight its consistency, in this post we visualize the performance 
of our method using <i>random</i> (not cherry-picked) samples.</p>

<p>Our corresponding paper can be found <a href="https://gradientscience.org/robust-apps.pdf">here</a>, and you 
can reproduce all of our results using open-source IPython notebooks 
<a href="http://git.io/robust-apps">here</a>.
We also have a live <a href="http://bit.ly/robustness_demo">demo</a> where you play with a 
trained robust model like this:</p>

<video align="right" style="width: 90%;">
    <source src="http://gradientscience.org/assets/rf-vision/robust_apps_demo.mp4" type="video/mp4">
    <source src="http://gradientscience.org/assets/rf-vision/robust_apps_demo.webm" type="video/webm">
</source></source></video>

<h3 id="robust-models-as-a-tool-for-input-manipulation">Robust Models as a Tool for Input Manipulation</h3>
<p>The key primitive of our approach is <i>class maximization</i>: maximization of class 
log-probabilities (scores) from an <a href="https://gradientscience.org/robust_opt_pt1/">adversarially robust model</a> 
using gradient descent in input space.</p>

<p>As discussed in our <a href="https://gradientscience.org/robust_reps/">last post</a>, robust models learn 
representations that are more aligned with human perception. As it turns out, 
performing class maximization on these models actually introduces class-relevant 
characteristics in the corresponding input (after all, these class log-probabilities 
are just linear combinations of learned representations). To visualize this, here is 
the result of class maximization for a few random inputs:</p>

<p><img alt="Targeted adversarial examples for a robust model" src="http://gradientscience.org/assets/rf-vision/targeted.jpg"/></p>
<div class="footnote"> For a robust
model, maximizing the predicted probability of a specific class enhances key
features of that class in the input. </div>

<p>These results are also in line with <a href="https://arxiv.org/abs/1805.12152">our previous
work</a>, where we observed that large
adversarial perturbations for robust models often actually resemble natural
examples of the corresponding incorrect class.</p>

<p>In the rest of this post, we will explore how to perform a variety of computer
vision tasks using only class maximization. It turns out that robustness is all
you need!</p>

<h3 id="generation">Generation</h3>
<p>We begin by leveraging robust models to generate diverse,
realistic images. As we saw in the previous section, it is possible to introduce
salient features of a target class into an input through class maximization.
This simple operation alone turns out to also suffice to (class-conditionally)
generate images.</p>

<p>To generate an image, we randomly sample a starting point (seed) and then
execute (starting from that seed) the projected gradient ascent operation
underlying class maximization. The key question here is: how do we sample the
seed input? A natural idea is to just fit a Gaussian distribution to each class
(in image space), and sample seeds from that distribution.. Despite its
simplicity, this approach already leads to fairly diverse and realistic samples:</p>

<div class="widget">
    <div class="choices_one_full" id="gen">
    <span class="widgetheading" id="genclass">Choose an Image</span>
    </div>
    <div style="border-right: 3px white solid;">
        <img class="image-container" id="gen1" style="width: 38%; margin: 8px;"/>
        <img class="image-container" id="gen2" style="width: 60%;"/>
    </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Interactive demo</strong>: select any image in the top two rows to see additional
samples of that class.
</div>

<p>We expect that there is still room for improvement; for example, one could
replace Gaussians with a sophisticated class of distributions for seed sampling.</p>

<h3 id="image-to-image-translation">Image-to-Image translation</h3>

<p>The ability to introduce perceptually meaningful, class-relevant features in
image space (via class maximization) enables a very intuitive approach to
performing <a href="https://arxiv.org/abs/1703.10593">image-to-image translation</a>, the
task of transforming inputs from a source to a target domain (e.g., transforming
horses into zebras in photos). To perform this task, we first train a classifier
to <em>robustly</em> distinguish between the two domains. This process encourages the
classifier to learn key characteristics of each domain. We then perform image
translation on an image from a given domain simply by using class maximization
towards the target domain—this suffices! Here are some instances of our
approach applied to typical datasets:</p>

<div class="widget">
    <div class="choices_one" id="translate">
    <span class="widgetheading">Choose an Image</span>
    </div>
    <span class="widgetheading" id="translatedclass">Translated Image</span>
    <div class="beer-slider selected_one" id="translate_slider">
    <img id="translate1" style="width: 336px;"/>
    <div class="beer-reveal" style="border-right: 3px white solid;">
        <img class="slider_img" id="translate2"/>
    </div>
    </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Interactive demo</strong>: choose an image on the left to see the
result of image-to-image translation.
</div>
<p>This is what a few particularly nice samples produced by our
method look like:</p>

<p><img alt="Select image-to-image translations" src="http://gradientscience.org/assets/rf-vision/translation.jpeg"/></p>

<p>Just as with image generation, we find reasonable solutions to the task using
only class maximization on a robust classifier.</p>

<h3 id="inpainting">Inpainting</h3>

<p>Next, we consider the task of image inpainting—recovering images with large
missing or corrupted regions. At a high level, we would like to fill in the
damaged regions with features that are human-meaningful and consistent with the
rest of the image. Within our framework, the most natural way to do this is to
perform class maximization (towards the original class)  while also penalizing
large changes to the uncorrupted regions of the image. The intuition being that
this process restores the “missing” features while only minimally modifying the
rest of the image. This is how a few random inpainted images produced by our
method look like:</p>

<div class="widget">
    <div class="choices_one" id="inpaint">
    <span class="widgetheading">Choose an Image</span>
    </div>
    <span class="widgetheading">Inpainted Image</span>
    <div class="beer-slider selected_one" id="inpaint_slider">
    <img id="inpaint1" style="width: 336px;"/>
    <div class="beer-reveal" style="border-right: 3px white solid;">
        <img class="slider_img" id="inpaint2"/>
    </div>
    </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Interactive demo</strong>: click on an image on the left to see
an instance of inpainting for that image.
</div>

<p>Interestingly, even when our method produces reconstructions that differ from
the original, they are often still perceptually plausible to a human. Here are a
few select samples:</p>

<p><img alt="Failure modes of inpainting using robust models" src="http://gradientscience.org/assets/rf-vision/inpainting_errors.jpeg"/></p>
<div class="footnote">
Even when inpainting fails to recover the original uncorrupted image, the result is 
often still perceptually plausible to a human.
</div>

<h3 id="superresolution">Superresolution</h3>

<p>To perform inpainting, we used class maximization to restore relevant features
in corrupted images. The exact same intuition applies to the task of
superresolution, i.e., improving the resolution of an image in a human-
meaningful way. Specifically, using class maximization towards the underlying
true class, we can accentuate image features that are distorted in the low-
resolution image. Here we apply this method to images from the CIFAR10 dataset
(32x32 pixels) to a resolution of 224x224 (7-fold upsampling):</p>

<div class="widget">
    <div class="choices_one" id="upsample">
    <span class="widgetheading">Choose an Image</span>
    </div>
    <span class="widgetheading">Upsampled Image</span>
    <div class="beer-slider selected_one" id="upsample_slider">
    <img id="upsample1" style="width: 336px;"/>
    <div class="beer-reveal" style="border-right: 3px white solid;">
        <img class="slider_img" id="upsample2"/>
    </div>
    </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Interactive demo</strong>: click on any of the images on the left to see its
7-fold superresolution.
</div>

<p>Since the starting point of the underlying class maximization comes from a crude
upsampling (i.e., nearest neighbor interpolation) of the low-resolution image,
the final images exhibit some pixelation artifacts. We expect, however, that
combining this approach with a more sophisticated initialization will yield even
more realistic samples.</p>

<h3 id="interactive-image-manipulation">Interactive Image Manipulation</h3>

<p>Finally, using this simple primitive, one can build an interactive toolkit for
performing input space manipulations.</p>

<h4 id="sketch-to-image">Sketch-to-image</h4>

<p>Class maximization with robust classifiers turns out to yield human-meaningful
transformations even for <em>arbitrary</em> inputs. This enables us to use this
primitive to even transform hand-drawn sketches into realistic images. Here is
the result of maximizing a chosen class probability from a very crude sketch:</p>

<div class="widget">
    <div class="choices_one" id="sketch">
    <span class="widgetheading">Choose an Image</span>
    </div>
    <span class="widgetheading" id="sketchclass">Enhanced Image</span>
    <div class="beer-slider selected_one" id="sketch_slider">
    <img id="sketch1" style="width: 336px;"/>
    <div class="beer-reveal" style="border-right: 3px white solid;">
        <img class="slider_img" id="sketch2"/>
    </div>
    </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Interactive demo</strong>: select any of the sketches on the left to see it 
converted into a realistic image.
</div>

<p>You can draw realistic looking images with this method interactively
<a href="http://bit.ly/robustness_demo">here</a>, without any artistic skill!</p>

<h4 id="paint-with-features">Paint-with-Features</h4>

<p>In fact, we can achieve an even more fine-grained level of manipulation if we
directly perform maximization on the <em>representations</em> learned by the robust
model, instead of the class probabilities. (Recall that in the <a href="https://gradientscience.org/robust_reps/">last
post</a> we saw how individual components can
correspond to human-level features such as “stripes”.) By adding a human in the
loop, we can choose particular regions of the image to modify and specific
features to add. This leads to a versatile paint tool (inspired by
<a href="http://gandissect.res.ibm.com/ganpaint.html">GANpaint</a>) that can perform
manipulation such as this:</p>

<div class="widget">
    <div class="choices_one_paint" id="paint_left1">
    <span class="widgetheading">Choose an Image</span>
    </div>
    <div class="selected_three" id="paintclass">
    <!-- <span class="widgetheading">Feature painting</span> -->
    <video id="paint_video" style="width: 100%;">
        <source id="paint_selected_mp4" type="video/mp4">
        <source id="paint_selected_webm" type="video/webm">
    </source></source></video>
    </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Interactive demo</strong>: Choose one of the images on the left to see
a demonstration of painting high-level features onto the image.
</div>

<p>Here, we added a feature to a given region of the image by simply maximize the
corresponding activation (a single component of the robust representation
vector) while penalizing changes to the rest of the image. By successively
performing such <i>activation maximization</i> in different parts of the image,
we can paint with high-level concepts (e.g., grass or stripes).</p>

<h3 id="takeaways">Takeaways</h3>

<p>In this blog post, we applied simple, first-order manipulations of the
representation learned by a <em>single</em> robust classifier to perform a number of
computer vision tasks. This is contrast to prior approaches that
often required specialized
and sophisticated techniques. Crucially, to highlight the potential of the core
methodology itself, we used the same simple toolkit for all tasks and datasets,
and with minimal tuning and no task-specific optimizations. We expect that the
addition of domain knowledge and leveraging more perceptually-aligned notions of
robustness will further boost the performance of this toolkit. Importantly,
the models we use here are truly off-the-shelf and
are trained in a standard (and stable) manner (via
<a href="https://gradientscience.org/robust_opt_pt1/">robust optimization</a>).</p>

<p>Furthermore, our results highlight the utility of the basic classification
toolkit outside of classification tasks. We hope that our framework will expand to
offer ways to perform other vision tasks, on par with the existing
state-of-the-art techniques (e.g., based on generative models). Finally, our
findings highlight the merits of adversarial robustness as a goal that goes
beyond the security and reliability contexts this goal was considered in so far.</p></div>
    </summary>
    <updated>2019-06-06T00:00:00Z</updated>
    <published>2019-06-06T00:00:00Z</published>
    <source>
      <id>http://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="http://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="http://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2019-06-08T23:22:46Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://gradientscience.org/robust_reps/</id>
    <link href="http://gradientscience.org/robust_reps/" rel="alternate" type="text/html"/>
    <title>Robustness beyond Security&amp;#58; Representation Learning</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a class="bbutton" href="https://arxiv.org/abs/1906.00945" style="float: left;">
<i class="fas fa-file-pdf"/>
    Read the paper
</a>
<a class="bbutton" href="http://git.io/robust-reps" style="float: right;">
<i class="fab fa-github"/>
   Download the notebooks
</a></p>

<p><i>This post discusses our <a href="https://arxiv.org/abs/1906.00945">latest paper</a>
on deep network representations—while representations of standard
networks are brittle and thus not fully reflective of the input geometry,
we find that the representations of robust networks are amenable to all
sorts of manipulation, and can truly be thought of (and dealt with) as just
high-level feature representations. Our work suggests that robustness might
be more broadly useful than just protection against adversarial examples.
</i></p>

<p>One of the most promising aspects of deep neural networks is their
potential to learn high-level <i>features</i> that are useful beyond the
classification task at hand. Our mental model of deep
learning classifiers is often similar to the following diagram, in which
the networks learns progressively higher-level features until the final
layer, which acts as a linear classifier over these high-level features:</p>

<p><img alt="" src="http://gradientscience.org/assets/rf1_images/visualization.png"/></p>
<div class="footnote">
A conceptual picture of our understanding of modern deep neural networks
(NVIDIA).
</div>

<p>This picture is consistent with the surprising versatility of deep neural
network <i>feature representations</i>—learned representations for one
task are useful for many others
(as in <a href="https://papers.nips.cc/paper/959-learning-many-related-tasks-at-the-same-time-with-backpropagation.pdf">transfer learning</a>), and
distance in representation space has often been proposed as a perceptual
metric on natural images (as in <a href="http://arxiv.org/abs/1801.03924">VGG distance</a>).</p>

<div class="footnote">
<strong>Note:</strong> In <a href="https://arxiv.org/abs/1906.00945">our paper</a> and in this blog post, we refer to the
<i>representation</i> $R(x)$ of an input $x$ for a network as the
values of the penultimate layer in the network for that input.
</div>

<p>But to what extent is this picture accurate? It turns out that it is
rather simple to (consistently) construct images that are <i>completely</i>
different to a human, but share very similar representations:</p>

<p><img alt="Standard representations are brittle" src="http://gradientscience.org/assets/rf1_images/standard_brittleness.png"/></p>
<div class="footnote">
The above two images, despite seeming completely different to humans, share very
similar representations.
</div>

<p>This phenomenon is somewhat troubling for our conceptual picture: if
feature representations actually encode high-level, human-meaningful
features, we should not be able to find two images with totally different
features that the model “sees” as very similar.</p>

<p>The phenomenon at play here turns out to be more fundamental than just
pairs of images with similar representations. Indeed, the
representations of neural networks seem to be <i>pervasively brittle</i>:
they can be manipulated arbitrarily without meaningful change to the input.
(In fact, this brittleness is similar to the phenomenon that we exploit
when making <a href="https://gradientscience.org/intro_adversarial">adversarial examples</a>.)</p>

<p>Clearly, this brittleness precludes standard representations from acting
how we want them to—in particular, distance in representation space is
not fully <i>aligned</i> with our human perception of distance in feature
space. So, how might we go about fixing this issue?</p>

<h3 id="adversarial-robustness-as-a-feature-prior">Adversarial Robustness as a Feature Prior</h3>

<p>Unfortunately, we don’t have a way to explicitly control which features
models learn (or in what way they learn them). We can, however,
disincentivize models from using features that humans <i>definitely</i>
don’t use by imposing a <i>prior</i> during training. In our paper, we
explore a very simple prior: namely, that imperceptible changes in the
input should not cause large changes in the model’s prediction (i.e.,
models should not rely on brittle features):</p>



<p>Note that this stability is a necessary, but not sufficient property: all
features that humans use certainly obey this property (for reasonably small
), but not every feature obeying this property is one that we
want our models to rely on.</p>

<p>How should we enforce this prior? Well, observe that the condition
$\eqref{eq:robustcond}$ above is actually <i>precisely</i> $\ell_2$-<a href="https://gradientscience.org/intro_adversarial">adversarial
robustness</a>! Thus, a natural method to employ is robust optimization,
which, as we discussed in a <a href="https://gradientscience.org/robust_opt_pt1">previous post</a>, provides reasonable
robustness to adversarial perturbations. Concretely, instead of just
minimizing loss, we opt to minimize <i>adversarial loss</i>:</p>



<h3 id="inverting-representations">Inverting representations</h3>

<p>Now, given a network trained in this manner, what happens if we look for
images with the same representations? Concretely, fixing some image $x$,
what happens if we look for an image $x’$ that has a matching
representation:</p>



<p>(Note that we found the image pairs presented earlier for standard networks
by solving exactly the above problem.) It turns out that when our model is
<i>robust</i>, we end up with an image that is remarkably similar to the
original:</p>

<div class="widget">
    <div class="choices_one" id="left2">
	<span class="widgetheading">Choose an Image</span>
    </div>
    <span class="widgetheading">Reconstructed Image</span>
    <div class="beer-slider selected_one" id="inv_slider">
	<img id="selectedinv1"/>
	<div class="beer-reveal" style="border-right: 3px white solid;">
	    <img class="slider_img" id="selectedinv2"/>
	</div>
    </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Interactive demo</strong>: click on any of the images on the left to see its
reconstruction via the representation of a robust network. The top row
contains random images from the test set, and the bottom row has random
<i>out-of-distribution</i> inputs (images without a correct class).
</div>

<p>Indeed, instead of being able to manipulate feature representations
arbitrarily within a small radius, we now find that matching the
representation of an image leads to (approximately) matching the image
itself.</p>

<h2 id="what-can-we-do-with-these-representations">What can we do with these representations?</h2>
<p>We just saw that the learned representation of a robust deep classifier
suffices to reconstruct its input pretty accurately (at least in terms of
human perception). This highlights two crucial properties of these
representations: a) optimizing for closeness in representation space leads
to perceptually similar images, b) representations contain a large amount
of information about the high-level features of the inputs. These
properties are very desirable and prompt us to further explore the
structure and potential of these representations.  <i>What we find is that
the representations of robust networks can truly be thought of as
high-level feature representations, and thus (in stark contrast to standard
networks) are naturally amenable to various types of manipulation.</i></p>

<p>In the following sections, we explore these “robust representations” in
more depth. A crucial theme in our exploration is
<i>model-faithfulness</i>. Though significant work has been done in
manipulating and interpreting standard (non-robust) models, it seems as
though getting anything meaningful from standard networks requires
enforcing <i>priors</i> into the visualization process
(see this excerpt <a href="https://distill.pub/2017/feature-visualization/#enemy-of-feature-vis">“The Enemy of Feature Visualization”</a>
for a discussion and illustration of this). This comes at the cost of either hiding
vital signals the model utilizes or introducing information that was not
already present in the model—thus blurring the line between what
information the model actually has, versus what information we introduced
when interacting with it. In contrast, throughout our exploration we will
rely on only direct optimization over representation space, without
introducing any priors or extra information.</p>

<h3 id="feature-visualization">Feature visualization</h3>

<p>We begin our exploration of robust representations by trying to understand
the features captured by their individual components. We visualize these
components in the simplest possible way: we perform gradient descent to
find inputs that maximally activate individual components of the
representation. This is how a few <em>random</em> visualizations look like:</p>

<p><img src="http://gradientscience.org/assets/rf1_images/features.png" style="margin: 0;"/></p>
<div class="footnote">
   Inputs maximizing various coordinates (separated by column) of a robust network, found via gradient descent starting from the "seed" image on the far left.
</div>

<p>We see a surprising alignment with human concepts. For instance, the last
component above seems to correspond to “anemone” and the second-last component to
“flowers”. In fact, these names are consistent with the test images
maximally activating these neurons—here are the images corresponding to
each component:</p>

<div class="widget">
    <div class="choices_one" id="left_maxact">
	<span class="widgetheading">Choose a Coordinate (Feature)</span>
    </div>
    <span class="widgetheading">Top Images</span>
    <div class="selected_one" id="maxact_selected"/>
</div>
<div style="clear: both;"/>
<div class="footnote">
    <strong>Interactive demo</strong>: On the left are components of the representation of a robust network
    (the thumbnails are a visualization of the components maximized from
    noise). On the right are the images from the test set that maximally activate the corresponding components.
</div>

<p>These visualizations might look familiar. Indeed, similar results have been
produced in prior work using non-robust models (e.g.
<a href="https://distill.pub/2017/feature-visualization/">here</a> or
<a href="https://distill.pub/2018/building-blocks/">here</a>). The difference is that
the images above are generated by directly maximizing representation
components with gradient descent in input space—we do not enforce any
priors or regularization. For standard networks, the same process is
unfruitful—to circumvent this, prior work imposes priors on the
optimization process.</p>

<h3 id="feature-manipulation">Feature Manipulation</h3>

<p>So far, we have seen that matching the representation of an image starting
from random noise, recovers the high-level features of the image itself. At
the same time, we saw that individual representation components correspond
to high-level human-meaningful concepts. These findings suggests an
intriguing possibility: perhaps we can directly modify high-level features
of an image by manipulating the corresponding representation over the input
space.</p>

<p>This turns out to yield remarkably plausible results! Here we visualize the
results of increasing a few select components via gradient descent over the
image space for a few <em>random</em> (not cherry-picked) inputs:</p>

<div class="widget">
    <div class="choices_right" id="right1">
	<span class="widgetheading">Choose a Coordinate</span>
    </div>
    <div class="selected_two">
	<span class="widgetheading">Feature Addition</span>
	<div class="beer-slider" id="manipulation_slider">
	    <img id="man_selected1"/> 
	    <div class="beer-reveal" style="border-right: 3px white solid;">
		<img class="slider_img" id="man_selected2"/> 
	    </div>
	</div>
    </div>
    <div class="choices_left" id="left1">
	<span class="widgetheading">Choose a Source Image</span>
    </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Interactive demo</strong>: On the left are randomly selected source images,
and on the right are components of the representation of a robust network
(the thumbnails are a visualization of the components maximized from
noise). In the middle is the feature from the selected component, "added"
to the selected image.
</div>

<p>These images end up actually exhibiting the relevant features in a way that
is plausible to humans (for example, stripes appear mostly on animals
instead of the background).</p>

<p>This opens up a wide range of fine-grained manipulations that one can
perform by leveraging the learned representations (in fact, stay tuned for
some applications in our next blog post).</p>

<h3 id="input-interpolation">Input interpolation</h3>
<p>In fact, this outlook can be pushed even further—robust models can be
leveraged as a tool for another kind of manipulation: input-to-input
interpolation. That is, if we think of robust representations as encoding
the high-level features of an input in a sensible manner, an intuitive way
to interpolate between any two inputs is to linearly interpolate their
representations. More precisely, given any two inputs, we can try to
construct an interpolation between them by linearly interpolating their
representations and then constructing inputs to match these
representations.</p>

<p>This rather intuitive way of dealing with representations turns out to work
reasonably well—we can interpolate between arbitrary images. Randomly
sampled interpolations are shown below:</p>

<div class="widget">
    <div class="choices_left" id="int_left1">
	<span class="widgetheading">Choose a Source Image</span>
    </div>
    <div class="selected_two">
	<span class="widgetheading">Interpolation</span>
	<video style="width: 100%;">
	    <source id="int_selected" type="video/mp4">
	</source></video>
    </div>
    <div class="choices_right" id="int_right1">
	<span class="widgetheading">Choose a Destination Image</span>
    </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Interactive demo</strong>: On the left are randomly selected source images,
and on the right are randomly selected target images.
In the middle is the feature interpolation from the selected source image
to the selected target.
</div>

<p>As we can see, the interpolations appears perceptually plausible. Note
that, in contrast to approaches based on generative models
(e.g. <a href="https://arxiv.org/abs/1511.06434">here</a> or
<a href="https://arxiv.org/abs/1809.11096">here</a>), this approach can interpolate
between arbitrary inputs and not only between those produced by the
generative model.</p>

<h3 id="insight-into-model-predictions">Insight into model predictions</h3>
<p>Expanding on our view of deep classifiers as simple linear classifiers on
top of the learned representations, there is also a simple way to gain
insight into predictions of (robust) models. In particular, for incorrect
predictions, we can identify the component most heavily contributing to the
incorrect class (in the same way we would for a linear classifier) and then
directly manipulate the input to increase the value of that component (with
input-space gradient descent). Here we perform this visualization for a few
random misclassified inputs:</p>

<p><img src="http://gradientscience.org/assets/rf1_images/misclassification_IN.jpg" style="margin: 0;"/></p>

<p>The resulting images could provide insight into the model’s incorrect
decision. For instance, we see the bug becoming a dog eye or negative space
becoming the face of a dog. At a high level, these inputs demonstrate which
parts of the image the incorrect prediction was most sensitive to.</p>

<p>Still, as a word of caution, it is important to note that just as with all
saliency methods (e.g. heatmaps, occlusion studies, etc.), visualizing
features and studying misclassification only gives insights into a “local”
sense of model behaviour. Deep neural networks are complex, highly
non-linear models and it’s important to keep in mind that <i>local
sensitivity does not necessarily entail causality</i>.</p>

<h2 id="towards-better-learned-representations">Towards better learned representations</h2>
<p>As we discussed, robust feature representations possess properties that
make them desirable from a broader point of view. In particular, we found
these representations to be better aligned with a perceptual notion of
distance, while allowing us to perform direct input manipulations in a
model-faithful way. These are properties that are fundamental to any “truly
human-level” representation. One can thus view adversarial robustness as a
very potent prior for obtaining representations that are more aligned with
human perception beyond the standard goals of security and reliability.</p></div>
    </summary>
    <updated>2019-06-03T00:00:00Z</updated>
    <published>2019-06-03T00:00:00Z</published>
    <source>
      <id>http://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="http://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="http://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2019-06-08T23:22:45Z</updated>
    </source>
  </entry>
</feed>
