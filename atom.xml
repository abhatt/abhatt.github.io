<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-07-14T11:22:24Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.06242</id>
    <link href="http://arxiv.org/abs/2007.06242" rel="alternate" type="text/html"/>
    <title>Settling the Price of Fairness for Indivisible Goods</title>
    <feedworld_mtime>1594684800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Barman:Siddharth.html">Siddharth Barman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bhaskar:Umang.html">Umang Bhaskar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shah:Nisarg.html">Nisarg Shah</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06242">PDF</a><br/><b>Abstract: </b>In the allocation of resources to a set of agents, how do fairness guarantees
impact the social welfare? A quantitative measure of this impact is the price
of fairness, which measures the worst-case loss of social welfare due to
fairness constraints. While initially studied for divisible goods, recent work
on the price of fairness also studies the setting of indivisible goods.
</p>
<p>In this paper, we resolve the price of two well-studied fairness notions for
the allocation of indivisible goods: envy-freeness up to one good (EF1), and
approximate maximin share (MMS). For both EF1 and 1/2-MMS guarantees, we show,
via different techniques, that the price of fairness is $O(\sqrt{n})$, where
$n$ is the number of agents. From previous work, it follows that our bounds are
tight. Our bounds are obtained via efficient algorithms. For 1/2-MMS, our bound
holds for additive valuations, whereas for EF1, our bound holds for the more
general class of subadditive valuations. This resolves an open problem posed by
Bei et al. (2019).
</p></div>
    </summary>
    <updated>2020-07-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.06167</id>
    <link href="http://arxiv.org/abs/2007.06167" rel="alternate" type="text/html"/>
    <title>Local Editing in LZ-End Compressed Data</title>
    <feedworld_mtime>1594684800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Daniel Roodt, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Speidel:Ulrich.html">Ulrich Speidel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Vimal.html">Vimal Kumar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Ko:Ryan_K=_L=.html">Ryan K. L. Ko</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06167">PDF</a><br/><b>Abstract: </b>This paper presents an algorithm for the modification of data compressed
using LZ-End, a derivate of LZ77, without prior decompression. The performance
of the algorithm and the impact of the modifications on the compression ratio
is evaluated. Finally, we discuss the importance of this work as a first step
towards local editing in Lempel-Ziv compressed data.
</p></div>
    </summary>
    <updated>2020-07-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.06110</id>
    <link href="http://arxiv.org/abs/2007.06110" rel="alternate" type="text/html"/>
    <title>Streaming Algorithms for Online Selection Problems</title>
    <feedworld_mtime>1594684800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Correa:Jos=eacute=.html">José Correa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/D=uuml=tting:Paul.html">Paul Dütting</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fischer:Felix.html">Felix Fischer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schewior:Kevin.html">Kevin Schewior</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Ziliotto:Bruno.html">Bruno Ziliotto</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06110">PDF</a><br/><b>Abstract: </b>The model of streaming algorithms is motivated by the increasingly common
situation in which the sheer amount of available data limits the ways in which
the data can be accessed. Streaming algorithms are typically allowed a single
pass over the data and can only store a sublinear fraction of the data at any
time. We initiate the study of classic online selection problems in a streaming
model where the data stream consists of two parts: historical data points that
an algorithm can use to learn something about the input; and data points from
which a selection can be made. Both types of data points are i.i.d. draws from
an unknown distribution. We consider the two canonical objectives for online
selection---maximizing the probability of selecting the maximum and maximizing
the expected value of the selection---and provide the first performance
guarantees for both these objectives in the streaming model.
</p></div>
    </summary>
    <updated>2020-07-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.06105</id>
    <link href="http://arxiv.org/abs/2007.06105" rel="alternate" type="text/html"/>
    <title>Efficient Labeling for Reachability in Digraphs</title>
    <feedworld_mtime>1594684800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Maciej Dulęba, Paweł Gawrychowski, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Janczewski:Wojciech.html">Wojciech Janczewski</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06105">PDF</a><br/><b>Abstract: </b>We consider labeling nodes of a directed graph for reachability queries. A
reachability labeling scheme for such a graph assigns a binary string, called a
label, to each node. Then, given the labels of nodes $u$ and $v$ and no other
information about the underlying graph, it should be possible to determine
whether there exists a directed path from $u$ to $v$. By a simple information
theoretical argument and invoking the bound on the number of partial orders, in
any scheme some labels need to consist of at least $n/4$ bits, where $n$ is the
number of nodes. On the other hand, it is not hard to design a scheme with
labels consisting of $n/2+O(\log n)$ bits. In the classical centralised
setting, Munro and Nicholson designed a data structure for reachability queries
consisting of $n^2/4+o(n^2)$ bits (which is optimal, up to the lower order
term). We extend their approach to obtain a scheme with labels consisting of
$n/3+o(n)$ bits.
</p></div>
    </summary>
    <updated>2020-07-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.06098</id>
    <link href="http://arxiv.org/abs/2007.06098" rel="alternate" type="text/html"/>
    <title>Graph Connectivity and Single Element Recovery via Linear and OR Queries</title>
    <feedworld_mtime>1594684800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Assadi:Sepehr.html">Sepehr Assadi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chakrabarty:Deeparnab.html">Deeparnab Chakrabarty</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khanna:Sanjeev.html">Sanjeev Khanna</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06098">PDF</a><br/><b>Abstract: </b>We study the problem of finding a spanning forest in an undirected,
$n$-vertex multi-graph under two basic query models. One is the Linear query
model which are linear measurements on the incidence vector induced by the
edges; the other is the weaker OR query model which only reveals whether a
given subset of plausible edges is empty or not. At the heart of our study lies
a fundamental problem which we call the {\em single element recovery} problem:
given a non-negative real vector $x$ in $N$ dimension, return a single element
$x_j &gt; 0$ from the support. Queries can be made in rounds, and our goals is to
understand the trade-offs between the query complexity and the rounds of
adaptivity needed to solve these problems, for both deterministic and
randomized algorithms. These questions have connections and ramifications to
multiple areas such as sketching, streaming, graph reconstruction, and
compressed sensing. Our main results are:
</p>
<p>* For the single element recovery problem, it is easy to obtain a
deterministic, $r$-round algorithm which makes $(N^{1/r}-1)$-queries per-round.
We prove that this is tight: any $r$-round deterministic algorithm must make
$\geq (N^{1/r} - 1)$ linear queries in some round. In contrast, a $1$-round
$O(\log^2 N)$-query randomized algorithm which succeeds 99% of the time is
known to exist.
</p>
<p>* We design a deterministic $O(r)$-round, $\tilde{O}(n^{1+1/r})$-OR query
algorithm for graph connectivity. We complement this with an
$\tilde{\Omega}(n^{1 + 1/r})$-lower bound for any $r$-round deterministic
algorithm in the OR-model.
</p>
<p>* We design a randomized, $2$-round algorithm for the graph connectivity
problem which makes $\tilde{O}(n)$-OR queries. In contrast, we prove that any
$1$-round algorithm (possibly randomized) requires $\tilde{\Omega}(n^2)$-OR
queries.
</p></div>
    </summary>
    <updated>2020-07-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.06060</id>
    <link href="http://arxiv.org/abs/2007.06060" rel="alternate" type="text/html"/>
    <title>Recognizing $k$-Clique Extendible Orderings</title>
    <feedworld_mtime>1594684800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Francis:Mathew.html">Mathew Francis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Neogi:Rian.html">Rian Neogi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Raman:Venkatesh.html">Venkatesh Raman</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06060">PDF</a><br/><b>Abstract: </b>A graph is $k$-clique-extendible if there is an ordering of the vertices such
that whenever two $k$-sized overlapping cliques $A$ and $B$ have $k-1$ common
vertices, and these common vertices appear between the two vertices $a,b\in
(A\setminus B)\cup (B\setminus A)$ in the ordering, there is an edge between
$a$ and $b$, implying that $A\cup B$ is a $(k+1)$-sized clique. Such an
ordering is said to be a $k$-C-E ordering. These graphs arise in applications
related to modelling preference relations. Recently, it has been shown that a
maximum sized clique in such a graph can be found in $n^{O(k)}$ time when the
ordering is given. When $k$ is $2$, such graphs are precisely the well-known
class of comparability graphs and when $k$ is $3$ they are called
triangle-extendible graphs. It has been shown that triangle-extendible graphs
appear as induced subgraphs of visibility graphs of simple polygons, and the
complexity of recognizing them has been mentioned as an open problem in the
literature.
</p>
<p>While comparability graphs (i.e. $2$-C-E graphs) can be recognized in
polynomial time, we show that recognizing $k$-C-E graphs is NP-hard for any
fixed $k \geq 3$ and co-NP-hard when $k$ is part of the input. While our
NP-hardness reduction for $k \geq 4$ is from the betweenness problem, for
$k=3$, our reduction is an intricate one from the $3$-colouring problem. We
also show that the problems of determining whether a given ordering of the
vertices of a graph is a $k$-C-E ordering, and that of finding an $\ell$-sized
(or maximum sized) clique in a $k$-C-E graph, given a $k$-C-E ordering, are
complete for the parameterized complexity classes co-W[1] and W[1]
respectively, when parameterized by $k$. However we show that the former is
fixed-parameter tractable when parameterized by the treewidth of the graph.
</p></div>
    </summary>
    <updated>2020-07-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.06052</id>
    <link href="http://arxiv.org/abs/2007.06052" rel="alternate" type="text/html"/>
    <title>City Guarding with Limited Field of View</title>
    <feedworld_mtime>1594684800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Daescu:Ovidiu.html">Ovidiu Daescu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Malik:Hemant.html">Hemant Malik</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.06052">PDF</a><br/><b>Abstract: </b>Drones and other small unmanned aerial vehicles are starting to get
permission to fly within city limits. While video cameras are easily available
in most cities, their purpose is to guard the streets at ground level. Guarding
the aerial space of a city with video cameras is a problem that so far has been
largely ignored.
</p>
<p>In this paper, we present bounds on the number of cameras needed to guard the
city's aerial space (roofs, walls, and ground) using cameras with 180-degree
range of vision (the region in front of the guard), which is common for most
commercial cameras. We assume all buildings are vertical and have a rectangular
base. Each camera is placed at a top corner of a building.
</p>
<p>We considered the following two versions: (i) buildings have an axis-aligned
ground base and, (ii) buildings have an arbitrary orientation. We give
necessary and sufficient results for (i), necessary results for (ii), and
conjecture sufficiency results for (ii). Specifically, for (i) we prove a
sufficiency bound of 2k + k/4 +4 on the number of vertex guards, while for (ii)
we show that 3k + 1 vertex guards are sometimes necessary, where k is the total
number of buildings in the city.
</p></div>
    </summary>
    <updated>2020-07-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.05912</id>
    <link href="http://arxiv.org/abs/2007.05912" rel="alternate" type="text/html"/>
    <title>Robust Learning of Mixtures of Gaussians</title>
    <feedworld_mtime>1594684800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kane:Daniel_M=.html">Daniel M. Kane</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05912">PDF</a><br/><b>Abstract: </b>We resolve one of the major outstanding problems in robust statistics. In
particular, if $X$ is an evenly weighted mixture of two arbitrary
$d$-dimensional Gaussians, we devise a polynomial time algorithm that given
access to samples from $X$ an $\eps$-fraction of which have been adversarially
corrupted, learns $X$ to error $\poly(\eps)$ in total variation distance.
</p></div>
    </summary>
    <updated>2020-07-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.05870</id>
    <link href="http://arxiv.org/abs/2007.05870" rel="alternate" type="text/html"/>
    <title>A subquadratic algorithm for the simultaneous conjugacy problem</title>
    <feedworld_mtime>1594684800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brodnik:Andrej.html">Andrej Brodnik</a>, Aleksander Malnič, Rok Požar <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05870">PDF</a><br/><b>Abstract: </b>The $d$-Simultaneous Conjugacy problem in the symmetric group $S_n$ asks
whether there exists a permutation $\tau \in S_n$ such that $b_j = \tau^{-1}a_j
\tau$ holds for all $j = 1,2,\ldots, d$, where $a_1, a_2,\ldots , a_d$ and
$b_1, b_2,\ldots , b_d$ are given sequences of permutations in $S_n$. The time
complexity of existing algorithms for solving the problem is $O(dn^2)$. We show
that for a given positive integer $d$ the $d$-Simultaneous Conjugacy problem in
$S_n$ can be solved in $o(n^2)$ time.
</p></div>
    </summary>
    <updated>2020-07-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.05852</id>
    <link href="http://arxiv.org/abs/2007.05852" rel="alternate" type="text/html"/>
    <title>Submodular Meta-Learning</title>
    <feedworld_mtime>1594684800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Adibi:Arman.html">Arman Adibi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mokhtari:Aryan.html">Aryan Mokhtari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hassani:Hamed.html">Hamed Hassani</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05852">PDF</a><br/><b>Abstract: </b>In this paper, we introduce a discrete variant of the meta-learning
framework. Meta-learning aims at exploiting prior experience and data to
improve performance on future tasks. By now, there exist numerous formulations
for meta-learning in the continuous domain. Notably, the Model-Agnostic
Meta-Learning (MAML) formulation views each task as a continuous optimization
problem and based on prior data learns a suitable initialization that can be
adapted to new, unseen tasks after a few simple gradient updates. Motivated by
this terminology, we propose a novel meta-learning framework in the discrete
domain where each task is equivalent to maximizing a set function under a
cardinality constraint. Our approach aims at using prior data, i.e., previously
visited tasks, to train a proper initial solution set that can be quickly
adapted to a new task at a relatively low computational cost. This approach
leads to (i) a personalized solution for each individual task, and (ii)
significantly reduced computational cost at test time compared to the case
where the solution is fully optimized once the new task is revealed. The
training procedure is performed by solving a challenging discrete optimization
problem for which we present deterministic and randomized algorithms. In the
case where the tasks are monotone and submodular, we show strong theoretical
guarantees for our proposed methods even though the training objective may not
be submodular. We also demonstrate the effectiveness of our framework on two
real-world problem instances where we observe that our methods lead to a
significant reduction in computational complexity in solving the new tasks
while incurring a small performance loss compared to when the tasks are fully
optimized.
</p></div>
    </summary>
    <updated>2020-07-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.05647</id>
    <link href="http://arxiv.org/abs/2007.05647" rel="alternate" type="text/html"/>
    <title>Finding Equilibrium in Multi-Agent Games with Payoff Uncertainty</title>
    <feedworld_mtime>1594684800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guo:Wenshuo.html">Wenshuo Guo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Curmei:Mihaela.html">Mihaela Curmei</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Serena.html">Serena Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Recht:Benjamin.html">Benjamin Recht</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jordan:Michael_I=.html">Michael I. Jordan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05647">PDF</a><br/><b>Abstract: </b>We study the problem of finding equilibrium strategies in multi-agent games
with incomplete payoff information, where the payoff matrices are only known to
the players up to some bounded uncertainty sets. In such games, an ex-post
equilibrium characterizes equilibrium strategies that are robust to the payoff
uncertainty. When the game is one-shot, we show that in zero-sum polymatrix
games, an ex-post equilibrium can be computed efficiently using linear
programming. We further extend the notion of ex-post equilibrium to stochastic
games, where the game is played repeatedly in a sequence of stages and the
transition dynamics are governed by an Markov decision process (MDP). We
provide sufficient condition for the existence of an ex-post Markov perfect
equilibrium (MPE). We show that under bounded payoff uncertainty, the value of
any two-player zero-sum stochastic game can be computed up to a tight value
interval using dynamic programming.
</p></div>
    </summary>
    <updated>2020-07-14T01:24:41Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.05637</id>
    <link href="http://arxiv.org/abs/2007.05637" rel="alternate" type="text/html"/>
    <title>Dynamic Graph Streaming Algorithm for Digital Contact Tracing</title>
    <feedworld_mtime>1594684800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mahapatra:Gautam.html">Gautam Mahapatra</a>, Priodyuti~Pradhan, Ranjan Chattaraj, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Banerjee:Soumya.html">Soumya Banerjee</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05637">PDF</a><br/><b>Abstract: </b>Digital contact tracing of an infected person, testing the possible infection
for the contacted persons, and isolation play a crucial role in alleviating the
outbreak. Here, we design a dynamic graph streaming algorithm that can trace
the contacts under the control of the Public Health Authorities (PHA). The
algorithm can work as the augmented part of the PHA for the crisis period. Our
algorithm receives proximity data from the mobile devices as contact data
streams and uses a sliding window model to construct a dynamic contact graph
sketch. Prominently, we introduce the edge label of the contact graph as a
contact vector, which acts like a sliding window and holds the latest D days of
social interactions. Importantly, the algorithm prepares the direct and
indirect (multilevel) contact list from the contact graph sketch for a given
set of infected persons. The algorithm also uses a disjoint set data structure
to construct the infectious trees for the trace list. The present study offers
the design of algorithms with underlying data structures for digital contact
trace relevant to the proximity data produced by Bluetooth enabled mobile
devices. Our analysis reveals that for COVID-19 close contact parameters, the
storage space requires maintaining the contact graph of ten million users
having 14 days close contact data in PHA server takes 55 Gigabytes of memory
and preparation of the contact list for a given set of the infected person
depends on the size of the infected list.
</p></div>
    </summary>
    <updated>2020-07-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.05634</id>
    <link href="http://arxiv.org/abs/2007.05634" rel="alternate" type="text/html"/>
    <title>Vector Balancing in Lebesgue Spaces</title>
    <feedworld_mtime>1594684800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reis:Victor.html">Victor Reis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rothvoss:Thomas.html">Thomas Rothvoss</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05634">PDF</a><br/><b>Abstract: </b>A tantalizing conjecture in discrete mathematics is the one of Koml\'os,
suggesting that for any vectors $\mathbf{a}_1,\ldots,\mathbf{a}_n \in B_2^m$
there exist signs $x_1, \dots, x_n \in \{ -1,1\}$ so that $\|\sum_{i=1}^n
x_i\mathbf{a}_i\|_\infty \le O(1)$. It is a natural extension to ask what
$\ell_q$-norm bound to expect for $\mathbf{a}_1,\ldots,\mathbf{a}_n \in B_p^m$.
We prove that, for $2 \le p \le q \le \infty$, such vectors admit fractional
colorings $x_1, \dots, x_n \in [-1,1]$ with a linear number of $\pm 1$
coordinates so that $\|\sum_{i=1}^n x_i\mathbf{a}_i\|_q \leq
O(\sqrt{\min(p,\log(2m/n))}) \cdot n^{1/2-1/p+ 1/q}$, and that one can obtain a
full coloring at the expense of another factor of $\frac{1}{1/2 - 1/p + 1/q}$.
In particular, for $p \in (2,3]$ we can indeed find signs $\mathbf{x} \in \{
-1,1\}^n$ with $\|\sum_{i=1}^n x_i\mathbf{a}_i\|_\infty \le O(n^{1/2-1/p} \cdot
\frac{1}{p-2})$. Our result generalizes Spencer's theorem, for which $p = q =
\infty$, and is tight for $m = n$.
</p>
<p>Additionally, we prove that for any fixed constant $\delta&gt;0$, in a centrally
symmetric body $K \subseteq \mathbb{R}^n$ with measure at least $e^{-\delta n}$
one can find such a fractional coloring in polynomial time. Previously this was
known only for a small enough constant -- indeed in this regime classical
nonconstructive arguments do not apply and partial colorings of the form
$\mathbf{x} \in \{ -1,0,1\}^n$ do not necessarily exist.
</p></div>
    </summary>
    <updated>2020-07-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.05580</id>
    <link href="http://arxiv.org/abs/2007.05580" rel="alternate" type="text/html"/>
    <title>A Strong XOR Lemma for Randomized Query Complexity</title>
    <feedworld_mtime>1594684800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brody:Joshua.html">Joshua Brody</a>, Jae Tak Kim, Peem Lerdputtipongporn, Hari Srinivasulu <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05580">PDF</a><br/><b>Abstract: </b>We give a strong direct sum theorem for computing $xor \circ g$.
Specifically, we show that for every function g and every $k\geq 2$, the
randomized query complexity of computing the xor of k instances of g satisfies
$\overline{R}_\eps(xor\circ g) = \Theta(k \overline{R}_{\eps/k}(g))$. This
matches the naive success amplification upper bound and answers a conjecture of
Blais and Brody (CCC19).
</p>
<p>As a consequence of our strong direct sum theorem, we give a total function g
for which $R(xor \circ g) = \Theta(k \log(k)\cdot R(g))$, answering an open
question from Ben-David et al.(arxiv:<a href="http://export.arxiv.org/abs/2006.10957">2006.10957v1</a>).
</p></div>
    </summary>
    <updated>2020-07-14T01:20:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.05557</id>
    <link href="http://arxiv.org/abs/2007.05557" rel="alternate" type="text/html"/>
    <title>Learning Entangled Single-Sample Gaussians in the Subset-of-Signals Model</title>
    <feedworld_mtime>1594684800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liang:Yingyu.html">Yingyu Liang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yuan:Hui.html">Hui Yuan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05557">PDF</a><br/><b>Abstract: </b>In the setting of entangled single-sample distributions, the goal is to
estimate some common parameter shared by a family of $n$ distributions, given
one single sample from each distribution. This paper studies mean estimation
for entangled single-sample Gaussians that have a common mean but different
unknown variances. We propose the subset-of-signals model where an unknown
subset of $m$ variances are bounded by 1 while there are no assumptions on the
other variances. In this model, we analyze a simple and natural method based on
iteratively averaging the truncated samples, and show that the method achieves
error $O \left(\frac{\sqrt{n\ln n}}{m}\right)$ with high probability when
$m=\Omega(\sqrt{n\ln n})$, matching existing bounds for this range of $m$. We
further prove lower bounds, showing that the error is
$\Omega\left(\left(\frac{n}{m^4}\right)^{1/2}\right)$ when $m$ is between
$\Omega(\ln n)$ and $O(n^{1/4})$, and the error is
$\Omega\left(\left(\frac{n}{m^4}\right)^{1/6}\right)$ when $m$ is between
$\Omega(n^{1/4})$ and $O(n^{1 - \epsilon})$ for an arbitrarily small
$\epsilon&gt;0$, improving existing lower bounds and extending to a wider range of
$m$.
</p></div>
    </summary>
    <updated>2020-07-14T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-14T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=3843</id>
    <link href="https://francisbach.com/gradient-descent-for-wide-two-layer-neural-networks-implicit-bias/" rel="alternate" type="text/html"/>
    <title>Gradient descent for wide two-layer neural networks – II: Generalization and implicit bias</title>
    <summary>In this blog post, we continue our investigation of gradient flows for wide two-layer “relu” neural networks. In the previous post, Francis explained that under suitable assumptions these dynamics converge to global minimizers of the training objective. Today, we build on this to understand qualitative aspects of the predictor learnt by such neural networks. The...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">In this blog post, we continue our investigation of gradient flows for wide two-layer “relu” neural networks. In the <a href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/">previous post</a>, Francis explained that under suitable assumptions these dynamics converge to global minimizers of the training objective. Today, we build on this to understand qualitative aspects of the predictor learnt by such neural networks. The content is mostly based on our recent joint work [<a href="https://arxiv.org/pdf/2002.04486.pdf">1</a>].</p>



<h2>1. Generalization with weight decay regularization</h2>



<p class="justify-text">Let us start our journey with the comfortable case where the training objective includes an explicit <em>weight decay</em> regularization (i.e. \(\ell_2\)-regularization on the parameters). Using the notations of the previous post, this consists in the following objective function on the space of probability measures on \(\mathbb{R}^{d+1}\):  $$ \underbrace{R\Big(\int_{\mathbb{R}^{d+1}} \Phi(w)d\mu(w)\Big)}_{\text{Data fitting term}} + \underbrace{\frac{\lambda}{2} \int_{\mathbb{R}^{d+1}} \Vert w \Vert^2_2d\mu(w)}_{\text{Regularization}} \tag{1}$$ where \(R\) is the loss and \(\lambda&gt;0\) is the regularization strength. Remember that a  neural network of finite width with \(m\) neurons is recovered with an empirical measure \(\mu = \frac1m \sum_{j=1}^m\delta_{w_j}\), in which case this regularization is proportional to the sum of the squares of all the parameters \(\frac{\lambda}{2m}\sum_{j=1}^m \Vert w_j\Vert^2_2\).</p>



<p class="justify-text"><strong>Variation norm.</strong> In the previous post, we have seen that the Wasserstein gradient flow of this objective function — an idealization of the gradient descent training dynamics in the large width limit — converges to a global minimizer \(\mu^*\) when initialized properly. An example of an admissible initialization is the hidden weights \(b_j\) distributed according to the uniform distribution \(\tau\) on the unit sphere \(\mathbb{S}^{d-1}\subset \mathbb{R}^d\) and the output weights \(a_j\) uniform in \(\{-1,1\}\). What does this minimizer look like in predictor space when the objective function is as in Eq. (1) ? </p>



<p class="justify-text">To answer this question, we define for a predictor \(h:\mathbb{R}^d\to \mathbb{R}\), the quantity $$ \Vert h \Vert_{\mathcal{F}_1} := \min_{\mu \in \mathcal{P}(\mathbb{R}^{d+1})} \frac{1}{2} \int_{\mathbb{R}^{d+1}} \Vert w\Vert^2_2 d\mu(w) \quad \text{s.t.}\quad h = \int_{\mathbb{R}^{d+1}} \Phi(w)d\mu(w).\tag{2} $$ As the notation suggests, \(\Vert \cdot \Vert_{\mathcal{F}_1}\) is a norm in the space of predictors. It is known as the <em>variation norm</em> [<a href="http://jmlr.org/papers/volume18/14-546/14-546.pdf">2</a>, <a href="https://www.cs.cas.cz/~vera/publications/journals/I3Edin.pdf">3</a>]. We call \(\mathcal{F}_1\) the space of functions with finite norm, which is a (non-Hilbertian) Banach space. By construction, the learnt predictor \(h^* = \int \Phi(w)d\mu^*(w)\) is a minimizer of the \(\mathcal{F}_1\)-regularized regression: $$ \min_{h:\mathbb{R}^d\to \mathbb{R}} R(h) + \lambda \Vert h \Vert_{\mathcal{F}_1} \tag{3}.$$ This \(\mathcal{F}_1\)-norm shares similarity with \(\ell_1\) regularization [<a href="https://arxiv.org/pdf/1412.6614.pdf">4</a>]. To see this, observe that the “magnitude” \(\vert a\vert \Vert b\Vert_2\) of a relu function \(x\mapsto a(b^\top x)_+\) with parameter \(w=(a,b)\) equals \(\Vert w\Vert^2_2/2\) if \(\vert a\vert = \Vert b\Vert_2\) and is smaller otherwise. Thus parameterizing the relus by their direction \(\theta = b/\Vert b\Vert_2\) and optimizing over their signed magnitude \(r(\theta) = a\Vert b\Vert_2\)  we have $$ \Vert h \Vert_{\mathcal{F}_1} = \inf_{r:\mathbb{S}^{d-1}\to \mathbb{R}} \int_{\mathbb{S}^{d-1}} \vert r(\theta)\vert d\tau(\theta) \quad \text{s.t.}\quad h(x) = \int _{\mathbb{S}^{d-1}} r(\theta) (\theta^\top x)_+ d\tau(\theta).\tag{4}$$</p>



<p class="justify-text"><strong>Conjugate RKHS norm.</strong> The regression in the space \(\mathcal{F}_1\) is best understood when compared with the regression obtained by only training the output weights. We consider the same training dynamics with weight decay except that we fix the hidden weights to their initial value, where they are distributed according to the uniform distribution \(\tau\) on the sphere. In that case, the Wasserstein gradient flow also converges to the solution of a regularized regression as in Eq. (3) – this is in fact a convex problem –  but the regularizing norm is different and now defined as $$ \Vert h \Vert_{\mathcal{F}_2}^2 := \min_{r:\mathbb{S}^{d-1}\to \mathbb{R}} \int_{\mathbb{S}^{d-1}} \vert r(\theta)\vert^2 d\tau(\theta) \quad \text{s.t.}\quad h(x) = \int _{\mathbb{S}^{d-1}} r(\theta) (\theta^\top x)_+ d\tau(\theta).$$ We call \(\mathcal{F}_2\) the set of functions with finite norm. It can be shown to be a <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">Reproducing Kernel Hilbert Space</a> (RKHS), with kernel  $$ K(x,x’) = \int_{\mathbb{S}^{d-1}} (\theta^\top x)_+ (\theta^\top x’)_+ d\tau(\theta),$$ which has a closed form expression [<a href="https://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf">5</a>]. In this context, taking a finite width neural network corresponds to a random feature approximation of the kernel [<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&amp;rep=rep1&amp;type=pdf">6</a>, <a href="https://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines">7</a>].</p>



<p class="justify-text">Let us informally compare the properties of these spaces \(\mathcal{F}_1\) and \(\mathcal{F}_2\) (see [<a href="https://arxiv.org/abs/1412.8690">2</a>] for details):</p>



<ul class="justify-text"><li><strong>Approximation power.</strong> In high dimension, only very smooth functions have small \(\mathcal{F}_2\)-norm (in rough terms, the \((d+1)/2\) first derivatives should be small). In contrast, there exists non-smooth functions with small \(\mathcal{F}_1\)-norm, an example being the relu function \(x\mapsto (\theta^\top x)_+\). Remarkably, if we define \(f(x)=g(Ux)\) where \(U\) is an orthogonal projection then \(\Vert f\Vert_{\mathcal{F}_1} \leq  \Vert g\Vert_{\mathcal{F}_2}\). This shows in particular that \(\mathcal{F}_1\) contains \(\mathcal{F}_2\) and that \(\mathcal{F}_1\) is <em>adaptive</em> to lower dimensional structures.</li><li><strong>Statistical complexity.</strong> It could be feared that the good approximation properties of \(\mathcal{F}_1\) come at the price of being “too large” as a hypothesis space, making it difficult to estimate a predictor in \(\mathcal{F}_1\) from few samples. But, as measured by bounds on their Rademacher complexities, the unit ball of \(\mathcal{F}_1\) is only \(O(\sqrt{d})\) larger than that of \(\mathcal{F}_2\). By going from \(\mathcal{F}_2\) to \(\mathcal{F}_1\), we thus add some nicely structured predictors to our hypothesis space, but not too much garbage that could fit unstructured noise.</li><li><strong>Generalization guarantees.</strong> By combining the two previous points, it is possible to prove that supervised learning in \(\mathcal{F}_1\) breaks the curse of dimensionality when the output depends on a lower dimensional projection of the input: the required number of training samples only depends mildly on the dimension \(d\).</li><li><strong>Optimization guarantees.</strong> However \(\mathcal{F}_1\) has a strong drawback : there is no known algorithm that solves the problem of Eq. (3) in polynomial time. On practical problems, gradient descent seems to behave well, but in general only qualitative results such as presented in the previous post are known. In contrast, various provably efficient algorithms can solve regression in \(\mathcal{F}_2\), which is a classical kernel ridge regression problem [Chap. 14.4.3, <a href="https://doc.lagout.org/science/Artificial%20Intelligence/Machine%20learning/Machine%20Learning_%20A%20Probabilistic%20Perspective%20%5BMurphy%202012-08-24%5D.pdf">8</a>].</li></ul>



<p class="justify-text">In the plot below, we compare the predictor learnt by gradient descent for a 2-D regression with the square loss and weight decay, after training (a) both layers — which is regression in \(\mathcal{F}_1\) — or (b) just the output layer — which is regression in \(\mathcal{F}_2\). This already illustrates some distinctive features of both spaces, although the differences become more stringent in higher dimensions. In particular, observe that in (a) the predictor is the combination of few relu functions, which illustrates  the sparsifying effect of the \(L^1\)-norm in Eq. (4). To simplify notations, we do not include a bias/intercept in the formulas but our numerical experiments include it, so in this plot the input is of the form \(x=(x_1,x_2,1)\) and \(d=3\).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4231" height="293" src="https://francisbach.com/wp-content/uploads/2020/07/regularized-2.png" width="564"/>Predictor learnt by the gradient flow on the square loss with weight decay, when training (a) both layers (b) only the output layer. The markers indicate the location of the training samples  \(x_i\). <a href="https://github.com/lchizat/2020_implicitbias_blog/blob/master/exp_weightdecay.jl">[code]</a></figure></div>



<p class="justify-text">The qualitative picture is quite clear so far, but something is a bit unsettling: weight decay is often not needed to obtain a good performance in practice. Our line of reasoning however completely falls apart without such a regularization: if the objective function depends on the predictor only via its values on the training set, being a minimizer does not guarantee anything about generalization outside of the training set (remember that wide relu neural networks are <a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">universal approximators</a>). Why does it still work in the unregularized case? There must be something in the algorithm…</p>



<h2>2. Implicit bias: linear classification</h2>



<p class="justify-text">This something is called the <em>implicit bias</em> : when there are several minimizers, the optimization algorithm makes a specific choice. In the unregularized case, the “quality” of this choice is a crucial property of an algorithm; much more crucial than, say, its convergence speed on the training objective. To gradually build our intuition of the implicit bias of gradient flows, let us put neural networks aside for a moment and consider, following Soudry, Hoffer, Nacson, Gunasekar and Srebro [<a href="http://www.jmlr.org/papers/volume19/18-188/18-188.pdf">9</a>], a linear classification task.</p>



<p class="justify-text"><strong>Gradient flow of the smooth-margin.</strong> Let \((x_i,y_i)_{i=1}^n\) be a training set of \(n\) pairs of inputs \(x_i\in \mathbb{R}^d\) and outputs \(y_i\in \{-1,1\}\). Let us choose the exponential loss. The analysis that follows also apply to the logistic loss (which is the same as the cross-entropy loss after a sigmoid non-linearity) because only the “tail” of the loss matters, but it is more straightforward with the exponential loss. In order to give a natural “scale” to the problem, we  renormalize the empirical risk by taking minus its logarithm and consider the concave objective $$ F_\beta(a) = -\frac{1}{\beta}\log\Big( \frac1n \sum_{i=1}^n \exp(-\beta y_i \ x_i^\top a) \Big).\tag{5}$$ </p>



<p class="justify-text">Here \(\beta&gt;0\) is a parameter that will be useful in a moment. For now, we take \(\beta=1\) and we note \(F(a)=F_1(a)\).  In this context, the <em>margin</em> of a vector \(a\in \mathbb{R}^d\) is the quantity \(\min_{i} y_i\ x_i^\top a\) which quantifies how far this linear predictor is from making a wrong prediction on the training set.  </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4201" height="329" src="https://francisbach.com/wp-content/uploads/2020/07/max_margin-3.png" width="386"/>The margin of the linear predictor \(x \mapsto a^\top x\) with parameters \(a \in \mathbb{S}^d\) is the smallest distance of a training point to the decision boundary. We show here the max-margin predictor.</figure></div>



<p class="justify-text">Obtained via simple manipulations, the inequalities  $$ \min_i y_i\ x_i^\top a \leq F_\beta(a) \leq \min_i y_i\ x_i^\top a +\frac{\log(n)}{\beta}, \tag{6}$$ suggest to call \(F_\beta\) the <em>smooth-margin</em> because, well, it is smooth and converges to the margin \(F_\infty(a) := \min_i y_i x_i^\top a\) as \(\beta\to \infty\). Let us look at the gradient flow in the ascent direction that maximizes the smooth-margin: $$ a'(t) = \nabla F(a(t))$$ initialized with \(a(0)=0\) (here the initialization does not matter so much). The path followed by this gradient flow is exactly the same as the gradient flow on the empirical risk: taking the logarithm only changes the time parameterization or, in practice, the step-size.</p>



<p class="justify-text"><strong>Convergence to the max-margin.</strong> Assume that the data set is linearly separable, which means that the \(\ell_2\)-max-margin $$ \gamma := \max_{\Vert a\Vert_2 \leq 1} \min_i y_i x_i^\top a$$ is positive. In this case \(F\) is unbounded (indeed \(\lim_{\alpha \to \infty} F(\alpha a) =\infty\) whenever \(a\)  has a positive margin) and thus \(a(t)\) diverges. This is not an issue as such, since for classification, only the sign of the prediction matters.  This just means that the relevant question is not “where does \(a(t)\) converge?” but rather “towards which direction does it diverge?”. In other words, we are interested in the limit of \(\bar a(t):= a(t)/\Vert a(t)\Vert_2\) (in convex analysis, this is called the <em>cosmic limit</em> of \(a(t)\) [Chap. 3, <a href="https://www.springer.com/gp/book/9783540627722">10</a>], isn’t it beautiful ?).</p>



<p class="justify-text">It can be shown with a duality argument (see the end of the blog post) that \(\Vert \nabla F(a)\Vert_2\geq \gamma\) for all \(a\in \mathbb{R}^d\). By the inequality of Eq. (6) and the gradient flow property \(\frac{d}{dt}F(a(t))=\Vert \nabla F(a(t))\Vert_2^2\), it follows $$\begin{aligned}\min_i y_i x_i^\top a(t) \geq F(a(t)) \  – \log(n) \geq \gamma \int_0^t \Vert \nabla F(a(s))\Vert_2ds -\log (n).\end{aligned}$$  For \(t&gt; \log(n)/\gamma^2\), this lower bound is positive. We can then divide the left-hand side by \(\Vert a(t)\Vert_2\) and the right-hand side by the larger quantity \(\int_0^t \Vert\nabla F(a(s))\Vert_2ds\), and we get $$\min_i y_i x_i^\top \bar a(t) \geq \gamma -\frac{\log(n)}{\int_0^t \Vert\nabla F(a(s))\Vert_2ds} \geq \gamma -\frac{\log(n)}{\gamma t}.$$ This shows that the margin of \(\bar a(t) := a(t)/\Vert a(t)\Vert_2\) converges to the \(\ell_2\)-max-margin at a rate \(\log(n)/\gamma t\). That’s it, the implicit bias of this gradient flow is exposed!</p>



<p class="justify-text"><strong>Stability to step-size choice.</strong> To translate this argument to discrete time, we need decreasing step-sizes of order \(1/\sqrt{t}\) which deteriorates the convergence rate to \(\tilde O(1/\sqrt{t})\), see [<a href="https://arxiv.org/pdf/1802.08246.pdf">11</a>]. In [<a href="https://arxiv.org/pdf/2002.04486.pdf">1</a>], we proposed a different proof strategy (based on an online optimization interpretation of \(\bar a(t)\), as below) which recovers the same convergence rate \(O(1/\sqrt{t})\) with <em>exponentially larger</em> step-sizes. This suggests that these diverging trajectories are extremely robust to the choice of step-size.</p>



<p class="justify-text"><strong>Illustration. </strong>In the figure below, we plot on the left the evolution of the parameter \(a(t)\) and on the right the predictor \(x\mapsto (x,1)^\top a(t)\) with \(x\in \mathbb{R}^2\). In parameter space, we apply the hyperbolic tangent to the radial component which allows to easily visualize diverging trajectories. This way, the unit sphere represents the <em>horizon</em> of \(\mathbb{R}^d\), i.e., the set of directions at infinity [Chap. 3 in <a href="https://www.springer.com/gp/book/9783540627722">9</a>]. We will use the same convention in the other plots below.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4106" height="288" src="https://francisbach.com/wp-content/uploads/2020/07/linear.gif" width="586"/>Implicit bias of gradient descent for a linear classification task with the exponential loss: (left) parameter space, (right) predictor space.</figure></div>



<h2>3. Implicit bias:  training only the output layer</h2>



<p class="justify-text">Despite its apparently restrictive setting, the previous result already tells us something about wide neural networks. Consider the situation touched upon earlier where we only train the output weights \(a_j\) and the hidden weights \(b_j\) are initialized uniformly at random on the sphere. This corresponds to learning a linear classifier on top of the random feature \([(b_j^\top x)_+]_{j=1}^m\). </p>



<p class="justify-text">As we have just shown, if the training set is separable, the normalized gradient flow of the unregularized exponential loss (or logistic loss) converges to a solution to  $$ \max_{\Vert a\Vert_2 \leq 1}\min_i y_i \sum_{j=1}^m  a_j (b_j^\top x_i)_+.$$ </p>



<p class="justify-text">This is a random feature approximation for the unregularized kernel support vector machine problem in the RKHS \(\mathcal{F}_2\), which is recovered in the large width limit \(m\to \infty\):  $$\max_{\Vert h\Vert_{\mathcal{F}_2}\leq 1} \min_i y_i h(x_i).$$ Notice that if \(m\) is large enough, the linear separability assumption is not even needed anymore, because any training set is separable in \(\mathcal{F}_2\) (at least if all \(x_i\)s are distinct and if we include the bias/intercept).</p>



<p class="justify-text"><strong>Illustration.</strong> In the animation below, we plot on the left the evolution of the parameters and on the right the predictor for a 2-D classification task. In parameter space, each particle represents a neuron: their direction is fixed and their weight is learnt (do not pay attention to the colors yet). As above, the unit sphere is at infinity and the particles diverge. In predictor space, +/- are the training samples of both classes, the color shows the predictor and the black line is the decision boundary. The fact that the predictor has a smooth decision boundary is in accordance with the properties of \(\mathcal{F}_2\) given above. </p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img alt="" class="wp-image-4192" src="https://francisbach.com/wp-content/uploads/2020/07/film_output_comp.gif"/>Gradient descent on the output layer of a two-layer relu neural network with the exponential loss: (left) parameter space, (right) predictor space. <a href="https://github.com/lchizat/2020_implicitbias_blog/blob/master/exp_output.jl">[code]</a></figure></div>



<h2>4. Implicit bias: 2-homogeneous linear classifiers</h2>



<p class="justify-text">Although the analyses where neural networks behave like kernel methods are pleasant for us theoreticians because we are in conquered territory, they miss essential aspects of neural networks such as their adaptivity and their ability to learn a representation. Let us see if we can characterize the implicit bias of the gradient flow of the unregularized exponential loss when training <em>both</em> layers of the neural network.</p>



<p class="justify-text"><strong>A 2-homogeneous linear model.</strong> From an optimization point of view, an important property of two layer relu neural networks is that \(\Phi(\alpha w)= \alpha^2 \Phi(w)\) for all \(\alpha&gt;0\), i.e., they are positively 2-homogeneous in the training parameters. In contrast, a linear model is 1-homogeneous in the parameter. This seemingly little difference leads to drastic changes in the gradient flow dynamics. </p>



<p class="justify-text">Let us again build our intuition with a simplified model that captures key aspects of the dynamics, namely the linear classification setting of above. This time, we take any initialization \(r(0)\in \mathbb{R}^d\) with positive entries and the gradient flow in the ascent direction of the function \( F(r\odot r)\) where \(\odot\) is the pointwise product between two vectors and \(F\) is defined in Eq. (5). This is just a trick to obtain a 2-homogeneous parameterization of a linear model. This gradient flow satisfies $$ r'(t) = 2 r(t)\odot \nabla F(r(t)\odot r(t)).$$ </p>



<p class="justify-text"><strong>Normalized dynamics.</strong> Let us define \(\bar a(t):=(r(t)\odot r(t))/\Vert r(t)\Vert_2^2\) the normalized predictor associated to our dynamics which, by definition, belongs to the simplex \(\Delta_d\), i.e., the set of nonnegative vectors in \(\mathbb{R}^d\) that sum to one. Using the fact that \(\nabla F(\beta a) = \nabla F_\beta (a)\) for all \(\beta&gt;0\), we obtain $$\begin{aligned} \bar a'(t) &amp;= 2\frac{r(t)\odot r'(t)}{\Vert r(t)\Vert_2^2} -2 (r(t)^\top r'(t))\frac{r(t)\odot r(t)}{\Vert r(t)\Vert_2^4}\\ &amp;=4\bar a(t) \nabla F_{\Vert r(t)\Vert_2^2}(\bar a(t))\ – \alpha(t) \bar a(t)\end{aligned}$$ where \(\alpha(t)\) is the scalar such that \(\sum_{i=1}^d a’_i(t) =0\). Online optimization experts might have recognized that this is (continuous time) <em>online mirror ascent in the simplex</em> for the sequence of smooth-margin functions \(F_{\Vert r(t)\Vert_2^2}\). Notice in particular the multiplicative updates: they correspond to the entropy mirror function, and they are particularly well suited for optimization in the high dimensional simplex [Chap.4, <a href="https://arxiv.org/pdf/1405.4980.pdf">12</a>].</p>



<p>What do we learn from this reformulation? </p>



<ul class="justify-text"><li>We can prove (by similar means) that if the data set is linearly separable then \(\Vert r(t)\Vert_2^2\) diverges. So the sequence of functions \(F_{\Vert r\Vert_2^2}\) converges to the margin \(F_\infty\) which means that \(\bar a(t)\) just ends up optimizing the function \(F_\infty\). As a consequence, we have $$\lim_{t\to \infty} y_i x_i^\top \bar a(t) = \max_{a\in \Delta_d} \min_{i} y_i x_i^\top a.$$ This exposes another implicit bias of gradient flow. Notice the key difference with the implicit bias obtained with a linear parameterization: we obtain here the \(\ell_1\)-max-margin (over classifiers with non-negative entries) instead of the \(\ell_2\)-max-margin.  </li><li>Beyond exposing the implicit bias, this reformulation shows that \(\bar a(t)\) implicitly optimizes a sequence of smooth objectives which converge to the margin \(F_\infty\). Unknowingly, we have recovered the well-principled optimization method that consists in approximating a non-smooth objective with smooth functions [<a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">13</a>].</li><li>While the conclusion above was only formal, this point of view leads to rigorous proofs of convergence and convergence rates in discrete time in \(\tilde O(1/\sqrt{t})\) with a step-size in \(O(1/\sqrt{t})\), by  exploiting tools from online optimization, see [<a href="https://arxiv.org/pdf/2002.04486.pdf">1</a>].</li></ul>



<h2>5. Implicit bias: fully trained 2-layer neural networks</h2>



<p class="justify-text">Once again this argument about linear predictors applies to neural networks: if we train both layers but only the magnitude of the hidden weights and not their direction, then this is equivalent to learning a 2-homogeneous linear model on top of the random feature \([  a_j(0) (x_i^\top b_j(0))_+]_{j=1}^m\). If each feature appears twice with opposite signs — which is essentially the case in the large width limit — then the simplex constraint can be equivalently replaced by an \(\ell_1\)-norm constraint on the weights. Recalling the definition of the \(\mathcal{F}_1\)-norm from Eq. (4), we thus obtain that, in the infinite-width limit, the normalized predictor converges to a solution to $$ \max_{\Vert h\Vert_{\mathcal{F}_1} \leq 1} \min_i y_i h(x_i).$$</p>



<p class="justify-text">This result is correct, but it is not relevant. In contrast to functions in \(\mathcal{F}_2\), functions in \(\mathcal{F}_1\) <em>can not</em> in general be approximated with few <em>random</em> features in high dimension. In fact, lower bounds that are exponential in the dimension exist in certain settings [Sec. X, <a href="http://www.stat.yale.edu/~arb4/publications_files/UniversalApproximationBoundsForSuperpositionsOfASigmoidalFunction.pdf">14</a>]. They can be approximated with a small number of features but those need to be data-dependent: in that sense, it is necessary to learn a representation – here,  a distribution over the hidden weights — in order to learn in \(\mathcal{F}_1\). </p>



<p class="justify-text">This raises the following question: do we obtain the same implicit bias when training both layers of the neural network ? In the following result which is the main theorem of our paper [<a href="https://arxiv.org/abs/2002.04486">1</a>], we answer by the affirmative.</p>



<p class="justify-text"><strong>Theorem</strong> (C. and Bach [<a href="https://arxiv.org/abs/2002.04486">1</a>], informal). Assume that for some \(\sigma&gt;0\), the hidden weights \(b_j\) are initialized uniformly on the sphere of radius \(\sigma\) and the output weights \(a_j\) are uniform in \(\{-\sigma,\sigma\}\). Let \(\mu_t\) be the Wasserstein gradient flow for the unregularized exponential loss and \(h_t = \int \Phi(w)d\mu_t(w)\) be the corresponding dynamics in predictor space. Under some technical assumptions, the normalized predictor \(h_t/\Vert h_t\Vert_{\mathcal{F}_1}\) converges to a solution to the \(\mathcal{F}_1\)-max-margin problem: $$\max_{\Vert h\Vert_{\mathcal{F}_1} \leq 1} \min_i y_i h(x_i).$$</p>



<p class="justify-text">Giving an idea of proof would be a bit too technical for this blog post, but let us make some remarks:</p>



<ul class="justify-text"><li>The strength of this result is that although this dynamics could get trapped towards limit directions which are not optimal, this choice of initialization allows to avoid them all and to only converge to <em>global</em> minimizers of this max-margin problem. The principle behind this is similar to the global convergence result in the previous blog post. </li><li>The fact that optimizing on the direction of the hidden weights is compatible with the global optimality conditions of the \(\mathcal{F}_1\)-max-margin problem is very specific to the structure of positively 2-homogeneous problems, and should not be taken for granted for other architectures of neural networks.</li><li>Although at a formal level this result works for any initialization that is diverse enough (such as the standard Gaussian initialization), the initialization proposed here yields dynamics with a better behavior for relu networks: by initializing the hidden and output weights with equal norms – a property preserved by the dynamics – we avoid some instabilities in the gradient. Also notice that this result applies to any scale \(\sigma\) of the initialization (we’ll see an intriguing consequence of this in the next section).</li></ul>



<p class="justify-text"><strong>Illustration.</strong> In the figure below, we plot the training dynamics when both layers are trained. In parameter space (left), each particle represents a neuron: its position is \(\vert a_j\vert b_j\) and its color depends on the sign of \(a_j\).  Here again the unit sphere is at infinity. The inactive neurons at the bottom correspond to those with a bias that is “too negative” at initialization. We observe that all the other neurons gather into few clusters: this is the sparsifying effect of the \(L^1\)-norm in Eq. (4). In predictor space, we obtain a polygonal classifier, as expected for a \(\mathcal{F}_1\)-max-margin classifier. See the paper [<a href="https://arxiv.org/pdf/2002.04486.pdf">1</a>] for experiments that illustrate the strengths of this classifier in terms of generalization.</p>



<div class="wp-block-image"><figure class="aligncenter size-large"><img alt="" class="wp-image-4194" src="https://francisbach.com/wp-content/uploads/2020/07/film_both_comp.gif"/>Training both layers of a wide relu neural network with the exponential loss: (left) space of parameters, (right) space of predictors. <a href="https://github.com/lchizat/2020_implicitbias_blog/blob/master/exp_bothlayers.jl">[code]</a></figure></div>



<h2>6. Lazy regime and the neural tangent kernel</h2>



<p class="justify-text">This blog post would not be complete without mentioning the <em>lazy regime</em>. This is yet another kind of implicit bias which, in our context, takes place when at initialization the weights have a large magnitude and the step-size is small. It was first exhibited in [<a href="https://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf">15</a>] for deep neural networks.</p>



<p class="justify-text"><strong>Lazy training via scaling.</strong> This phenomenon is in fact very general so let us present it with a generic parametric predictor \(h(W)\) with differential \(Dh(W)\). We introduce a scaling factor \(\alpha&gt;0\) and look at the gradient flow of \(F(W) := R(\alpha h(W))\) with a step-size \(1/\alpha^2\), that is $$ W'(t) = \ – \frac{1}{\alpha}Dh(W(t))^\top \nabla R(\alpha h(W(t))),$$ with initialization \(W(0)\). In terms of the predictor \(\alpha h(W)\), this yields the dynamics $$\frac{d}{dt} \alpha h(W(t)) = \ – Dh(W(t))Dh(W(t))^\top \nabla R(\alpha h(W(t)).$$ </p>



<p class="justify-text">Lazy training [<a href="https://arxiv.org/pdf/1812.07956.pdf">16</a>] happens when we take \(\alpha\) large while making sure that \(\alpha h(W(0))\) stays bounded. In this case, we see that the parameters change at a rate \(O(1/\alpha)\), while the predictor changes at a rate independent of \(\alpha\). In the limit of  a large \(\alpha\), the parameters only move infinitesimally on any given time interval, while the predictor still makes significant progress, hence the name <em>lazy training</em>.</p>



<p class="justify-text"><strong>Equivalent linear model.</strong> Since the parameters hardly move, we can replace the map \(h\) by its linearization \(\bar h(w) = h(w(0))+Dh(w(0))(w-w(0))\) if we assume that \(Dh(w(0))\neq 0\). This means that the dynamics essentially follows the gradient flow of $$ R\big ( \alpha h(W(0)) + \alpha Dh(W(0))(W-W(0)) \big)$$ which is convex as soon as \(R\) is convex.</p>



<p class="justify-text">If this linearized objective admits a minimizer that is not too far away from \(W(0)\), then \(W(t)\) converges to this minimizer. If in contrast all  the minimizers are too far away (think of the exponential loss), then the lazy regime is just a transient regime in the early phase of training.  Of course, all these behaviors can be quantified and made more precise, because this puts us back to the realm of linear models. But what all of this has to do with two-layer neural networks? As it happens, this scale factor appears implicit in various situations for these models; let us detail two of them. </p>



<p class="justify-text"><strong>Neural networks with \(1/\sqrt{m}\) scaling.</strong> For two-layer neural networks, lazy training occurs if we define \(h = \frac{1}{\sqrt{m}} \sum_{i=1}^n \Phi(w_i)\) instead of \(h=\frac{1}{m} \sum_{i=1}^m \Phi(w_i)\) before taking the infinite width limit. Indeed:</p>



<ul class="justify-text"><li>This induces a scaling factor \(\alpha = \sqrt{m} \to \infty\) compared to \(1/m\) which, as we have already seen, is the “correct” scaling that leads to a non-degenerate dynamics in parameter space as \(m\) increases. </li><li>Moreover, by the central limit theorem,  \(\frac{1}{\sqrt{m}} \sum_{i=1}^m \Phi(w_i(0)) = O(1)\) for typical random initializations of the parameters. So the initial predictor stays bounded.</li></ul>



<p class="justify-text">To take the Wasserstein gradient flow limit, the step-size was of order \(m\) (see previous blog post). So here we should take a step-size of order \(m/\alpha^2 = 1\). With such a step-size, all the conditions for lazy training are gathered when \(m\) is large. Intuitively, each neuron only moves infinitesimally, but they collectively produce a significant movement in predictor space.</p>



<p class="justify-text"><strong>Neural networks with large initialization.</strong> Coming back to our scaling in \(1/m\) and our Wasserstein gradient flow that is obtained in the large width limit, there is another way to enter the lazy regime: by increasing the variance of the initialization. </p>



<p class="justify-text">To see this, assume that \(h\) is a positively \(p\)-homogeneous parametric predictor, which means that \(h(\sigma W)=\sigma^p h(W)\) for all \(\sigma&gt;0\) and some \(p&gt;1\) (remember that this is true with \(p=2\) for our two-layer relu neural network). Take an initialization of the form \(W(0) = \sigma \bar W_0\) where \(h(\bar W_0)=0\) and \(\sigma&gt;0\) and consider the gradient flow of \(R(h(W))\) with step-size \(\sigma^{2-2p}\).   By defining \(\bar W(t) = W(t)/\sigma\) and using the fact that the differential of a p-homogeneous function <a href="https://en.wikipedia.org/wiki/Homogeneous_function#Positive_homogeneity">is (p-1)-homogeneous</a>, we have, on the one hand $$ \bar W'(t) = -\sigma^{1-2p} Dh(\bar W(t))^\top \nabla R(\sigma^p h(\bar W(t))), $$ and on the other hand $$\frac{d}{dt} \sigma^p h(\bar W(t)) =\  – Dh(\bar W(t))Dh(\bar W(t))^\top \nabla R(\sigma^p h(\bar W(t))).$$ So in terms of the dynamics \(\bar W(t)\), the situation is quite similar than if we had a scaling factor \(\alpha=\sigma^p\): as the magnitude \(\sigma\) of the initialization increases, we enter the lazy regime, provided the step-size is of order \(\sigma^{2-2p}\).</p>



<p class="justify-text"><strong>Neural tangent kernel. </strong>What does the lazy regime tell us about the learnt predictor for two-layer neural networks? Assuming for simplicity that the predictor at initialization is \(0\), this regime amounts to learning a linear model on top of the feature \([(b_j^\top x)_+]_{j=1}^m\) — the derivative with respect to the output weights — concatenated with the feature \([x a_j 1_{b_j^\top x &gt; 0} ]_{i=1}^m\)  — the derivative with respect to the input weights. Compared to training only the output layer, this simply adds some features. </p>



<p class="justify-text">Assume for concreteness, that at initialization the hidden weights \(b_j\) are uniform on a sphere of large radius \(\sigma&gt;0\) and the output weights are uniform on \(\{-\kappa\sigma, \kappa\sigma\}\) where \(\kappa\geq 0\). In the large width and large \(\sigma\) limit we enter the lazy regime which amounts to learning in a RKHS — let us call it \(\mathcal{F}_{2,\kappa}\) — that is slightly different from \(\mathcal{F}_2 = \mathcal{F}_{2,0}\), with kernel $$ K_\kappa(x,x’) = \int_{\mathbb{S}^{d-1}} (\theta^\top x)_+ (\theta^\top x’)_+d\tau(\theta) + \kappa^2 \int_{\mathbb{S}^{d-1}} (x^\top x’) 1_{\theta^\top x &gt; 0}1_{\theta^\top x’ &gt; 0}d\tau(\theta). $$</p>



<p class="justify-text">This kernel is called the Neural Tangent Kernel [<a href="https://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf">15</a>] and the properties of the associated RKHS have been studied in [<a href="https://arxiv.org/pdf/1904.12191.pdf">17</a>, <a href="http://papers.nips.cc/paper/9449-on-the-inductive-bias-of-neural-tangent-kernels.pdf">18</a>], where it is shown to include functions that are slightly less smooth than those of \(\mathcal{F}_2\) when \(\kappa\) increases. This is illustrated in the plot below, obtained by training a wide neural network with \(\sigma\) large (to reach the lazy regime) on the square loss, and various values of \(\kappa\).</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-4213" height="287" src="https://francisbach.com/wp-content/uploads/2020/07/interp-4.png" width="574"/>1-D regression with a wide two-layer relu neural network (gradient descent on square loss, in the lazy regime) with 4 training samples (black dots). At initialization, output weights have \(\kappa\) times the (large) magnitude of the hidden weights. This implicitly solves kernel ridgeless regression for a kernel that depends on \(\kappa\). <a href="https://github.com/lchizat/2020_implicitbias_blog/blob/master/exp_NTK.jl">[code]</a></figure></div>



<p class="justify-text"><strong>Two implicit biases in one shot.</strong> The attentive reader might have notice that for large initialization scale \(\sigma\gg 1\), when training both layers on the unregularized exponential loss, two of our analyses apply:  lazy training — that leads to a max-margin predictor in \(\mathcal{F}_{2,\kappa}\) — and the asymptotic implicit bias — that leads to a max-margin predictor in \(\mathcal{F}_{1}\). Is there a contradiction ?</p>



<p class="justify-text">No : since the minimizers of this loss are at infinity, the lazy regime is just a transient phase, so we will observe both implicit bias along the training dynamics! Take a look at the video below: we observe that in early phases of training, the neurons do not move while learning a smooth classifier — this is the lazy regime. In later stages of training, the neurons start moving and the predictor converges to a \(\mathcal{F}_1\)-max-margin classifier as stated by the main theorem. The predictor jitters a little bit during training because I have chosen rather aggressive step-sizes.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large"><img alt="" class="wp-image-4219" src="https://francisbach.com/wp-content/uploads/2020/07/film_lazy2sparse_ns_comp-1.gif"/>Training both layers with gradient descent for the unregularized logistic loss. The only difference with the previous video is that at initialization the variance \(\sigma^2\) is larger and the step-size smaller \(\approx \sigma^{-2}\). First the network learns a classifier in the lazy regime (a kernel max-margin classifier) and eventually converges to the \(\mathcal{F}_1\)-max-margin.</figure></div>



<h2>Discussion</h2>



<p class="justify-text">In this blog post, I described how analyses of the training dynamics can help us understand the properties of the predictor learnt by neural networks even in the absence of an explicit regularization. Already for the simplest algorithm one can think of — gradient descent — we have found a variety of behaviors depending on the loss, the initialization or the step-size. </p>



<p class="justify-text">To achieve this description, the infinite width limit was of great help. It allowed us to obtain synthetic and simple characterizations of the learnt predictor, that lead to generalization bounds. Yet, there are many interesting non-asymptotic effects caused by having a finite width.  In that sense, we were only concerned with the end of the curve of double descent [<a href="https://www.pnas.org/content/pnas/116/32/15849.full.pdf">18</a>].</p>



<h2>References</h2>



<p class="justify-text">[1] Lénaïc Chizat, Francis Bach. <a href="https://arxiv.org/pdf/2002.04486.pdf">Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss.</a> <em>To appear in Conference On Learning Theory</em>, 2020.<br/>[2] Francis Bach. <a href="http://jmlr.org/papers/volume18/14-546/14-546.pdf">Breaking the curse of dimensionality with convex neural networks.</a> <em>The Journal of Machine Learning Research</em>, <em>18</em>(1), 629-681, 2017.<br/>[3]  Vera Kurková, Marcello Sanguineti. <a href="https://www.cs.cas.cz/~vera/publications/journals/I3Edin.pdf">Bounds on rates of variable-basis and neural-network approximation.</a> <em>IEEE Transactions on Information Theory</em>, 47(6):2659-2665, 2001.  <br/>[4] Behnam Neyshabur, Ryota Tomioka, Nathan Srebro. <a href="https://arxiv.org/pdf/1412.6614.pdf">In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning.</a> <em>ICLR (Workshop)</em>. 2015.<br/>[5] Youngmin Cho, Lawrence K. SAUL.  <a href="https://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf">Kernel methods for deep learning.</a> <em>Advances in neural information processing systems</em>. 342-350, 2009.<br/>[6] Radford M. Neal. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&amp;rep=rep1&amp;type=pdf"><em>Bayesian learning for neural networks</em>.</a> Springer Science &amp; Business Media, 2012.<br/>[7] Ali Rahimi, Benjamin Recht. <a href="https://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf">Random features for large-scale kernel machines.</a> <em>Advances in neural information processing systems</em>. 1177-1184, 2008.<br/>[8] Kevin P. Murphy. Machine Learning: A Probabilistic Perspective. <em>The MIT Press</em>, 2012<br/>[9] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, Nathan Srebro. <a href="http://www.jmlr.org/papers/volume19/18-188/18-188.pdf">The Implicit Bias of Gradient Descent on Separable Data.</a><em> The Journal of Machine Learning Research</em>, <em>19</em>(1), 2822-2878, 2018.<br/>[10] R. Tyrrell Rockafellar, Roger J-B. Wets. <a href="https://www.springer.com/gp/book/9783540627722"><em>Variational analysis</em>.</a> Springer Science &amp; Business Media, 2009.<br/>[11] Suriya Gunasekar,  Jason D. Lee, Daniel Soudry, Nathan Srebro.  <a href="https://par.nsf.gov/servlets/purl/10107856">Characterizing implicit bias in terms of optimization geometry.</a> <em>International Conference on Machine Learning</em>, 2018.<br/>[12] Sébastien Bubeck. <a href="https://arxiv.org/pdf/1405.4980.pdf">Convex Optimization: Algorithms and Complexity.</a> Foundations and Trends in Machine Learning, 8(3-4):231-357, 2015.<br/>[13] Yuri Nesterov. <a href="https://www.math.ucdavis.edu/~sqma/MAT258A_Files/Nesterov-2005.pdf">Smooth minimization of non-smooth functions.</a> <em>Mathematical programming</em>, 103(1):127-152, 2005.<br/>[14] Anrew R. Barron. <a href="http://www.stat.yale.edu/~arb4/publications_files/UniversalApproximationBoundsForSuperpositionsOfASigmoidalFunction.pdf">Universal approximation bounds for superpositions of a sigmoidal function.</a> <em>IEEE Transactions on Information theory. </em>39(3), 930-945, 1993.<br/>[15] Jacot, Arthur, Franck Gabriel, Clément Hongler. <a href="https://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf">Neural tangent kernel: Convergence and generalization in neural networks.</a> <em>Advances in neural information processing systems.</em> 8571-8580, 2018.<br/>[16] Lénaïc Chizat, Édouard Oyallon, Francis Bach. <a href="https://papers.nips.cc/paper/8559-on-lazy-training-in-differentiable-programming.pdf">On lazy training in differentiable programming.</a> <em>Advances in Neural Information Processing Systems.</em> 2937-2947, 2019.<br/>[17] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, Andrea Montanari. <a href="https://arxiv.org/pdf/1904.12191.pdf">Linearized two-layers neural networks in high dimension.</a> To appear in <em>Annals of Statistics</em>. 2019.<br/>[18] Alberto Bietti, Julien Mairal. <a href="http://papers.nips.cc/paper/9449-on-the-inductive-bias-of-neural-tangent-kernels">On the Inductive Bias of Neural Tangent Kernels</a>. <em>Advances in Neural Information Processing Systems.</em> p. 12893-12904, 2019.<br/>[19] Mikhail Belkin, Daniel Hsu, Siyuan Ma, Soumik Mandal. <a href="https://www.pnas.org/content/pnas/116/32/15849.full.pdf">Reconciling modern machine-learning practice and the classical bias–variance trade-off.</a> <em>Proceedings of the National Academy of Sciences.</em> <em>116</em>(32), 15849-15854, 2019.</p>



<h3>Lower bound on the gradient norm for linear classification with the exponential loss</h3>



<p class="justify-text">In the context of Section 2, we want to prove that \(\Vert \nabla F(a)\Vert_2\geq \gamma\). For this, let \(Z\in \mathbb{R}^{n\times d}\) be the matrix with rows \(y_i x_i\) and let \(\Delta_n\) be the simplex in \(\mathbb{R}^n\). We have by duality $$ \gamma = \max_{\Vert a\Vert_2\leq 1}\min_{p\in \Delta_n} p^\top Z a =   \min_{p\in \Delta_n} \max_{\Vert a\Vert_2\leq 1} a^\top Z^\top p = \min_{p\in \Delta_n} \Vert Z^\top p\Vert_2 .$$  Also, notice that \(\nabla F(a) = Z^\top p\) with \(p_i = \frac{e^{-y_ix_i^\top a}}{\sum_{j=1}^n e^{-y_{j}x_{j}^\top a}}\). Since \(p \in \Delta_n\), we conclude that \(\Vert \nabla F(a)\Vert_2\geq \min_{p\in \Delta_n} \Vert Z^\top p\Vert_2 = \gamma\).</p></div>
    </content>
    <updated>2020-07-13T19:39:11Z</updated>
    <published>2020-07-13T19:39:11Z</published>
    <category term="Machine learning"/>
    <author>
      <name>Lénaïc Chizat</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2020-07-14T11:22:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7770</id>
    <link href="https://windowsontheory.org/2020/07/13/simons-institute-lectures-on-analysis-of-boolean-functions/" rel="alternate" type="text/html"/>
    <title>Simons institute lectures on analysis of Boolean functions</title>
    <summary>(Does it still make sense to blog such announcements or is these days Twitter the only way to go about this? Asking for a friend 🙂 ) Prasad Raghavendra and Avishay Tal have organized a sequence of 6 lectures on some of the exciting recent advances in analysis of Boolean functions. Lecture Series: Advances in […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>(Does it still make sense to blog such announcements or is these days <a href="https://twitter.com/boazbaraktcs/status/1282443765224017920">Twitter</a> the only way to go about this? Asking for a friend <img alt="&#x1F642;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;"/> )</em></p>



<p>Prasad Raghavendra and Avishay Tal have organized a sequence of 6 lectures on some of the exciting recent advances in analysis of Boolean functions. </p>



<p><a href="https://simons.berkeley.edu/events/boolean" rel="noreferrer noopener" target="_blank">Lecture Series: Advances in Boolean Function Analysis</a><br/></p>



<p>The Simons Institute is organizing a series of lectures on Advances in Boolean Function Analysis, that will highlight a few major developments in the area. The series will feature weekly two-hour lectures from July 15th to Aug 18th.  The lectures aim to address both the broad context of the results and their technical details. Though closely related in theme, each lecture will be self-contained.  The schedule is attached below (more info at <a href="https://simons.berkeley.edu/events/boolean" rel="noreferrer noopener" target="_blank">link</a>). </p>



<p>Talks take place on Wednesdays at 10am Pacific time (1pm Eastern). If you can’t catch them live, they will be redcorded.</p>



<p><strong>Zoom Link: </strong><a href="https://berkeley.zoom.us/j/93086371156" rel="noreferrer noopener" target="_blank">https://berkeley.zoom.us/j/93086371156</a><br/></p>



<p><strong><u>Talk Schedule:</u></strong></p>



<p>July 15, Wednesday  10:00am PDT (1pm EDT) <em>Dor Minzer (Institute of Advanced Study)<a href="https://simons.berkeley.edu/events/boolean-1" rel="noreferrer noopener" target="_blank">On the Fourier-Entropy Influence Conjecture</a></em></p>



<p><br/>July 22, Wednesday, 10:00am PDT (1pm EDT) <em>Hao Huang (Emory University) &amp; Avishay Tal (UC Berkeley)<a href="https://simons.berkeley.edu/events/boolean-3" rel="noreferrer noopener" target="_blank">Sensitivity Conjecture and Its Applications</a></em></p>



<p><br/>August 3rd, Monday, 10:00am PDT (1pm EDT)<em>  Shachar Lovett (UC San Diego)<a href="https://simons.berkeley.edu/events/boolean-2" rel="noreferrer noopener" target="_blank">Improved Bounds for the Sunflower Lemma</a></em></p>



<p><br/>August 5, Wednesday, 10:00am PDT (1pm EDT) <em>Ronen Eldan (Weizmann Institute)</em><a href="https://simons.berkeley.edu/events/boolean-4" rel="noreferrer noopener" target="_blank"><em>Concentration on the Boolean Hypercube via Pathwise Stochastic Analysis</em></a></p>



<p><br/>August 12, Wednesday, 10:00am <em>Esty Kelman (Tel Aviv University) <a href="https://simons.berkeley.edu/events/boolean-5" rel="noreferrer noopener" target="_blank">KKL via Random Restrictions</a></em></p>



<p><br/>August 18, Tuesday, 10:00am <em>Pooya Hatami (Ohio State University)<a href="https://simons.berkeley.edu/events/boolean-6" rel="noreferrer noopener" target="_blank">Pseudorandom Generators from Polarizing Random Walks</a></em></p></div>
    </content>
    <updated>2020-07-13T16:18:40Z</updated>
    <published>2020-07-13T16:18:40Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-07-14T11:21:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3507</id>
    <link href="https://agtb.wordpress.com/2020/07/13/adfocs-2020-market-design-and-computational-fair-division/" rel="alternate" type="text/html"/>
    <title>ADFOCS 2020 (Market Design and Computational Fair Division)</title>
    <summary>Via Pieter Kleer: 21st Max Planck Summer School: Advanced Course on the Foundations of Computer Science (ADFOCS 2020) August 24 – 28, 2020 Saarbruecken, Germany THIS IS A VIRTUAL EVENT http://www.mpi-inf.mpg.de/conference/adfocs ————————————————————————————————— About ADFOCS ADFOCS is an international summer school that has been held annually for the last twenty years at the Max Planck Institute […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Via Pieter Kleer:</p>
<hr/>
<p>21st Max Planck Summer School:<br/>
Advanced Course on the Foundations of Computer Science (ADFOCS 2020)</p>
<p>August 24 – 28, 2020</p>
<p>Saarbruecken, Germany</p>
<p>THIS IS A VIRTUAL EVENT<a href="http://www.mpi-inf.mpg.de/conference/adfocs" rel="noopener noreferrer" target="_blank"/></p><a href="http://www.mpi-inf.mpg.de/conference/adfocs" rel="noopener noreferrer" target="_blank">
</a><p><a href="http://www.mpi-inf.mpg.de/conference/adfocs" rel="noopener noreferrer" target="_blank">http://www.mpi-inf.mpg.de/conference/adfocs</a><br/>
—————————————————————————————————</p>
<p><b>About ADFOCS</b><br/>
ADFOCS is an international summer school that has been held annually for the last twenty years at the Max Planck Institute for Informatics (MPII) in Saarbruecken, Germany. It is organized as part of the activities of the MPII, in particular the International Max Planck Research School (IMPRS), MPII’s graduate program. The purpose of this summer school is to introduce young researchers to topics which are the focus of current research in theoretical computer science. We bring together leading researchers in the field and international participants at the graduate level and above. This year’s focus is on:</p>
<p><b>*** Market Design and Computational Fair Division ***</b><br/>
<b>Program</b><br/>
Our invited speakers give five 60-min lectures with subsequent exercise and discussion sessions. These sessions will take place daily from 14:30 to 18:30 UTC+2 (CEST) in the week of August 24-28. On some days there will be a social event after the regular schedule. This year’s speakers are:</p>
<p>* Nicole Immorlica, Microsoft Research Lab, New York City, USA<br/>
* Jugal Garg and Ruta Mehta, University of Illinois at Urbana-Champaign, USA<br/>
<b>Registration</b><br/>
This year registration is free as the event takes place virtually. Nevertheless, registration is MANDATORY and can be done through the website (at the latest August 10)</p>
<p><b>Contact</b><br/>
The homepage of ADFOCS, including forms for registration, can be found at <a href="http://www.mpi-inf.mpg.de/conference/adfocs" rel="noopener noreferrer" target="_blank">http://www.mpi-inf.mpg.de/conference/adfocs</a></p>
<p>If you have further questions, please do not hesitate to contact the ADFOCS team by sending an email to <a href="mailto:adfocs@mpi-inf.mpg.de" rel="noopener" target="_blank">adfocs@mpi-inf.mpg.de</a></p>
<p>Organizers: Cosmina Croitoru, Sandor Kisfaludi-Bak and Pieter Kleer</p></div>
    </content>
    <updated>2020-07-13T01:25:45Z</updated>
    <published>2020-07-13T01:25:45Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>timroughgarden</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2020-07-14T11:20:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.05523</id>
    <link href="http://arxiv.org/abs/2007.05523" rel="alternate" type="text/html"/>
    <title>Local Access to Sparse Connected Subgraphs Via Edge Sampling</title>
    <feedworld_mtime>1594598400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Epstein:Rogers.html">Rogers Epstein</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05523">PDF</a><br/><b>Abstract: </b>We contribute an approach to the problem of locally computing sparse
connected subgraphs of dense graphs. In this setting, given an edge in a
connected graph $G = (V, E)$, an algorithm locally decides its membership in a
sparse connected subgraph $G^* = (V, E^*)$, where $E^* \subseteq E$ and $|E^*|
= o(|E|)$. Such an approach to subgraph construction is useful when dealing
with massive graphs, where reading in the graph's full network description is
impractical.
</p>
<p>While most prior results in this area require assumptions on $G$ or that
$|E'| \le (1+\epsilon)|V|$ for some $\epsilon &gt; 0$, we relax these assumptions.
Given a general graph and a parameter $T$, we provide membership queries to a
subgraph with $O(|V|T)$ edges using $\widetilde{O}(|E|/T)$ probes. This is the
first algorithm to work on general graphs and allow for a tradeoff between its
probe complexity and the number of edges in the resulting subgraph.
</p>
<p>We achieve this result with ideas motivated from edge sparsification
techniques that were previously unused in this problem. We believe these
techniques will motivate new algorithms for this problem and related ones.
Additionally, we describe an efficient method to access any node's neighbor set
in a sparsified version of $G$ where each edge is deleted with some i.i.d.
probability.
</p></div>
    </summary>
    <updated>2020-07-13T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.05469</id>
    <link href="http://arxiv.org/abs/2007.05469" rel="alternate" type="text/html"/>
    <title>Efficient ancilla-free reversible and quantum circuits for the Hidden Weighted Bit function</title>
    <feedworld_mtime>1594598400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bravyi:Sergey.html">Sergey Bravyi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yoder:Theodore_J=.html">Theodore J. Yoder</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maslov:Dmitri.html">Dmitri Maslov</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05469">PDF</a><br/><b>Abstract: </b>The Hidden Weighted Bit function plays an important role in the study of
classical models of computation. A common belief is that this function is
exponentially hard for the implementation by reversible ancilla-free circuits,
even though introducing a small number of ancillae allows a very efficient
implementation. In this paper, we refute the exponential hardness conjecture by
developing a polynomial-size reversible ancilla-free circuit computing the
Hidden Weighted Bit function. Our circuit has size $O(n^{6.42})$, where $n$ is
the number of input bits. We also show that the Hidden Weighted Bit function
can be computed by a quantum ancilla-free circuit of size $O(n^2)$. The
technical tools employed come from a combination of Theoretical Computer
Science (Barrington's theorem) and Physics (simulation of fermionic
Hamiltonians) techniques.
</p></div>
    </summary>
    <updated>2020-07-13T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.05458</id>
    <link href="http://arxiv.org/abs/2007.05458" rel="alternate" type="text/html"/>
    <title>Border rank non-additivity for higher order tensors</title>
    <feedworld_mtime>1594598400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Christandl:Matthias.html">Matthias Christandl</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gesmundo:Fulvio.html">Fulvio Gesmundo</a>, Mateusz Michałek, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zuiddam:Jeroen.html">Jeroen Zuiddam</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05458">PDF</a><br/><b>Abstract: </b>Whereas matrix rank is additive under direct sum, in 1981 Sch\"onhage showed
that one of its generalizations to the tensor setting, tensor border rank, can
be strictly subadditive for tensors of order three. Whether border rank is
additive for higher order tensors has remained open. In this work, we settle
this problem by providing analogs of Sch\"onhage's construction for tensors of
order four and higher. Sch\"onhage's work was motivated by the study of the
computational complexity of matrix multiplication; we discuss implications of
our results for the asymptotic rank of higher order generalizations of the
matrix multiplication tensor.
</p></div>
    </summary>
    <updated>2020-07-13T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.05453</id>
    <link href="http://arxiv.org/abs/2007.05453" rel="alternate" type="text/html"/>
    <title>New Oracle-Efficient Algorithms for Private Synthetic Data Release</title>
    <feedworld_mtime>1594598400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vietri:Giuseppe.html">Giuseppe Vietri</a>, Grace Tian, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bun:Mark.html">Mark Bun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Steinke:Thomas.html">Thomas Steinke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Zhiwei_Steven.html">Zhiwei Steven Wu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05453">PDF</a><br/><b>Abstract: </b>We present three new algorithms for constructing differentially private
synthetic data---a sanitized version of a sensitive dataset that approximately
preserves the answers to a large collection of statistical queries. All three
algorithms are \emph{oracle-efficient} in the sense that they are
computationally efficient when given access to an optimization oracle. Such an
oracle can be implemented using many existing (non-private) optimization tools
such as sophisticated integer program solvers. While the accuracy of the
synthetic data is contingent on the oracle's optimization performance, the
algorithms satisfy differential privacy even in the worst case. For all three
algorithms, we provide theoretical guarantees for both accuracy and privacy.
Through empirical evaluation, we demonstrate that our methods scale well with
both the dimensionality of the data and the number of queries. Compared to the
state-of-the-art method High-Dimensional Matrix Mechanism \cite{McKennaMHM18},
our algorithms provide better accuracy in the large workload and high privacy
regime (corresponding to low privacy loss $\varepsilon$).
</p></div>
    </summary>
    <updated>2020-07-13T23:27:03Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.05346</id>
    <link href="http://arxiv.org/abs/2007.05346" rel="alternate" type="text/html"/>
    <title>Extending Nearly Complete 1-Planar Drawings in Polynomial Time</title>
    <feedworld_mtime>1594598400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Eiben:Eduard.html">Eduard Eiben</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ganian:Robert.html">Robert Ganian</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hamm:Thekla.html">Thekla Hamm</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Klute:Fabian.html">Fabian Klute</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/N=ouml=llenburg:Martin.html">Martin Nöllenburg</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05346">PDF</a><br/><b>Abstract: </b>The problem of extending partial geometric graph representations such as
plane graphs has received considerable attention in recent years. In
particular, given a graph $G$, a connected subgraph $H$ of $G$ and a drawing
$\mathcal{H}$ of $H$, the extension problem asks whether $\mathcal{H}$ can be
extended into a drawing of $G$ while maintaining some desired property of the
drawing (e.g., planarity).
</p>
<p>In their breakthrough result, Angelini et al. [ACM TALG 2015] showed that the
extension problem is polynomial-time solvable when the aim is to preserve
planarity. Very recently we considered this problem for partial 1-planar
drawings [ICALP 2020], which are drawings in the plane that allow each edge to
have at most one crossing. The most important question identified and left open
in that work is whether the problem can be solved in polynomial time when $H$
can be obtained from $G$ by deleting a bounded number of vertices and edges. In
this work, we answer this question positively by providing a constructive
polynomial-time decision algorithm.
</p></div>
    </summary>
    <updated>2020-07-13T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.05316</id>
    <link href="http://arxiv.org/abs/2007.05316" rel="alternate" type="text/html"/>
    <title>On Distributed Listing of Cliques</title>
    <feedworld_mtime>1594598400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Censor=Hillel:Keren.html">Keren Censor-Hillel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gall:Fran=ccedil=ois_Le.html">François Le Gall</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leitersdorf:Dean.html">Dean Leitersdorf</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05316">PDF</a><br/><b>Abstract: </b>We show an $\tilde{O}(n^{p/(p+2)})$-round algorithm in the \congest model for
\emph{listing} of $K_p$ (a clique with $p$ nodes), for all $p =4, p\geq 6$. For
$p = 5$, we show an $\tilde{O}(n^{3/4})$-round algorithm.
</p>
<p>For $p=4$ and $p=5$, our results improve upon the previous state-of-the-art
of $O(n^{5/6+o(1)})$ and $O(n^{21/22+o(1)})$, respectively, by Eden et al.
[DISC 2019]. For all $p\geq 6$, ours is the first sub-linear round algorithm
for $K_p$ listing.
</p>
<p>We leverage the recent expander decomposition algorithm of Chang et al. [SODA
2019] to create clusters with a good mixing time. Three key novelties in our
algorithm are: (1) we carefully iterate our listing process with coupled values
of min-degree within the clusters and arboricity outside the clusters, (2) all
the listing is done within the cluster, which necessitates new techniques for
bringing into the cluster the information about \emph{all} edges that can
potentially form $K_p$ instances with the cluster edges, and (3) within each
cluster we use a sparsity-aware listing algorithm, which is faster than a
general listing algorithm and which we can allow the cluster to use since we
make sure to sparsify the graph as the iterations proceed.
</p>
<p>As a byproduct of our algorithm, we show an \emph{optimal} sparsity-aware
algorithm for $K_p$ listing, which runs in $\tilde{\Theta}(1 + m/n^{1 + 2/p})$
rounds in the \clique model. Previously, Pandurangan et al. [SPAA 2018], Chang
et al. [SODA 2019], and Censor-Hillel et al. [TCS 2020] showed sparsity-aware
algorithms for the case of $p = 3$, yet ours is the first such sparsity aware
algorithm for $p \geq 4$.
</p></div>
    </summary>
    <updated>2020-07-13T23:26:13Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.05246</id>
    <link href="http://arxiv.org/abs/2007.05246" rel="alternate" type="text/html"/>
    <title>Target set selection with maximum activation time</title>
    <feedworld_mtime>1594598400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Lucas Keiler, Carlos Vinicius G. C. Lima, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maia:Ana_Karolinna.html">Ana Karolinna Maia</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sampaio:Rudini.html">Rudini Sampaio</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sau:Ignasi.html">Ignasi Sau</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05246">PDF</a><br/><b>Abstract: </b>A target set selection model is a graph $G$ with a threshold function
$\tau:V\to \mathbb{N}$ upper-bounded by the vertex degree. For a given model, a
set $S_0\subseteq V(G)$ is a target set if $V(G)$ can be partitioned into
non-empty subsets $S_0,S_1,\dotsc,S_t$ such that, for $i \in \{1, \ldots, t\}$,
$S_i$ contains exactly every vertex $v$ having at least $\tau(v)$ neighbors in
$S_0\cup\dots\cup S_{i-1}$. We say that $t$ is the activation time
$t_{\tau}(S_0)$ of the target set $S_0$. The problem of, given such a model,
finding a target set of minimum size has been extensively studied in the
literature. In this article, we investigate its variant, which we call
TSS-time, in which the goal is to find a target set $S_0$ that maximizes
$t_{\tau}(S_0)$. That is, given a graph $G$, a threshold function $\tau$ in
$G$, and an integer $k$, the objective of the TSS-time problem is to decide
whether $G$ contains a target set $S_0$ such that $t_{\tau}(S_0)\geq k$. Let
$\tau^* = \max_{v \in V(G)} \tau(v)$. Our main result is the following
dichotomy about the complexity of TSS-time when $G$ belongs to a minor-closed
graph class ${\cal C}$: if ${\cal C}$ has bounded local treewidth, the problem
is FPT parameterized by $k$ and $\tau^{\star}$; otherwise, it is NP-complete
even for fixed $k=4$ and $\tau^{\star}=2$. We also prove that, with $\tau^*=2$,
the problem is NP-hard in bipartite graphs for fixed $k=5$, and from previous
results we observe that TSS-time is NP-hard in planar graphs and W[1]-hard
parameterized by treewidth. Finally, we present a linear-time algorithm to find
a target set $S_0$ in a given tree maximizing $t_{\tau}(S_0)$.
</p></div>
    </summary>
    <updated>2020-07-13T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.05020</id>
    <link href="http://arxiv.org/abs/2007.05020" rel="alternate" type="text/html"/>
    <title>Dota Underlords game is NP-complete</title>
    <feedworld_mtime>1594598400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Alexander A. Ponomarenko, Dmitry V. Sirotkin <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05020">PDF</a><br/><b>Abstract: </b>In this paper, we demonstrate how the problem of the optimal team choice in
the popular computer game Dota Underlords can be reduced to the problem of
linear integer programming. We propose a model and solve it for the real data.
We also prove that this problem belongs to the NP-complete class and show that
it reduces to the maximum edge weighted clique problem.
</p></div>
    </summary>
    <updated>2020-07-13T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.05014</id>
    <link href="http://arxiv.org/abs/2007.05014" rel="alternate" type="text/html"/>
    <title>Fast Adaptive Non-Monotone Submodular Maximization Subject to a Knapsack Constraint</title>
    <feedworld_mtime>1594598400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Amanatidis:Georgios.html">Georgios Amanatidis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fusco:Federico.html">Federico Fusco</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lazos:Philip.html">Philip Lazos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leonardi:Stefano.html">Stefano Leonardi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reiffenh=auml=user:Rebecca.html">Rebecca Reiffenhäuser</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.05014">PDF</a><br/><b>Abstract: </b>Constrained submodular maximization problems encompass a wide variety of
applications, including personalized recommendation, team formation, and
revenue maximization via viral marketing. The massive instances occurring in
modern day applications can render existing algorithms prohibitively slow,
while frequently, those instances are also inherently stochastic. Focusing on
these challenges, we revisit the classic problem of maximizing a (possibly
non-monotone) submodular function subject to a knapsack constraint. We present
a simple randomized greedy algorithm that achieves a $5.83$ approximation and
runs in $O(n \log n)$ time, i.e., at least a factor $n$ faster than other
state-of-the-art algorithms. The robustness of our approach allows us to
further transfer it to a stochastic version of the problem. There, we obtain a
$9$-approximation to the best adaptive policy, which is the first constant
approximation for non-monotone objectives. Experimental evaluation of our
algorithms showcases their improved performance on real and synthetic data.
</p></div>
    </summary>
    <updated>2020-07-13T23:24:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-127536327837647663</id>
    <link href="https://blog.computationalcomplexity.org/feeds/127536327837647663/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/07/ronald-graham-summary-of-blog-posts-we.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/127536327837647663" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/127536327837647663" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/07/ronald-graham-summary-of-blog-posts-we.html" rel="alternate" type="text/html"/>
    <title>Ronald Graham: A summary of blog Posts We had about his work</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">To Honor Ronald Graham I summarize the blog posts we had about his work.<br/>
<br/>
1) Blog post <a href="https://blog.computationalcomplexity.org/2016/05/new-ramsey-result-that-will-be-hard-to.html">New Ramsey Result that will be hard to verify but Ronald Graham thinks its right which is good enough for me</a>.<br/>
<br/>
Wikipedia (see <a href="https://en.wikipedia.org/wiki/Boolean_Pythagorean_triples_problem">here</a>) says that in the early 1980's (can't Wikipedia be more precise than that?) Ronald Graham conjectured the following:<br/>
<br/>
For all 2-colorings of N, there exists x,y,z all the same color such that (x,y,z) form a Pythagorean triple.<br/>
<br/>
I cannot imagine he did not also conjecture this to be true for all finite colorings.<br/>
<br/>
I suspect that when he conjectured it, the outcomes thought to be likely were:<br/>
<br/>
a) A purely combinatorial (my spell check says that combinatorial  is not a word. Really? It gets 14,000,000 hits) proof. Perhaps a difficult one. (I think Szemeredi's proof of his density theorem is a rather difficult but purely combinatorial proof).<br/>
<br/>
b) A proof that uses advanced mathematics, like Roth's proof of the k=3 case of Sz-density, or Furstenberg's proof of Sz theorem.<br/>
<br/>
c) The question stays open though with some progress over the years, like R(5).<br/>
<br/>
What actually happened was<br/>
<br/>
d) A SAT Solver solves it AND gets exact bounds:<br/>
<br/>
For all 2-colorings of {1,...,7285} there is a mono Pythag triple.<br/>
<br/>
There exists a 2-coloring of {1,...,7284} with no mono Pythag triple.<br/>
<br/>
I wonder if this would have been guessed as the outcome back in the early 1980's.<br/>
<br/>
-------------------------------------------------------------------------------<br/>
2) Blog Post <a href="https://blog.computationalcomplexity.org/2019/05/ronald-grahams-other-large-number-well.html">Ronald Graham's Other Large Number- well it was large in 1964 anyway</a><br/>
<br/>
Let<br/>
<br/>
a(n) = a(n-1) + a(n-2)<br/>
<br/>
I have not given a(0) and a(1). Does there exists rel prime values of a(0) and a(1) such that for all n, a(n) is composite.<br/>
<br/>
In 1964 Ronald Graham showed yes, though the numbers he found (with the help of 1964-style computing) were<br/>
<br/>
a(0) = 1786772701928802632268715130455793<br/>
<br/>
a(1) = 2059683225053915111058164141686995<br/>
<br/>
I suspect it is open to get smaller numbers, though I do not know.<br/>
<br/>
<br/>
------------------------------------------------------------------------------<br/>
3) Blog Post <a href="https://blog.computationalcomplexity.org/2011/12/solution-to-reciprocals-problem.html">Solution to the reciprocals problem</a><br/>
<br/>
Prove or disprove that there exists 10 natural numbers a,...,j such that<br/>
<br/>
2011= a+ ... + j<br/>
1 = 1/a + ... + 1/j<br/>
<br/>
I had pondered putting this on a HS math competition in 2011; however, the committee thought it was too hard. I blogged on the problem asking for solutions, seeing if there was one that a HS student could have gotten. The following post (this one) gave those solutions. My conclusion is that it could have been put on the competition, but its a close call.<br/>
<br/>
All of the answers submitted had some number repeated.<br/>
<br/>
So I wondered if there was a way to do this with distinct a,...,j.<br/>
<br/>
 I was told about Ronald Grahams result:<br/>
<br/>
For all n at least 78, n can be written as the sum of DISTINCT naturals, where the sum of<br/>
the reciprocals is 1.<br/>
<br/>
This is tight: 77 cannot be so written.<br/>
<br/>
Comment on that blog DID include solutions  to my original problem with all distinct numbers<br/>
<br/>
----------------------------------------------------------------------<br/>
4) Blog Post <a href="https://blog.computationalcomplexity.org/2013/04/a-nice-case-of-interdisciplinary.html">A nice case of interdisplanary research</a> tells the story of how the study of history lead to R(5) being determined (see <a href="http://www.cs.umd.edu/~gasarch/BLOGPAPERS/ramseykings.pdf">here</a> for the actual paper on the subject). One of the main players in the story is the mathematician<br/>
<br/>
Alma Grand-Rho.<br/>
<br/>
Note that this is an anagram of<br/>
<br/>
Ronald Graham.<br/>
<br/>
What is the probability that two mathematicians have names that are anagrams. I suspect very small. However, see <a href="https://blog.computationalcomplexity.org/2013/04/post-mortem-on-april-fools-day-joke.html">this</a> blog post to see why the probability is not as small as it might be.<br/>
<br/>
-----------------------------------------------------------------------<br/>
5) Blog Post <a href="https://blog.computationalcomplexity.org/search?q=Meme">Winner of Ramsey Meme Contest</a> This post didn't mention Ronald Graham; however I think he would have liked it.<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/></div>
    </content>
    <updated>2020-07-12T20:18:00Z</updated>
    <published>2020-07-12T20:18:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-07-13T13:52:03Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/07/12/graham-pollak-partitions</id>
    <link href="https://11011110.github.io/blog/2020/07/12/graham-pollak-partitions.html" rel="alternate" type="text/html"/>
    <title>Graham–Pollak partitions</title>
    <summary>As you’ve probably already seen, Ron Graham recently died. I first met him many years ago at Xerox PARC; what I remember from that meeting is this old guy easily beating me at ping-pong, and I was startled to learn (while working to beef up his Wikipedia article after his death) that that was exactly Graham’s first impression of Paul Erdős. We’ve chatted about research, most recently in 2018 in Barbados, but somehow never published anything together; on the other hand, Graham’s work in computational geometry, Ramsey theory, and approximation algorithms has certainly had a strong influence on me. Anyway, as part of the project of improving his Wikipedia article, I put together a separate new article on the Graham–Pollak theorem, the theorem that partitioning the edges of an -vertex complete graph into complete bipartite subgraphs requires at least subgraphs. And while doing that, I started to wonder about what the optimal partitions look like, and how many there are.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>As you’ve probably already seen, Ron Graham recently died. I first met him many years ago at Xerox PARC; what I remember from that meeting is this old guy easily beating me at ping-pong, and I was startled to learn (while working to beef up <a href="https://en.wikipedia.org/wiki/Ronald_Graham">his Wikipedia article</a> after his death) that that was exactly Graham’s first impression of Paul Erdős. We’ve chatted about research, most recently in <a href="https://www.ics.uci.edu/~eppstein/pix/bellairs18/index.html">2018 in Barbados</a>, but somehow never published anything together; on the other hand, Graham’s work in computational geometry, Ramsey theory, and approximation algorithms has certainly had a strong influence on me. Anyway, as part of the project of improving his Wikipedia article, I put together a separate new article on <a href="https://en.wikipedia.org/wiki/Graham%E2%80%93Pollak_theorem">the Graham–Pollak theorem</a>, the theorem that partitioning the edges of an -vertex complete graph into complete bipartite subgraphs requires at least  subgraphs. And while doing that, I started to wonder about what the optimal partitions look like, and how many there are.</p>

<p>In <em>Proofs from THE BOOK</em>, Aigner and Ziegler describe a simple construction for an -subgraph partition: just order the vertices of the complete graph, and make a star connecting each vertex (except the last) to its later neighbors.
But there are a lot more partitions than that. For instance, you can take any rooted binary tree whose leaves are the vertices of the complete graph, and form a partition in which each complete bipartite subgraph connects the left and right descendants of one of the interior nodes of the tree. The ordered star partition is the special case of this where each internal node has one leaf child.</p>

<p style="text-align: center;"><img alt="Graham&#x2013;Pollak partitions from binary trees" src="https://11011110.github.io/blog/assets/2020/graham-pollak-hierarchy.svg"/></p>

<p>Even these are not the only possibilities. For instance, a four-vertex complete graph can be partitioned into  subgraphs in this triskelion pattern:</p>

<p style="text-align: center;"><img alt="Graham&#x2013;Pollak partitions from binary trees" src="https://11011110.github.io/blog/assets/2020/graham-pollak-triskelion.svg"/></p>

<p>More generally, whenever one has a partition of , one can form a partition of a larger complete graph by partitioning its vertices into  subsets, applying the partition of  to the edges that go from one subset to another, and then recursively partitioning the edges within each subset. This is already enough to show that there is a rapidly growing number of these partitions, but not enough to count them more precisely.</p>

<p>This still leaves many questions. How many Graham–Pollak partitions does  have, as a function of ? How complicated can they be? If we define a state space whose states are Graham–Pollak partitions, and whose state transitions correspond to re-partitioning the subgraph formed by two of the complete bipartite graphs, is it connected? Can a graph traversal of this state space list all the Graham–Pollak partitions faster than a brute force search? What does a random partition look like?</p>

<p>It’s too bad Ron’s no longer around to help answer some of them.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/104503441875881282">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-07-12T15:46:00Z</updated>
    <published>2020-07-12T15:46:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-07-12T23:20:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/104</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/104" rel="alternate" type="text/html"/>
    <title>TR20-104 |  On Counting $t$-Cliques Mod 2 | 

	Oded Goldreich</title>
    <summary>For a constant integer $t$, we consider the problem of counting the number of $t$-cliques $\bmod 2$ in a given graph. 
We show that this problem is not easier than determining whether a given graph contains a $t$-clique, and present a simple worst-case to average-case reduction for it. The reduction runs in linear time when graphs are presented by their adjacency matrices, and average-case is with respect to the uniform distribution over graphs with a given number of vertices.</summary>
    <updated>2020-07-12T15:14:08Z</updated>
    <published>2020-07-12T15:14:08Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-07-14T11:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/103</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/103" rel="alternate" type="text/html"/>
    <title>TR20-103 |  One-Tape Turing Machine and Branching Program Lower Bounds for MCSP | 

	Mahdi Cheraghchi, 

	Shuichi Hirahara, 

	Dimitrios Myrisiotis, 

	Yuichi Yoshida</title>
    <summary>For a size parameter $s\colon\mathbb{N}\to\mathbb{N}$, the Minimum Circuit Size Problem (denoted by ${\rm MCSP}[s(n)]$) is the problem of deciding whether the minimum circuit size of a given function $f \colon \{0,1\}^n \to \{0,1\}$ (represented by a string of length $N := 2^n$) is at most a threshold $s(n)$. A recent line of work exhibited ``hardness magnification'' phenomena for MCSP: A very weak lower bound for MCSP implies a breakthrough result in complexity theory. For example, McKay, Murray, and Williams (STOC 2019) implicitly showed that, for some constant $\mu_1 &gt; 0$, if ${\rm MCSP}[2^{\mu_1\cdot n}]$ cannot be computed by a one-tape Turing machine (with an additional one-way read-only input tape) running in time $N^{1.01}$, then ${\rm P}\neq{\rm NP}$.
    
    In this paper, we present the following new lower bounds against one-tape Turing machines and branching programs:
    \begin{enumerate}
        \item  A randomized two-sided error one-tape Turing machine (with an additional one-way read-only input tape) cannot compute ${\rm MCSP}[2^{\mu_2\cdot n}]$ in time $N^{1.99}$, for some constant $\mu_2 &gt; \mu_1$.  
        \item A non-deterministic (or parity) branching program of size $o(N^{1.5}/\log N)$ cannot compute MKTP, which is a time-bounded Kolmogorov complexity analogue of MCSP. This is shown by directly applying the Nechiporuk method to MKTP, which previously appeared to be difficult.
    \end{enumerate}
    These results are the first non-trivial lower bounds for MCSP and MKTP against one-tape Turing machines and non-deterministic branching programs, and essentially match the best-known lower bounds for any explicit functions against these computational models.
    
    The first result is based on recent constructions of pseudorandom generators for read-once oblivious branching programs (ROBPs) and combinatorial rectangles (Forbes and Kelley, FOCS 2018; Viola 2019). En route, we obtain several related results:
    \begin{enumerate}
        \item There exists a (local) hitting set generator with seed length $\widetilde{O}(\sqrt{N})$ secure against read-once polynomial-size non-deterministic branching programs on $N$-bit inputs.
        \item Any read-once co-non-deterministic branching program computing MCSP must have size at least $2^{\widetilde{\Omega}(N)}$.
    \end{enumerate}</summary>
    <updated>2020-07-11T05:31:21Z</updated>
    <published>2020-07-11T05:31:21Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-07-14T11:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2020/07/11/2020-virtual-telluride-neuromorphic-cognition-engineering-workshop/</id>
    <link href="https://cstheory-events.org/2020/07/11/2020-virtual-telluride-neuromorphic-cognition-engineering-workshop/" rel="alternate" type="text/html"/>
    <title>2020 Virtual Telluride Neuromorphic Cognition Engineering Workshop</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">July 27-31, 2020 Telluride CO (virtual) https://sites.google.com/view/telluride2020/home We are happy to announce a Virtual Telluride Neuromorphic Cognition Engineering Workshop 2020 (https://tellurideneuromorphic.org/) this year in replacement of our usual Workshop in Telluride. The workshop will take place from July 27 to July 31 (8am to 10am PDT, or 17:00 to 19:00 CET). The format will be … <a class="more-link" href="https://cstheory-events.org/2020/07/11/2020-virtual-telluride-neuromorphic-cognition-engineering-workshop/">Continue reading <span class="screen-reader-text">2020 Virtual Telluride Neuromorphic Cognition Engineering Workshop</span></a></div>
    </summary>
    <updated>2020-07-11T05:05:01Z</updated>
    <published>2020-07-11T05:05:01Z</published>
    <category term="workshop"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2020-07-14T11:21:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2020/07/11/international-conference-on-neuromorphic-systems/</id>
    <link href="https://cstheory-events.org/2020/07/11/international-conference-on-neuromorphic-systems/" rel="alternate" type="text/html"/>
    <title>International Conference on Neuromorphic Systems</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">July 28-30, 2020 Oak Ridge National Laboratory (virtual) https://icons.ornl.gov ICONS 2020 will be held as a virtual conference. The goal of this conference is to bring together leading researchers in neuromorphic computing to present new research, develop new collaborations, and provide a forum to publish work in this area. Our focus will be on architectures, … <a class="more-link" href="https://cstheory-events.org/2020/07/11/international-conference-on-neuromorphic-systems/">Continue reading <span class="screen-reader-text">International Conference on Neuromorphic Systems</span></a></div>
    </summary>
    <updated>2020-07-11T05:04:39Z</updated>
    <published>2020-07-11T05:04:39Z</published>
    <category term="conference"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2020-07-14T11:21:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2020/07/11/neuromorphic-computing-opportunities-challenges-and-perspectives/</id>
    <link href="https://cstheory-events.org/2020/07/11/neuromorphic-computing-opportunities-challenges-and-perspectives/" rel="alternate" type="text/html"/>
    <title>Neuromorphic Computing: Opportunities, Challenges, and Perspectives</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">July 19, 2020 Virtual https://teuscher-lab.com/dac2020_neuromorphic_workshop/program/ The objective of this workshop is to bring together researchers from multiple disciplines, ranging from physical to biological sciences, to discuss the most promising approaches and overarching goals of neuromorphic computing technologies and paradigms that have the potential to drastically improve conventional approaches. The neuromorphic computing workshop aims to establish … <a class="more-link" href="https://cstheory-events.org/2020/07/11/neuromorphic-computing-opportunities-challenges-and-perspectives/">Continue reading <span class="screen-reader-text">Neuromorphic Computing: Opportunities, Challenges, and Perspectives</span></a></div>
    </summary>
    <updated>2020-07-11T05:04:18Z</updated>
    <published>2020-07-11T05:04:18Z</published>
    <category term="workshop"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2020-07-14T11:21:34Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2020-07-10-encrypted-blockchain-databases-part-ii/</id>
    <link href="https://decentralizedthoughts.github.io/2020-07-10-encrypted-blockchain-databases-part-ii/" rel="alternate" type="text/html"/>
    <title>Encrypted Blockchain Databases (Part II)</title>
    <summary>In this second part of the series on Encrypted Blockchain Databases, we are going to describe three schemes to store dynamic encrypted multi-maps (EMMs) on blockchains, each of which achieves different tradeoffs between query, add and delete efficiency. A List-Based Scheme (LSX) Recall that a multi-map is a collection of...</summary>
    <updated>2020-07-10T20:25:00Z</updated>
    <published>2020-07-10T20:25:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2020-07-13T23:33:40Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2020-07-10-encrypted-blockchain-databases-part-i/</id>
    <link href="https://decentralizedthoughts.github.io/2020-07-10-encrypted-blockchain-databases-part-i/" rel="alternate" type="text/html"/>
    <title>Encrypted Blockchain Databases (Part I)</title>
    <summary>Blockchain databases are storage systems that combine properties of both blockchains and databases like decentralization, tamper-resistance, low query latency, and support for complex queries. As they gain wider adoption, concerns over the confidentiality of the data they manage will increase. Already, several projects use blockchains to store sensitive data like...</summary>
    <updated>2020-07-10T20:10:00Z</updated>
    <published>2020-07-10T20:10:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2020-07-13T23:33:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7764</id>
    <link href="https://windowsontheory.org/2020/07/10/tcs-book-call-for-github-issues/" rel="alternate" type="text/html"/>
    <title>TCS book: Call for GitHub issues</title>
    <summary>I originally planned this summer to finish the work on my Introduction to Theoretical Computer Science book, and in particular write the two missing chapters on space complexity and interactive proof systems. Needless to say, this summer did not go as planned and I won’t be able to write these chapters. However, I still intend […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I originally planned this summer to finish the work on my <a href="https://introtcs.org/">Introduction to Theoretical Computer Science</a> book, and in particular write the two missing chapters on space complexity and interactive proof systems. Needless to say, this summer did not go as planned and I won’t be able to write these chapters. However, I still intend to go over the existing chapters, fixing typos, adding examples, exercises, and generally making it friendlier to beginning undergraduate students. </p>



<p>Toward this end, I would be grateful for people posting bugs, typos, and suggestions as <a href="https://github.com/boazbk/tcs/issues">GitHub issues</a> (I currently have 267 closed and 14 open issues which I hope to get to soon). Of course, if you are technically inclined and there’s a simple local fix, you can also make  a <a href="https://github.com/boazbk/tcs/pulls">pull request</a>.</p>



<p>Aside from these fixes, I am making two more “global” changes to the book. First, I am adding a “non mathy overview” for each chapter. While some students got a lot from reading the book prior to lectures, others were intimidated by the mathematical notation, and so I hope this more gentle introduction will be helpful. I am also adding more examples &amp; solved exercises toward this end. </p>



<p>Another change is that I now follow the more traditional way of presenting deterministic finite automata <em>before </em>Turing machines – DFAs are still optional and can be skipped without missing anything, but some instructors find them as a good introduction to Turing Machines. Thus the order of presentation of materials in the book is roughly as follows:<br/></p>



<ol><li><strong>Introduction, representing objects as strings</strong> –  Representing numbers, lists, etc. Specifying computational tasks as functions mapping binary strings to binary strings,  Cantor’s theorem.</li><li><strong>Finite functions and Boolean circuits</strong> – Every function can be computed by some circuit, circuits as straightline programs, representing circuits as strings, universal circuit evaluator, counting lower bound.</li><li><strong>Computing on unbounded inputs</strong> – DFAs (optional), Turing Machines, equivalence between Turing machines, RAM machines and programming languages, λ calculus (optional), cellular automata (optional)</li><li><strong>Uncomputability</strong> – Universal Turing machine, Halting problem, reductions, Rice’s Theorem. Optional: Gödel’s incompleteness theorem, uncomputability of quantified arithmetic statements, context free grammars.</li><li><strong>Efficient computation</strong> – Modeling running time, time hierarchy theorem,  <strong>P</strong> and <strong>EXP</strong></li><li><strong>NP and NP completeness</strong> – Polynomial-time reductions, Cook-Levin Theorem (using circuits), definition of <strong>NP</strong> using “proof system”/”verifying algorithms” (no non-deterministic TMs), <strong>NP</strong> completeness, consequences of <strong>P</strong>=<strong>NP</strong>: search to decision, optimal machine learning, etc..</li><li><strong>Randomized computation:</strong> Worst-case randomized computation, defining <strong>BPP</strong>,  Sipser-Gács, does <strong>BPP</strong>=<strong>P</strong>? (a little on derandomization)</li><li><strong>Cryptography:</strong> One time pad, necessity of long keys for information theoretic crypto,  pseudorandom generators and stream ciphers, taste of public key and “magic” (ZKP, FHE, MPC)</li><li><strong>Quantum computing:</strong> Some quantum mechanics background – double slit experiment,  Bell’s inequality. Modeling quantum computation. Bird’s eye view of Shor’s algorithm and quantum Fourier transform.</li></ol>



<p/></div>
    </content>
    <updated>2020-07-10T17:29:02Z</updated>
    <published>2020-07-10T17:29:02Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2020-07-14T11:21:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17276</id>
    <link href="https://rjlipton.wordpress.com/2020/07/10/ron-graham-1935-2020/" rel="alternate" type="text/html"/>
    <title>Ron Graham, 1935–2020</title>
    <summary>Ron Graham passed away, but he lives on… Cropped from tribute by Tom Leighton Ron Graham just passed away Monday at the age of in La Jolla near UCSD. Today Ken and I wish to say a few words about Ron. Tributes are being written as we write, including this from the Simons Foundation. Here […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Ron Graham passed away, but he lives on…</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2020/07/grahamjuggling.jpg"><img alt="" class="alignright wp-image-17278" height="128" src="https://rjlipton.files.wordpress.com/2020/07/grahamjuggling.jpg?w=175&amp;h=128" width="175"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from <a href="https://securityboulevard.com/2020/07/ronald-graham-and-the-magic-of-math/">tribute</a> by Tom Leighton</font></td>
</tr>
</tbody>
</table>
<p>
Ron Graham just passed away Monday at the age of <img alt="{84}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B84%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{84}"/> in La Jolla near UCSD. </p>
<p>
Today Ken and I wish to say a few words about Ron.</p>
<p>
Tributes are being written as we write, including <a href="https://www.simonsfoundation.org/2016/01/11/ronald-graham/">this</a> from the Simons Foundation. Here is the American Mathematical Society <a href="https://www.ams.org/news?news_id=6244">announcement</a>, which we saw first: </p>
<blockquote><p><b> </b> <em> Ron Graham, a leader in discrete mathematics and a former president of both the AMS (1993-1994) and the MAA (2003-2004), died on July 6. He was 84. Graham published more than 350 papers and books with many collaborators, including more than 90 with his wife, Fan Chung, and more than 30 with Paul Erdős. He was known for his infectious enthusiasm, his originality, and his accessibility to anyone who had a mathematics question. </em>
</p></blockquote>
<p/><p>
A <a href="https://www.bradyharanblog.com/blog/the-day-i-met-ron-graham">tribute</a> by Brady Haran embeds several short videos of Ron and his work. Fan’s own <a href="http://www.math.ucsd.edu/~fan/ron/">page</a> for Ron has much more. We have made a collage of images from his life:</p>
<p/><p><br/>
<a href="https://rjlipton.files.wordpress.com/2020/07/rongrahamcollage.jpg"><img alt="" class="aligncenter size-full wp-image-17279" src="https://rjlipton.files.wordpress.com/2020/07/rongrahamcollage.jpg?w=600"/></a></p>
<p/><p><br/>
Ron was special and will be greatly missed by all. We at GLL send our thoughts to his dear wife, Fan. Ken and I knew Ron for many years. Ken knew Ron since a visit to Bell Labs in the 1980s and meeting Fan too at STOC 1990. I knew Ron since I was at Yale in the 1970’s—a long time ago. I recall fondly meeting him for the first time when he was at Bell Labs.</p>
<p>
</p><p/><h2> Some Stories </h2><p/>
<p/><p>
Ken and I thought we would give some personal stories about Graham. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> Ken’s story is told <a href="https://rjlipton.wordpress.com/2013/03/28/happy-100th-birthday-paul-erdos/">here</a>. In breaking a confidence by telling Erdős the secret about Bobby Fischer recounted there, Ken hoped that it would spread behind the scenes to enough people that Fischer would be less blamed for failing to play Anatoly Karpov in 1975. Since Erdős was staying with the Grahams, presumably it would have emerged there. The social excursion during STOC 1990 was a dinner cruise in Baltimore’s harbor. Ron and Fan and Ken found each other right away, and some questions to Ken about chess quickly went to the Fischer topic. At least Ken knows the secret was retold at least once. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> Ron told me once that he was the accountant for Erdős. One of Ron’s jobs was to keep track of the prize money that Erdős owed. Ron would send out the checks to whoever solved the next problem. One of the brilliant insights of Erdős was to make the problems hard, but at least some where solvable. Ron told me that for years no one would actually cash the checks. They would frame them and proudly display them.</p>
<p/><p>
<a href="https://rjlipton.files.wordpress.com/2020/07/check.png"><img alt="" class="aligncenter wp-image-17280" height="102" src="https://rjlipton.files.wordpress.com/2020/07/check.png?w=220&amp;h=102" width="220"/></a></p>
<p/><p><br/>
Ron said that he liked this for the obvious reason—less cash for Erdős to have to pay. But the advent of color xerox machines in the 1970’s changed this. He told me that people began cashing the checks and displaying the color copy. Bummer.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> My first talk at Bell Labs was on my work on the planar separator theorem—joint work with Bob Tarjan. At the beginning of the talk I saw that Ron had a pile of papers on his desk. He was a manager and I guessed he had some paper work to do. I gave my talk. At the end I when up to Ron in the back and he said:</p>
<blockquote><p><b> </b> <em> I did not get any work done. </em>
</p></blockquote>
<p/><p>
I still fondly remember that as high praise. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> Graham loved to do hand stands. I recall walking around Bell Labs one day when out of the blue Ron did a full handstand. He said that he liked to do these on the hand rail of the stairs. The trick he said was: “To not fall down.” </p>
<p/><p><br/>
<a href="https://rjlipton.files.wordpress.com/2020/07/well.jpg"><img alt="" class="aligncenter size-full wp-image-17281" src="https://rjlipton.files.wordpress.com/2020/07/well.jpg?w=600"/></a></p>
<p/><p><br/>
I searched for him doing handstands and found out he and Fan lived in a modern beautiful <a href="http://www.math.ucsd.edu/~fan/home/">house</a>. </p>
<blockquote><p><b> </b> <em> When two mathematicians found a circular home designed by architect Kendrick Bangs Kellogg in La Jolla, they treasured their unique discovery. </em>
</p></blockquote>
<p/><p><br/>
<a href="https://rjlipton.files.wordpress.com/2020/07/home1.jpg"><img alt="" class="aligncenter size-full wp-image-17283" src="https://rjlipton.files.wordpress.com/2020/07/home1.jpg?w=600"/></a></p>
<p>
</p><p/><h2> Fun and Games </h2><p/>
<p/><p>
Ron kept a simply organized <a href="http://www.math.ucsd.edu/~ronspubs/">page</a> of all his papers. They are not sorted by subject or kind, but the titles are so descriptive that you can tell at a glance where the fun is. A number of them are expositions in the popular magazines of the AMS and MAA. </p>
<p>
Among them, we’ll mention this <a href="http://www.math.ucsd.edu/~ronspubs/16_02_insert_and_add.pdf">note</a> from 2016, titled “Inserting Plus Signs and Adding.” It is joint with Steve Butler, who penned his own <a href="https://blog.computationalcomplexity.org/2020/07/reflections-on-ronald-graham-by-steve.html">reminiscence</a> for Lance and Bill’s blog, and Richard Strong. </p>
<p>
Say that a number <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> is “reducible” to a number <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/> in one step (in base <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/>) if there is a way to insert one or more <img alt="{+}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%2B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{+}"/> signs into the base-<img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/> representation of <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> so that the resulting numbers add up to <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/>. For example, 1935 is reducible to 99 via <img alt="{1 + 93 + 5}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%2B+93+%2B+5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 + 93 + 5}"/>. The number 99 reduces only to 18 via <img alt="{9+9}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B9%2B9%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{9+9}"/>, and 18 reduces only to 9, which cannot be reduced further. Thus Ron’s birth year took <img alt="{3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3}"/> reduction steps to become a single digit. However, doing <img alt="{1+9+3+5}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2B9%2B3%2B5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1+9+3+5}"/> gives 18 straightaway and thus saves a step. The paper gives cases where inserting <img alt="{+}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%2B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{+}"/> everywhere is <em>not</em> a quickest way to reduce to a single digit.</p>
<blockquote><p><b>Definition 1</b> <em> For any base <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{b}"/> and number <img alt="{n \geq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cgeq+1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n \geq 1}"/> denoting an input <b>length</b>, not magnitude, define <img alt="{f_b(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_b%28n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f_b(n)}"/> to be the least integer <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{m}"/> such that all base-<img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{b}"/> numbers of length <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/> can be reduced to a single digit within <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{m}"/> steps. </em>
</p></blockquote>
<p/><p>
The question—of a complexity theoretic nature—is:</p>
<blockquote><p><b> </b> <em> Given <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{b}"/>, what is the growth rate of <img alt="{f_b(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_b%28n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f_b(n)}"/> as <img alt="{n \rightarrow \infty}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Crightarrow+%5Cinfty%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n \rightarrow \infty}"/>? </em>
</p></blockquote>
<p/><p>
Here are some possible answers—which would you expect to be correct in the case where <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/> is base 10?</p>
<ul>
<li>
<img alt="{f_{10}(n) = \Theta(\sqrt{n})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_%7B10%7D%28n%29+%3D+%5CTheta%28%5Csqrt%7Bn%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_{10}(n) = \Theta(\sqrt{n})}"/>. <p/>
</li><li>
<img alt="{f_{10}(n) = \Theta(n^{1/10})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_%7B10%7D%28n%29+%3D+%5CTheta%28n%5E%7B1%2F10%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_{10}(n) = \Theta(n^{1/10})}"/>. <p/>
</li><li>
<img alt="{f_{10}(n) = O(\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_%7B10%7D%28n%29+%3D+O%28%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_{10}(n) = O(\log n)}"/>. <p/>
</li><li>
<img alt="{f_{10}(n) = O(\log\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_%7B10%7D%28n%29+%3D+O%28%5Clog%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_{10}(n) = O(\log\log n)}"/>. <p/>
</li><li>
<img alt="{f_{10}(n) = O(\alpha(n))}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_%7B10%7D%28n%29+%3D+O%28%5Calpha%28n%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_{10}(n) = O(\alpha(n))}"/>, where <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/> is the inverse Ackermann <a href="https://en.wikipedia.org/wiki/Ackermann_function#Inverse">function</a>.
</li></ul>
<p>
Your expectation might be wrong—see the paper for the answer and its nifty proof. For a warmup, if you want to answer without looking at the paper, prove that the final reduced digit is the same regardless of the sequence of reductions.</p>
<p>
Ron is also known for very big integers, including <a href="https://en.wikipedia.org/wiki/Graham's_number">one</a> that held the record for largest to appear in a published mathematical proof. You can find it among the above tributes and also on a <a href="https://www.zazzle.com/store/grahamsnumber">T-shirt</a>.  We could also mention his role in the largest <a href="https://news.slashdot.org/story/16/05/30/2241225/computer-generates-largest-math-proof-ever-at-200tb-of-data">proof</a> known to date—at 200 terabytes it almost doubles the size of the <a href="http://tb7.chessok.com/">tables</a> for proving results of seven-piece chess endgames.</p>
<p>
If you desire serious fun, look also to Ron’s books. He wrote several, including co-authoring the nonpareil <a href="https://en.wikipedia.org/wiki/Concrete_Mathematics">textbook</a> <em>Concrete Mathematics</em> with Don Knuth and Oren Patashnik.</p>
<p>
</p><p/><h2> Some Prizes </h2><p/>
<p/><p>
Ron, in the tradition famously followed by Erdős, liked to put <a href="https://www.quantamagazine.org/cash-for-math-the-erdos-prizes-live-on-20170605/">money</a> on problems. A $10 dollar problem was much easier than a $100 one. A $1,000 one is extremely hard, and so on. In Ron’s paper on his favorite <a href="http://www.math.ucsd.edu/~ronspubs/20_02_favorite.pdf">problems</a> he stated this one: </p>
<blockquote><p><b> </b> <em> Let <img alt="{H_{n} = \sum_{j=1}^{n} \frac{1}{j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH_%7Bn%7D+%3D+%5Csum_%7Bj%3D1%7D%5E%7Bn%7D+%5Cfrac%7B1%7D%7Bj%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{H_{n} = \sum_{j=1}^{n} \frac{1}{j}}"/>. Challenge: prove the inequality for all <img alt="{n \ge 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cge+1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n \ge 1}"/>, </em></p><em>
<p align="center"><img alt="\displaystyle  \sum_{d | n} d \le H_{n} + \exp(H_{n})\log(H_{n}). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bd+%7C+n%7D+d+%5Cle+H_%7Bn%7D+%2B+%5Cexp%28H_%7Bn%7D%29%5Clog%28H_%7Bn%7D%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{d | n} d \le H_{n} + \exp(H_{n})\log(H_{n}). "/></p>
</em><p><em>	 </em>
</p></blockquote>
<p/><p>
And he put the prize at $1,000,000. He added:</p>
<blockquote><p><b> </b> <em/></p><em>
</em><p><em>
Why is this reward so outrageous? Because this <a href="https://arxiv.org/pdf/math/0008177.pdf">conjecture</a> is equivalent to the Riemann Hypothesis! A single <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/> violating would imply there are infinitely many zeroes of the Riemann zeta function off the critical line <img alt="{R(z) = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28z%29+%3D+1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{R(z) = 1}"/>. Of course, the $1,000,000 prize is not from me but rather is offered by the Clay Mathematics Institute since the Riemann Hypothesis is one of their six remaining Millennium Prize Problems. We hope to live to see progress in the Challenges and Conjectures mentioned in this note, especially the last one! </em>
</p></blockquote>
<p/><p>
Alas Ron did not get to see this resolved. Nor of course did Erdős, nor may any of us. But Ron is prominently mentioned on another Simons <a href="https://www.simonsfoundation.org/2015/12/10/new-erdos-paper-solves-egyptian-fraction-problem/">page</a> where Erdős lives on, and so may Ron.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Ron died at age <img alt="{84}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B84%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{84}"/>. Perhaps he liked that it is the sum of a twin prime <img alt="{41 + 43}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B41+%2B+43%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{41 + 43}"/>, and also three times a perfect number. We will always remember <img alt="{84}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B84%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{84}"/> because of Ron.  <b>Added 7/10:</b> <img alt="{84}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B84%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{84}"/> is also his current h-index <a href="https://scholar.google.com/citations?user=qrPaF3QAAAAJ&amp;hl=en&amp;oi=sra&amp;fbclid=IwAR2Tx1GkRQ6-6K-hlumvpBqWUku2Msea6_dybwrYK8tVeNUuYOD6czZ24ZY">according to</a> Google Scholar.  HT in <a href="https://rjlipton.wordpress.com/2020/07/10/ron-graham-1935-2020/#comment-111482">comment</a>.</p>
<p>
[some word changes, update about h-index]</p></font></font></div>
    </content>
    <updated>2020-07-10T16:08:04Z</updated>
    <published>2020-07-10T16:08:04Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="News"/>
    <category term="People"/>
    <category term="Teaching"/>
    <category term="trick"/>
    <category term="Fan Chung Graham"/>
    <category term="games"/>
    <category term="in memoriam"/>
    <category term="number theory"/>
    <category term="Paul Erdos"/>
    <category term="Ron Graham"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-07-14T11:20:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/102</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/102" rel="alternate" type="text/html"/>
    <title>TR20-102 |  Notes on Hazard-Free Circuits | 

	Stasys Jukna</title>
    <summary>The problem of constructing hazard-free Boolean circuits (those avoiding electronic glitches) dates back to the 1940s and is an important problem in circuit design. Recently, Ikenmeyer et al. [J. ACM, 66:4 (2019), Article 25] have shown that the hazard-free circuit complexity of any Boolean function $f(x)$ is lower-bounded by the monotone circuit complexity of the monotone Boolean function which accepts an input $x$ iff $f(z)=1$ for some vector $z\leq x$. We give a short and amazingly simple proof of this interesting result. We also show that a circuit is hazard-free if and only if the circuit and its dual produce (purely syntactically) all prime implicants of the functions they compute. This extends a classical result of Eichelberger [IBM J. Res. Develop., 9 (1965)] showing this property for depth-two circuits producing no terms containing a variable together with its negation. Finally, we give a very simple non-monotone Boolean function whose hazard-free circuit complexity is super-polynomially larger than its unrestricted circuit complexity.</summary>
    <updated>2020-07-09T19:01:07Z</updated>
    <published>2020-07-09T19:01:07Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-07-14T11:20:39Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-713901807945793095</id>
    <link href="https://blog.computationalcomplexity.org/feeds/713901807945793095/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/07/reflections-on-ronald-graham-by-steve.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/713901807945793095" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/713901807945793095" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/07/reflections-on-ronald-graham-by-steve.html" rel="alternate" type="text/html"/>
    <title>Reflections on Ronald Graham by Steve Butler</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div>
<i>Ronald Graham passed away on July 6 at the age of 84. We present reflections on Ronald Graham by </i><i>Steve Butler.</i></div>
<div>
<i><br/></i></div>
<hr/>
<div>
<br/></div>
<div>
Getting to work with Ron Graham</div>
<div>
<br/></div>
<div>
Ron Graham has helped transform the mathematics community and in particular been a leader in discrete mathematics for more than 50 years. It is impossible to fully appreciate the breadth of his work in one sitting, and I will not try to do so here. Ron has put his papers online and made them <a href="http://www.math.ucsd.edu/~ronspubs/">freely available</a>, a valuable treasure; and there are still many a hidden gem inside of these papers that are waiting to be picked up, polished, and pushed further.</div>
<div>
<br/></div>
<div>
I want to share about how I got to know and work with Ron. To be fair I knew about Ron long before I ever knew Ron. He was that rare pop-star mathematician who had managed to reach out and become visible outside of the mathematical community. And so as a teenager I read about Ron in a book about Erdos. I thought to myself that this guy sounds really cool and someday I might even get to see him give a talk (if I was lucky).</div>
<div>
<br/></div>
<div>
I went to UC San Diego for graduate school and after a series of near-misses ended up studying under Fan Chung. I passed Ron in the stairwell once, and then also helped them move some furniture between their two adjoining homes (graduate students are great for manual labor). But I became determined to try and find a way to start a conversation with Ron and maybe work up to working on a problem. So I took the usual route: I erased the chalkboards for him.</div>
<div>
<br/></div>
<div>
Before his class on discrete mathematics would start, I would come in and clean the chalkboards making them pristine. It also gave me time to occasionally engage in some idle chat, and he mentioned that his papers list was far from complete. I jumped on it and got to work right away and put his papers online and have been maintaining that list for the last fifteen years. This turned out to be no small feat and required about six months of work.  Many papers had no previous online version, and there were even a few papers that Ron had written that he had forgotten about! But this gave me a reason to come to Ron and talk with him about his various papers and then he would mention some problems he was working on with others and where they were stuck and thought I might give them a try.</div>
<div>
<br/></div>
<div>
So I started to work on these problems and started to make progress. And Ron saw what I was able to do and would send me more problems that fit my abilities and interests, and I would come back and show him partial solutions, or computations, and then he would often times fill in the gaps. He was fun to work with, because we almost always made progress; even when we didn't make progress we still understood things more fully. Little by little our publications (and friendship) grew and we now have 25+ joint publications, and one more book that will be coming out in the next few years about the enumerating juggling patterns.</div>
<div>
<br/></div>
<div>
After all of that though, I discovered something. I could have just gone to Ron's door and knocked and he would have talked to me, and given me problems (though our friendship would not become so deep if I had chosen the forthright method). But almost no graduate students in math were brave enough to do it; they were scared off by his reputation. As a consequence, Ron had far fewer math graduate students than you would expect. (To any math graduate student out there, don't let fear stop you from talking with professors; many of them are much nicer than you think, and the ones that are not nice are probably not that great to work with.)</div>
<div>
<br/></div>
<div>
So one of the most important lessons I learned from Ron was the importance of kindness. Ron was generous and kind to everyone (and I really stress the word everyone) that he met. It didn't matter what walk of life you were in, what age you were, or what level of math (if any) that you knew, he was kind and willing to share his time and talents. He always had something in reach in his bag or pocket that he could pull out and show someone and give them an unexpected sense of wonder.</div>
<div>
<br/></div>
<div>
Richard Hamming <a href="https://www.cs.virginia.edu/~robins/YouAndYourResearch.html">once said</a> "you can be a nice guy or you can be a great scientist", the implication being that you cannot do both. Ron showed that you can be a nice guy and a great scientist. And I believe that a significant portion of his success is owed to his being kind; all of us should learn from his examples and show more kindness towards others.</div>
<div>
<br/></div>
<div>
This is only one of many lessons I learned from Ron. Another thing I learned from Ron is the importance of data. I have seen multiple times when we would work on a problem and generate data resulting in what I thought were hopeless numbers to understand. But Ron looked at that same data and with a short bit of trial and error was able to make a guess of what the general form was. And almost inevitably he would be right! One way that Ron could do this was to start by factoring the values, and if all the prime factors were small he could guess that the expression was some combination of factorials and powers and then start to play with expressions until things worked out. Even when I knew what he did, I still am amazed that he was able to do it.</div>
<div>
<br/></div>
<div>
I will miss Ron, I will never have a collaboration as deep, as meaningful, and as personal. I am better for having worked with him, and learning from him about how to be a better mathematician and a better person.</div>
<div>
<br/></div>
<div>
Thank you, Ron.</div></div>
    </content>
    <updated>2020-07-09T15:57:00Z</updated>
    <published>2020-07-09T15:57:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-07-13T13:52:03Z</updated>
    </source>
  </entry>
</feed>
