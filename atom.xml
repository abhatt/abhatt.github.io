<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-07-08T11:22:28Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/095</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/095" rel="alternate" type="text/html"/>
    <title>TR21-095 |  LEARN-Uniform Circuit Lower Bounds and Provability in Bounded Arithmetic | 

	Valentine Kabanets, 

	Igor Oliveira, 

	Marco Carmosino, 

	Antonina Kolokolova</title>
    <summary>We investigate randomized LEARN-uniformity, which captures the power of randomness and equivalence queries (EQ) in the construction of Boolean circuits for an explicit problem. This is an intermediate notion between P-uniformity and non-uniformity motivated by connections to learning, complexity, and logic.  Building on a number of techniques, we establish the first unconditional lower bounds against LEARN-uniform circuits:

-- For all $c\geq 1$, there is $L \in P$ that is not computable by circuits of size $n \cdot (\log n)^c$ generated in deterministic polynomial time with $o(\log n/\log \log n)$ equivalence queries to $L$. In other words, small circuits for $L$ cannot be efficiently learned using a bounded number of EQs.
-- For each $k\geq 1$, there is $L \in NP$ such that circuits for $L$ of size $O(n^k)$ cannot be learned in deterministic polynomial time with access to $n^{o(1)}$ EQs.
--  For each $k\geq 1$, there is a problem in promise-ZPP that is not in FZPP-uniform $SIZE[n^k]$.
-- Conditional and unconditional lower bounds against LEARN-uniform circuits in the general setting that combines randomized uniformity and access to EQs.

In all these lower bounds, the learning algorithm is allowed to run in arbitrary polynomial time, while the hard problem is computed in some fixed polynomial time.

We employ these results to investigate the (un)provability of non-uniform circuit upper bounds (e.g., Is NP contained in $SIZE[n^3]$?) in theories of bounded arithmetic. Some questions of this form have been addressed in recent papers of Krajicek-Oliveira (2017), Muller-Bydzovsky (2020), and Bydzovsky-Krajicek-Oliveira (2020) via a mixture of techniques from proof theory, complexity theory, and model theory. In contrast, by extracting computational information from proofs via a direct translation to LEARN-uniformity, we establish robust unprovability theorems that unify, simplify, and extend nearly all previous results. In addition, our lower bounds against randomized LEARN-uniformity yield unprovability results for theories augmented with the \emph{dual weak pigeonhole principle}, such as $APC^1$ (Jerabek, 2007), which is known to formalize a large fragment of modern complexity theory.

Finally, we make precise potential limitations of theories of bounded arithmetic such as PV (Cook, 1975) and Jerabek's theory $APC^1$, by showing unconditionally that these theories cannot prove statements like ``$NP\not\subseteq BPP \wedge NP\subset io$-P/poly'', i.e., that NP is uniformly ``hard'' but non-uniformly ``easy'' on infinitely many input lengths. In other words, if we live in such a complexity world, then this cannot be established feasibly.</summary>
    <updated>2021-07-07T21:27:49Z</updated>
    <published>2021-07-07T21:27:49Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-08T11:20:37Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/open-problem-optimal-query-release/</id>
    <link href="https://differentialprivacy.org/open-problem-optimal-query-release/" rel="alternate" type="text/html"/>
    <title>Open Problem - Optimal Query Release for Pure Differential Privacy</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Releasing large sets of statistical queries is a centerpiece of the theory of differential privacy.  Here, we are given a <em>dataset</em> \(x = (x_1,\dots,x_n) \in [T]^n\), and a set of <em>statistical queries</em> \(f_1,\dots,f_k\), where each query is defined by some bounded function \(f_j : [T] \to [-1,1]\), and (abusing notation) is defined as
\[
f_j(x) = \frac{1}{n} \sum_{i=1}^{n} f_j(x_i).
\]
We use \(f(x) = (f_1(x),\dots,f_k(x))\) to denote the vector consisting of the true answers to all these queries.
Our goal is to design an \((\varepsilon, \delta)\)-differentially private algorithm \(M\) that takes a dataset \(x\in [T]^n\) and outputs a random vector \(M(x)\in \mathbb{R}^k\) such that \(\| M(x) - f(x) \|\) is small in expectation for some norm \(\|\cdot\|\). Usually algorithms for this problem also give high probability bounds on the error, but we focus on expected error for simplicity.</p>

<p>This problem has been studied for both <em>pure differential privacy</em> (\(\delta = 0\)) and <em>appproximate differential privacy</em> (\(\delta &gt; 0\)), and for both \(\ell_\infty\)-error
\[
\mathbb{E}( \| M(x) - f(x)\|_{\infty} ) \leq \alpha,
\]
and \(\ell_2\)-error
\[
\mathbb{E}( \| M(x) - f(x)\|_{2} ) \leq \alpha k^{1/2},
\]
giving four variants of the problem.  By now we know tight worst-case upper and lower bounds for two of these variants, and nearly tight bounds (up to logarithmic factors) for a third. The tightest known upper bounds are given in the following table.</p>

<table>
  <tbody>
    <tr>
      <td>Â </td>
      <td>Pure DP</td>
      <td>Approx DP</td>
    </tr>
    <tr>
      <td>\( \ell_2 \)<br/>error</td>
      <td>\( \alpha \lesssim \left(\frac{\log^2 k ~\cdot~ \log^{3/2}T}{\varepsilon n} \right)^{1/2} \) <br/> [<a href="https://arxiv.org/abs/1212.0297">NTZ13</a>]</td>
      <td>\( \alpha \lesssim \left(\frac{\log^{1/2} T}{\varepsilon n} \right)^{1/2} \) <br/> [<a href="https://guyrothblum.files.wordpress.com/2014/11/drv10.pdf">DRV10</a>]</td>
    </tr>
    <tr>
      <td>\( \ell_\infty \)<br/>error</td>
      <td>\( \alpha \lesssim \left(\frac{\log k ~\cdot~ \log T}{\varepsilon n} \right)^{1/3} \)  <br/> [<a href="https://arxiv.org/abs/1109.2229">BLR13</a>]</td>
      <td>\( \alpha \lesssim \left(\frac{\log k ~\cdot~ \log^{1/2} T}{\varepsilon n} \right)^{1/2} \) <br/> [<a href="https://guyrothblum.files.wordpress.com/2014/11/hr10.pdf">HR10</a>, <a href="https://arxiv.org/abs/1107.3731">GRU12</a>]</td>
    </tr>
  </tbody>
</table>

<p>The bounds for approximate DP are known to be tight [<a href="https://arxiv.org/abs/1311.3158">BUV14</a>].  Our two open problems both involve improving the best known upper bounds for pure differential privacy.</p>

<blockquote>
  <p><b>Open Problem 1:</b> What is the best possible \(\ell_\infty\)-error for answering a worst-case set of \(k\) statistical queries over a domain of size \(T\) subject to \((\varepsilon,0)\)-differential privacy?</p>
</blockquote>

<p>We conjecture that the known upper bound in the table can be improved to
\[
\alpha = \left(\frac{\log k \cdot \log T}{\varepsilon n} \right)^{1/2},
\]
which is known to be the best possible [<a href="https://dataspace.princeton.edu/handle/88435/dsp01vq27zn422">Har11</a>, Theorem 4.5.1].</p>

<blockquote>
  <p><b>Open Problem 2:</b> What is the best possible \(\ell_2\)-error for answering a worst-case set of \(k\) statistical queries over a domain of size \(T\) subject to \((\varepsilon,0)\)-differential privacy?</p>
</blockquote>

<p>We conjecture that the upper bound can be improved to
\[
\alpha = \left(\frac{\log T}{\varepsilon n} \right)^{1/2}.
\]
The construction used in [<a href="https://dataspace.princeton.edu/handle/88435/dsp01vq27zn422">Har11</a>, Theorem 4.5.1] can be analyzed to show this bound would be tight. Note, in particular, that this conjecture implies that the tight upper bound has no dependence on the number of queries, similarly to the case of \(\ell_2\) error and approximate DP.</p></div>
    </summary>
    <updated>2021-07-07T17:45:00Z</updated>
    <published>2021-07-07T17:45:00Z</published>
    <author>
      <name>Jonathan Ullman</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2021-07-07T23:07:03Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8892467103290123546</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8892467103290123546/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-1.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8892467103290123546" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8892467103290123546" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/07/would-you-take-this-bet-part-1.html" rel="alternate" type="text/html"/>
    <title>Would you take this bet (Part 1) ?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Â I am going to present a well known paradox (I didn't know it until last week, but the source I read said it was well known) and ask your opinion in this post, and reveal my thoughts in my next post.</p><p>I don't want you to go to the web and find out about it, I want your natural thoughts. Of course I can't stop you, but note that I did not give the name of the paradox.Â </p><p>Here it is:</p><p>I offer you the following bet:Â </p><p>I will flip a coin.</p><p>IfÂ  HEADS you get 1 dollar and we end there.</p><p>If TAILS I flip again</p><p><br/></p><p>IfÂ  HEADS you get 2 dollars and we end there.</p><p>IfÂ  TAILS I flip again</p><p><br/></p><p>If HEADS you get 4 dollars and we end there.</p><p>If TAILS I flip again</p><p><br/></p><p>etc.Â </p><p>1) Expected value:Â </p><p>Prob of getting 1 dollar is 1/2</p><p>Prob of getting 2 dollars is 1/2^2</p><p>Prob of getting 2^2 dollars is 1/2^3</p><p>etc</p><p>Hence the Expected Value isÂ </p><p>1/2 + 1/2 + 1/2 + ... = INFINITY</p><p><br/></p><p>QUESTION: Would you pay $1000 to play the game?</p><p>Leave your answer in the comments and you may say whatever you want as well,</p><p>but I request you don't give the name of the paradox if you know it.Â </p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p></div>
    </content>
    <updated>2021-07-07T00:58:00Z</updated>
    <published>2021-07-07T00:58:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-07-08T03:26:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.02787</id>
    <link href="http://arxiv.org/abs/2107.02787" rel="alternate" type="text/html"/>
    <title>Space Efficient Two-Dimensional Orthogonal Colored Range Counting</title>
    <feedworld_mtime>1625616000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gao:Younan.html">Younan Gao</a>, Meng He <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02787">PDF</a><br/><b>Abstract: </b>In the two-dimensional orthogonal colored range counting problem, we
preprocess a set, $P$, of $n$ colored points on the plane, such that given an
orthogonal query rectangle, the number of distinct colors of the points
contained in this rectangle can be computed efficiently.
</p>
<p>For this problem, we design three new solutions, and the bounds of each can
be expressed in some form of time-space tradeoff.
</p>
<p>By setting appropriate parameter values for these solutions, we can achieve
new specific results with (the space are in words and $\epsilon$ is an
arbitrary constant in $(0,1)$):
</p>
<p>** $O(n\lg^3 n)$ space and $O(\sqrt{n}\lg^{5/2} n \lg \lg n)$ query time;
</p>
<p>** $O(n\lg^2 n)$ space and $O(\sqrt{n}\lg^{4+\epsilon} n)$ query time;
</p>
<p>** $O(n\frac{\lg^2 n}{\lg \lg n})$ space and $O(\sqrt{n}\lg^{5+\epsilon} n)$
query time;
</p>
<p>** $O(n\lg n)$ space and $O(n^{1/2+\epsilon})$ query time.
</p>
<p>A known conditional lower bound to this problem based on Boolean matrix
multiplication gives some evidence on the difficulty of achieving near-linear
space solutions with query time better than $\sqrt{n}$ by more than a
polylogarithmic factor using purely combinatorial approaches. Thus the time and
space bounds in all these results are efficient.
</p>
<p>Previously, among solutions with similar query times, the most
space-efficient solution uses $O(n\lg^4 n)$ space to answer queries in
$O(\sqrt{n}\lg^8 n)$ time (SIAM. J. Comp.~2008).
</p>
<p>Thus the new results listed above all achieve improvements in space
efficiency, while all but the last result achieve speed-up in query time as
well.
</p></div>
    </summary>
    <updated>2021-07-07T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-07-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.02748</id>
    <link href="http://arxiv.org/abs/2107.02748" rel="alternate" type="text/html"/>
    <title>MAJORITY-3SAT (and Related Problems) in Polynomial Time</title>
    <feedworld_mtime>1625616000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Akmal:Shyan.html">Shyan Akmal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Williams:Ryan.html">Ryan Williams</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02748">PDF</a><br/><b>Abstract: </b>Majority-SAT is the problem of determining whether an input $n$-variable
formula in conjunctive normal form (CNF) has at least $2^{n-1}$ satisfying
assignments. Majority-SAT and related problems have been studied extensively in
various AI communities interested in the complexity of probabilistic planning
and inference. Although Majority-SAT has been known to be PP-complete for over
40 years, the complexity of a natural variant has remained open:
Majority-$k$SAT, where the input CNF formula is restricted to have clause width
at most $k$.
</p>
<p>We prove that for every $k$, Majority-$k$SAT is in P. In fact, for any
positive integer $k$ and rational $\rho \in (0,1)$ with bounded denominator, we
give an algorithm that can determine whether a given $k$-CNF has at least $\rho
\cdot 2^n$ satisfying assignments, in deterministic linear time (whereas the
previous best-known algorithm ran in exponential time). Our algorithms have
interesting positive implications for counting complexity and the complexity of
inference, significantly reducing the known complexities of related problems
such as E-MAJ-$k$SAT and MAJ-MAJ-$k$SAT. At the heart of our approach is an
efficient method for solving threshold counting problems by extracting
sunflowers found in the corresponding set system of a $k$-CNF.
</p>
<p>We also show that the tractability of Majority-$k$SAT is somewhat fragile.
For the closely related GtMajority-SAT problem (where we ask whether a given
formula has greater than $2^{n-1}$ satisfying assignments) which is known to be
PP-complete, we show that GtMajority-$k$SAT is in P for $k\le 3$, but becomes
NP-complete for $k\geq 4$. These results are counterintuitive, because the
``natural'' classifications of these problems would have been PP-completeness,
and because there is a stark difference in the complexity of GtMajority-$k$SAT
and Majority-$k$SAT for all $k\ge 4$.
</p></div>
    </summary>
    <updated>2021-07-07T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.02743</id>
    <link href="http://arxiv.org/abs/2107.02743" rel="alternate" type="text/html"/>
    <title>Submodular Order Functions and Assortment Optimization</title>
    <feedworld_mtime>1625616000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Udwani:Rajan.html">Rajan Udwani</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02743">PDF</a><br/><b>Abstract: </b>We define a new class of set functions that in addition to being monotone and
subadditive, also admit a very limited form of submodularity defined over a
permutation of the ground set. We refer to this permutation as a submodular
order. This class of functions includes monotone submodular functions as a
sub-family. To understand the importance of this structure in optimization
problems we consider the problem of maximizing function value under various
types of constraints.
</p>
<p>To demonstrate the modeling power of submodular order functions we show
applications in two different settings. First, we apply our results to the
extensively studied problem of assortment optimization. While the objectives in
assortment optimization are known to be non-submodular (and non-monotone) even
for simple choice models, we show that they are compatible with the notion of
submodular order. Consequently, we obtain new and in some cases the first
constant factor guarantee for constrained assortment optimization in
fundamental choice models. As a second application of submodular order
functions, we show an intriguing algorithmic connection to the maximization of
monotone submodular functions in the streaming model. We recover some best
known guarantees for this problem as a corollary of our results.
</p></div>
    </summary>
    <updated>2021-07-07T22:41:45Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.02738</id>
    <link href="http://arxiv.org/abs/2107.02738" rel="alternate" type="text/html"/>
    <title>Dueling Bandits with Team Comparisons</title>
    <feedworld_mtime>1625616000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cohen:Lee.html">Lee Cohen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schmidt=Kraepelin:Ulrike.html">Ulrike Schmidt-Kraepelin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mansour:Yishay.html">Yishay Mansour</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02738">PDF</a><br/><b>Abstract: </b>We introduce the dueling teams problem, a new online-learning setting in
which the learner observes noisy comparisons of disjoint pairs of $k$-sized
teams from a universe of $n$ players. The goal of the learner is to minimize
the number of duels required to identify, with high probability, a Condorcet
winning team, i.e., a team which wins against any other disjoint team (with
probability at least $1/2$). Noisy comparisons are linked to a total order on
the teams. We formalize our model by building upon the dueling bandits setting
(Yue et al.2012) and provide several algorithms, both for stochastic and
deterministic settings. For the stochastic setting, we provide a reduction to
the classical dueling bandits setting, yielding an algorithm that identifies a
Condorcet winning team within $\mathcal{O}((n + k \log (k)) \frac{\max(\log\log
n, \log k)}{\Delta^2})$ duels, where $\Delta$ is a gap parameter. For
deterministic feedback, we additionally present a gap-independent algorithm
that identifies a Condorcet winning team within $\mathcal{O}(nk\log(k)+k^5)$
duels.
</p></div>
    </summary>
    <updated>2021-07-07T22:56:36Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.02736</id>
    <link href="http://arxiv.org/abs/2107.02736" rel="alternate" type="text/html"/>
    <title>DEANN: Speeding up Kernel-Density Estimation using Approximate Nearest Neighbor Search</title>
    <feedworld_mtime>1625616000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Karppa:Matti.html">Matti Karppa</a>, Martin AumÃ¼ller, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pagh:Rasmus.html">Rasmus Pagh</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02736">PDF</a><br/><b>Abstract: </b>Kernel Density Estimation (KDE) is a nonparametric method for estimating the
shape of a density function, given a set of samples from the distribution.
Recently, locality-sensitive hashing, originally proposed as a tool for nearest
neighbor search, has been shown to enable fast KDE data structures. However,
these approaches do not take advantage of the many other advances that have
been made in algorithms for nearest neighbor algorithms. We present an
algorithm called Density Estimation from Approximate Nearest Neighbors (DEANN)
where we apply Approximate Nearest Neighbor (ANN) algorithms as a black box
subroutine to compute an unbiased KDE. The idea is to find points that have a
large contribution to the KDE using ANN, compute their contribution exactly,
and approximate the remainder with Random Sampling (RS). We present a
theoretical argument that supports the idea that an ANN subroutine can speed up
the evaluation. Furthermore, we provide a C++ implementation with a Python
interface that can make use of an arbitrary ANN implementation as a subroutine
for KDE evaluation. We show empirically that our implementation outperforms
state of the art implementations in all high dimensional datasets we
considered, and matches the performance of RS in cases where the ANN yield no
gains in performance.
</p></div>
    </summary>
    <updated>2021-07-07T22:54:30Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.02666</id>
    <link href="http://arxiv.org/abs/2107.02666" rel="alternate" type="text/html"/>
    <title>Distance Estimation Between Unknown Matrices Using Sublinear Projections on Hamming Cube</title>
    <feedworld_mtime>1625616000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bishnu:Arijit.html">Arijit Bishnu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghosh:Arijit.html">Arijit Ghosh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mishra:Gopinath.html">Gopinath Mishra</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02666">PDF</a><br/><b>Abstract: </b>Using geometric techniques like projection and dimensionality reduction, we
show that there exists a randomized sub-linear time algorithm that can estimate
the Hamming distance between two matrices. Consider two matrices ${\bf A}$ and
${\bf B}$ of size $n \times n$ whose dimensions are known to the algorithm but
the entries are not. The entries of the matrix are real numbers. The access to
any matrix is through an oracle that computes the projection of a row (or a
column) of the matrix on a vector in $\{0,1\}^n$. We call this query oracle to
be an {\sc Inner Product} oracle (shortened as {\sc IP}). We show that our
algorithm returns a $(1\pm \epsilon)$ approximation to ${{\bf D}}_{\bf M} ({\bf
A},{\bf B})$ with high probability by making ${\cal
O}\left(\frac{n}{\sqrt{{{\bf D}}_{\bf M} ({\bf A},{\bf
B})}}\mbox{poly}\left(\log n, \frac{1}{\epsilon}\right)\right)$ oracle queries,
where ${{\bf D}}_{\bf M} ({\bf A},{\bf B})$ denotes the Hamming distance (the
number of corresponding entries in which ${\bf A}$ and ${\bf B}$ differ)
between two matrices ${\bf A}$ and ${\bf B}$ of size $n \times n$. We also show
a matching lower bound on the number of such {\sc IP} queries needed. Though
our main result is on estimating ${{\bf D}}_{\bf M} ({\bf A},{\bf B})$ using
{\sc IP}, we also compare our results with other query models.
</p></div>
    </summary>
    <updated>2021-07-07T22:41:51Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.02617</id>
    <link href="http://arxiv.org/abs/2107.02617" rel="alternate" type="text/html"/>
    <title>On Search Complexity of Discrete Logarithm</title>
    <feedworld_mtime>1625616000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Pavel HubÃ¡Äek, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/V=aacute=clavek:Jan.html">Jan VÃ¡clavek</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02617">PDF</a><br/><b>Abstract: </b>In this work, we study the discrete logarithm problem in the context of TFNP
- the complexity class of search problems with a syntactically guaranteed
existence of a solution for all instances. Our main results establish that
suitable variants of the discrete logarithm problem are complete for the
complexity class PPP, respectively PWPP, i.e., the subclasses of TFNP capturing
total search problems with a solution guaranteed by the pigeonhole principle,
respectively the weak pigeonhole principle. Besides answering an open problem
from the recent work of Sotiraki, Zampetakis, and Zirdelis (FOCS'18), our
completeness results for PPP and PWPP have implications for the recent line of
work proving conditional lower bounds for problems in TFNP under cryptographic
assumptions. In particular, they highlight that any attempt at basing
average-case hardness in subclasses of TFNP (other than PWPP and PPP) on the
average-case hardness of the discrete logarithm problem must exploit its
structural properties beyond what is necessary for constructions of
collision-resistant hash functions.
</p>
<p>Additionally, our reductions provide new structural insights into the class
PWPP by establishing two new PWPP-complete problems. First, the problem DOVE, a
relaxation of the PPP-complete problem PIGEON. DOVE is the first PWPP-complete
problem not defined in terms of an explicitly shrinking function. Second, the
problem CLAW, a total search problem capturing the computational complexity of
breaking claw-free permutations. In the context of TFNP, the PWPP-completeness
of CLAW matches the known intrinsic relationship between collision-resistant
hash functions and claw-free permutations established in the cryptographic
literature.
</p></div>
    </summary>
    <updated>2021-07-07T22:40:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.02605</id>
    <link href="http://arxiv.org/abs/2107.02605" rel="alternate" type="text/html"/>
    <title>Making Three Out of Two: Three-Way Online Correlated Selection</title>
    <feedworld_mtime>1625616000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shin:Yongho.html">Yongho Shin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/An:Hyung=Chan.html">Hyung-Chan An</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02605">PDF</a><br/><b>Abstract: </b>Two-way online correlated selection (two-way OCS) is an online algorithm
that, at each timestep, takes a pair of elements from the ground set and
irrevocably chooses one of the two elements, while ensuring negative
correlation in the algorithm's choices. Whilst OCS was initially invented by
Fahrbach, Huang, Tao, and Zadimoghaddam to solve the edge-weighted online
bipartite matching problem, it is an interesting technique on its own due to
its capability of introducing a powerful algorithmic tool, namely negative
correlation, to online algorithms. As such, Fahrbach et al. posed two
tantalizing open questions in their paper, one of which was the following: Can
we obtain n-way OCS for n&gt;2, in which the algorithm can be given n&gt;2 elements
to choose from at each timestep?
</p>
<p>In this paper, we affirmatively answer this open question by presenting a
three-way OCS. Our algorithm uses two-way OCS as its building block and is
simple to describe; however, as it internally runs two instances of two-way
OCS, one of which is fed with the output of the other, the final output
probability distribution becomes highly elusive. We tackle this difficulty by
approximating the output distribution of OCS by a flat, less correlated
function and using it as a safe "surrogate" of the real distribution. Our
three-way OCS also yields a 0.5093-competitive algorithm for edge-weighted
online matching, demonstrating its usefulness.
</p></div>
    </summary>
    <updated>2021-07-07T22:52:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.02581</id>
    <link href="http://arxiv.org/abs/2107.02581" rel="alternate" type="text/html"/>
    <title>A General Approach to Approximate Multistage Subgraph Problems</title>
    <feedworld_mtime>1625616000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chimani:Markus.html">Markus Chimani</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Troost:Niklas.html">Niklas Troost</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wiedera:Tilo.html">Tilo Wiedera</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02581">PDF</a><br/><b>Abstract: </b>In a Subgraph Problem we are given some graph and want to find a feasible
subgraph that optimizes some measure. We consider Multistage Subgraph Problems
(MSPs), where we are given a sequence of graph instances (stages) and are asked
to find a sequence of subgraphs, one for each stage, such that each is optimal
for its respective stage and the subgraphs for subsequent stages are as similar
as possible. We present a framework that provides a
$(1/\sqrt{2\chi})$-approximation algorithm for the $2$-stage restriction of an
MSP if the similarity of subsequent solutions is measured as the intersection
cardinality and said MSP is preficient, i.e., we can efficiently find a
single-stage solution that prefers some given subset. The approximation factor
is dependent on the instance's intertwinement $\chi$, a similarity measure for
multistage graphs. We also show that for any MSP, independent of similarity
measure and preficiency, given an exact or approximation algorithm for a
constant number of stages, we can approximate the MSP for an unrestricted
number of stages. Finally, we combine and apply these results and show that the
above restrictions describe a very rich class of MSPs and that proving
membership for this class is mostly straightforward. As examples, we explicitly
state these proofs for natural multistage versions of Perfect Matching,
Shortest s-t-Path, Minimum s-t-Cut and further classical problems on bipartite
or planar graphs, namely Maximum Cut, Vertex Cover, Independent Set, and
Biclique.
</p></div>
    </summary>
    <updated>2021-07-07T22:53:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.02578</id>
    <link href="http://arxiv.org/abs/2107.02578" rel="alternate" type="text/html"/>
    <title>Noisy Boolean Hidden Matching with Applications</title>
    <feedworld_mtime>1625616000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kapralov:Michael.html">Michael Kapralov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Musipatla:Amulya.html">Amulya Musipatla</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tardos:Jakab.html">Jakab Tardos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woodruff:David_P=.html">David P. Woodruff</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Samson.html">Samson Zhou</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02578">PDF</a><br/><b>Abstract: </b>The Boolean Hidden Matching (BHM) problem, introduced in a seminal paper of
Gavinsky et. al. [STOC'07], has played an important role in the streaming lower
bounds for graph problems such as triangle and subgraph counting, maximum
matching, MAX-CUT, Schatten $p$-norm approximation, maximum acyclic subgraph,
testing bipartiteness, $k$-connectivity, and cycle-freeness. The one-way
communication complexity of the Boolean Hidden Matching problem on a universe
of size $n$ is $\Theta(\sqrt{n})$, resulting in $\Omega(\sqrt{n})$ lower bounds
for constant factor approximations to several of the aforementioned graph
problems. The related (and, in fact, more general) Boolean Hidden Hypermatching
(BHH) problem introduced by Verbin and Yu [SODA'11] provides an approach to
proving higher lower bounds of $\Omega(n^{1-1/t})$ for integer $t\geq 2$.
Reductions based on Boolean Hidden Hypermatching generate distributions on
graphs with connected components of diameter about $t$, and basically show that
long range exploration is hard in the streaming model of computation with
adversarial arrivals.
</p>
<p>In this paper we introduce a natural variant of the BHM problem, called noisy
BHM (and its natural noisy BHH variant), that we use to obtain higher than
$\Omega(\sqrt{n})$ lower bounds for approximating several of the aforementioned
problems in graph streams when the input graphs consist only of components of
diameter bounded by a fixed constant. We also use the noisy BHM problem to show
that the problem of classifying whether an underlying graph is isomorphic to a
complete binary tree in insertion-only streams requires $\Omega(n)$ space,
which seems challenging to show using BHM or BHH alone.
</p></div>
    </summary>
    <updated>2021-07-07T22:42:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.02573</id>
    <link href="http://arxiv.org/abs/2107.02573" rel="alternate" type="text/html"/>
    <title>Irregular Invertible Bloom Look-Up Tables</title>
    <feedworld_mtime>1625616000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/L=aacute=zaro:Francisco.html">Francisco LÃ¡zaro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Matuz:Bal=aacute=zs.html">BalÃ¡zs Matuz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02573">PDF</a><br/><b>Abstract: </b>We consider invertible Bloom lookup tables (IBLTs) which are probabilistic
data structures that allow to store keyvalue pairs. An IBLT supports insertion
and deletion of key-value pairs, as well as the recovery of all key-value pairs
that have been inserted, as long as the number of key-value pairs stored in the
IBLT does not exceed a certain number. The recovery operation on an IBLT can be
represented as a peeling process on a bipartite graph. We present a density
evolution analysis of IBLTs which allows to predict the maximum number of
key-value pairs that can be inserted in the table so that recovery is still
successful with high probability. This analysis holds for arbitrary irregular
degree distributions and generalizes results in the literature. We complement
our analysis by numerical simulations of our own IBLT design which allows to
recover a larger number of key-value pairs as state-of-the-art IBLTs of same
size.
</p></div>
    </summary>
    <updated>2021-07-07T22:47:43Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.02570</id>
    <link href="http://arxiv.org/abs/2107.02570" rel="alternate" type="text/html"/>
    <title>Practical I/O-Efficient Multiway Separators</title>
    <feedworld_mtime>1625616000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Svendsen:Svend_C=.html">Svend C. Svendsen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02570">PDF</a><br/><b>Abstract: </b>We revisit the fundamental problem of I/O-efficiently computing $r$-way
separators on planar graphs. An $r$-way separator divides a planar graph with
$N$ vertices into $O(r)$ regions of size $O(N/r)$ and $O(\sqrt {Nr})$ boundary
vertices in total, where boundary vertices are vertices that are adjacent to
more than one region. Such separators are used in I/O-efficient solutions to
many fundamental problems on planar graphs such as breadth-first search,
finding single-source shortest paths, topological sorting, and finding strongly
connected components. Our main result is an I/O-efficient sampling-based
algorithm that, given a Koebe-embedding of a graph with $N$ vertices and a
parameter $r$, computes an $r$-way separator for the graph under certain
assumptions on the size of internal memory. Computing a Koebe-embedding of a
planar graph is difficult in practice and no known I/O-efficient algorithm
currently exists. Therefore, we show how our algorithm can be generalized and
applied directly to Delaunay triangulations without relying on a
Koebe-embedding. This adaptation can produce many boundary vertices in the
worst-case, however, to our knowledge our result is the first to be implemented
in practice due to the many non-trivial and complex techniques used in previous
results. Furthermore, we show that our algorithm performs well on real-world
data and that the number of boundary vertices is small in practice.
</p>
<p>Motivated by applications in geometric information systems, we show how our
algorithm for Delaunay triangulations can be applied to compute the flow
accumulation over a terrain, which models how much water flows over the
vertices of a terrain. When given an $r$-way separator, our implementation of
the algorithm outperforms traditional sweep-line-based algorithms on the
publicly available digital elevation model of Denmark.
</p></div>
    </summary>
    <updated>2021-07-07T23:03:16Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-07-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.02554</id>
    <link href="http://arxiv.org/abs/2107.02554" rel="alternate" type="text/html"/>
    <title>On the Hardness of Compressing Weights</title>
    <feedworld_mtime>1625616000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jansen:Bart_M=_P=.html">Bart M. P. Jansen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roy:Shivesh_K=.html">Shivesh K. Roy</a>, MichaÅ WÅodarczyk <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02554">PDF</a><br/><b>Abstract: </b>We investigate computational problems involving large weights through the
lens of kernelization, which is a framework of polynomial-time preprocessing
aimed at compressing the instance size. Our main focus is the weighted Clique
problem, where we are given an edge-weighted graph and the goal is to detect a
clique of total weight equal to a prescribed value. We show that the weighted
variant, parameterized by the number of vertices $n$, is significantly harder
than the unweighted problem by presenting an $O(n^{3 - \varepsilon})$ lower
bound on the size of the kernel, under the assumption that NP $\not \subseteq$
coNP/poly. This lower bound is essentially tight: we show that we can reduce
the problem to the case with weights bounded by $2^{O(n)}$, which yields a
randomized kernel of $O(n^3)$ bits.
</p>
<p>We generalize these results to the weighted $d$-Uniform Hyperclique problem,
Subset Sum, and weighted variants of Boolean Constraint Satisfaction Problems
(CSPs). We also study weighted minimization problems and show that weight
compression is easier when we only want to preserve the collection of optimal
solutions. Namely, we show that for node-weighted Vertex Cover on bipartite
graphs it is possible to maintain the set of optimal solutions using integer
weights from the range $[1, n]$, but if we want to maintain the ordering of the
weights of all inclusion-minimal solutions, then weights as large as
$2^{\Omega(n)}$ are necessary.
</p></div>
    </summary>
    <updated>2021-07-07T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.02503</id>
    <link href="http://arxiv.org/abs/2107.02503" rel="alternate" type="text/html"/>
    <title>On Arithmetically Progressed Suffix Arrays and related Burrows-Wheeler Transforms</title>
    <feedworld_mtime>1625616000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Daykin:Jacqueline_W=.html">Jacqueline W. Daykin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/K=ouml=ppl:Dominik.html">Dominik KÃ¶ppl</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/K=uuml=bel:David.html">David KÃ¼bel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stober:Florian.html">Florian Stober</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02503">PDF</a><br/><b>Abstract: </b>We characterize those strings whose suffix arrays are based on arithmetic
progressions, in particular, arithmetically progressed permutations where all
pairs of successive entries of the permutation have the same difference modulo
the respective string length. We show that an arithmetically progressed
permutation $P$ coincides with the suffix array of a unary, binary, or ternary
string. We further analyze the conditions of a given $P$ under which we can
find a uniquely defined string over either a binary or ternary alphabet having
$P$ as its suffix array. For the binary case, we show its connection to lower
Christoffel words, balanced words, and Fibonacci words. In addition to solving
the arithmetically progressed suffix array problem, we give the shape of the
Burrows-Wheeler transform of those strings solving this problem. These results
give rise to numerous future research directions.
</p></div>
    </summary>
    <updated>2021-07-07T22:55:54Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.02432</id>
    <link href="http://arxiv.org/abs/2107.02432" rel="alternate" type="text/html"/>
    <title>Unifying Width-Reduced Methods for Quasi-Self-Concordant Optimization</title>
    <feedworld_mtime>1625616000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Adil:Deeksha.html">Deeksha Adil</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bullins:Brian.html">Brian Bullins</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sachdeva:Sushant.html">Sushant Sachdeva</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02432">PDF</a><br/><b>Abstract: </b>We provide several algorithms for constrained optimization of a large class
of convex problems, including softmax, $\ell_p$ regression, and logistic
regression. Central to our approach is the notion of width reduction, a
technique which has proven immensely useful in the context of maximum flow
[Christiano et al., STOC'11] and, more recently, $\ell_p$ regression [Adil et
al., SODA'19], in terms of improving the iteration complexity from $O(m^{1/2})$
to $\tilde{O}(m^{1/3})$, where $m$ is the number of rows of the design matrix,
and where each iteration amounts to a linear system solve. However, a
considerable drawback is that these methods require both problem-specific
potentials and individually tailored analyses.
</p>
<p>As our main contribution, we initiate a new direction of study by presenting
the first unified approach to achieving $m^{1/3}$-type rates. Notably, our
method goes beyond these previously considered problems to more broadly capture
quasi-self-concordant losses, a class which has recently generated much
interest and includes the well-studied problem of logistic regression, among
others. In order to do so, we develop a unified width reduction method for
carefully handling these losses based on a more general set of potentials.
Additionally, we directly achieve $m^{1/3}$-type rates in the constrained
setting without the need for any explicit acceleration schemes, thus naturally
complementing recent work based on a ball-oracle approach [Carmon et al.,
NeurIPS'20].
</p></div>
    </summary>
    <updated>2021-07-07T22:53:59Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.02320</id>
    <link href="http://arxiv.org/abs/2107.02320" rel="alternate" type="text/html"/>
    <title>Memory-Sample Lower Bounds for Learning Parity with Noise</title>
    <feedworld_mtime>1625616000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garg:Sumegha.html">Sumegha Garg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kothari:Pravesh_K=.html">Pravesh K. Kothari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Pengda.html">Pengda Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Raz:Ran.html">Ran Raz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02320">PDF</a><br/><b>Abstract: </b>In this work, we show, for the well-studied problem of learning parity under
noise, where a learner tries to learn $x=(x_1,\ldots,x_n) \in \{0,1\}^n$ from a
stream of random linear equations over $\mathrm{F}_2$ that are correct with
probability $\frac{1}{2}+\varepsilon$ and flipped with probability
$\frac{1}{2}-\varepsilon$, that any learning algorithm requires either a memory
of size $\Omega(n^2/\varepsilon)$ or an exponential number of samples.
</p>
<p>In fact, we study memory-sample lower bounds for a large class of learning
problems, as characterized by [GRT'18], when the samples are noisy. A matrix
$M: A \times X \rightarrow \{-1,1\}$ corresponds to the following learning
problem with error parameter $\varepsilon$: an unknown element $x \in X$ is
chosen uniformly at random. A learner tries to learn $x$ from a stream of
samples, $(a_1, b_1), (a_2, b_2) \ldots$, where for every $i$, $a_i \in A$ is
chosen uniformly at random and $b_i = M(a_i,x)$ with probability
$1/2+\varepsilon$ and $b_i = -M(a_i,x)$ with probability $1/2-\varepsilon$
($0&lt;\varepsilon&lt; \frac{1}{2}$). Assume that $k,\ell, r$ are such that any
submatrix of $M$ of at least $2^{-k} \cdot |A|$ rows and at least $2^{-\ell}
\cdot |X|$ columns, has a bias of at most $2^{-r}$. We show that any learning
algorithm for the learning problem corresponding to $M$, with error, requires
either a memory of size at least $\Omega\left(\frac{k \cdot \ell}{\varepsilon}
\right)$, or at least $2^{\Omega(r)}$ samples. In particular, this shows that
for a large class of learning problems, same as those in [GRT'18], any learning
algorithm requires either a memory of size at least $\Omega\left(\frac{(\log
|X|) \cdot (\log |A|)}{\varepsilon}\right)$ or an exponential number of noisy
samples.
</p>
<p>Our proof is based on adapting the arguments in [Raz'17,GRT'18] to the noisy
case.
</p></div>
    </summary>
    <updated>2021-07-07T22:38:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.02318</id>
    <link href="http://arxiv.org/abs/2107.02318" rel="alternate" type="text/html"/>
    <title>Incremental Edge Orientation in Forests</title>
    <feedworld_mtime>1625616000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bender:Michael_A=.html">Michael A. Bender</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kopelowitz:Tsvi.html">Tsvi Kopelowitz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuszmaul:William.html">William Kuszmaul</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Porat:Ely.html">Ely Porat</a>, Clifford Stein <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02318">PDF</a><br/><b>Abstract: </b>For any forest $G = (V, E)$ it is possible to orient the edges $E$ so that no
vertex in $V$ has out-degree greater than $1$. This paper considers the
incremental edge-orientation problem, in which the edges $E$ arrive over time
and the algorithm must maintain a low-out-degree edge orientation at all times.
We give an algorithm that maintains a maximum out-degree of $3$ while flipping
at most $O(\log \log n)$ edge orientations per edge insertion, with high
probability in $n$. The algorithm requires worst-case time $O(\log n \log \log
n)$ per insertion, and takes amortized time $O(1)$. The previous state of the
art required up to $O(\log n / \log \log n)$ edge flips per insertion.
</p>
<p>We then apply our edge-orientation results to the problem of dynamic Cuckoo
hashing. The problem of designing simple families $\mathcal{H}$ of hash
functions that are compatible with Cuckoo hashing has received extensive
attention. These families $\mathcal{H}$ are known to satisfy \emph{static
guarantees}, but do not come typically with \emph{dynamic guarantees} for the
running time of inserts and deletes. We show how to transform static guarantees
(for $1$-associativity) into near-state-of-the-art dynamic guarantees (for
$O(1)$-associativity) in a black-box fashion. Rather than relying on the family
$\mathcal{H}$ to supply randomness, as in past work, we instead rely on
randomness within our table-maintenance algorithm.
</p></div>
    </summary>
    <updated>2021-07-07T22:48:07Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.02239</id>
    <link href="http://arxiv.org/abs/2107.02239" rel="alternate" type="text/html"/>
    <title>Vision Xformers: Efficient Attention for Image Classification</title>
    <feedworld_mtime>1625616000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jeevan:Pranav.html">Pranav Jeevan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sethi:Amit.html">Amit Sethi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02239">PDF</a><br/><b>Abstract: </b>Linear attention mechanisms provide hope for overcoming the bottleneck of
quadratic complexity which restricts application of transformer models in
vision tasks. We modify the ViT architecture to work on longer sequence data by
replacing the quadratic attention with efficient transformers like Performer,
Linformer and Nystr\"omformer of linear complexity creating Vision X-formers
(ViX). We show that ViX performs better than ViT in image classification
consuming lesser computing resources. We further show that replacing the
embedding linear layer by convolutional layers in ViX further increases their
performance. Our test on recent visions transformer models like LeViT and
Compact Convolutional Transformer (CCT) show that replacing the attention with
Nystr\"omformer or Performer saves GPU usage and memory without deteriorating
performance. Incorporating these changes can democratize transformers by making
them accessible to those with limited data and computing resources.
</p></div>
    </summary>
    <updated>2021-07-07T22:37:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2107.02205</id>
    <link href="http://arxiv.org/abs/2107.02205" rel="alternate" type="text/html"/>
    <title>DGO: A new DIRECT-type MATLAB toolbox for derivative-free global optimization</title>
    <feedworld_mtime>1625616000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stripinis:Linas.html">Linas Stripinis</a>, Remigijus PaulaviÄius <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2107.02205">PDF</a><br/><b>Abstract: </b>In this work, we introduce DGO, a new MATLAB toolbox for derivative-free
global optimization. DGO collects various deterministic derivative-free
DIRECT-type algorithms for box-constrained, generally-constrained, and problems
with hidden constraints. Each sequential algorithm is implemented in two
different ways: using static and dynamic data structures for more efficient
information storage and organization. Furthermore, parallel schemes are applied
to some promising algorithms within DGO. The toolbox is equipped with a
graphical user interface (GUI), which ensures the user-friendly use of all
functionalities available in DGO. Available features are demonstrated in
detailed computational studies using a created comprehensive library of global
optimization problems. Additionally, eleven classical engineering design
problems are used to illustrate the potential of DGO to solve challenging
real-world problems.
</p></div>
    </summary>
    <updated>2021-07-07T22:55:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-07-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.04521</id>
    <link href="http://arxiv.org/abs/1907.04521" rel="alternate" type="text/html"/>
    <title>The complexity of the first-order theory of pure equality</title>
    <feedworld_mtime>1625616000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Latkin:Ivan_V=.html">Ivan V. Latkin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.04521">PDF</a><br/><b>Abstract: </b>We will find a lower bound on the recognition complexity of the theories that
are nontrivial relative to some equivalence relation (this relation may be
equality), namely, each of these theories is consistent with the formula, whose
sense is that there exist two non-equivalent elements. However, at first, we
will obtain a lower bound on the computational complexity for the first-order
theory of Boolean algebra that has only two elements. For this purpose, we will
code the long-continued deterministic Turing machine computations by the
relatively short-length quantified Boolean formulae; the modified Stockmeyer
and Meyer method will appreciably be used for this simulation. Then, we will
transform the modeling formulae of the theory of this Boolean algebra to the
simulation ones of the first-order theory of the only equivalence relation in
polynomial time. Since the computational complexity of these theories is not
polynomial, we obtain that the class $\mathbf{P}$ is a proper subclass of
$\mathbf{PSPACE}$ (Polynomial Time is a proper subset of Polynomial Space).
</p>
<p>Keywords: Computational complexity, the theory of equality, the coding of
computations, simulation by means formulae, polynomial time, polynomial space,
lower complexity bound
</p></div>
    </summary>
    <updated>2021-07-07T22:41:38Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-07-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=879</id>
    <link href="https://emanueleviola.wordpress.com/2021/07/06/windows-never-changes/" rel="alternate" type="text/html"/>
    <title>Windows never changes</title>
    <summary>For months, Windows 10 complained that it didnât have enough space on the hard disk, but the options it gave me to clean up space were ridiculous. Worse, the âstorageâ function that supposedly tells you whatâs taking space wasnât even close to the truth. This became so bad that I was forced to remove some [â¦]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>For months, Windows 10 complained that it didnât have enough space on the hard disk, but the options it gave me to clean up space were ridiculous. Worse, the âstorageâ function that supposedly tells you whatâs taking space wasnât even close to the truth. This became so bad that I was forced to remove some things I didnât want to remove, often with a lot of effort, because space was so tight that Windows didnât even have enough to run the uninstaller! In the end I became so desperate that I installed <em>TreeSize Free</em>. It quickly revealed that <em>crash plan </em> was taking up a huge amount of space. This revealed to be associated to the <em>Code42</em> program â a program that the system was listing as taking 200MB. Well, uninstalling Code42 freed SIXTY PERCENT of the hard disk space, 140GB.</p></div>
    </content>
    <updated>2021-07-06T16:42:32Z</updated>
    <published>2021-07-06T16:42:32Z</published>
    <category term="Uncategorized"/>
    <category term="tech"/>
    <author>
      <name>Manu</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>by Manu</subtitle>
      <title>Thoughts</title>
      <updated>2021-07-08T11:21:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://ptreview.sublinear.info/?p=1546</id>
    <link href="https://ptreview.sublinear.info/?p=1546" rel="alternate" type="text/html"/>
    <title>News for June 2021</title>
    <summary>A quieter month after last monthâs bonanza. One (applied!) paper on distribution testing, a paper on tolerant distribution testing, and a compendium of open problems. (Ed: alas, I missed the paper on tolerant distribution testing, authored by one of our editors. Sorry, ClÃ©ment!) Learning-based Support Estimation in Sublinear Time by Talya Eden,Â Piotr Indyk,Â Shyam Narayanan,Â Ronitt Rubinfeld,Â Sandeep [â¦]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p id="block-79f77829-3f22-4392-8cc9-3f466129d855">A quieter month after last monthâs bonanza. One (applied!) paper on distribution testing, a paper on tolerant distribution testing, and a compendium of open problems. <em>(Ed: alas, I missed the paper on tolerant distribution testing, authored by one of our editors. Sorry, ClÃ©ment!)</em></p>



<p id="block-db4c65bf-46c4-4c3e-ad70-d3348de84ff3"><strong>Learning-based Support Estimation in Sublinear Time</strong> by Talya Eden,Â Piotr Indyk,Â Shyam Narayanan,Â Ronitt Rubinfeld,Â Sandeep Silwal,Â and Tal Wagner (<a href="https://arxiv.org/abs/2106.08396">arXiv</a>). A classic problem in distribution testing is that of estimating the support size \(n\) of an unknown distribution \(\mathcal{D}\). (Assume that all elements in the support have probability at least \(1/n\).) A fundamental result of <a href="http://theory.stanford.edu/~valiant/papers/VV_stoc11.pdf">Valiant-Valiant</a> (2011)  proves that the sample complexity of this problem is \(\Theta(n/\log n)\). A line of work has emerged in trying to reduce this complexity, with additional sources of information. <a href="http://dspace.mit.edu/bitstream/handle/1721.1/101001/Rubinfeld_Testing%20probability.pdf;sequence=1">Canonne-Rubinfeld (2014)</a> showed that, if one can query the exact probabilities of elements, then the complexity can be made independent of \(n\). This paper studies a robust version of this assumption: suppose, we can get constant factor approximations to the probabilities. Then, the main result is that we can get a query complexity of \(n^{1-1/\log(\varepsilon^{-1})} \ll n/\log n\) (where the constant \(\varepsilon\) denotes the additive approximation to the support size). This paper also does empirical experiments to show that the new algorithm is indeed better in practice. Moreover, it shows that existing methods degraded rapidly with poorer probability estimates, while the new algorithm maintains its accuracy even with such estimates. </p>



<p><strong>The Price of Tolerance in Distribution Testing</strong> by ClÃ©ment L. Canonne, Ayush Jain, Gautam Kamath, and Jerry Li (<a href="https://arxiv.org/abs/2106.13414">arXiv</a>). While we have seen many results in distribution testing, the subject of tolerance is one that hasnât received as much attention. Consider the problem of testing if unknown distribution \(\mathcal{p}\) (over domain \([n]\)) is the same as known distribution \(\mathcal{q}\). We wish to distinguish \(\varepsilon_1\)-close from \(\varepsilon_2\)-far, under total variation distance. When \(\varepsilon_1\) is zero, this is the standard property testing setting, and classic results yield \(\Theta(\sqrt{n})\) sample complexity. If \(\varepsilon_1 = \varepsilon_2/2\), then we are looking for a constant factor approximation to the distance. And the complexity is \(\Theta(n/\log n)\). Surprisingly, nothing was known in better. Until this paper, that is. The main result gives a complete characterization of sample complexity (up to log factors), for all values of \(\varepsilon_1, \varepsilon_2\). Remarkably, the sample complexity has an additive term \((n/\log n) \cdot (\varepsilon_1/\varepsilon^2_2)\). Thus, when \(\varepsilon_1 &gt; \sqrt{\varepsilon_2}\), the sample complexity is \(\Theta(n/\log n)\). When \(\varepsilon_1\) is smaller, the main result gives a smooth dependence on the sample complexity. One the main challenges is that existing results use two very different techniques for the property testing vs constant-factor approximation regimes. The former uses simpler \(\ell_2\)-statistics (e.g. collision counting), while the latter is based on polynomial approximations (estimating moments). The upper bound in this paper shows that simpler statistics based on just the first two moments suffice to getting results for all regimes of \(\varepsilon_1, \varepsilon_2\).</p>



<p><strong>Open Problems in Property Testing of Graphs</strong> by Oded Goldreich (<a href="https://eccc.weizmann.ac.il/report/2021/088/">ECCC</a>). As the title clearly states, this is a survey covering a number of open problems in graph property testing. The broad division is based on the query model: dense graphs, bounded degree graphs, and general graphs. A reader will see statements of various classic open problems, such as the complexity of testing triangle freeness for dense graphs and characterizing properties that can be tested in \(poly(\varepsilon^{-1})\) queries. Arguably, there are more open problems (and fewer results) for testing in bounded degree graphs, where we lack broad characterizations of testable properties. An important, though less famous (?), open problem is that of the complexity of testing isomorphism. It would appear that the setting of general graphs, where we know the least, may be the next frontier for graph property testing. A problem that really caught my eye: can we transform testers that work for bounded degree graphs into those that work for bounded arboricity graphs? The latter is a generalization of bounded degree that has appeared in a number of recent results on sublinear graph algorithms.</p></div>
    </content>
    <updated>2021-07-06T02:27:19Z</updated>
    <published>2021-07-06T02:27:19Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Seshadhri</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2021-07-07T23:06:28Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/094</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/094" rel="alternate" type="text/html"/>
    <title>TR21-094 |  New Non-FPT Lower Bounds for Some Arithmetic Formulas | 

	SÃ©bastien Tavenas, 

	Nutan Limaye, 

	Srikanth Srinivasan</title>
    <summary>An Algebraic Formula for a polynomial $P\in F[x_1,\ldots,x_N]$ is an algebraic expression for $P(x_1,\ldots,x_N)$ using variables, field constants, additions and multiplications.  Such formulas capture an algebraic analog of the Boolean complexity class $\mathrm{NC}^1.$ Proving lower bounds against this model is thus an important problem.


It is known that, to prove superpolynomial lower bounds against algebraic formulas, it suffices to prove good enough lower bounds against restricted kinds of formulas known as Set-Multilinear formulas, for computing a polynomial $P(x_1,...,x_N)$ of degree $O(\log N/\log \log N)$. In the past, many superpolynomial lower bounds were found, but they are of the form $\Omega(f(d) poly(N))$ (where $f$ is typically a subexponential function) which is insufficient to get lower bounds for general formulas. Recently, the authors proved the first non-FPT lower bounds, i.e., a lower bound of the form $N^{\Omega{(f(d)})}$, against small-depth set-multilinear formulas (and also for circuits). In this work, we extend this result in two directions. 

1) Large-depth set-multilinear formulas. In the setting of general set-multilinear formulas, we prove a lower bound of $(\log n)^{\Omega(\log d)}$ for computing the Iterated Matrix Multiplication polynomial $IMM_{n,d}.$ In particular, this implies the first superpolynomial lower bound against unbounded-depth set-multilinear formulas computing $IMM_{n,n}.$ 

As a corollary, this also resolves the homogeneous version of a question of Nisan (STOC 1991) regarding the relative power of Algebraic formulas and Branching programs in the non-commutative setting.

2) Stronger bounds for homogeneous non-commutative small-depth circuits. In the small-depth homogeneous non-commutative case, we prove  a lower bound of $n^{d^{1/\Delta}/2^{O(\Delta)}}$, which yields non-FPT bounds for depths up to $o(\sqrt{\log d}).$ In comparison, the previous bound works in the harder commutative set-multilinear setting, but only up to depths $o(\log \log d)$. 
Moreover, our lower bound holds for all values of $d$, as opposed to the previous set-multilinear lower bound, which holds as long as $d$ is small, i.e., $d = O(\log n)$.</summary>
    <updated>2021-07-06T02:24:48Z</updated>
    <published>2021-07-06T02:24:48Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-08T11:20:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5554</id>
    <link href="https://www.scottaaronson.com/blog/?p=5554" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5554#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5554" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Open thread on new quantum supremacy claims</title>
    <summary xml:lang="en-US">Happy 4th to those in the US! The group of Chaoyang Lu and Jianwei Pan, based at USTC in China, has been on a serious quantum supremacy tear lately. Recall that last December, USTC announced the achievement of quantum supremacy via Gaussian BosonSampling, with 50-70 detected photonsâthe second claim of sampling-based quantum supremacy, after Googleâs [â¦]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Happy 4th to those in the US!</p>



<p>The group of Chaoyang Lu and Jianwei Pan, based at USTC in China, has been on a <em>serious</em> quantum supremacy tear lately.  Recall that last December, USTC announced the achievement of quantum supremacy via Gaussian BosonSampling, with 50-70 detected photonsâthe second claim of sampling-based quantum supremacy, after Googleâs in Fall 2019.  However, skeptics then poked holes in the USTC claim, showing how they could spoof the results with a classical computer, basically by reproducing the k-photon correlations for relatively small values of k.  Debate over the details continues, but the Chinese group seeks to render the debate largely moot with a <a href="https://arxiv.org/abs/2106.15534">new and better Gaussian BosonSampling experiment</a>, with 144 modes and up to 113 detected photons.  They say they were able to measure k-photon correlations for k up to about 19, which if true would constitute a serious obstacle to the classical simulation strategies that people discussed for the previous experiment.</p>



<p>In the meantime, though, an overlapping group of authors had <a href="https://arxiv.org/abs/2106.14734">put out another paper the day before</a> (!) reporting a sampling-based quantum supremacy experiment using superconducting qubitsâextremely similar to what Google did (the same circuit depth and everything), except now with 56 qubits rather than 53.</p>



<p>I confess that I havenât yet studied either paper in detailâamong other reasons, because Iâm on vacation with my family at the beach, and because Iâm trying to spend what work-time I have on my own projects.  But anyone who <em>has</em> read them, please use the comments of this post to discuss!  Hopefully Iâll learn something.</p>



<p>To confine myself to some general comments: since Googleâs announcement in Fall 2019, Iâve consistently said that sampling-based quantum supremacy is <em>not</em> yet a done deal.  Iâve said that quantum supremacy seems important enough to want independent replications, and demonstrations in other hardware platforms like ion traps and photonics, and better gate fidelity, and better classical hardness, and better verification protocols.  Most of all, Iâve said that we needed a genuine dialogue between the âquantum supremacistsâ and the classical skeptics: the former doing experiments and releasing all their data, the latter trying to design efficient classical simulations for those experiments, and so on in an iterative process.  Just like in applied cryptography, weâd only have real confidence in a quantum supremacy claim once it had survived at least a few years of attacks by skeptics.  So Iâm delighted that this is precisely whatâs now happening.  USTCâs papers are two new volleys in this back-and-forth; we all eagerly await the next volley, whichever side it comes from.</p>



<p>While Iâve been trying for years to move away from the expectation that I blog about each and every QC announcement that someone messages me about, maybe Iâll also say a word about the recent announcement by IBM of a quantum advantage in space complexity (see <a href="https://www.zdnet.com/article/ibm-researchers-demonstrate-the-advantage-that-quantum-computers-have-over-classical-computers/?fbclid=IwAR0KHVoI8W83Kwmeq9XwmuLHknlKeHjxWcvIjrqjH0QUtLZ8kaIWi6z42yk">here</a> for popular article and <a href="https://arxiv.org/abs/2008.06478">here</a> for arXiv preprint).  There appears to be a nice theoretical result here, about the ability to evaluate any symmetric Boolean function with a single qubit in a branching-program-like model.  Iâd love to understand that result better.  But to answer the question I received, this is another case where, once you know the protocol, you know both that the experiment can be done <em>and</em> exactly what its result will be (namely, the thing predicted by QM).  So I think the interest is almost entirely in the protocol itself.</p></div>
    </content>
    <updated>2021-07-04T22:34:01Z</updated>
    <published>2021-07-04T22:34:01Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-07-04T22:34:01Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/093</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/093" rel="alternate" type="text/html"/>
    <title>TR21-093 |  Bounded Indistinguishability for Simple Sources | 

	Yuval Filmus, 

	Andrej Bogdanov, 

	Yuval Ishai, 

	Akshayaram Srinivasan, 

	Krishnamoorthy Dinesh, 

	Avi Kaplan</title>
    <summary>A pair of sources $\mathbf{X},\mathbf{Y}$ over $\{0,1\}^n$ are $k$-indistinguishable if their projections to any $k$ coordinates are identically distributed. Can some $\mathit{AC^0}$ function distinguish between two such sources when $k$ is big, say $k=n^{0.1}$? Braverman's theorem (Commun. ACM 2011) implies a negative answer when $\mathbf{X}$ is uniform, whereas Bogdanov et al. (Crypto 2016) observe that this is not the case in general.

We initiate a systematic study of this question for natural classes of low-complexity sources, including ones that arise in cryptographic applications, obtaining positive results, negative results, and barriers. In particular: 

â There exist $\Omega(\sqrt{n})$-indistinguishable $\mathbf{X},\mathbf{Y}$, samplable by degree $O(\log n)$ polynomial maps (over $\mathbb{F}_2$) and by $\mathit{poly}(n)$-size decision trees, that are $\Omega(1)$-distinguishable by OR.

â There exists a function $f$ such that all $f(d, \epsilon)$-indistinguishable $\mathbf{X},\mathbf{Y}$ that are samplable by degree-$d$ polynomial maps are $\epsilon$-indistinguishable by OR for all sufficiently large $n$.  Moreover, $f(1, \epsilon) = \lceil\log(1/\epsilon)\rceil + 1$ and $f(2, \epsilon) = O(\log^{10}(1/\epsilon))$. 

â Extending (weaker versions of) the above negative results to $\mathit{AC^0}$ distinguishers would require  settling a conjecture of Servedio and Viola (ECCC 2012).
Concretely, if every pair of $n^{0.9}$-indistinguishable $\mathbf{X},\mathbf{Y}$ that are samplable by linear maps is $\epsilon$-indistinguishable by $\mathit{AC^0}$ circuits, then the binary inner product function can have at most an $\epsilon$-correlation with $\mathit{AC^0}\circ\oplus$ circuits.</summary>
    <updated>2021-07-04T02:37:53Z</updated>
    <published>2021-07-04T02:37:53Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-08T11:20:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2021/07/03/adfocs-2021-convex-optimization-and-graph-algorithms/</id>
    <link href="https://cstheory-events.org/2021/07/03/adfocs-2021-convex-optimization-and-graph-algorithms/" rel="alternate" type="text/html"/>
    <title>ADFOCS 2021: Convex Optimization and Graph Algorithms</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">July 26 â August 13, 2021 Online https://conferences.mpi-inf.mpg.de/adfocs-22/ ADFOCS is an international summer school held annually at the Max Planck Institute for Informatics (MPII). The topic of this yearâs edition is Convex Optimization and its applications on Graph Algorithms. The event will take place online over the span of three weeks from July 26 to â¦ <a class="more-link" href="https://cstheory-events.org/2021/07/03/adfocs-2021-convex-optimization-and-graph-algorithms/">Continue reading <span class="screen-reader-text">ADFOCS 2021: Convex Optimization and GraphÂ Algorithms</span></a></div>
    </summary>
    <updated>2021-07-03T13:59:31Z</updated>
    <published>2021-07-03T13:59:31Z</published>
    <category term="school"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2021-07-08T11:21:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8161</id>
    <link href="https://windowsontheory.org/2021/07/02/itc-2021-call-for-participation-guest-post-by-benny-applebaum/" rel="alternate" type="text/html"/>
    <title>ITC 2021: Call for participation (guest post by Benny Applebaum)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The second edition of the recently created conference on Information-Theoretic Cryptography (ITC 2021) will take place virtually on July 24-26, 2021. The final program is out and contains exciting new works and invited talks that highlight the recent advances in the area by Benny Applebaum, Elaine Shi, Irit Dinur, Salman Avestimehr, Matthieu Bloch, and Mark â¦ <a class="more-link" href="https://windowsontheory.org/2021/07/02/itc-2021-call-for-participation-guest-post-by-benny-applebaum/">Continue reading <span class="screen-reader-text">ITC 2021: Call for participation (guest post by BennyÂ Applebaum)</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The second edition of the recently created conference on <em>Information-Theoretic Cryptography (ITC 2021)</em> will take place virtually on July 24-26, 2021. The final program is out and contains exciting new works and invited talks that highlight the recent advances in the area by Benny Applebaum, Elaine Shi, Irit Dinur, Salman Avestimehr, Matthieu Bloch, and Mark Zhandry.</p>



<p>Registration to the conference is free (but required!)</p>



<p>Visit the webpage <a href="https://itcrypto.github.io/2021/index.html" rel="noreferrer noopener" target="_blank">https://itcrypto.github.io/2021/index.html</a> for more information and for the full program.Â Â </p>



<p>Hope to see you there!</p>



<p>â The Organising Committee</p></div>
    </content>
    <updated>2021-07-02T13:08:48Z</updated>
    <published>2021-07-02T13:08:48Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-07-08T11:21:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://ptreview.sublinear.info/?p=1542</id>
    <link href="https://ptreview.sublinear.info/?p=1542" rel="alternate" type="text/html"/>
    <title>Looking back at WOLAâ21: Videos available</title>
    <summary>The fifth Workshop on Local Algorithms (WOLAâ21) took place earlier this month, and the recordings of the invited talks are now available on YouTube. If you missed the workshop, or want to refresh your memory, here are the recordings (ordered by the workshop schedule): James Aspnes (Yale) on Population Protocols Uri Stemmer (Ben-Gurion University) on [â¦]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The fifth <a href="http://www.local-algorithms.com/">Workshop on Local Algorithms</a> (WOLAâ21) took place <a href="https://ptreview.sublinear.info/?p=1517">earlier this month</a>, and the recordings of the invited talks are now <a href="https://www.youtube.com/watch?v=aFaJz3HPluM&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm">available on YouTube</a>. If you missed the workshop, or want to refresh your memory, here are the recordings (ordered by the workshop schedule):</p>



<ul><li>James Aspnes (Yale) on <a href="https://www.youtube.com/watch?v=aFaJz3HPluM&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=2"><em>Population Protocols</em></a></li><li>Uri Stemmer (Ben-Gurion University) on <em><a href="https://www.youtube.com/watch?v=yxOmND76k4M&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=4">The Local Model of Differential Privacy: A Survey</a></em></li><li>Mary Wootters (Stanford University) on <em><a href="https://www.youtube.com/watch?v=oHGvGgSdvAc&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=7">Lifted Codes and Disjoint Repair Groups</a></em></li><li>Christian Sohler (University of Cologne) on <em><a href="https://www.youtube.com/watch?v=DuJQe0pv224&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=6">Property Testing in Planar Graphs</a></em></li><li>Elaine Shi (Carnegie Mellon University) on <em><a href="https://www.youtube.com/watch?v=jq5MuJJLnDk&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=5">Game-Theoretically Secure Protocols Inspired by Blockchains</a></em></li><li>Jelani Nelson (UC Berkeley) on <em><a href="https://www.youtube.com/watch?v=ZQ1ekannlw0&amp;list=PLrzWWJ35eA2wG5aGbagb6bVo8HMH4FrNm&amp;index=3">Optimal bounds for approximate counting</a></em></li></ul>



<p>Thanks again to the speakers and organizers, and looking forward to WOLAâ22!</p></div>
    </content>
    <updated>2021-07-02T04:59:48Z</updated>
    <published>2021-07-02T04:59:48Z</published>
    <category term="Conference reports"/>
    <author>
      <name>Clement Canonne</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2021-07-07T23:06:28Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/092</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/092" rel="alternate" type="text/html"/>
    <title>TR21-092 |  A Note on One-way Functions and Sparse Languages | 

	Yanyi Liu, 

	Rafael Pass</title>
    <summary>We show equivalence between the existence of one-way
functions and the existence of a \emph{sparse} language that is
hard-on-average w.r.t. some efficiently samplable ``high-entropy''
distribution.
In more detail, the following are equivalent:
  - The existentence of a $S(\cdot)$-sparse language $L$ that is
    hard-on-average with respect to some samplable distribution with
    Shannon entropy $h(\cdot)$ such that $h(n)-\log(S(n)) \geq 4\log n$;
  - The existentence of a $S(\cdot)$-sparse language $L \in
    \NP$, that is
    hard-on-average with respect to some samplable distribution with
    Shannon entropy $h(\cdot)$ such that $h(n)-\log(S(n)) \geq n/3$;
  - The existence of one-way functions.

Our results are insipired by, and generalize, the recent elegant paper by Ilango,
Ren and Santhanam (ECCC'21), which presents similar characterizations for
concrete sparse languages.</summary>
    <updated>2021-07-02T01:24:40Z</updated>
    <published>2021-07-02T01:24:40Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-08T11:20:37Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-4580221543262282386</id>
    <link href="https://blog.computationalcomplexity.org/feeds/4580221543262282386/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/07/intersecting-classes.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4580221543262282386" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4580221543262282386" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/07/intersecting-classes.html" rel="alternate" type="text/html"/>
    <title>Intersecting Classes</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>If you have two complexity classes that have complete sets, the intersection might not, for example NP â© co-NP. The world of total-function classes acts differently.</p><p>Christos Papadimitriou and others defined a <a href="https://en.wikipedia.org/wiki/TFNP">number of classes</a> based on finding solutions to problems where solutions are known to exists for some combinatorial reason. While TFNP, the set of all such problems, might not have complete sets, all the other classes are defined basically based on the complete search problem for the class, such as <a href="https://en.wikipedia.org/wiki/TFNP#PLS">PLS</a>, finding a local minimum. If you have two such classes <b><span style="font-family: Cedarville Cursive;">A</span></b> and <b><span style="font-family: Cedarville Cursive;">B</span></b> with complete search problems A and B define the search problem D asÂ </p><p style="text-align: center;">D(x,y): Find a solution to either A(x) or B(y)</p><p>D is complete for the intersection ofÂ <b><span style="font-family: Cedarville Cursive;">A</span></b>Â andÂ <span style="font-family: Cedarville Cursive; font-weight: bold;">B</span><span style="font-family: inherit;">:</span><span style="font-family: inherit;"><span style="font-family: inherit;">Â </span>First theÂ </span><span style="font-family: inherit;">pr</span>oblem D is in <b><span style="font-family: Cedarville Cursive;">A</span></b> since you can reduce the problem of finding a solution to either A or B to finding a solution to A. Likewise D is in <span style="font-family: Cedarville Cursive;"><b>B</b></span>.</p><p>Suppose you have a problem Z inÂ <b><span style="font-family: Cedarville Cursive;">A</span></b>â©<span style="font-family: Cedarville Cursive; font-weight: bold;">B</span>.<span style="font-family: inherit;">Â Then since Z is in </span><b><span style="font-family: Cedarville Cursive;">A</span></b><span style="font-family: inherit;"> and A is complete for </span><span style="font-family: Cedarville Cursive;">A</span><span style="font-family: inherit;">, finding a solution to Z(u) reduces a finding a solution of A(x) where x is easily computed from u. Likewise Z(u) reduces to finding a solution of B(y). So whatever solution D(x,y) gives you, it allows you to find a solution to Z(u). Thus D is complete forÂ </span><b><span style="font-family: Cedarville Cursive;">A</span></b>â©<span style="font-family: Cedarville Cursive; font-weight: bold;">B</span><span>.</span></p><p>Some people nerd out to <a href="https://mars.nasa.gov/technology/helicopter">helicopters on mars</a>. I nerd out to the complexity of complete sets.Â </p><p>I learned about complete sets of intersections of total function classes from the talk by one ofÂ <a href="http://acm-stoc.org/stoc2021/STOCprogram.html">last week's STOC</a>Â best paper awardees,Â <a href="https://doi.org/10.1145/3406325.3451052">The Complexity of Gradient Descent</a>Â by John Fearnley, Paul W. Goldberg, Alexandros Hollender and Rahul Savani. The part above was well known but the paper goes much further.</p><p>Consider <a href="https://blog.computationalcomplexity.org/2005/12/what-is-ppad.html">PPAD</a> famously with Nash Equilibrium as a complete problem and PLS. PPAD â© PLS has complete sets by the argument above. But we can go further.</p><p>The class <a href="https://en.wikipedia.org/wiki/TFNP#CLS">CLS</a>Â is a variation of PLS where you find a local minimum in a continuous domain underÂ some Lipschitz conditions and is known to sit in the intersection of PPAD and PLS. Fearnley et al. look at finding a minimum using gradient descent (the main tool for deep learning), and showing not only is it CLS-compete but complete for PPAD â© PLS. As a consequence CLS = PPAD â© PLS. Pretty cool stuff.</p></div>
    </content>
    <updated>2021-07-01T14:44:00Z</updated>
    <published>2021-07-01T14:44:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-07-08T03:26:10Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/06/30/linkage</id>
    <link href="https://11011110.github.io/blog/2021/06/30/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Flow lines (\(\mathbb{M}\)). Web gadget editable open source code thingy to draw streamlines of mathematical formulas, in svg format, by Maksim Surguy.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://maxoffsky.com/code-blog/flow-lines/">Flow lines</a> (<a href="https://mathstodon.xyz/@11011110/106428733923482392">\(\mathbb{M}\)</a>). Web gadget editable open source code thingy to draw streamlines of mathematical formulas, in svg format, by Maksim Surguy.</p>
  </li>
  <li>
    <p><a href="https://daily.jstor.org/the-soap-bubble-trope/">The soap bubble trope</a> (<a href="https://mathstodon.xyz/@11011110/106430498906810187">\(\mathbb{M}\)</a>, <a href="https://3quarksdaily.com/3quarksdaily/2021/06/soap-bubbles.html">via</a>). Soap bubbles as a recurring theme in art, literature, and popular culture, including âthe roof of the Munich Olympic Stadium, Glinda the Good Witch, the first viral ad campaign of the late Victorian era, and morose Dutch still-life paintingsâ.</p>
  </li>
  <li>
    <p><a href="http://gosper.org/homeplate.html">Officially, home plate doesnât exist</a> (<a href="https://mathstodon.xyz/@esoterica/106435964222477352">\(\mathbb{M}\)</a>). The rules of baseball define it as a 90-45-90-90-45 pentagon with two 12â sides at one of the right angles and a 17â side between the other two, not possible.</p>
  </li>
  <li>
    <p><a href="https://www.natureindex.com/news-blog/microsoft-academic-graph-discontinued-whats-next">Microsoft Academic Graph being discontinued</a> (<a href="https://mathstodon.xyz/@11011110/106438809410223323">\(\mathbb{M}\)</a>, <a href="https://retractionwatch.com/2021/06/19/weekend-reads-biotech-ceo-on-leave-after-allegations-on-pubpeer-a-researcher-disavows-his-own-paper-plagiarism-here-there-and-everywhere/">via</a>). I didnât much use that one but I live in fear that one day Google will do the same thing to Google Scholar, as they have to so many other useful but nonprofitable Google services.</p>
  </li>
  <li>
    <p>My current workflow for preparing technical talk videos (<a href="https://mathstodon.xyz/@11011110/106444988062063872">\(\mathbb{M}\)</a>):</p>

    <ul>
      <li>
        <p>Use LaTeX+beamer (169 option) to make pdf talk slides</p>
      </li>
      <li>
        <p>For each slide, print open-in-Preview with custom 16x9 zero-margin layout then export to png</p>
      </li>
      <li>
        <p>Write a script and use quicktime to record 1-2 minute voiceover clips</p>
      </li>
      <li>
        <p>Compose slides and audio in iMovie, export to a huge mp4</p>
      </li>
      <li>
        <p>Use Handbrake to convert to reasonably-sized mp4</p>
      </li>
    </ul>

    <p>It works, but is a bit tedious and produces very dry results. The discussion includes suggestion of alternatives.</p>
  </li>
  <li>
    <p><a href="https://youtu.be/7vEgc7cNarI">The points rotated, and the lines danced</a> (<a href="https://mastodon.social/@sarielhp/106439137323288004">\(\mathbb{M}\)</a>). Video illustrating point-line duality by Sariel Har-Peled.</p>
  </li>
  <li>
    <p><a href="https://www.bbc.com/future/article/20210616-how-the-forgotten-tricks-of-letterlocking-shaped-history">Letterlocking</a> (<a href="https://mathstodon.xyz/@11011110/106458633659042049">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=27549256">via</a>, <a href="https://en.wikipedia.org/wiki/Letterlocking">see also</a>): the art of folding your letters so intricately that readers will be forced to tear the paper to unfold and read them.</p>
  </li>
  <li>
    <p><a href="https://blog.computationalcomplexity.org/2021/06/i-went-to-debate-about-program-verif.html">Bill Gasarch summarizes an online debate</a> (<a href="https://mathstodon.xyz/@11011110/106463907058053228">\(\mathbb{M}\)</a>) with Richard DeMillo and Richard Lipton, moderated by Harry Lewis, looking back at the idea of proving programs correct and at <a href="https://doi.org/10.1145/359104.359106">a classic 1979 paper by DeMillo, Lipton, and Perlis</a> arguing that this idea was already problematic.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/X_%2B_Y_sorting">\(X+Y\) sorting</a> (<a href="https://mathstodon.xyz/@11011110/106468019908924208">\(\mathbb{M}\)</a>), now a Good Article on Wikipedia. This is on an old open problem in comparison sorting: can you sort pairs of elements from two sets by their sums, faster than unstructured data of the same length? Itâs still an active topic of research; see e.g. <a href="https://doi.org/10.1145%2F3285953">Kane, Lovett, and Moran, âNear-optimal linear decision trees for \(k\)-sum and related problemsâ, <em>JACM</em> 2019</a>.</p>

    <p>It was not easy to persuade the GA reviewer that this article was as accessible as it could be. I have hopes of <a href="https://en.wikipedia.org/wiki/Dehn_invariant">Dehn invariant</a> also becoming a Good Article but its âRealizabilityâ section is far more advanced.</p>
  </li>
  <li>
    <p>This week I participated in the International Workshop on Graph-Theoretic Concepts in Computer Science, WG (<a href="https://mathstodon.xyz/@11011110/106474212936524737">\(\mathbb{M}\)</a>). The 9-hour time difference made live participation awkward for me, but fortunately prerecorded contributed talks and the three invited talks (DujmoviÄ on product structures, Samotij on independent set numeration, and Bonnet on twin-width) are linked from <a href="https://wg2021.mimuw.edu.pl/program/">the conference program</a>. The proceedings is not yet out but many preprints of papers are also linked.</p>
  </li>
  <li>
    <p><a href="https://theintercept.com/2021/06/23/anming-hu-trial-fbi-china/">âA juror says the FBI owes an apology to University of Tennessee scientist Anming Huâ</a> (<a href="https://mathstodon.xyz/@11011110/106481625501499687">\(\mathbb{M}\)</a>, <a href="https://retractionwatch.com/2021/06/25/weekend-reads-the-obesity-wars-and-the-education-of-a-researcher-zombie-research-hijacked-journals/">via</a>) after putting Hu on trial for allegedly hiding ties to China despite his repeated disclosures of those ties and possibly in retaliation for his refusal to become a spy in China for the FBI. Beyond hurting US research both directly and by motivating good people to go elsewhere, this racist witch hunt has provided fuel for Chinese propaganda.</p>
  </li>
  <li>
    <p>The Wikipedia âBook:â namespace for curated collections of articles is being killed off (<a href="https://mathstodon.xyz/@11011110/106484876399456966">\(\mathbb{M}\)</a>) after the software to collate them into pdfs stopped working. I created five of these, and used two as readings for my courses. All five have moved to my user space:</p>

    <ul>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/User:David_Eppstein/Fundamental_Data_Structures"><em>Fundamental Data Structures</em></a></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/User:David_Eppstein/Graph_Algorithms"><em>Graph Algorithms</em></a></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/User:David_Eppstein/Graph_Drawing"><em>Graph Drawing</em></a></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/User:David_Eppstein/Matroid_Theory"><em>Matroid Theory</em></a></p>
      </li>
      <li>
        <p><a href="https://en.wikipedia.org/wiki/User:David_Eppstein/Perfect_Graphs"><em>Perfect Graphs</em></a></p>
      </li>
    </ul>
  </li>
  <li>
    <p><a href="https://danilafe.com/blog/math_rendering_is_wrong/">Math rendering is wrong</a> (<a href="https://mathstodon.xyz/@11011110/106492765314826160">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=27656446">via</a>). This blog post from a year ago argues that, for the same reasons one might write a web site in a markup language before compiling it to html, we should also compile LaTeX to html at that time rather than using browser-side scripts (as in most deploys of MathJax or KaTeX) or conversion to images (Wikipedia). It doesnât present a solution, but is more a call for that solution to be made.</p>
  </li>
  <li>
    <p><a href="https://kleinbottle.com/#AMAZON%20BRAND%20HIJACKING">Amazon stands by and does nothing as Chinese scammers hijack Cliff Stollâs Klein bottle business to usurp its positive reviews</a> (<a href="https://mathstodon.xyz/@11011110/106500881370367192">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=27684807">via</a>).</p>
  </li>
</ul></div>
    </content>
    <updated>2021-06-30T10:36:00Z</updated>
    <published>2021-06-30T10:36:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-06-30T17:39:13Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18933</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/06/29/scaling-and-fame/" rel="alternate" type="text/html"/>
    <title>Scaling and Fame</title>
    <summary>Scaling the pandemic is different from scaling the US budget Sydney Morning Herald interview source Terence Tao is now âproperlyâ famous. He was cited earlier this month in the NYT science section for help in explaining large numbers. Numbers such as the US federal budget. Today we discuss caveats on such explanations, after a riff [â¦]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Scaling the pandemic is different from scaling the US budget</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/06/29/scaling-and-fame/tt/" rel="attachment wp-att-18935"><img alt="" class="alignright wp-image-18935" height="126" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/tt.png?resize=225%2C126&amp;ssl=1" width="225"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Sydney Morning Herald interview <a href="https://www.smh.com.au/lifestyle/terence-tao-the-mozart-of-maths-20150216-13fwcv.html">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Terence Tao is now âproperlyâ famous. He was cited earlier this month in the NYT <a href="https://www.nytimes.com/2021/06/17/science/math-numbers-federal-budget-tao.html">science</a> section for help in explaining large numbers. Numbers such as the US federal budget.</p>
<p>
Today we discuss caveats on such explanations, after a riff on the popular explanation of mathematics.</p>
<p>
Regarding that, let us forget Taoâs work on primes in progressions with Ben Green, forget the ErdÅs discrepancy problem, and forget his almost-resolution of the Collatz conjecture. Forget it all. Better than a headline, he got his name embedded into the NYT articleâs URL. This was for something much less deep that he wrote in 2009âas a blogger. </p>
<p>
<em>En passant</em>, we mention that Ken has an event tomorrow (Wed. 6/30) at 3:30 ET. It is a webinar hosted by Marc Rotenberg, who heads the Washington-based Center for AI and Digital Policy (<a href="https://www.caidp.org">CAIDP</a>), on âChess and AI: The Role of Transparency.â Registration is free at this <a href="https://www.caidp.org/events/chess/">link</a>. One aspect of transparency in Kenâs work is that he writes about his modelâs methodology hereâas a blogger.</p>
<p>
</p><p/><h2> Rescaling the Budget </h2><p/>
<p/><p>
Tao was referenced for a <a href="https://terrytao.wordpress.com/2009/05/04/the-federal-budget-rescaled/">post</a> he wrote in May 2009 when Barack Obama was working on his first budget as President. The ratio of $100 million to $3 that he used scaled the budget income to about $75,000. </p>
<p>
With Joe Biden engaged in budget deliberations, Aiyana Green and Steven Strogatz wrote the NYT <a href="https://www.human.cornell.edu/pam/news/aiyana_green_nyt">article</a> on explaining the US federal budget. Green is a student at Cornell: she just completed her junior year in the Department of Policy Analysis and Management. </p>
<p/><p><br/>
<a href="https://rjlipton.wpcomstaging.com/2021/06/29/scaling-and-fame/agss/" rel="attachment wp-att-18936"><img alt="" class="aligncenter wp-image-18936" height="200" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/AGSS.png?resize=355%2C200&amp;ssl=1" width="355"/></a></p>
<p/><p><br/>
They updated Taoâs post to scale the income to $100,000. Besides being a round number, this is close to the estimated <a href="https://www.in2013dollars.com/us/inflation/2009">inflation since 2009</a>. Here is the NYT graphic of the numbers. </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/06/29/scaling-and-fame/bud-2/" rel="attachment wp-att-18938"><img alt="" class="aligncenter wp-image-18938" height="500" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/06/bud.png?resize=545%2C500&amp;ssl=1" width="545"/></a></p>
<p>
</p><p/><h2> A Scaling Caveat </h2><p/>
<p/><p>
It is attractive to apply this scaling trick elsewhere, even to grim subjects like the coronavirus pandemic. But there we find an element that does not scale.</p>
<p>
Suppose we use the same figure of 100,000 to scale down the worldâs population. Besides its famous pandemic numbers <a href="https://www.worldometers.info/coronavirus/">pages</a>, Worldometer also keeps a running <a href="https://www.worldometers.info/world-population/">estimate</a> of the total world population, now nearing 7.9 billion. Scaling down means multiplying every ther human number by <img alt="{\gamma =}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cgamma+%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> 0.000012697445274. </p>
<p>
We can think of 100,000 people as a city that is not a metropolis. Scaling down the current pandemic figures, we get:</p>
<ul>
<li>
182,000,000 total cases become <b>2,309</b>. <p/>
</li><li>
11,496,147 active cases become <b>145</b>. <p/>
</li><li>
4 million total deaths (the numbers are approaching that millstone as we write) become <b>50</b> deaths. <p/>
</li><li>
80,346 currently listed in critical or serious condition become <b>exactly one</b>.
</li></ul>
<p>
These numbers are not at all unusual for our size of city if one considers all kinds of illness and mortality. The scaling trick may seem to have reduced the scope of the pandemic, as opposed to statements such as 182 million being over half the US population. Yet in terms of the raw numbers it preserves the proportions.</p>
<p>
What the scaling doesnât preserve is the proportion of <em>relations</em>. The number of possible binary relationsâperson <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> knows person <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>âis quadratic in the number <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of people. Suppose the number of pairs who know each other is <img alt="{an^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ban%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> where <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a small but fixed constant. (Note: we will redo this with something more reasonable below.) If we then scale <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> down to <img alt="{n' = n\gamma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%27+%3D+n%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, the situation becomes:</p>
<ul>
<li>
If we estimate the relatedness of our city, we get <img alt="{an'^2 = an^2\gamma^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ban%27%5E2+%3D+an%5E2%5Cgamma%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. <p/>
</li><li>
But if we scaled down the number of <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-knows-<img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> relations directly we would get <img alt="{an^2\gamma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ban%5E2%5Cgamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which is substantially bigger by a factor of <img alt="{\frac{1}{\gamma}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B%5Cgamma%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.
</li></ul>
<p>
Thus what the scaling really underestimates is the impact of how people are affected by their loved ones being among the 182 million (or the worse numbers). The underestimation logic applies to any form <img alt="{an^c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ban%5Ec%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> where <img alt="{c &gt; 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%3E+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. This is not an issue with the budget because dollar bills donât feel relatednessâat least not so much, even absent a line-item veto. Now we will make values of <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> approaching <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> more reasonable.</p>
<p>
</p><p/><h2> Small World: Tao and Strogatz Again </h2><p/>
<p/><p>
Letâs consider people <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> who have <img alt="{3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> degrees of separation in the graph of who-knows-who. Then there are <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> who know each other such that <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> knows <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> knows <img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. If something strikes <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, then <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> will feel a deep connection by that impact. The feeling is amplified if <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> have multiple pairs <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that make a path. This quantifies the relation that Tao calls âawarenessâ in his âLecture Notes 3 FOR 254Aâ course <a href="https://rjlipton.wpcomstaging.com/feed/LECTURE NOTES 3 FOR 254A">notes</a> (pages 51â57 overall). A fact way more basic than what Tao is actually talking about in those notes is the following:</p>
<blockquote><p><b> </b> <em> The sum over pairs <img alt="{(X,Y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28X%2CY%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> of the number of pairs <img alt="{(A,B)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28A%2CB%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> between them who would be affected equals the sum over edges <img alt="{(A,B)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28A%2CB%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> of the number of pairs <img alt="{(X,Y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28X%2CY%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> they can be struck by: both are equal to the number of paths of length <img alt="{3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> in the graph. </em>
</p></blockquote>
<p/><p>
That number of paths is what we say is reasonable to model by a function <img alt="{an^c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ban%5Ec%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> with <img alt="{c \gg 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc+%5Cgg+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. This is the simplest way of approaching why we feel that treating the pandemic like the US budget underestimates its human effect. One can still rebut that other kinds of illness and death have the same scaling properties, but ultimately we are talking about the <em>excess</em> caused by the pandemicâthe effect on top of everything else.</p>
<p>
We have not tried to make this analysis become rigorous using more-realistic models of human networks. Perhaps our readers can point us to such analysis. But one inkling of why we expect our point to be borne out comes from a key conclusion of the famous 1998 <a href="https://www.nature.com/articles/30918">paper</a> of Strogatz with Duncan Watts on âsmall-worldâ networks: The phase transition from a lattice network with large average distances to a small-world network with small distances takes place in a range where small clusters cannot recognize it happening locally. </p>
<p>
Thus, if our scaling carried the intuitive picture of an isolated city, it would miss the expanding sphere of relations. At the opposite extreme would be taking the union of Monaco and central Venice, which sum to 100,000 people who fan out mightily. There is also the argument that while small-world networks are held tight by â<a href="https://sociology.stanford.edu/sites/g/files/sbiybj9501/f/publications/the_strength_of_weak_ties_and_exch_w-gans.pdf">weak ties</a>,â the shared knowledge of misery is something that most tends to strengthen ties. And of course, our point about relations extends to many other activities impacted by the pandemic.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
What should govern the appropriateness of scaling down?</p>
<p>
It should be noted that if we scale down the US to 100,000 people, the numbers are appreciably higher:</p>
<ul>
<li>
34.5 million total cases become <b>10,366</b>. <p/>
</li><li>
4,928,564 active cases become <b>1,480</b>. <p/>
</li><li>
620,000 total deaths become <b>186</b> deaths. <p/>
</li><li>
3,833 currently listed in critical or serious condition still become <b>exactly one</b>.
</li></ul></font></font></div>
    </content>
    <updated>2021-06-29T23:09:46Z</updated>
    <published>2021-06-29T23:09:46Z</published>
    <category term="All Posts"/>
    <category term="chess"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="Aiyana Green"/>
    <category term="coronavirus"/>
    <category term="mathematical writing"/>
    <category term="New York Times"/>
    <category term="pandemic"/>
    <category term="scaling"/>
    <category term="Steven Strogatz"/>
    <category term="Terence Tao"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>GÃ¶del's Lost Letter and P=NP</title>
      <updated>2021-07-08T11:20:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=2420</id>
    <link href="https://theorydish.blog/2021/06/29/trace-reconstruction/" rel="alternate" type="text/html"/>
    <title>Trace Reconstruction from Complex Analysis</title>
    <summary>Suppose that is an unknown binary string of length . We are asked to recover from its traces, and each trace is a random subsequence obtained by deleting the bits of independently with probability . More formally, let denote the distribution of traces obtained from string . For example, when , assigns a probability mass of to each element in the multiset We then ask: what is the smallest number such that we can recover any (say, with probability ) given independent samples from ? This trace reconstruction problem was first formulated by Batu-Kannan-Khanna-McGregor in 2004, and their central motivation is from the multiple sequence alignment problem in computational biology. Trace reconstruction is also a fundamental problem related to the deletion channel in communication theory: We view the hidden string as the transmitted message, and each trace as a received message that went through a deletion channel, which drops each bit with probability . Then the sample complexity tells us the number of independent copies that need to be sent for the receiver to determine the original message. Despite being a natural problem, trace reconstruction is still far from being well-understood, even from the information-theoretic perspective (i.e., without considering the [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Suppose that <img alt="s" class="latex" src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is an unknown binary string of length <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. We are asked to recover <img alt="s" class="latex" src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> from its <em>traces</em>, and each <em>trace</em> <img alt="\tilde s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a random subsequence obtained by deleting the bits of <img alt="s" class="latex" src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> independently with probability <img alt="1/2" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>



<p>More formally, let <img alt="\mathcal{D}_s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> denote the distribution of traces obtained from string <img alt="s" class="latex" src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. For example, when <img alt="s = 110" class="latex" src="https://s0.wp.com/latex.php?latex=s+%3D+110&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="\mathcal{D}_s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> assigns a probability mass of <img alt="1/8" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F8&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to each element in the multiset<br/></p>



<p class="has-text-align-center"><img alt="\{\text{empty}, 1, 1, 0, 11, 10, 10, 110\}." class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Ctext%7Bempty%7D%2C+1%2C+1%2C+0%2C+11%2C+10%2C+10%2C+110%5C%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/><br/></p>



<p>We then ask: what is the smallest number <img alt="M(n)" class="latex" src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> such that we can recover any <img alt="s \in \{0, 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=s+%5Cin+%5C%7B0%2C+1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (say, with probability <img alt="\ge 0.99" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cge+0.99&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>) given <img alt="M(n)" class="latex" src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> independent samples from <img alt="\mathcal{D}_s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>?</p>



<p>This <em>trace reconstruction</em> problem was first formulated by <a href="https://people.cs.umass.edu/~mcgregor/papers/04-soda.pdf" rel="noreferrer noopener" target="_blank">Batu-Kannan-Khanna-McGregor</a> in 2004, and their central motivation is from the <a href="https://en.wikipedia.org/wiki/Multiple_sequence_alignment" rel="noreferrer noopener" target="_blank">multiple sequence alignment</a> problem in computational biology. Trace reconstruction is also a fundamental problem related to the <a href="https://en.wikipedia.org/wiki/Deletion_channel" rel="noreferrer noopener" target="_blank">deletion channel</a> in communication theory: We view the hidden string as the transmitted message, and each trace as a received message that went through a deletion channel, which drops each bit with probability <img alt="1/2" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Then the sample complexity <img alt="M(n)" class="latex" src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> tells us the number of independent copies that need to be sent for the receiver to determine the original message.</p>



<p>Despite being a natural problem, trace reconstruction is still far from being well-understood, even from the information-theoretic perspective (i.e., without considering the computational complexity). In 2017, <a href="https://arxiv.org/pdf/1612.03148.pdf" rel="noreferrer noopener" target="_blank">De-OâDonnell-Servedio</a> and <a href="https://arxiv.org/pdf/1612.03599.pdf" rel="noreferrer noopener" target="_blank">Nazarov-Peres</a> independently proved <img alt="M(n) \le \exp(O(n^{1/3}))" class="latex" src="https://s0.wp.com/latex.php?latex=M%28n%29+%5Cle+%5Cexp%28O%28n%5E%7B1%2F3%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. A very recent breakthrough due to <a href="https://arxiv.org/pdf/2009.03296.pdf" rel="noreferrer noopener" target="_blank">Chase</a> further improved this bound to <img alt="\exp(\tilde O(n^{1/5}))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28%5Ctilde+O%28n%5E%7B1%2F5%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which is still super-polynomial. On the other hand, the best known sample complexity lower bound is merely <img alt="\tilde\Omega(n^{3/2})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde%5COmega%28n%5E%7B3%2F2%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, proved by <a href="https://arxiv.org/pdf/1905.03031.pdf" rel="noreferrer noopener" target="_blank">Chase</a> in another recent work.</p>



<p>In this blog post, I will explain why this problem is much more non-trivial than it might appear at first glance. I will also give an overview on the work of [DOS17, NP17], which, interestingly, reduces this seemingly combinatorial problem to complex analysis.</p>



<p><strong>Observation: reconstruction <img alt="\approx" class="latex" src="https://s0.wp.com/latex.php?latex=%5Capprox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> distinguishing <img alt="\approx" class="latex" src="https://s0.wp.com/latex.php?latex=%5Capprox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> TV-distance.</strong> Let us start with a natural first attempt at the problem. Define <img alt="\delta_n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> as the minimum statistical distance between the trace distribution of two different length-<img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> strings:<br/></p>



<p class="has-text-align-center"><img alt="\delta_n = \min_{x \ne y \in \{0, 1\}^n}d_{\textrm{TV}}(\mathcal{D}_x, \mathcal{D}_y)." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta_n+%3D+%5Cmin_%7Bx+%5Cne+y+%5Cin+%5C%7B0%2C+1%5C%7D%5En%7Dd_%7B%5Ctextrm%7BTV%7D%7D%28%5Cmathcal%7BD%7D_x%2C+%5Cmathcal%7BD%7D_y%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/><br/></p>



<p>It is not hard to show that <img alt="1/\delta_n" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> bounds <img alt="M(n)" class="latex" src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> on both sides, up to a polynomial factor:<br/></p>



<p class="has-text-align-center"><img alt="1/\delta_n \lesssim M(n) \lesssim n/\delta_n^2." class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Cdelta_n+%5Clesssim+M%28n%29+%5Clesssim+n%2F%5Cdelta_n%5E2.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/><br/></p>



<p class="has-black-color has-text-color">The lower bound holds because any trace reconstruction algorithm must be able to distinguish <img alt="s = x" class="latex" src="https://s0.wp.com/latex.php?latex=s+%3D+x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> from <img alt="s = y" class="latex" src="https://s0.wp.com/latex.php?latex=s+%3D+y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for every pair of different strings <img alt="(x, y)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2C+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and this requires <img alt="\Omega(1 / \delta_n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5COmega%281+%2F+%5Cdelta_n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> samples for the minimizer <img alt="(x, y)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2C+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in the definition of <img alt="\delta_n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. For the upper bound, we note that every pair <img alt="(x, y)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2C+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> can be distinguished with an <img alt="o(2^{-n})" class="latex" src="https://s0.wp.com/latex.php?latex=o%282%5E%7B-n%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> error probability using <img alt="O(n/\delta_n^2)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28n%2F%5Cdelta_n%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> samples. We say that string <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> âbeatsâ <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, if the distinguisher for <img alt="(x, y)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2C+y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> decides that â<img alt="s = x" class="latex" src="https://s0.wp.com/latex.php?latex=s+%3D+x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>â. By a union bound, the correct answer <img alt="s" class="latex" src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> âbeatsâ every other string with high probability. We can then obtain a reconstruction algorithm by running the distinguisher for every string pair, and outputting the unique string that âbeatsâ the other <img alt="2^n-1" class="latex" src="https://s0.wp.com/latex.php?latex=2%5En-1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> strings.</p>



<p>Thus, to determine whether the sample complexity <img alt="M(n)" class="latex" src="https://s0.wp.com/latex.php?latex=M%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is polynomial in <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, it suffices to determine whether <img alt="\delta_n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> scales as <img alt="1/\mathrm{poly}(n)" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Cmathrm%7Bpoly%7D%28n%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> or is much smaller. Unfortunately, it turns out to be highly non-trivial to bound <img alt="\delta_n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and even bounding <img alt="d_{\textrm{TV}}(\mathcal{D}_x, \mathcal{D}_y)" class="latex" src="https://s0.wp.com/latex.php?latex=d_%7B%5Ctextrm%7BTV%7D%7D%28%5Cmathcal%7BD%7D_x%2C+%5Cmathcal%7BD%7D_y%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for âsimpleâ <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> can be hard. Imagine that we try to reason about the distribution <img alt="\mathcal{D}_x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>: the probability mass that <img alt="\mathcal{D}_x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> assigns to string <img alt="\tilde s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is proportional to the number of times that <img alt="\tilde s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> appears as a subsequence in <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, but this count is already hard to express or control, unless <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> has a very simple pattern. This is roughly where this natural attempt gets stuck.</p>



<p><strong>Distinguishing using estimators.</strong> Now we turn to a different approach that underlies the recent breakthrough on trace reconstruction algorithms. As discussed earlier, we can focus on the problem of distinguishing the case <img alt="s = x" class="latex" src="https://s0.wp.com/latex.php?latex=s+%3D+x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> from <img alt="s = y" class="latex" src="https://s0.wp.com/latex.php?latex=s+%3D+y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for fixed strings <img alt="x \ne y \in \{0, 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cne+y+%5Cin+%5C%7B0%2C+1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Let <img alt="f: \{0, 1\}^{\le n} \to \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=f%3A+%5C%7B0%2C+1%5C%7D%5E%7B%5Cle+n%7D+%5Cto+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be a function defined over all possible traces from a length-<img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> string. We consider the following algorithm for distinguishing <img alt="\mathcal{D}_x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> from <img alt="\mathcal{D}_y" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>:<br/><br/><strong>Step 1.</strong> Given traces <img alt="\tilde s_1, \tilde s_2, \ldots" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+s_1%2C+%5Ctilde+s_2%2C+%5Cldots&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, compute the average of <img alt="f(\tilde s_1), f(\tilde s_2), \ldots" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Ctilde+s_1%29%2C+f%28%5Ctilde+s_2%29%2C+%5Cldots&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/><br/><strong>Step 2.</strong> Output <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> if the average is closer to <img alt="\mathbb{E}_{\tilde x \sim \mathcal{D}_x}[f(\tilde x)]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Ctilde+x+%5Csim+%5Cmathcal%7BD%7D_x%7D%5Bf%28%5Ctilde+x%29%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> than to <img alt="\mathbb{E}_{\tilde y \sim \mathcal{D}_y}[f(\tilde y)]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Ctilde+y+%5Csim+%5Cmathcal%7BD%7D_y%7D%5Bf%28%5Ctilde+y%29%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and output <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> otherwise.<br/></p>



<p>For the above to succeed, <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> needs to satisfy the following two conditions:<br/></p>



<p><strong>(Separation)</strong> <img alt="\mathcal{D}_x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="\mathcal{D}_y" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are well-separated under <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, namely <img alt="|\mathbb{E}[f(\tilde x)] - \mathbb{E}[f(\tilde y)]| \ge \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cmathbb%7BE%7D%5Bf%28%5Ctilde+x%29%5D+-+%5Cmathbb%7BE%7D%5Bf%28%5Ctilde+y%29%5D%7C+%5Cge+%5Cepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for some <img alt="\epsilon &gt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3E+0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.<br/><strong>(Boundedness)</strong> Expectation of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> can be efficiently estimated. This can be guaranteed if for some <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="|f(\tilde s)| \le B" class="latex" src="https://s0.wp.com/latex.php?latex=%7Cf%28%5Ctilde+s%29%7C+%5Cle+B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> holds for every <img alt="\tilde s \in \{0, 1\}^{\le n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+s+%5Cin+%5C%7B0%2C+1%5C%7D%5E%7B%5Cle+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.<br/></p>



<p>Assuming the above, a standard concentration argument shows that we can distinguish <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> using <img alt="O((B/\epsilon)^2)" class="latex" src="https://s0.wp.com/latex.php?latex=O%28%28B%2F%5Cepsilon%29%5E2%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> samples, and thus <img alt="M(n) \lesssim n \cdot (B/\epsilon)^2" class="latex" src="https://s0.wp.com/latex.php?latex=M%28n%29+%5Clesssim+n+%5Ccdot+%28B%2F%5Cepsilon%29%5E2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>



<p>In principle, a near-optimal choice of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> would be setting <img alt="f(\tilde s)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Ctilde+s%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to be the indicator of <img alt="\mathcal{D}_x(\tilde s) \ge \mathcal{D}_y(\tilde s)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D_x%28%5Ctilde+s%29+%5Cge+%5Cmathcal%7BD%7D_y%28%5Ctilde+s%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which gives boundedness <img alt="B = 1" class="latex" src="https://s0.wp.com/latex.php?latex=B+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and separation <img alt="\epsilon = d_{\textrm{TV}}(\mathcal{D}_x, \mathcal{D}_y) \ge \delta_n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3D+d_%7B%5Ctextrm%7BTV%7D%7D%28%5Cmathcal%7BD%7D_x%2C+%5Cmathcal%7BD%7D_y%29+%5Cge+%5Cdelta_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. However, as argued above, this optimal choice of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> can still be hard to analyze. Instead, we focus on choosing a simpler function <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which potentially gives a suboptimal <img alt="B/\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=B%2F%5Cepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> but admits simple analyses.</p>



<p><strong>Sketch of the <img alt="\exp(O(n^{1/3}))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28O%28n%5E%7B1%2F3%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Upper Bound.</strong> The main results of [DOS17] and [NP17] follow from choosing <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to be a linear function. Given a trace <img alt="\tilde s = \tilde s_0 \tilde s_1 \tilde s_2 \cdots" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+s+%3D+%5Ctilde+s_0+%5Ctilde+s_1+%5Ctilde+s_2+%5Ccdots&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, we consider the following polynomial with coefficients being the bits of <img alt="\tilde s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>:<br/></p>



<p class="has-text-align-center"><img alt="\tilde S(z) = \sum_{k}\tilde s_k z^k = \tilde s_0 + \tilde s_1 z + \tilde s_2 z^2 + \cdots." class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+S%28z%29+%3D+%5Csum_%7Bk%7D%5Ctilde+s_k+z%5Ek+%3D+%5Ctilde+s_0+%2B+%5Ctilde+s_1+z+%2B+%5Ctilde+s_2+z%5E2+%2B+%5Ccdots.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/><br/></p>



<p>For some number <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to be determined later, we consider the estimator <img alt="f(\tilde s) = \tilde S(z) = \tilde s_0 + \tilde s_1 z + \tilde s_2 z^2 + \cdots" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Ctilde+s%29+%3D+%5Ctilde+S%28z%29+%3D+%5Ctilde+s_0+%2B+%5Ctilde+s_1+z+%2B+%5Ctilde+s_2+z%5E2+%2B+%5Ccdots&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which is indeed a linear function in the trace <img alt="\tilde s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>



<p>At first glance, it might be unclear why we choose the coefficients to be powers of <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The following fact justifies this choice by showing that polynomials interact with the deletion channel very nicely: The expectation of <img alt="\tilde S(z)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+S%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is exactly the evaluation of the polynomial <img alt="S(\cdot)" class="latex" src="https://s0.wp.com/latex.php?latex=S%28%5Ccdot%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> with coefficients <img alt="s_0, s_1, \ldots, s_{n-1}" class="latex" src="https://s0.wp.com/latex.php?latex=s_0%2C+s_1%2C+%5Cldots%2C+s_%7Bn-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, but at a slightly different point.<br/></p>



<p><strong>Fact:</strong> For any string <img alt="s \in \{0, 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=s+%5Cin+%5C%7B0%2C+1%5C%7D%5En&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>,<br/></p>



<p class="has-text-align-center"><img alt="\mathbb{E}_{\tilde s \sim \mathcal{D}_s}\left[\tilde S(z)\right] = \frac{1}{2}S\left(\frac{z+1}{2}\right)." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%5Ctilde+s+%5Csim+%5Cmathcal%7BD%7D_s%7D%5Cleft%5B%5Ctilde+S%28z%29%5Cright%5D+%3D+%5Cfrac%7B1%7D%7B2%7DS%5Cleft%28%5Cfrac%7Bz%2B1%7D%7B2%7D%5Cright%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/><br/></p>



<p>Equivalently, we have <img alt="\mathbb{E}[\tilde S(2z-1)] = \frac{1}{2}S(z)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5B%5Ctilde+S%282z-1%29%5D+%3D+%5Cfrac%7B1%7D%7B2%7DS%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which allows us to rephrase our requirements on the choice of <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> as follows:<br/></p>



<p><strong>(Separation)</strong> <img alt="|X(z) - Y(z)| \ge \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%7CX%28z%29+-+Y%28z%29%7C+%5Cge+%5Cepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for some <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that is not too small.<br/><strong>(Boundedness)</strong> <img alt="|\tilde S(2z - 1)| \le B" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Ctilde+S%282z+-+1%29%7C+%5Cle+B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for all possible trace <img alt="\tilde s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, for some <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that is not too large.<br/></p>



<p>After some thought, it is beneficial to choose <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> such that both <img alt="|z|" class="latex" src="https://s0.wp.com/latex.php?latex=%7Cz%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="|2z - 1|" class="latex" src="https://s0.wp.com/latex.php?latex=%7C2z+-+1%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are close to <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, since this ensures that different bits in the string are assigned weights of similar magnitudes in the polynomials above. These two conditions hold if and only if <img alt="z \approx 1" class="latex" src="https://s0.wp.com/latex.php?latex=z+%5Capprox+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-2442" height="230" src="https://theorydish.files.wordpress.com/2021/06/plan.png?w=1024" width="512"/>The overall plan for distinguishing strings <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</figure></div>



<p>The crucial idea in both papers [DOS17, NP17] is to consider the polynomials in the complex plane <img alt="\mathbb{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> instead of on the real line. Fortunately, all the previous discussion still holds for complex numbers, with <img alt="|\cdot|" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Ccdot%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> interpreted as modulus instead of absolute value. In [NP17], the authors chose <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> from a small arc of the unit circle:</p>



<p class="has-text-align-center"><img alt="A_L = \{e^{i\theta}: \theta\in[-\pi/L, \pi/L]\}" class="latex" src="https://s0.wp.com/latex.php?latex=A_L+%3D+%5C%7Be%5E%7Bi%5Ctheta%7D%3A+%5Ctheta%5Cin%5B-%5Cpi%2FL%2C+%5Cpi%2FL%5D%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-2445" height="262" src="https://theorydish.files.wordpress.com/2021/06/a_l.png?w=622" width="311"/>Arc <img alt="A_L" class="latex" src="https://s0.wp.com/latex.php?latex=A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> marked by the red box and dotted lines.</figure></div>



<p>For every <img alt="z \in A_L" class="latex" src="https://s0.wp.com/latex.php?latex=z+%5Cin+A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, it is easy to upper bound <img alt="|\tilde S(2z - 1)|" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Ctilde+S%282z+-+1%29%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>: it follows from simple calculus that <img alt="|2z - 1| \le 1 + O(L^{-2})" class="latex" src="https://s0.wp.com/latex.php?latex=%7C2z+-+1%7C+%5Cle+1+%2B+O%28L%5E%7B-2%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and thus for every <img alt="\tilde s" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde+s&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>:<br/></p>



<p class="has-text-align-center"><img alt="\left|\tilde S(2z-1)\right| \le \sum_{k=0}^{n-1}\left|(2z-1)^k\right| \le n \cdot \left[1 + O(L^{-2})\right]^n = e^{O(n/L^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Ctilde+S%282z-1%29%5Cright%7C+%5Cle+%5Csum_%7Bk%3D0%7D%5E%7Bn-1%7D%5Cleft%7C%282z-1%29%5Ek%5Cright%7C+%5Cle+n+%5Ccdot+%5Cleft%5B1+%2B+O%28L%5E%7B-2%7D%29%5Cright%5D%5En+%3D+e%5E%7BO%28n%2FL%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>



<p>To lower bound <img alt="X(z) - Y(z)" class="latex" src="https://s0.wp.com/latex.php?latex=X%28z%29+-+Y%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, note that since both <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are binary, all the coefficients of <img alt="X-Y" class="latex" src="https://s0.wp.com/latex.php?latex=X-Y&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are in <img alt="\{-1, 0, 1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B-1%2C+0%2C+1%5C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Such polynomials are known as <em>Littlewood polynomials</em>. The separation condition is then reduced to the following claim in complex analysis:</p>



<p class="has-text-align-center"><em><strong>Littlewood polynomials cannot to be too âflatâ over a short arc around 1.</strong></em></p>



<p>This is indeed the case:</p>



<p id="lemma-1"><strong>Lemma 1.</strong> (<a href="https://www.jstor.org/stable/pdf/24899666.pdf" rel="noreferrer noopener" target="_blank">[Borwein and ErdÃ©lyi, 1997]</a>) For every nonzero Littlewood polynomial <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>,<br/></p>



<p class="has-text-align-center"><img alt="\max_{z \in A_L}|p(z)| \ge e^{-O(L)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmax_%7Bz+%5Cin+A_L%7D%7Cp%28z%29%7C+%5Cge+e%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.<br/></p>



<p>By <a href="https://theorydish.blog/feed/#lemma-1">Lemma 1</a>, there exists <img alt="z \in A_L" class="latex" src="https://s0.wp.com/latex.php?latex=z+%5Cin+A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> such that the separation condition holds for <img alt="\epsilon = e^{-O(L)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon+%3D+e%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Recall that the boundedness holds for <img alt="B = e^{O(n/L^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=B+%3D+e%5E%7BO%28n%2FL%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. This shows that the sample complexity is upper bounded by <img alt="(B/\epsilon)^2 = \exp(O(n/L^2 + L))" class="latex" src="https://s0.wp.com/latex.php?latex=%28B%2F%5Cepsilon%29%5E2+%3D+%5Cexp%28O%28n%2FL%5E2+%2B+L%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which is minimized at <img alt="L = n^{1/3}" class="latex" src="https://s0.wp.com/latex.php?latex=L+%3D+n%5E%7B1%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. This proves the upper bound <img alt="M(n) = \exp(O(n^{1/3}))" class="latex" src="https://s0.wp.com/latex.php?latex=M%28n%29+%3D+%5Cexp%28O%28n%5E%7B1%2F3%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>



<p><strong>Proof of a weaker lemma.</strong> While the original proof of <a href="https://theorydish.blog/feed/#lemma-1">Lemma 1</a> is a bit technical, [NP17] presented a beautiful and much simpler proof of the following weaker result, in which the <img alt="e^{-O(L)}" class="latex" src="https://s0.wp.com/latex.php?latex=e%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> lower bound is replaced by <img alt="n^{-O(L)}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, where <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is the degree of the Littlewood polynomial.</p>



<p><strong>Lemma 2.</strong> ([Lemma 3.1, NP17]) For every nonzero Littlewood polynomial <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of degree <img alt="&lt; n" class="latex" src="https://s0.wp.com/latex.php?latex=%3C+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>,<br/></p>



<p class="has-text-align-center"><img alt="\max_{z \in A_L}|p(z)| \ge n^{-O(L)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmax_%7Bz+%5Cin+A_L%7D%7Cp%28z%29%7C+%5Cge+n%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.<br/></p>



<p><strong>Proof.</strong> Without loss of generality, we assume that the constant term of <img alt="p(z)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Suppose otherwise, that the lowest order term of <img alt="p(z)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is <img alt="z^m" class="latex" src="https://s0.wp.com/latex.php?latex=z%5Em&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for some <img alt="m \ge 1" class="latex" src="https://s0.wp.com/latex.php?latex=m+%5Cge+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. We may consider the polynomial <img alt="p(z) / z^m" class="latex" src="https://s0.wp.com/latex.php?latex=p%28z%29+%2F+z%5Em&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> instead.</p>



<p>Let <img alt="M = \max_{z \in A_L}|p(z)|" class="latex" src="https://s0.wp.com/latex.php?latex=M+%3D+%5Cmax_%7Bz+%5Cin+A_L%7D%7Cp%28z%29%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be the maximum modulus of <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> over arc <img alt="A_L" class="latex" src="https://s0.wp.com/latex.php?latex=A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and <img alt="\omega = e^{2\pi i/L}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega+%3D+e%5E%7B2%5Cpi+i%2FL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be an <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-th root of unity. Consider the following polynomial:<br/></p>



<p class="has-text-align-center"><img alt="q(z) = \prod_{j=0}^{L-1}p(\omega^j z) = p(z) \cdot p(\omega z) \cdot \cdots \cdot p(\omega^{L-1} z)" class="latex" src="https://s0.wp.com/latex.php?latex=q%28z%29+%3D+%5Cprod_%7Bj%3D0%7D%5E%7BL-1%7Dp%28%5Comega%5Ej+z%29+%3D+p%28z%29+%5Ccdot+p%28%5Comega+z%29+%5Ccdot+%5Ccdots+%5Ccdot+p%28%5Comega%5E%7BL-1%7D+z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>



<p>For every <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> on the unit circle, at least one point <img alt="\omega^j z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega%5Ej+z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> falls into the arc <img alt="A_L" class="latex" src="https://s0.wp.com/latex.php?latex=A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, so that <img alt="|p(\omega^j z)| \le M" class="latex" src="https://s0.wp.com/latex.php?latex=%7Cp%28%5Comega%5Ej+z%29%7C+%5Cle+M&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. (See <a href="https://theorydish.blog/feed/#windmill">figure below</a> for a proof by picture.) The moduli of the remaining <img alt="L - 1" class="latex" src="https://s0.wp.com/latex.php?latex=L+-+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> factors are trivially bounded by <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Thus, <img alt="|q(z)| \le M\cdot n^{L-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Cq%28z%29%7C+%5Cle+M%5Ccdot+n%5E%7BL-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>



<p>On the other hand, we note <img alt="q(0) = [p(0)]^L = 1" class="latex" src="https://s0.wp.com/latex.php?latex=q%280%29+%3D+%5Bp%280%29%5D%5EL+%3D+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The <a href="https://en.wikipedia.org/wiki/Maximum_modulus_principle" rel="noreferrer noopener" target="_blank">maximum modulus principle</a> implies that for some <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> on the unit circle, we have <img alt="|q(z)| \ge 1" class="latex" src="https://s0.wp.com/latex.php?latex=%7Cq%28z%29%7C+%5Cge+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Therefore, we must have <img alt="M \cdot n^{L - 1} \ge 1" class="latex" src="https://s0.wp.com/latex.php?latex=M+%5Ccdot+n%5E%7BL+-+1%7D+%5Cge+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which implies <img alt="M \ge n^{-(L - 1)} = n^{-O(L)}" class="latex" src="https://s0.wp.com/latex.php?latex=M+%5Cge+n%5E%7B-%28L+-+1%29%7D+%3D+n%5E%7B-O%28L%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and completes the proof. <img alt="\square" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csquare&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<div class="wp-block-image" id="windmill"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-2449" height="261" src="https://theorydish.files.wordpress.com/2021/06/example.png?w=924" width="462"/>Points involved in the definition of <img alt="q(z)" class="latex" src="https://s0.wp.com/latex.php?latex=q%28z%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for <img alt="L=4" class="latex" src="https://s0.wp.com/latex.php?latex=L%3D4&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.<br/>In this case, <img alt="\omega^3 z" class="latex" src="https://s0.wp.com/latex.php?latex=%5Comega%5E3+z&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is inside arc <img alt="A_L" class="latex" src="https://s0.wp.com/latex.php?latex=A_L&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which is marked by the red lines.</figure></div>



<p><strong>Beyond linear estimators?</strong> The work of [DOS17, NP17] not only proved the <img alt="\exp(O(n^{1/3}))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28O%28n%5E%7B1%2F3%7D%29%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> upper bound, but also showed that this is the best sample complexity we can get from linear estimator <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The recent work of [Cha20] goes beyond these linear estimators by taking the higher moments of the trace into account. The idea turns out to be a natural one: consider the â<img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-gramsâ (i.e., length-<img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> substrings) of the string for some <img alt="k &gt; 1" class="latex" src="https://s0.wp.com/latex.php?latex=k+%3E+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. In more detail, we fix some string <img alt="w \in \{0, 1\}^k" class="latex" src="https://s0.wp.com/latex.php?latex=w+%5Cin+%5C%7B0%2C+1%5C%7D%5Ek&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and consider the binary polynomial with coefficients corresponding to the positions at which <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> appears as a (contiguous) substring. The key observation is that there exists a choice of <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> such that the resulting polynomial is sparse (in the sense that the degrees of the nonzero monomials are far away). The technical part of [Cha20] is then devoted to proving an analogue of <a href="https://theorydish.blog/feed/#lemma-1">Lemma 1</a> that is specialized to these âsparseâ Littlewood polynomials.</p>



<p><strong>Acknowledgments.</strong> I would like to thank Moses Charikar, Li-Yang Tan and Gregory Valiant for being on my quals committee and for helpful discussions about this problem.</p></div>
    </content>
    <updated>2021-06-29T16:20:49Z</updated>
    <published>2021-06-29T16:20:49Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Mingda Qiao</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2021-07-08T11:22:02Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/06/29/greedy-orderings-transposition</id>
    <link href="https://11011110.github.io/blog/2021/06/29/greedy-orderings-transposition.html" rel="alternate" type="text/html"/>
    <title>Greedy orderings with transposition</title>
    <summary>Iâm a big fan of using antimatroids to model vertex-ordering processes in graphs such as the construction of topological orderings in directed acyclic graphs and perfect elimination orderings in chordal graphs. In each case a vertex can be removed from the graph and added to the order when it obeys a local condition: its remaining neighbors are all outgoing for topological orderings, or all adjacent for perfect elimination orderings. Once this condition becomes true of a vertex it remains true until the vertex is added to the order, the defining property of an antimatroid. Because of this property, a greedy algorithm for finding these orderings can never make a mistake: if there exists an ordering of all of the vertices, it is always a safe choice to add any vertex that can be added.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Iâm a big fan of using <a href="https://en.wikipedia.org/wiki/Antimatroid">antimatroids</a> to model vertex-ordering processes in graphs such as the construction of <a href="https://en.wikipedia.org/wiki/Topological_sorting">topological orderings</a> in <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">directed acyclic graphs</a> and perfect elimination orderings in <a href="https://en.wikipedia.org/wiki/Chordal_graph">chordal graphs</a>. In each case a vertex can be removed from the graph and added to the order when it obeys a local condition: its remaining neighbors are all outgoing for topological orderings, or all adjacent for perfect elimination orderings. Once this condition becomes true of a vertex it remains true until the vertex is added to the order, the defining property of an antimatroid. Because of this property, a greedy algorithm for finding these orderings can never make a mistake: if there exists an ordering of all of the vertices, it is always a safe choice to add any vertex that can be added.</p>

<p>But there are some greedy vertex-ordering processes that do not form antimatroids, even though they do have the same inability to make mistakes. Two of these are the dismantling orders of <a href="https://en.wikipedia.org/wiki/Cop-win_graph">cop-win graphs</a> and the reverse construction orders of <a href="https://en.wikipedia.org/wiki/Distance-hereditary_graph">distance-hereditary graphs</a>. I wrote about cop-win graphs <a href="https://11011110.github.io/blog/2016/08/18/game-of-cop.html">here in 2016</a>; a graph is cop-win if a cop can always land on the same vertex as a robber when they take turns either moving from a vertex to a neighboring vertex or staying put. In distance-hereditary graphs, all induced subgraphs have the same distances; <a href="https://11011110.github.io/blog/2005/10/11/delta-confluent-drawing-paper.html">these graphs also have nice confluent drawings</a>. Both of these classes of graphs can be recognized by greedy algorithms that remove one vertex at a time until either getting stuck (for graphs not in the class) or succeeding by reaching a single-vertex graph. But although the conditions for removing vertices in these algorithms are local, they are not antimatroidal.</p>

<h1 id="an-example-graph">An example graph</h1>

<p style="text-align: center;"><img alt="A six-vertex graph with six vertices A, B, C, D, E, and F, and seven edges AD, BC, BD, BE, CE, and EF" src="https://11011110.github.io/blog/assets/2021/Ptolemaic.svg"/></p>

<p>The graph shown above happens to be chordal, distance-hereditary, and cop-win, making it a convenient example both of how to order the vertices of these graph classes and of why the distance-hereditary and cop-win orderings are not antimatroidal.</p>

<ul>
  <li>
    <p>In chordal graphs, a perfect elimination ordering can be constructed by repeatedly removing <em>simplicial vertices</em>, vertices whose neighborhoods form a clique. For an elimination ordering of the example graph, vertices \(A\), \(C\), and \(F\) are already available to be listed: \(A\) and \(F\) only have one neighbor (automatically a clique), and \(C\) has two neighbors forming a two-vertex clique. The other vertices will become available later in the removal process, once enough of their neighbors have been removed and all remaining vertices become adjacent. For instance, once \(A\) has been removed, \(D\) will become available, and once \(C\) has been removed, \(B\) will become available. Once this removal process makes a vertex simplicial, it remains simplicial until removed, so elimination orderings form an antimatroid.</p>
  </li>
  <li>
    <p>Distance-hereditary graphs can be constructed from a single vertex by repeatedly adding leaf vertices (with one neighbor connecting to previous vertices) or twins (duplicates of previous vertices). Reversing this process, these graphs can be deconstructed by repeatedly removing leaves or twins. The graph above has no twins, but \(A\) and \(F\) are leaves, and can be removed immediately. If \(A\) is removed, \(C\) and \(D\) become false twins (not adjacent to each other), and either of them can be removed. Similarly, if \(F\) is removed, \(B\) and \(E\) become true twins (adjacent to each other), after which one can be removed, but not both: after removing \(F\) and \(B\), \(E\) is no longer a leaf or a twin (because its twin, \(B\), has gone), and must remain until later steps. Because the removal orders can start \(FB\) or \(FE\) but not \(FBE\), they are not described by an antimatroid.</p>
  </li>
  <li>
    <p>Similarly, cop-win graphs can be dismantled by repeatedly removing a vertex \(v\) that is dominated by another vertex \(w\), meaning that the neighborhood of \(v\) (including \(v\) itself) is a subset of the neighborhoood of \(w\). In the given graph, \(A\) is dominated by \(D\), \(B\) is dominated by \(E\), \(C\) is dominated by both \(B\) and \(E\), and \(F\) is dominated by \(E\). So any one of these four dominated vertices starts out as removable. But if we remove first \(F\) and then \(E\) (dominated by \(B\) after the removal of \(F\)) we can no longer remove \(B\). So because the ability to be removed can go away before the removal happens, we do not have an antimatroid.</p>
  </li>
</ul>

<p>Thereâs another complication here as well. For both distance-hereditary graphs and cop-win graphs, removing leaves and twins or dominated vertices will never eliminate all graph vertices. Instead, both removal processes stop when we reach a single remaining vertex. But this is different from antimatroids, where all elements must be included in all orderings.</p>

<h1 id="some-axiomatics">Some axiomatics</h1>

<p>To understand why greedy orderings still work in these cases, I think itâs helpful to start by understanding why they work for antimatroids, as a general class of structures. The following is not quite the usual system of axioms for antimatroids, but they can be defined as non-empty formal languages (that is, sets of strings over a finite alphabet) with the following properties:</p>

<dl>
  <dt>Hereditary:</dt>
  <dd>
    <p>Every prefix of a string in the language is also in the language. Thinking about this in the other direction: every string in the language can be built up by adding one character at a time, starting from the empty string, at all times remaining within the language.</p>
  </dd>
  <dt>Normal:</dt>
  <dd>
    <p>Every character occurs at most once in any string in the language. An element can only be added to the sequence of elements once. Because we are assuming the alphabet to be finite, this means that the language itself is also finite.</p>
  </dd>
  <dt>Oblivious:</dt>
  <dd>
    <p>If \(S\) and \(T\) are permutations of each other in the language, then for every character \(x\), \(Sx\) is in the language if and only if \(Tx\) is in the language. This means that what can be added next depends only on the set of characters that have been added already, forgetting about the order in which they were added.</p>
  </dd>
  <dt>Anti-exchange:</dt>
  <dd>
    <p>If \(S\) is a string, \(x\) and \(y\) are different characters, and \(Sx\) and \(Sy\) both belong to the language, then so does \(Sxy\). Adding \(x\) doesnât prevent \(y\) from being added later. This is the key property of an antimatroid and the one that is violated by the distance-hereditary and cop-win orderings.</p>
  </dd>
</dl>

<p>Usually a stronger version of obliviousness is used, stating that when \(S\) and a permutation of \(Sx\) are in the language, then \(Sx\) is in the language, but itâs not immediately obvious why this should be true for the vertex-ordering processes Iâm considering here, so Iâve gone with a weaker version. Weâll see later that the stronger version is implied by a combination of this and other properties. It is standard to also require that all characters be usable, but I havenât done this, because I want to understand the behavior of antimatroidal greedy algorithms on graphs not in the given graph class, for which they get stuck before ordering the whole graph. But this is not important, because one could instead redefine the alphabet to consist only of usable characters.</p>

<p>Given a language that satisfies all of these properties, one can show that all non-extendable strings are equally long and use the same alphabet as each other. For, if we have two different non-extendable strings \(S\) and \(T\), we can morph \(S\) into \(T\) one step at a time, never shortening it or changing its character set, by finding the first position at which \(S\) and \(T\) differ, finding the character \(t\) that \(T\) has at that position (necessarily also used later in \(S\) because it was usable at that position and would have remained usable until it was used), and repeatedly using the anti-exchange axiom to swap \(t\) for the previous character in \(S\) until it has been swapped into a match with \(T\). The oblivious property ensures that the part of the string after the swap remains valid. So \(S\) cannot be shorter than or miss any characters from \(T\), nor vice versa.</p>

<p>Instead of the anti-exchange axiom, the distance-hereditary and cop-win orderings satisfy a weaker property, based on the notion of swapping two characters.</p>

<dl>
  <dt>Transposition:</dt>
  <dd>
    <p>Suppose \(S\) is a string, \(x\) and \(y\) are different characters, and \(Sx\) and \(Sy\) both belong to the language, but \(Sxy\) does not. Then for all \(T\) not containing \(x\) or \(y\), \(SxT\) is in the language if and only if \(SyT\) is also in the language, and \(SxTy\) is in the language if and only if \(SyTx\) is also in the language.</p>
  </dd>
</dl>

<p>The last part of the transposition property, about \(SxTy\) and \(SyTx\), is only included because we used a weak version of obliviousness; if we used the stronger version, it would follow from the earlier part of the transposition property.</p>

<h1 id="cop-win-and-distance-hereditary-orderings-have-the-transposition-property">Cop-win and distance-hereditary orderings have the transposition property</h1>

<p>Letâs suppose weâre trying to dismantle a cop-win graph by repeatedly removing dominated vertices, in the hope of getting down to a single vertex. After weâve removed some vertices already in a sequence \(S\), two vertices \(x\) and \(y\) might become included in the set of dominated vertices. This can happen in several different ways:</p>

<ul>
  <li>It might be the case that \(x\) is dominated by a vertex that is not \(y\), and that \(y\) is dominated by a vertex that is not \(x\). When this happens, they can be removed in either order: removing one wonât change the fact that the other is dominated by whatever other vertex dominated it already.</li>
  <li>It might be the case that \(x\) is dominated by \(y\), and \(y\) is dominated by a third vertex \(z\). But then \(x\) is also dominated by \(z\), and again they can be removed in either order. When one is removed, the other is still dominated by \(z\).</li>
  <li>The only remaining case is that \(x\) and \(y\) are each dominated only by the other of these two vertices. In this case, we can remove one or the other but not both. But if \(x\) and \(y\) dominate each other, they have the same neighbors (they are twins), and there is a symmetry of the remaining subgraph swapping \(x\) and \(y\). So in this case, any continuation of the removal sequence \(S\) can have \(x\) replaced by \(y\) and vice versa, and still be a valid continuation. This is exactly what the transposition property states.</li>
</ul>

<p>The argument for distance-hereditary orderings is even easier. If \(x\) and \(y\) are not twins of each other, then removing one wonât affect the removability of the other. If they are twins, then they are symmetric and any continuation of the removal sequence can exchange \(x\) for \(y\) without changing its validity.</p>

<h1 id="orderings-with-transposition-form-greedoids">Orderings with transposition form greedoids</h1>

<p>If we canât obtain an antimatroid from the cop-win or distance-hereditary graphs, we might at least hope for a more general structure, a greedoid. The key property of greedoids (viewed as hereditary normal languages rather than their usual definition as set systems) is the following axiom:</p>

<dl>
  <dt>Exchange:</dt>
  <dd>If \(S\) is a longer string in the language of a greedoid, and \(T\) is a longer string in the same language, then there is a character \(x\) in \(T\) such that \(Sx\) is a string in the language.</dd>
</dl>

<p>This implies that all maximal strings in the language have the same length, and in the cop-win and distance-hereditary cases it implies that all greedy dismantling or deconstruction sequences reach a single vertex without getting stuck along the way. The greedoid exchange property also immediately implies the strong version of the obliviousness property, by plugging in a permutation of \(Sx\) as the string \(T\) in the exchange property.</p>

<p>To prove that indistinguishability implies the exchange property, let \(S\) be any string in an indistinguishable (hereditary normal oblivious) language, and let \(T\) be a longer string, which we might as well assume to be maximal. If \(S\) is a prefix of \(T\), then obviously we can satisfy the exchange property: just take the prefix of \(T\) that has one more character.</p>

<p>Otherwise, I claim that we can replace \(T\) by a different string \(T'\) of the same length that agrees with \(S\) for more positions. To find \(T'\), let \(y\) be the first character of \(S\) that differs from the corresponding character of \(T\); this must exist by the assumption that \(S\) is not a prefix of \(T\). Obviously, at the position of \(y\) in \(S\), we could have added it to \(T\), but instead some other character was chosen. Maybe, \(y\) remained available to be chosen throughout the remaining positions of \(T\), until it actually was chosen. If so, just as in the antimatroid case, we could repeatedly swap \(y\) with its predecessor in \(T\) until reaching a string \(T'\) where \(y\) is in the correct position. Alternatively, maybe at some point during the construction of sequence \(T\), we chose a character \(z\) causing \(y\) to become unavailable. In this case, by the transposition property, we can swap \(y\) for \(z\) in \(T\) and then as before repeatedly swap \(y\) with its predecessor in \(T\) until reaching a string \(T'\) where \(y\) is in the correct position.</p>

<p>By repeatedly replacing \(T\) by equally long strings that agree with more and more positions of \(S\), we eventually reach a string for which \(S\) is a prefix, and can append one more character. This construction of \(T'\) from \(T\) does not include any new characters that werenât already in \(S\) or \(T\), so the appended character must have come from \(T\), proving the exchange axiom.</p>

<p>The use of the transposition property to form greedoids is standard; these greedoids are called transposition greedoids, and are described e.g. by BjÃ¶rner and Ziegler in their introduction to greedoids in the book <em>Matroid Applications</em>. Another <a href="https://doi.org/10.1007/978-3-642-58191-5_10">book chapter on transposition greedoids</a>, in the book <em>Greedoids</em> by Korte, Schrader, and LovÃ¡sz, includes another graph-theoretic example where the elements are edges of series-parallel graphs. The part that appears to be less standard is the use of this property to explain the ability of greedy algorithms to recognize cop-win and distance-hereditary graphs. I looked, but was unable to find publications observing that these two classes of graphs lead to greedoids or transposition greedoids, despite some suspiciously-similar terminology (âtwinsâ, âdismantlingâ) on both sides. If anyone knows of such publications, Iâd appreciate hearing of them, so that I could add this connection to their Wikipedia articles.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106495619434427598">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-06-29T12:02:00Z</updated>
    <published>2021-06-29T12:02:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-06-30T17:39:13Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/091</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/091" rel="alternate" type="text/html"/>
    <title>TR21-091 |  Expander Random Walks: The General Case and Limitations | 

	Gil Cohen, 

	Dor Minzer, 

	Shir Peleg, 

	Aaron Potechin, 

	Amnon Ta-Shma</title>
    <summary>Cohen, Peri and Ta-Shma (STOC'21) considered the following question: Assume the vertices of an expander graph are labelled by $\pm 1$. What "test" functions $f : \{\pm 1\}^t \to \{\pm1 \}$ can or cannot distinguish $t$ independent samples from those obtained by a random walk? [CPTS'21] considered only balanced labelling, and proved that all symmetric functions are fooled by random walks on expanders with constant spectral gap. Furthermore, it was shown that functions computable by $\mathbf{AC}^0$ circuits are fooled by expanders with vanishing spectral expansion. 

We continue the study of this question and, in particular, resolve all open problems raised by [CPTS'21]. First, we generalize the result to all labelling, not merely balanced. In doing so, we improve the known bound for symmetric functions and prove that the bound we obtain is optimal (up to a multiplicative constant). Furthermore, we prove that a random walk on expanders with constant spectral gap does not fool $\mathbf{AC}^0$. In fact, we prove that the bound obtained by [CPTS'21] for $\mathbf{AC}^0$ circuits is optimal up to a polynomial factor.</summary>
    <updated>2021-06-29T07:19:40Z</updated>
    <published>2021-06-29T07:19:40Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-08T11:20:37Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-5479161699112157490</id>
    <link href="https://blog.computationalcomplexity.org/feeds/5479161699112157490/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/06/someone-thinks-i-am-fine-artist-why.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5479161699112157490" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5479161699112157490" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/06/someone-thinks-i-am-fine-artist-why.html" rel="alternate" type="text/html"/>
    <title>Someone thinks I am a fine artist! Why?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A while back I got anÂ  email asking me to submit to a Fine Arts Journal. Why me? Here are some possibilities:</p><p>1) They were impressed with my play:Â </p><p><b>Sure he created the universe, but would he get Tenure?</b> (seeÂ <a href="http://www.cs.umd.edu/~gasarch/MYWRITINGS/god.html">here</a>) which did get into a play-writing contest and was performed (one of the actressesÂ  scolded me since I took a slot from <i>a real</i> <i>playwrigh</i>t).</p><p>2) They were impressedÂ  with my <b>Daria Fan Fiction</b> (see the four entriesÂ <a href="http://www.cs.umd.edu/~gasarch/MYWRITINGS/mywritings.html">here</a>Â labelled as Daria Fan Fiction).</p><p>3) They were impressed with my play <b>JFK: The Final chapter</b>Â (seeÂ <a href="http://www.cs.umd.edu/~gasarch/MYWRITINGS/jfk.html">here</a>). Unlikely since this was rejected by a play writing contest and is not well known (as opposed to my other works in the fine arts which are well known?)</p><p>4) They were impressed with my collection of satires ofÂ  Nobel Laureate Bob Dylan (<a href="https://www.cs.umd.edu/users/gasarch/dylan/dylan.html">here</a>) .</p><p>5) They were impressed with some subset of (a) complexityblog, (b)Â <a href="https://www.amazon.com/Problems-Point-Exploring-Computer-Science/dp/9813279729/ref=sr_1_3?dchild=1&amp;keywords=gasarch&amp;qid=1609867944&amp;sr=8-3" target="_blank">Problems with a Point</a>,Â  (c)Â Â <a href="https://www.amazon.com/Mathematical-Muffin-Morsels-Problem-Mathematics/dp/9811215979/ref=sr_1_2?dchild=1&amp;keywords=gasarch&amp;qid=1609868018&amp;sr=8-2">Mathematical Muffin Morsels</a>,Â and (d)Â <a href="https://www.amazon.com/Bounded-Queries-Recursion-Progress-Computer-ebook/dp/B000W98WU4/ref=sr_1_4?dchild=1&amp;keywords=gasarch&amp;qid=1609868084&amp;sr=8-4">Bounded Queries in Recursion Theory</a>.Â Or maybe just having 3 books on amazon is their threshold.Â  If it's complexityblog then Lance and I should co-author something for them.</p><p>6) It is a vanity-journal where you pay toÂ  publish. So why email me who (a)Â  is not an artist, (b) isÂ  not a fine artist, and most important (3)Â <b>does not think of himself as a fine artist</b>. The PRO of emailing me or people like me is they cast a wide net. The CON is--- there is no CON! It costs nothing to email me, and emailing me does not affect their credibility. That still raises the question of how they got my name.</p><p>7) Could it be a phishing? If I click on something in the email would theyÂ  get my credit card number? Their email begins <i>Dear Professor</i>Â notÂ Â <i>Dear Professor Gasarch.Â </i>Â  SoÂ they know I am a professor. Then again, I have known of ugrads who get emails that begin <i>Dear Professor</i>. (The emails to HS student Naveen and ugrad Nichole in the story I tellÂ <a href="https://blog.computationalcomplexity.org/search?q=Naveen">here</a>Â were addressed to <i>Dear Professor.)Â </i></p><p>8) They mistook me for my parents who, in 1973,Â  put together an anthology of short stories titledÂ <i>Fiction:The Universal elements</i>,Â  for a Freshman Comp course my mom taught, seeÂ <a href="https://www.amazon.com/Fiction-Universal-Element-P-Gasarch/dp/0442226322">here</a>. I note that their book ranks around 18,000,000, so even that explanation is unlikely. Actually the rank changes a lot- it was 12,000,000 this morning. Still, not what one would call a best seller. It's fun to see what is doing better: <i>Bounded Queries in Recursion Theory (currently at around rank 6.000.000)Â </i>Â or <i>Fiction: The Universal Elements.</i></p><p>Â If I ever get one of these emails from a History Journal I will submit my Satirical <i>Ramsey Theory and the History of Pre-Christian England: An Example of Interdisciplinary Research</i>Â (seeÂ <a href="https://www.cs.umd.edu/~gasarch/COURSES/389/W14/ramseykings.pdf">here</a>) just to see what happens- butÂ  I will stop short of paying-to-publish. Or maybe I will pay-to-publish so that the next time I try to fool a class with it I can point to a seemingly real journal which has the article.Â </p><p><br/></p><div class="gE iv gt" style="background-color: white; color: #222222; cursor: auto; font-family: Roboto, RobotoDraft, Helvetica, Arial, sans-serif; font-size: 0.875rem; padding: 20px 0px 0px;"><table cellpadding="0" class="cf gJ"><tbody style="display: block;"><tr class="acZ" style="display: flex; height: auto;"><td class="gF gK" style="display: block; line-height: 20px; margin: 0px; padding: 0px; vertical-align: top; white-space: nowrap; width: 571.172px;"><br/></td></tr></tbody></table></div></div>
    </content>
    <updated>2021-06-28T03:00:00Z</updated>
    <published>2021-06-28T03:00:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-07-08T03:26:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/090</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/090" rel="alternate" type="text/html"/>
    <title>TR21-090 |  On Secret Sharing, Randomness, and Random-less Reductions for Secret Sharing | 

	Divesh Aggarwal, 

	Maciej Obremski, 

	Eldon Chung, 

	Joao Ribeiro</title>
    <summary>Secret-sharing is one of the most basic and oldest primitives in cryptography, introduced by Shamir and Blakely in the 70s. It allows to strike a meaningful balance between availability and confidentiality of secret information. It has a host of applications most notably in threshold cryptography and multi-party computation. All known constructions of secret sharing (with the exception of those with a pathological choice of parameters) require access to uniform randomness. In practice, it is extremely challenging to generate a source of uniform randomness. This has led to a large body of research devoted to designing randomized algorithms and cryptographic primitives from imperfect sources of randomness.

Motivated by this, 15 years ago, Bosley and Dodis asked whether it is even possible to build 2-out-of-2 secret sharing without access to uniform randomness. In this work, we make progress towards resolving this question.

We answer this question for secret sharing schemes with important additional properties, i.e., either leakage-resilience or non-malleability. We prove that, unfortunately, for not too small secrets, it is impossible to construct any of 2-out-of-2 leakage-resilient secret sharing or 2-out-of-2 non-malleable secret sharing without access to uniform randomness.

Given that the problem whether 2-out-of-2 secret sharing requires uniform randomness has been open for a long time, it is reasonable to consider intermediate problems towards resolving the open question. In a spirit similar to NP-completeness, we study how the existence of a t-out-of-n secret sharing without access to uniform randomness is related to the existence of a t'-out-of-n' secret sharing without access to uniform randomness for a different choice of the parameters t,n,t',n'.</summary>
    <updated>2021-06-27T05:51:49Z</updated>
    <published>2021-06-27T05:51:49Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-07-08T11:20:37Z</updated>
    </source>
  </entry>
</feed>
