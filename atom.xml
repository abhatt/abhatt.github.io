<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-01-26T20:21:42Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7420</id>
    <link href="https://windowsontheory.org/2019/01/26/introduction-to-amp-and-the-replica-trick/" rel="alternate" type="text/html"/>
    <title>Introduction to AMP and the Replica Trick</title>
    <summary>(This post from the lecture by Yueqi Sheng) In this post, we will talk about detecting phase transitions using Approximate-Message-Passing (AMP), which is an extension of Belief-Propagation to “dense” models. We will also discuss the Replica Symmetric trick, which is a heuristic method of analyzing phase transitions. We focus on the Rademacher spiked Wigner model (defined below), […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>(This post from the lecture by <span class="qu"><span class="gD">Yueqi </span></span><span class="qu"><span class="gD">Sheng)</span></span></em></p>
<p>In this post, we will talk about detecting phase transitions using<br/>
Approximate-Message-Passing (AMP), which is an extension of<br/>
Belief-Propagation to “dense” models. We will also discuss the Replica<br/>
Symmetric trick, which is a heuristic method of analyzing phase<br/>
transitions. We focus on the Rademacher spiked Wigner model (defined<br/>
below), and show how both these methods yield the same phrase transition<br/>
in this setting.</p>
<p>The Rademacher spiked Wigner model (RSW) is the following. We are given<br/>
observations <img alt="Y = \frac{\lambda}{n}xx^T + \frac{1}{\sqrt{n}}W" class="latex" src="https://s0.wp.com/latex.php?latex=Y+%3D+%5Cfrac%7B%5Clambda%7D%7Bn%7Dxx%5ET+%2B+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7DW&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y = \frac{\lambda}{n}xx^T + \frac{1}{\sqrt{n}}W"/> where<br/>
<img alt="x \in \{\pm 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x \in \{\pm 1\}^n"/> (sampled uniformly) is the true signal and <img alt="W" class="latex" src="https://s0.wp.com/latex.php?latex=W&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="W"/> is a<br/>
Gaussian-Orthogonal-Ensemble (GOE) matrix:<br/>
<img alt="W_{i, j} \sim \mathbb{N}(0, 1)" class="latex" src="https://s0.wp.com/latex.php?latex=W_%7Bi%2C+j%7D+%5Csim+%5Cmathbb%7BN%7D%280%2C+1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="W_{i, j} \sim \mathbb{N}(0, 1)"/> for <img alt="i \neq j" class="latex" src="https://s0.wp.com/latex.php?latex=i+%5Cneq+j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i \neq j"/> and<br/>
<img alt="W_{i, i} \sim \mathbb{N}(0, 2)" class="latex" src="https://s0.wp.com/latex.php?latex=W_%7Bi%2C+i%7D+%5Csim+%5Cmathbb%7BN%7D%280%2C+2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="W_{i, i} \sim \mathbb{N}(0, 2)"/>. Here <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda"/> is the signal to noise<br/>
ratio. The goal is to approximately recover <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/>.</p>
<p>The question here is: how small can <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda"/> be such that it is<br/>
impossible to recover anything reasonably correlated with the<br/>
ground-truth <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/>? And what do the approximate-message-passing algorithm<br/>
(or the replica method) have to say about this?</p>
<p>To answer the first question, one can think of the task here is to<br/>
distinguish <img alt="Y \sim \frac{\lambda}{n}xx^T + \frac{1}{\sqrt{n}}W" class="latex" src="https://s0.wp.com/latex.php?latex=Y+%5Csim+%5Cfrac%7B%5Clambda%7D%7Bn%7Dxx%5ET+%2B+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7DW&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y \sim \frac{\lambda}{n}xx^T + \frac{1}{\sqrt{n}}W"/> vs<br/>
<img alt="Y \sim W" class="latex" src="https://s0.wp.com/latex.php?latex=Y+%5Csim+W&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y \sim W"/>. One approach to distinguishing these distributions is to<br/>
look at the spectrum of the observation matrix <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/>. (In fact, it turns<br/>
out that this is an asymptotically optimal distinguisher [1]). The spectrum of <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/> behaves as ([2]):</p>
<ul>
<li>When <img alt="\lambda \leq 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%5Cleq+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda \leq 1"/>, the empirical distribution of eigenvalues in<br/>
spiked model still follows the semicircle law, with the top<br/>
eigenvalues <img alt="\approx 2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Capprox+2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\approx 2"/><p/>
</li>
<li>
<p>When <img alt="\lambda &gt; 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%3E+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda &gt; 1"/>, we start to see an eigenvalue <img alt="&gt; 2" class="latex" src="https://s0.wp.com/latex.php?latex=%3E+2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="&gt; 2"/> in the<br/>
planted model.</p>
</li>
</ul>
<h1>Approximate message passing</h1>
<p>This section approximately follows the exposition in [3].</p>
<p>First, note that in the Rademacher spiked Wigner model, the posterior<br/>
distribution of the signal <img alt="\sigma" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma"/> conditioned on the observation <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/><br/>
is: <img alt="\Pr[\sigma | Y] \propto \Pr[Y | \sigma] \propto \prod_{i \neq j} \exp(\lambda Y_{i, j} \sigma_i \sigma_j /2 )" class="latex" src="https://s0.wp.com/latex.php?latex=%5CPr%5B%5Csigma+%7C+Y%5D+%5Cpropto+%5CPr%5BY+%7C+%5Csigma%5D+%5Cpropto+%5Cprod_%7Bi+%5Cneq+j%7D+%5Cexp%28%5Clambda+Y_%7Bi%2C+j%7D+%5Csigma_i+%5Csigma_j+%2F2+%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Pr[\sigma | Y] \propto \Pr[Y | \sigma] \propto \prod_{i \neq j} \exp(\lambda Y_{i, j} \sigma_i \sigma_j /2 )"/> This<br/>
defines a graphical-model (or “factor-graph”), over which we can perform<br/>
Belief-Propogation to infer the posterior distribution of <img alt="\sigma" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma"/>.<br/>
However, in this case the factor-graph is dense (the distribution is a<br/>
product of potentials <img alt="\exp(\lambda Y_{i, j} \sigma_i\sigma_j)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28%5Clambda+Y_%7Bi%2C+j%7D+%5Csigma_i%5Csigma_j%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\exp(\lambda Y_{i, j} \sigma_i\sigma_j)"/> for all<br/>
pairs of <img alt="i, j" class="latex" src="https://s0.wp.com/latex.php?latex=i%2C+j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i, j"/>).</p>
<p>In the previous <a href="https://windowsontheory.org/2018/10/20/belief-propagation-and-the-stochastic-block-model/">blog post</a>, we saw belief propagation works great when the underlying interaction<br/>
graph is sparse. Intuitively, this is because <img alt="G" class="latex" src="https://s0.wp.com/latex.php?latex=G&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G"/> is locally tree like,<br/>
which allows us to assume each messages are independent random<br/>
variables. In dense model, this no longer holds. One can think of dense<br/>
model as each node receive a weak signal from all its neighbors.</p>
<p>In the dense model setting, a class of algorithms called Approximate<br/>
message passing (AMP) is proposed as an alternative of BP. We will<br/>
define AMP for RWM in terms of its state evolution.</p>
<h2>State evolution of AMP for Rademacher spiked Wigner model</h2>
<p>Recall that in BP, we wish to infer the posterior distributon of<br/>
<img alt="\sigma" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma"/>, and the messages we pass between nodes correspond to marginal<br/>
probability distribution over values on nodes. In our setting, since the<br/>
distributions are over <img alt="\{\pm 1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%5Cpm+1%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{\pm 1\}"/>, we can represent distributions by<br/>
their expected values. Let <img alt="m^t_{u \to v} \in [-1, 1]" class="latex" src="https://s0.wp.com/latex.php?latex=m%5Et_%7Bu+%5Cto+v%7D+%5Cin+%5B-1%2C+1%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m^t_{u \to v} \in [-1, 1]"/> denote the<br/>
message from <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="u"/> to <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/> at time <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/>. That is, <img alt="m_{u \to v}" class="latex" src="https://s0.wp.com/latex.php?latex=m_%7Bu+%5Cto+v%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m_{u \to v}"/> corresponds<br/>
to the expected value <img alt="{{\mathbb{E}}}[\sigma_u]" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5B%5Csigma_u%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{\mathbb{E}}}[\sigma_u]"/>.</p>
<p>To derive the BP update rules, we want to compute the expectation<br/>
<img alt="{{\mathbb{E}}}[\sigma_v]" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5B%5Csigma_v%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{\mathbb{E}}}[\sigma_v]"/> of a node <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/>, given the<br/>
messages <img alt="{{\mathbb{E}}}[\sigma_u]" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5B%5Csigma_u%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{\mathbb{E}}}[\sigma_u]"/> for <img alt="u \neq v" class="latex" src="https://s0.wp.com/latex.php?latex=u+%5Cneq+v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="u \neq v"/>. We can<br/>
do this using the posterior distribution of the RWM, <img alt="\Pr[\sigma | Y]" class="latex" src="https://s0.wp.com/latex.php?latex=%5CPr%5B%5Csigma+%7C+Y%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Pr[\sigma | Y]"/>,<br/>
which we computed above.<br/>
<img alt="\displaystyle \Pr[\sigma_v = 1 | Y, \{\sigma_u\}_{u \neq v}] = \frac{ \prod_u \exp(\lambda Y_{u, v} \sigma_u) - \prod_u \exp(-\lambda Y_{u, v} \sigma_u) }{ \prod_u \exp(\lambda Y_{u, v} \sigma_u) + \prod_u \exp(-\lambda Y_{u, v} \sigma_u) }" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr%5B%5Csigma_v+%3D+1+%7C+Y%2C+%5C%7B%5Csigma_u%5C%7D_%7Bu+%5Cneq+v%7D%5D+%3D+%5Cfrac%7B+%5Cprod_u+%5Cexp%28%5Clambda+Y_%7Bu%2C+v%7D+%5Csigma_u%29+-+%5Cprod_u+%5Cexp%28-%5Clambda+Y_%7Bu%2C+v%7D+%5Csigma_u%29+%7D%7B+%5Cprod_u+%5Cexp%28%5Clambda+Y_%7Bu%2C+v%7D+%5Csigma_u%29+%2B+%5Cprod_u+%5Cexp%28-%5Clambda+Y_%7Bu%2C+v%7D+%5Csigma_u%29+%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle \Pr[\sigma_v = 1 | Y, \{\sigma_u\}_{u \neq v}] = \frac{ \prod_u \exp(\lambda Y_{u, v} \sigma_u) - \prod_u \exp(-\lambda Y_{u, v} \sigma_u) }{ \prod_u \exp(\lambda Y_{u, v} \sigma_u) + \prod_u \exp(-\lambda Y_{u, v} \sigma_u) }"/></p>
<p>And similarly for <img alt="\Pr[\sigma_v = -1 | Y, \{\sigma_u\}_{u \neq v}]" class="latex" src="https://s0.wp.com/latex.php?latex=%5CPr%5B%5Csigma_v+%3D+-1+%7C+Y%2C+%5C%7B%5Csigma_u%5C%7D_%7Bu+%5Cneq+v%7D%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Pr[\sigma_v = -1 | Y, \{\sigma_u\}_{u \neq v}]"/>.<br/>
From the above, we can take expectations over <img alt="\sigma_u" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_u&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_u"/>, and express<br/>
<img alt="{{\mathbb{E}}}[\sigma_v]" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5B%5Csigma_v%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{\mathbb{E}}}[\sigma_v]"/> in terms of<br/>
<img alt="\{{{\mathbb{E}}}[\sigma_u]\}_{u \neq v}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%7B%7B%5Cmathbb%7BE%7D%7D%7D%5B%5Csigma_u%5D%5C%7D_%7Bu+%5Cneq+v%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{{{\mathbb{E}}}[\sigma_u]\}_{u \neq v}"/>. Doing this (and<br/>
using the heuristic assumption that the distribution of <img alt="\sigma" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma"/> is a<br/>
product distribution), we find that the BP state update can be written<br/>
as:<br/>
<img alt="m^{t}_{u \to v} = f(\sum_{w \neq v}f^{-1}(A_{w, u} m^{t - 1}_{w \to u}))" class="latex" src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D_%7Bu+%5Cto+v%7D+%3D+f%28%5Csum_%7Bw+%5Cneq+v%7Df%5E%7B-1%7D%28A_%7Bw%2C+u%7D+m%5E%7Bt+-+1%7D_%7Bw+%5Cto+u%7D%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m^{t}_{u \to v} = f(\sum_{w \neq v}f^{-1}(A_{w, u} m^{t - 1}_{w \to u}))"/><br/>
where the interaction matrix <img alt="A_{w, u} = \lambda Y_{w, u}" class="latex" src="https://s0.wp.com/latex.php?latex=A_%7Bw%2C+u%7D+%3D+%5Clambda+Y_%7Bw%2C+u%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A_{w, u} = \lambda Y_{w, u}"/>, and<br/>
<img alt="f(x) = tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29+%3D+tanh%28x%29+%3D+%5Cfrac%7B%5Cexp%28x%29+-+%5Cexp%28-x%29%7D%7B%5Cexp%28x%29+%2B+%5Cexp%28x%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(x) = tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(x)}"/>.</p>
<p>Now, Taylor expanding <img alt="f^{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=f%5E%7B-1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f^{-1}"/> around <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/>, we find<br/>
<img alt="m^{t}_{u \to v} = f\left( (\sum_{w \neq v} A_{w, u} m^{t - 1}_{w \to u}) + O(1/\sqrt{n}) \right)" class="latex" src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D_%7Bu+%5Cto+v%7D+%3D+f%5Cleft%28+%28%5Csum_%7Bw+%5Cneq+v%7D+A_%7Bw%2C+u%7D+m%5E%7Bt+-+1%7D_%7Bw+%5Cto+u%7D%29+%2B+O%281%2F%5Csqrt%7Bn%7D%29+%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m^{t}_{u \to v} = f\left( (\sum_{w \neq v} A_{w, u} m^{t - 1}_{w \to u}) + O(1/\sqrt{n}) \right)"/><br/>
since the terms <img alt="A_{w, u}" class="latex" src="https://s0.wp.com/latex.php?latex=A_%7Bw%2C+u%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A_{w, u}"/> are of order <img alt="O(1/\sqrt{n})" class="latex" src="https://s0.wp.com/latex.php?latex=O%281%2F%5Csqrt%7Bn%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="O(1/\sqrt{n})"/>.</p>
<p>At this point, we could try dropping the “non-backtracking” condition<br/>
<img alt="w \neq v" class="latex" src="https://s0.wp.com/latex.php?latex=w+%5Cneq+v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="w \neq v"/> from the above sum (since the node <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/> contributes at most<br/>
<img alt="O(1/\sqrt{n})" class="latex" src="https://s0.wp.com/latex.php?latex=O%281%2F%5Csqrt%7Bn%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="O(1/\sqrt{n})"/> to the sum anyway), to get the state update:<br/>
<img alt="m^{t}_{u} = f\left( \sum_{w} A_{w, u} m^{t - 1}_{w}) \right)" class="latex" src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D_%7Bu%7D+%3D+f%5Cleft%28+%5Csum_%7Bw%7D+A_%7Bw%2C+u%7D+m%5E%7Bt+-+1%7D_%7Bw%7D%29+%5Cright%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m^{t}_{u} = f\left( \sum_{w} A_{w, u} m^{t - 1}_{w}) \right)"/> (note the messages no longer<br/>
depend on receiver – so we write <img alt="m_u" class="latex" src="https://s0.wp.com/latex.php?latex=m_u&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m_u"/> in place of <img alt="m_{u \to v}" class="latex" src="https://s0.wp.com/latex.php?latex=m_%7Bu+%5Cto+v%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m_{u \to v}"/>).<br/>
However, this simplification turns out not to work for estimating the<br/>
signal. The problem is that the “backtracking” terms which we added<br/>
amplify over two iterations.</p>
<p>In AMP, we simply perform the above procedure, except we add a<br/>
correction term to account for the backtracking issue above. Given <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="u"/>,<br/>
for all <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/>, the AMP update is:<br/>
<img alt="m^{t}_{u \to v} = m^{t}_u = f(\sum_{w}A_{w, u} m^{t - 1}_{w}) + [\text{some correction term}]" class="latex" src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D_%7Bu+%5Cto+v%7D+%3D+m%5E%7Bt%7D_u+%3D+f%28%5Csum_%7Bw%7DA_%7Bw%2C+u%7D+m%5E%7Bt+-+1%7D_%7Bw%7D%29+%2B+%5B%5Ctext%7Bsome+correction+term%7D%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m^{t}_{u \to v} = m^{t}_u = f(\sum_{w}A_{w, u} m^{t - 1}_{w}) + [\text{some correction term}]"/></p>
<p>The correction term corresponds to error introduced by the backtracking<br/>
terms. Suppose everything is good until step <img alt="t - 2" class="latex" src="https://s0.wp.com/latex.php?latex=t+-+2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t - 2"/>. We will examine<br/>
the influence of backtracking term to a node <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/> through length 2 loops.<br/>
At time <img alt="t - 1" class="latex" src="https://s0.wp.com/latex.php?latex=t+-+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t - 1"/>, <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/> exert <img alt="Y_{v, u}m^{t - 2}_v" class="latex" src="https://s0.wp.com/latex.php?latex=Y_%7Bv%2C+u%7Dm%5E%7Bt+-+2%7D_v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y_{v, u}m^{t - 2}_v"/> additional influence to<br/>
each of it’s neighbor <img alt="u" class="latex" src="https://s0.wp.com/latex.php?latex=u&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="u"/>. At time <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/>, <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/> receive roughly<br/>
<img alt="Y_{u, v}^2m^{t - 2}_v" class="latex" src="https://s0.wp.com/latex.php?latex=Y_%7Bu%2C+v%7D%5E2m%5E%7Bt+-+2%7D_v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y_{u, v}^2m^{t - 2}_v"/>. Since <img alt="Y_{u, v}^2" class="latex" src="https://s0.wp.com/latex.php?latex=Y_%7Bu%2C+v%7D%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y_{u, v}^2"/> has magnitude<br/>
<img alt="\approx \frac{1}{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Capprox+%5Cfrac%7B1%7D%7Bn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\approx \frac{1}{n}"/> and we need to sum over all of <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/>’s neighbors,<br/>
this error term is to large to ignore. To characterize the exact form of<br/>
correction, we simply do a taylor expansion</p>
<p><img alt="m^{t}_v = \sum_{u}f(Y_{u, v}m^{t - 1}_u) = \sum_{u}f(Y_{u, v} \left(\sum_{w}f(Y_{w, u}m^{t - 2}_w) - f(Y_{u, v}m^{t - 2}_w)\right) )\\ \approx \sum_u f(Y_{u, v} m^{t - 1}_u) - Y_{u, v}f'(m^{t - 1}_u)m^{t - 2}_v\\ \approx \sum_u f(Y_{u, v} m^{t - 1}_u) - \frac{1}{n}\sum_{u}f'(m^{t - 1}_u)m^{t - 2}_v" class="latex" src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D_v+%3D+%5Csum_%7Bu%7Df%28Y_%7Bu%2C+v%7Dm%5E%7Bt+-+1%7D_u%29+%3D+%5Csum_%7Bu%7Df%28Y_%7Bu%2C+v%7D+%5Cleft%28%5Csum_%7Bw%7Df%28Y_%7Bw%2C+u%7Dm%5E%7Bt+-+2%7D_w%29+-+f%28Y_%7Bu%2C+v%7Dm%5E%7Bt+-+2%7D_w%29%5Cright%29+%29%5C%5C+%5Capprox+%5Csum_u+f%28Y_%7Bu%2C+v%7D+m%5E%7Bt+-+1%7D_u%29+-+Y_%7Bu%2C+v%7Df%27%28m%5E%7Bt+-+1%7D_u%29m%5E%7Bt+-+2%7D_v%5C%5C+%5Capprox+%5Csum_u+f%28Y_%7Bu%2C+v%7D+m%5E%7Bt+-+1%7D_u%29+-+%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bu%7Df%27%28m%5E%7Bt+-+1%7D_u%29m%5E%7Bt+-+2%7D_v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m^{t}_v = \sum_{u}f(Y_{u, v}m^{t - 1}_u) = \sum_{u}f(Y_{u, v} \left(\sum_{w}f(Y_{w, u}m^{t - 2}_w) - f(Y_{u, v}m^{t - 2}_w)\right) )\\ \approx \sum_u f(Y_{u, v} m^{t - 1}_u) - Y_{u, v}f'(m^{t - 1}_u)m^{t - 2}_v\\ \approx \sum_u f(Y_{u, v} m^{t - 1}_u) - \frac{1}{n}\sum_{u}f'(m^{t - 1}_u)m^{t - 2}_v"/></p>
<h2>State evolution of AMP</h2>
<p>In this section we attempt to obtain the phase transition of Rademacher<br/>
spiked Wigner model via looking at <img alt="m^{\infty}" class="latex" src="https://s0.wp.com/latex.php?latex=m%5E%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m^{\infty}"/>.</p>
<p>We assume that each message could be written as a sum of signal term and<br/>
noise term. <img alt="m^t = \mu_t x + \sigma_t g" class="latex" src="https://s0.wp.com/latex.php?latex=m%5Et+%3D+%5Cmu_t+x+%2B+%5Csigma_t+g&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m^t = \mu_t x + \sigma_t g"/> where<br/>
<img alt="g \sim \mathbb{N}(0, I)" class="latex" src="https://s0.wp.com/latex.php?latex=g+%5Csim+%5Cmathbb%7BN%7D%280%2C+I%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g \sim \mathbb{N}(0, I)"/>. To the dynamics of AMP (and find its phase<br/>
transition), we need to look at how the signal <img alt="\mu_t" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu_t"/> and noise<br/>
<img alt="\sigma_t" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_t"/> evolves with <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/>.</p>
<p>We do the following simplification: ignore the correction term and<br/>
assume each time we obtain an independent noise <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g"/>.</p>
<p><img alt="m^{t} = Yf(m^{t - 1}) = (\frac{\lambda}{n}x^Tx + \frac{1}{\sqrt{n}}W)f(m^{t - 1}) = \frac{\lambda}{n} &lt; f(m^{t - 1}), x &gt; x + \frac{1}{\sqrt{n}} Wf(m^{t - 1})" class="latex" src="https://s0.wp.com/latex.php?latex=m%5E%7Bt%7D+%3D+Yf%28m%5E%7Bt+-+1%7D%29+%3D+%28%5Cfrac%7B%5Clambda%7D%7Bn%7Dx%5ETx+%2B+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7DW%29f%28m%5E%7Bt+-+1%7D%29+%3D+%5Cfrac%7B%5Clambda%7D%7Bn%7D+%3C+f%28m%5E%7Bt+-+1%7D%29%2C+x+%3E+x+%2B+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7D+Wf%28m%5E%7Bt+-+1%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m^{t} = Yf(m^{t - 1}) = (\frac{\lambda}{n}x^Tx + \frac{1}{\sqrt{n}}W)f(m^{t - 1}) = \frac{\lambda}{n} &lt; f(m^{t - 1}), x &gt; x + \frac{1}{\sqrt{n}} Wf(m^{t - 1})"/></p>
<p>Here, we see that <img alt="\mu_t = \frac{\lambda}{n}&lt; f(m^{t - 1}), x&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu_t+%3D+%5Cfrac%7B%5Clambda%7D%7Bn%7D%3C+f%28m%5E%7Bt+-+1%7D%29%2C+x%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu_t = \frac{\lambda}{n}&lt; f(m^{t - 1}), x&gt;"/><br/>
and <img alt="\sigma_t = \frac{1}{\sqrt{n}}Wf(m^{t - 1})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_t+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7DWf%28m%5E%7Bt+-+1%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_t = \frac{1}{\sqrt{n}}Wf(m^{t - 1})"/>.</p>
<p><em>Note that <img alt="\mu_{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu_%7Bt%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu_{t}"/> is essentially proportional to overlap between<br/>
ground truth and current belief</em>, since the function <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f"/> keeps the<br/>
magnitude of the current beliefs bounded.</p>
<p><img alt="\frac{\lambda}{n} &lt;f(m^{t - 1}), x&gt;= \frac{\lambda}{n} &lt;f(\mu_{t - 1}x + \sigma_{t - 1}g), x&gt; \approx\lambda {{\mathbb{E}}}_{X \sim unif(\pm 1), G\sim \mathbb{N}(0, 1)}[X f(\mu_{t - 1}X + \sigma_{t - 1}G)] = \lambda {{\mathbb{E}}}_G[f(\mu_{t - 1} + \sigma_{t - 1}G)]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Clambda%7D%7Bn%7D+%3Cf%28m%5E%7Bt+-+1%7D%29%2C+x%3E%3D+%5Cfrac%7B%5Clambda%7D%7Bn%7D+%3Cf%28%5Cmu_%7Bt+-+1%7Dx+%2B+%5Csigma_%7Bt+-+1%7Dg%29%2C+x%3E+%5Capprox%5Clambda+%7B%7B%5Cmathbb%7BE%7D%7D%7D_%7BX+%5Csim+unif%28%5Cpm+1%29%2C+G%5Csim+%5Cmathbb%7BN%7D%280%2C+1%29%7D%5BX+f%28%5Cmu_%7Bt+-+1%7DX+%2B+%5Csigma_%7Bt+-+1%7DG%29%5D+%3D+%5Clambda+%7B%7B%5Cmathbb%7BE%7D%7D%7D_G%5Bf%28%5Cmu_%7Bt+-+1%7D+%2B+%5Csigma_%7Bt+-+1%7DG%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{\lambda}{n} &lt;f(m^{t - 1}), x&gt;= \frac{\lambda}{n} &lt;f(\mu_{t - 1}x + \sigma_{t - 1}g), x&gt; \approx\lambda {{\mathbb{E}}}_{X \sim unif(\pm 1), G\sim \mathbb{N}(0, 1)}[X f(\mu_{t - 1}X + \sigma_{t - 1}G)] = \lambda {{\mathbb{E}}}_G[f(\mu_{t - 1} + \sigma_{t - 1}G)]"/></p>
<p>For the noise term, each coordinate of <img alt="\sigma_t" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_t"/> is a gaussian random<br/>
variable with <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/> mean and variance</p>
<p><img alt="\frac{1}{n} \sum_v f(m^{t - 1})_v^2 \approx {{\mathbb{E}}}_{X, G}[f(\mu_{t - 1}X + \sigma_{t - 1}G)^2] = {{\mathbb{E}}}_{G}[f(\mu_{t - 1} + \sigma_{t - 1}G)^2]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B1%7D%7Bn%7D+%5Csum_v+f%28m%5E%7Bt+-+1%7D%29_v%5E2+%5Capprox+%7B%7B%5Cmathbb%7BE%7D%7D%7D_%7BX%2C+G%7D%5Bf%28%5Cmu_%7Bt+-+1%7DX+%2B+%5Csigma_%7Bt+-+1%7DG%29%5E2%5D+%3D+%7B%7B%5Cmathbb%7BE%7D%7D%7D_%7BG%7D%5Bf%28%5Cmu_%7Bt+-+1%7D+%2B+%5Csigma_%7Bt+-+1%7DG%29%5E2%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{1}{n} \sum_v f(m^{t - 1})_v^2 \approx {{\mathbb{E}}}_{X, G}[f(\mu_{t - 1}X + \sigma_{t - 1}G)^2] = {{\mathbb{E}}}_{G}[f(\mu_{t - 1} + \sigma_{t - 1}G)^2]"/></p>
<p>It was shown in [4] that we can introduce a new<br/>
parameter <img alt="\gamma_t" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\gamma_t"/> s.t.<br/>
<img alt="\gamma_t = \lambda^2 {{\mathbb{E}}}[f(\gamma_{t - 1} + \sqrt{\gamma_{t - 1}}G)]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_t+%3D+%5Clambda%5E2+%7B%7B%5Cmathbb%7BE%7D%7D%7D%5Bf%28%5Cgamma_%7Bt+-+1%7D+%2B+%5Csqrt%7B%5Cgamma_%7Bt+-+1%7D%7DG%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\gamma_t = \lambda^2 {{\mathbb{E}}}[f(\gamma_{t - 1} + \sqrt{\gamma_{t - 1}}G)]"/><br/>
As <img alt="t \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=t+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t \to \infty"/>, turns out <img alt="\mu_t = \frac{\gamma_t}{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu_t+%3D+%5Cfrac%7B%5Cgamma_t%7D%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mu_t = \frac{\gamma_t}{\lambda}"/> and<br/>
<img alt="\sigma_t^2 = \frac{\sigma_t}{\lambda^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma_t%5E2+%3D+%5Cfrac%7B%5Csigma_t%7D%7B%5Clambda%5E2%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\sigma_t^2 = \frac{\sigma_t}{\lambda^2}"/>. To study the behavior of<br/>
<img alt="m^t" class="latex" src="https://s0.wp.com/latex.php?latex=m%5Et&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m^t"/> as <img alt="t \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=t+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t \to \infty"/>, it is enough to track the evolution of<br/>
<img alt="\gamma_t" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\gamma_t"/>.</p>
<p>This heuristic analysis of AMP actually gives a phase transition at<br/>
<img alt="\lambda = 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda = 1"/> (in fact, the analysis of AMP can be done rigorously as in [5]):</p>
<ul>
<li>For <img alt="\lambda &lt; 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%3C+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda &lt; 1"/>: If <img alt="\gamma_t \approx 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_t+%5Capprox+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\gamma_t \approx 0"/>, <img alt="|\gamma_t + \sqrt{\gamma_t}G| &lt; 1" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cgamma_t+%2B+%5Csqrt%7B%5Cgamma_t%7DG%7C+%3C+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\gamma_t + \sqrt{\gamma_t}G| &lt; 1"/> w.h.p., thus we have <img alt="\gamma_{t + 1} \approx \lambda^2 (\gamma_t) &lt; \gamma_t" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_%7Bt+%2B+1%7D+%5Capprox+%5Clambda%5E2+%28%5Cgamma_t%29+%3C+%5Cgamma_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\gamma_{t + 1} \approx \lambda^2 (\gamma_t) &lt; \gamma_t"/>. Taking <img alt="t \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=t+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t \to \infty"/>, we have <img alt="\gamma_{\infty} = 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cgamma_%7B%5Cinfty%7D+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\gamma_{\infty} = 0"/>, which means there AMP solution has no overlap with the ground truth.<p/>
</li>
<li>
<p>For <img alt="\lambda &gt; 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda+%3E+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda &gt; 1"/>: In this case, AMP’s solution has some correlation with the ground truth.</p>
</li>
</ul>
<p><img alt="screenshot 2019-01-26 13.49.39" class="alignnone size-full wp-image-7422" src="https://windowsontheory.files.wordpress.com/2019/01/screenshot-2019-01-26-13.49.39.png?w=600"/></p>
<p>(Figure from [6])</p>
<h1>Replica symmetry trick</h1>
<p>Another way of obtaining the phase transition is via a non-rigorous<br/>
analytic method called the replica method. Although non-rigorous, this<br/>
method from statistical physics has been used to predict the fixed point<br/>
of many message passing algorithms and has the advantage of being easy<br/>
to simulate. In our case, we will see that we obtain the same phase<br/>
transition temperature as AMP above. The method is non-rigorous due to<br/>
several assumptions made during the computation.</p>
<h2>Outline of replica method</h2>
<p>Recall that we are interested in minizing the free energy of a given<br/>
system <img alt="f(\beta, Y) = \frac{1}{\beta n} \log Z(\beta, Y)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%2C+Y%29+%3D+%5Cfrac%7B1%7D%7B%5Cbeta+n%7D+%5Clog+Z%28%5Cbeta%2C+Y%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta, Y) = \frac{1}{\beta n} \log Z(\beta, Y)"/> where <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/> is<br/>
the partition function as before:<br/>
<img alt="Z(\beta, Y) = \sum_{x \in \{\pm 1\}^n} exp(-\beta H(Y, x))" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28%5Cbeta%2C+Y%29+%3D+%5Csum_%7Bx+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En%7D+exp%28-%5Cbeta+H%28Y%2C+x%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(\beta, Y) = \sum_{x \in \{\pm 1\}^n} exp(-\beta H(Y, x))"/> and<br/>
<img alt="H(Y, x) = -&lt;Y, x^Tx&gt; = -xYx^T = -\sum_{i, j} Y_{i, j}x_ix_j" class="latex" src="https://s0.wp.com/latex.php?latex=H%28Y%2C+x%29+%3D+-%3CY%2C+x%5ETx%3E+%3D+-xYx%5ET+%3D+-%5Csum_%7Bi%2C+j%7D+Y_%7Bi%2C+j%7Dx_ix_j&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H(Y, x) = -&lt;Y, x^Tx&gt; = -xYx^T = -\sum_{i, j} Y_{i, j}x_ix_j"/>.</p>
<p>In replica method, <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/> is not fixed but a random variable. The<br/>
assumption is that as <img alt="n \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n \to \infty"/>, free energy doesn’t vary with <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/><br/>
too much, so we will look at the mean of <img alt="f_Y" class="latex" src="https://s0.wp.com/latex.php?latex=f_Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_Y"/> to approximate free<br/>
energy of the system.</p>
<p><img alt="f(\beta) = \lim_{n \to \infty}\frac{1}{\beta n}{{\mathbb{E}}}_{Y}[\log Z(\beta, Y)]" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29+%3D+%5Clim_%7Bn+%5Cto+%5Cinfty%7D%5Cfrac%7B1%7D%7B%5Cbeta+n%7D%7B%7B%5Cmathbb%7BE%7D%7D%7D_%7BY%7D%5B%5Clog+Z%28%5Cbeta%2C+Y%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta) = \lim_{n \to \infty}\frac{1}{\beta n}{{\mathbb{E}}}_{Y}[\log Z(\beta, Y)]"/></p>
<p><img alt="f(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta)"/> is called the free energy density and the goal now is to<br/>
compute the free energy density as a function of only <img alt="\beta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta"/> , the<br/>
temperature of the system.</p>
<p>The <strong>replica method</strong> is first proposed as a simplification of the<br/>
computation of <img alt="f(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta)"/></p>
<p>It is a generally hard problem to compute <img alt="f(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta)"/> in a clear way. A<br/>
naive attempt of approximate <img alt="f(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta)"/> is to simply pull the log out<br/>
<img alt="g(\beta) = \frac{1}{\beta n}\log {{\mathbb{E}}}_Y[Z(\beta, Y)]" class="latex" src="https://s0.wp.com/latex.php?latex=g%28%5Cbeta%29+%3D+%5Cfrac%7B1%7D%7B%5Cbeta+n%7D%5Clog+%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%28%5Cbeta%2C+Y%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g(\beta) = \frac{1}{\beta n}\log {{\mathbb{E}}}_Y[Z(\beta, Y)]"/><br/>
Unfortunately <img alt="g(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=g%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g(\beta)"/> and <img alt="f(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta)"/> are quite different quantities,<br/>
at least when temperature is low. Intuitively, <img alt="f(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta)"/> is looking at<br/>
system with a fixed <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/> while in <img alt="g(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=g%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g(\beta)"/>, <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x"/> and <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/> are allowed to<br/>
fluctuate together. When the temperature is high, <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/> doesn’t play a big<br/>
roll in system thus they could be close. However, when temperature is<br/>
low, there could be a problems. Let <img alt="\beta \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta \to \infty"/>,<br/>
<img alt="f(\beta) \approx \int_Y (\beta x_Y Y x_Y)\mu(Y) dY" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29+%5Capprox+%5Cint_Y+%28%5Cbeta+x_Y+Y+x_Y%29%5Cmu%28Y%29+dY&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta) \approx \int_Y (\beta x_Y Y x_Y)\mu(Y) dY"/>,<br/>
<img alt="g(\beta) \approx \log \int_Y exp(\beta x_J Y x_Y)\mu(Y)dY \approx \beta x^* Yx^*" class="latex" src="https://s0.wp.com/latex.php?latex=g%28%5Cbeta%29+%5Capprox+%5Clog+%5Cint_Y+exp%28%5Cbeta+x_J+Y+x_Y%29%5Cmu%28Y%29dY+%5Capprox+%5Cbeta+x%5E%2A+Yx%5E%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="g(\beta) \approx \log \int_Y exp(\beta x_J Y x_Y)\mu(Y)dY \approx \beta x^* Yx^*"/>.</p>
<p>While <img alt="{{\mathbb{E}}}_X[\log(f(X))]" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D_X%5B%5Clog%28f%28X%29%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{\mathbb{E}}}_X[\log(f(X))]"/> is hard to compute,<br/>
<img alt="{{\mathbb{E}}}[f(X)^r]" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5Bf%28X%29%5Er%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{\mathbb{E}}}[f(X)^r]"/> is a much easier quantity. The<br/>
replica trick starts from rewriting <img alt="f(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta)"/> with moments of <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z"/>:<br/>
Recall that <img alt="x^r \approx 1 + r \log x" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Er+%5Capprox+1+%2B+r+%5Clog+x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x^r \approx 1 + r \log x"/> for <img alt="r \approx 0" class="latex" src="https://s0.wp.com/latex.php?latex=r+%5Capprox+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r \approx 0"/> and<br/>
<img alt="\ln(1 + x)\approx x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cln%281+%2B+x%29%5Capprox+x&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\ln(1 + x)\approx x"/>, using this we can rewrite <img alt="f(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(x)"/> in the following<br/>
way:</p>
<p><strong>Claim 1.</strong> <em>Let <img alt="f_r(\beta) = \frac{1}{r \beta n}\ln[{{\mathbb{E}}}_Y[Z(\beta, Y)^r]]" class="latex" src="https://s0.wp.com/latex.php?latex=f_r%28%5Cbeta%29+%3D+%5Cfrac%7B1%7D%7Br+%5Cbeta+n%7D%5Cln%5B%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%28%5Cbeta%2C+Y%29%5Er%5D%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f_r(\beta) = \frac{1}{r \beta n}\ln[{{\mathbb{E}}}_Y[Z(\beta, Y)^r]]"/></em><br/>
<em>Then, <img alt="f(\beta) = \lim_{r \to 0}f_r(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29+%3D+%5Clim_%7Br+%5Cto+0%7Df_r%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta) = \lim_{r \to 0}f_r(\beta)"/></em></p>
<p>The idea of replica method is quite simple</p>
<ul>
<li>Define a function <img alt="f(r, \beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28r%2C+%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(r, \beta)"/> for <img alt="r \in \mathbb{Z}_+" class="latex" src="https://s0.wp.com/latex.php?latex=r+%5Cin+%5Cmathbb%7BZ%7D_%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r \in \mathbb{Z}_+"/> s.t. <img alt="f(r, \beta) = f_r(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28r%2C+%5Cbeta%29+%3D+f_r%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(r, \beta) = f_r(\beta)"/> for all such <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r"/>.<p/>
</li>
<li>
<p>Extend <img alt="f(r, \beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28r%2C+%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(r, \beta)"/> analytically to all <img alt="r \in {{\mathbb{R}}}_+" class="latex" src="https://s0.wp.com/latex.php?latex=r+%5Cin+%7B%7B%5Cmathbb%7BR%7D%7D%7D_%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r \in {{\mathbb{R}}}_+"/> and take the limit of <img alt="r \to 0" class="latex" src="https://s0.wp.com/latex.php?latex=r+%5Cto+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r \to 0"/>.</p>
</li>
</ul>
<p>The second step may sound crazy, but for some unexplained reason, it has<br/>
been surprisingly effective at making correct predictions.</p>
<p>The term replica comes from the way used to compute<br/>
<img alt="{{\mathbb{E}}}[Z^r]" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D%5BZ%5Er%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{\mathbb{E}}}[Z^r]"/> in Claim 1. We expand the <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r"/>-th moment<br/>
in terms of <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r"/> replicas of the system</p>
<p><img alt="Z(\beta, Y)^r = (\sum_x exp(-\beta H(Y, x)))^r = \sum_{x^1, \cdots, x^r} \Pi_{k = 1}^r exp(-\beta H(Y, x^i))" class="latex" src="https://s0.wp.com/latex.php?latex=Z%28%5Cbeta%2C+Y%29%5Er+%3D+%28%5Csum_x+exp%28-%5Cbeta+H%28Y%2C+x%29%29%29%5Er+%3D+%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+%5CPi_%7Bk+%3D+1%7D%5Er+exp%28-%5Cbeta+H%28Y%2C+x%5Ei%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z(\beta, Y)^r = (\sum_x exp(-\beta H(Y, x)))^r = \sum_{x^1, \cdots, x^r} \Pi_{k = 1}^r exp(-\beta H(Y, x^i))"/></p>
<h2>For Rademacher spiked Wigner model</h2>
<p>In this section, we will see how one can apply the replica trick to<br/>
obtain phase transition in the Rademacher spiked Wigner model. Recall<br/>
that given a hidden <img alt="a \in \{\pm 1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=a+%5Cin+%5C%7B%5Cpm+1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a \in \{\pm 1\}^n"/>, the observable<br/>
<img alt="Y = \frac{\lambda}{n}a^Ta + \frac{1}{\sqrt n} W" class="latex" src="https://s0.wp.com/latex.php?latex=Y+%3D+%5Cfrac%7B%5Clambda%7D%7Bn%7Da%5ETa+%2B+%5Cfrac%7B1%7D%7B%5Csqrt+n%7D+W&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y = \frac{\lambda}{n}a^Ta + \frac{1}{\sqrt n} W"/> where<br/>
<img alt="W_{i, j} \sim \mathcal{N}(0, 1)" class="latex" src="https://s0.wp.com/latex.php?latex=W_%7Bi%2C+j%7D+%5Csim+%5Cmathcal%7BN%7D%280%2C+1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="W_{i, j} \sim \mathcal{N}(0, 1)"/> and <img alt="W_{i, i} \sim \mathcal{N}(0, 2)" class="latex" src="https://s0.wp.com/latex.php?latex=W_%7Bi%2C+i%7D+%5Csim+%5Cmathcal%7BN%7D%280%2C+2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="W_{i, i} \sim \mathcal{N}(0, 2)"/>.<br/>
We are interested in finding the smallest <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda"/> where we can still<br/>
recover a solution with some correlation to the ground truth <img alt="a" class="latex" src="https://s0.wp.com/latex.php?latex=a&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="a"/>. <em>Note<br/>
that <img alt="\{W_{i, i}\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7BW_%7Bi%2C+i%7D%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{W_{i, i}\}"/> is not so important here as <img alt="x_i^2" class="latex" src="https://s0.wp.com/latex.php?latex=x_i%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_i^2"/> doesn’t carry<br/>
any information in this case.</em></p>
<p>Given by the posterior <img alt="{{\mathbb{P}}}[x|Y]" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BP%7D%7D%7D%5Bx%7CY%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{\mathbb{P}}}[x|Y]"/>, the system we<br/>
set up corresponding to Rademacher spiked Wigner model is the following:</p>
<ul>
<li>the system consists of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> particles and the interactions between<br/>
each particle are give by <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/><p/>
</li>
<li>
<p>the signal to noise ratio <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lambda"/> as the inverse temperature<br/>
<img alt="\beta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\beta"/>.</p>
</li>
</ul>
<p>Following the steps above, we begin by computing<br/>
<img alt="f(r, \beta) = \frac{1}{r\beta n}\ln{{\mathbb{E}}}_Y[Z^r]" class="latex" src="https://s0.wp.com/latex.php?latex=f%28r%2C+%5Cbeta%29+%3D+%5Cfrac%7B1%7D%7Br%5Cbeta+n%7D%5Cln%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%5Er%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(r, \beta) = \frac{1}{r\beta n}\ln{{\mathbb{E}}}_Y[Z^r]"/><br/>
for <img alt="r \in \mathbb{Z}_+" class="latex" src="https://s0.wp.com/latex.php?latex=r+%5Cin+%5Cmathbb%7BZ%7D_%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r \in \mathbb{Z}_+"/>: Denote <img alt="X^k = (x^k)^Tx^k" class="latex" src="https://s0.wp.com/latex.php?latex=X%5Ek+%3D+%28x%5Ek%29%5ETx%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X^k = (x^k)^Tx^k"/> where <img alt="x^k" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x^k"/> is the<br/>
<img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>th replica of the system.</p>
<p><img alt="{{\mathbb{E}}}_Y[Z^r] = \int_Y \sum_{x^1, \cdots, x^r} exp(\beta \sum_k &lt;Y, X^k&gt; \mu(Y) dY\\ = \int_Y \sum_{x^1, \cdots, x^r} exp(\beta &lt;Y, \sum_k X^k&gt;) \mu(Y) dY" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%5Er%5D+%3D+%5Cint_Y+%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+exp%28%5Cbeta+%5Csum_k+%3CY%2C+X%5Ek%3E+%5Cmu%28Y%29+dY%5C%5C+%3D+%5Cint_Y+%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+exp%28%5Cbeta+%3CY%2C+%5Csum_k+X%5Ek%3E%29+%5Cmu%28Y%29+dY&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{\mathbb{E}}}_Y[Z^r] = \int_Y \sum_{x^1, \cdots, x^r} exp(\beta \sum_k &lt;Y, X^k&gt; \mu(Y) dY\\ = \int_Y \sum_{x^1, \cdots, x^r} exp(\beta &lt;Y, \sum_k X^k&gt;) \mu(Y) dY"/></p>
<p>We then simplify the above expression with a technical claim.</p>
<p><strong>Claim 2.</strong><em> Let <img alt="Y = A + \frac{1}{\sqrt{n}}W" class="latex" src="https://s0.wp.com/latex.php?latex=Y+%3D+A+%2B+%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7DW&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y = A + \frac{1}{\sqrt{n}}W"/> where <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A"/> is a fixed matrix and</em><br/>
<em><img alt="W" class="latex" src="https://s0.wp.com/latex.php?latex=W&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="W"/> is the GOE matrix defined as above. Then,</em><br/>
<em><img alt="\int_Y exp(\beta&lt;Y, X&gt;) \mu(Y) dY = exp(\frac{\beta^2}{n}{{\|{X}\|_{F}}}^2 + \frac{\beta}{2} &lt;A, X&gt;)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cint_Y+exp%28%5Cbeta%3CY%2C+X%3E%29+%5Cmu%28Y%29+dY+%3D+exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7D%7B%7B%5C%7C%7BX%7D%5C%7C_%7BF%7D%7D%7D%5E2+%2B+%5Cfrac%7B%5Cbeta%7D%7B2%7D+%3CA%2C+X%3E%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\int_Y exp(\beta&lt;Y, X&gt;) \mu(Y) dY = exp(\frac{\beta^2}{n}{{\|{X}\|_{F}}}^2 + \frac{\beta}{2} &lt;A, X&gt;)"/></em><br/>
<em>for some constant <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C"/> depending on distribution of <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/>.</em></p>
<p>Denote <img alt="X = \sum_k X^k" class="latex" src="https://s0.wp.com/latex.php?latex=X+%3D+%5Csum_k+X%5Ek&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X = \sum_k X^k"/>. Apply Claim 2 with<br/>
<img alt="A = \frac{\beta}{n}a^Ta" class="latex" src="https://s0.wp.com/latex.php?latex=A+%3D+%5Cfrac%7B%5Cbeta%7D%7Bn%7Da%5ETa&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A = \frac{\beta}{n}a^Ta"/>, we have<br/>
<img alt="{{\mathbb{E}}}_Y[Z^r] = \sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}{{\|{X}\|_{F}}}^2 + \frac{\beta^2}{2n} &lt;a^Ta, X&gt;)" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%5Er%5D+%3D+%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7D%7B%7B%5C%7C%7BX%7D%5C%7C_%7BF%7D%7D%7D%5E2+%2B+%5Cfrac%7B%5Cbeta%5E2%7D%7B2n%7D+%3Ca%5ETa%2C+X%3E%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{\mathbb{E}}}_Y[Z^r] = \sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}{{\|{X}\|_{F}}}^2 + \frac{\beta^2}{2n} &lt;a^Ta, X&gt;)"/><br/>
To understand the term inside exponent better, we can rewrite the inner<br/>
sum in terms of overlap between replicas:</p>
<p><img alt="{{\|{X}\|_{F}}}^2 = \sum_{i, j}X_{i, j}^2 = \sum_{i, j}(\sum_{k = 1}^r x^k_ix^k_j)^2 =\sum_{i, j}(\sum_{k = 1}^r x^k_ix^k_j)(\sum_{l = 1}^r x^l_ix^l_j)\\ = \sum_{k, l} (\sum_{i = 1}^n x^k_ix^{l}_i)^2 = \sum_{k, l} &lt;x^k, x^l&gt;^2" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5C%7C%7BX%7D%5C%7C_%7BF%7D%7D%7D%5E2+%3D+%5Csum_%7Bi%2C+j%7DX_%7Bi%2C+j%7D%5E2+%3D+%5Csum_%7Bi%2C+j%7D%28%5Csum_%7Bk+%3D+1%7D%5Er+x%5Ek_ix%5Ek_j%29%5E2+%3D%5Csum_%7Bi%2C+j%7D%28%5Csum_%7Bk+%3D+1%7D%5Er+x%5Ek_ix%5Ek_j%29%28%5Csum_%7Bl+%3D+1%7D%5Er+x%5El_ix%5El_j%29%5C%5C+%3D+%5Csum_%7Bk%2C+l%7D+%28%5Csum_%7Bi+%3D+1%7D%5En+x%5Ek_ix%5E%7Bl%7D_i%29%5E2+%3D+%5Csum_%7Bk%2C+l%7D+%3Cx%5Ek%2C+x%5El%3E%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{\|{X}\|_{F}}}^2 = \sum_{i, j}X_{i, j}^2 = \sum_{i, j}(\sum_{k = 1}^r x^k_ix^k_j)^2 =\sum_{i, j}(\sum_{k = 1}^r x^k_ix^k_j)(\sum_{l = 1}^r x^l_ix^l_j)\\ = \sum_{k, l} (\sum_{i = 1}^n x^k_ix^{l}_i)^2 = \sum_{k, l} &lt;x^k, x^l&gt;^2"/></p>
<p>where the last equality follows from rearranging and switch the inner<br/>
and outer summations.</p>
<p>Using a similar trick, we can view the other term as</p>
<p><img alt="&lt;a^Ta, X&gt; = \sum_{i, j}\sum_{k = 1}^rx^k_ix^k_ja_ia_j = \sum_{k = 1}^r (\sum_{i = 1}^n a_ix^k_i)^2 = \sum_{k}&lt;a, x^k&gt;^2" class="latex" src="https://s0.wp.com/latex.php?latex=%3Ca%5ETa%2C+X%3E+%3D+%5Csum_%7Bi%2C+j%7D%5Csum_%7Bk+%3D+1%7D%5Erx%5Ek_ix%5Ek_ja_ia_j+%3D+%5Csum_%7Bk+%3D+1%7D%5Er+%28%5Csum_%7Bi+%3D+1%7D%5En+a_ix%5Ek_i%29%5E2+%3D+%5Csum_%7Bk%7D%3Ca%2C+x%5Ek%3E%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="&lt;a^Ta, X&gt; = \sum_{i, j}\sum_{k = 1}^rx^k_ix^k_ja_ia_j = \sum_{k = 1}^r (\sum_{i = 1}^n a_ix^k_i)^2 = \sum_{k}&lt;a, x^k&gt;^2"/></p>
<p>Note that <img alt="Q_{k, l} = &lt;x^k, x^l&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bk%2C+l%7D+%3D+%3Cx%5Ek%2C+x%5El%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q_{k, l} = &lt;x^k, x^l&gt;"/> represents overlaps between the<br/>
<img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> and <img alt="l" class="latex" src="https://s0.wp.com/latex.php?latex=l&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="l"/>th replicas and <img alt="Q_k = &lt;a, x^k&gt;" class="latex" src="https://s0.wp.com/latex.php?latex=Q_k+%3D+%3Ca%2C+x%5Ek%3E&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q_k = &lt;a, x^k&gt;"/> represents the<br/>
overlaps between the <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>th replica and the ground truth vector.</p>
<p>In the end, we get for any integer <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r"/>, (Equation 1):</p>
<p><img alt="\displaystyle f(r, \beta) = \frac{1}{r\beta n}\ln(\sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2)) \label{e:1}\\ = \frac{1}{r\beta n} \ln(\sum_{Q}\nu_{x^k}(Q)exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+f%28r%2C+%5Cbeta%29+%3D+%5Cfrac%7B1%7D%7Br%5Cbeta+n%7D%5Cln%28%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7D%5Csum_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D%5E2+%2B+%5Cfrac%7B%5Cbeta%5E2%7D%7B2n%7D%5Csum_k+Q_k%5E2%29%29+%5Clabel%7Be%3A1%7D%5C%5C+%3D+%5Cfrac%7B1%7D%7Br%5Cbeta+n%7D+%5Cln%28%5Csum_%7BQ%7D%5Cnu_%7Bx%5Ek%7D%28Q%29exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7D%5Csum_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D%5E2+%2B+%5Cfrac%7B%5Cbeta%5E2%7D%7B2n%7D%5Csum_k+Q_k%5E2%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle f(r, \beta) = \frac{1}{r\beta n}\ln(\sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2)) \label{e:1}\\ = \frac{1}{r\beta n} \ln(\sum_{Q}\nu_{x^k}(Q)exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2))"/></p>
<p>Our goal becomes to approximate this quantity. Intuitively, if we think<br/>
of <img alt="Q_{k, l}" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bk%2C+l%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q_{k, l}"/> as indices on a <img alt="(r + 1) \times (r + 1)" class="latex" src="https://s0.wp.com/latex.php?latex=%28r+%2B+1%29+%5Ctimes+%28r+%2B+1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(r + 1) \times (r + 1)"/> matrices, <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q"/>,<br/>
with <img alt="Q(i,i) = 1" class="latex" src="https://s0.wp.com/latex.php?latex=Q%28i%2Ci%29+%3D+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q(i,i) = 1"/>, then <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q"/> is the average of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> i.i.d matrices. So we<br/>
expect <img alt="Q_{j, k} \in [\pm \frac{1}{n}]" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bj%2C+k%7D+%5Cin+%5B%5Cpm+%5Cfrac%7B1%7D%7Bn%7D%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q_{j, k} \in [\pm \frac{1}{n}]"/> for <img alt="j \neq k" class="latex" src="https://s0.wp.com/latex.php?latex=j+%5Cneq+k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="j \neq k"/> w.h.p. In the<br/>
remaining part, We find the correct <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q"/> via rewriting Equation 1.</p>
<p>Observe that by introducing a new variable <img alt="Z_{k, l}" class="latex" src="https://s0.wp.com/latex.php?latex=Z_%7Bk%2C+l%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z_{k, l}"/> for <img alt="k \neq l" class="latex" src="https://s0.wp.com/latex.php?latex=k+%5Cneq+l&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k \neq l"/> and<br/>
using the property of gaussian intergal (Equation 4):</p>
<p><img alt="\label{e:4} exp(\frac{\beta^2}{n}Q_{k, l}^2) = \sqrt{\frac{n}{4\pi}}\int_{Z_{k, l}} exp(-\frac{n}{4}Z_{k, l}^2 + \beta Q_{k, l}Z_{k, l})dZ_k" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clabel%7Be%3A4%7D+exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7DQ_%7Bk%2C+l%7D%5E2%29+%3D+%5Csqrt%7B%5Cfrac%7Bn%7D%7B4%5Cpi%7D%7D%5Cint_%7BZ_%7Bk%2C+l%7D%7D+exp%28-%5Cfrac%7Bn%7D%7B4%7DZ_%7Bk%2C+l%7D%5E2+%2B+%5Cbeta+Q_%7Bk%2C+l%7DZ_%7Bk%2C+l%7D%29dZ_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\label{e:4} exp(\frac{\beta^2}{n}Q_{k, l}^2) = \sqrt{\frac{n}{4\pi}}\int_{Z_{k, l}} exp(-\frac{n}{4}Z_{k, l}^2 + \beta Q_{k, l}Z_{k, l})dZ_k"/></p>
<p><img alt="\exp(\frac{\beta^2}{2n}Q_k^2) = \sqrt{\frac{1}{8\pi n}}\int_{Z_k}exp(-(2n)Z_k^2 + 2\beta Q_{k}Z_k)dZ_k" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28%5Cfrac%7B%5Cbeta%5E2%7D%7B2n%7DQ_k%5E2%29+%3D+%5Csqrt%7B%5Cfrac%7B1%7D%7B8%5Cpi+n%7D%7D%5Cint_%7BZ_k%7Dexp%28-%282n%29Z_k%5E2+%2B+2%5Cbeta+Q_%7Bk%7DZ_k%29dZ_k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\exp(\frac{\beta^2}{2n}Q_k^2) = \sqrt{\frac{1}{8\pi n}}\int_{Z_k}exp(-(2n)Z_k^2 + 2\beta Q_{k}Z_k)dZ_k"/><br/>
Replace each <img alt="exp(\frac{\beta^2}{n}Q_{k, l}^2)" class="latex" src="https://s0.wp.com/latex.php?latex=exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7DQ_%7Bk%2C+l%7D%5E2%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="exp(\frac{\beta^2}{n}Q_{k, l}^2)"/> by a such integral, we<br/>
have (Equation 2):</p>
<p><img alt="\begin{gathered} {{\mathbb{E}}}[Z^r] = \sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2) \label{e:2}\\ = C\sum_{x^1, \cdots, x^r} \exp(\beta^2 n)\int_{Z_{k, l}}exp(-\frac{n}{4}\sum_{k \neq l}Z_{k, l}^2 - \frac{n}{2}\sum_k Z_k^2 + \beta \sum_{k \neq l}Y_{k, l}Q_{k, l} + 2\beta\sum_kZ_k Q_k) dZ \\ =C\exp(\beta^n) \int_{Y_{k, l}}exp(-\frac{n}{4}\sum_{k \neq l}Y_{k, l}^2 - \frac{n}{2}\sum_k Z_k^2 + \ln(\sum_{x_1,\cdots, x_r}exp(\beta \sum_{k\neq l}Y_{k, l}Q_{k, l} + 2\beta\sum_kY_k Q_k)) dY \label{e:2}\end{gathered}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Bgathered%7D+%7B%7B%5Cmathbb%7BE%7D%7D%7D%5BZ%5Er%5D+%3D+%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+exp%28%5Cfrac%7B%5Cbeta%5E2%7D%7Bn%7D%5Csum_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D%5E2+%2B+%5Cfrac%7B%5Cbeta%5E2%7D%7B2n%7D%5Csum_k+Q_k%5E2%29+%5Clabel%7Be%3A2%7D%5C%5C+%3D+C%5Csum_%7Bx%5E1%2C+%5Ccdots%2C+x%5Er%7D+%5Cexp%28%5Cbeta%5E2+n%29%5Cint_%7BZ_%7Bk%2C+l%7D%7Dexp%28-%5Cfrac%7Bn%7D%7B4%7D%5Csum_%7Bk+%5Cneq+l%7DZ_%7Bk%2C+l%7D%5E2+-+%5Cfrac%7Bn%7D%7B2%7D%5Csum_k+Z_k%5E2+%2B+%5Cbeta+%5Csum_%7Bk+%5Cneq+l%7DY_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D+%2B+2%5Cbeta%5Csum_kZ_k+Q_k%29+dZ+%5C%5C+%3DC%5Cexp%28%5Cbeta%5En%29+%5Cint_%7BY_%7Bk%2C+l%7D%7Dexp%28-%5Cfrac%7Bn%7D%7B4%7D%5Csum_%7Bk+%5Cneq+l%7DY_%7Bk%2C+l%7D%5E2+-+%5Cfrac%7Bn%7D%7B2%7D%5Csum_k+Z_k%5E2+%2B+%5Cln%28%5Csum_%7Bx_1%2C%5Ccdots%2C+x_r%7Dexp%28%5Cbeta+%5Csum_%7Bk%5Cneq+l%7DY_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D+%2B+2%5Cbeta%5Csum_kY_k+Q_k%29%29+dY+%5Clabel%7Be%3A2%7D%5Cend%7Bgathered%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\begin{gathered} {{\mathbb{E}}}[Z^r] = \sum_{x^1, \cdots, x^r} exp(\frac{\beta^2}{n}\sum_{k, l}Q_{k, l}^2 + \frac{\beta^2}{2n}\sum_k Q_k^2) \label{e:2}\\ = C\sum_{x^1, \cdots, x^r} \exp(\beta^2 n)\int_{Z_{k, l}}exp(-\frac{n}{4}\sum_{k \neq l}Z_{k, l}^2 - \frac{n}{2}\sum_k Z_k^2 + \beta \sum_{k \neq l}Y_{k, l}Q_{k, l} + 2\beta\sum_kZ_k Q_k) dZ \\ =C\exp(\beta^n) \int_{Y_{k, l}}exp(-\frac{n}{4}\sum_{k \neq l}Y_{k, l}^2 - \frac{n}{2}\sum_k Z_k^2 + \ln(\sum_{x_1,\cdots, x_r}exp(\beta \sum_{k\neq l}Y_{k, l}Q_{k, l} + 2\beta\sum_kY_k Q_k)) dY \label{e:2}\end{gathered}"/></p>
<p>where <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C"/> is the constant given by introducing gaussian intergals.</p>
<p>To compute the integral in (Equation 2), we need to cheat a little bit and take<br/>
<img alt="n \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n \to \infty"/> before letting <img alt="r \to 0" class="latex" src="https://s0.wp.com/latex.php?latex=r+%5Cto+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r \to 0"/>. Note that free energy density<br/>
is defined as<br/>
<img alt="f(\beta) = \lim_{n \to \infty}\lim_{r \to 0}\frac{1}{r\beta n}\ln {{\mathbb{E}}}_Y[Z(\beta, Y)^r]" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29+%3D+%5Clim_%7Bn+%5Cto+%5Cinfty%7D%5Clim_%7Br+%5Cto+0%7D%5Cfrac%7B1%7D%7Br%5Cbeta+n%7D%5Cln+%7B%7B%5Cmathbb%7BE%7D%7D%7D_Y%5BZ%28%5Cbeta%2C+Y%29%5Er%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta) = \lim_{n \to \infty}\lim_{r \to 0}\frac{1}{r\beta n}\ln {{\mathbb{E}}}_Y[Z(\beta, Y)^r]"/><br/>
This is the second assumption made in the replica method and it is<br/>
commonly believed that switching the order is okay here. Physically,<br/>
this is plausible because we believe intrinsic physical quantities<br/>
should not depend on the system size.</p>
<p>Now the <a href="https://en.wikipedia.org/wiki/Laplace%27s_method">Laplace method</a> tells us when <img alt="n \to \infty" class="latex" src="https://s0.wp.com/latex.php?latex=n+%5Cto+%5Cinfty&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n \to \infty"/>, the integral in (Equation 2) is dominated by the max of the exponent.</p>
<p><strong>Theorem 1 (Laplace Method).</strong> <em>Let <img alt="h(x): {{\mathbb{R}}}^n \to {{\mathbb{R}}}" class="latex" src="https://s0.wp.com/latex.php?latex=h%28x%29%3A+%7B%7B%5Cmathbb%7BR%7D%7D%7D%5En+%5Cto+%7B%7B%5Cmathbb%7BR%7D%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h(x): {{\mathbb{R}}}^n \to {{\mathbb{R}}}"/>, </em><em>then </em></p>
<p><em><img alt="\int e^{nh(x)} \approx e^{nh(x^*)}(\frac{2\pi}{n})^{\frac{d}{2}}\frac{1}{\sqrt{det(H)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cint+e%5E%7Bnh%28x%29%7D+%5Capprox+e%5E%7Bnh%28x%5E%2A%29%7D%28%5Cfrac%7B2%5Cpi%7D%7Bn%7D%29%5E%7B%5Cfrac%7Bd%7D%7B2%7D%7D%5Cfrac%7B1%7D%7B%5Csqrt%7Bdet%28H%29%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\int e^{nh(x)} \approx e^{nh(x^*)}(\frac{2\pi}{n})^{\frac{d}{2}}\frac{1}{\sqrt{det(H)}}"/></em></p>
<p><em>where <img alt="x^* = argmax_x \{h(x)\}" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%2A+%3D+argmax_x+%5C%7Bh%28x%29%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x^* = argmax_x \{h(x)\}"/> and <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="H"/> is the Hessian of <img alt="h" class="latex" src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h"/> evaluated at the point <img alt="x^*" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%2A&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x^*"/>.</em></p>
<p>Fix a pair of <img alt="k, l" class="latex" src="https://s0.wp.com/latex.php?latex=k%2C+l&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k, l"/> and apply Laplace method with<br/>
<img alt="h(Z_{k, l}) = -\frac{1}{2}\sum_{0 \leq k &lt; l \leq r}Z_{k, l}^2 + \frac{1}{n}\ln(\sum_{x_1,\cdots, x_r}exp(\beta \sum_{k \neq l}Z_{k, l}Q_{k, l} + 2\beta\sum_kZ_k Q_k))" class="latex" src="https://s0.wp.com/latex.php?latex=h%28Z_%7Bk%2C+l%7D%29+%3D+-%5Cfrac%7B1%7D%7B2%7D%5Csum_%7B0+%5Cleq+k+%3C+l+%5Cleq+r%7DZ_%7Bk%2C+l%7D%5E2+%2B+%5Cfrac%7B1%7D%7Bn%7D%5Cln%28%5Csum_%7Bx_1%2C%5Ccdots%2C+x_r%7Dexp%28%5Cbeta+%5Csum_%7Bk+%5Cneq+l%7DZ_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D+%2B+2%5Cbeta%5Csum_kZ_k+Q_k%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h(Z_{k, l}) = -\frac{1}{2}\sum_{0 \leq k &lt; l \leq r}Z_{k, l}^2 + \frac{1}{n}\ln(\sum_{x_1,\cdots, x_r}exp(\beta \sum_{k \neq l}Z_{k, l}Q_{k, l} + 2\beta\sum_kZ_k Q_k))"/><br/>
what’s left to do is to find the critical point of <img alt="h" class="latex" src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h"/>. Taking the<br/>
derivatives gives<br/>
<img alt="-Y_{k, l} + \frac{A(Z_{k, l})\beta Q_{k, l}}{n A(Z_{k, l})} = 0" class="latex" src="https://s0.wp.com/latex.php?latex=-Y_%7Bk%2C+l%7D+%2B+%5Cfrac%7BA%28Z_%7Bk%2C+l%7D%29%5Cbeta+Q_%7Bk%2C+l%7D%7D%7Bn+A%28Z_%7Bk%2C+l%7D%29%7D+%3D+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="-Y_{k, l} + \frac{A(Z_{k, l})\beta Q_{k, l}}{n A(Z_{k, l})} = 0"/><br/>
where<br/>
<img alt="A(Z_{k, l}) = \sum_{x_1,\cdots, x_r}exp(\beta \sum_{k \neq l}Z_{k, l}Q_{k, l} + \beta\sum_kY_k Q_k)" class="latex" src="https://s0.wp.com/latex.php?latex=A%28Z_%7Bk%2C+l%7D%29+%3D+%5Csum_%7Bx_1%2C%5Ccdots%2C+x_r%7Dexp%28%5Cbeta+%5Csum_%7Bk+%5Cneq+l%7DZ_%7Bk%2C+l%7DQ_%7Bk%2C+l%7D+%2B+%5Cbeta%5Csum_kY_k+Q_k%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="A(Z_{k, l}) = \sum_{x_1,\cdots, x_r}exp(\beta \sum_{k \neq l}Z_{k, l}Q_{k, l} + \beta\sum_kY_k Q_k)"/>.</p>
<p>We now need to find a saddle point of <img alt="h" class="latex" src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="h"/> where the hessian is PSD. To<br/>
do that, we choose to assume the order of the replicas does not matter,<br/>
which is refer to as the replica symmetry case. <sup id="fnref-7420-1"><a class="jetpack-footnote" href="https://windowsontheory.org/feed/#fn-7420-1">1</a></sup> One simplest form<br/>
of <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Y"/> is the following: <img alt="\forall k, l &gt; 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cforall+k%2C+l+%3E+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\forall k, l &gt; 0"/>, <img alt="Z_{k, l} = y" class="latex" src="https://s0.wp.com/latex.php?latex=Z_%7Bk%2C+l%7D+%3D+y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z_{k, l} = y"/> and<br/>
<img alt="Z_{k} = y" class="latex" src="https://s0.wp.com/latex.php?latex=Z_%7Bk%7D+%3D+y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Z_{k} = y"/> for some <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/>. This also implies that <img alt="Q_{k, l} = q" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bk%2C+l%7D+%3D+q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q_{k, l} = q"/> for some<br/>
<img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q"/> and <img alt="y =\frac{\beta}{n} q" class="latex" src="https://s0.wp.com/latex.php?latex=y+%3D%5Cfrac%7B%5Cbeta%7D%7Bn%7D+q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y =\frac{\beta}{n} q"/></p>
<p>Plug this back in to Equation 2 gives: (Equation 3)</p>
<p><img alt="\label{e:3} {{\mathbb{E}}}[Z^r] = C\exp(\beta n)\exp(-\frac{n}{2}(\frac{r^2 - r}{2})y^2 - \frac{n^2}{2} + \ln(\sum_{x^i}\exp(y\beta\sum_{k \neq l}Q_{k, l} + 2y\beta \sum_k Q_k))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clabel%7Be%3A3%7D+%7B%7B%5Cmathbb%7BE%7D%7D%7D%5BZ%5Er%5D+%3D+C%5Cexp%28%5Cbeta+n%29%5Cexp%28-%5Cfrac%7Bn%7D%7B2%7D%28%5Cfrac%7Br%5E2+-+r%7D%7B2%7D%29y%5E2+-+%5Cfrac%7Bn%5E2%7D%7B2%7D+%2B+%5Cln%28%5Csum_%7Bx%5Ei%7D%5Cexp%28y%5Cbeta%5Csum_%7Bk+%5Cneq+l%7DQ_%7Bk%2C+l%7D+%2B+2y%5Cbeta+%5Csum_k+Q_k%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\label{e:3} {{\mathbb{E}}}[Z^r] = C\exp(\beta n)\exp(-\frac{n}{2}(\frac{r^2 - r}{2})y^2 - \frac{n^2}{2} + \ln(\sum_{x^i}\exp(y\beta\sum_{k \neq l}Q_{k, l} + 2y\beta \sum_k Q_k))"/></p>
<p>To obtain <img alt="f(r, \beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28r%2C+%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(r, \beta)"/>, we only need to deal with the last term in<br/>
(Equation 3) as <img alt="r \to 0" class="latex" src="https://s0.wp.com/latex.php?latex=r+%5Cto+0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="r \to 0"/>. Using the fact that <img alt="Q_{k, l} = y" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7Bk%2C+l%7D+%3D+y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q_{k, l} = y"/> for all<br/>
<img alt="k, l" class="latex" src="https://s0.wp.com/latex.php?latex=k%2C+l&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k, l"/> and using the same trick of introducing new gaussain integral as<br/>
in (Equation 4) we have<br/>
<img alt="\lim_{r \to 0}\frac{1}{r}\ln(\sum_{x^i}\exp(y\beta\sum_{k \neq l}Q_{k, l} + n\beta \sum_k Q_k)) = -\beta + {{\mathbb{E}}}_{z \sim \mathcal{N}(0, 1)}[\log(2cosh(y\beta + \sqrt{y\beta}z))]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clim_%7Br+%5Cto+0%7D%5Cfrac%7B1%7D%7Br%7D%5Cln%28%5Csum_%7Bx%5Ei%7D%5Cexp%28y%5Cbeta%5Csum_%7Bk+%5Cneq+l%7DQ_%7Bk%2C+l%7D+%2B+n%5Cbeta+%5Csum_k+Q_k%29%29+%3D+-%5Cbeta+%2B+%7B%7B%5Cmathbb%7BE%7D%7D%7D_%7Bz+%5Csim+%5Cmathcal%7BN%7D%280%2C+1%29%7D%5B%5Clog%282cosh%28y%5Cbeta+%2B+%5Csqrt%7By%5Cbeta%7Dz%29%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lim_{r \to 0}\frac{1}{r}\ln(\sum_{x^i}\exp(y\beta\sum_{k \neq l}Q_{k, l} + n\beta \sum_k Q_k)) = -\beta + {{\mathbb{E}}}_{z \sim \mathcal{N}(0, 1)}[\log(2cosh(y\beta + \sqrt{y\beta}z))]"/></p>
<p>Using the fact that we want the solution to minimizes free energy,<br/>
taking the derivative of the current <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f"/> w.r.t. <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/> gives<br/>
<img alt="\frac{y}{\beta} = n{{\mathbb{E}}}_z[tanh(y\beta + \sqrt{y\beta}z)]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7By%7D%7B%5Cbeta%7D+%3D+n%7B%7B%5Cmathbb%7BE%7D%7D%7D_z%5Btanh%28y%5Cbeta+%2B+%5Csqrt%7By%5Cbeta%7Dz%29%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac{y}{\beta} = n{{\mathbb{E}}}_z[tanh(y\beta + \sqrt{y\beta}z)]"/><br/>
which matches the fixed point of AMP. Plug in <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y"/> will give us<br/>
<img alt="f(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta)"/>. The curve of <img alt="f(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta)"/> looks like the Figure below, where<br/>
the solid line is the curve of <img alt="f(\beta)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28%5Cbeta%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="f(\beta)"/> with the given <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q"/> and the<br/>
dotted line is the curve given by setting all variables <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="0"/>.</p>
<p><img alt="screenshot 2019-01-26 13.54.49" class="alignnone size-full wp-image-7423" src="https://windowsontheory.files.wordpress.com/2019/01/screenshot-2019-01-26-13.54.49.png?w=600"/></p>
<p> </p>
<p><strong>References</strong></p>
<div>[1] Amelia Perry, Alexander S Wein, Afonso S Bandeira, and Ankur Moitra. Optimality and sub-optimality of pca for spiked random matrices and synchronization.</div>
<div>arXiv preprint arXiv:1609.05573, 2016.</div>
<div/>
<div>[2] D. Feral and S. Pech e. The Largest Eigenvalue of Rank One Deformation of Large Wigner Matrices. Communications in Mathematical Physics, 272:185–228, May 2007.</div>
<div/>
<div>[3] Afonso S Bandeira, Amelia Perry, and Alexander S Wein. Notes on computational-to-statistical gaps: predictions using statistical physics. arXiv preprint arXiv:1803.11132, 2018.</div>
<div/>
<div>[4] Yash Deshpande, Emmanuel Abbe, and Andrea Montanari. Asymptotic mutual information for thebinary stochastic block model. In</div>
<div>Information Theory (ISIT), 2016 IEEE International Symposium on, pages 185–189. IEEE, 2016.</div>
<div/>
<div>[5] Adel Javanmard and Andrea Montanari. State evolution for general approximate message passing algorithms, with applications to spatial coupling. Information and Inference: A Journal of the IMA, 2(2):115–144, 2013.</div>
<div/>
<div>[6] A. Perry, A. S. Wein, and A. S. Bandeira. Statistical limits of spiked tensor models.</div>
<div>ArXiv e-prints, December 2016.</div>
<div class="footnotes">
<hr/>
<ol>
<li id="fn-7420-1">
Turns out for this problem, replica symmetry is the only case. We<br/>
will not talk about replica symmetry breaking here, which<br/>
intuitively means we partition replicas into groups and re-curse. <a href="https://windowsontheory.org/feed/#fnref-7420-1">↩</a>
</li>
</ol>
</div></div>
    </content>
    <updated>2019-01-26T19:10:18Z</updated>
    <published>2019-01-26T19:10:18Z</published>
    <category term="physics"/>
    <author>
      <name>preetum</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2019-01-26T20:21:13Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7413</id>
    <link href="https://windowsontheory.org/2019/01/25/quantum-circuits-and-their-role-in-demonstrating-quantum-supremacy/" rel="alternate" type="text/html"/>
    <title>Quantum circuits and their role in demonstrating quantum supremacy</title>
    <summary>There’s a lot of discussion and (possibly well-deserved) hype nowadays about quantum computation and its potential for computation at speeds we simply can’t reach with the classical computers we’re used to today. The excitement about this has been building for years, even decades, but it’s only very recently that we’ve really been approaching a solid […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>There’s a lot of discussion and (possibly well-deserved) hype nowadays about quantum computation and its potential for computation at speeds we simply can’t reach with the classical computers we’re used to today. The excitement about this has been building for years, even decades, but it’s only very recently that we’ve really been approaching a solid proof that quantum computers do have an <a href="https://en.wikipedia.org/wiki/Quantum_supremacy">advantage</a> over classical computers. </p>



<p>What’s rather tricky about showing such a result is that, rather than a direct argument about the capability of quantum computers, what we really need to demonstrate is the incapability of classical computers to achieve tasks that can be done with quantum computers. </p>



<p>One of the major leaps forward in demonstrating quantum supremacy was taken by Terhal and DiVincenzo in their 2008 paper “<a href="https://arxiv.org/abs/quant-ph/0205133">Adaptive quantum computation, constant depth quantum circuits and arthur-merlin games</a>“. Their approach was to appeal to a complexity-theoretic argument: they gave evidence that there exists a certain class of quantum circuits that cannot be simulated classically by proving that if a classical simulation existed, certain complexity classes strongly believed to be distinct would collapse to the same class. While this doesn’t quite provide a proof of quantum supremacy – since the statement about the distinction between complexity classes upon which it hinges is not a proven fact – because the complexity statement appears overwhelmingly likely to be true, so too does the proposed existence of non-classically-simulatable quantum circuits. The Terhal and DiVincenzo paper is a complex and highly technical one, but in this post I hope to explain a little bit and give some intuition for the major points. </p>



<p>Now, let’s start at the beginning. What is a <a href="https://en.wikipedia.org/wiki/Quantum_circuit">quantum circuit</a>? I’m going to go ahead and assume you already know what a <a href="https://en.wikipedia.org/wiki/Circuit_(computer_science)">classical circuit</a> is – the extension to a quantum circuit is rather straightforward: it’s a circuit in which all gates are <a href="https://en.wikipedia.org/wiki/Quantum_logic_gate">quantum gates</a>, where a quantum gate can be thought of as a classical gate whose output is, rather than a deterministic function of the inputs, instead a probability distribution over all possible outputs given the size of the inputs. For example, given two single-bit inputs, a classical AND gate outputs 0 or 1 deterministically given the inputs. A quantum AND gate on the analogous single-<a href="https://en.wikipedia.org/wiki/Qubit">qubit</a> inputs would output 0 with some probability <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/> and 1 with some probability <img alt="1-p" class="latex" src="https://s0.wp.com/latex.php?latex=1-p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1-p"/>. Similarly, a classical AND gate on two 4-bit inputs outputs the bitwise AND, while the quantum analog has associated with it a 4-qubit output: some probability distribution over all 4-bit binary strings. A priori there is no particular string that is the “output” of the computation by the quantum gate; it’s only after taking a <a href="https://en.wikipedia.org/wiki/Measurement_in_quantum_mechanics">quantum measurement</a> of the output that we get an actual string that we can think of as the outcome of the computation done by the gate. The actual string we “observe” upon taking the measurement follows the probability distribution computed by the gate on its inputs. In this way, a quantum circuit can then be thought of as producing, via a sequence of probabilistic classical gates (i.e., quantum gates) some probability distribution over possible outputs given the input lengths. It’s not hard to see that in this way, we can compose circuits: suppose we have a quantum circuit <img alt="c_1" class="latex" src="https://s0.wp.com/latex.php?latex=c_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_1"/> and another quantum circuit <img alt="c_2" class="latex" src="https://s0.wp.com/latex.php?latex=c_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_2"/>. Let <img alt="c_1" class="latex" src="https://s0.wp.com/latex.php?latex=c_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_1"/> have an input of <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> qubits and an output of <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/> qubits; suppose we measure <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> of the output qubits of <img alt="c_1" class="latex" src="https://s0.wp.com/latex.php?latex=c_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_1"/> – then we can feed the remaining <img alt="m-k" class="latex" src="https://s0.wp.com/latex.php?latex=m-k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m-k"/> unmeasured qubits as inputs into <img alt="c_2" class="latex" src="https://s0.wp.com/latex.php?latex=c_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_2"/>(assuming that those <img alt="m-k" class="latex" src="https://s0.wp.com/latex.php?latex=m-k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m-k"/> qubits do indeed constitute a valid input to <img alt="c_2" class="latex" src="https://s0.wp.com/latex.php?latex=c_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="c_2"/>). </p>



<p>Consider, then, the following sort of quantum circuit: it’s a composition of <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N"/> quantum circuits, such that after each <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="i"/>-th circuit we take a measurement some of its output qubits (so that the remaining unmeasured qubits become inputs to the <img alt="(i+1)" class="latex" src="https://s0.wp.com/latex.php?latex=%28i%2B1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(i+1)"/>-th circuit), and then the structure of the <img alt="(i+1)" class="latex" src="https://s0.wp.com/latex.php?latex=%28i%2B1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(i+1)"/>-th circuit is dependent on this measurement. That is, it’s as though, given a quantum circuit, we’re checking every so often at intermediate layers over the course of the circuit’s computation what the value of some of the variables are (leaving the rest to keep going along through the circuit to undergo more computational processing), and based on what we measure is the current computed value, the remainder of the circuit “adapts” in a way determined by that measurement. Aptly enough, this is called an “adaptive circuit”. But since the “downstream” structure of the circuit depends on the outcomes of all the measurements made “upstream”, each adaptive circuit actually comprises a family of circuits, each of which is specified by the sequence of intermediate measurement outcomes. That is, we can alternatively characterize an adaptive circuit as a set of ordinary quantum circuits that is parameterized by a list of measurement outcomes. Terhal and DiVincenzo call this way of viewing an adaptive circuit, as a family of circuits parametrized by a sequence of measurement values, a “non-adaptive circuit” – since we replace the idea that the circuit “adapts” to intermediate measurements with the idea that there are just many regular circuits, one for each possible sequence of measurements. It’s this non-adaptive circuit concept that’ll be our main object of study going forward.</p>



<h2>Simulating quantum circuits</h2>



<p>Now, the result we wanted to demonstrate about quantum circuits had to do with their efficient simulatability by classical circuits – and so we should establish some notion of what we mean when we talk about an “efficient simulation”. </p>



<p>Terhal and DiVincenzo offer the following notion of a classical simulation – which in their paper they call an “efficient density computation”: consider a quantum circuit with some output of length <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> qubits. Recall that to actually obtain an output value, we need to take a measurement of the circuit output – imagine doing this in disjoint subsets of cubits at a time. That is, we can break up the <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> qubits into <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> disjoint subsets and consider the entire output measurement as a process of taking <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/> measurements, subset by subset. An efficient density computation exists if there’s a classical procedure for computing, in time polynomial in the width and depth of the quantum circuit, the conditional probability distribution over the set of possible measurement outcomes of a particular subset of qubits, given any subset of the <img alt="k-1" class="latex" src="https://s0.wp.com/latex.php?latex=k-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k-1"/> other measurement outcomes. Intuitively, this is a good notion of what a classical simulation should consist of, or at least what data it should contain, since if you know the conditional probabilities given any (possibly empty) subset of the other measurements, you can just flip coins for the outputs according to the conditional probabilities as a way of actually exhibiting a “working” simulation.</p>



<p>It’s with this notion of simulation, along with our concept of an adaptive quantum circuit as a family of regular circuits parameterized by a sequence of intermediate measurement outcomes, we may now arrive at the main result of Terhal and DiVincenzo’s paper. Recall that what we wanted to show from the very beginning is that there exists some quantum circuit that can’t be simulated classically. The argument for this proceeds like a proof by contradiction: suppose the contrary, and that all quantum circuits can be simulated classically. We want to show that we can find, then, a quantum circuit which, if it were possible to be simulated classically (as per our assumption), we’d wind up with some strange consequences that we believe are false, leading us to conclude that those circuits probably can’t be simulated classically.</p>



<p>Thus, we shall now exhibit such a quantum circuit whose classical simulatability leads (as far as we believe) to a contradiction. Consider a special case of adaptive quantum circuits, considered as a parameterized family of regular circuits, in which the circuit’s output distribution is independent of the intermediate measurement outcomes; that is, the case in which the entire family of circuits corresponding to an adaptive circuit is logically the same – that is, is the same logical circuit on input qubits independent of intermediate measurements. I’d like to point out, just for clarification’s sake, the subtlety here, which makes this consideration non-redundant, and not simply a reduction of an adaptive quantum circuit (again, thought of as a family) to a single fixed circuit (i.e., a family of one): the situation in which the family is reduced to a single fixed circuit occurs when the<em> structure of the circuit</em> is independent of the intermediate measurement outcomes. If the structure were independent of the measurements, then no matter what we observed in the measurements, we’d get the same circuit – hence a trivial family of one. What we’re considering instead is the case in which the <em>structure</em> of the circuit is still dependent on the intermediate measurements (and so the circuit is still adaptive), but where the <em>distribution over the possible outputs of the circuit</em> is identical no matter what the intermediate measurements are. In this case, the circuit can still be considered as a parameterized and in general non-trivial family of circuits, but for which each member produces the same distribution over outputs – hence, a family of potentially <em>structurally</em> different circuits, but which are <em>logically</em> identical.</p>



<p>Suppose there’s some set of such circuits that’s universal – that is, that’s sufficient to implement all polynomial-time quantum computations. (This is a reasonable assumption to make, since there do in fact exist <a href="https://en.wikipedia.org/wiki/Quantum_logic_gate\#Universal_quantum_gates">universal quantum gate sets.</a> But now if a simulation of the kind we defined (an efficient density computation) existed for every circuit in this set, then we could calculate the outcome probability of any polynomial-depth quantum circuit, since any polynomial-depth quantum circuit could be realized as some composition of circuits in this universal set (and in particular as a composition of particular family members of each adaptive circuit in the universal set), and an efficient density computation, as we mentioned above, precisely gives us a way to compute the output distribution. </p>



<p>But now here is where our believed contradiction lies: </p>



<p><em>Theorem:</em> Suppose there exists a universal set of adaptive quantum circuits whose output distributions are independent of intermediate measurements. If there is an efficient density computation for each family member of each adaptive circuit in this universal set, then for the polynomial hierarchy PH we have PH = BPP = BQP. </p>



<p>The proof goes something like this: if we can do our desired efficient density computations (as we assumed, for the sake of contradiction, we could for all quantum circuits), this is equivalent to being able to determine the acceptance probability of a quantum computation, which was shown in the paper “<a href="https://arxiv.org/abs/quant-ph/9812056">Determining Acceptance Possibility for a Quantum Computation is Hard for the Polynomial Hierarchy”</a> by Fenner, Green, Homer and Pruim to be equivalent to the class <img alt="\text{coC=P}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BcoC%3DP%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\text{coC=P}"/>. Thus, we have that <img alt="\text{coC=P} \subseteq \text{BPP}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BcoC%3DP%7D+%5Csubseteq+%5Ctext%7BBPP%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\text{coC=P} \subseteq \text{BPP}"/>. But it’s known that <img alt="\text{PH} \subseteq \text{BPP}^{\text{coC=P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BPH%7D+%5Csubseteq+%5Ctext%7BBPP%7D%5E%7B%5Ctext%7BcoC%3DP%7D%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\text{PH} \subseteq \text{BPP}^{\text{coC=P}}"/> and so <img alt="\text{PH} \subseteq \text{BPP}^{\text{BPP}} = \text{BPP}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BPH%7D+%5Csubseteq+%5Ctext%7BBPP%7D%5E%7B%5Ctext%7BBPP%7D%7D+%3D+%5Ctext%7BBPP%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\text{PH} \subseteq \text{BPP}^{\text{BPP}} = \text{BPP}"/>. That is, we have <img alt="\text{PH} = \text{BPP} = \text{BQP}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BPH%7D+%3D+%5Ctext%7BBPP%7D+%3D+%5Ctext%7BBQP%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\text{PH} = \text{BPP} = \text{BQP}"/>, and so the polynomial hierarchy would collapse to <img alt="\Sigma^P_2" class="latex" src="https://s0.wp.com/latex.php?latex=%5CSigma%5EP_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\Sigma^P_2"/> since <img alt="\text{BPP} \subseteq \Sigma^P_2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7BBPP%7D+%5Csubseteq+%5CSigma%5EP_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\text{BPP} \subseteq \Sigma^P_2"/> (for more on these more obscure complexity classes, see <a href="https://complexityzoo.uwaterloo.ca/Complexity_Zoo">here</a>). Again, this is our “contradiction”: while it hasn’t been quite proven, it is widely believed, with strong supporting evidence, that the polynomial hierarchy does not collapse as would be the case if all quantum circuits were classically simulatable. Thus this provides a strong argument that not all quantum circuits are classically simulatable, which was precisely what we were looking to demonstrate.</p>



<h2>Conclusion</h2>



<p>Terhal and DiVincenzo actually go even further and show that there is a certain class of constant-depth quantum circuits that are unlikely to be simulatable by classical circuits – this, indeed, seems to provide even stronger evidence for quantum supremacy. This argument, which is somewhat more complex, uses the idea of teleportation and focuses on a particular class of circuits implementable by a certain restricted set of quantum gates. If you’re interested, I highly recommend reading their paper, where this is explained. </p>



<h2>Recommended reading</h2>



<ul><li>Terhal, Barbara and David DiVincenzo.  “Adptive quantum computation, constant depth quantum circuits and arthur-merlin games.” <em>Quantum Info. Comput.</em> 4, 2 (2004); 134-145.</li><li>Bravyil, Sergey, David Gosset, and Robert König. “Quantum advantage with shallow circuits.” <em>Science</em> 362, 6412 (2018); 308-311.</li><li>Harrow, Aram Wettroth and Ashley Montanaro. “Quantum computational supremacy.” <em>Nature</em> 549 (2017): 203-209.</li><li>Boixo, Sergio et. al. “Characterizing quantum supremacy in near-term devices.” <em>Nature Physics</em> 14 (2018); 595-600.</li></ul>



<h2>References</h2>



<p>\begin{enumerate}</p>



<ul><li>Terhal, Barbara and David DiVincenzo.  “Adptive quantum computation, constant depth quantum circuits and arthur-merlin games.” <em>Quantum Info. Comput.</em> 4, 2 (2004); 134-145.</li><li>Fenner, Stephen et. al. “Determining Acceptance Possibility for a Quantum Computation is Hard for the Polynomial Hierarchy.” <a href="https://arxiv.org/abs/quant-ph/9812056" rel="nofollow">https://arxiv.org/abs/quant-ph/9812056</a>. (1998).</li><li>Bravyil, Sergey, David Gosset, and Robert König. “Quantum advantage with shallow circuits.” <em>Science</em> 362, 6412 (2018); 308-311.</li></ul></div>
    </content>
    <updated>2019-01-25T23:42:09Z</updated>
    <published>2019-01-25T23:42:09Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>hksorens</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2019-01-26T20:21:14Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/01/25/postdoc-at-uc-san-diego-apply-by-march-1-2019/</id>
    <link href="https://cstheory-jobs.org/2019/01/25/postdoc-at-uc-san-diego-apply-by-march-1-2019/" rel="alternate" type="text/html"/>
    <title>postdoc at UC San Diego (apply by March 1, 2019)</title>
    <summary>We are looking for strong theory candidates working in the areas of machine learning, optimization, high dimensional statistics, privacy, fairness, and broadly interpreted data science. The postdoc is part of the Data Science fellows program at UCSD. Website: http://dsfellows.ucsd.edu/ Email: shachar.lovett@gmail.com</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We are looking for strong theory candidates working in the areas of machine learning, optimization, high dimensional statistics, privacy, fairness, and broadly interpreted data science. The postdoc is part of the Data Science fellows program at UCSD.</p>
<p>Website: <a href="http://dsfellows.ucsd.edu/">http://dsfellows.ucsd.edu/</a><br/>
Email: shachar.lovett@gmail.com</p></div>
    </content>
    <updated>2019-01-25T19:46:17Z</updated>
    <published>2019-01-25T19:46:17Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-01-26T20:20:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7411</id>
    <link href="https://windowsontheory.org/2019/01/25/looking-a-postdoc-opportunity/" rel="alternate" type="text/html"/>
    <title>Looking a postdoc opportunity?</title>
    <summary>This is the season that people are applying for postdoc positions. Unlike student and faculty hiring, which each have a fairly fixed schedule, postdoc availability can change from time to time, with new opportunities opening up all the time. So, I encourage everyone looking for a postdoc position to periodically check out https://cstheory-jobs.org/ . (For […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This is the season that people are applying for postdoc positions. Unlike student and faculty hiring, which each have a fairly fixed schedule, postdoc availability can change from time to time, with new opportunities opening up all the time. So, I encourage everyone looking for a postdoc position to periodically check out <a href="https://cstheory-jobs.org/">https://cstheory-jobs.org/</a> . </p>



<p>(For example, a new ad was just posted on Wednesday by <a href="https://cstheory-jobs.org/2019/01/23/postdoctoral-fellow-at-carnegie-mellon-university-apply-by-february-28-2019/">Venkat Guruswami and Pravesh Kothari </a> )</p></div>
    </content>
    <updated>2019-01-25T17:51:37Z</updated>
    <published>2019-01-25T17:51:37Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>windowsontheory</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2019-01-26T20:21:14Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.08575</id>
    <link href="http://arxiv.org/abs/1901.08575" rel="alternate" type="text/html"/>
    <title>Deterministic 2-Dimensional Temperature-1 Tile Assembly Systems Cannot Compute</title>
    <feedworld_mtime>1548374400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Durand=Lose:J=eacute=r=ocirc=me.html">Jérôme Durand-Lose</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoogeboom:Hendrik_Jan.html">Hendrik Jan Hoogeboom</a>, Nataša Jonoska <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08575">PDF</a><br/><b>Abstract: </b>We consider non cooperative binding in so called `temperature 1', in
deterministic (here called {\it confluent}) tile self-assembly systems (1-TAS)
and prove the standing conjecture that such systems do not have universal
computational power. We call a TAS whose maximal assemblies contain at least
one ultimately periodic assembly path {\it para-periodic}. We observe that a
confluent 1-TAS has at most one maximal producible assembly, $\alpha_{max}$,
that can be considered a union of path assemblies, and we show that such a
system is always para-periodic. This result is obtained through a superposition
and a combination of two paths that produce a new path with desired properties,
a technique that we call \emph{co-grow} of two paths. Moreover we provide a
characterization of an $\alpha_{max}$ of a confluent 1-TAS as one of two
possible cases, so called, a grid or a disjoint union of combs. To a given
$\alpha_{max}$ we can associate a finite labeled graph, called \emph{quipu},
such that the union of all labels of paths in the quipu equals $\alpha_{max}$,
therefore giving a finite description for $\alpha_{max}$. This finite
description implies that $\alpha_{max}$ is a union of semi-affine subsets of
$\mathbb{Z}^2$ and since such a finite description can be algorithmicly
generated from any 1-TAS, 1-TAS cannot have universal computational power.
</p></div>
    </summary>
    <updated>2019-01-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-01-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.08564</id>
    <link href="http://arxiv.org/abs/1901.08564" rel="alternate" type="text/html"/>
    <title>Infinite All-Layers Simple Foldability</title>
    <feedworld_mtime>1548374400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Akitaya:Hugo_A=.html">Hugo A. Akitaya</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Avery:Cordelia.html">Cordelia Avery</a>, Joseph Bergeron, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Demaine:Erik_D=.html">Erik D. Demaine</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kopinsky:Justin.html">Justin Kopinsky</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Ku:Jason.html">Jason Ku</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08564">PDF</a><br/><b>Abstract: </b>We study the problem of deciding whether a crease pattern can be folded by
simple folds (folding along one line at a time) under the infinite all-layers
model introduced by [Akitaya et al., 2017], in which each simple fold is
defined by an infinite line and must fold all layers of paper that intersect
this line. This model is motivated by folding in manufacturing such as
sheet-metal bending. We improve on [Arkin et al., 2004] by giving a
deterministic $O(n)$-time algorithm to decide simple foldability of 1D crease
patterns in the all-layers model. Then we extend this 1D result to 2D, showing
that simple foldability in this model can be decided in linear time for
unassigned axis-aligned orthogonal crease patterns on axis-aligned 2D
orthogonal paper. On the other hand, we show that simple foldability is
strongly NP-complete if a subset of the creases have a mountain-valley
assignment, even for an axis-aligned rectangle of paper.
</p></div>
    </summary>
    <updated>2019-01-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-01-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.08544</id>
    <link href="http://arxiv.org/abs/1901.08544" rel="alternate" type="text/html"/>
    <title>Learning Sublinear-Time Indexing for Nearest Neighbor Search</title>
    <feedworld_mtime>1548374400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dong:Yihe.html">Yihe Dong</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Indyk:Piotr.html">Piotr Indyk</a>, Ilya Razenshteyn, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wagner:Tal.html">Tal Wagner</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08544">PDF</a><br/><b>Abstract: </b>Most of the efficient sublinear-time indexing algorithms for the
high-dimensional nearest neighbor search problem (NNS) are based on space
partitions of the ambient space $\mathbb{R}^d$. Inspired by recent theoretical
work on NNS for general metric spaces [Andoni, Naor, Nikolov, Razenshteyn,
Waingarten STOC 2018, FOCS 2018], we develop a new framework for constructing
such partitions that reduces the problem to balanced graph partitioning
followed by supervised classification. We instantiate this general approach
with the KaHIP graph partitioner [Sanders, Schulz SEA 2013] and neural
networks, respectively, to obtain a new partitioning procedure called Neural
Locality-Sensitive Hashing (Neural LSH). On several standard benchmarks for
NNS, our experiments show that the partitions found by Neural LSH consistently
outperform partitions found by quantization- and tree-based methods.
</p></div>
    </summary>
    <updated>2019-01-25T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-01-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.08525</id>
    <link href="http://arxiv.org/abs/1901.08525" rel="alternate" type="text/html"/>
    <title>Solving linear program with Chubanov queries and bisection moves</title>
    <feedworld_mtime>1548374400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chan=Hon=Tong:Adrien.html">Adrien Chan-Hon-Tong</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08525">PDF</a><br/><b>Abstract: </b>This short article focus on the link between linear feasibility and generic
linear program. An algorithm is presented to solve generic linear program using
linear feasibility queries and working at constraint level instead of raw
values level. Even if the number of required linear feasibility queries is not
established, this algorithm may be especially interesting, since, thank to
Chubanov algorithm, there is a strongly polynomial time algorithm to solve
linear feasibility problem.
</p></div>
    </summary>
    <updated>2019-01-25T23:36:55Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.08419</id>
    <link href="http://arxiv.org/abs/1901.08419" rel="alternate" type="text/html"/>
    <title>Spherical sampling methods for the calculation of metamer mismatch volumes</title>
    <feedworld_mtime>1548374400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mackiewicz:Michal.html">Michal Mackiewicz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rivertz:Hans_Jakob.html">Hans Jakob Rivertz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Finlayson:Graham_D=.html">Graham D. Finlayson</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08419">PDF</a><br/><b>Abstract: </b>In this paper, we propose two methods of calculating theoretically maximal
metamer mismatch volumes. Unlike prior art techniques, our methods do not make
any assumptions on the shape of spectra on the boundary of the mismatch
volumes. Both methods utilize a spherical sampling approach, but they calculate
mismatch volumes in two different ways. The first method uses a linear
programming optimization, while the second is a computational geometry approach
based on half-space intersection. We show that under certain conditions the
theoretically maximal metamer mismatch volume is significantly larger than the
one approximated using a prior art method.
</p></div>
    </summary>
    <updated>2019-01-25T23:38:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-01-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.08246</id>
    <link href="http://arxiv.org/abs/1901.08246" rel="alternate" type="text/html"/>
    <title>Reachability Problem in Non-uniform Cellular Automata</title>
    <feedworld_mtime>1548374400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Adak:Sumit.html">Sumit Adak</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mukherjee:Sukanya.html">Sukanya Mukherjee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Das:Sukanta.html">Sukanta Das</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08246">PDF</a><br/><b>Abstract: </b>This paper deals with the CREP (Configuration REachability Problem) for
non-uniform cellular automata (CAs). The cells of non-uniform CAs, we have
considered here, can use different Wolfram's rules to generate their next
states. We report an algorithm which decides whether or not a configuration of
a given (non-uniform) cellular automaton is reachable from another
configuration. A characterization tool, named Reachability tree, is used to
develop theories and the decision algorithm for the CREP. Though the worst case
complexity of the algorithm is exponential in time and space, but the average
performance is very good.
</p></div>
    </summary>
    <updated>2019-01-25T23:20:32Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-01-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.08235</id>
    <link href="http://arxiv.org/abs/1901.08235" rel="alternate" type="text/html"/>
    <title>Multi-Frequency Phase Synchronization</title>
    <feedworld_mtime>1548374400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gao:Tingran.html">Tingran Gao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhao:Zhizhen.html">Zhizhen Zhao</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08235">PDF</a><br/><b>Abstract: </b>We propose a novel formulation for phase synchronization -- the statistical
problem of jointly estimating alignment angles from noisy pairwise comparisons
-- as a nonconvex optimization problem that enforces consistency among the
pairwise comparisons in multiple frequency channels. Inspired by harmonic
retrieval in signal processing, we develop a simple yet efficient two-stage
algorithm that leverages the multi-frequency information. We demonstrate in
theory and practice that the proposed algorithm significantly outperforms
state-of-the-art phase synchronization algorithms, at a mild computational
costs incurred by using the extra frequency channels. We also extend our
algorithmic framework to general synchronization problems over compact Lie
groups.
</p></div>
    </summary>
    <updated>2019-01-25T23:21:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.08219</id>
    <link href="http://arxiv.org/abs/1901.08219" rel="alternate" type="text/html"/>
    <title>Greedy Strategy Works for Clustering with Outliers and Coresets Construction</title>
    <feedworld_mtime>1548374400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Ding:Hu.html">Hu Ding</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08219">PDF</a><br/><b>Abstract: </b>We study the problems of clustering with outliers in high dimension. Though a
number of methods have been developed in the past decades, it is still quite
challenging to design quality guaranteed algorithms with low complexities for
the problems. Our idea is inspired by the greedy method, Gonzalez's algorithm,
for solving the problem of ordinary $k$-center clustering. Based on some novel
observations, we show that this greedy strategy actually can handle
$k$-center/median/means clustering with outliers efficiently, in terms of
qualities and complexities. We further show that the greedy approach yields
small coreset for the problem in doubling metrics, so as to reduce the time
complexity significantly. Moreover, a by-product is that the coreset
construction can be applied to speedup the popular density-based clustering
approach DBSCAN.
</p></div>
    </summary>
    <updated>2019-01-25T23:39:40Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-01-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.08210</id>
    <link href="http://arxiv.org/abs/1901.08210" rel="alternate" type="text/html"/>
    <title>Pseudo-Polynomial Time Algorithm for Computing Moments of Polynomials in Free Semicircular Elements</title>
    <feedworld_mtime>1548374400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Rei Mizuta <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.08210">PDF</a><br/><b>Abstract: </b>We consider about calculating $M$th moments of a given polynomial in free
independent semicircular elements in free probability theory. By a naive
approach, this calculation requires exponential time with respect to $M$. We
explicitly give an algorithm for calculating them in polynomial time by
rearranging Sch\"utzenberger's algorithm.
</p></div>
    </summary>
    <updated>2019-01-25T23:21:45Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/01/24/faculty-at-uc-san-diego-apply-by-february-15-2019/</id>
    <link href="https://cstheory-jobs.org/2019/01/24/faculty-at-uc-san-diego-apply-by-february-15-2019/" rel="alternate" type="text/html"/>
    <title>faculty at UC San Diego (apply by February 15, 2019)</title>
    <summary>Apply to be an Assistant Professor, with a focus on Algorithmic Approaches to Socially Innovative Product Architectures and Business Models. This is a joint search by the Computer Science department and the Rady business school. Website: https://apol-recruit.ucsd.edu/JPF01987 Email: slovett@ucsd.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Apply to be an Assistant Professor, with a focus on Algorithmic Approaches to Socially Innovative Product Architectures and Business Models. This is a joint search by the Computer Science department and the Rady business school.</p>
<p>Website: <a href="https://apol-recruit.ucsd.edu/JPF01987">https://apol-recruit.ucsd.edu/JPF01987</a><br/>
Email: slovett@ucsd.edu</p></div>
    </content>
    <updated>2019-01-24T18:10:56Z</updated>
    <published>2019-01-24T18:10:56Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-01-26T20:20:46Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-7530001663397385709</id>
    <link href="https://blog.computationalcomplexity.org/feeds/7530001663397385709/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/01/machine-learning-and-wind-turbines.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7530001663397385709" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7530001663397385709" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/01/machine-learning-and-wind-turbines.html" rel="alternate" type="text/html"/>
    <title>Machine Learning and Wind Turbines</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div class="separator" style="clear: both; text-align: center;">
<a href="https://3.bp.blogspot.com/-IwXAr_ptgYo/XEYxZWFiAzI/AAAAAAABmPw/6t_lphNZigA3sXSix1MpcmKNANZFiGkawCLcBGAs/s1600/Turbines.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="180" src="https://3.bp.blogspot.com/-IwXAr_ptgYo/XEYxZWFiAzI/AAAAAAABmPw/6t_lphNZigA3sXSix1MpcmKNANZFiGkawCLcBGAs/s320/Turbines.png" width="320"/></a></div>
<br/>
My daughter Molly spent six weeks on an environmental program in China last summer. When she got back she had to do a report on machine learning and wind turbines used for clean energy generation. What does machine learning have to do with wind turbines? Plenty it turns out and it tell us a lot about the future of programming.<br/>
<br/>
Sudden changes in wind can cause damage to the blades of the turbine. Maintenance is very expensive especially for turbines in the sea and a broken turbine generates no electricity. To catch these changes ahead of time you can mount a Lidar on top of the turbine.<br/>
<br/>
<div class="separator" style="clear: both; text-align: center;">
<a href="https://4.bp.blogspot.com/-9LFv8FGZNG8/XEYxYFyCYDI/AAAAAAABmPs/TLY7-bfPWhMWjRLl8Jn9ixXrTBa_sAk7ACLcBGAs/s1600/Turbine%2BLidar.jpg" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="180" src="https://4.bp.blogspot.com/-9LFv8FGZNG8/XEYxYFyCYDI/AAAAAAABmPs/TLY7-bfPWhMWjRLl8Jn9ixXrTBa_sAk7ACLcBGAs/s320/Turbine%2BLidar.jpg" width="320"/></a></div>
<br/>
The Lidar can detect wind gusts from about 100 meters ahead, giving about 10 seconds to react. In that time you can rotate the blades, or the whole turbine itself to minimize any damage. Here's a <a href="https://www.youtube.com/watch?v=j5zLp6UuC70">video</a> describing the situation.<br/>
<br/>
<center>

</center>
<br/>
How do you do the computations to convert the Lidar data into accurate representations of wind gusts and then how to best adjust for them? You could imagine some complex fluid dynamics computation, which gets even more complex when you several wind turbines in front of each other. Instead you can use the massive amount of data you have collected by sensors on the turbine and the Lidar information and train a neural network. Training takes a long time but a trained network can quickly determine a good course of action. Now neural networks can always make mistakes but unlike self-driving cars, a mistake won't kill anyone, just possibly cause more damage. Since on average you can save considerable maintenance costs, using ML here is a big win.<br/>
<br/>
I've obviously over simplified the above but I really like this example. This is not an ML solution to a standard AI question like image recognition or playing chess. Rather we are using ML to make a difficult computation tractable mostly by using ML on available data and that changes how we think about programming complex tasks.</div>
    </content>
    <updated>2019-01-24T13:58:00Z</updated>
    <published>2019-01-24T13:58:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>https://plus.google.com/101693130490639305932</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>https://plus.google.com/101693130490639305932</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-01-26T19:33:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://corner.mimuw.edu.pl/?p=1054</id>
    <link href="http://corner.mimuw.edu.pl/?p=1054" rel="alternate" type="text/html"/>
    <title>Prophet inequality and auction design</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Suppose you want to sell a car and there are 10 agents willing to buy it. You are not sure how much they could pay but for each of them you know a probability distribution of how high the offer … <a href="http://corner.mimuw.edu.pl/?p=1054">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Suppose you want to sell a car and there are 10 agents willing to buy it. You are not sure how much they could pay but for each of them you know a probability distribution of how high the offer will be. For example, a car salon would always pay 10K  but some person might offer 5K or 15K with equal probability. The best you could do is to first negotiate with all of them and then pick the highest bid. Unfortunately, you cannot do so - after seeing each offer you must irrevocably choose either to sell the car or to refuse the offer. What is the best strategy to maximize your revenue in this case?</p>
<p>In turns out that this is a well-studied optimization problem with a simple strategy that guarantees you can (on expectation) earn at least half as much as a hypothetical prophet, who knows all the bids in advance. This result is known as the prophet inequality. What is more surprising, this strategy would work even if you are a car retailer and want to sell five cars. Moreover, you might want to have some constraints, for example you do not want to sell two cars to buyers from the same city, or have multiple kinds of cars with different evaluations, and you can always guarantee an expected revenue comparable to the one of the prophets.</p>
<p>This problem not only exploits beautiful math but also has important applications in internet ad display. Actually, whenever you type a query into a web search engine, the ad system performs this kind of car-selling game with the ad suppliers, who offer different bids for their ad to be displayed to you.</p>
<p>Here is a link to our recent work with new developments in this theory: <a href="http://ieee-focs.org/FOCS-2018-Papers/pdfs/59f790.pdf">http://ieee-focs.org/FOCS-2018-Papers/pdfs/59f790.pdf</a></p>
<p>Michał Włodarczyk</p></div>
    </content>
    <updated>2019-01-24T10:21:58Z</updated>
    <published>2019-01-24T10:21:58Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Renata Czarniecka</name>
    </author>
    <source>
      <id>http://corner.mimuw.edu.pl</id>
      <link href="http://corner.mimuw.edu.pl/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="http://corner.mimuw.edu.pl" rel="alternate" type="text/html"/>
      <subtitle>University of Warsaw</subtitle>
      <title>Banach's Algorithmic Corner</title>
      <updated>2019-01-25T23:53:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/01/23/postdoctoral-fellow-at-carnegie-mellon-university-apply-by-february-28-2019/</id>
    <link href="https://cstheory-jobs.org/2019/01/23/postdoctoral-fellow-at-carnegie-mellon-university-apply-by-february-28-2019/" rel="alternate" type="text/html"/>
    <title>Postdoctoral Fellow at Carnegie Mellon University (apply by February 28, 2019)</title>
    <summary>Postdoctoral Position at the computer science department, CMU. Hosted by Venkatesan Gurusawami and Pravesh Kothari. Start Date: Fall 2019 (Flexible) Application Deadline: Feb 28, 2019 (earlier applications encouraged). To apply, send CV, a research statement and arrange for 2 letters of recommendation to be set to be sent to bcook@cs.cmu.edu with subject line mentioning “CMU […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Postdoctoral Position at the computer science department, CMU. Hosted by Venkatesan Gurusawami and Pravesh Kothari. Start Date: Fall 2019 (Flexible)<br/>
Application Deadline: Feb 28, 2019 (earlier applications encouraged).</p>
<p>To apply, send CV, a research statement and arrange for 2 letters of recommendation to be set to be sent to bcook@cs.cmu.edu with subject line mentioning “CMU theory postdoc.”</p>
<p>Website: <a href="https://www.cs.princeton.edu/~kothari/theory-postdoc.html">https://www.cs.princeton.edu/~kothari/theory-postdoc.html</a><br/>
Email: kothari@cs.princeton.edu</p></div>
    </content>
    <updated>2019-01-23T22:24:07Z</updated>
    <published>2019-01-23T22:24:07Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-01-26T20:20:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/01/22/postdoc-at-charles-university-in-prague-apply-by-february-28-2019-2/</id>
    <link href="https://cstheory-jobs.org/2019/01/22/postdoc-at-charles-university-in-prague-apply-by-february-28-2019-2/" rel="alternate" type="text/html"/>
    <title>Postdoc at Charles University in Prague (apply by February 28, 2019)</title>
    <summary>Several one‐year post‐doc positions are available at the Computer Science Institute of Charles University, with a possibility of one-year extension. The positions are in areas of algorithms, cryptography, computational complexity, combinatorics and graph theory. Starting date is in Fall 2019, and can be negotiated. Website: https://iuuk.mff.cuni.cz/~koucky/iuuk-postdocs.html Email: koucky@iuuk.mff.cuni.cz</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Several one‐year post‐doc positions are available at the Computer Science Institute of Charles University, with a possibility of one-year extension. The positions are in areas of algorithms, cryptography, computational complexity, combinatorics and graph theory. Starting date is in Fall 2019, and can be negotiated.</p>
<p>Website: <a href="https://iuuk.mff.cuni.cz/~koucky/iuuk-postdocs.html">https://iuuk.mff.cuni.cz/~koucky/iuuk-postdocs.html</a><br/>
Email: koucky@iuuk.mff.cuni.cz</p></div>
    </content>
    <updated>2019-01-22T12:38:31Z</updated>
    <published>2019-01-22T12:38:31Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-01-26T20:20:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/01/22/phd-student-at-charles-university-in-prague-apply-by-february-28-2019/</id>
    <link href="https://cstheory-jobs.org/2019/01/22/phd-student-at-charles-university-in-prague-apply-by-february-28-2019/" rel="alternate" type="text/html"/>
    <title>PhD student at Charles University in Prague (apply by February 28, 2019)</title>
    <summary>Positions for PhD students are available at the Computer Science Institute of Charles University within the framework of the project “Efficient approximation algorithms and circuit complexity” funded by the Grant Agency of the Czech Republic. Applications are invited from candidates who have strong background in theoretical computer science or discrete mathematics. Starting date: Fall 2019. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Positions for PhD students are available at the Computer Science Institute of Charles University within the framework of the project “Efficient approximation algorithms and circuit complexity” funded by the Grant Agency of the Czech Republic. Applications are invited from candidates who have strong background in theoretical computer science or discrete mathematics. Starting date: Fall 2019.</p>
<p>Website: <a href="https://iuuk.mff.cuni.cz/~koucky/EPAC/positions.html">https://iuuk.mff.cuni.cz/~koucky/EPAC/positions.html</a><br/>
Email: koucky@iuuk.mff.cuni.cz</p></div>
    </content>
    <updated>2019-01-22T12:36:30Z</updated>
    <published>2019-01-22T12:36:30Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-01-26T20:20:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/01/22/postdoc-at-charles-university-in-prague-apply-by-february-28-2019/</id>
    <link href="https://cstheory-jobs.org/2019/01/22/postdoc-at-charles-university-in-prague-apply-by-february-28-2019/" rel="alternate" type="text/html"/>
    <title>Postdoc at Charles University in Prague (apply by February 28, 2019)</title>
    <summary>The position is supported by a five year project “EPAC: Efficient approximation algorithms and circuit complexity” funded by the Grant Agency of the Czech Republic. The project is focused on approximation algorithms in fine-grained and parameterized complexity and on lower bound techniques. Starting date is in Fall 2019, and can be negotiated. Website: https://iuuk.mff.cuni.cz/~koucky/EPAC/ Email: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The position is supported by a five year project “EPAC: Efficient approximation algorithms and circuit complexity” funded by the Grant Agency of the Czech Republic. The project is focused on approximation algorithms in fine-grained and parameterized complexity and on lower bound techniques. Starting date is in Fall 2019, and can be negotiated.</p>
<p>Website: <a href="https://iuuk.mff.cuni.cz/~koucky/EPAC/">https://iuuk.mff.cuni.cz/~koucky/EPAC/</a><br/>
Email: koucky@iuuk.mff.cuni.cz</p></div>
    </content>
    <updated>2019-01-22T12:33:00Z</updated>
    <published>2019-01-22T12:33:00Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-01-26T20:20:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3377</id>
    <link href="https://agtb.wordpress.com/2019/01/22/guest-post-like-a-swarm-of-locusts-vijay-vazirani/" rel="alternate" type="text/html"/>
    <title>Guest post: “Like a Swarm of Locusts…” (Vijay Vazirani)</title>
    <summary>  [The following guest post was written by Vijay Vazirani.]           What the cutting locust left, the swarming locust has eaten. What the swarming locust left, the hopping locust has eaten, and what the hopping locust left, the destroying locust has eaten. Joel 1:4 A few years ago, while attending a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em><img align="RIGHT" alt="My Picture" border="1" class=" alignleft" height="161" src="https://www.ics.uci.edu/~vazirani/Vijay2.png" width="135"/></em></p>
<p> </p>
<p style="text-align: right;"><em>[The following guest post was written by Vijay Vazirani.] </em></p>
<p> </p>
<p> </p>
<p> </p>
<p><em> </em></p>
<p> </p>
<blockquote><p><em>What the cutting locust left, the swarming locust has eaten. What the swarming locust left, the hopping locust has eaten, and what the hopping locust left, the destroying locust has eaten.</em></p>
<p style="text-align: right;">Joel 1:4</p>
</blockquote>
<p>A few years ago, while attending a Dagstuhl program on Equilibrium Computation, I embarked on one of the traditional long hikes through the beautiful woods surrounding Schloss Dagstuhl and happened to be walking next to a senior member of the Operations Research community. In no time we got immersed in a lively discussion on the style of research going on in Algorithmic Game Theory. We both agreed that the progress made in a matter of a few years was nothing short of phenomenal. At this point, my colleague remarked that AGT was no exception and that when TCS researchers enter a new area, they go in with such energies and enthusiasm that in no time not only is all low hanging fruit gone but in fact the entire area is devoid of any reasonable open problems! “They attack the area like swarms of locusts devouring foliage in biblical lands, consuming not only fruit and leaves but even shrubs and twigs,” he added. “Yes, and only sh*t is left behind!” I chimed in.</p>
<p>As AGT is reaching that stage, with researchers yearning for new issues/problems for their students and their own research and grant proposals, there is a reprieve emerging on the horizon: a program at Simons on “<a href="https://simons.berkeley.edu/programs/market2019">Online and Matching-Based Market Design</a>” in Fall 2019.</p>
<p>This is by no means a new area. In fact, the first such market, for matching medical residents to hospitals, dates back to 1920s, well before the classic — and by now Nobel Prize winning — work of Gale and Shapley on the stable matching problem, which provided the canonical algorithm for this market. In recent years, the advent of the Internet and the relocation of our most important activities to online platforms have led to an explosion of such marketplaces (for examples, see the Simons web page) and they have been occupying an ever-increasing fraction of our economy.</p>
<p>AGT, CS and Economics have already had a massive impact via these markets. My own experience comes from Google’s multi-billion dollar Adwords market in which sophisticated algorithmic ideas have also played a central role, e.g., see the Simons talk “<a href="https://simons.berkeley.edu/talks/vijay-vazirani-4-30-18">Google’s AdWords Market: How Theory Influenced Practice</a>“.</p>
<p>Clearly, the stakes are high. There is a real opportunity of extending the already existing highly inter-disciplinary theory in substantial ways and making an even bigger impact. Considering this and the enthusiasm of the researchers who have agreed to be participants in this program, Federico Echenique, Nicole Immorlica and I have launched the project of publishing a comprehensive volume of contributed chapters on this topic. More information on this will be coming soon.</p></div>
    </content>
    <updated>2019-01-21T22:24:51Z</updated>
    <published>2019-01-21T22:24:51Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>robertkleinberg</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2019-01-26T20:20:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15597</id>
    <link href="https://rjlipton.wordpress.com/2019/01/21/a-simple-fact/" rel="alternate" type="text/html"/>
    <title>A Simple Fact</title>
    <summary>Can we find a simplest proof? Composite crop from src1, src2 Joseph Wedderburn and Leonard Dickson proved Wedderburn’s “Little” Theorem: that every finite ring with the zero-product property is a field. Which of them proved it first is hard to tell. Today we discuss the issue of finding simple proofs for simple facts—not just Wedderburn’s […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Can we find a simplest proof?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2019/01/wedderburndickson.png"><img alt="" class="alignright wp-image-15598" height="142" src="https://rjlipton.files.wordpress.com/2019/01/wedderburndickson.png?w=193&amp;h=142" width="193"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite crop from <a href="https://en.wikipedia.org/wiki/Joseph_Wedderburn">src1</a>, <a href="http://www.ams.org/about-us/presidents/14-dickson">src2</a></font></td>
</tr>
</tbody>
</table>
<p>
Joseph Wedderburn and Leonard Dickson proved Wedderburn’s “Little” Theorem: that every finite ring with the zero-product property is a field. Which of them proved it first is hard to tell.</p>
<p>
Today we discuss the issue of finding simple proofs for simple facts—not just Wedderburn’s theorem but the zero-product property on which it leans.</p>
<p>
Dickson was a professor at the University of Chicago when Wedderburn visited on a Carnegie Fellowship in 1904–05. To <a href="https://en.wikipedia.org/wiki/Wedderburn's_little_theorem#History">judge</a> <a href="https://books.google.com/books?id=0CApaWJwry8C&amp;pg=PA318&amp;lpg=PA318&amp;dq=In+pursuit+of+the+finite+division+algebra+theorem+and+beyond&amp;source=bl&amp;ots=esoyfvVJAl&amp;sig=ACfU3U0N05-c467jVhDz1ElYLcsE4e_9Og&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwjRvPv41_3fAhVOnuAKHdy3DgcQ6AEwBXoECAIQAQ#v=onepage&amp;q=In pursuit of the finite division algebra theorem and beyond&amp;f=false">from</a> <a href="https://books.google.com/books?id=O7R-DwAAQBAJ&amp;pg=PA105&amp;lpg=PA105&amp;dq=wedderburn+dickson+proofs&amp;source=bl&amp;ots=VdkQrceyrN&amp;sig=ACfU3U3u7Qn3ibDSszBPg0AitsrXUNUZpA&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwixkqzl0f3fAhWMTt8KHVo9DVgQ6AEwBnoECAUQAQ#v=onepage&amp;q=wedderburn dickson proofs&amp;f=false">several</a> <a href="https://api.research-repository.uwa.edu.au/portalfiles/portal/9699048/BambergPenttilaRevised.pdf">sources</a>, what happened is that Wedderburn first claimed a proof. Dickson did not believe the <em>result</em> and tried to build a counterexample. Instead he found a lemma that convinced him it was true and used it to give a simpler proof. They gave back-to-back presentations on 20 January, 1905, Wedderburn wrote up his paper with his proof and two more based on the lemma, which Wedderburn ascribed to an earlier paper by George Birkhoff and Harry Vandiver. Dickson wrote a paper with his approach, saying in a footnote:</p>
<blockquote><p><b> </b> <em> First proved by Wedderburn … Following my simpler proof above, and using the same lemma, Wedderburn constructed two further simpler proofs. </em>
</p></blockquote>
<p/><p>
However, Wedderburn’s first proof was found to have a gap. As <a href="https://www.math.uni-bielefeld.de/lag/man/099.pdf">detailed</a> by Michael Adam and Birte Specht, the gap was a statement that was not false <em>per se</em> but whose vague argument used only properties from a class of weaker structures in which it can fail. So:</p>
<blockquote><p><b> </b> <em> Who first proved the theorem? </em>
</p></blockquote>
<p>
</p><p/><h2> Other Proofs, Other Properties </h2><p/>
<p/><p>
Our sources linked above differ on whether the gap was noticed at the time or anytime before Emil Artin remarked on it in 1927. Artin didn’t discuss the gap but gave his own proof instead. There <a>seems</a> <a href="https://www.theoremoftheday.org/Docs/WedderburnShamil.pdf">to</a> <a href="https://ysharifi.wordpress.com/2011/09/24/wedderburns-little-theorem-1/">be</a> <a href="https://en.wikipedia.org/wiki/Wedderburn's_little_theorem#Proof">consensus</a> that the “proof from the Book” was <a href="http://www.math.leidenuniv.nl/~hfinkeln/seminarium/eindige_delingsring_is_lichaam.pdf">given</a> by Ernst Witt four years later in 1931. But two other proofs, <a href="https://www.jstor.org/stable/2311457?seq=1#page_scan_tab_contents">by</a> Israel Herstein and <a href="https://www.cambridge.org/core/journals/glasgow-mathematical-journal/article/grouptheoretic-proof-of-a-theorem-of-maclaganwedderburn/7BBA5D9EA9EDEE000BAE4859EDE64DA6">by</a> Hans Zassenhaus, receive prominent mention. </p>
<p>
As our sources attest, interest in finding other proofs continues. The surprise in the theorem, which is that finiteness and the zero-product property force the multiplication to be commutative, informs what happens in other mathematical structures. Proofs that draw on results about these structures create connections among many areas. The shortest proof drawing on deeper results was a two-page <a href="https://www.jstor.org/stable/2312328">paper</a> in the summer 1964 <em>Monthly</em> by Theodore Kaczynski (of <a href="https://www.fbi.gov/history/famous-cases/unabomber">ill</a> <a href="https://www.biography.com/news/unabomber-ted-kaczynski-today">note</a>). We won’t try to compare all these proofs, but will try to flip the question by focusing on the other property involved—the zero-product property:</p>
<blockquote><p><b> </b> <em> If <img alt="{a \cdot b = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%5Ccdot+b+%3D+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{a \cdot b = 0}"/> then <img alt="{a = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%3D+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{a = 0}"/> or <img alt="{b = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb+%3D+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{b = 0}"/>. </em>
</p></blockquote>
<p/><p>
Given a ring <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/> with this property, how easy is it to prove? If <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/> has inverses then it is immediate, else multiplying both sides of <img alt="{a\cdot b = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%5Ccdot+b+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a\cdot b = 0}"/> by <img alt="{a^{-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%5E%7B-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a^{-1}}"/> in front and <img alt="{b^{-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%5E%7B-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b^{-1}}"/> in back would yield the contradiction <img alt="{1 = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 = 0}"/>. And we know if <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R}"/> is finite then this property makes it a field. So our quest for a different angle leads us to thinking about infinite rings. </p>
<p>
Incidentally, a ring with the zero-product property is called a <em>domain</em>. If the multiplication is commutative then it is an <em>integral domain</em>, though Serge Lang preferred the term <em>entire ring</em>. Our example goes beyond the integers though.</p>
<p>
</p><p/><h2> No Zero-Divisors I </h2><p/>
<p/><p>
A natural example that arises is the ring of integer polynomials over multiple variables. Thus for two variables <img alt="{x,y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x,y}"/> the elements of this ring are 	</p>
<p align="center"><img alt="\displaystyle  \sum_{i,j} c_{ij}x^{i}y^{j}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%2Cj%7D+c_%7Bij%7Dx%5E%7Bi%7Dy%5E%7Bj%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{i,j} c_{ij}x^{i}y^{j}. "/></p>
<p>It is easy to see that they form a ring with the usual high school rules for adding and multiplying polynomials. The ring is an integral domain in general.</p>
<p>
To show this, we claim that it has no zero-divisors. Suppose that it does: let <img alt="{f(x,y) \cdot g(x,y) = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%2Cy%29+%5Ccdot+g%28x%2Cy%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x,y) \cdot g(x,y) = 0}"/>. Then let 	</p>
<p align="center"><img alt="\displaystyle  f(x,y) = \sum_{k} f_{k}(x) y^{k}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%2Cy%29+%3D+%5Csum_%7Bk%7D+f_%7Bk%7D%28x%29+y%5E%7Bk%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f(x,y) = \sum_{k} f_{k}(x) y^{k}, "/></p>
<p>and 	</p>
<p align="center"><img alt="\displaystyle  g(x,y) = \sum_{k} g_{k}(x) y^{k}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%28x%2Cy%29+%3D+%5Csum_%7Bk%7D+g_%7Bk%7D%28x%29+y%5E%7Bk%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  g(x,y) = \sum_{k} g_{k}(x) y^{k}. "/></p>
<p>Assume that the maximum degree in <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> for <img alt="{f(x,y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x,y)}"/> is <img alt="{df}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bdf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{df}"/> and is <img alt="{dg}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bdg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{dg}"/> for <img alt="{g(x,y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g(x,y)}"/>. Then <img alt="{f_{df}(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_%7Bdf%7D%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_{df}(x)}"/> and <img alt="{g_{df}(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg_%7Bdf%7D%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g_{df}(x)}"/> are both non-zero polynomials in one variable. It is clear by induction that <img alt="{f_{df}(x) \cdot g_{dg}(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_%7Bdf%7D%28x%29+%5Ccdot+g_%7Bdg%7D%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_{df}(x) \cdot g_{dg}(x)}"/> is not zero. This proves the claim.</p>
<p>
</p><p/><h2> No Zero-Divisors II </h2><p/>
<p/><p>
The trouble with the above property of polynomials is that it is only about formal objects. In many applications to computer science we want to view polynomials as objects that can be evaluated. So a natural issue is a slightly different property: Suppose that <img alt="{F(x,y,\dots)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28x%2Cy%2C%5Cdots%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(x,y,\dots)}"/> and <img alt="{G(x,y,\dots)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%28x%2Cy%2C%5Cdots%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G(x,y,\dots)}"/> are two integral polynomials. Suppose further that 	</p>
<p align="center"><img alt="\displaystyle  F(a,b,\dots) \cdot G(a,b,\dots) = 0, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28a%2Cb%2C%5Cdots%29+%5Ccdot+G%28a%2Cb%2C%5Cdots%29+%3D+0%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F(a,b,\dots) \cdot G(a,b,\dots) = 0, "/></p>
<p>for all integers <img alt="{a,b,\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%2Cb%2C%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a,b,\dots}"/>. Then it clearly must be the case that either <img alt="{F=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F=0}"/> identically or <img alt="{G=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G=0}"/> identically. Right?</p>
<p>
The trouble is that this seems to be obvious but how do we prove that it is true? Note, it must be the case that some use of the fact that <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> and <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> are polynomials. The fact is not true for more complex functions. For example, 	</p>
<p align="center"><img alt="\displaystyle  \cos(\pi x) \cdot \sin(\pi x) = 0, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Ccos%28%5Cpi+x%29+%5Ccdot+%5Csin%28%5Cpi+x%29+%3D+0%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \cos(\pi x) \cdot \sin(\pi x) = 0, "/></p>
<p>for all integers <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>. But neither the function <img alt="{\cos(\pi x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccos%28%5Cpi+x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\cos(\pi x)}"/> nor <img alt="{\sin(\pi x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csin%28%5Cpi+x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sin(\pi x)}"/> is identically zero for all integers.</p>
<p>
</p><p/><h2> A Proof </h2><p/>
<p/><p>
Here is a relatively simple proof of the fact. It uses the famous Schwarz-Zippel (SZ) Theorem. See <a href="https://rjlipton.wordpress.com/2009/11/30/the-curious-history-of-the-schwartz-zippel-lemma/">here</a> for our discussion of the theorem. </p>
<blockquote><p><b>Theorem 1</b> <em> Let <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{P}"/> be in <img alt="{\mathbb{F}[x_1,x_2,\ldots,x_n]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D%5Bx_1%2Cx_2%2C%5Cldots%2Cx_n%5D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathbb{F}[x_1,x_2,\ldots,x_n]}"/> be a non-zero polynomial of total degree <img alt="{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{d}"/> over a field <img alt="{\mathbb{F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathbb{F}}"/>. Let <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{S}"/> be a finite subset of <img alt="{\mathbb{F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathbb{F}}"/> and let <img alt="{r_{1},\dots,r_{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br_%7B1%7D%2C%5Cdots%2Cr_%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{r_{1},\dots,r_{n}}"/> be selected at random independently and uniformly from <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{S}"/>. Then 	</em></p><em>
<p align="center"><img alt="\displaystyle  \Pr[P(r_1,r_2,\ldots,r_n)=0] \leq \frac{d}{|S|}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPr%5BP%28r_1%2Cr_2%2C%5Cldots%2Cr_n%29%3D0%5D+%5Cleq+%5Cfrac%7Bd%7D%7B%7CS%7C%7D.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  \Pr[P(r_1,r_2,\ldots,r_n)=0] \leq \frac{d}{|S|}. "/></p>
</em><p><em/>
</p></blockquote>
<p>
The S-Z lemma of course has manifest applications throughout complexity theory. Often it is used in design of randomized algorithms. What we found interesting is that here we use it for a different purpose: to prove a structural property of polynomials.</p>
<p>
Back to our fact. Assume that 	</p>
<p align="center"><img alt="\displaystyle  F(a,b) \cdot G(a,b) = 0, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28a%2Cb%29+%5Ccdot+G%28a%2Cb%29+%3D+0%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F(a,b) \cdot G(a,b) = 0, "/></p>
<p>for all integers <img alt="{a \in S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%5Cin+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a \in S}"/> and <img alt="{b \in S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb+%5Cin+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b \in S}"/> for some finite set. Now either <img alt="{F(a, b)=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28a%2C+b%29%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(a, b)=0}"/> or <img alt="{G(a, b)=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%28a%2C+b%29%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G(a, b)=0}"/> must be true for at least one half of the values in <img alt="{S \times S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS+%5Ctimes+S%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S \times S}"/>. Assume that it is <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/>. Then by the S-Z theorem it must be that <img alt="{F=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F=0}"/> always if the set <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> is large enough. Therefore, the fact is proved.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Is there a simpler proof of this key fact? Is it possible to find an easier reference in the literature of this basic fact of polynomials? We have yet to discover one—any help would be appreciated.</p>
<p/></font></font></div>
    </content>
    <updated>2019-01-21T18:37:22Z</updated>
    <published>2019-01-21T18:37:22Z</published>
    <category term="History"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="Teaching"/>
    <category term="algebra"/>
    <category term="fields"/>
    <category term="history of mathematics"/>
    <category term="Joseph Wedderburn"/>
    <category term="Leonard Dickson"/>
    <category term="polynomials"/>
    <category term="rings"/>
    <category term="Schwartz-Zippel"/>
    <category term="Wedderburn's Little Theorem"/>
    <category term="zero divisors"/>
    <category term="zero-product property"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-01-26T20:20:42Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-800609330387332388</id>
    <link href="https://blog.computationalcomplexity.org/feeds/800609330387332388/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/01/acm-prize-and-some-thoughts-on-godel.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/800609330387332388" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/800609330387332388" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/01/acm-prize-and-some-thoughts-on-godel.html" rel="alternate" type="text/html"/>
    <title>ACM prize and some thoughts on the Godel Prize</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><br/>
<br/>
As Lance Tweeted, and I will re-iterate, nominations for the following prizes<br/>
are due soon and you can nominate people <a href="https://sigact.org/articles/prizes.html">here</a><br/>
<br/>
Godel Prize for outstanding paper in TCS. (Godel mentioned P vs NP in a letter to Von Neumann. I've heard it said that its too bad they didn't work on it-- either it would be solved or we'd know its hard. Frankly, I think enough smart people have worked on it that we already know its hard.)<br/>
<br/>
Knuth Prize for outstanding contributions to foundations of Computer Science. (its a greater honor to have a prize  named after you in your lifetime then to win a prize!)<br/>
<br/>
Dijkstra Prize (I wonder if having `ijk' in his name inspired him to work in algorithms)<br/>
<br/>
Kanellakis Theory and Practice Award.<br/>
<br/>
Lawler Award for Humanitarian Contributions within CS and Informatics.<br/>
<br/>
ACM Distinguished Service Award<br/>
<br/>
Danny Lewin Best Student Paper Award (best student paper at STOC)<br/>
<br/>
The Best Paper award (Best paper at STOC, Best paper at FOCS)<br/>
<br/>
(The last two I doubt you can nominate someone for.)<br/>
<br/>
A few thoughts on the Godel Prize:<br/>
<div>
<br/></div>
<div>
<div>
<br/></div>
<div>
1) You can win the Godel Prize twice and some people have: Goldwasser, Hastad, Arora, Szegedy, Spielman, Teng. Spielman-Teng have won it as a team twice.</div>
<div>
<br/></div>
<div>
2) GLAD there is no limit to how many can win. If a paper has a lot of people on it (and this has happened) then FINE, they're all winners! According to The Big Bang Theory (the TV show, not the theory) in Physics at most 3 can win a Nobel Prize in Physics for the same breakthrough in a given year. The show itself shows how stupid the policy is.</div>
<div>
<br/></div>
<div>
3) I either knew and forgot or never knew that DPDA Equiv is decidable! Glad to now it just in time for teaching Automata theory this spring.</div>
<div>
<br/></div>
<div>
4) Looking over the list reminded me that there are some papers in the intersection of those I want to read and those I am able to read! Though not many. Most I want to read but they seem hard.</div>
<div>
<br/></div>
<div>
5) The Kanellakis award is for theory that is PRACTICAL. Could someone win a Godel AND a Kannellakis award for the same paper (or set of papers). I found one sort-of case ( (a) below) and one definite case ( (b) below).</div>
<div>
<br/></div>
<div>
a) Moshe and Wolper won the 2000 Godel Prize for Temporal Logic and Finite Automata (I should also read that before my class starts)</div>
<div>
<br/></div>
<div>
Holtzmann, Kurshan, Vardi, and Wolpert won the 2005 Kanellakis prize for Formal Verification Tools.</div>
<div>
<br/></div>
<div>
I assume the two works are related.</div>
<div>
<br/></div>
<div>
b) Freund and Schapire won the 2003 Godel Prize and the 2004 Kanellakis Award, both for their work on boosting in Machine Learning.</div>
<div>
<br/></div>
<div>
6) Why is it the  Godel <i>Prize </i>and the Kanellakis <i>Award? </i>What is the diff between a prize and an award? A quick Google Search says that an Award is a token of effort and merit, while a Prize is something you win in a competition. I doubt that applies. I suspect they are called Prize and Award from historical accident. Does anyone know?</div>
<div>
<br/></div>
</div></div>
    </content>
    <updated>2019-01-20T22:44:00Z</updated>
    <published>2019-01-20T22:44:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>https://plus.google.com/101693130490639305932</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-01-26T19:33:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/008</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/008" rel="alternate" type="text/html"/>
    <title>TR19-008 |  Efficiently factoring polynomials modulo $p^4$ | 

	Ashish Dwivedi, 

	Rajat Mittal, 

	Nitin Saxena</title>
    <summary>Polynomial factoring has famous practical algorithms over fields-- finite, rational \&amp; $p$-adic. However, modulo prime powers it gets hard as there is non-unique factorization and a combinatorial blowup ensues. For example, $x^2+p \bmod p^2$ is irreducible, but $x^2+px \bmod p^2$ has exponentially many factors! We present the first randomized poly($\deg f, \log p$) time algorithm to factor a given univariate integral $f(x)$ modulo $p^k$, for a prime $p$ and $k \leq 4$. Thus, we solve the open question of factoring modulo $p^3$ posed in (Sircana, ISSAC'17). 

Our method reduces the general problem of factoring $f(x) \bmod p^k$ to that of {\em root finding} in a related polynomial $E(y) \bmod\langle p^k, \varphi(x)^\ell \rangle$ for some irreducible $\varphi \bmod p$. We could efficiently solve the latter for $k\le4$, by incrementally transforming $E(y)$. Moreover, we discover an efficient and strong generalization of Hensel lifting to lift factors of $f(x) \bmod p$ to those  $\bmod\ p^4$ (if possible). This was previously unknown, as the case of repeated factors of $f(x) \bmod p$ forbids classical Hensel lifting.</summary>
    <updated>2019-01-20T09:00:53Z</updated>
    <published>2019-01-20T09:00:53Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-01-26T20:20:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7400</id>
    <link href="https://windowsontheory.org/2019/01/20/the-firewall-paradox-in-context/" rel="alternate" type="text/html"/>
    <title>Why physicists care about the Firewall Paradox</title>
    <summary>[Guest post by Noah Miller – a Harvard Physics Ph.D student that took our seminar. Noah’s webpage contains wonderful and extensive notes that can be of interest to computer scientists. –Boaz] (The following blog post serves as an introduction to the following notes:) Black Holes, Hawking Radiation, and the Firewall There are many different types of “theoretical physicists.” There are theoretical […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Guest post by </em><a href="https://scholar.harvard.edu/noahmiller"><em>Noah Miller</em></a><em> – a Harvard Physics Ph.D student that took our seminar. Noah’s webpage contains wonderful and extensive notes that can be of interest to computer scientists. –Boaz]</em></p>



<p/>



<p>(The following blog post serves as an introduction to the following notes:)</p>



<p><a href="https://scholar.harvard.edu/files/noahmiller/files/firewall.pdf">Black Holes, Hawking Radiation, and the Firewall</a></p>



<p>There are many different types of “theoretical physicists.” There are theoretical astrophysicists, theoretical condensed matter physicists, and even theoretical biophysicists. However, the general public seems to be most interested in the exploits of what you might call “theoretical high energy theorists.” (Think Stephen Hawking.)</p>



<p>The holy grail for theoretical high energy physicists (who represent only a small fraction of all physicists) would be to find a theory of quantum gravity. As it stands now, physicists have two theories of nature: quantum field theory (or, more specifically, the “Standard Model”) and Einstein’s theory of general relativity. Quantum field theory describes elementary particles, like electrons, photons, quarks, gluons, etc. General relativity describes the force of gravity, which is really just a consequence of the curvature of spacetimes. </p>



<p>Sometimes people like to say that quantum field theory describes “small stuff” like particles, while general relativity describes “big stuff” like planets and stars. This is maybe not the best way to think about it, though, because planets and stars are ultimately just made out of a bunch of quantum particles.</p>



<p>Theoretical physicists are unhappy with having two theories of nature. In order to describe phenomena that depend on both quantum field theory and general relativity, the two theories must be combined in an “ad hoc” way. A so-called “theory of everything,” another name for the currently unknown theory of “quantum gravity,” would hypothetically be able to describe all the phenomena we know about. Just so we’re all on the same page, they don’t even have a fuly worked out hypothesis. (“String theory,” a popular candidate, is still not even a complete “theory” in the normal sense of the word, although it could become one eventually.)</p>



<p>So what should these high energy theoretical physicists do if they want to discover what this theory of quantum gravity is? For the time being, nobody can think up an experiment that would be sensitive to quantum gravitation effects which is feasible with current technology. We are limited to so-called “thought experiments.”</p>



<p>This brings us to Hawking radiation. In the 1970’s, Stephen Hawking considered what would happen to a black hole once quantum field theory was properly taken into account. (Of course, this involved a bit of “ad hoc” reasoning, as mentioned previously.) Hawking found that, much to everybody’s surprise, the black hole evaporated, realeasing energy in the form of “Hawking radiation” (mostly low energy photons). More strangely, this radiation comes out exactly in the spectrum you would expect from something “hot.” For example, imagine heating a piece of metal. At low temperatures, it emits low energy photons invisible to the human eye. Once it gets hotter, it glows red, then yellow, then perhaps eventually blue. The spectrum of light emitted follows a very specific pattern. Amazingly, Hawking found that the radiation which black holes emit follow the exact same pattern. By analogy, they have a temperature too!</p>



<p>This is more profound than you might realize. This is because things which have an temperature should also have an “entropy.” You see, there are two notions of “states” in physics: “microstates” and “macrostates.” A microstate gives you the complete physical information of what comprises a physical system. For example, imagine you have a box of gas, which contains many particles moving in a seemingly random manner. A “microstate” of this box would be a list of all the positions and momenta of every last particle in that box. This would be impossible to measure in pratice. A “macrostate,” on the other hand, is a set of microstates. You may not know what the exact microstate your box of gas is in, but you can measure macroscopic quantities (like the total internal energy, volume, particle number) and consider the set of all possible microstates with those measured quantities.</p>



<p>The “entropy” of a macrostate is the logarithm of the number of possible microstates. If black holes truly are thermodynamic systems with some entropy, that means there should be some “hidden variables” or microstates to the black hole that we currently don’t understand. Perhaps if we understood the microstates of the black hole, we would be much closer to understanding quantum gravity!</p>



<p>However, Hawking also discovered something else. Because the black hole is radiating out energy, its mass will actually decrease as time goes on. Eventually, it should disappear entirely. This means that the information of what went into the black hole will be lost forever.</p>



<p>Physicists did not like this, however, because it seemed to them that the information of what went into the black hole should never be lost. Many physicists believe that the information of what went into the black hole should somehow be contained in the outgoing Hawking radiation, although they do not currently understand how. According to Hawking’s original calculation, the Hawking radiation only depends on a parameters of the black hole (like its mass) and have nothing to do with the many finer details on what went in, the exact “microstate” of what went in.</p>



<p>However, physicists eventually realized a problem with the idea that the black hole releases its information in the form of outgoing Hawking radiation. The problem has to do with quantum mechanics. In quantum mechanics, it is impossible to clone a qubit. That means that if you threw a qubit in a black hole and then waited for it to eventually come out in the form of Hawking radiation, then the qubit could no longer be “inside” the black hole. However, if Einstein is to be believed, you should also be able to jump into the black hole and see the qubit on the inside. This seems to imply that the qubit is cloned, as it is present on both the inside and outside of the black hole.</p>



<p>Physicists eventually came up with a strange fix called “Black Hole Complementarity” (BHC). According to BHC, according to people outside the black hole, the interior does not exist. Also, according to people who have entered the black hole, the outside ceases to exist. Both descriptions of the world are “correct” because once someone has entered the black hole, they will be unable to escape and compare notes with the person on the outside.</p>



<p>Of course, it must be emphasized that BHC remains highly hypothetical. People have been trying to poke holes in it for a long time. The largest hole is the so called “Firewall Paradox,” first proposed in 2012. Essentially, the Firewall paradox tries to show that the paradigm of BHC is self-contradictory.  In fact, it was able to use basic quantum mechanics to show that, under some reasonable assumptions, the interior of the black hole <em>truly</em> doesn’t exist, and that anyone who tries to enter would be fried at an extremely hot “firewall!” Now, I don’t think most physicists actually believe that black holes really have a firewall (although this might depend on what day of the week you ask them). The interesting thing about the Firewall paradox is that it derives a seemingly crazy result from seemingly harmless starting suppositions. So these suppositions would have to be tweaked in the theory of quantum gravity order to get rid of the firewall.</p>



<p>This is all to say that all this thinking about black holes really might help physicists figure out something about quantum gravity. (Then again, who can really say for sure.)</p>



<p>If you would like to know more about the Firewall paradox, I suggest you read my notes, pasted at the top of this post!</p>



<p>The goal of the notes was to write an introduction to the Black Hole Information Paradox and Firewall Paradox that could be read by computer scientists with no physics background.</p>



<p>The structure of the notes goes as follows:</p>



<ol><li>Special relativity</li><li>General relativity</li><li>Quantum Field Theory (in which I ambitiously tell you what QFT actually is!)</li><li>Statistical Mechanics (this is the best part)</li><li>Hawking radiation</li><li>The information paradox</li></ol>



<p>Because the information paradox touches on all areas of physics, I thought it was necessary to take “zero background, infinite intelligence” approach, introducing the all the necessary branches of physics (GR, QFT, Stat Mech) in order to understand what the deal with Hawking radiation really is, and why physicists think it is so important. I think it is safe to say that if you read these notes, you’ll learn a non-trivial amount of physics.</p></div>
    </content>
    <updated>2019-01-20T06:00:25Z</updated>
    <published>2019-01-20T06:00:25Z</published>
    <category term="physics"/>
    <category term="cs229r"/>
    <author>
      <name>noahmiller5490</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2019-01-26T20:21:14Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/01/18/postdoc-at-ut-austin-apply-by-february-5-2019/</id>
    <link href="https://cstheory-jobs.org/2019/01/18/postdoc-at-ut-austin-apply-by-february-5-2019/" rel="alternate" type="text/html"/>
    <title>Postdoc at UT Austin (apply by February 5, 2019)</title>
    <summary>UT Austin invites applications for a Postdoctoral Fellow in theoretical computer science for the 2019-20 academic year to work with David Zuckerman. Research interests should overlap with his: pseudorandomness, computational complexity, coding theory, and more. Applications will be considered until the position is filled, but review of applicants will begin on February 5. Website: https://utaustin.wd1.myworkdayjobs.com/en-US/UTstaff/job/UT-MAIN-CAMPUS/Postdoctoral-Fellow_R_00001518 […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>UT Austin invites applications for a Postdoctoral Fellow in theoretical computer science for the 2019-20 academic year to work with David Zuckerman. Research interests should overlap with his: pseudorandomness, computational complexity, coding theory, and more. Applications will be considered until the position is filled, but review of applicants will begin on February 5.</p>
<p>Website: <a href="https://utaustin.wd1.myworkdayjobs.com/en-US/UTstaff/job/UT-MAIN-CAMPUS/Postdoctoral-Fellow_R_00001518">https://utaustin.wd1.myworkdayjobs.com/en-US/UTstaff/job/UT-MAIN-CAMPUS/Postdoctoral-Fellow_R_00001518</a><br/>
Email: diz@utexas.edu</p></div>
    </content>
    <updated>2019-01-18T16:52:08Z</updated>
    <published>2019-01-18T16:52:08Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-01-26T20:20:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://thmatters.wordpress.com/?p=1263</id>
    <link href="https://thmatters.wordpress.com/2019/01/18/sigact-award-deadlines/" rel="alternate" type="text/html"/>
    <title>SIGACT Award deadlines</title>
    <summary>From the SIGACT executive committee: The deadlines to submit nominations for the Gödel Prize, Knuth Prize, and SIGACT Distinguished Service Award are coming soon. Calls for nominations for all three awards can be found at the links below. Note that March 1 is now the permanent deadline for SIGACT Distinguished Service Award nominations, this year […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>From the SIGACT executive committee:</p>
<p>The deadlines to submit nominations for the Gödel Prize, Knuth Prize, and SIGACT Distinguished Service Award are coming soon. Calls for nominations for all three awards can be found at the links below. Note that March 1 is now the permanent deadline for SIGACT Distinguished Service Award nominations, this year and in future years.</p>
<ul>
<li><a href="https://sigact.org/prizes/g%C3%B6del/g%C3%B6del_call19.pdf" rel="noopener" target="_blank">Gödel Prize</a>: deadline <strong>February 15, 2019</strong></li>
<li><a href="https://sigact.org/prizes/knuth/knuth19.pdf" rel="noopener" target="_blank">Knuth Prize</a>: deadline <strong>February 15, 2019</strong></li>
<li><a href="https://sigact.org/prizes/service.html" rel="noopener" target="_blank">SIGACT Distinguished Service Award</a>: deadline <strong>March 1, every year (including 2019)</strong><br/>
<em>Those who intend to submit a nomination for the Distinguished Service Award are strongly encouraged to inform the Selection Committee Chair at least two weeks in advance.</em></li>
</ul></div>
    </content>
    <updated>2019-01-18T14:37:13Z</updated>
    <published>2019-01-18T14:37:13Z</published>
    <category term="awards"/>
    <category term="Deadlines"/>
    <author>
      <name>shuchic</name>
    </author>
    <source>
      <id>https://thmatters.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://thmatters.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://thmatters.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://thmatters.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://thmatters.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theory Matters</title>
      <updated>2019-01-26T20:21:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3375</id>
    <link href="https://agtb.wordpress.com/2019/01/18/upcoming-nomination-deadlines-godel-knuth-and-sigact-distinguished-service-award/" rel="alternate" type="text/html"/>
    <title>Upcoming Nomination Deadlines: Gödel, Knuth, and SIGACT Distinguished Service Award</title>
    <summary>The deadlines to submit nominations for the Gödel Prize, Knuth Prize, and SIGACT Distinguished Service Award are coming soon. Calls for nominations for all three awards can be found at the links below. Note that March 1 is now the permanent deadline for SIGACT Distinguished Service Award nominations, this year and in future years. Gödel […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div>
<p>The deadlines to submit nominations for the Gödel Prize, Knuth Prize, and SIGACT Distinguished Service Award are coming soon. Calls for nominations for all three awards can be found at the links below. Note that March 1 is now the permanent deadline for SIGACT Distinguished Service Award nominations, this year and in future years.</p>
<ul>
<li><a href="https://sigact.org/prizes/g&#xF6;del/g&#xF6;del_call19.pdf" rel="noopener" target="_blank">Gödel Prize</a>: deadline <strong>February 15, 2019</strong></li>
<li><a href="https://sigact.org/prizes/knuth/knuth19.pdf" rel="noopener" target="_blank">Knuth Prize</a>: deadline <strong>February 15, 2019</strong></li>
<li><a href="https://sigact.org/prizes/service.html" rel="noopener" target="_blank">SIGACT Distinguished Service Award</a>: deadline <strong>March 1, every year (including 2019)</strong><br/>
<em>Those who intend to submit a nomination for the Distinguished Service Award are strongly encouraged to inform the Selection Committee Chair at least two weeks in advance.</em></li>
</ul>
<div class="yj6qo"/>
<div class="adL"/>
</div></div>
    </content>
    <updated>2019-01-18T03:35:08Z</updated>
    <published>2019-01-18T03:35:08Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>robertkleinberg</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2019-01-26T20:20:41Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/01/17/orientations-infinite-graphs</id>
    <link href="https://11011110.github.io/blog/2019/01/17/orientations-infinite-graphs.html" rel="alternate" type="text/html"/>
    <title>Orientations of infinite graphs</title>
    <summary>An orientation of an undirected graph is the directed graph that you get by assigning a direction to each edge. Several kinds of orientations have been studied. For instance, in a graph with even vertex degrees, an Eulerian orientation makes the numbers of incoming and outgoing degrees equal at each vertex. In a bridgeless graph, a strong orientation makes the resulting directed graph strongly connected. In finite connected graphs, every Eulerian orientation is strong, but that is untrue in infinite graphs. Consider, for instance, the graph of unit distances on the integers, which is Eulerian (every vertex has degree two) but has no strong orientation (every edge is a bridge). Even when a strong orientation exists, an Eulerian orientation might not be strong: the graph of distance-1 and distance-2 integers, with the orientation from smaller to larger numbers, is Eulerian but not strong. So when does an Eulerian strong orientation exist? The answer turns out to be: whenever the obvious obstacles are not present. Every bridgeless connected even-degree infinite graph has an Eulerian strong orientation.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>An <a href="https://en.wikipedia.org/wiki/Orientation_(graph_theory)">orientation</a> of an undirected graph is the directed graph that you get by assigning a direction to each edge. Several kinds of orientations have been studied. For instance, in a graph with even vertex degrees, an Eulerian orientation makes the numbers of incoming and outgoing degrees equal at each vertex. In a bridgeless graph, a <a href="https://en.wikipedia.org/wiki/Strong_orientation">strong orientation</a> makes the resulting directed graph strongly connected. In finite connected graphs, every Eulerian orientation is strong, but that is untrue in infinite graphs. Consider, for instance, the graph of unit distances on the integers, which is Eulerian (every vertex has degree two) but has no strong orientation (every edge is a bridge). Even when a strong orientation exists, an Eulerian orientation might not be strong: the graph of distance-1 and distance-2 integers, with the orientation from smaller to larger numbers, is Eulerian but not strong. So when does an Eulerian strong orientation exist? The answer turns out to be: whenever the obvious obstacles are not present. Every bridgeless connected even-degree infinite graph has an Eulerian strong orientation.</p>

<p style="text-align: center;"><img alt="For infinite graphs, Eulerian orientations might not be strong" src="https://11011110.github.io/blog/assets/2019/euler-not-strong.svg"/></p>

<p>To prove this, we can use a convenient tool for dealing with infinite orientations by looking only at finite graphs, a result of Rado from 1949 that is simultaneously a predecessor and generalization of the <a href="https://en.wikipedia.org/wiki/De_Bruijn%E2%80%93Erd%C5%91s_theorem_(graph_theory)">De Bruijn–Erdős theorem on graph colorings</a>. Suppose each element in some infinite set  has a finite set of labels, and we choose an assignment of labels for each finite subset of . These choices may be inconsistent with each other, so there may be no way of labeling all of  consistently with all of the choices. But Rado proved (assuming the axiom of choice) that there exists a global labeling that, on every finite subset, is consistent with the assignment to one of its finite supersets. Another way of thinking about this is that if certain finite patterns must be avoided, and every finite subset has a labeling that avoids them, then some global labeling will also avoid them.<sup id="fnref:rado"><a class="footnote" href="https://11011110.github.io/blog/2019/01/17/orientations-infinite-graphs.html#fn:rado">1</a></sup> The De Bruijn–Erdős theorem is the case where the elements are vertices, the labels are colors, and the patterns to be avoided are pairs of equal color on adjacent vertices. In our case the set elements will be edges, the labels will be which way to orient each edge, and the choice of assignment will be some way of defining a “good” orientation for finite subgraphs.</p>

<p style="text-align: center;"><img alt="Graph vertices can be labeled by color; edges, by orientation" src="https://11011110.github.io/blog/assets/2019/graph-labels.svg"/></p>

<p>So suppose we’re given a finite subgraph  of an infinite 2-edge-connected even-degree graph. What kind of orientation on  should we look for? A complication is that  might have bridges or odd-degree vertices. So we’ll try to come as close as we can to what we want, an Eulerian strong orientation, while taking those deficiencies into account. Let’s define a good orientation of  to be an orientation that is almost Eulerian, in that at each vertex the in-degree and out-degree differ by at most one, and almost strong, in that each edge of  that belongs to an undirected cycle also belongs to a directed cycle. Another way of stating the almost strong condition is that the strongly connected components of the oriented graph should coincide with the 2-edge-connected components, or blocks, of the undirected graph.</p>

<p style="text-align: center;"><img alt="A good orientation of a graph" src="https://11011110.github.io/blog/assets/2019/good-orientation.svg"/></p>

<p>The existence of a good orientation of an arbitrary finite undirected graph (also having some other stronger properties) was proven in the 1960s by Nash-Williams.<sup id="fnref:nw"><a class="footnote" href="https://11011110.github.io/blog/2019/01/17/orientations-infinite-graphs.html#fn:nw">2</a></sup> But now Rado’s method proves that every infinite graph also has a good orientation. If we find a global orientation that’s nearly-Eulerian on a supergraph of the star of neighbors of every vertex, then it must be nearly-Eulerian. And if it’s almost strong on a supergraph of every cycle, then it must be almost strong. This proves the theorem that a bridgeless connected even-degree infinite graph has an Eulerian strong orientation, because every good orientation in these graphs must be Eulerian and strong. Nash-Williams already considered the infinite case of his orientation theorem, but wrote that the details were too heavy to include. Perhaps he didn’t know about Rado’s theorem (despite it being published in the same journal), which makes the extension from finite to infinite easy.</p>

<p>The same method of extending finite to infinite orientations can also be used to study notions of graph sparseness including <a href="https://en.wikipedia.org/wiki/Arboricity">arboricity</a>, <a href="https://en.wikipedia.org/wiki/Degeneracy_(graph_theory)">degeneracy</a>, and pseudoarboricity. Historically the pseudoarboricity of infinite graphs was first. A graph has pseudoarboricity at most  if it can be oriented so that each vertex has out-degree  (or equivalently if it can be partitioned into subgraphs each with out-degree one). In the 1950 paper in which he first announced the De Bruijn–Erdős theorem, Erdős used it to prove that, when such an orientation is given, the resulting graph can be -colored.<sup id="fnref:e"><a class="footnote" href="https://11011110.github.io/blog/2019/01/17/orientations-infinite-graphs.html#fn:e">3</a></sup> But he didn’t write about the conditions under which such an orientation exists. Rado’s theorem shows that they are the same as in the finite case: an outdegree- orientation exists if and only if every finite -vertex subgraph has at most  edges.</p>

<p>A graph has degeneracy at most  if it has an acyclic orientation so that each vertex has out-degree . Unlike in the finite case, infinite graphs with low degeneracy might have high minimum degree; for instance, there exist graphs with degeneracy one in which all vertices have infinite degree. A finite -degenerate graph can be -colored greedily, and the De Bruijn–Erdős theorem shows that even in the infinite case such a coloring exists. Rado’s theorem shows that infinite graphs with pseudoarboricity  are -degenerate, and that a graph is -degenerate if and only if every finite subgraph has a vertex with degree at most . As in the case of Eulerian strong orientations, this involves checking the orientation only on two kinds of finite subgraphs, stars and cycles.</p>

<p>Arboricity is based on forests and there are multiple incompatible definitions of infinite forests. But the one we want to use is that a forest is a 1-degenerate graph. A graph has arboricity  if its edges can be partitioned into  forests. This is clearly at least as large as the degeneracy (no Rado needed). Rado’s theorem shows that infinite graphs with arboricity  are -degenerate, and that a graph has arboricity at most  if and only if every finite -vertex subset has at most  edges.</p>

<div class="footnotes">
  <ol>
    <li id="fn:rado">
      <p>Rado, R. (1949), “<a href="https://doi.org/10.4153/cjm-1949-031-1">Axiomatic treatment of rank in infinite sets</a>”, <em>Canad. J. Math.</em> 1: 337–343. <a class="reversefootnote" href="https://11011110.github.io/blog/2019/01/17/orientations-infinite-graphs.html#fnref:rado">↩</a></p>
    </li>
    <li id="fn:nw">
      <p>Nash-Williams, C. St. J. A. (1960), “<a href="https://doi.org/10.4153/cjm-1960-049-6">On orientations, connectivity and odd-vertex-pairings in finite graphs</a>”, <em>Canad. J. Math.</em>, 12: 555–567; —— (1969), “Well-balanced orientations of finite graphs and unobtrusive odd-vertex-pairings”, <em>Recent Progress in Combinatorics (Proc. Third Waterloo Conf. on Combinatorics, 1968)</em>, New York: Academic Press, pp. 133–149. <a class="reversefootnote" href="https://11011110.github.io/blog/2019/01/17/orientations-infinite-graphs.html#fnref:nw">↩</a></p>
    </li>
    <li id="fn:e">
      <p>Erdős, P. (1950), “<a href="https://users.renyi.hu/~p_erdos/1950-13.pdf">Some remarks on set theory</a>”, <em>Proc. AMS</em>, 1: 127–141. <a class="reversefootnote" href="https://11011110.github.io/blog/2019/01/17/orientations-infinite-graphs.html#fnref:e">↩</a></p>
    </li>
  </ol>
</div>

<p>(<a href="https://mathstodon.xyz/@11011110/101435740463423422">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2019-01-17T20:39:00Z</updated>
    <published>2019-01-17T20:39:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-01-18T04:45:13Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8539994327270054973</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8539994327270054973/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/01/the-cost-of-privacy.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8539994327270054973" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8539994327270054973" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/01/the-cost-of-privacy.html" rel="alternate" type="text/html"/>
    <title>The Cost of Privacy</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody>
<tr><td style="text-align: center;"><a href="https://2.bp.blogspot.com/-8HqZkaSkvdw/XD82UgwmqHI/AAAAAAABmMQ/6zYMalG9fXUS5OF28uajL8MOiXo6igmaACLcBGAs/s1600/iphone.jpeg" style="margin-left: auto; margin-right: auto;"><img border="0" height="182" src="https://2.bp.blogspot.com/-8HqZkaSkvdw/XD82UgwmqHI/AAAAAAABmMQ/6zYMalG9fXUS5OF28uajL8MOiXo6igmaACLcBGAs/s200/iphone.jpeg" width="200"/></a></td></tr>
<tr><td class="tr-caption" style="text-align: center;">Billboard at 2019 CES</td></tr>
</tbody></table>
<br/>Computer scientists tend to obsess about privacy and we've had a privacy/security debate for decades now. But now machine learning has really given us a whole new spin on what privacy protects and takes away.<br/>
<div>
<br/>
I take an open approach and basically allow Google to know everything about my life. Google knows where I've been--sometimes my Pixel asks me which store in a shopping center I visited and I give up that info. Google knows who I communicate with, what websites I visit, what music and movies I listen to and watch, all my photos, what temperature makes me comfortable and so on.<br/>
<br/>
What do I get? A Google ecosystem that knows me sometimes better than I know myself. Google works best when it learns and integrates. I get asked to download maps for trips Google knows I'm about to take. I have Google assistant throughout my house, in my phone, in my car and it tailor answers and sometimes even the questions that I need answers to. If anything I wish there was further integration, like Google Voice should ring my office phone only when I'm in the office.<br/>
<br/>
Georgia Tech now forces us to use Microsoft Exchange for email. Outlook is not a bad email program but its capabilities, especially for search, does not work as well and think of all that unused knowledge.<br/>
<br/>
I trust Google to keep my information safe, with a random password and 2-factor encryption and even if someone would manage to break in they would find I'm a pretty boring person with an unhealthy obsession of opera (the musical form not the browser).<br/>
<br/>
Doesn't work for everyone and companies should make it easy to keep your info secure. But I say go use your machine learning on me and find ways to make my life easier and more fun, and sure send me some targeted ads as payment. The Internets will find a way to discover you anyway, might as well take advantage. </div></div>
    </content>
    <updated>2019-01-17T17:45:00Z</updated>
    <published>2019-01-17T17:45:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>https://plus.google.com/101693130490639305932</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>https://plus.google.com/101693130490639305932</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-01-26T19:33:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/007</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/007" rel="alternate" type="text/html"/>
    <title>TR19-007 |  Lower Bounds for Linear Decision Lists | 

	Arkadev Chattopadhyay, 

	Meena Mahajan, 

	Nikhil Mande, 

	Nitin Saurabh</title>
    <summary>We demonstrate a lower bound technique for linear decision lists, which are decision lists where the queries are arbitrary linear threshold functions.
We use this technique to prove an explicit lower bound by showing that any linear decision list computing the function $MAJ \circ XOR$ requires size $2^{0.18 n}$.  This completely answers an open question of Tur{\'a}n and Vatan [FoCM'97]. We also show that the spectral classes $PL_1, PL_\infty$, and the polynomial threshold function classes $\widehat{PT}_1, PT_1$,  are incomparable to linear decision lists.</summary>
    <updated>2019-01-17T16:40:39Z</updated>
    <published>2019-01-17T16:40:39Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-01-26T20:20:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2019/01/17/dimea-days-2019/</id>
    <link href="https://cstheory-events.org/2019/01/17/dimea-days-2019/" rel="alternate" type="text/html"/>
    <title>DIMEA Days 2019</title>
    <summary>June 12-13, 2019 Brno, Czech Republic https://www.fi.muni.cz/research/dimea/days19.html Registration deadline: March 31, 2019</summary>
    <updated>2019-01-17T10:51:30Z</updated>
    <published>2019-01-17T10:51:30Z</published>
    <category term="school"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2019-01-26T20:21:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2019/01/17/gdansk-summer-school-of-advanced-science-on-algorithms-for-discrete-optimization/</id>
    <link href="https://cstheory-events.org/2019/01/17/gdansk-summer-school-of-advanced-science-on-algorithms-for-discrete-optimization/" rel="alternate" type="text/html"/>
    <title>Gdańsk Summer School of Advanced Science on Algorithms for Discrete Optimization</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">July 6-12, 2019 Gdansk, Poland https://eti.pg.edu.pl/advanced-science-on-algorithms/advanced-science The school aims at providing an opportunity for graduate and undergraduate students as well as young researchers to get together and attend advanced courses and talks on current topics in the field of algorithms and data structures for discrete optimization problems. During 8 days of the school, 6 advanced … <a class="more-link" href="https://cstheory-events.org/2019/01/17/gdansk-summer-school-of-advanced-science-on-algorithms-for-discrete-optimization/">Continue reading <span class="screen-reader-text">Gdańsk Summer School of Advanced Science on Algorithms for Discrete Optimization</span></a></div>
    </summary>
    <updated>2019-01-17T10:51:24Z</updated>
    <published>2019-01-17T10:51:24Z</published>
    <category term="school"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2019-01-26T20:21:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2019/01/17/nice-summer-school-on-random-walks-and-complex-networks/</id>
    <link href="https://cstheory-events.org/2019/01/17/nice-summer-school-on-random-walks-and-complex-networks/" rel="alternate" type="text/html"/>
    <title>Nice summer school on random walks and complex networks</title>
    <summary>July 8-19, 2019 Nice, France https://math.unice.fr/~dmitsche/Summerschool/Summerschool.html Registration deadline: May 1, 2019 This 2-week-summer school aims to explain concepts on random walks and random models for complex networks at a level suitable for second year master students and PhD students in mathematics and theoretical computer science.</summary>
    <updated>2019-01-17T10:51:20Z</updated>
    <published>2019-01-17T10:51:20Z</published>
    <category term="school"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2019-01-26T20:21:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/006</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/006" rel="alternate" type="text/html"/>
    <title>TR19-006 |  Upper Bounds on Communication in terms of Approximate Rank | 

	Anna Gal, 

	Ridwan Syed</title>
    <summary>We show that any Boolean function with approximate rank $r$ can be computed by bounded error quantum protocols without prior entanglement of complexity $O( \sqrt{r} \log r)$. In addition, we show that any Boolean function with approximate rank $r$ and discrepancy $\delta$ can be computed by deterministic protocols of complexity $O(r)$, and private coin bounded error randomized protocols of complexity $O((\frac{1}{\delta})^2 + \log r)$. Our results yield lower bounds on approximate rank. We also obtain a strengthening of Newman's theorem with respect to approximate rank.</summary>
    <updated>2019-01-16T22:58:33Z</updated>
    <published>2019-01-16T22:58:33Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-01-26T20:20:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/005</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/005" rel="alternate" type="text/html"/>
    <title>TR19-005 |  An Exponential Lower Bound on the Sub-Packetization of MSR Codes | 

	Omar Alrabiah, 

	Venkatesan Guruswami</title>
    <summary>An $(n,k,\ell)$-vector MDS code is a $\mathbb{F}$-linear subspace of $(\mathbb{F}^\ell)^n$ (for some field $\mathbb{F}$) of dimension $k\ell$, such that any $k$ (vector) symbols of the codeword suffice to determine the remaining $r=n-k$ (vector) symbols. The length $\ell$ of each codeword symbol is called the sub-packetization of the code. Such a code is called minimum storage regenerating (MSR), if any single symbol of a codeword can be recovered by downloading $\ell/r$ field elements (which is known to be the least possible) from each of the other symbols.

MSR codes are attractive for use in distributed storage systems, and by now a variety of ingenious constructions of MSR codes are available. However, they all suffer from exponentially large sub-packetization of at least $r^{k/r}$. Our main result is an almost tight lower bound showing that for an MSR code, one must have $\ell \ge \exp(\Omega(k/r))$. Previously, a lower bound of $\approx \exp(\sqrt{k/r})$, and a tight lower bound for a restricted class of optimal access MSR codes, were known. Our work settles a key question concerning MSR codes that has received much attention, with a short proof hinging on one key definition that is somewhat inspired by Galois theory.</summary>
    <updated>2019-01-16T18:36:24Z</updated>
    <published>2019-01-16T18:36:24Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-01-26T20:20:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/01/16/motwani-postdoctoral-fellowship-at-stanford-computer-science-apply-by-december-21-2019/</id>
    <link href="https://cstheory-jobs.org/2019/01/16/motwani-postdoctoral-fellowship-at-stanford-computer-science-apply-by-december-21-2019/" rel="alternate" type="text/html"/>
    <title>Motwani Postdoctoral Fellowship at Stanford Computer Science (apply by December 21, 2019)</title>
    <summary>The theory group at Stanford invites applications for the Motwani postdoctoral fellowship in theoretical computer science. Information and application instructions below. Applications will be accepted until the positions are filled, but review of applicants will begin after Jan 21. Website: https://academicjobsonline.org/ajo/jobs/13134 Email: theory.stanford@gmail.com</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The theory group at Stanford invites applications for the Motwani postdoctoral fellowship in theoretical computer science. Information and application instructions below.</p>
<p>Applications will be accepted until the positions are filled, but review of applicants will begin after Jan 21.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/13134">https://academicjobsonline.org/ajo/jobs/13134</a><br/>
Email: theory.stanford@gmail.com</p></div>
    </content>
    <updated>2019-01-16T18:32:56Z</updated>
    <published>2019-01-16T18:32:56Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-01-26T20:20:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2019/01/15/9th-phd-summer-school-in-discrete-mathematics/</id>
    <link href="https://cstheory-events.org/2019/01/15/9th-phd-summer-school-in-discrete-mathematics/" rel="alternate" type="text/html"/>
    <title>9th PhD Summer School in Discrete Mathematics</title>
    <summary>June 30 – July 6, 2019 Rogla, Slovenia https://conferences.famnit.upr.si/event/12/ The summer school will offer two mini-courses: “Combinatorial limits and their applications in extremal combinatorics” and “Coxeter groups”, as well as invited talks and student talks</summary>
    <updated>2019-01-15T23:09:50Z</updated>
    <published>2019-01-15T23:09:50Z</published>
    <category term="school"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2019-01-26T20:21:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2019/01/15/5th-algorithmic-and-enumerative-combinatorics-summer-school-2019/</id>
    <link href="https://cstheory-events.org/2019/01/15/5th-algorithmic-and-enumerative-combinatorics-summer-school-2019/" rel="alternate" type="text/html"/>
    <title>5th Algorithmic and Enumerative Combinatorics Summer School 2019</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">July 29 – August 2, 2019 Hagenberg, Austria https://www3.risc.jku.at/conferences/aec2019/index.html n the spirit of the AEC2014, AEC2015, AEC2016 and AEC2018, the goal of this summer school is to put forward the interplay between the fields of Enumerative Combinatorics, Analytic Combinatorics, and Algorithmics. This is a very active research area, which, aside from the three fields fueling … <a class="more-link" href="https://cstheory-events.org/2019/01/15/5th-algorithmic-and-enumerative-combinatorics-summer-school-2019/">Continue reading <span class="screen-reader-text">5th Algorithmic and Enumerative Combinatorics Summer School 2019</span></a></div>
    </summary>
    <updated>2019-01-15T23:09:30Z</updated>
    <published>2019-01-15T23:09:30Z</published>
    <category term="school"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2019-01-26T20:21:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2019/01/15/spring-school-and-workshop-on-polytopes/</id>
    <link href="https://cstheory-events.org/2019/01/15/spring-school-and-workshop-on-polytopes/" rel="alternate" type="text/html"/>
    <title>Spring School and Workshop on Polytopes</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">March 11-15, 2019 Bochum, Germany https://www.rub.de/polytopes2019/ Registration deadline: February 11, 2019 The Spring School is designed for advanced bachelor, master and PhD students. The aim is to prepare the participants in such a way that they can follow the lectures of the subsequent workshop. It consists of three crash courses: Geometry of Polytopes, Monday (2x2h) … <a class="more-link" href="https://cstheory-events.org/2019/01/15/spring-school-and-workshop-on-polytopes/">Continue reading <span class="screen-reader-text">Spring School and Workshop on Polytopes</span></a></div>
    </summary>
    <updated>2019-01-15T23:08:56Z</updated>
    <published>2019-01-15T23:08:56Z</published>
    <category term="school"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2019-01-26T20:21:30Z</updated>
    </source>
  </entry>
</feed>
