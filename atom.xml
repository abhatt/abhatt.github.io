<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-02-25T08:22:55Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=532</id>
    <link href="https://tcsplus.wordpress.com/2021/02/24/tcs-talk-wednesday-march-3-steve-hanneke-ttic/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, March 3 — Steve Hanneke, TTIC</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, March 3th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). Steve Hanneke from TTIC will speak about “A Theory of Universal Learning” (abstract below). You can reserve a spot as an individual or a group to join us […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, March 3th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Steve Hanneke</strong> from TTIC will speak about “<em>A Theory of Universal Learning</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our website</a> aftwerwards, so people who did not sign up will still be able to watch the talk.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: How quickly can a given class of concepts be learned from examples? It is common to measure the performance of a supervised machine learning algorithm by plotting its “learning curve”, that is, the decay of the error rate as a function of the number of training examples. However, the classical theoretical framework for understanding learnability, the PAC model of Vapnik-Chervonenkis and Valiant, does not explain the behavior of learning curves: the distribution-free PAC model of learning can only bound the upper envelope of the learning curves over all possible data distributions. This does not match the practice of machine learning, where the data source is typically fixed in any given scenario, while the learner may choose the number of training examples on the basis of factors such as computational resources and desired accuracy.</p>
<p>In this work, we study an alternative learning model that better captures such practical aspects of machine learning, but still gives rise to a complete theory of the learnable in the spirit of the PAC model. More precisely, we consider the problem of universal learning, which aims to understand the performance of learning algorithms on every data distribution, but without requiring uniformity over the distribution. The main result of this work is a remarkable trichotomy: there are only three possible rates of universal learning. More precisely, we show that the learning curves of any given concept class decay either at an exponential, linear, or arbitrarily slow rates. Moreover, each of these cases is completely characterized by appropriate combinatorial parameters, and we exhibit optimal learning algorithms that achieve the best possible rate in each case.</p>
<p>Joint work with Olivier Bousquet, Shay Moran, Ramon van Handel, and Amir Yehudayoff.</p></blockquote></div>
    </content>
    <updated>2021-02-25T02:03:43Z</updated>
    <published>2021-02-25T02:03:43Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2021-02-25T08:21:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8008</id>
    <link href="https://windowsontheory.org/2021/02/24/unsupervised-learning-and-generative-models/" rel="alternate" type="text/html"/>
    <title>Unsupervised Learning and generative models</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Scribe notes by Richard Xu Previous post: What do neural networks learn and when do they learn it Next post: TBD. See also all seminar posts and course webpage. lecture slides (pdf) – lecture slides (Powerpoint with animation and annotation) – video In this lecture, we move from the world of supervised learning to unsupervised … <a class="more-link" href="https://windowsontheory.org/2021/02/24/unsupervised-learning-and-generative-models/">Continue reading <span class="screen-reader-text">Unsupervised Learning and generative models</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>Scribe notes by <a href="https://github.com/rxu18">Richard Xu</a></em></p>



<p><strong>Previous post:</strong> <a href="https://windowsontheory.org/2021/02/17/what-do-deep-networks-learn-and-when-do-they-learn-it/">What do neural networks learn and when do they learn it</a> <strong>Next post:</strong> TBD. See also <a href="https://windowsontheory.org/category/ml-theory-seminar/">all seminar posts</a> and <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">course webpage</a>.</p>



<p><a href="http://files.boazbarak.org/misc/mltheory/ML_seminar_lecture_3.pdf">lecture slides (pdf)</a> – <a href="http://files.boazbarak.org/misc/mltheory/ML_seminar_lecture_3.pptx">lecture slides (Powerpoint with animation and annotation)</a> – <a href="https://harvard.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=70cafab0-bdea-412b-a353-acc90173fd61">video</a></p>



<p>In this lecture, we move from the world of supervised learning to unsupervised learning, with a focus on generative models. We will</p>



<ul><li>Introduce unsupervised learning and the relevant notations.</li><li>Discuss various approaches for generative models, such as PCA, VAE, Flow Models, and GAN.</li><li>Discuss theoretical and practical results we currently have for these approaches.</li></ul>



<h2>Setup for Unsupervised Learning</h2>



<p>In <em>supervised learning</em>, we have data <img alt="x_i\sim p\subset \mathbb R^d" class="latex" src="https://s0.wp.com/latex.php?latex=x_i%5Csim+p%5Csubset+%5Cmathbb+R%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_i\sim p\subset \mathbb R^d"/> and we want to understand the distribution <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>. For example,</p>



<ol><li><em>Probability estimation:</em> Given <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>, can we compute/approximate <img alt="p(x)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(x)"/> (the probability that <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> is output under <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>)?</li><li><em>Generation:</em> Can we sample from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>, or from a “nearby” distribution?</li><li><em>Encoding:</em> Can we find a representation <img alt="E:\mathrm{Support}(p) \rightarrow \mathbb{R}^r" class="latex" src="https://s0.wp.com/latex.php?latex=E%3A%5Cmathrm%7BSupport%7D%28p%29+%5Crightarrow+%5Cmathbb%7BR%7D%5Er&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E:\mathrm{Support}(p) \rightarrow \mathbb{R}^r"/> such that for <img alt="x \sim p" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x \sim p"/>, <img alt="E(x)" class="latex" src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E(x)"/> makes it easy to answer semantic questions on <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>? And such that <img alt="\langle E(x) , E(x') \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+E%28x%29+%2C+E%28x%27%29+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle E(x) , E(x') \rangle"/> corresponds to “semantic similarity” of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> and <img alt="x'" class="latex" src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x'"/>?</li><li><em>Prediction:</em> We would like to be able to predict (for example) the second half of <img alt="x \sim p" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x \sim p"/> from the first half. More generally, we want to solve the <em>conditional generation</em> task, where given some function <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> (e.g., the projection to the first half) and some value <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y"/>, we can sample from the conditional probability distribution <img alt="p|f(x)=y" class="latex" src="https://s0.wp.com/latex.php?latex=p%7Cf%28x%29%3Dy&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p|f(x)=y"/>.</li></ol>



<p>Our “dream” is to solve all of those by the following setup:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/EPyXsSW.png"/></figure>



<p>There is an “encoder” <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/> that maps <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> into a representation <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> in the latent space, and then a “decoder” <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/> that can transform such a representation back into <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>. We would like it to be the case that:</p>



<ol><li><em>Generation:</em> For <img alt="x\sim p" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x\sim p"/>, the induced distribution <img alt="E(x)" class="latex" src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E(x)"/> is “nice” and efficiently samples (e.g., the standard normal <img alt="N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N(0,I)"/> over <img alt="\mathbb{R}^r" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5Er&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{R}^r"/>) such that we can (approximately) sample from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> by sampling <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> and outputting <img alt="D(z)" class="latex" src="https://s0.wp.com/latex.php?latex=D%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D(z)"/>.</li><li><em>Density estimation:</em> We would like to be able to evaluate the probability that <img alt="D(z)=x" class="latex" src="https://s0.wp.com/latex.php?latex=D%28z%29%3Dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D(z)=x"/>. For example, if <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/> is the inverse of <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/>, and <img alt="z \sim N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=z+%5Csim+N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z \sim N(0,I)"/> we could do so by computing <img alt="| E(x) |" class="latex" src="https://s0.wp.com/latex.php?latex=%7C+E%28x%29+%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="| E(x) |"/>.</li><li><em>Semantic representation:</em> We would like the latent representation <img alt="E(z)" class="latex" src="https://s0.wp.com/latex.php?latex=E%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E(z)"/> to map <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> into meaningful latent space. Ideally, linear directions in this space will correspond to semantic attributes.</li><li><em>Conditional sampling:</em> We would like to be able to do conditional generation, and in particular for some functions <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> and values <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y"/>, be able to sample from the set of <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/>‘s such that <img alt="f(E(z))=y" class="latex" src="https://s0.wp.com/latex.php?latex=f%28E%28z%29%29%3Dy&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(E(z))=y"/></li></ol>



<p>Ideally, if we could map images to the latent variables used to generate them and vice versa (as in the cartoon from the last lecture), then we could achieve these goals:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/3X4aqfl.png"/></figure>



<p>At the moment, we do not have a single system that can solve all these problems for a natural domain such as images or language, but we have several approaches that achieve part of the dream.</p>



<p><strong>Digressions.</strong> Before discussing concrete models, we make three digressions. One will be non-technical, and the other three technical. The three technical digressions are the following:</p>



<ol><li>If we have multiple objectives, we want a way to interpolate between them.</li><li>To measure how good our models are, we have to measure distances between statistical distributions.</li><li>Once we come up with generating models, we would <em>metrics</em> for measuring how good they are.</li></ol>



<h2>Non-technical digression: Is deep learning a cargo cult science? (spoiler: no)</h2>



<p>In an <a href="https://calteches.library.caltech.edu/51/2/CargoCult.htm">influential essay</a>, Richard Feynman coined the term “cargo cult science” for the activities that have superficial similarities to science but do not follow the scientific method. Some of the tools we use in machine learning look suspiciously close to “cargo cult science.” We use the tools of classical learning, but in a setting in which they were not designed to work in and on which we have no guarantees that they will work. For example, we run (stochastic) gradient descent – an algorithm designed to minimize a convex function – to minimize convex loss. We also write use <em>empirical risk minimization</em> – minimizing loss on our training set – in a setting where we have no guarantee that it will not lead to “overfitting.”</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/tBw6UsX.png"/></figure>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/s5G6xfj.png"/></figure>



<p>And yet, unlike the original cargo cults, in deep learning, “the planes do land”, or at least they often do. When we use a tool <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/> in a situation <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="X"/> that it was not designed to work in, it can play out in one (or mixture) of the following scenarios:</p>



<ul><li><strong>Murphy’s Law:</strong> “Anything that can go wrong will go wrong.” As computer scientists, we are used to this scenario. The natural state of our systems is that they have bugs and errors. There is a reason why software engineering talks about “contracts”, “invariants”, preconditions” and “postconditions”: typically, if we try to use a component <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/> in a situation that it wasn’t designed for, it will not turn out well. This is doubly the case in security and cryptography, where people have learned the hard way time and again that Murphy’s law holds sway.</li><li><strong>“Marley’s Law”:</strong> “Every little thing gonna be alright”. In machine learning, we sometimes see the opposite phenomenon- we use algorithms outside the conditions under which they have been analysed or designed to work in, but they still produce good results. Part of it could be because ML algorithms are already robust to certain errors in their inputs, and their output was only guaranteed to be approximately correct in the first place.</li></ul>



<p>Murphy’s law does occasionally pop up, even in machine learning. We will see examples of both phenomena in this lecture.</p>



<h2>Technical digression 1: Optimization with Multiple Objectives</h2>



<p>During machine learning, we often have multiple objectives to optimize. For example, we may want both an efficient encoder and an effective decoder, but there is a tradeoff between them.</p>



<p>Suppose we have 2 loss functions <img alt="L_1(w)" class="latex" src="https://s0.wp.com/latex.php?latex=L_1%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L_1(w)"/> and <img alt="L_2(w)" class="latex" src="https://s0.wp.com/latex.php?latex=L_2%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L_2(w)"/>, but there can be a trade off between them. The <em>pareto curve</em> is the set <img alt="P={(a,b): \forall w\in W, L_1(w)\ge a\vee L_2(w)\ge b.}" class="latex" src="https://s0.wp.com/latex.php?latex=P%3D%7B%28a%2Cb%29%3A+%5Cforall+w%5Cin+W%2C+L_1%28w%29%5Cge+a%5Cvee+L_2%28w%29%5Cge+b.%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P={(a,b): \forall w\in W, L_1(w)\ge a\vee L_2(w)\ge b.}"/></p>



<figure class="wp-block-image"><img alt="Pareto curve for 2 loss functions" src="https://i.imgur.com/QbPRQtR.jpg"/></figure>



<p>If a model is above the curve, it is not optimal. If it is below the curve, the model is infeasible.</p>



<p>When the set <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P"/> is convex, we can reach any point on the curve <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P"/> by minimizing <img alt="L_1(w)+\lambda L_2(w)" class="latex" src="https://s0.wp.com/latex.php?latex=L_1%28w%29%2B%5Clambda+L_2%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L_1(w)+\lambda L_2(w)"/>. The proof is by the picture above: for any point <img alt="(a_0,b_0)" class="latex" src="https://s0.wp.com/latex.php?latex=%28a_0%2Cb_0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(a_0,b_0)"/> on the curve, there is a tangent line at <img alt="(a_0,b_0)" class="latex" src="https://s0.wp.com/latex.php?latex=%28a_0%2Cb_0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(a_0,b_0)"/> that is strictly below the curve. If <img alt="a+\lambda b" class="latex" src="https://s0.wp.com/latex.php?latex=a%2B%5Clambda+b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="a+\lambda b"/> is the normal vector for this line, then the global minimum of <img alt="a+\lambda b" class="latex" src="https://s0.wp.com/latex.php?latex=a%2B%5Clambda+b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="a+\lambda b"/> on the feasible set will be <img alt="(a_0,b_0)" class="latex" src="https://s0.wp.com/latex.php?latex=%28a_0%2Cb_0%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(a_0,b_0)"/>.<br/>This motivates the common practice of minimizing two introducing a hyperparameter <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\lambda"/> to aggregate two objectives into one.</p>



<p>When <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P"/> is not convex, it may well be that:</p>



<ul><li>Some points on <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="P"/> are not minima of <img alt="L_1 + \lambda L_2" class="latex" src="https://s0.wp.com/latex.php?latex=L_1+%2B+%5Clambda+L_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L_1 + \lambda L_2"/></li><li><img alt="L_1 + \lambda L_2" class="latex" src="https://s0.wp.com/latex.php?latex=L_1+%2B+%5Clambda+L_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L_1 + \lambda L_2"/> might have multiple minima</li><li>Depending on the path one takes, it is possible to get “stuck” in a point that is <em>not</em> a global minima</li></ul>



<p>The following figure demonstrates all three possibilities</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/Rjg4iZU.png"/></figure>



<p>Par for the course, this does not stop people in machine learning from using this approach to minimize different objectives, and often “Marley’s Law” holds, and this works fine. But this is not always the case. A <a href="https://engraved.ghost.io/why-machine-learning-algorithms-are-hard-to-tune/">nice blog post by Degrave and Kurshonova</a> discusses this issue and why sometimes we do in fact, see “Murphy’s law” when we combine objectives. They also detail some other approaches for combining objectives, but there is no single way that will work in all cases.</p>



<p>Figure from Degrave-Kurshonova demonstrating where the algorithm could reach in the non-convex case depending on initialization and <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\lambda"/>:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/4VZZaRR.gif"/></figure>



<h2>Technical digression 2: Distances between probability measures</h2>



<p>Suppose we have two distributions <img alt="p,q" class="latex" src="https://s0.wp.com/latex.php?latex=p%2Cq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p,q"/> over <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/>. There are two common ways of measuring the distances between them.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/5JTzg6D.png"/></figure>



<p>The <em>Total Variance (TV)</em> (also known as statistical distance) between <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> and <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/> is equal to</p>



<p><img alt="\Delta_{TV}(p,q)=\frac12 \sum_{x\in D}|p(x)-q(x)| = \max_{f:D\to {0,1}} | \mathbb{E}_p(f)-\mathbb{E}_q(f)|." class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BTV%7D%28p%2Cq%29%3D%5Cfrac12+%5Csum_%7Bx%5Cin+D%7D%7Cp%28x%29-q%28x%29%7C+%3D+%5Cmax_%7Bf%3AD%5Cto+%7B0%2C1%7D%7D+%7C+%5Cmathbb%7BE%7D_p%28f%29-%5Cmathbb%7BE%7D_q%28f%29%7C.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{TV}(p,q)=\frac12 \sum_{x\in D}|p(x)-q(x)| = \max_{f:D\to {0,1}} | \mathbb{E}_p(f)-\mathbb{E}_q(f)|."/></p>



<p>The second equality can be proved by constructing <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> that outputs 1 on <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> where <img alt="p(x)-q(x)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29-q%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(x)-q(x)"/> and vice versa. The <img alt="\max_f" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmax_f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\max_f"/> definition has a crypto-flavored interpretation: For any adversary <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/>, the TV measures the advantage they can have over half of determining whether <img alt="x\sim p" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x\sim p"/> or <img alt="x\sim q" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Csim+q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x\sim q"/>.</p>



<p>Second, the <em>Kullback–Leibler (KL) Divergence</em> between <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> and <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/> is equal to</p>



<p><img alt="\Delta_{KL}(p||q)=\mathbb{E}_{x\sim p}(\log p(x)/q(x))." class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29%3D%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7D%28%5Clog+p%28x%29%2Fq%28x%29%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q)=\mathbb{E}_{x\sim p}(\log p(x)/q(x))."/></p>



<p>(The total variation distance is symmetric, in the sense that <img alt="\Delta_{TV}(p,q)=\Delta_{TV}(q,p)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BTV%7D%28p%2Cq%29%3D%5CDelta_%7BTV%7D%28q%2Cp%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{TV}(p,q)=\Delta_{TV}(q,p)"/>, but the KL divergence is not. Both have the property that they are non-negative and equal to zero if and only if <img alt="p=q" class="latex" src="https://s0.wp.com/latex.php?latex=p%3Dq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p=q"/>.)</p>



<p>Unlike the total variation distance, which is bounded between <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="0"/> and <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1"/>, the KL divergence can be arbitrarily large and even infinite (though it can be shown using the concavity of log that it is always non-negative). To interpret the KL divergence, it is helpful to separate between the case that <img alt="\Delta_{KL}(p||q)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q)"/> is close to zero and the case where it is a large number. If <img alt="\Delta_{KL}(p||q) \approx \delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q) \approx \delta"/> for some <img alt="\delta \ll 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%5Cll+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta \ll 1"/>, then we would need about <img alt="1/\delta" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1/\delta"/> samples to distinguish between samples of <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> and samples of <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/>. In particular, suppose that we get <img alt="x_1,\ldots,x_n" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_1,\ldots,x_n"/> and we want to distinguish between the case that we they were independently sampled from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> and the case that they were independently sampled from <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/>. A natural (and as it turns out, optimal) approach is to use a <em>likelihood ratio test</em> where we decide the samples came from <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="T"/> if <img alt="\Pr_p[x_1,\ldots,x_n]/\Pr_q[x_1,\ldots,x_n]&gt;T" class="latex" src="https://s0.wp.com/latex.php?latex=%5CPr_p%5Bx_1%2C%5Cldots%2Cx_n%5D%2F%5CPr_q%5Bx_1%2C%5Cldots%2Cx_n%5D%3ET&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Pr_p[x_1,\ldots,x_n]/\Pr_q[x_1,\ldots,x_n]&gt;T"/>. For example, if we set <img alt="T=20" class="latex" src="https://s0.wp.com/latex.php?latex=T%3D20&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="T=20"/> then this approach will guarantee that our “false positive rate” (announcing that samples came from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> when they really came from <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/>) will be most <img alt="1/20=5\%" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F20%3D5%5C%25&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1/20=5\%"/>. Taking logs and using the fact that the probability of these independent samples is the product of probabilities, this amounts to testing whether <img alt="\sum_{i=1}^n \log \left(\tfrac{p(x_i)}{q(x_i)}\right) \geq \log T" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bi%3D1%7D%5En+%5Clog+%5Cleft%28%5Ctfrac%7Bp%28x_i%29%7D%7Bq%28x_i%29%7D%5Cright%29+%5Cgeq+%5Clog+T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sum_{i=1}^n \log \left(\tfrac{p(x_i)}{q(x_i)}\right) \geq \log T"/>. When samples come from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>, the expectation of the righthand side is <img alt="n\cdot \Delta_{KL}(p||q)" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Ccdot+%5CDelta_%7BKL%7D%28p%7C%7Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n\cdot \Delta_{KL}(p||q)"/>, so we see that to ensure <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="T"/> is larger than <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1"/> we need the number samples to be at least <img alt="1/\Delta_{KL}(p||q)" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F%5CDelta_%7BKL%7D%28p%7C%7Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1/\Delta_{KL}(p||q)"/> (and as it turns out, this will do).</p>



<p>When the <img alt="KL" class="latex" src="https://s0.wp.com/latex.php?latex=KL&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="KL"/> divergence is a large number <img alt="k&gt;1" class="latex" src="https://s0.wp.com/latex.php?latex=k%3E1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="k&gt;1"/>, we can think of it as the number of bits of “surprise” in <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/> as opposed to <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>. For example, in the common case where <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/> is obtained by conditioning <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> on some event <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/>, <img alt="\Delta_{KL}(p||q)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q)"/> will typically be <img alt="\log 1/\Pr[A]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+1%2F%5CPr%5BA%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\log 1/\Pr[A]"/> (some fine print applies). In general, if <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/> is obtained from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> by revealing <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="k"/> bits of information (i.e., by conditioning on a random variable whose mutual information with <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> is <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="k"/>) then <img alt="\Delta_{KL}(p||q)=k" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29%3Dk&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q)=k"/>.</p>



<p><strong>Generalizations:</strong> The total variation distance is a special case of metrics of the form <img alt="\Delta(p,q) = \max_{f \in \mathcal{F}} |\mathbb{E}_{x\sim p} f(x) - \mathbb{E}_{x \sim q} f(x)|" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta%28p%2Cq%29+%3D+%5Cmax_%7Bf+%5Cin+%5Cmathcal%7BF%7D%7D+%7C%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7D+f%28x%29+-+%5Cmathbb%7BE%7D_%7Bx+%5Csim+q%7D+f%28x%29%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta(p,q) = \max_{f \in \mathcal{F}} |\mathbb{E}_{x\sim p} f(x) - \mathbb{E}_{x \sim q} f(x)|"/>. These are known as <a href="https://arxiv.org/abs/0901.2698">integral probability metrics</a> and include examples such as the Wasserstein distance, Dudley metric, and Maximum Mean Discrepancy. KL divergence is a special case of divergence measures known as <a href="https://en.wikipedia.org/wiki/F-divergence"><img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/>-divergence</a>, which are measures of the form <img alt="\Delta_f(p||q)= \mathbb{E}_{x \sim q} f\left(\tfrac{p(x)}{q(x)}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_f%28p%7C%7Cq%29%3D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+q%7D+f%5Cleft%28%5Ctfrac%7Bp%28x%29%7D%7Bq%28x%29%7D%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_f(p||q)= \mathbb{E}_{x \sim q} f\left(\tfrac{p(x)}{q(x)}\right)"/>. The KL divergence is obtained by setting <img alt="f(t) = t \log t" class="latex" src="https://s0.wp.com/latex.php?latex=f%28t%29+%3D+t+%5Clog+t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(t) = t \log t"/>. (In fact even the TV distance is a special case of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> divergence by setting <img alt="f(t)=|t-1|/2" class="latex" src="https://s0.wp.com/latex.php?latex=f%28t%29%3D%7Ct-1%7C%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(t)=|t-1|/2"/>.)</p>



<p><strong>Normal distributions:</strong> It is a useful exercise to calculate the TV and KL distances for normal random variables. If <img alt="p=N(0,1)" class="latex" src="https://s0.wp.com/latex.php?latex=p%3DN%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p=N(0,1)"/> and <img alt="q=N(-\epsilon,1)" class="latex" src="https://s0.wp.com/latex.php?latex=q%3DN%28-%5Cepsilon%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q=N(-\epsilon,1)"/>, then since most probability mass in the regime where <img alt="p(x) \approx (1\pm \epsilon) q(x)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29+%5Capprox+%281%5Cpm+%5Cepsilon%29+q%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(x) \approx (1\pm \epsilon) q(x)"/>, <img alt="\Delta_{TV}(p,q) \approx \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BTV%7D%28p%2Cq%29+%5Capprox+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{TV}(p,q) \approx \epsilon"/> (i.e., up to some multiplicative constant). For KL divergence, if we selected <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> from a normal between <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> and <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/> then with probability about half we’ll have <img alt="p(x) \approx q(x)(1+\epsilon)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29+%5Capprox+q%28x%29%281%2B%5Cepsilon%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(x) \approx q(x)(1+\epsilon)"/> and with probability about half we will have <img alt="p(q) \approx q(x)(1-\epsilon)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28q%29+%5Capprox+q%28x%29%281-%5Cepsilon%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(q) \approx q(x)(1-\epsilon)"/>. By selecting <img alt="x\sim p" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x\sim p"/>, we increase probability of the former to <img alt="\approx 1/2+\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Capprox+1%2F2%2B%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\approx 1/2+\epsilon"/> and the decrease the probability of the latter to <img alt="\approx 1/2 - \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Capprox+1%2F2+-+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\approx 1/2 - \epsilon"/>. So we have <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\epsilon"/> bias towards <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>‘s where <img alt="p(x)/q(x) \approx 1+\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29%2Fq%28x%29+%5Capprox+1%2B%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(x)/q(x) \approx 1+\epsilon"/>, or <img alt="\log p(x)/q(x) \approx \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+p%28x%29%2Fq%28x%29+%5Capprox+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\log p(x)/q(x) \approx \epsilon"/>. Hence <img alt="\Delta_{KL}(p||q) \approx \epsilon^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+%5Cepsilon%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q) \approx \epsilon^2"/>. The above generalizes to higher dimensions. If <img alt="p= N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=p%3D+N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p= N(0,I)"/> is a <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/>-variate normal, and <img alt="q=N(\mu,I)" class="latex" src="https://s0.wp.com/latex.php?latex=q%3DN%28%5Cmu%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q=N(\mu,I)"/> for <img alt="\mu \in \mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu+%5Cin+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mu \in \mathbb{R}^d"/>, then (for small <img alt="|\mu|" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cmu%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="|\mu|"/>) <img alt="\Delta_{TV}(p,q) \approx |\mu|" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BTV%7D%28p%2Cq%29+%5Capprox+%7C%5Cmu%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{TV}(p,q) \approx |\mu|"/> while <img alt="\Delta_{KL}(p||q)\approx |\mu|^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29%5Capprox+%7C%5Cmu%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q)\approx |\mu|^2"/>.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/w3aXr99.png"/></figure>



<p>If <img alt="p=N(0,1)" class="latex" src="https://s0.wp.com/latex.php?latex=p%3DN%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p=N(0,1)"/> and <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/> is a “narrow normal” of the form <img alt="q=N(0,\epsilon^2)" class="latex" src="https://s0.wp.com/latex.php?latex=q%3DN%280%2C%5Cepsilon%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q=N(0,\epsilon^2)"/> then their TV distance is close to <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1"/> while <img alt="\Delta_{KL}(p||q) \approx 1/\epsilon^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+1%2F%5Cepsilon%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q) \approx 1/\epsilon^2"/>. In the <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> dimensional case, if <img alt="p=N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=p%3DN%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p=N(0,I)"/> and <img alt="q=N(0,V)" class="latex" src="https://s0.wp.com/latex.php?latex=q%3DN%280%2CV%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q=N(0,V)"/> for some covariance matrix <img alt="V" class="latex" src="https://s0.wp.com/latex.php?latex=V&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="V"/>, then <img alt="\Delta_{KL}(p||q) \approx \mathrm{Tr}(V^{-1}) - d + \ln \det V" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+%5Cmathrm%7BTr%7D%28V%5E%7B-1%7D%29+-+d+%2B+%5Cln+%5Cdet+V&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||q) \approx \mathrm{Tr}(V^{-1}) - d + \ln \det V"/>. The two last terms are often less significant. For example if <img alt="V = \epsilon^2 I" class="latex" src="https://s0.wp.com/latex.php?latex=V+%3D+%5Cepsilon%5E2+I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="V = \epsilon^2 I"/> then <img alt="\delta_{KL}(p||q) \approx d/\epsilon^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta_%7BKL%7D%28p%7C%7Cq%29+%5Capprox+d%2F%5Cepsilon%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta_{KL}(p||q) \approx d/\epsilon^2"/>.</p>



<h2>Technical digression 3: benchmarking generative models</h2>



<p>Given a distribution <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> of natural data and a purported generative model <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g"/>, how do we measure the quality of <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g"/>?</p>



<p>A natural measure is the KL divergence <img alt="\Delta_{KL}(p||g)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cg%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||g)"/> but it can be hard to evaluate, since it involves the term <img alt="p(x)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(x)"/> which we cannot evaluate. However, we can rewrite the KL divergence as <img alt="\mathbb{E}_{x\sim p}(\log p(x)) - \mathbb{E}_{x\sim p}(\log q(x))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7D%28%5Clog+p%28x%29%29+-+%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7D%28%5Clog+q%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}_{x\sim p}(\log p(x)) - \mathbb{E}_{x\sim p}(\log q(x))"/>. The term <img alt="\mathbb{E}{x\sim p} \log p(x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%7Bx%5Csim+p%7D+%5Clog+p%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}{x\sim p} \log p(x)"/> is equal to <img alt="-H(p)" class="latex" src="https://s0.wp.com/latex.php?latex=-H%28p%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-H(p)"/> where <img alt="H(p)" class="latex" src="https://s0.wp.com/latex.php?latex=H%28p%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="H(p)"/> is the <em>entropy</em> of <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>. The term <img alt="-\mathbb{E}_{x \sim p} \log q(x)" class="latex" src="https://s0.wp.com/latex.php?latex=-%5Cmathbb%7BE%7D_%7Bx+%5Csim+p%7D+%5Clog+q%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-\mathbb{E}_{x \sim p} \log q(x)"/> is known as the <em>cross entropy </em>of <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> and <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q"/>. Note that the cross-entropy of <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> and <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g"/> is simply the expectation of the negative log likelihood of <img alt="g(x)" class="latex" src="https://s0.wp.com/latex.php?latex=g%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g(x)"/> for <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> sampled from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>.</p>



<p>When <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> is fixed, minimizing <img alt="\Delta_{KL}(p||g)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%7C%7Cg%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(p||g)"/> corresponds to minimizing the cross entropy <img alt="H(p,g)" class="latex" src="https://s0.wp.com/latex.php?latex=H%28p%2Cg%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="H(p,g)"/> or equivalently, maximizing the log likelihood. This is useful since often is the case that we can sample elements from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> (e.g., natural images) but can only evaluate the probability function for <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g"/>. Hence a common metric in such cases is minimizing the cross-entropy / negative log likelihood <img alt="H(p,g)= -\mathbb{E}_{x sim p} \log g(x) = \mathbb{E}_{x \sim p} \log (1/g(x))" class="latex" src="https://s0.wp.com/latex.php?latex=H%28p%2Cg%29%3D+-%5Cmathbb%7BE%7D_%7Bx+sim+p%7D+%5Clog+g%28x%29+%3D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+p%7D+%5Clog+%281%2Fg%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="H(p,g)= -\mathbb{E}_{x sim p} \log g(x) = \mathbb{E}_{x \sim p} \log (1/g(x))"/>. For images, a common metric is “bits per pixel” which simply equals <img alt="H(p,q)/d" class="latex" src="https://s0.wp.com/latex.php?latex=H%28p%2Cq%29%2Fd&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="H(p,q)/d"/> where <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> is the length of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>. Another metric (often used in natural language processing) is perplexity, which interchanges the expectation and the logarithm. The logarithm of the perplexity of <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g"/> is <img alt="- \tfrac{1}{d}\log \mathbb{E}_{x \sim p} g(x)" class="latex" src="https://s0.wp.com/latex.php?latex=-+%5Ctfrac%7B1%7D%7Bd%7D%5Clog+%5Cmathbb%7BE%7D_%7Bx+%5Csim+p%7D+g%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="- \tfrac{1}{d}\log \mathbb{E}_{x \sim p} g(x)"/> where <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> is the length of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> (e.g., in tokens). Another way to write this is that log of the perplexity is the average of <img alt="\log g(x_i|x{&lt;i})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+g%28x_i%7Cx%7B%3Ci%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\log g(x_i|x{&lt;i})"/> where <img alt="g(x_i|x_{&lt;i})" class="latex" src="https://s0.wp.com/latex.php?latex=g%28x_i%7Cx_%7B%3Ci%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g(x_i|x_{&lt;i})"/> is the probability of <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_i"/> under <img alt="g" class="latex" src="https://s0.wp.com/latex.php?latex=g&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g"/> conditioned on the first <img alt="i-1" class="latex" src="https://s0.wp.com/latex.php?latex=i-1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i-1"/> parts of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>.</p>



<p><strong>Memorization for log-likelihood.</strong> The issue of “overfitting” is even more problematic for generative models than for classifiers. Given samples <img alt="x_1,\ldots,x_n" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_1,\ldots,x_n"/> and enough parameters, we can easily come up with a model corresponding to the uniform distribution <img alt="{ x_1,\ldots, x_n }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+x_1%2C%5Cldots%2C+x_n+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="{ x_1,\ldots, x_n }"/>. This is obviously a useless model that will never generate new examples. However, this model will not only get a large log likelihood value on the training set, in fact, it will get <em>even better log likelihood</em> than the true distribution! For example, any reasonable natural distribution on images would have at least tens of millions, if not billions or trillions of potential images. In contrast, a typical training set might have fewer than 1M samples. Hence, unlike in the classification setting, for generation, the “overfitting” model will not only match but can, in fact, beat the ground truth. (This is reminiscent of the following quote from <a href="https://etc.usf.edu/lit2go/86/peter-pan/1602/chapter-12-the-children-are-carried-off/">Peter and Wendy</a>: <em>“Not a sound is to be heard, save when they give vent to a wonderful imitation of the lonely call of the coyote. The cry is answered by other braves; and some of them do it even better than the coyotes, who are not very good at it.”</em>)</p>



<p>If we cannot compute the density function, then benchmarking becomes more difficult. What often happens in practice is an “I know it when I see it” approach. The paper includes a few pictures generated by the model, and if the pictures look realistic, we think it is a good model. However, this can be deceiving. After all, we are feeding in good pictures into the model, so generating a good photo may not be particularly hard (e.g. the model might memorize some good pictures and use those as outputs).</p>



<p>There is another metric called the <em>log inception score</em>, but it too has its problems. <a href="https://arxiv.org/pdf/1905.10887.pdf">Ravuri-Vinyalis 2019</a> used a GAN model with a good inception score used its outputs to train a different model on ImageNet. Despite the high inception score (which should have indicated that the GANs output are as good as ImageNets) the accuracy when training on the GAN output dropped from the original value of <img alt="74\%" class="latex" src="https://s0.wp.com/latex.php?latex=74%5C%25&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="74\%"/> to as low as <img alt="5\%" class="latex" src="https://s0.wp.com/latex.php?latex=5%5C%25&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="5\%"/>!</p>



<p>This figure from <a href="https://arxiv.org/abs/1701.00160">Goodfellow’s tutorial</a> describes generative models where we know and don’t know how to compute the density function:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/3hVJBPl.png"/></figure>



<h1>Auto Encoder / Decoder</h1>



<p>We now shift our attention to the encoder/decoder architecture mentioned above.</p>



<p>Recall that we want to understand <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/>, generate new elements <img alt="x^*" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x^*"/>, and find a good representation of the elements. Our dream is to solve all of the issues with auto encoder/decoder, whose setup is as follows:</p>



<figure class="wp-block-image"><img alt="Setup for Auto Encoder/Decoder" src="https://i.imgur.com/udIY089.png"/></figure>



<p>That is, we want <img alt="E:\mathbb{R}^d \rightarrow \mathbb{R}^r" class="latex" src="https://s0.wp.com/latex.php?latex=E%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D%5Er&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E:\mathbb{R}^d \rightarrow \mathbb{R}^r"/>, <img alt="D:\mathbb{R}^r \rightarrow \mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=D%3A%5Cmathbb%7BR%7D%5Er+%5Crightarrow+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D:\mathbb{R}^r \rightarrow \mathbb{R}^d"/> such that</p>



<ul><li><img alt="D(E(x)) \approx x" class="latex" src="https://s0.wp.com/latex.php?latex=D%28E%28x%29%29+%5Capprox+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D(E(x)) \approx x"/></li><li>The representation <img alt="E,D" class="latex" src="https://s0.wp.com/latex.php?latex=E%2CD&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E,D"/> enables us to solve tasks such as generation, classification, etc..</li></ul>



<p>To each the first point, we can aim to minimize <img alt="\sum_i ||x_i - D(E(x_i))||^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_i+%7C%7Cx_i+-+D%28E%28x_i%29%29%7C%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sum_i ||x_i - D(E(x_i))||^2"/>. However, we can of course, make this loss zero by letting <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/> and <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/> be the identity function. Much of the framework of generative models can be considered as placing some restrictions on the “communication channel” that rule out this trivial approach, with the hope that would require the encoder and decoder to “intelligently” correspond to the structure of the natural data.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/XW6cDE7.png"/></figure>



<h2>Auto Encoders: noiseless short <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/></h2>



<p>A natural idea is to simply restrict the dimension of the latent space to be small (<img alt="r \ll d" class="latex" src="https://s0.wp.com/latex.php?latex=r+%5Cll+d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r \ll d"/>). In principle, the optimal compression scheme for a probability distribution will require knowing the distribution. Moreover, the optimal compression will maximize the entropy of the latent data <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/>. Since the maximum entropy distribution is uniform (in the discrete case), we could easily sample from it. (In the continuous setting, the standard normal distribution plays the role of the uniform distribution.)</p>



<p>For starter, consider the case of picking <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r"/> to be small and minimizing <img alt="\sum ||x_i - D(E(x_i))||^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum+%7C%7Cx_i+-+D%28E%28x_i%29%29%7C%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sum ||x_i - D(E(x_i))||^2"/> for <em>linear</em> <img alt="E:\mathbb{R}^d \rightarrow \mathbb{R}^r" class="latex" src="https://s0.wp.com/latex.php?latex=E%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D%5Er&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E:\mathbb{R}^d \rightarrow \mathbb{R}^r"/>, <img alt="D:\mathbb{R}^r \rightarrow \mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=D%3A%5Cmathbb%7BR%7D%5Er+%5Crightarrow+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D:\mathbb{R}^r \rightarrow \mathbb{R}^d"/>. Since <img alt="DE" class="latex" src="https://s0.wp.com/latex.php?latex=DE&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="DE"/> is a rank <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r"/> matrix, we can write this as finding a rank <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r"/> matrix <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L"/> that minimizes <img alt="| (I-L)X|^2" class="latex" src="https://s0.wp.com/latex.php?latex=%7C+%28I-L%29X%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="| (I-L)X|^2"/> where <img alt="X = (x_1,\ldots,x_n)" class="latex" src="https://s0.wp.com/latex.php?latex=X+%3D+%28x_1%2C%5Cldots%2Cx_n%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="X = (x_1,\ldots,x_n)"/> is our input data. It can be shown that <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L"/> that would minimize this will be the projection to the top <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r"/> eigenvectors of <img alt="XX^\top" class="latex" src="https://s0.wp.com/latex.php?latex=XX%5E%5Ctop&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="XX^\top"/> which exactly corresponds to <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis (PCA)</a>.</p>



<p>In the nonlinear case, we can obtain better compression. However, we do not achieve our other goals:</p>



<ul><li>It is not the case that we can generate realistic data by sampling uniform/normal <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> and output <img alt="D(z)" class="latex" src="https://s0.wp.com/latex.php?latex=D%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D(z)"/></li><li>It is not the case that semantic similarity between <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> and <img alt="x'" class="latex" src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x'"/> corresponds to large dot product between <img alt="E(x)" class="latex" src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E(x)"/> and <img alt="E(x')" class="latex" src="https://s0.wp.com/latex.php?latex=E%28x%27%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E(x')"/>.</li></ul>



<p>It seems that model just rediscovers a compression algorithm like JPEG. We do not expect the JPEG encoding of an image to be semantically informative, and JPEG decoding of a random file will not be a good way to generate realistic images.</p>



<h2>Variational Auto Encoder (VAE)</h2>



<p>We now discuss <em>variational auto encoders</em> (VAEs). We can think of these as generalization auto-encoders to the case where the channel has some Gaussian noise. We will describe VAEs in two nearly equivalent ways:</p>



<ul><li>We can think of VAEs as trying to optimize two objectives: both the auto-encoder objective of minimizing <img alt="| D(E(x))-x|^2" class="latex" src="https://s0.wp.com/latex.php?latex=%7C+D%28E%28x%29%29-x%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="| D(E(x))-x|^2"/> and another objective of minimizing the KL divergence between <img alt="D(x)" class="latex" src="https://s0.wp.com/latex.php?latex=D%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D(x)"/> and the standard normal distribution <img alt="N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N(0,I)"/>.</li><li>We can think of VAEs as trying to maximize a proxy for the log-likelihood. This proxy is a quantity known as the “Evidence Lower Bound (ELBO)” which we can evaluate using <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/> and <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/> and is always smaller or equal to the log-likelihood.</li></ul>



<p>We start with the first description. One view of VAEs is that we search for a pair <img alt="E,D" class="latex" src="https://s0.wp.com/latex.php?latex=E%2CD&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E,D"/> of encoder and decoder that are aimed at minimizing the following two objectives:</p>



<ul><li><img alt="| x - D(E(x))|^2" class="latex" src="https://s0.wp.com/latex.php?latex=%7C+x+-+D%28E%28x%29%29%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="| x - D(E(x))|^2"/> (standard AE objective)</li><li><img alt="\Delta_{KL}( E(x) || N(0,I) )" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28+E%28x%29+%7C%7C+N%280%2CI%29+%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}( E(x) || N(0,I) )"/> (distance of latent from the standard normal)</li></ul>



<p>To make the second term a function of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>, we consider <img alt="E(x)" class="latex" src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E(x)"/> as a probability distribution with respect to a <em>fixed</em> <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>. To ensure this makes sense, we need to make <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/> <em>randomized</em>. A randomized Neural network has “sampling neurons” that take no input, have parameters <img alt="\mu,\sigma" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu%2C%5Csigma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mu,\sigma"/> and produce an element <img alt="v \sim N(\mu,\sigma^2)" class="latex" src="https://s0.wp.com/latex.php?latex=v+%5Csim+N%28%5Cmu%2C%5Csigma%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="v \sim N(\mu,\sigma^2)"/>. We can train such a network by fixing a random <img alt="t \sim N(0,1)" class="latex" src="https://s0.wp.com/latex.php?latex=t+%5Csim+N%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="t \sim N(0,1)"/> and defining the neuron to simply output <img alt="\mu + \sigma t" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu+%2B+%5Csigma+t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mu + \sigma t"/>.</p>



<p><strong>ELBO derivation:</strong> Another view of VAEs is that they aim at maximizing a term known as the evidence lower bound or ELBO. We start by deriving this bound. Let <img alt="Z=N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=Z%3DN%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Z=N(0,I)"/> be the standard normal distribution over the latent space. Define <img alt="p_x" class="latex" src="https://s0.wp.com/latex.php?latex=p_x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p_x"/> to be the distribution of <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Z"/> conditioned on <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> decoding to <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> (i.e., <img alt="Z= z\sim Z|D(z)=x" class="latex" src="https://s0.wp.com/latex.php?latex=Z%3D+z%5Csim+Z%7CD%28z%29%3Dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Z= z\sim Z|D(z)=x"/>, and define <img alt="q_x" class="latex" src="https://s0.wp.com/latex.php?latex=q_x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q_x"/> be the distribution <img alt="E(x)" class="latex" src="https://s0.wp.com/latex.php?latex=E%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E(x)"/>. Since <img alt="\Delta_{KL}(q_x||p_x) \geq 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28q_x%7C%7Cp_x%29+%5Cgeq+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Delta_{KL}(q_x||p_x) \geq 0"/>, we know that</p>



<p><img alt="0 \leq -H(q_x)- \mathbb{E}_{z \sim q_x} \log p_x(z)" class="latex" src="https://s0.wp.com/latex.php?latex=0+%5Cleq+-H%28q_x%29-+%5Cmathbb%7BE%7D_%7Bz+%5Csim+q_x%7D+%5Clog+p_x%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="0 \leq -H(q_x)- \mathbb{E}_{z \sim q_x} \log p_x(z)"/></p>



<p>By the definition of <img alt="p_x" class="latex" src="https://s0.wp.com/latex.php?latex=p_x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p_x"/>, <img alt="p_x(z) = \Pr[ Z=z \;\wedge\; D(z)=x ] / \Pr[D(Z)=x]" class="latex" src="https://s0.wp.com/latex.php?latex=p_x%28z%29+%3D+%5CPr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D+%2F+%5CPr%5BD%28Z%29%3Dx%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p_x(z) = \Pr[ Z=z \;\wedge\; D(z)=x ] / \Pr[D(Z)=x]"/>. Hence we can derive that</p>



<p><img alt="0 \leq -H(q_x) - \mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ] + \log \Pr[ D(Z)=x]" class="latex" src="https://s0.wp.com/latex.php?latex=0+%5Cleq+-H%28q_x%29+-+%5Cmathbb%7BE%7D_%7Bz+%5Csim+q_x%7D+%5Clog+%5CPr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D+%2B+%5Clog+%5CPr%5B+D%28Z%29%3Dx%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="0 \leq -H(q_x) - \mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ] + \log \Pr[ D(Z)=x]"/><br/>(since <img alt="\Pr[ D(Z)=x]" class="latex" src="https://s0.wp.com/latex.php?latex=%5CPr%5B+D%28Z%29%3Dx%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\Pr[ D(Z)=x]"/> depends only on <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>, given that <img alt="Z=N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=Z%3DN%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Z=N(0,I)"/>.)</p>



<p>Rearranging, we see that</p>



<p><img alt="\log Pr[ D(Z)=x] \geq \mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ] + H(q_x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+Pr%5B+D%28Z%29%3Dx%5D+%5Cgeq+%5Cmathbb%7BE%7D_%7Bz+%5Csim+q_x%7D+%5Clog+%5CPr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D+%2B+H%28q_x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\log Pr[ D(Z)=x] \geq \mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ] + H(q_x)"/></p>



<p>or in other words, we have the following theorem:</p>



<p><strong>Theorem (ELBO):</strong> For every (possibly randomized) maps <img alt="E:\mathcal{X} \rightarrow \mathcal{Z}" class="latex" src="https://s0.wp.com/latex.php?latex=E%3A%5Cmathcal%7BX%7D+%5Crightarrow+%5Cmathcal%7BZ%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E:\mathcal{X} \rightarrow \mathcal{Z}"/> and <img alt="D:\mathcal{Z} \rightarrow \mathcal{X}" class="latex" src="https://s0.wp.com/latex.php?latex=D%3A%5Cmathcal%7BZ%7D+%5Crightarrow+%5Cmathcal%7BX%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D:\mathcal{Z} \rightarrow \mathcal{X}"/>, distribution <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Z"/> over <img alt="\mathcal{Z}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BZ%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathcal{Z}"/> and <img alt="x\in \mathcal{X}" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Cin+%5Cmathcal%7BX%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x\in \mathcal{X}"/>,</p>



<p><img alt="\log \Pr[ D(Z)=x] \geq \Pr_{z \sim E(x)}[ D(z) = x \wedge Z=E(z) ] + H(E(x))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+%5CPr%5B+D%28Z%29%3Dx%5D+%5Cgeq+%5CPr_%7Bz+%5Csim+E%28x%29%7D%5B+D%28z%29+%3D+x+%5Cwedge+Z%3DE%28z%29+%5D+%2B+H%28E%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\log \Pr[ D(Z)=x] \geq \Pr_{z \sim E(x)}[ D(z) = x \wedge Z=E(z) ] + H(E(x))"/></p>



<p>The left-hand side of this inequality is simply the log-likelihood of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>. The right-hand side (which, as the inequality shows, is always smaller or equal to it) is known as the <em>evidence lower bound</em> or ELBO. We can think of VAEs as trying to maximize the ELBO.</p>



<p>The reason that the two views are roughly equivalent is the follows:</p>



<ul><li>The first term of the ELBO, known as the <em>reconstruction term</em>, is <img alt="\mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bz+%5Csim+q_x%7D+%5Clog+%5CPr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}_{z \sim q_x} \log \Pr[ Z=z \;\wedge\; D(z)=x ]"/> if we assume some normal noise, then the probabiility taht <img alt="D(z)=x" class="latex" src="https://s0.wp.com/latex.php?latex=D%28z%29%3Dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D(z)=x"/> will be proportional to <img alt="\exp(-|x-D(z)|^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28-%7Cx-D%28z%29%7C%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\exp(-|x-D(z)|^2)"/> since for <img alt="q_x" class="latex" src="https://s0.wp.com/latex.php?latex=q_x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q_x"/>, <img alt="z=E(x)" class="latex" src="https://s0.wp.com/latex.php?latex=z%3DE%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z=E(x)"/> we get that <img alt="\log Pr[ Z=z \;\wedge\; D(z)=x ] \approx -| x- D(E(x))|^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+Pr%5B+Z%3Dz+%5C%3B%5Cwedge%5C%3B+D%28z%29%3Dx+%5D+%5Capprox+-%7C+x-+D%28E%28x%29%29%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\log Pr[ Z=z \;\wedge\; D(z)=x ] \approx -| x- D(E(x))|^2"/> and hence maximizing this term corresponds to minimizing the square distance.</li><li>The second term of the ELBO, known as the <em>divergence term</em>, is <img alt="H(q_x)" class="latex" src="https://s0.wp.com/latex.php?latex=H%28q_x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="H(q_x)"/> which is roughly equal to <img alt="r -\Delta_{KL}(q_x||N(0,I))" class="latex" src="https://s0.wp.com/latex.php?latex=r+-%5CDelta_%7BKL%7D%28q_x%7C%7CN%280%2CI%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r -\Delta_{KL}(q_x||N(0,I))"/>, where <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="r"/> is the dimension of the latent space. Hence maximizing this term corresponds to minimizing the KL divergence between <img alt="q_x=E(x)" class="latex" src="https://s0.wp.com/latex.php?latex=q_x%3DE%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="q_x=E(x)"/> and the standard normal distribution.</li></ul>



<p>How well does VAE work? We find that <img alt="z\approx z'\to D(z)\approx D(z')" class="latex" src="https://s0.wp.com/latex.php?latex=z%5Capprox+z%27%5Cto+D%28z%29%5Capprox+D%28z%27%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z\approx z'\to D(z)\approx D(z')"/>, which is good. However, We find that the VAEs can still sometimes cheat (as in auto encoders). There is a risk that the learned model will split <img alt="Z" class="latex" src="https://s0.wp.com/latex.php?latex=Z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Z"/> to two parts of the form <img alt="(N(0,I), JPEG(x))" class="latex" src="https://s0.wp.com/latex.php?latex=%28N%280%2CI%29%2C+JPEG%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(N(0,I), JPEG(x))"/>. The first part of the data is there to minimize divergence, while the second part is there for reconstruction. Such a model is similarly uninformative.</p>



<p>However, VAEs have found practical success. For example, <a href="https://arxiv.org/pdf/1610.00291.pdf">Hou et. al 2016</a> used VAE to create an encoding where two dimensions seem to correspond to “sunglasses” and “blondness”, as illustrated below. We do note that “sunglasses” and “blondness” are somewhere between “semantic” and “syntactic” attributes. They do correspond to relatively local changes in “pixel space”.</p>



<figure class="wp-block-image"><img alt="VAE Example 1" src="https://i.imgur.com/O48nnWB.jpg"/></figure>



<p>The picture can be blurry because of the noise we injected to make <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/> random. However, recent models have used new techniques (e.g. vector quantized VAE and hierarchical VAE) to resolve the blurriness.</p>



<h2>Flow Models</h2>



<p>In a flow model, we flip the order of <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/> and <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/> and set <img alt="E=D^{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=E%3DD%5E%7B-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E=D^{-1}"/> (so <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/> must be invertible). The input <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> to <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D"/> will come from the standard normal distribution <img alt="N(0,I)" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="N(0,I)"/>. The idea is that we obtain <img alt="E" class="latex" src="https://s0.wp.com/latex.php?latex=E&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="E"/> by a composition of simple invertible functions. We use the fact that if we can compute the density function of a distribution <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p"/> over <img alt="\mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{R}^d"/> and <img alt="f:\mathbb{R}^d \rightarrow \mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=f%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f:\mathbb{R}^d \rightarrow \mathbb{R}^d"/> is invertible and differentiable, then we can compute the density function of <img alt="f\circ p" class="latex" src="https://s0.wp.com/latex.php?latex=f%5Ccirc+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f\circ p"/> (i.e., the distribution obtained by sampling <img alt="w \sim p" class="latex" src="https://s0.wp.com/latex.php?latex=w+%5Csim+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w \sim p"/> and outputting <img alt="f(w)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(w)"/>). To see why this is the case, consider the setting when <img alt="d=2" class="latex" src="https://s0.wp.com/latex.php?latex=d%3D2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d=2"/> and a small <img alt="\delta \times \delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%5Ctimes+%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta \times \delta"/> rectangle <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/>. If <img alt="\delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta"/> is small enough, <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> will be roughly linear and hence will map <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/> into a parallelogram <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="B"/>. Shifting the <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> coordinate by <img alt="\delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta"/> corresponds to shifting the output of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> by the vector <img alt="\delta (\tfrac{d f_x}{dx}, \tfrac{d f_y}{dx})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%28%5Ctfrac%7Bd+f_x%7D%7Bdx%7D%2C+%5Ctfrac%7Bd+f_y%7D%7Bdx%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta (\tfrac{d f_x}{dx}, \tfrac{d f_y}{dx})"/> and shifting the <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y"/> coordinate by <img alt="\delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta"/> corresponds to shifting the output of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> by the vector <img alt="\delta (\tfrac{d f_x}{dy}, \tfrac{d f_y}{dy})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%28%5Ctfrac%7Bd+f_x%7D%7Bdy%7D%2C+%5Ctfrac%7Bd+f_y%7D%7Bdy%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\delta (\tfrac{d f_x}{dy}, \tfrac{d f_y}{dy})"/>. For every <img alt="z \in B" class="latex" src="https://s0.wp.com/latex.php?latex=z+%5Cin+B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z \in B"/>, the density of <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> under <img alt="f\circ p" class="latex" src="https://s0.wp.com/latex.php?latex=f%5Ccirc+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f\circ p"/> will be proportional to the density of <img alt="f^{-1}(z)" class="latex" src="https://s0.wp.com/latex.php?latex=f%5E%7B-1%7D%28z%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f^{-1}(z)"/> with the proportionality fector being <img alt="vol(A)/vol(B)" class="latex" src="https://s0.wp.com/latex.php?latex=vol%28A%29%2Fvol%28B%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="vol(A)/vol(B)"/>.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/pet8tBU.png"/></figure>



<p>Overall we the density of <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/> under <img alt="f \circ p" class="latex" src="https://s0.wp.com/latex.php?latex=f+%5Ccirc+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f \circ p"/> will equal <img alt="p(f^{-1}(z))" class="latex" src="https://s0.wp.com/latex.php?latex=p%28f%5E%7B-1%7D%28z%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="p(f^{-1}(z))"/> times the inverse determinant of the <em>Jacobian</em> of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> at the point <img alt="z" class="latex" src="https://s0.wp.com/latex.php?latex=z&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="z"/></p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/ItOPqSX.png"/></figure>



<p>There are different ways to compose together simple reversible functions to compute a complex one. Indeed, this issue also arises in cryptography and quantum computing (e.g., the <a href="https://en.wikipedia.org/wiki/Feistel_cipher">Fiestel cipher</a>). Using similar ideas, it is not hard to show that any probability distribution can be approximated by a (sufficiently big) combination of simple reversible functions.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/AlYrWJx.png"/></figure>



<p>In practice, we have some recent succcessful flow models. A few examples of these models are in the lecture slides.</p>



<h1>Giving up on the dream</h1>



<p>In section 2, we had a dream of doing both representation and generation at once. So far, we have not been able to find success with these models. What if we do each goal separately?</p>



<p>The tasks of representation becomes self-supervised learning with approaches such SIMCLR. The task of generation can be solved by GANs. Both areas have had recent success.</p>



<figure class="wp-block-image"><img alt="Model after we separate E and D" src="https://i.imgur.com/D0CpobJ.jpg"/></figure>



<p>Open-AI <a href="https://openai.com/blog/clip">CLIP</a> and <a href="https://openai.com/blog/dall-e/">DALL-E</a> is a pair of models that perform each part of these tasks well, and suggest an approach to merge them.<br/>CLIP does representation for both texts and images where the two encoders are aligned, i.e. <img alt="\langle E(\text{'cat'}), E(\text{img of cat)}\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+E%28%5Ctext%7B%27cat%27%7D%29%2C+E%28%5Ctext%7Bimg+of+cat%29%7D%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle E(\text{'cat'}), E(\text{img of cat)}\rangle"/> is large. DALL-E, given some text, generates an image corresponding to the text. Below are images generated by DALL-E when asked for an armchair in the shape of an avocado.</p>



<figure class="wp-block-image"><img alt="DALL-E Example" src="https://i.imgur.com/ZcsHXKE.png"/></figure>



<h2>Contrastive learning</h2>



<p>The general approach used in CLIP is called contrastive learning.</p>



<p>Suppose we have some representation function <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> and inputs <img alt="u_i,v_i" class="latex" src="https://s0.wp.com/latex.php?latex=u_i%2Cv_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u_i,v_i"/> which represent similar objects. Let <img alt="M_{i,j}=f(u_i\cdot v_j)" class="latex" src="https://s0.wp.com/latex.php?latex=M_%7Bi%2Cj%7D%3Df%28u_i%5Ccdot+v_j%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="M_{i,j}=f(u_i\cdot v_j)"/>, then we want <img alt="M_{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=M_%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="M_{i,j}"/> to be large when <img alt="i=j" class="latex" src="https://s0.wp.com/latex.php?latex=i%3Dj&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i=j"/>, but small when <img alt="i\neq j" class="latex" src="https://s0.wp.com/latex.php?latex=i%5Cneq+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i\neq j"/>. So, let the loss function be <img alt="L(M)=\sum M_{i,i} / \sum_{i\neq j} M_{i,j}." class="latex" src="https://s0.wp.com/latex.php?latex=L%28M%29%3D%5Csum+M_%7Bi%2Ci%7D+%2F+%5Csum_%7Bi%5Cneq+j%7D+M_%7Bi%2Cj%7D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L(M)=\sum M_{i,i} / \sum_{i\neq j} M_{i,j}."/> How do we create similar <img alt="u_i,v_i" class="latex" src="https://s0.wp.com/latex.php?latex=u_i%2Cv_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u_i,v_i"/>? In SIMCLR, <img alt="u_i,v_i" class="latex" src="https://s0.wp.com/latex.php?latex=u_i%2Cv_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="u_i,v_i"/> are augmentations of the same image <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_i"/>. In CLIP, <img alt="(u_i,v_i)" class="latex" src="https://s0.wp.com/latex.php?latex=%28u_i%2Cv_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(u_i,v_i)"/> is an image and a text that describes it.</p>



<h2>GANs</h2>



<p>The theory of GANs is currently not well-developed. As an objective, we want images that “look real” (which is not well defined), and we have no posterior distribution. If we just define the distribution based on real images, our GAN might memorize the photos to beat us.</p>



<p>However, we know that Neural Networks are good at discriminating real vs. fake images. So, we add in a discriminator <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> and define the loss function <img alt="L(D) = \max_{f:\mathbb R^d\to \mathbb R} |\mathbb{E}_{\hat x\sim D(z)}f(\hat x)-\mathbb{E}_{x\sim p}f(x)|." class="latex" src="https://s0.wp.com/latex.php?latex=L%28D%29+%3D+%5Cmax_%7Bf%3A%5Cmathbb+R%5Ed%5Cto+%5Cmathbb+R%7D+%7C%5Cmathbb%7BE%7D_%7B%5Chat+x%5Csim+D%28z%29%7Df%28%5Chat+x%29-%5Cmathbb%7BE%7D_%7Bx%5Csim+p%7Df%28x%29%7C.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L(D) = \max_{f:\mathbb R^d\to \mathbb R} |\mathbb{E}_{\hat x\sim D(z)}f(\hat x)-\mathbb{E}_{x\sim p}f(x)|."/></p>



<p>The generator model and discriminator model form a 2-player game, which are often harder to train and very delicate. We typically train by changing a player’s action to the best response. However, we need to be careful if the two players have very different skill levels. They may be stuck in a setting where no change of strategies will make much difference, since the stronger player always dominates the weaker one. In particular in GANs we need to ensure that the generator is not cheating by using a degenerate distribution that still succeeds with respect to the discriminator.</p>



<p>If a 2-player model makes training more difficult, why do we use it? If we fix the discriminator, then the generator can find a picture that the discriminator thinks is real and only output that one, obtaining low loss. As a result, the discriminator needs to update along with the generator. This example also highlights that the discriminator’s job is often harder. To fix this, we have to somehow require the generator to give us good entropy.</p>



<p>Finally, how good are GANs in practice? Recently, we have had GANs that make great images as well as audios. For example, modern deepfake techniques often use GANs in their architecture. However, it is still unclear how rich the images are.</p></div>
    </content>
    <updated>2021-02-24T23:21:58Z</updated>
    <published>2021-02-24T23:21:58Z</published>
    <category term="ML Theory seminar"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-02-25T08:21:18Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=18187</id>
    <link href="https://rjlipton.wordpress.com/2021/02/24/a-quiz-of-quotes/" rel="alternate" type="text/html"/>
    <title>A Quiz of Quotes</title>
    <summary>Everything that can be invented has been invented—Charles Duell, Commissioner, U.S. Office of Patents, 1899 MathQuotes src George Cantor has been featured here and here and here before on GLL. Of course, he invented modern set theory and changed math forever. His birthday is soon, so we thought we would talk about him now—he was […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Everything that can be invented has been invented—Charles Duell, Commissioner, U.S. Office of Patents, 1899</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/24/a-quiz-of-quotes/cantormathquote/" rel="attachment wp-att-18201"><img alt="" class="alignright wp-image-18201" height="150" src="https://rjlipton.files.wordpress.com/2021/02/cantormathquote.jpg?w=150&amp;h=150" width="150"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">MathQuotes <a href="https://www.facebook.com/mathsqoutes/posts/the-mathematician-does-not-study-pure-mathematics-because-it-is-useful-he-studie/144012037239958/">src</a></font></td>
</tr>
</tbody>
</table>
<p>
George Cantor has been featured <a href="https://rjlipton.wordpress.com/2014/07/31/the-cantor-bernstein-schroder-theorem/">here</a> and <a href="https://rjlipton.wordpress.com/2009/04/18/cantors-non-diagonal-proof/">here</a> and <a href="https://rjlipton.wordpress.com/2012/09/04/thinking-out-of-the-notation-box/">here</a> before on GLL. Of course, he invented modern set theory and changed math forever. His birthday is soon, so we thought we would talk about him now—he was born on March 3rd in 1845.</p>
<p>
Today we thought it might be fun to have a quiz on math quotes.</p>
<p>
Wait. Cantor did not invent quotation marks, nor is he known for many quotes. He does of course have many famous results, and they will live forever. But his results were subject to immediate horrible criticism and therefore memorable quotes. </p>
<p>Leopold Kronecker was a particular source of barbs.  For example: “What good is your beautiful proof on the transcendence of <img alt="{\pi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\pi}"/>? Why investigate such problems, given that irrational numbers do not even exist?” </p>
<p>
As a complexity theorist I must say that Kronecker has a point when he also said: </p>
<blockquote><p><b> </b> <em> “Definitions must contain the means of reaching a decision in a finite number of steps, and existence proofs must be conducted so that the quantity in question can be calculated with any degree of accuracy.” </em>
</p></blockquote>
<p>David Hilbert defended Cantor and said: “No one shall expel us from the paradise that Cantor has created.”</p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/24/a-quiz-of-quotes/cantor/" rel="attachment wp-att-18190"><img alt="" class="aligncenter wp-image-18190" height="205" src="https://rjlipton.files.wordpress.com/2021/02/cantor.png?w=600&amp;h=205" width="600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">BBVA Open Mind <a href="https://www.bbvaopenmind.com/en/science/mathematics/georg-cantor-the-man-who-discovered-different-infinities/">src</a>
</font></td>
</tr>
</tbody></table>
<p/><h2> Quotes Quiz </h2><p/>
<p/><p>
On to the quiz. Each quote is followed by two possible authors in alphabetical order. You should pick the one you think is correct. The players are: </p>
<blockquote><p><b> </b> <em> 1. Douglas Adams  2. Bernard Baruch  3. Eric Temple Bell  4. Raoul Bott<br/>
5. Paul Erdős  6. Richard Hamming  7. Godfrey Hardy  8. David Hilbert<br/>
9. Admiral Grace Hooper  10. Alan Kay  11. Donald Knuth  12. John von Neumann<br/>
13. Alan Perlis  14. Henri Poincaré  15. Srinivasa Ramanujan  16. Marcus du Sautoy<br/>
17. Raymond Smullyan  18. Alan Turing  19. Moshe Vardi  20. Andrew Wiles<br/>
</em>
</p></blockquote>
<p>
</p><ol>
<p/><li>
Those who can imagine anything, can create the impossible.<br/>
—Kay <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Turing<p/>
<p/></li><li>
I really didn’t foresee the Internet. But then, neither did the computer industry. Not that that tells us very much of course–the computer industry didn’t even foresee that the century was going to end.<br/>
— Adams <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Knuth<p/>
<p/></li><li>
One man’s constant is another man’s variable.<br/>
—Perlis <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> du Sautoy<p/>
<p/></li><li>
The most damaging phrase in the language is: “It’s always been done that way.”<br/>
—-Hopper <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Perlis<p/>
<p/></li><li>
The best way to predict the future is to invent it.<br/>
—Kay <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Turing<p/>
<p/></li><li>
The purpose of computing is insight, not numbers.<br/>
Adams <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Hamming<p/>
<p/></li><li>
Beware of bugs in the above code; I have only proved it correct, not tried it.<br/>
—Knuth <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Vardi<p/>
<p/></li><li>
No, it is a very interesting number, it is the smallest number expressible as a sum of two cubes in two different ways.<br/>
—Bell <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Ramanujan <p/>
<p/></li><li>
Beauty is the first test: there is no permanent place in the world for ugly mathematics. <br/>
—Erdős <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Hardy<p/>
<p/></li><li>
Mathematics is the art of giving the same name to different things. <br/>
—Hooper <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Poincaré <p/>
<p/></li><li>
There’s no sense in being precise when you don’t even know what you’re talking about.<br/>
—Bott <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> von Neumann<p/>
<p/></li><li>
I hope we’ll be able to solve these problems before we leave. <br/>
—Erdős <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Perlis<p/>
<p/></li><li>
Some people are always critical of vague statements. I tend rather to be critical of precise statements; they are the only ones which can correctly be labeled ‘wrong’. <br/>
—Knuth <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Smullyan<p/>
<p/></li><li>
Everything that humans can do a machine can do. <br/>
—Perlis <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Vardi<p/>
<p/></li><li>
“Obvious” is the most dangerous word in mathematics.<br/>
— Bell <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Hooper<p/>
<p/></li><li>
Just because we can’t find a solution, it doesn’t mean there isn’t one.<br/>
— Adams <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Wiles<p/>
<p/></li><li>
Mathematics is a place where you can do things which you can’t do in the real world.<br/>
— du Sautoy <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Turing<p/>
<p/></li><li>
Millions saw the apple fall, but Newton asked why.<br/>
— Baruch <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Hopper<p/>
<p/></li><li>
The definition of a good mathematical problem is the mathematics it generates rather than the problem itself.<br/>
— Hilbert <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Wiles<p/>
<p/></li><li>
There are two ways to do great mathematics. The first is to be smarter than everybody else. The second way is to be stupider than everybody else – but persistent.<br/>
— Bott <img alt="{||}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{||}"/> Knuth<p/>
</li></ol>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
“I always have a quotation for everything—it saves original thinking.”<br/>
—Dorothy Sayers</p>
<p>
Here are the answers:</p>
<p><a href="https://rjlipton.wordpress.com/2021/02/24/a-quiz-of-quotes/ans/" rel="attachment wp-att-18194"><img alt="" class="alignright size-full wp-image-18194" height="702" src="https://rjlipton.files.wordpress.com/2021/02/ans.png?w=600&amp;h=702" width="600"/></a></p></font></font></div>
    </content>
    <updated>2021-02-24T17:44:38Z</updated>
    <published>2021-02-24T17:44:38Z</published>
    <category term="History"/>
    <category term="Oldies"/>
    <category term="People"/>
    <category term="Cantor"/>
    <category term="fun"/>
    <category term="quiz"/>
    <category term="quote"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2021-02-25T08:20:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/027</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/027" rel="alternate" type="text/html"/>
    <title>TR21-027 |  Almost Optimal Super-Constant-Pass Streaming Lower Bounds for Reachability | 

	Lijie Chen, 

	Gillat Kol, 

	Dmitry Paramonov, 

	Raghuvansh Saxena, 

	Zhao Song, 

	Huacheng Yu</title>
    <summary>We give an almost quadratic $n^{2-o(1)}$ lower bound on the space consumption of any $o(\sqrt{\log n})$-pass streaming algorithm solving the (directed) $s$-$t$ reachability problem. This means that any such algorithm must essentially store the entire graph. As corollaries, we obtain almost quadratic space lower bounds for additional fundamental problems, including maximum matching, shortest path, matrix rank, and linear programming.

Our main technical contribution is the definition and construction of set hiding graphs, that may be of independent interest: we give a general way of encoding a set $S \subseteq [k]$ as a directed graph with $n = k^{ 1 + o( 1 ) }$ vertices, such that deciding whether $i \in S$ boils down to deciding if $t_i$ is reachable from $s_i$, for a specific pair of vertices $(s_i,t_i)$ in the graph. Furthermore, we prove that our graph ``hides'' $S$, in the sense that no low-space streaming algorithm with a small number of passes can learn (almost) anything about $S$.</summary>
    <updated>2021-02-24T02:18:04Z</updated>
    <published>2021-02-24T02:18:04Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-02-25T08:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.11854</id>
    <link href="http://arxiv.org/abs/2102.11854" rel="alternate" type="text/html"/>
    <title>Conditional Dichotomy of Boolean Ordered Promise CSPs</title>
    <feedworld_mtime>1614124800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brakensiek:Joshua.html">Joshua Brakensiek</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guruswami:Venkatesan.html">Venkatesan Guruswami</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sandeep:Sai.html">Sai Sandeep</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.11854">PDF</a><br/><b>Abstract: </b>Promise Constraint Satisfaction Problems (PCSPs) are a generalization of
Constraint Satisfaction Problems (CSPs) where each predicate has a strong and a
weak form and given a CSP instance, the objective is to distinguish if the
strong form can be satisfied vs. even the weak form cannot be satisfied. Since
their formal introduction by Austrin, Guruswami, and H\aa stad, there has been
a flurry of works on PCSPs [BBKO19,KO19,WZ20]. The key tool in studying PCSPs
is the algebraic framework developed in the context of CSPs where the closure
properties of the satisfying solutions known as the polymorphisms are analyzed.
</p>
<p>The polymorphisms of PCSPs are much richer than CSPs. In the Boolean case, we
still do not know if dichotomy for PCSPs exists analogous to Schaefer's
dichotomy result for CSPs. In this paper, we study a special case of Boolean
PCSPs, namely Boolean Ordered PCSPs where the Boolean PCSPs have the predicate
$x \leq y$. In the algebraic framework, this is the special case of Boolean
PCSPs when the polymorphisms are monotone functions. We prove that Boolean
Ordered PCSPs exhibit a computational dichotomy assuming the Rich 2-to-1
Conjecture [BKM21] which is a perfect completeness surrogate of the Unique
Games Conjecture.
</p>
<p>Assuming the Rich 2-to-1 Conjecture, we prove that a Boolean Ordered PCSP can
be solved in polynomial time if for every $\epsilon&gt;0$, it has polymorphisms
where each coordinate has Shapley value at most $\epsilon$, else it is NP-hard.
The algorithmic part of our dichotomy is based on a structural lemma that
Boolean monotone functions with each coordinate having low Shapley value have
arbitrarily large threshold functions as minors. The hardness part proceeds by
showing that the Shapley value is consistent under a uniformly random 2-to-1
minor. Of independent interest, we show that the Shapley value can be
inconsistent under an adversarial 2-to-1 minor.
</p></div>
    </summary>
    <updated>2021-02-24T22:40:48Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-02-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.11797</id>
    <link href="http://arxiv.org/abs/2102.11797" rel="alternate" type="text/html"/>
    <title>Conditional Lower Bounds for Variants of Dynamic LIS</title>
    <feedworld_mtime>1614124800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Paweł Gawrychowski, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Janczewski:Wojciech.html">Wojciech Janczewski</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.11797">PDF</a><br/><b>Abstract: </b>In this note, we consider the complexity of maintaining the longest
increasing subsequence (LIS) of an array under (i) inserting an element, and
(ii) deleting an element of an array. We show that no algorithm can support
queries and updates in time $\mathcal{O}(n^{1/2-\epsilon})$ and
$\mathcal{O}(n^{1/3-\epsilon})$ for the dynamic LIS problem, for any constant
$\epsilon&gt;0$, when the elements are weighted or the algorithm supports
1D-queries (on subarrays), respectively, assuming the All-Pairs Shortest Paths
(APSP) conjecture or the Online Boolean Matrix-Vector Multiplication (OMv)
conjecture. The main idea in our construction comes from the work of Abboud and
Dahlgaard [FOCS 2016], who proved conditional lower bounds for dynamic planar
graph algorithm. However, this needs to be appropriately adjusted and
translated to obtain an instance of the dynamic LIS problem.
</p></div>
    </summary>
    <updated>2021-02-24T22:51:53Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.11782</id>
    <link href="http://arxiv.org/abs/2102.11782" rel="alternate" type="text/html"/>
    <title>Parameterized Complexity of Logic-Based Argumentation in Schaefer's Framework</title>
    <feedworld_mtime>1614124800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Yasir Mahmood, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meier:Arne.html">Arne Meier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schmidt:Johannes.html">Johannes Schmidt</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.11782">PDF</a><br/><b>Abstract: </b>Logic-based argumentation is a well-established formalism modelling
nonmonotonic reasoning. It has been playing a major role in AI for decades,
now. Informally, a set of formulas is the support for a given claim if it is
consistent, subset-minimal, and implies the claim. In such a case, the pair of
the support and the claim together is called an argument. In this paper, we
study the propositional variants of the following three computational tasks
studied in argumentation: ARG (exists a support for a given claim with respect
to a given set of formulas), ARG-Check (is a given set a support for a given
claim), and ARG-Rel (similarly as ARG plus requiring an additionally given
formula to be contained in the support). ARG-Check is complete for the
complexity class DP, and the other two problems are known to be complete for
the second level of the polynomial hierarchy (Parson et al., J. Log. Comput.,
2003) and, accordingly, are highly intractable. Analyzing the reason for this
intractability, we perform a two-dimensional classification: first, we consider
all possible propositional fragments of the problem within Schaefer's framework
(STOC 1978), and then study different parameterizations for each of the
fragment. We identify a list of reasonable structural parameters (size of the
claim, support, knowledge-base) that are connected to the aforementioned
decision problems. Eventually, we thoroughly draw a fine border of
parameterized intractability for each of the problems showing where the
problems are fixed-parameter tractable and when this exactly stops.
Surprisingly, several cases are of very high intractability (paraNP and
beyond).
</p></div>
    </summary>
    <updated>2021-02-24T22:39:37Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-02-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.11728</id>
    <link href="http://arxiv.org/abs/2102.11728" rel="alternate" type="text/html"/>
    <title>Testing Hamiltonicity (and other problems) in Minor-Free Graphs</title>
    <feedworld_mtime>1614124800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Levi:Reut.html">Reut Levi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shoshan:Nadav.html">Nadav Shoshan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.11728">PDF</a><br/><b>Abstract: </b>In this paper we provide sub-linear algorithms for several fundamental
problems in the setting in which the input graph excludes a fixed minor, i.e.,
is a minor-free graph. In particular, we provide the following algorithms for
minor-free unbounded degree graphs. A tester for Hamiltonicity with two-sided
error with $poly(1/\epsilon)$-query complexity, where $\epsilon$ is the
proximity parameter. A local algorithm, as defined by Rubinfeld et al. (ICS
2011), for constructing a spanning subgraph with almost minimum weight,
specifically, at most a factor $(1+\epsilon)$ of the optimum, with
$poly(1/\epsilon)$-query complexity. \end{enumerate} Both our algorithms use
partition-oracles, a tool introduced by Hassidim et al. (FOCS 2009), which are
oracles that provide access to a partition of the graph such that the number of
cut-edges is small and each part of the partition is small. The polynomial
dependence in $1/\epsilon$ of our algorithms is achieved by combining the
recent $poly(d/\epsilon)$-query partition oracle of Kumar-Seshadhri-Stolman
(ECCC 2021) for minor-free graphs with degree bounded by $d$.
</p>
<p>For bounded degree minor-free graphs we introduce the notion of {\em covering
partition oracles} which is a relaxed version of partition oracles and design a
$poly(d/\epsilon)$-time covering partition oracle for this family of graphs.
Using our covering partition oracle we provide the same results as above
(except that the tester for Hamiltonicity has one sided error) for minor free
bounded degree graphs, as well as showing that any property which is monotone
and additive (e.g. bipartiteness) can be tested in minor-free graphs by making
$poly(d/\epsilon)$-queries.
</p>
<p>The benefit of using the covering partition oracle rather than the partition
oracle in our algorithms is its simplicity and an improved polynomial
dependence in $1/\epsilon$ in the obtained query complexity.
</p></div>
    </summary>
    <updated>2021-02-24T22:53:09Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.11727</id>
    <link href="http://arxiv.org/abs/2102.11727" rel="alternate" type="text/html"/>
    <title>Functional norms, condition numbers and numerical algorithms in algebraic geometry</title>
    <feedworld_mtime>1614124800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cucker:Felipe.html">Felipe Cucker</a>, Alperen A. Ergür, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tonelli=Cueto:Josu=eacute=.html">Josué Tonelli-Cueto</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.11727">PDF</a><br/><b>Abstract: </b>In numerical linear algebra, a well-established practice is to choose a norm
that exploits the structure of the problem at hand in order to optimize
accuracy or computational complexity. In numerical polynomial algebra, a single
norm (attributed to Weyl) dominates the literature. This article initiates the
use of $L_p$ norms for numerical algebraic geometry, with an emphasis on
$L_{\infty}$. This classical idea yields strong improvements in the analysis of
the number of steps performed by numerous iterative algorithms. In particular,
we exhibit three algorithms where, despite the complexity of computing
$L_{\infty}$-norm, the use of $L_p$-norms substantially reduces computational
complexity: a subdivision-based algorithm in real algebraic geometry for
computing the homology of semialgebraic sets, a well-known meshing algorithm in
computational geometry, and the computation of zeros of systems of complex
quadratic polynomials (a particular case of Smale's 17th problem).
</p></div>
    </summary>
    <updated>2021-02-24T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-02-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.11684</id>
    <link href="http://arxiv.org/abs/2102.11684" rel="alternate" type="text/html"/>
    <title>A System for 3D Reconstruction Of Comminuted Tibial Plafond Bone Fractures</title>
    <feedworld_mtime>1614124800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Pengcheng.html">Pengcheng Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hewitt:Nathan.html">Nathan Hewitt</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shadid:Waseem.html">Waseem Shadid</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Willis:Andrew.html">Andrew Willis</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.11684">PDF</a><br/><b>Abstract: </b>High energy impacts at joint locations often generate highly fragmented, or
comminuted, bone fractures. Current approaches for treatment require physicians
to decide how to classify the fracture within a hierarchy fracture severity
categories. Each category then provides a best-practice treatment scenario to
obtain the best possible prognosis for the patient. This article identifies
shortcomings associated with qualitative-only evaluation of fracture severity
and provides new quantitative metrics that serve to address these shortcomings.
We propose a system to semi-automatically extract quantitative metrics that are
major indicators of fracture severity. These include: (i) fracture surface
area, i.e., how much surface area was generated when the bone broke apart, and
(ii) dispersion, i.e., how far the fragments have rotated and translated from
their original anatomic positions. This article describes new computational
tools to extract these metrics by computationally reconstructing 3D bone
anatomy from CT images with a focus on tibial plafond fracture cases where
difficult qualitative fracture severity cases are more prevalent.
Reconstruction is accomplished within a single system that integrates several
novel algorithms that identify, extract and piece-together fractured fragments
in a virtual environment. Doing so provides objective quantitative measures for
these fracture severity indicators. The availability of such measures provides
new tools for fracture severity assessment which may lead to improved fracture
treatment. This paper describes the system, the underlying algorithms and the
metrics of the reconstruction results by quantitatively analyzing six clinical
tibial plafond fracture cases.
</p></div>
    </summary>
    <updated>2021-02-24T22:55:25Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-02-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.11660</id>
    <link href="http://arxiv.org/abs/2102.11660" rel="alternate" type="text/html"/>
    <title>Massively Parallel Correlation Clustering in Bounded Arboricity Graphs</title>
    <feedworld_mtime>1614124800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cambus:M=eacute=lanie.html">Mélanie Cambus</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Choo:Davin.html">Davin Choo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Miikonen:Havu.html">Havu Miikonen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Uitto:Jara.html">Jara Uitto</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.11660">PDF</a><br/><b>Abstract: </b>Identifying clusters of similar elements in a set is a common objective in
data analysis. With the immense growth of data and physical limitations on
single processor speed, it is necessary to find efficient parallel algorithms
for clustering tasks. In this paper, we study the problem of correlation
clustering in bounded arboricity graphs with respect to the Massively Parallel
Computation (MPC) model. More specifically, we are given a complete graph where
the vertices correspond to the elements and each edge is either positive or
negative, indicating whether pairs of vertices are similar or dissimilar. The
task is to partition the vertices into clusters with as few disagreements as
possible. That is, we want to minimize the number of positive inter-cluster
edges and negative intra-cluster edges.
</p>
<p>Consider an input graph $G$ on $n$ vertices such that the positive edges
induce a $\lambda$-arboric graph. Our main result is a 3-approximation
(\emph{in expectation}) algorithm that runs in $\mathcal{O}(\log \lambda \cdot
\log \log n)$ MPC rounds in the \emph{sublinear memory regime}. This is
obtained by combining structural properties of correlation clustering on
bounded arboricity graphs with the insights of Fischer and Noever (SODA '18) on
randomized greedy MIS and the \PIVOT algorithm of Ailon, Charikar, and Newman
(STOC '05). Combined with known graph matching algorithms, our structural
property also implies an exact algorithm and algorithms with \emph{worst case}
$(1+\epsilon)$-approximation guarantees in the special case of forests, where
$\lambda=1$.
</p></div>
    </summary>
    <updated>2021-02-24T22:49:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.11578</id>
    <link href="http://arxiv.org/abs/2102.11578" rel="alternate" type="text/html"/>
    <title>PEMesh: a Graphical Framework for the Analysis of the InterplayBetween Geometry and PEM Solvers</title>
    <feedworld_mtime>1614124800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cabiddu:Daniela.html">Daniela Cabiddu</a>, Giuseppe Patanè, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Spagnuolo:Michela.html">Michela Spagnuolo</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.11578">PDF</a><br/><b>Abstract: </b>Partial differential equations can be solved on general polygonal and
polyhedral meshes, through Polytopal Element Methods (PEMs). Unfortunately, the
relation between geometry and analysis is still unknown and subject to ongoing
research in order to identify weaker shape-regularity criteria under which PEMs
can reliably work. We propose PEMesh, a graphical framework to support the
analysis of the relation between the geometric properties of polygonal meshes
and the numerical performances of PEM solvers. PEMesh allows the design of
polygonal meshes that increasingly stress some geometric properties, by
exploiting any external PEM solver, and supports the study of the correlation
between the performances of such a solver and geometric properties of the input
mesh. Furthermore, it is highly modular, customisable, easy to use, and
provides the possibility to export analysis results both as numerical values
and graphical plots. PEMesh has a potential practical impact on ongoing and
future research activities related to PEM methods, polygonal mesh generation
and processing.
</p></div>
    </summary>
    <updated>2021-02-24T22:53:26Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-02-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.11548</id>
    <link href="http://arxiv.org/abs/2102.11548" rel="alternate" type="text/html"/>
    <title>Maximizing Agreements for Ranking, Clustering and Hierarchical Clustering via MAX-CUT</title>
    <feedworld_mtime>1614124800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chatziafratis:Vaggos.html">Vaggos Chatziafratis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mahdian:Mohammad.html">Mohammad Mahdian</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ahmadian:Sara.html">Sara Ahmadian</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.11548">PDF</a><br/><b>Abstract: </b>In this paper, we study a number of well-known combinatorial optimization
problems that fit in the following paradigm: the input is a collection of
(potentially inconsistent) local relationships between the elements of a ground
set (e.g., pairwise comparisons, similar/dissimilar pairs, or ancestry
structure of triples of points), and the goal is to aggregate this information
into a global structure (e.g., a ranking, a clustering, or a hierarchical
clustering) in a way that maximizes agreement with the input. Well-studied
problems such as rank aggregation, correlation clustering, and hierarchical
clustering with triplet constraints fall in this class of problems.
</p>
<p>We study these problems on stochastic instances with a hidden embedded ground
truth solution. Our main algorithmic contribution is a unified technique that
uses the maximum cut problem in graphs to approximately solve these problems.
Using this technique, we can often get approximation guarantees in the
stochastic setting that are better than the known worst case inapproximability
bounds for the corresponding problem. On the negative side, we improve the
worst case inapproximability bound on several hierarchical clustering
formulations through a reduction to related ranking problems.
</p></div>
    </summary>
    <updated>2021-02-24T22:51:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.11489</id>
    <link href="http://arxiv.org/abs/2102.11489" rel="alternate" type="text/html"/>
    <title>Optimal Sorting Circuits for Short Keys</title>
    <feedworld_mtime>1614124800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lin:Wei=Kai.html">Wei-Kai Lin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shi:Elaine.html">Elaine Shi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.11489">PDF</a><br/><b>Abstract: </b>A long-standing open question in the algorithms and complexity literature is
whether there exist sorting circuits of size $o(n \log n)$. A recent work by
Asharov, Lin, and Shi (SODA'21) showed that if the elements to be sorted have
short keys whose length $k = o(\log n)$, then one can indeed overcome the
$n\log n$ barrier for sorting circuits, by leveraging non-comparison-based
techniques. More specifically, Asharov et al.~showed that there exist $O(n)
\cdot \min(k, \log n)$-sized sorting circuits for $k$-bit keys, ignoring
$poly\log^*$ factors. Interestingly, the recent works by Farhadi et al.
(STOC'19) and Asharov et al. (SODA'21) also showed that the above result is
essentially optimal for every key length $k$, assuming that the famous Li-Li
network coding conjecture holds. Note also that proving any {\it unconditional}
super-linear circuit lower bound for a wide class of problems is beyond the
reach of current techniques.
</p>
<p>Unfortunately, the approach taken by Asharov et al.~to achieve optimality in
size somewhat crucially relies on sacrificing the depth: specifically, their
circuit is super-{\it poly}logarithmic in depth even for 1-bit keys. Asharov et
al.~phrase it as an open question how to achieve optimality both in size and
depth. In this paper, we close this important gap in our understanding. We
construct a sorting circuit of size $O(n) \cdot \min(k, \log n)$ (ignoring
$poly\log^*$ terms) and depth $O(\log n)$. To achieve this, our approach
departs significantly from the prior works. Our result can be viewed as a
generalization of the landmark result by Ajtai, Koml\'os, and Szemer\'edi
(STOC'83), simultaneously in terms of size and depth. Specifically, for $k =
o(\log n)$, we achieve asymptotical improvements in size over the AKS sorting
circuit, while preserving optimality in depth.
</p></div>
    </summary>
    <updated>2021-02-24T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.11435</id>
    <link href="http://arxiv.org/abs/2102.11435" rel="alternate" type="text/html"/>
    <title>Robust $k$-Center with Two Types of Radii</title>
    <feedworld_mtime>1614124800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chakrabarty:Deeparnab.html">Deeparnab Chakrabarty</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Negahbani:Maryam.html">Maryam Negahbani</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.11435">PDF</a><br/><b>Abstract: </b>In the non-uniform $k$-center problem, the objective is to cover points in a
metric space with specified number of balls of different radii. Chakrabarty,
Goyal, and Krishnaswamy [ICALP 2016, Trans. on Algs. 2020] (CGK, henceforth)
give a constant factor approximation when there are two types of radii. In this
paper, we give a constant factor approximation for the two radii case in the
presence of outliers. To achieve this, we need to bypass the technical barrier
of bad integrality gaps in the CGK approach. We do so using "the ellipsoid
method inside the ellipsoid method": use an outer layer of the ellipsoid method
to reduce to stylized instances and use an inner layer of the ellipsoid method
to solve these specialized instances. This idea is of independent interest and
could be applicable to other problems.
</p>
<p>Keywords: Approximation, Clustering, Outliers, and Round-or-Cut.
</p></div>
    </summary>
    <updated>2021-02-24T22:51:59Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.11393</id>
    <link href="http://arxiv.org/abs/2102.11393" rel="alternate" type="text/html"/>
    <title>No-Reference Quality Assessment for 360-degree Images by Analysis of Multi-frequency Information and Local-global Naturalness</title>
    <feedworld_mtime>1614124800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Wei.html">Wei Zhou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Jiahua.html">Jiahua Xu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jiang:Qiuping.html">Qiuping Jiang</a>, Zhibo Chen <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.11393">PDF</a><br/><b>Abstract: </b>360-degree/omnidirectional images (OIs) have achieved remarkable attentions
due to the increasing applications of virtual reality (VR). Compared to
conventional 2D images, OIs can provide more immersive experience to consumers,
benefitting from the higher resolution and plentiful field of views (FoVs).
Moreover, observing OIs is usually in the head mounted display (HMD) without
references. Therefore, an efficient blind quality assessment method, which is
specifically designed for 360-degree images, is urgently desired. In this
paper, motivated by the characteristics of the human visual system (HVS) and
the viewing process of VR visual contents, we propose a novel and effective
no-reference omnidirectional image quality assessment (NR OIQA) algorithm by
Multi-Frequency Information and Local-Global Naturalness (MFILGN).
Specifically, inspired by the frequency-dependent property of visual cortex, we
first decompose the projected equirectangular projection (ERP) maps into
wavelet subbands. Then, the entropy intensities of low and high frequency
subbands are exploited to measure the multi-frequency information of OIs.
Besides, except for considering the global naturalness of ERP maps, owing to
the browsed FoVs, we extract the natural scene statistics features from each
viewport image as the measure of local naturalness. With the proposed
multi-frequency information measurement and local-global naturalness
measurement, we utilize support vector regression as the final image quality
regressor to train the quality evaluation model from visual quality-related
features to human ratings. To our knowledge, the proposed model is the first
no-reference quality assessment method for 360-degreee images that combines
multi-frequency information and image naturalness. Experimental results on two
publicly available OIQA databases demonstrate that our proposed MFILGN
outperforms state-of-the-art approaches.
</p></div>
    </summary>
    <updated>2021-02-24T22:55:57Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-02-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.11360</id>
    <link href="http://arxiv.org/abs/2102.11360" rel="alternate" type="text/html"/>
    <title>Partially Optimal Edge Fault-Tolerant Spanners</title>
    <feedworld_mtime>1614124800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bodwin:Greg.html">Greg Bodwin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dinitz:Michael.html">Michael Dinitz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Robelle:Caleb.html">Caleb Robelle</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.11360">PDF</a><br/><b>Abstract: </b>Recent work has established that, for every positive integer $k$, every
$n$-node graph has a $(2k-1)$-spanner on $O(f^{1-1/k} n^{1+1/k})$ edges that is
resilient to $f$ edge or vertex faults. For vertex faults, this bound is tight.
However, the case of edge faults is not as well understood: the best known
lower bound for general $k$ is $\Omega(f^{\frac12 - \frac{1}{2k}} n^{1+1/k}
+fn)$. Our main result is to nearly close this gap with an improved upper
bound, thus separating the cases of edge and vertex faults. For odd $k$, our
new upper bound is $O_k(f^{\frac12 - \frac{1}{2k}} n^{1+1/k} + fn)$, which is
tight up to hidden $poly(k)$ factors. For even $k$, our new upper bound is
$O_k(f^{1/2} n^{1+1/k} +fn)$, which leaves a gap of $poly(k) f^{1/(2k)}$. Our
proof is an analysis of the fault-tolerant greedy algorithm, which requires
exponential time, but we also show that there is a polynomial-time algorithm
which creates edge fault tolerant spanners that are larger only by factors of
$k$.
</p></div>
    </summary>
    <updated>2021-02-24T22:50:30Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2102.11349</id>
    <link href="http://arxiv.org/abs/2102.11349" rel="alternate" type="text/html"/>
    <title>Quantum query complexity with matrix-vector products</title>
    <feedworld_mtime>1614124800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Childs:Andrew_M=.html">Andrew M. Childs</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hung:Shih=Han.html">Shih-Han Hung</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Tongyang.html">Tongyang Li</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2102.11349">PDF</a><br/><b>Abstract: </b>We study quantum algorithms that learn properties of a matrix using queries
that return its action on an input vector. We show that for various problems,
including computing the trace, determinant, or rank of a matrix or solving a
linear system that it specifies, quantum computers do not provide an asymptotic
speedup over classical computation. On the other hand, we show that for some
problems, such as computing the parities of rows or columns or deciding if
there are two identical rows or columns, quantum computers provide exponential
speedup. We demonstrate this by showing equivalence between models that provide
matrix-vector products, vector-matrix products, and vector-matrix-vector
products, whereas the power of these models can vary significantly for
classical computation.
</p></div>
    </summary>
    <updated>2021-02-24T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-02-24T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5350</id>
    <link href="https://www.scottaaronson.com/blog/?p=5350" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5350#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5350" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Stop emailing my utexas address</title>
    <summary xml:lang="en-US">A month ago, UT Austin changed its email policies—banning auto-forwarding from university accounts to Gmail accounts, apparently as a way to force the faculty and other employees to separate their work email from their personal email, and thereby comply with various government regulations. Ever since that change, the email part of my life has been […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>A month ago, UT Austin changed its email policies—banning auto-forwarding from university accounts to Gmail accounts, apparently as a way to force the faculty and other employees to separate their work email from their personal email, and thereby comply with various government regulations.  Ever since that change, the email part of my life has been a <em>total, unmitigated disaster</em>.  I’ve missed (or been late to see) dozens of important work emails, with the only silver lining being that that’s arguably UT’s problem more than it is mine!</p>



<p>And yes, I’ve already gone to technical support; the only answer I’ve gotten is that (in so many words) there <em>is</em> no answer.  Other UT faculty are somehow able to deal with this because they are them; I am unable to deal with it because I am me.  As a mere PhD in computer science, I’m utterly unqualified to set up a technical fix for this sort of problem.</p>



<p>So the bottom line is: <strong>from now on, if you want me to see an email, send it to scott@scottaaronson.com</strong>.  Really.  If you try sending it to aaronson@cs.utexas.edu, it will land in a separate inbox that I can access only with great inconvenience.  And if, God forbid, you try sending it to aaronson@utexas.edu, the email will bounce and I’ll never see it at all.  Indeed, a central purpose of this post is just to have a place to point the people who contact me every day, shocked that their emails to me bounced.</p>



<p>This whole episode has given me <em>immense</em> sympathy for Hillary Clinton, and for the factors that led her to set up clintonemail.com from her house.  It’s not merely that her private email server was a laughably trivial reason to end the United States’ 240-year run of democratic government.  Rather it’s that, even on the narrow question of emails, I now feel certain that <em>Hillary was 100% right</em>.  Bureaucracy that impedes communication is a cancer on human civilization.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update:</span></strong> Thanks so much to commenter Avraham and to my colleague Etienne Vouga, who quickly gave me the crucial information that tech support would not, and thereby let me solve this problem.  I can once again easily read emails sent to aaronson@cs.utexas.edu … well, at least for now!  I’m now checking about aaronson@utexas.edu.  Again, though, <strong>scott@scottaaronson.com to be safe</strong>.</p></div>
    </content>
    <updated>2021-02-23T21:00:37Z</updated>
    <published>2021-02-23T21:00:37Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Rage Against Doofosity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-02-24T23:14:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/026</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/026" rel="alternate" type="text/html"/>
    <title>TR21-026 |  Conditional Dichotomy of Boolean Ordered Promise CSPs | 

	Joshua Brakensiek, 

	Venkatesan Guruswami, 

	Sai Sandeep</title>
    <summary>Promise Constraint Satisfaction Problems (PCSPs) are a generalization of Constraint Satisfaction Problems (CSPs) where each predicate has a strong and a weak form and given a CSP instance, the objective is to distinguish if the strong form can be satisfied vs. even the weak form cannot be satisfied. Since their formal introduction by Austrin, Guruswami, and Håstad, there has been a flurry of works on PCSPs, including recent breakthroughs in approximate graph coloring. The key tool in studying PCSPs is the algebraic framework developed in the context of CSPs where the closure properties of the satisfying solutions known as *polymorphisms* are analyzed. 
    
    The polymorphisms of PCSPs are significantly richer than CSPs---this is illustrated by the fact that even in the Boolean case, we still do not know if there exists a dichotomy result for PCSPs analogous to Schaefer's dichotomy result for CSPs. In this paper, we study a special case of Boolean PCSPs, namely Boolean *Ordered* PCSPs where the Boolean PCSPs have the predicate $x \leq y$. In the algebraic framework, this is the special case of Boolean PCSPs when the polymorphisms are *monotone* functions. We prove that Boolean Ordered PCSPs exhibit a computational dichotomy assuming the Rich $2$-to-$1$ Conjecture due to Braverman, Khot, and Minzer, which is a perfect completeness surrogate of the Unique Games Conjecture. 
    
    In particular, assuming the Rich $2$-to-$1$ Conjecture, we prove that a Boolean Ordered PCSP can be solved in polynomial time if for every $\epsilon &gt;0$, it has polymorphisms where each coordinate has *Shapley value* at most $\epsilon$, else it is NP-hard. The algorithmic part of our dichotomy result is based on a structural lemma showing that Boolean monotone functions with each coordinate having low Shapley value have arbitrarily large threshold functions as minors. The hardness part proceeds by showing that the Shapley value is consistent under a uniformly random $2$-to-$1$ minor. As a structural result of independent interest, we construct an example to show that the Shapley value can be inconsistent under an adversarial $2$-to-$1$ minor.</summary>
    <updated>2021-02-23T17:37:03Z</updated>
    <published>2021-02-23T17:37:03Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-02-25T08:20:35Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6398537358110172858</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6398537358110172858/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/02/good-names-and-bad-names-of-game-shows.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6398537358110172858" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6398537358110172858" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/02/good-names-and-bad-names-of-game-shows.html" rel="alternate" type="text/html"/>
    <title>Good Names and Bad Names of Game Shows and theorems</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> In my post on Alex Trebek, see <a href="https://blog.computationalcomplexity.org/2020/11/alex-trebekwhat-is-todays-post-about.html">here</a>, I noted that <i>Jeopardy!</i> is not a good name for the game show since it doesn't tell you much about the show. Perhaps <i>Answers and Questions </i>is a better name.</p><p>The following game shows have names that tell you something about the game and hence have better names: </p><p>Wheel of Fortune, The Price is Right, Lets make a Deal, Beautiful women have suitcases full of money (the original name for Deal-No Deal), Win Ben Stein's Money, Beat the Geeks. </p><p>In Math we often name a concept  after a person. While this may be a good way to honor someone, the name does not tell us much about the concept and it leads to statements like:</p><p><br/></p><p><i>A Calabi-Yau manifold is a compact complex Kahler manifold with a trivial first Chern class. </i></p><p><i>A Kahler manifold is a Hermitian manifold for which the Hermitian form is closed.</i></p><p><i>A Hermitian manifold is the complex analog of the Riemann manifold. </i></p><p>(These examples are from an article I will point to later---I do not understand <i>any </i>of these terms, though I once knew what a <i>Riemann manifold</i> was. I heard the term <i>Kahler Manifold </i>in the song <a href="https://www.youtube.com/watch?v=2rjbtsX7twc">Bohemian Gravity</a>.  It's at about the 4 minute 30 second place.) </p><p>While I am amused by the name <i>Victoria Delfino Problems</i> (probably the only realtor who has problems in math named after her, see my post <a href="https://blog.computationalcomplexity.org/2021/02/the-victoria-delfino-problems-example.html">here</a>) it's not a descriptive way to name open problems in descriptive set theory. </p><p><br/></p><p>Sometimes  a name becomes SO connected to a concept that it IS descriptive, e.g.:</p><p><i>The first proof of VDW's theorem yields ACKERMAN-LIKE bounds. </i></p><p>but you cannot count on that happening AND it is only descriptive to people already somewhat in the field. </p><p><br/></p><p>What to do? <a href="http://nautil.us/issue/89/the-dark-side/why-mathematicians-should-stop-naming-things-after-each-other">This</a> article makes the  ballian point that we should   STOP DOING THIS and that the person who first proves the theorem should name it in a way that tells you something about the concept. I would agree. But this can still be hard to really do.</p><p><br/></p><p>In my book on Muffin Mathematics (see <a href="https://www.amazon.com/Mathematical-Muffin-Morsels-Problem-Mathematics/dp/9811215170">here</a>) I have a sequence of methods called</p><p>Floor Ceiling, Half, Mid, Interval, Easy-Buddy-Match, Hard-Buddy-Match, Gap, Train. </p><p>There was one more method that I didn't quite name, but I used the phrase `Scott Muffin Problem' to honors Scott Huddleton who came up with the method, in my description of it. </p><p>All but the last concept were given ballian names.  Even so, you would need to read the book to see why the names make sense. Still, that would be easier than trying to figure out what a Calabi-Yau manifold is. </p><p><br/></p><p/><br/></div>
    </content>
    <updated>2021-02-23T05:36:00Z</updated>
    <published>2021-02-23T05:36:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-02-24T22:01:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=18152</id>
    <link href="https://rjlipton.wordpress.com/2021/02/21/riemann-hypothesis-why-so-hard/" rel="alternate" type="text/html"/>
    <title>Riemann Hypothesis—Why So Hard?</title>
    <summary>“If I were to awaken after having slept a thousand years, my first question would be: has the Riemann Hypothesis been proven?” — David Hilbert Steklov Institute memorial page Sergei Voronin was an expert in number theory, who studied the Riemann zeta function, but who sadly died young over twenty years ago. We discussed his […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>“If I were to awaken after having slept a thousand years, my first question would be: has the Riemann Hypothesis been proven?” — David Hilbert</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/21/riemann-hypothesis-why-so-hard/voroninsteklov/" rel="attachment wp-att-18177"><img alt="" class="alignright wp-image-18177" height="190" src="https://rjlipton.files.wordpress.com/2021/02/voroninsteklov.jpg?w=142&amp;h=190" width="142"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Steklov Institute memorial <a href="http://www.mi-ras.ru/index.php?c=inmemoria&amp;l=1">page</a></font></td>
</tr>
</tbody>
</table>
<p>
Sergei Voronin was an expert in number theory, who studied the Riemann zeta function, but who sadly died young over twenty years ago. We discussed his amazing 1975 result about the Riemann zeta function <a href="https://rjlipton.wordpress.com/2012/12/04/the-amazing-zeta-code/">here</a>. Others call the result the amazing theorem. I (Dick) am getting old—I almost forgot that we did a <a href="https://rjlipton.wordpress.com/2016/12/20/hunting-complexity-in-zeta/">post</a> on his theorem again over four years ago. </p>
<p>
Today I thought we would recall his theorem, sketch why the theorem is true, and then discuss some extensions.<br/>
<span id="more-18152"/></p>
<p>
Starting with Alan Turing we have been interested in universal objects. Turing famously <a href="https://en.wikipedia.org/wiki/Universal_Turing_machine">proved</a> that there are universal machines: these can simulate any other machine on any input. Martin Davis has an entire <a href="https://www.amazon.com/Universal-Computer-Road-Leibniz-Turing/dp/0393047857">book</a> on this subject. </p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/21/riemann-hypothesis-why-so-hard/davis/" rel="attachment wp-att-18156"><img alt="" class="aligncenter wp-image-18156" height="375" src="https://rjlipton.files.wordpress.com/2021/02/davis.jpg?w=235&amp;h=375" width="235"/></a>
</td>
</tr>
</tbody></table>
<p>
Universal objects are basic to complexity theory. Besides Turing’s notion, a universal property is key to the definition of NP-complete. A set <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{S}"/> in NP is NP-complete provided all other sets in NP can be reduced to <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{S}"/> in polynomial time. Michael Nielsen once began a <a href="https://www.quantamagazine.org/the-physical-origin-of-universal-computing-20151027/">discussion</a> of universality in this amusing fashion:</p>
<blockquote><p><b> </b> <em> Imagine you’re shopping for a new car, and the salesperson says, “Did you know, this car doesn’t just drive on the road.” “Oh?” you reply. “Yeah, you can also use it to do other things. For instance, it folds up to make a pretty good bicycle. And it folds out to make a first-rate airplane. Oh, and when submerged it works as a submarine. And it’s a spaceship too!” </em>
</p></blockquote>
<p/><h2> Voronin’s Insight </h2><p/>
<p>
In 1975 Voronin had the brilliant insight that the Riemann zeta <img alt="{\zeta(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\zeta(s)}"/> function has an interesting universality property.  Roughly speaking, it says that a wide class of analytic functions can be approximated by shifts <img alt="{\zeta(s+it)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%2Bit%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\zeta(s+it)}"/> with real <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{t}"/>. Recall 	</p>
<p align="center"><img alt="\displaystyle  \zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Czeta%28s%29+%3D+%5Csum_%7Bn%3D1%7D%5E%5Cinfty+%5Cfrac%7B1%7D%7Bn%5Es%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  \zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s} "/></p>
<p>for <img alt="{\Re(s) &gt;1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CRe%28s%29+%3E1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\Re(s) &gt;1}"/>, and it has an analytic extension for all other values but <img alt="{s=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{s=1}"/>.</p>
<p>
The intense interest in the <img alt="{\zeta(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\zeta(s)}"/> function started in 1859 with Bernhard Riemann’s breakthrough <a href="https://en.wikipedia.org/wiki/On_the_Number_of_Primes_Less_Than_a_Given_Magnitude">article</a>. This was the first statement of what we call the Riemann Hypothesis (RH).</p>
<p>
In over a century of research on RH before Voronin’s theorem, many identities, many results, many theorems were proved about the zeta function. But none saw that the <img alt="{\zeta(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\zeta(s)}"/> function was universal before Voronin. Given the zeta function’s importance in understanding the structure of prime numbers this seems to be surprising. </p>
<p>
Before we define the universal property I thought it might be useful to state a related <a href="https://arxiv.org/pdf/1305.3933.pdf">property</a> that the <img alt="{\zeta(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\zeta(s)}"/> function has:</p>
<blockquote><p><b>Theorem 1</b> <em> Suppose that <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{P}"/> is a polynomial so that for all <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{s}"/>, 	</em></p><em>
<p align="center"><img alt="\displaystyle  P\left(\zeta(s), \zeta{'}(s),\dots,\zeta^{(m)}(s) \right) = 0. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++P%5Cleft%28%5Czeta%28s%29%2C+%5Czeta%7B%27%7D%28s%29%2C%5Cdots%2C%5Czeta%5E%7B%28m%29%7D%28s%29+%5Cright%29+%3D+0.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  P\left(\zeta(s), \zeta{'}(s),\dots,\zeta^{(m)}(s) \right) = 0. "/></p>
</em><p><em>Then <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{P}"/> is identically zero. </em>
</p></blockquote>
<p>Since <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{s}"/> is a single variable, this says that <img alt="{\zeta(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\zeta(s)}"/> and its derivatives <img alt="{\zeta'(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%27%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\zeta'(s)}"/> and <img alt="{\zeta''(s) \dots }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%27%27%28s%29+%5Cdots+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\zeta''(s) \dots }"/> do not satisfy any polynomial relationship. This means intuitively that <img alt="{\zeta(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\zeta(s)}"/> must be <a href="https://en.wikipedia.org/wiki/Hypertranscendental_function">hypertranscendental</a>. Let’s now make this formal.</p>
<p/><h2> Voronin’s Theorem </h2><p/>
<p>
Here is his <a href="https://en.wikipedia.org/wiki/Zeta_function_universality">theorem</a>: </p>
<blockquote><p><b>Theorem 2</b> <em> Let <img alt="{0&lt;r&lt;1/4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%3Cr%3C1%2F4%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{0&lt;r&lt;1/4}"/>. Let <img alt="{f(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28s%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{f(s)}"/> be an analytic function that never is zero for <img alt="{|s| \le r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Cs%7C+%5Cle+r%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{|s| \le r}"/>. Then for any <img alt="{\epsilon&gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\epsilon&gt;0}"/> there is a real <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{t}"/> so that 	</em></p><em>
<p align="center"><img alt="\displaystyle  \max_{\left | s \right | \leq r} \left | \zeta(s + \frac{3}{4} + i t) - f(s) \right | &lt; \epsilon. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmax_%7B%5Cleft+%7C+s+%5Cright+%7C+%5Cleq+r%7D+%5Cleft+%7C+%5Czeta%28s+%2B+%5Cfrac%7B3%7D%7B4%7D+%2B+i+t%29+-+f%28s%29+%5Cright+%7C+%3C+%5Cepsilon.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  \max_{\left | s \right | \leq r} \left | \zeta(s + \frac{3}{4} + i t) - f(s) \right | &lt; \epsilon. "/></p>
</em><p><em/>
</p></blockquote>
<p/><p>
See the <a href="https://www.researchgate.net/profile/Renata-Macaitiene/publication/321139128_Zeros_of_the_Riemann_zeta-function_and_its_universality/links/5c7f7b5092851c695058d6fe/Zeros-of-the-Riemann-zeta-function-and-its-universality.pdf">paper</a> “Zeroes of the Riemann zeta-function and its universality,” by Ramunas Garunkstis, Antanas Laurincikas, and Renata Macaitiene, for a detailed modern discussion of his theorem. </p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/21/riemann-hypothesis-why-so-hard/thm/" rel="attachment wp-att-18158"><img alt="" class="aligncenter wp-image-18158" height="275" src="https://rjlipton.files.wordpress.com/2021/02/thm.png?w=320&amp;h=275" width="320"/></a>
</td>
</tr>
</tbody></table>
<p/><p><br/>
Note that the theorem is not constructive. However, the values of <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{t}"/> that work have a positive density—there are lots of them. Also note the restriction that <img alt="{f(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{f(s)}"/> is never zero is critical. Otherwise one would be able to show that the Riemann Hypothesis is false. In 2003, Garunkstis et al. did prove a constructive version, in a <a href="https://www.jstor.org/stable/43736941?seq=1">paper</a> titled, “Effective Uniform Approximation By The Riemann Zeta-Function.”</p>
<p/><h2> Voronin’s Proof </h2><p/>
<p>
The key insight is to combine two properties of the zeta <img alt="{\zeta(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\zeta(s)}"/> function: The usual definition with the Euler product. Recall the Riemann zeta-function has an Euler product expression 	</p>
<p align="center"><img alt="\displaystyle  \zeta(s) = \prod_p \frac{1}{1-p^{-s}}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Czeta%28s%29+%3D+%5Cprod_p+%5Cfrac%7B1%7D%7B1-p%5E%7B-s%7D%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  \zeta(s) = \prod_p \frac{1}{1-p^{-s}}. "/></p>
<p>where <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{p}"/> runs over prime numbers. This is valid only in the region <img alt="{\Re(s) &gt; 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CRe%28s%29+%3E+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\Re(s) &gt; 1}"/>, but it makes sense in a approximate sense in the critical strip: 	</p>
<p align="center"><img alt="\displaystyle  1/2 &lt; \Re(s) &lt; 1. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1%2F2+%3C+%5CRe%28s%29+%3C+1.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  1/2 &lt; \Re(s) &lt; 1. "/></p>
<p>Then take logarithms and since <img alt="{\log(p)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog%28p%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\log(p)}"/> are linearly independent over <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{Q}"/>, we can apply the Kronecker approximation theorem to obtain that any target function <img alt="{f(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{f(s)}"/> can be approximated by the above finite truncation. This is the basic structure of the <a href="https://en.wikipedia.org/wiki/Zeta_function_universality">proof</a>.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Voronin’s insight was immediately interesting to number theorists. Many found new methods for proving universality and for extending it to other functions. Some methods work for all zeta-functions defined by Euler products. See this <a href="https://arxiv.org/pdf/1407.4216.pdf">survey</a> by Kohji Matsumoto and a recent <a href="https://www.semanticscholar.org/paper/Quantized-Number-Theory%2C-Fractal-Strings-and-the-to-Herichi-Lapidus/37dbcf9c28c316b8cfcfe74394f3a1f2d709235d">paper</a><br/>
by Hafedh Herichi and Michel Lapidus, the latter titled “Quantized Number Theory, Fractal Strings and the Riemann Hypothesis: From Spectral Operators to Phase Transitions and Universality.”</p>
<p>
Perhaps the most interesting question is: </p>
<p><i>Can universality be used to finally unravel the RH?</i> </p>
<p>See Paul Gauthier’s 2014 IAS <a href="http://www.math.kent.edu/~zvavitch/informal/Informal_Analysis_Seminar/Slides,_April_2014_files/IAS2014_Gauthier_1.pdf">talk</a>, “Universality and the Riemann Hypothesis,” for some ideas.</p>
<p>
[fixed missing line at end]</p></font></font></div>
    </content>
    <updated>2021-02-21T23:39:00Z</updated>
    <published>2021-02-21T23:39:00Z</published>
    <category term="Oldies"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="amazing"/>
    <category term="theorem"/>
    <category term="universal"/>
    <category term="Voronin"/>
    <category term="zeta"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2021-02-25T08:20:47Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-1990979806491986425</id>
    <link href="http://processalgebra.blogspot.com/feeds/1990979806491986425/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=1990979806491986425" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/1990979806491986425" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/1990979806491986425" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2021/02/article-by-sergey-kitaev-and-anthony.html" rel="alternate" type="text/html"/>
    <title>Article by Sergey Kitaev and Anthony Mendes in Jeff Remmel's memory</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://www.strath.ac.uk/staff/kitaevsergeydr/" target="_blank">Sergey Kitaev</a> just shared with me <a href="http://ecajournal.haifa.ac.il/Volume2021/ECA2021_S1H2.pdf" target="_blank">an article</a> he wrote with Anthony Mendes in <a href="https://senate.universityofcalifornia.edu/in-memoriam/files/jeffrey-b-remmel.html" target="_blank">Jeff Remmel's memory</a>. Jeff Remmel was a distinguished mathematician with a very successful career in both logic and combinatorics. </p><p>The short biography at the start of the article paints a vivid picture of Jeff Remmel's  personality, and will be of interest and inspiration to many readers. His hiring as "an Assistant Professor in the Department of Mathematics at UC San Diego at age 25, without officially finishing his Ph.D. and without having published a single paper" was, in Jeff Remmel's own words, a "fluke that will never happen again."<br/></p><p>I had the pleasure of making Jeff Remmel's acquaintance when he visited Sergey in Reykjavik and thoroughly enjoyed talking to him about a variety of subjects. He was truly a larger-than-life academic. <br/></p></div>
    </content>
    <updated>2021-02-21T11:47:00Z</updated>
    <published>2021-02-21T11:47:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2021-02-21T11:47:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/025</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/025" rel="alternate" type="text/html"/>
    <title>TR21-025 |  Improved Maximally Recoverable LRCs using Skew Polynomials | 

	Sivakanth Gopi, 

	Venkatesan Guruswami</title>
    <summary>An $(n,r,h,a,q)$-Local Reconstruction Code is a linear code over $\mathbb{F}_q$ of length $n$, whose codeword symbols are partitioned into $n/r$ local groups each of size $r$. Each local group satisfies `$a$' local parity checks to recover from `$a$' erasures in that local group and there are further $h$ global parity checks to provide fault tolerance from more global erasure patterns. Such an LRC is Maximally Recoverable (MR), if it offers the best blend of locality and global erasure resilience---namely it can correct all erasure patterns whose recovery is information-theoretically feasible given the locality structure (these are precisely patterns with up to `$a$' erasures in each local group and an additional $h$ erasures anywhere in the codeword).

Random constructions can easily show the existence of MR LRCs over very large fields, but a major algebraic challenge is to construct MR LRCs, or even show their existence, over smaller fields, as well as understand inherent lower bounds on their field size. We give an explicit construction of $(n,r,h,a,q)$-MR LRCs with field size $q$ bounded by $\left(O\left(\max\{r,n/r\}\right)\right)^{\min\{h,r-a\}}$. This improves upon known constructions in many relevant parameter ranges. Moreover, it matches the lower bound from Gopi et al. (2020) in an interesting range of parameters where $r=\Theta(\sqrt{n})$, $r-a=\Theta(\sqrt{n})$ and $h$ is a fixed constant with $h\le a+2$, achieving the optimal field size of $\Theta_{h}(n^{h/2}).$

Our construction is based on the theory of skew polynomials.  We believe skew polynomials should have further applications in coding and complexity theory; as a small illustration we show how to capture algebraic results underlying list decoding folded Reed-Solomon and multiplicity codes in a unified way within this theory.</summary>
    <updated>2021-02-21T10:21:05Z</updated>
    <published>2021-02-21T10:21:05Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-02-25T08:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/024</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/024" rel="alternate" type="text/html"/>
    <title>TR21-024 |  A Majority Lemma for Randomised Query Complexity | 

	Mika Göös, 

	Gilbert Maystre</title>
    <summary>We show that computing the majority of $n$ copies of a boolean function $g$ has randomised query complexity $\mathrm{R}(\mathrm{Maj} \circ g^n) = \Theta(n\cdot \bar{\mathrm{R}}_{1/n}(g))$. In fact, we show that to obtain a similar result for any composed function $f\circ g^n$, it suffices to prove a sufficiently strong form of the result only in the special case $g=\mathrm{GapOr}$.</summary>
    <updated>2021-02-21T10:18:33Z</updated>
    <published>2021-02-21T10:18:33Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-02-25T08:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/023</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/023" rel="alternate" type="text/html"/>
    <title>TR21-023 |  $3.1n - o(n)$ Circuit Lower Bounds for Explicit Functions | 

	Tianqi Yang, 

	Jiatu Li</title>
    <summary>Proving circuit lower bounds has been an important but extremely hard problem for decades. Although one may show that almost every function $f:\mathbb{F}_2^n\to\mathbb{F}_2$ requires circuit of size $\Omega(2^n/n)$ by a simple counting argument, it remains unknown whether there is an explicit function (for example, a function in $NP$) not computable by circuits of size $10n$. In fact, a $3n-o(n)$ explicit lower bound by Blum (TCS, 1984) was unbeaten for over 30 years until a recent breakthrough by Find et al. (FOCS, 2016), which proved a $(3+\frac{1}{86})n-o(n)$ lower bound for affine dispersers, a class of functions known to be constructible in $P$.

In this paper, we prove a stronger lower bound $3.1n - o(n)$ for affine dispersers. To get this result, we strengthen the gate elimination approach for $(3+\frac{1}{86})n$ lower bound, by a more sophisticated case analysis that significantly decreases the number of bottleneck structures introduced during the elimination procedure. Intuitively, our improvement relies on three observations: adjacent bottleneck structures becomes less troubled; the gates eliminated are usually connected; and the hardest cases during gate elimination have nice local properties to prevent the introduction of new bottleneck structures.</summary>
    <updated>2021-02-21T06:59:55Z</updated>
    <published>2021-02-21T06:59:55Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-02-25T08:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/022</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/022" rel="alternate" type="text/html"/>
    <title>TR21-022 |  Depth lower bounds in Stabbing Planes for combinatorial principles | 

	Stefan Dantchev, 

	Nicola  Galesi, 

	Abdul Ghani, 

	Barnaby Martin</title>
    <summary>We prove logarithmic depth lower bounds in Stabbing Planes for the classes of  combinatorial principles known as  the Pigeonhole principle and the Tseitin contradictions. The depth lower bounds are new, obtained by giving almost linear length lower bounds which do not depend on the bit-size of the inequalities and in the case of  the Pigeonhole principle are tight. 

The technique known so far to prove depth lower bounds for Stabbing Planes is a generalization of that used for the Cutting Planes proof system.  In this work  we  introduce two  new approaches to prove length/depth lower bounds in Stabbing Planes: one relying on Sperner's Theorem which works for the Pigeonhole principle and Tseitin contradictions over the complete graph; a second proving the lower bound for Tseitin contradictions over a grid graph, which uses a result on essential  coverings of the boolean cube by linear polynomials, which in turn relies on Alon's combinatorial Nullenstellensatz</summary>
    <updated>2021-02-20T20:20:01Z</updated>
    <published>2021-02-20T20:20:01Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-02-25T08:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/021</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/021" rel="alternate" type="text/html"/>
    <title>TR21-021 |  Average-Case Perfect Matching Lower Bounds from Hardness of Tseitin Formulas | 

	Kilian Risse, 

	Per Austrin</title>
    <summary>We study the complexity of proving that a sparse random regular graph on an odd number of vertices does not have a perfect matching, and related problems involving each vertex being matched some pre-specified number of times. We show that this requires proofs of degree $\Omega(n/\log n)$ in the Polynomial Calculus (over fields of characteristic $\ne 2$) and Sum-of-Squares proof systems, and exponential size in the bounded-depth Frege proof system. This resolves a question by Razborov asking whether the Lovász-Schrijver proof system requires $n^\delta$ rounds to refute these formulas for some $\delta &gt; 0$. The results are obtained by a worst-case to average-case reduction of these formulas relying on a topological embedding theorem which may be of independent interest.</summary>
    <updated>2021-02-20T18:26:59Z</updated>
    <published>2021-02-20T18:26:59Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-02-25T08:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/020</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/020" rel="alternate" type="text/html"/>
    <title>TR21-020 |  Error Reduction For Weighted PRGs Against Read Once Branching Programs | 

	Gil Cohen, 

	Dean Doron, 

	Amnon Ta-Shma, 

	Ori Sberlo, 

	Oren Renard</title>
    <summary>Weighted pseudorandom generators (WPRGs), introduced by Braverman, Cohen and Garg [BCG20], is a generalization of pseudorandom generators (PRGs) in which arbitrary real weights are considered rather than a probability mass. Braverman et al. constructed WPRGs against read once branching programs (ROBPs) with near-optimal dependence on the error parameter. Chattopadhyay and Liao [CL20] somewhat simplified the technically involved BCG construction, also obtaining some improvement in parameters.

In this work we devise an error reduction procedure for PRGs against ROBPs. More precisely, our procedure transforms any PRG against length n width w ROBP with error 1/poly(n) having seed length s to a WPRG with seed length s + O(log(w/?)loglog(1/?)). By instantiating our procedure with Nisan’s PRG [Nis92] we obtain a WPRG with seed length O(log(n)log(nw) + log(w/?)loglog(1/?)). This improves upon [BCG20] and is incomparable with [CL20].

Our construction is significantly simpler on the technical side and is conceptually cleaner. Another advantage of our construction is its low space complexity O(log nw)+ poly(loglog(1/?)) which is logarithmic in n for interesting values of the error parameter ?. Previous constructions (like [BCG20, CL20]) specify the seed length but not the space complexity, though it is plausible they can also achieve such (or close) space complexity.</summary>
    <updated>2021-02-20T06:12:44Z</updated>
    <published>2021-02-20T06:12:44Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-02-25T08:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/019</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/019" rel="alternate" type="text/html"/>
    <title>TR21-019 |  Pseudodistributions That Beat All Pseudorandom Generators | 

	Edward Pyne, 

	Salil Vadhan</title>
    <summary>A recent paper of Braverman, Cohen, and Garg (STOC 2018) introduced the concept of a pseudorandom pseudodistribution generator (PRPG), which amounts to a pseudorandom generator (PRG) whose outputs are accompanied with real coefficients that scale the acceptance probabilities of any potential distinguisher. They gave an explicit construction of PRPGs for ordered branching programs whose seed length has a better dependence on the error parameter $\epsilon$ than the classic PRG construction of Nisan (STOC 1990 and Combinatorica 1992). 
    
    In this work, we give an explicit construction of PRPGs that achieve parameters that are impossible to achieve by a PRG.  In particular, we construct a PRPG for ordered permutation branching programs of unbounded width with a single accept state that has seed length $\tilde{O}(\log^{3/2} n)$ for error parameter $\epsilon=1/\text{poly}(n)$, where $n$ is the input length.  In contrast, recent work of Hoza et al. (ITCS 2021) shows that any PRG for this model requires seed length $\Omega(\log^2 n)$ to achieve error $\epsilon=1/\text{poly}(n)$.
    
    As a corollary, we obtain explicit PRPGs with seed length $\tilde{O}(\log^{3/2} n)$ and error $\epsilon=1/\text{poly}(n)$ for ordered permutation branching programs of width $w=\text{poly}(n)$ with an arbitrary number of accept states.  Previously, seed length $o(\log^2 n)$ was only known when both the width and the reciprocal of the error are subpolynomial, i.e. $w=n^{o(1)}$ and $\epsilon=1/n^{o(1)}$ (Braverman, Rao, Raz, Yehudayoff, FOCS 2010 and SICOMP 2014).
    
    The starting point for our results are the recent space-efficient algorithms for estimating random-walk probabilities in directed graphs by Ahmadenijad, Kelner, Murtagh, Peebles, Sidford, and Vadhan (FOCS 2020), which are based on spectral graph theory and space-efficient Laplacian solvers.  We interpret these algorithms as giving PRPGs with large seed length, which we then derandomize to obtain our results.  We also note that this approach gives a simpler proof of the original result of Braverman, Cohen, and Garg, as independently discovered by Cohen, Doron, Renard, Sberlo, and Ta-Shma (personal communication, January 2021).</summary>
    <updated>2021-02-20T06:10:41Z</updated>
    <published>2021-02-20T06:10:41Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-02-25T08:20:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/018</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/018" rel="alternate" type="text/html"/>
    <title>TR21-018 |  Monotone Branching Programs: Pseudorandomness and Circuit Complexity | 

	Dean Doron, 

	Raghu Meka, 

	Omer Reingold, 

	Avishay Tal, 

	Salil Vadhan</title>
    <summary>We study monotone branching programs, wherein the states at each time step can be ordered so that edges with the same labels never cross each other. Equivalently, for each fixed input, the transition functions are a monotone function of the state. 

We prove that constant-width monotone branching programs of polynomial size are equivalent in power to $AC^{0}$ circuits. This complements the celebrated theorem of Barrington, which states that constant-width branching programs, without the monotonicity constraint, are equivalent in power to $NC^{1}$ circuits.

Next we turn to read-once monotone branching programs of constant width, which we note are strictly more powerful than read-once $AC^0$.  Our main result is an explicit pseudorandom generator that $\varepsilon$-fools length $n$ programs with seed length $\widetilde{O}(\log(n/\varepsilon))$. This extends the families of constant-width read-once branching programs for which we have an explicit pseudorandom generator with near-logarithmic seed length. 

Our pseudorandom generator construction follows Ajtai and Wigderson's approach of iterated pseudorandom restrictions [AW89,GMRTV12]. We give a randomness-efficient width-reduction process which allows us to simplify the branching program after only $O(\log\log n)$ independent applications of the Forbes--Kelley pseudorandom restrictions [FK18].</summary>
    <updated>2021-02-20T06:07:23Z</updated>
    <published>2021-02-20T06:07:23Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-02-25T08:20:35Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/02/19/loops-degrees-matchings</id>
    <link href="https://11011110.github.io/blog/2021/02/19/loops-degrees-matchings.html" rel="alternate" type="text/html"/>
    <title>Loops, degrees, and matchings</title>
    <summary>A student in my graph algorithms class asked how self-loops in undirected graphs affect the vertex degrees and matchings of a graph. The standard answer is that a self-loop adds two to the degree (because each edge has two endpoints) and that they are useless in matching because matchings should have at most one incidence to each vertex, not two. But that’s just a convention; one could reasonably declare that the contribution of a self-loop to the degree is one, and I’m pretty sure I’ve seen sources that do just that. With that alternative convention, it should be possible to include a self-loop in a matching, and use it to match only a single vertex.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A student in my graph algorithms class asked how <a href="https://11011110.github.io/blog/2021/02/19/Loop (graph theory)">self-loops</a> in undirected graphs affect the vertex degrees and matchings of a graph. The standard answer is that a self-loop adds two to the degree (because each edge has two endpoints) and that they are useless in matching because matchings should have at most one incidence to each vertex, not two. But that’s just a convention; one could reasonably declare that the contribution of a self-loop to the degree is one, and I’m pretty sure I’ve seen sources that do just that. With that alternative convention, it should be possible to include a self-loop in a matching, and use it to match only a single vertex.</p>

<p>However, this turns out not to make much difference to many matching problems, because the following simple transformation turns a problem with self-loops (allowed in matchings in this way) into a problem with no self-loops (so it doesn’t matter whether they are allowed or not). Simply form a <a href="https://en.wikipedia.org/wiki/Covering_graph">double cover</a>\(^*\) of the given graph (let’s call it the “loopless double cover”) by making two copies of the graph and replacing all corresponding pairs of loops by simple edges from one copy to the other. In weighted matching problems, give the replacement edges for the loops the sum of the weights of the two loops they replace; all other edges keep their original weights.</p>

<p style="text-align: center;"><img alt="The loopless double cover of a graph and of one of its loopy matchings" src="https://11011110.github.io/blog/assets/2021/loopless-double-cover.svg"/></p>

<p>Then (unlike the <a href="https://en.wikipedia.org/wiki/Bipartite_double_cover">bipartite double cover</a>, which also eliminates loops) the cardinality or optimal weight of a matching in the loopy graph can be read off from the corresponding solution in its loopless double cover. Any matching of the original loopy graph can be translated into a matching of the loopless cover by applying the same loopless cover translation to the matching instead of to the whole graph; this doubles the total weight of the matching and the total number of matched vertices. And among matchings on the loopless cover, when trying to optimize weight or matched vertices, it is never helpful to match the two copies differently, so there is an optimal solution that can be translated back to the original graph without changing its optimality.</p>

<p>This doesn’t quite work for the problem of finding a matching that maximizes the total number of matched edges, rather than the total number of matched vertices. These two problems are the same in simple graphs, but different in loopy graphs. However, in a loopy graph, if you are trying to maximize matched edges, you might as well include all loops in the matching, and then search for a maximum matching of the simple graph induced by the remaining unmatched vertices. Again, in this case, you don’t get a problem that requires any new algorithms to solve it.</p>

<p>In the case of my student, I only provided the conventional answer, because really all they wanted to know was whether these issues affected how they answered one of the homework questions, and the answer was that the question didn’t involve and didn’t need loops. However it seems that the more-complicated answer is that even if you allow loops to count only one unit towards degree, and to be included in matchings, they don’t change the matching problem much.</p>

<p>\(^*\) This is only actually a covering graph under the convention that the degree of a loop is one. For the usual degree-2 convention for loops, you would need to replace each loop by a pair of parallel edges, forming a multigraph, to preserve the degrees of the vertices.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/105762400402127534">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-02-19T18:57:00Z</updated>
    <published>2021-02-19T18:57:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-02-20T07:30:23Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=21277</id>
    <link href="https://gilkalai.wordpress.com/2021/02/19/nostalgia-corner-john-riordans-referee-report-of-my-first-paper/" rel="alternate" type="text/html"/>
    <title>Nostalgia corner: John Riordan’s referee report of my first paper</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">In 1971/1972 academic year, I was an undergraduate student at the Hebrew University of Jerusalem and toward the end of the year I wrote a paper about Abel’s sums. I sent it to John Riordan the author of the books  … <a href="https://gilkalai.wordpress.com/2021/02/19/nostalgia-corner-john-riordans-referee-report-of-my-first-paper/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In 1971/1972 academic year, I was an undergraduate student at the Hebrew University of Jerusalem and toward the end of the year I wrote a paper about Abel’s sums. I sent it to John Riordan the author of the books  “Combinatorial Identities” and “Combinatorial Analysis”.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan1.png"><img alt="" class="alignnone size-full wp-image-21279" height="834" src="https://gilkalai.files.wordpress.com/2021/02/riordan1.png?w=640&amp;h=834" width="640"/></a></p>
<p>I was surely very happy to read the sentence <span style="color: #0000ff;">“I think you have had a splendid idea”</span>.  Here is part of Riordan’s remarks. The full report is <a href="https://gilkalai.files.wordpress.com/2021/02/riordan-rep1.pdf">here</a>.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan3.png"><img alt="" class="alignnone size-full wp-image-21280" height="827" src="https://gilkalai.files.wordpress.com/2021/02/riordan3.png?w=640&amp;h=827" width="640"/></a></p>
<p>It took me some time to revise the paper and get it printed. And here is the report for the second version.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan2.png"><img alt="" class="alignnone size-full wp-image-21281" height="839" src="https://gilkalai.files.wordpress.com/2021/02/riordan2.png?w=640&amp;h=839" width="640"/></a></p>
<p>And here is part of Riordan’s second round of remarks. The full report is <a href="https://gilkalai.files.wordpress.com/2021/02/riordan-rep2.pdf">here</a>.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan4.png"><img alt="" class="alignnone size-full wp-image-21282" height="843" src="https://gilkalai.files.wordpress.com/2021/02/riordan4.png?w=640&amp;h=843" width="640"/></a></p>
<p>I was certainly happy to read the following sentence: <span style="color: #0000ff;">“I would remark that the result for  <em>p = -1 </em> is new and perhaps <span style="color: #ff0000;">the simplest derivation of Abel’s result</span>.”</span></p>
<p>In 1978 I actually visited John Riordan in his office at Rockefeller University, NYC. I remember him as very cheerful and he told me that when his first book appeared he was working at Bell Labs and his managers wanted to talk to him. He was a bit worried that they would not approve of him spending time and effort to write a book in pure mathematics. But actually, they gave him a salary raise!</p>
<p>(If you have a picture of John Riordan, please send me.)</p>
<p>In 1979 the paper <a href="https://gilkalai.files.wordpress.com/2021/02/1-s2.0-0097316579900475-main-1.pdf">appeared</a>.</p>
<p><a href="https://gilkalai.files.wordpress.com/2021/02/riordan5.png"><img alt="" class="alignnone size-full wp-image-21286" src="https://gilkalai.files.wordpress.com/2021/02/riordan5.png?w=640"/></a></p></div>
    </content>
    <updated>2021-02-19T09:23:53Z</updated>
    <published>2021-02-19T09:23:53Z</published>
    <category term="Combinatorics"/>
    <category term="personal"/>
    <category term="Abel sums"/>
    <category term="John Riordan"/>
    <category term="Niels Henrik Abel"/>
    <category term="refereeing"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2021-02-25T08:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=18125</id>
    <link href="https://rjlipton.wordpress.com/2021/02/18/computings-role-in-the-pandemic/" rel="alternate" type="text/html"/>
    <title>Computing’s Role In The Pandemic</title>
    <summary>How can we help? Joe Biden is the 46th president of the USA. Note is called a centered triangular number. These numbers obey the formula: and start with The previous one, the , was Herbert Hoover, hmmm. Biden has promised to make controlling the Covid-19 pandemic one of his top priorities. Today I thought we […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>How can we help?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wordpress.com/2021/02/18/computings-role-in-the-pandemic/unknown-145/" rel="attachment wp-att-18132"><img alt="" class="alignright size-full wp-image-18132" src="https://rjlipton.files.wordpress.com/2021/02/unknown.jpeg?w=600"/></a></p>
<p>
Joe Biden is the 46th president of the USA. Note <img alt="{46}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B46%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{46}"/> is called a <a href="https://en.wikipedia.org/wiki/Centered_triangular_number">centered triangular number</a>. These numbers obey the formula: 	</p>
<p align="center"><img alt="\displaystyle  \frac{3n^2 + 3n + 2}{2} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B3n%5E2+%2B+3n+%2B+2%7D%7B2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  \frac{3n^2 + 3n + 2}{2} "/></p>
<p>and start with <img alt="{1,4,10,19,31,46,\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2C4%2C10%2C19%2C31%2C46%2C%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{1,4,10,19,31,46,\dots}"/> The previous one, the <img alt="{31^{st}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B31%5E%7Bst%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{31^{st}}"/>, was Herbert Hoover, hmmm. Biden has promised to make controlling the Covid-19 <a href="https://en.wikipedia.org/wiki/COVID-19_pandemic">pandemic</a> one of his top priorities. </p>
<p>
Today I thought we would discuss how he might use computer technology to help get the virus under control.<br/>
<span id="more-18125"/></p>
<p>
First, we thank the drug companies since we now have <a href="https://en.wikipedia.org/wiki/COVID-19_pandemic_in_the_United_States#Vaccines">vaccines</a> that work against the virus. Without these we would have little chance to bring the pandemic under control at all. </p>
<p>
Second, we must state that we are worried that the virus is mutating and this may render the current vaccines less useful, if not useless. We hope this is not happening, or that the drug companies will be able to respond with vaccine boosters. Today there seems to be <a href="https://www.usnews.com/news/health-news/articles/2021-02-18/pfizer-coronavirus-vaccine-protects-against-uk-south-africa-variants-study-shows">good news</a> and <a href="https://www.reuters.com/article/us-health-coronavirus-vaccines-variants/pfizer-says-south-african-variant-could-significantly-reduce-protective-antibodies-idUSKBN2AH2VG">bad news</a>.  </p>
<p>
Results will fluctuate, but in any case, vaccines will definitely play a key role in defeating the pandemic. We want to ask the same about computing technology.</p>
<p>
</p><p/><h2> Computing’s Role—I </h2><p/>
<p/><p>
There are many web sites that discuss how computing technology can play a role in defeating the pandemic. Here are some of the main points:</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\bullet }"/> <i>Tracking People:</i> Many places are interested in tracking who are sick. Tracking can by itself help stop the spreading of the virus, and thus help save lives. For example, <a href="https://www.computer.org/publications/tech-news/five-ways-tech-is-being-used-to-fight-covid-19">IEEE</a> says: </p>
<blockquote><p><b> </b> <em> “We believe software can help combat this global pandemic, and that’s why we’re launching the Code Against COVID-19 initiative…,” said Weiting Liu, founder and CEO of Arc. “From tracking outbreaks and reducing the spread to scaling testing and supporting healthcare, teams around the world are using software to flatten the curve. The eMask app (real-time mask inventory in Taiwan) and TraceTogether (contact tracing in Singapore) are just two of the many examples.” </em>
</p></blockquote>
<p/><p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\bullet }"/> <i>Changing Behavior:</i> A powerful idea is to avoid human to human contact and thus stop the spread of the virus. For example, here are <a href="https://www.weforum.org/agenda/2020/04/10-technology-trends-coronavirus-covid19-pandemic-robotics-telehealth">examples</a> from a longer list of ideas: </p>
<ul>
<li>
Robot Deliveries; <p/>
</li><li>
Digital and Contactless Payments; <p/>
</li><li>
Remote Work and Remote Learning and more.
</li></ul>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\bullet }"/> <i>Changing Health Delivery:</i> An important idea is how can we reduce the risk of health delivery. A paradox is that health care may need to be avoided, since traditional delivery requires human contact. There are many examples of ways to make health care online, and therefore safer. Shwetak Patel won the 2018 ACM Prize in Computing for contributions to creative and practical sensing systems for sustainability and health. He outlined here <a href="https://cccblog.org/2020/09/24/what-role-can-computing-play-in-battling-the-covid-19-pandemic/">CCC blog</a> how health care could be made more online.</p>
<p>
</p><p/><h2> Computing’s Role—II </h2><p/>
<p/><p>
The above ideas are fine but I believe the real role for computing is simple: </p>
<blockquote><p><b> </b> <em> <i>Make signing up and obtaining an appointment for a vaccine easier, fairer, and sooner.</i> </em>
</p></blockquote>
<p/><p>
In the US each state is in charge of running web sites that allow people to try and get an appointment for a vaccine shot. <i>Try</i> is the key word. Almost all sites require an appointment to get a shot—walk-ins are mostly not allowed. </p>
<p>
I cannot speak for all states and all web sites, but my direct experience is that the sites are terrible. Signing up for a vaccination shot is a disaster. The web sites that I have seen are poorly written, clumsy, and difficult to use. They are some of the worst sites I have ever needed to use, for anything. Some of the top issues: </p>
<ol>
<li>
The sites require you to sign in each time from scratch. <p/>
</li><li>
The sites require you to sign in each time from scratch. <p/>
</li><li>
The sites require you to sign in each time from scratch. <p/>
</li><li>
The sites rules are confusing and unclear. <p/>
</li><li>
You may need to search for particular vaccine locations, rather than for any locations. <p/>
</li><li>
And more <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\dots}"/>
</li></ol>
<p>Repeating (1,2,3) is a poor joke, but one that reflects reality. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
If Amazon, Google, Apple had sites that worked this way, they would be out of business quickly. Perhaps this is the key: <i>Can our top companies help build the state sites?</i> Is it too late to help? See <a href="https://www.nytimes.com/2021/01/12/technology/the-problem-with-vaccine-websites.html">here</a> for a New York Times article on this issue: </p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/02/18/computings-role-in-the-pandemic/game/" rel="attachment wp-att-18129"><img alt="" class="aligncenter wp-image-18129" src="https://rjlipton.files.wordpress.com/2021/02/game.png?w=300" width="300"/></a>
</td>
</tr>
<tr>

</tr>
</tbody></table>
<blockquote><p><b> </b> <em> When you start to pull your hair out because you can’t register for a vaccine on a local website, remember that it’s not (only) the fault of a bad tech company or misguided choices by government leaders today. It’s a systematic failure years in the making. </em>
</p></blockquote>
<p/><p>
Also is the issue of <a href="https://medium.com/berkeleyischool/fairness-in-the-age-of-algorithms-feb11c56a709">algorithmic fairness</a> relevant here? We know that it is unfortunately easy to have web sites that are unfair—that assign vaccine sign up dates unfairly, that favor one class of people over another. </p>
<p/></font></font></div>
    </content>
    <updated>2021-02-19T03:03:24Z</updated>
    <published>2021-02-19T03:03:24Z</published>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="19"/>
    <category term="covid"/>
    <category term="states"/>
    <category term="vaccine"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2021-02-25T08:20:48Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5347</id>
    <link href="https://www.scottaaronson.com/blog/?p=5347" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5347#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5347" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Brief thoughts on the Texas catastrophe</title>
    <summary xml:lang="en-US">This past week, I spent so much mental energy worrying about the fate of Scott Alexander that I almost forgot that right here in Texas, I’m surrounded by historic scenes of Third-World-style devastation: snowstorms and sub-freezing temperatures for which our infrastructure was completely unprepared; impassable roads; burst gas and water pipes; millions without electricity or […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>This past week, I spent so much mental energy worrying about the fate of Scott Alexander that I almost forgot that right here in Texas, I’m surrounded by historic scenes of Third-World-style devastation: snowstorms and sub-freezing temperatures for which our infrastructure was completely unprepared; impassable roads; burst gas and water pipes; millions without electricity or heat or clean water; the UT campus a short walk from me converted into a giant refugee camp.</p>



<p>For all those who asked: my family and I are fine.  While many we know were without power for days (or are <em>still</em> without power), we lucked out by living close to a hospital, which means that they can’t shut off the electricity to our block.  We <em>are</em> now on a boil-water notice, like all of Austin, and we can’t take deliveries or easily go anywhere, and the university and schools and daycares are all closed (even for remote learning).  Which means: we’re simply holed up in our house, eating through our stockpiled food, the kids running around being crazy, Dana and I watching them with one eye and our laptops with the other.  Could be worse.</p>



<p>In some sense, it’s not surprising that the Texas infrastructure would buckle under weather stresses outside the envelope of anything it was designed for or saw for decades.  The central problem is that our elected leaders have shown zero indication of understanding the urgent need, for Texas’ economic viability, to do whatever it takes to make sure nothing like this ever happens again.  Ted Cruz, as everyone now knows, left for Cancun; the mayor of Colorado City angrily told everyone to fend for themselves (and then resigned); and Governor Abbott has been blaming frozen wind turbines, a tiny percentage of the problem (frozen gas pipes are a much bigger issue) but one that plays with the base.  The bare minimum of a sane response might be, I dunno,</p>



<ul><li>acknowledging the reality that climate change means that “once-per-century” weather events will be every couple years from now on,</li><li>building spare capacity (nuclear would be ideal … well, I can dream),</li><li>winterizing what we have now, and</li><li>connecting the Texas grid to the rest of the US.</li></ul>



<p>If I were a Texas Democrat, I’d consider making Republican incompetence on infrastructure, utilities, and public health my <em>only</em> campaign issues.</p>



<p>Alright, now back to watching the Mars lander, which is apparently easier to build and deploy than a reliable electric grid.</p></div>
    </content>
    <updated>2021-02-18T21:40:22Z</updated>
    <published>2021-02-18T21:40:22Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Adventures in Meatspace"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Rage Against Doofosity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-02-24T23:14:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4494</id>
    <link href="https://lucatrevisan.wordpress.com/2021/02/18/this-year-for-lent-we-realized-it-has-been-lent-all-along/" rel="alternate" type="text/html"/>
    <title>This year, for Lent, we realized it has been Lent all along</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Yesterday was Ash Wednesday, the beginning of Lent, the 40-day period that precedes Easter and that is observed by Catholics and other Christians as a period of reflection. It is, often, a period in which the faithful choose to give … <a href="https://lucatrevisan.wordpress.com/2021/02/18/this-year-for-lent-we-realized-it-has-been-lent-all-along/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Yesterday was Ash Wednesday, the beginning of Lent, the 40-day period that precedes Easter and that is observed by Catholics and other Christians as a period of reflection. It is, often, a period in which the faithful choose to give something up as a penance, such as giving up eating meat.</p>



<p>The period that immediately precedes Lent is known as Carnival, and, perhaps incongruously, it is a time for having fun, playing pranks, and eating special sweets, often deep-fried ones. Traditionally kids, and also grownups, dress up in costumes and attend costume parties. The idea being, let’s have fun and eat now, because soon we are “entirely voluntarily”  going to fast and to reflect on sin and death, and stuff like that. The day before Ash Wednesday, indeed, is called “Fat Tuesday”.</p>



<p>In Milan, however, the tradition is to power through Ash Wednesday and to continue the Carnival festivities until the following Sunday. There are a number of legends that explain this unique tradition, that is apparently ancient. One such legend is that a plague epidemic had been ravaging Milan in the IV century around the time that should have been Carnival, and life was beginning to go back to normal right around Ash Wednesday. So people rebelled against Lent, and were like, haven’t we suffered enough, what more penance do we need, and celebrated Carnival later.</p>



<p>It has now been nearly a year since the first lockdown, and we still cannot travel between regions (for example, we cannot travel from Milan to Bologna, or to Venice), cannot eat dinner in a restaurant, cannot go see a movie, a play or a sporting event, cannot ski, and so on.</p>



<p>My proposal is that when (if?) we go back to a normal life, we shorten Lent to three days (start with “Ash Thursday” the day before Good Friday), and that we make Carnival  start on Easter Monday and last for 361 days. Not because we have had it worse than a IV century plague epidemic: indeed, even in the best of times, IV century people in Milan did not usually eat in restaurants, travel to Venice, see movies, or ski. We, however, are spoiled XXI century people, we are not used to inconveniences, and when (if?) this is over we will need a lot of self-care, especially the eating-deep-fried-sweets-and-partying kind of self-care.</p></div>
    </content>
    <updated>2021-02-18T13:28:07Z</updated>
    <published>2021-02-18T13:28:07Z</published>
    <category term="Milan"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2021-02-25T08:20:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/02/18/postdoc-at-imperial-college-london-in-complexity-apply-by-march-31-2021-at-imperial-college-london-apply-by-march-31-2021/</id>
    <link href="https://cstheory-jobs.org/2021/02/18/postdoc-at-imperial-college-london-in-complexity-apply-by-march-31-2021-at-imperial-college-london-apply-by-march-31-2021/" rel="alternate" type="text/html"/>
    <title>Postdoc at Imperial College London in Complexity  (apply by March 31, 2021) at Imperial College London (apply by March 31, 2021)</title>
    <summary>The Complexity group of Iddo Tzameret at Imperial College London invites expressions of interest for a postdoctoral position funded by the ERC. The position is for two years with a possible one-year extension. The start date is flexible, and the salary is generous and includes funding for equipment and travel. This position will be based […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Complexity group of Iddo Tzameret at Imperial College London invites expressions of interest for a postdoctoral position funded by the ERC. The position is for two years with a possible one-year extension. The start date is flexible, and the salary is generous and includes funding for equipment and travel. This position will be based at the South Kensington campus at the heart of London.</p>
<p>Website: <a href="https://www.cs.rhul.ac.uk/home/ubac001/PhD_Postdoc_Post.html">https://www.cs.rhul.ac.uk/home/ubac001/PhD_Postdoc_Post.html</a><br/>
Email: iddo.tzameret@gmail.com</p></div>
    </content>
    <updated>2021-02-18T11:52:18Z</updated>
    <published>2021-02-18T11:52:18Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-02-25T08:20:52Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/02/18/phd-positions-at-imperial-college-london-apply-by-march-31-2021-at-imperial-college-london-apply-by-march-31-2021/</id>
    <link href="https://cstheory-jobs.org/2021/02/18/phd-positions-at-imperial-college-london-apply-by-march-31-2021-at-imperial-college-london-apply-by-march-31-2021/" rel="alternate" type="text/html"/>
    <title>PhD positions at Imperial College London (apply by March 31, 2021) at Imperial College London (apply by March 31, 2021)</title>
    <summary>Imperial’s Computing is seeking up to two highly motivated PhD students interested in computational complexity. The positions are based at the South Kensington campus at the heart of London, and include a generous stipend, as well as funding for equipment and travel. The successful candidate will join the complexity group at Imperial College led by […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Imperial’s Computing is seeking up to two highly motivated PhD students interested in computational complexity. The positions are based at the South Kensington campus at the heart of London, and include a generous stipend, as well as funding for equipment and travel. The successful candidate will join the complexity group at Imperial College led by Iddo Tzameret.</p>
<p>Website: <a href="https://www.cs.rhul.ac.uk/home/ubac001/PhD_Postdoc_Post.html">https://www.cs.rhul.ac.uk/home/ubac001/PhD_Postdoc_Post.html</a><br/>
Email: iddo.tzameret@gmail.com</p></div>
    </content>
    <updated>2021-02-18T11:51:55Z</updated>
    <published>2021-02-18T11:51:55Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-02-25T08:20:52Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7992</id>
    <link href="https://windowsontheory.org/2021/02/17/what-do-deep-networks-learn-and-when-do-they-learn-it/" rel="alternate" type="text/html"/>
    <title>What do deep networks learn and when do they learn it</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Scribe notes by Manos Theodosis Previous post: A blitz through statistical learning theory Next post: Unsupervised learning and generative models. See also all seminar posts and course webpage. Lecture video – Slides (pdf) – Slides (powerpoint with ink and animation) In this lecture, we talk about what neural networks end up learning (in terms of … <a class="more-link" href="https://windowsontheory.org/2021/02/17/what-do-deep-networks-learn-and-when-do-they-learn-it/">Continue reading <span class="screen-reader-text">What do deep networks learn and when do they learn it</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>Scribe notes by <a href="https://manosth.github.io/">Manos Theodosis</a></em></p>



<p><strong>Previous post:</strong> <a href="https://windowsontheory.org/2021/01/31/a-blitz-through-classical-statistical-learning-theory/">A blitz through statistical learning theory</a> <strong>Next post:</strong> <a href="https://windowsontheory.org/2021/02/24/unsupervised-learning-and-generative-models/">Unsupervised learning and generative models</a>. See also <a href="https://windowsontheory.org/category/ml-theory-seminar/">all seminar posts</a> and <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">course webpage</a>.</p>



<p><a href="https://harvard.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=c518b9e4-5f63-4278-871d-acc2017b8984">Lecture video</a> – <a href="https://boazbk.github.io/mltheoryseminar/lectures/seminar_lecture2.pdf">Slides (pdf)</a> – <a href="http://files.boazbarak.org/misc/mltheory/ML_seminar_lecture_2.pptx">Slides (powerpoint with ink and animation)</a></p>



<p>In this lecture, we talk about <em>what</em> neural networks end up learning (in terms of their weights) and <em>when</em>, during training, they learn it.</p>



<p>In particular, we’re going to discuss</p>



<ul><li><strong>Simplicity bias</strong>: how networks favor “simple” features first.</li><li><strong>Learning dynamics</strong>: what is learned early in training.</li><li><strong>Different layers</strong>: do the different layers learn the same features?</li></ul>



<p>The type of results we will discuss are:</p>



<ul><li>Gradient-based deep learning algorithms have a bias toward learning simple classifiers. In particular this often holds when the optimization problem they are trying to solve is “underconstrained/overparameterized”, in the sense that there are exponentially many different models that fit the data.</li><li>Simplicity also affects the <em>timing</em> of learning. Deep learning algorithms tend to learn simple (but still predictive!) features first.</li><li>Such “simple predictive features” tend to be in lower (closer to input) levels of the network. Hence deep learning also tends to learn lower levels earlier.</li><li>On the other side, the above means that distributions that do not have “simple predictive features” pose significant challenges for deep learning. Even if there is a small neural network that works very well for the distribution, gradient-based algorithms will not “get off the ground” in such cases. We will see a lower bound for <em>learning parities</em> that makes this intuition formal.</li></ul>



<h2>What do neural networks learn, and when do they learn it?</h2>



<p>As a first example to showcase what is learned by neural networks, we’ll consider the following data distribution where we sample points <img alt="(X, Y)" class="latex" src="https://s0.wp.com/latex.php?latex=%28X%2C+Y%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(X, Y)"/>, with <img alt="Y \in {1, -1}" class="latex" src="https://s0.wp.com/latex.php?latex=Y+%5Cin+%7B1%2C+-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Y \in {1, -1}"/> (<img alt="Y = 1" class="latex" src="https://s0.wp.com/latex.php?latex=Y+%3D+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Y = 1"/> corresponding to orange points and <img alt="Y=-1" class="latex" src="https://s0.wp.com/latex.php?latex=Y%3D-1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Y=-1"/> corresponding to blue points).<br/><img alt="" src="https://i.imgur.com/xiXoqDj.png"/></p>



<p>If we <a href="http://tfmeter.icsi.berkeley.edu/#activation=tanh&amp;batchSize=10&amp;dataset=byod&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;trueLearningRate=0&amp;regularizationRate=0&amp;noise=35&amp;networkShape=10,6,4,2&amp;seed=0.36834921&amp;showTestData=false&amp;discretize=false&amp;percTrainData=74&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false">train</a> a neural network to fit this distribution, we can see below that the neurons that are closest to the input data end up learning features that are highly correlated with the input (mostly linear subspaces at 45-degree angle, which correspond to one of the stripes). In the subsequent layers, the features learned are more sophisticated and have increased complexity.<br/><img alt="" src="https://i.imgur.com/p2FQdUd.png"/><br/></p>



<h3>Neural networks have simpler but useful features in lower layers</h3>



<p>Some people have spent a lot of time trying to understand what is learned by different layers. In a <a href="https://distill.pub/2020/circuits/early-vision/">recent</a> work, Olah et al. dig deep into a particular architecture for computer vision, trying to interpret the features learned by neurons at different layers.</p>



<p>They found that earlier layers learn features that resemble edge detectors.<br/><img alt="" src="https://i.imgur.com/I8veBVf.png"/><br/>However, as we go deeper, the neurons at those layers start learning more convoluted (for example, these features from layer <img alt="3b" class="latex" src="https://s0.wp.com/latex.php?latex=3b&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="3b"/> resemble heads).<br/><img alt="" src="https://i.imgur.com/rJsmKHR.png"/></p>



<h3>SGD learns simple (but still predictive) features earlier.</h3>



<p>There is evidence that <a href="https://arxiv.org/abs/1905.11604">SGD learns simpler classifiers first</a>. The following figure tracks how much of a learned classifier’s performance can be accounted for by a linear classifier. We see that up to a certain point in training <em>all</em> of the performance of the neural network learned by SGD (measured as mutual information with the label or as accuracy) can be ascribed to the linear classifier. They diverge only very near the point where the linear classifier “saturates,” in the sense that the classifier reachers the best possible accuracy for linear models. (We use the quantity <img alt="I(f(x);y |L(x))" class="latex" src="https://s0.wp.com/latex.php?latex=I%28f%28x%29%3By+%7CL%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I(f(x);y |L(x))"/> – the mutual information of <img alt="f(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x)"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y"/> conditioned on the prediction of the linear classifier <img alt="L(x)" class="latex" src="https://s0.wp.com/latex.php?latex=L%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L(x)"/> – to measure how much of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/>‘s performance <em>cannot</em> be accounted for by <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L"/>.)</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/pGxap3e.png"/></figure>



<h3>The benefits and pitfalls of simplicity bias</h3>



<p>In general, simplicity bias is a very good thing. For example, the most “complex” function is a random function. However, if given some observed data <img alt="{ (x_i,y_i)}_{i\in [n]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+%28x_i%2Cy_i%29%7D_%7Bi%5Cin+%5Bn%5D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="{ (x_i,y_i)}_{i\in [n]}"/>, SFD were to find a random function <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> that perfectly fits it, then it would never generalize (since for every fresh <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/>, the value of <img alt="f(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x)"/> would be random).</p>



<p>At the same time, simplicity bias means that our algorithms might focus too much on simple solutions and miss more complex ones. Sometimes the complex solutions actually do perform better. In the following cartoon a person could go to the low-hanging fruit tree on the right-hand side and miss the bigger rewards on the left-hand side.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/eijQfpl.png"/></figure>



<p>This <a href="https://arxiv.org/abs/2006.07710">can actually happen</a> in neural networks. We also saw a simple example in class:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/LvONKsw.png"/></figure>



<p>The two datasets are equally easy to represent, but on the righthand side, there is a very strong “simple classifier” (the 45-degree halfspace) that SGD will “latch onto.” Once it gets stuck with that classifier, it is hard for SGD to get “unstuck.” As a result, SGD has a much harder time learning the righthand dataset than the lefthand dataset.</p>



<h2>Analysing SGD for over-parameterized linear regression</h2>



<p>So, what can we prove about the dynamics of gradient descent? Often we can gain insights by studying <em>linear regression</em>.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/T9VGkXj.png"/></figure>



<p>Formally, given <img alt="(x_i, y_i)_{i=1}^n \in \mathbb{R}^{d+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%28x_i%2C+y_i%29_%7Bi%3D1%7D%5En+%5Cin+%5Cmathbb%7BR%7D%5E%7Bd%2B1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x_i, y_i)_{i=1}^n \in \mathbb{R}^{d+1}"/> with <img alt="d\gg n" class="latex" src="https://s0.wp.com/latex.php?latex=d%5Cgg+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d\gg n"/> we would like to find a vector <img alt="w\in\mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=w%5Cin%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w\in\mathbb{R}^d"/> such that <img alt="\langle w, x_i\rangle\approx y_i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+w%2C+x_i%5Crangle%5Capprox+y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle w, x_i\rangle\approx y_i"/>.</p>



<p>In this setting, we can prove that running SGD (from zero or tiny initialization) on the loss <img alt="\mathcal{L}(w) =\lVert Xw -y \rVert^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28w%29+%3D%5ClVert+Xw+-y+%5CrVert%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathcal{L}(w) =\lVert Xw -y \rVert^2"/> will converge to solution <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w"/> of minimum norm. To see whym note that SGD performs updates of the form<br/><img alt="w_{t+1} = w_t - \eta x_i^T(\langle x_i, w\rangle - y_i)" class="latex" src="https://s0.wp.com/latex.php?latex=w_%7Bt%2B1%7D+%3D+w_t+-+%5Ceta+x_i%5ET%28%5Clangle+x_i%2C+w%5Crangle+-+y_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w_{t+1} = w_t - \eta x_i^T(\langle x_i, w\rangle - y_i)"/>.<br/>However note that <img alt="\eta(\langle x_i, w\rangle - y_i)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta%28%5Clangle+x_i%2C+w%5Crangle+-+y_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta(\langle x_i, w\rangle - y_i)"/> is a scalar. Therefore all of the updates keep the updated vector <img alt="w_{t+1}" class="latex" src="https://s0.wp.com/latex.php?latex=w_%7Bt%2B1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w_{t+1}"/> within <img alt="\mathrm{span}(x_1^T, \ldots, x_n^T)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bspan%7D%28x_1%5ET%2C+%5Cldots%2C+x_n%5ET%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathrm{span}(x_1^T, \ldots, x_n^T)"/>. This implies that the converging solution <img alt="w_{\infty}" class="latex" src="https://s0.wp.com/latex.php?latex=w_%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w_{\infty}"/> will also lie in <img alt="\mathrm{span}(x_1^T, \ldots, x_n^T)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bspan%7D%28x_1%5ET%2C+%5Cldots%2C+x_n%5ET%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathrm{span}(x_1^T, \ldots, x_n^T)"/>.<br/><img alt="" src="https://i.imgur.com/G8tFMrF.png"/><br/>Geometrically this translates into <img alt="w_{\infty}" class="latex" src="https://s0.wp.com/latex.php?latex=w_%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w_{\infty}"/> being the projection of <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y"/> onto the the subspace <img alt="\mathrm{span}(x_1^T, \ldots, x_n^T)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bspan%7D%28x_1%5ET%2C+%5Cldots%2C+x_n%5ET%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathrm{span}(x_1^T, \ldots, x_n^T)"/> which results in the least norm solution.</p>



<p>Analyzing the dynamics of descent, we can write the distance between consecutive weight updates and the converging solution as<br/><img alt="w_{t+1} - w_{\infty} = (I - \eta X^TX)(w_t - w_{\infty})" class="latex" src="https://s0.wp.com/latex.php?latex=w_%7Bt%2B1%7D+-+w_%7B%5Cinfty%7D+%3D+%28I+-+%5Ceta+X%5ETX%29%28w_t+-+w_%7B%5Cinfty%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w_{t+1} - w_{\infty} = (I - \eta X^TX)(w_t - w_{\infty})"/>.<br/>We see that we are applying the linear operator <img alt="(I - \eta X^TX)" class="latex" src="https://s0.wp.com/latex.php?latex=%28I+-+%5Ceta+X%5ETX%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(I - \eta X^TX)"/> at every step we take. As long as this operator is contractive, we will continue to progress and converge to <img alt="w_{\infty}" class="latex" src="https://s0.wp.com/latex.php?latex=w_%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w_{\infty}"/>. Formally, to make progress, we require<br/><img alt="0 \prec I -\eta X^TX\prec 1" class="latex" src="https://s0.wp.com/latex.php?latex=0+%5Cprec+I+-%5Ceta+X%5ETX%5Cprec+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="0 \prec I -\eta X^TX\prec 1"/>.<br/>This directly translates into <img alt="\eta &lt; \frac{1}{\lambda_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta+%3C+%5Cfrac%7B1%7D%7B%5Clambda_1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta &lt; \frac{1}{\lambda_1}"/> and then the progress we make is approximately <img alt="\frac{\lambda_d}{\lambda_1}=\frac{1}{\kappa}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Clambda_d%7D%7B%5Clambda_1%7D%3D%5Cfrac%7B1%7D%7B%5Ckappa%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\frac{\lambda_d}{\lambda_1}=\frac{1}{\kappa}"/>, where <img alt="\kappa" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ckappa&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\kappa"/> is the <em>condition number</em> of <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="X"/>.</p>



<p>What happens now if the matrix <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="X"/> is random? Then, results from random matrix theory (specifically the <a href="https://en.wikipedia.org/wiki/Marchenko%E2%80%93Pastur_distribution">Marchenko-Pastur distribution</a>) state that</p>



<ul><li>if <img alt="d &lt; n" class="latex" src="https://s0.wp.com/latex.php?latex=d+%3C+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d &lt; n"/>, then the matrix <img alt="X^\top X" class="latex" src="https://s0.wp.com/latex.php?latex=X%5E%5Ctop+X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="X^\top X"/> has <img alt="\mathrm{rank}(X^\top X)=d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Brank%7D%28X%5E%5Ctop+X%29%3Dd&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathrm{rank}(X^\top X)=d"/> and the eigenvalues are bounded away from <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="0"/>. This means that the matrix is well conditioned.</li><li>if <img alt="d \approx n" class="latex" src="https://s0.wp.com/latex.php?latex=d+%5Capprox+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d \approx n"/>, then the spectrum of <img alt="X^\top X" class="latex" src="https://s0.wp.com/latex.php?latex=X%5E%5Ctop+X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="X^\top X"/> starts shifting towards <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="0"/>, with some eigenvalues being equal to zero, resulting in an ill-conditioned matrix.</li><li>if <img alt="d &gt; n" class="latex" src="https://s0.wp.com/latex.php?latex=d+%3E+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d &gt; n"/>, then the spectrum has some zero eigenvalues, but is otherwise bounded away from zero. If we restrict to the subspace of positive eigenvalues, we achieve again a good condition number.</li></ul>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/hLZtfz5.png"/></figure>



<p/>



<h2>Deep linear networks</h2>



<p>We now want to go beyond linear regression and talk about deep networks. As deep networks are very hard to understand, we will first start analyzing a depth <img alt="2" class="latex" src="https://s0.wp.com/latex.php?latex=2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="2"/> network. We will also consider a <em>linear</em> network and omit the nonlinearity. This might seem strange, as we could consider the corresponding linear model, which has exactly the same expressiveness. However, note that these two models have a different parameter space. This means that gradient-based algorithms will travel on different paths when optimizing these two models.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/ekh9k8M.png"/></figure>



<p>Specifically, we can see that the minimum loss attained by the two models will coincide, i.e., <img alt="\min \mathcal{L}(A_1, A_2) = \min \mathcal{L}(B)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmin+%5Cmathcal%7BL%7D%28A_1%2C+A_2%29+%3D+%5Cmin+%5Cmathcal%7BL%7D%28B%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\min \mathcal{L}(A_1, A_2) = \min \mathcal{L}(B)"/>, but the SGD path and the solution will be different.</p>



<p>We will analyze the gradient flow on these two networks (which is gradient descent with the learning rate <img alt="\eta \rightarrow 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta+%5Crightarrow+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\eta \rightarrow 0"/>). We will make the simplifying assumption that <img alt="A_1 = A_2" class="latex" src="https://s0.wp.com/latex.php?latex=A_1+%3D+A_2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A_1 = A_2"/> and symmetric. Then, we can see that <img alt="B = A^2 \Rightarrow A = \sqrt{B}" class="latex" src="https://s0.wp.com/latex.php?latex=B+%3D+A%5E2+%5CRightarrow+A+%3D+%5Csqrt%7BB%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="B = A^2 \Rightarrow A = \sqrt{B}"/>. We will try and compare the gradient flow of two different loss functions: <img alt="\mathcal{L}(B)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%28B%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathcal{L}(B)"/> (doing gradient flow on a linear model) and <img alt="\tilde{\mathcal{L}}(A) = \mathcal{L}(A^2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde%7B%5Cmathcal%7BL%7D%7D%28A%29+%3D+%5Cmathcal%7BL%7D%28A%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\tilde{\mathcal{L}}(A) = \mathcal{L}(A^2)"/> (doing gradient flow on a depth linear model).</p>



<p>Gradient flow on the linear model simply gives <img alt="\frac{dB(t)}{dt}=-\nabla\mathcal{L}(B(t))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdB%28t%29%7D%7Bdt%7D%3D-%5Cnabla%5Cmathcal%7BL%7D%28B%28t%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\frac{dB(t)}{dt}=-\nabla\mathcal{L}(B(t))"/>, whereas for the deep linear network we have (using the chain rule)<br/><img alt="\frac{dA(t)}{dt}=-\nabla\tilde{\mathcal{L}}(A(t)) = \nabla\mathcal{L}(A^2)A = A\nabla\mathcal{L}(A^2)," class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdA%28t%29%7D%7Bdt%7D%3D-%5Cnabla%5Ctilde%7B%5Cmathcal%7BL%7D%7D%28A%28t%29%29+%3D+%5Cnabla%5Cmathcal%7BL%7D%28A%5E2%29A+%3D+A%5Cnabla%5Cmathcal%7BL%7D%28A%5E2%29%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\frac{dA(t)}{dt}=-\nabla\tilde{\mathcal{L}}(A(t)) = \nabla\mathcal{L}(A^2)A = A\nabla\mathcal{L}(A^2),"/><br/>since <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="A"/> is symmetric.</p>



<p>For simplicity, let’s denote <img alt="\nabla\mathcal{L}(B) = \nabla\mathcal{L}(A^2) = \nabla" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5Cmathcal%7BL%7D%28B%29+%3D+%5Cnabla%5Cmathcal%7BL%7D%28A%5E2%29+%3D+%5Cnabla&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla\mathcal{L}(B) = \nabla\mathcal{L}(A^2) = \nabla"/> and <img alt="\nabla\tilde{\mathcal{L}}(A) = \tilde{\nabla}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%5Ctilde%7B%5Cmathcal%7BL%7D%7D%28A%29+%3D+%5Ctilde%7B%5Cnabla%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla\tilde{\mathcal{L}}(A) = \tilde{\nabla}"/>. We then have<br/><img alt="\frac{dA^2(t)}{dt}=\frac{dA(t)}{dt}A=-\tilde{\nabla}A = -A \nabla A" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdA%5E2%28t%29%7D%7Bdt%7D%3D%5Cfrac%7BdA%28t%29%7D%7Bdt%7DA%3D-%5Ctilde%7B%5Cnabla%7DA+%3D+-A+%5Cnabla+A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\frac{dA^2(t)}{dt}=\frac{dA(t)}{dt}A=-\tilde{\nabla}A = -A \nabla A"/>.</p>



<p>Another way to view the comparison between the models of interest, <img alt="\frac{dA^2(t)}{dt}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdA%5E2%28t%29%7D%7Bdt%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\frac{dA^2(t)}{dt}"/> and <img alt="\frac{dB(t)}{dt}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdB%28t%29%7D%7Bdt%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\frac{dB(t)}{dt}"/> is as follows: let <img alt="B = A^2" class="latex" src="https://s0.wp.com/latex.php?latex=B+%3D+A%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="B = A^2"/>, then <img alt="\frac{dB(t)}{dt} = -A\nabla A = -\sqrt{B}\nabla\mathcal{L}(B(t))\sqrt{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdB%28t%29%7D%7Bdt%7D+%3D+-A%5Cnabla+A+%3D+-%5Csqrt%7BB%7D%5Cnabla%5Cmathcal%7BL%7D%28B%28t%29%29%5Csqrt%7BB%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\frac{dB(t)}{dt} = -A\nabla A = -\sqrt{B}\nabla\mathcal{L}(B(t))\sqrt{B}"/>.<br/>We can view this as follows: when we multiply the gradient with <img alt="\sqrt{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csqrt%7BB%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sqrt{B}"/> we end up making the “big bigger and the small smaller”. Basically, this accenuates the differences between the eigenvalues and is biasing <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="B"/> to become a low-rank matrix. </p>



<p>To see why, you can think of a low rank matrix has one that has few large eigenvalues and the others small. If <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="B"/> is already close low rank, then replacing a gradient by <img alt="\sqrt{B}\nabla\mathcal{L}(B(t))\sqrt{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csqrt%7BB%7D%5Cnabla%5Cmathcal%7BL%7D%28B%28t%29%29%5Csqrt%7BB%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sqrt{B}\nabla\mathcal{L}(B(t))\sqrt{B}"/> encourages the gradient steps to mostly happen in the top eigenspace of <img alt="B" class="latex" src="https://s0.wp.com/latex.php?latex=B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="B"/>. This result <a href="https://arxiv.org/abs/1910.05505">generalizes</a> to networks of greater depth, and the gradient evolves as <img alt="\frac{dB(t)}{dt} = -\psi_{B(t)}(\nabla\mathcal{L}(B(t)))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7BdB%28t%29%7D%7Bdt%7D+%3D+-%5Cpsi_%7BB%28t%29%7D%28%5Cnabla%5Cmathcal%7BL%7D%28B%28t%29%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\frac{dB(t)}{dt} = -\psi_{B(t)}(\nabla\mathcal{L}(B(t)))"/>, with <img alt="\psi_{B}(\nabla) = \sum B^{\alpha}\nabla B^{1-\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7BB%7D%28%5Cnabla%29+%3D+%5Csum+B%5E%7B%5Calpha%7D%5Cnabla+B%5E%7B1-%5Calpha%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\psi_{B}(\nabla) = \sum B^{\alpha}\nabla B^{1-\alpha}"/>.</p>



<p>This means that we end up doing gradient flow on a <em>Riemannian manifold</em>. An interesting result is that the flow induced by the operator <img alt="\psi_{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi_%7BB%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\psi_{B}"/> is provably not equivalent to a regularized minimization problem <img alt="\min\mathcal{L} + \lambda R(B)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmin%5Cmathcal%7BL%7D+%2B+%5Clambda+R%28B%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\min\mathcal{L} + \lambda R(B)"/> for any <img alt="R(\cdot)" class="latex" src="https://s0.wp.com/latex.php?latex=R%28%5Ccdot%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="R(\cdot)"/>.</p>



<h2>What is learned at different layers?</h2>



<p>Finally, let’s discuss what is learned by the different layers in a neural network. Some intuition people have is that learning proceeds roughly like the following cartoon:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/rteesY5.png"/></figure>



<p>We can think of our data as being “built up” as a sequence of choices from higher level to lower level features. For example, the data is generated by first deciding that it would be a photo of a dog, then that it would be on the beach, and finally low-level details such as the type of fur and light. This is also how a human would describe this photo. In contrast, a neural network builds up the features in the opposite direction. It starts from the simplest (lowest-level) features in the image (edges, textures, etc.) and gradually builds up complexity until it finally classifies the image.</p>



<h2>How neural networks learn features?</h2>



<p>To build a bit of intuition, consider an example of combining different simple features. We can see that if we try to combine two good edge detectors with different orientations, the end result will hardly be an edge detector.<br/><img alt="" src="https://i.imgur.com/CsTB2LV.png"/></p>



<p>So the intuition is that there is competitive/evolutionary pressure on neurons to “specialize” and recognize useful features. Initially, all the neurons are random features, which can be thought of as random linear combination of the various detectors. However, after training, the symmetry will break between the neurons, and they will specialize (in this simple example, they will either become vertical or horizontal edge detectors).</p>



<p><a href="https://arxiv.org/abs/1706.05806">Raghu, Gilmer, Yosinski, and Sohl-Dickstein</a> tracked the speed at which features learned by different layers reach their final learned state. In the figure below the diagonal elements denote the similarity of the current state of a layer to its final one, where lighter color means that the state is more similar. We can see that earlier layer (more to the left) reach their final state earlier (with th exception of the 2 layers closest to the output that also converge very early).</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/qHTIAzz.png"/></figure>



<p>The “symmetry breaking” intuition is explored by a recent work of <a href="https://arxiv.org/abs/1912.05671">Frankle, Dziugaite, Roy, and Carbin</a>. Intuitively, because the average of two good features is generally <em>not</em> a good feature, averaging the weights of two neural networks with small loss will likely result in a network with large loss. That is, if we start from two random initializations <img alt="w_0" class="latex" src="https://s0.wp.com/latex.php?latex=w_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w_0"/>, <img alt="w'_0" class="latex" src="https://s0.wp.com/latex.php?latex=w%27_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w'_0"/> and train two networks until we reach weights <img alt="w_\infty" class="latex" src="https://s0.wp.com/latex.php?latex=w_%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w_\infty"/> and <img alt="w'_\infty" class="latex" src="https://s0.wp.com/latex.php?latex=w%27_%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w'_\infty"/> with small loss, then we expect the average of <img alt="w_\infty" class="latex" src="https://s0.wp.com/latex.php?latex=w_%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w_\infty"/> and <img alt="w'_\infty" class="latex" src="https://s0.wp.com/latex.php?latex=w%27_%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w'_\infty"/> to result in a network with poor loss:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/uLPBPpN.png"/></figure>



<p>In contrast, Frankle et al showed that sometimes, when we start from the same initialization (especially after pruning) and use random SGD noise (obtained by randomly shuffling the training set) then we reach a “linear plateu” of the loss function in which averaging two networks yields a network with similar loss:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/owtRGUL.png"/></figure>



<h2>The contrapositive of simplicity: lower bounds for learning parities</h2>



<p>If we believe that networks learn simple features first, and learn them in the early layers, then this has an interesting consequence. If the data has the form that simple features (e.g. linear or low degree) are completely uninformative (have no correlation with the label) then we may expect that learning cannot “get off the ground”. That is, even if there exists a small neural network that can learn the class, gradient based algorithms such as SGD will never find it. (In fact, it is possible that <em>no</em> efficient algorithm could find it.) There are some settings where we can prove such conjectures. (For gradient-based algorithms that is; proving this for all efficient algorithms would require settling the P vs NP question.)</p>



<p>We discuss one of the canonical “hard” examples for neural networks: parities. Formally, for <img alt="I\subset [d]" class="latex" src="https://s0.wp.com/latex.php?latex=I%5Csubset+%5Bd%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I\subset [d]"/>, the distribution <img alt="D_I" class="latex" src="https://s0.wp.com/latex.php?latex=D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D_I"/> is the distribution over <img alt="(x,y) \in { \pm 1 }^{d+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29+%5Cin+%7B+%5Cpm+1+%7D%5E%7Bd%2B1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x,y) \in { \pm 1 }^{d+1}"/> defined as follows: <img alt="x\sim {\pm 1}^d" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Csim+%7B%5Cpm+1%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x\sim {\pm 1}^d"/> and <img alt="y = \prod_{i\in I}x_i" class="latex" src="https://s0.wp.com/latex.php?latex=y+%3D+%5Cprod_%7Bi%5Cin+I%7Dx_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y = \prod_{i\in I}x_i"/>. The “learning parity” problem is as follows: given <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n"/> samples <img alt="{ (x_i,y_i) }_{i=1..n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+%28x_i%2Cy_i%29+%7D_%7Bi%3D1..n%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="{ (x_i,y_i) }_{i=1..n}"/> drawn from <img alt="D_I" class="latex" src="https://s0.wp.com/latex.php?latex=D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D_I"/>, either recover <img alt="I" class="latex" src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I"/> or do the weaker task of finding a predictor <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f"/> such that <img alt="f(x)=y" class="latex" src="https://s0.wp.com/latex.php?latex=f%28x%29%3Dy&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f(x)=y"/> with high probability over future samples <img alt="(x,y) \sim D_I" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29+%5Csim+D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x,y) \sim D_I"/>.</p>



<p>It turns out that if we don’t restrict ourselves to deep learning, given <img alt="2d" class="latex" src="https://s0.wp.com/latex.php?latex=2d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="2d"/> samples we can recover <img alt="I" class="latex" src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I"/>. Consider the transformations <img alt="Z_{i,j} = (1 - x_{i,j})/2" class="latex" src="https://s0.wp.com/latex.php?latex=Z_%7Bi%2Cj%7D+%3D+%281+-+x_%7Bi%2Cj%7D%29%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="Z_{i,j} = (1 - x_{i,j})/2"/> and <img alt="b_i = (1 - y_i)/2" class="latex" src="https://s0.wp.com/latex.php?latex=b_i+%3D+%281+-+y_i%29%2F2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="b_i = (1 - y_i)/2"/>. If we let <img alt="s_i=1" class="latex" src="https://s0.wp.com/latex.php?latex=s_i%3D1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="s_i=1"/> if <img alt="i\in I" class="latex" src="https://s0.wp.com/latex.php?latex=i%5Cin+I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i\in I"/> and <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="0"/> otherwise, we can write <img alt="\sum_j Z_{i, j}s_j = b_i (\text{mod } 2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_j+Z_%7Bi%2C+j%7Ds_j+%3D+b_i+%28%5Ctext%7Bmod+%7D+2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sum_j Z_{i, j}s_j = b_i (\text{mod } 2)"/>. Basically, we transformed the problem of parity to a problem of counting if we have an odd or an even number of <img alt="-1" class="latex" src="https://s0.wp.com/latex.php?latex=-1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="-1"/>. In this setting, we can think of every sample <img alt="(x,y) \in D_I" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29+%5Cin+D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x,y) \in D_I"/> as providing a <em>linear equation</em> moudlo 2 over the <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="d"/> unknown variables <img alt="s_1,\ldots,s_d" class="latex" src="https://s0.wp.com/latex.php?latex=s_1%2C%5Cldots%2Cs_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="s_1,\ldots,s_d"/>. When <img alt="n&gt;d" class="latex" src="https://s0.wp.com/latex.php?latex=n%3Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="n&gt;d"/>, these linear equations will be very likely to be of full rank, and hence we can use Gaussian elimination to find <img alt="s_1,\ldots,s_d" class="latex" src="https://s0.wp.com/latex.php?latex=s_1%2C%5Cldots%2Cs_d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="s_1,\ldots,s_d"/> and hence <img alt="I" class="latex" src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I"/>.</p>



<p>Switching to the learning setting, we can express parities by using few ReLUs. In particular, we’ve shown that we can create a step function using <img alt="4" class="latex" src="https://s0.wp.com/latex.php?latex=4&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="4"/> ReLUs. Therefore for every <img alt="k \in {0,1,\ldots, d }" class="latex" src="https://s0.wp.com/latex.php?latex=k+%5Cin+%7B0%2C1%2C%5Cldots%2C+d+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="k \in {0,1,\ldots, d }"/>, there is a combination of four ReLUs that computes the function <img alt="f_k:\mathbb{R} :\rightarrow \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=f_k%3A%5Cmathbb%7BR%7D+%3A%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f_k:\mathbb{R} :\rightarrow \mathbb{R}"/> such that <img alt="f_k(s)" class="latex" src="https://s0.wp.com/latex.php?latex=f_k%28s%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f_k(s)"/> outputs <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="1"/> for <img alt="s=k" class="latex" src="https://s0.wp.com/latex.php?latex=s%3Dk&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="s=k"/>, and <img alt="f_k(s)" class="latex" src="https://s0.wp.com/latex.php?latex=f_k%28s%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f_k(s)"/> outputs <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="0"/> if <img alt="|x-k|&gt;0.5" class="latex" src="https://s0.wp.com/latex.php?latex=%7Cx-k%7C%3E0.5&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="|x-k|&gt;0.5"/>. We can then write the parity function (for example for <img alt="I=[d]" class="latex" src="https://s0.wp.com/latex.php?latex=I%3D%5Bd%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I=[d]"/>) as <img alt="\sum_{k \text{ odd } \in [d]} f_k(\sum_{i=1}^d (1-x_i)/2 )" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csum_%7Bk+%5Ctext%7B+odd+%7D+%5Cin+%5Bd%5D%7D+f_k%28%5Csum_%7Bi%3D1%7D%5Ed+%281-x_i%29%2F2+%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sum_{k \text{ odd } \in [d]} f_k(\sum_{i=1}^d (1-x_i)/2 )"/>. This will be a linear combination of at most <img alt="4d" class="latex" src="https://s0.wp.com/latex.php?latex=4d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="4d"/> ReLUs.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/NwQ31Jy.png"/></figure>



<p>Parities are an example of a case where simple feature are uninformative. For example, if <img alt="|I|&gt;1" class="latex" src="https://s0.wp.com/latex.php?latex=%7CI%7C%3E1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="|I|&gt;1"/> then for every linear function <img alt="L:\mathbb{R}^d \rightarrow \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=L%3A%5Cmathbb%7BR%7D%5Ed+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L:\mathbb{R}^d \rightarrow \mathbb{R}"/>,</p>



<p><img alt="\mathbb{E}_{(x,y) \sim D_I}[ L(x)y] = 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7B%28x%2Cy%29+%5Csim+D_I%7D%5B+L%28x%29y%5D+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}_{(x,y) \sim D_I}[ L(x)y] = 0"/></p>



<p>in other words, there is no correlation between the linear function and the label.<br/>To see why this is true, write <img alt="L(x) = \sum L_i x_i" class="latex" src="https://s0.wp.com/latex.php?latex=L%28x%29+%3D+%5Csum+L_i+x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="L(x) = \sum L_i x_i"/>. By linearity of expectation, it suffices to show that $latex \mathbb{E}<em>{(x,y) \sim D_I}[ L_ix_i y] = L_i \mathbb{E}</em>{(x,y) \sim D_I}[ x_i y] = 0&amp;bg=ffffff$. Both <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_i"/> and <img alt="y_i" class="latex" src="https://s0.wp.com/latex.php?latex=y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y_i"/> are just values in <img alt="{\pm 1 }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="{\pm 1 }"/>. To evaluate the expectation <img alt="\mathbb{E}[x_i y]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Bx_i+y%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}[x_i y]"/> we simply need to know the marginal distribution that <img alt="D_I" class="latex" src="https://s0.wp.com/latex.php?latex=D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D_I"/> induces on <img alt="{ \pm 1 }^2" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+%5Cpm+1+%7D%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="{ \pm 1 }^2"/> when we restrict it to these two coordinates. This distribution is just the uniform distribution. To see why this is the case, consider a coordinate <img alt="j\in I \setminus { i }" class="latex" src="https://s0.wp.com/latex.php?latex=j%5Cin+I+%5Csetminus+%7B+i+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="j\in I \setminus { i }"/> and let’s condition on the values of all coordinates other than <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i"/> and <img alt="j" class="latex" src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="j"/>. After conditioning on these values, <img alt="y = \sigma x_i x_j" class="latex" src="https://s0.wp.com/latex.php?latex=y+%3D+%5Csigma+x_i+x_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y = \sigma x_i x_j"/> for some <img alt="\sigma \in { \pm 1 }" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma+%5Cin+%7B+%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\sigma \in { \pm 1 }"/> and <img alt="x_i,x_j" class="latex" src="https://s0.wp.com/latex.php?latex=x_i%2Cx_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_i,x_j"/> are chosen uniformly and independently from <img alt="{ \pm 1 }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="{ \pm 1 }"/>. For every choice of <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_i"/>, if we flip <img alt="x_j" class="latex" src="https://s0.wp.com/latex.php?latex=x_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_j"/> then that would flip the value of <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y"/>, and hence the marginal distribution on <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_i"/> and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="y"/> will be uniform.</p>



<p>This lack of correlation turns out to be a real obstacle for gradient-based algorithms. While small neural networks for parities exist, and Gaussian elimination can find them, it turns out that gradient-based algorithms such as SGD will <em>fail</em> to do so. Parities are hard to learn, and even if the capacity of the network is such that it can memorize the input, it will still perform poorly in a test set. Indeed, we can prove that for <em>every</em> neural network architecture <img alt="f_w(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f_w%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f_w(x)"/>, running SGD on <img alt="\min\lVert f_w(x) -\prod_{i\in I} x_i\rVert^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmin%5ClVert+f_w%28x%29+-%5Cprod_%7Bi%5Cin+I%7D+x_i%5CrVert%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\min\lVert f_w(x) -\prod_{i\in I} x_i\rVert^2"/> will require <img alt="e^{\Omega(d)}" class="latex" src="https://s0.wp.com/latex.php?latex=e%5E%7B%5COmega%28d%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="e^{\Omega(d)}"/> steps. (Note that if we add <em>noise</em> to parities, then Gaussian elimination will fail and it is believed that <em>no efficient algorithm</em> can learn the distribution in this case. This is known as the <a href="https://wiki.epfl.ch/edicpublic/documents/Candidacy%20exam/Cryptography_from_learning_parity_with_noise.pdf">learning parity with noise</a> problem, which is also related to the <a href="https://en.wikipedia.org/wiki/Learning_with_errors">learning with errors</a> problem that is the foundation of modern lattice-based cryptography.)</p>



<p>We now sketch the proof that gradient-based algorithms require exponentially many steps to learn parities, following Theorem 1 of <a href="https://arxiv.org/abs/1703.07950">Shalev-Shwartz,Shamir and Shammah</a>. We think of an idealized setting where we have an unlimited number of samples and only use a sample <img alt="(x,y) \sim D_I" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29+%5Csim+D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x,y) \sim D_I"/> only once (this should only make learning easier). We will show that we make very little progress in learning <img alt="D_I" class="latex" src="https://s0.wp.com/latex.php?latex=D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D_I"/>, by showing that for any given <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w"/>, the expected gradient over <img alt="(x,y)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2Cy%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(x,y)"/> will be exponentially small, and hence we make very little progress toward learning <img alt="I" class="latex" src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I"/>. Specifically, using the notation <img alt="\chi_I(x)=\prod_{i\in I}x_i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi_I%28x%29%3D%5Cprod_%7Bi%5Cin+I%7Dx_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\chi_I(x)=\prod_{i\in I}x_i"/>, for any <img alt="w,x,I" class="latex" src="https://s0.wp.com/latex.php?latex=w%2Cx%2CI&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w,x,I"/>,</p>



<p><img alt="\nabla_w \parallel f_w(x) - \chi_I(x) \parallel^2 = 2\sum_{i=1}^d \left [ f_w(x) \tfrac{d}{d x_i}f_{w}(x)- \chi_I(x)\tfrac{d}{d x_i}f_{w}(x) \right]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla_w+%5Cparallel+f_w%28x%29+-+%5Cchi_I%28x%29+%5Cparallel%5E2+%3D+2%5Csum_%7Bi%3D1%7D%5Ed+%5Cleft+%5B+f_w%28x%29+%5Ctfrac%7Bd%7D%7Bd+x_i%7Df_%7Bw%7D%28x%29-+%5Cchi_I%28x%29%5Ctfrac%7Bd%7D%7Bd+x_i%7Df_%7Bw%7D%28x%29+%5Cright%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\nabla_w \parallel f_w(x) - \chi_I(x) \parallel^2 = 2\sum_{i=1}^d \left [ f_w(x) \tfrac{d}{d x_i}f_{w}(x)- \chi_I(x)\tfrac{d}{d x_i}f_{w}(x) \right]"/></p>



<p>The term <img alt="f_w(x) \tfrac{d}{d x_i}f_{w}(x)" class="latex" src="https://s0.wp.com/latex.php?latex=f_w%28x%29+%5Ctfrac%7Bd%7D%7Bd+x_i%7Df_%7Bw%7D%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="f_w(x) \tfrac{d}{d x_i}f_{w}(x)"/> is independent of <img alt="I" class="latex" src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I"/> and so does not contribute toward learning <img alt="D_I" class="latex" src="https://s0.wp.com/latex.php?latex=D_I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="D_I"/>. Hence intuitively to show we make exponentially small progress, it suffices to show that typically for every <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i"/>, <img alt="\left(\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x) ] \right)^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%28%5Cmathbb%7BE%7D_x%5B+%5Cchi_I%28x%29%5Ctfrac%7Bd%7D%7Bd+x_i%7Df%7Bw%7D%28x%29+%5D+%5Cright%29%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\left(\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x) ] \right)^2"/> will be exponentially small. (That is, even if for a fixed <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> we make a large step, these all cancel out and give us exponentially small progress toward actually learning <img alt="I" class="latex" src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I"/>.)</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/lWCuan2.png"/></figure>



<p>Formally, we will prove the following lemma:</p>



<p><strong>Lemma:</strong> For every <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="w"/>, <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i"/></p>



<p><img alt="\mathbb{E}_I \left(\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x) ] \right)^2 \leq \tfrac{poly(d,n)\mathbb{E}_x \frac{d}{d x_i}f{w}(x)^2}{2^d}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_I+%5Cleft%28%5Cmathbb%7BE%7D_x%5B+%5Cchi_I%28x%29%5Ctfrac%7Bd%7D%7Bd+x_i%7Df%7Bw%7D%28x%29+%5D+%5Cright%29%5E2+%5Cleq+%5Ctfrac%7Bpoly%28d%2Cn%29%5Cmathbb%7BE%7D_x+%5Cfrac%7Bd%7D%7Bd+x_i%7Df%7Bw%7D%28x%29%5E2%7D%7B2%5Ed%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}_I \left(\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x) ] \right)^2 \leq \tfrac{poly(d,n)\mathbb{E}_x \frac{d}{d x_i}f{w}(x)^2}{2^d}"/></p>



<p><strong>Proof:</strong> Let us fix <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i"/> and define <img alt="g(x) = \tfrac{d}{d x_i}f_{w}(x)" class="latex" src="https://s0.wp.com/latex.php?latex=g%28x%29+%3D+%5Ctfrac%7Bd%7D%7Bd+x_i%7Df_%7Bw%7D%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="g(x) = \tfrac{d}{d x_i}f_{w}(x)"/>. The quantity <img alt="\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x)]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_x%5B+%5Cchi_I%28x%29%5Ctfrac%7Bd%7D%7Bd+x_i%7Df%7Bw%7D%28x%29%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}_x[ \chi_I(x)\tfrac{d}{d x_i}f{w}(x)]"/> can be written as <img alt="\langle \chi_I,g \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cchi_I%2Cg+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle \chi_I,g \rangle"/> with respect to the inner product <img alt="\langle f,g \rangle = \mathbb{E}_x f(x)g(x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+f%2Cg+%5Crangle+%3D+%5Cmathbb%7BE%7D_x+f%28x%29g%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle f,g \rangle = \mathbb{E}_x f(x)g(x)"/>. However, <img alt="{ \chi_I }{I\subseteq [d]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+%5Cchi_I+%7D%7BI%5Csubseteq+%5Bd%5D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="{ \chi_I }{I\subseteq [d]}"/> is an orhtonormal basis with respect to this inner product. To see this note that since <img alt="\chi_I(x) \in { \pm 1 }" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi_I%28x%29+%5Cin+%7B+%5Cpm+1+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\chi_I(x) \in { \pm 1 }"/>, <img alt="\mathbb{E}_x \chi_I(x)^2 = 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_x+%5Cchi_I%28x%29%5E2+%3D+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}_x \chi_I(x)^2 = 1"/> for every <img alt="I" class="latex" src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I"/>, and for <img alt="I \neq J" class="latex" src="https://s0.wp.com/latex.php?latex=I+%5Cneq+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I \neq J"/>, <img alt="\chi_I(x)\chi_J(x) = (\prod{i \in I} x_i)(\prod_{j \in J} x_j) = \prod_{k \in I \oplus H} x_k" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cchi_I%28x%29%5Cchi_J%28x%29+%3D+%28%5Cprod%7Bi+%5Cin+I%7D+x_i%29%28%5Cprod_%7Bj+%5Cin+J%7D+x_j%29+%3D+%5Cprod_%7Bk+%5Cin+I+%5Coplus+H%7D+x_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\chi_I(x)\chi_J(x) = (\prod{i \in I} x_i)(\prod_{j \in J} x_j) = \prod_{k \in I \oplus H} x_k"/> where <img alt="I\oplus J" class="latex" src="https://s0.wp.com/latex.php?latex=I%5Coplus+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I\oplus J"/> is the symmetric difference of <img alt="I" class="latex" src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I"/> and <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J"/>. The reason is that <img alt="x_i^2 =1" class="latex" src="https://s0.wp.com/latex.php?latex=x_i%5E2+%3D1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_i^2 =1"/> for all <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="i"/> and so elements that appear in both <img alt="I" class="latex" src="https://s0.wp.com/latex.php?latex=I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I"/> and <img alt="J" class="latex" src="https://s0.wp.com/latex.php?latex=J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="J"/> “cancel out”. Since the coordinates of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x"/> are distributed independently and uniformly, the expectation of the product is the product of expectations. This means that as long as <img alt="I \oplus J" class="latex" src="https://s0.wp.com/latex.php?latex=I+%5Coplus+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I \oplus J"/> is not empty (i.e., <img alt="I \neq J" class="latex" src="https://s0.wp.com/latex.php?latex=I+%5Cneq+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I \neq J"/>) this will be a product of one or more terms of the form <img alt="(\mathbb{E} x_k)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Cmathbb%7BE%7D+x_k%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="(\mathbb{E} x_k)"/>. Since <img alt="x_k" class="latex" src="https://s0.wp.com/latex.php?latex=x_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="x_k"/> is uniform over <img alt="\{ \pm 1 \}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B+%5Cpm+1+%5C%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\{ \pm 1 \}"/>, <img alt="\mathbb{E} x_k = 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D+x_k+%3D+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E} x_k = 0"/> and so we get that if <img alt="I \neq J" class="latex" src="https://s0.wp.com/latex.php?latex=I+%5Cneq+J&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="I \neq J"/>, <img alt="\langle \chi_I,\chi_J \rangle =0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+%5Cchi_I%2C%5Cchi_J+%5Crangle+%3D0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle \chi_I,\chi_J \rangle =0"/>.</p>



<p>Given the above</p>



<p><img alt="\mathbb{E}_x g(x)^2 = \langle g,g\rangle = \sum_{I \subseteq [d]} \langle g , \chi_I \rangle^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_x+g%28x%29%5E2+%3D+%5Clangle+g%2Cg%5Crangle+%3D+%5Csum_%7BI+%5Csubseteq+%5Bd%5D%7D+%5Clangle+g+%2C+%5Cchi_I+%5Crangle%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\mathbb{E}_x g(x)^2 = \langle g,g\rangle = \sum_{I \subseteq [d]} \langle g , \chi_I \rangle^2"/></p>



<p>which means that (since there are <img alt="2^d" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="2^d"/> subsets of <img alt="[d]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bd%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="[d]"/>) on average <img alt="\langle g, \chi_I \rangle = \parallel g \parallel^2 / 2^d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+g%2C+%5Cchi_I+%5Crangle+%3D+%5Cparallel+g+%5Cparallel%5E2+%2F+2%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle g, \chi_I \rangle = \parallel g \parallel^2 / 2^d"/>. In other words, <img alt="\langle g,\chi_I \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+g%2C%5Cchi_I+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002" title="\langle g,\chi_I \rangle"/> is typically exponentially small which is what we wanted to prove.</p></div>
    </content>
    <updated>2021-02-17T21:30:14Z</updated>
    <published>2021-02-17T21:30:14Z</published>
    <category term="ML Theory seminar"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-02-25T08:21:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=18134</id>
    <link href="https://rjlipton.wordpress.com/2021/02/17/alan-selman/" rel="alternate" type="text/html"/>
    <title>Alan Selman</title>
    <summary>A special journal issue in his honor Elvira Mayordomo, Mitsu Ogihara, and Atri Rudra are going to be the editors of a special issue of the journal Theory of Computing Systems dedicated to Alan Selman. Alan passed away this January 2021. Today we circulate their request for contributions. The details of the call say: This […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>A special journal issue in his honor</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2021/02/17/alan-selman/all-9/" rel="attachment wp-att-18136"><img alt="" class="alignright  wp-image-18136" src="https://rjlipton.files.wordpress.com/2021/02/all.png?w=200" width="200"/></a></p>
<p>
Elvira Mayordomo, Mitsu Ogihara, and Atri Rudra are going to be the editors of a special issue of the journal <a href="https://www.springer.com/journal/224">Theory of Computing Systems</a> dedicated to Alan Selman. Alan passed away this January 2021. </p>
<p>
Today we circulate their request for contributions.</p>
<p><span id="more-18134"/></p>
<p>
The details of the <a href="https://www.springer.com/journal/224/updates/18863610">call</a> say:  This special issue celebrates Alan’s life and commemorate his extraordinary contributions to the field. The topics of interest include but are not limited to: </p>
<ul>
<li>
average-case complexity <p/>
</li><li>
circuit complexity <p/>
</li><li>
comparison of reducibilities <p/>
</li><li>
complexity theoretic characterizations of models <p/>
</li><li>
function complexity <p/>
</li><li>
hierarchy theorems <p/>
</li><li>
	parameterized complexity <p/>
</li><li>
promise problems and disjoint NP-pairs <p/>
</li><li>
public-key cryptography <p/>
</li><li>
relativization <p/>
</li><li>
semi-feasible algorithms <p/>
</li><li>
sparse sets <p/>
</li><li>
structure of complete sets
</li></ul>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p>Please look at <a href="https://www.springer.com/journal/224/updates/18863610">this</a> for details—the deadline for submission is 31st July 2021. You have 164 days to write your paper. Which is 3936 hours or 236160 minutes.</p>
<p>
Please send a contribution. </p>
<p/></font></font></div>
    </content>
    <updated>2021-02-17T17:07:25Z</updated>
    <published>2021-02-17T17:07:25Z</published>
    <category term="News"/>
    <category term="People"/>
    <category term="Alan Selman"/>
    <category term="special issue"/>
    <category term="TCD"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2021-02-25T08:20:48Z</updated>
    </source>
  </entry>
</feed>
