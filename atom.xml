<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-11-11T18:39:06Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/11/tenure-track-faculty-at-stanford-university-apply-by-november-15-2021/</id>
    <link href="https://cstheory-jobs.org/2021/11/11/tenure-track-faculty-at-stanford-university-apply-by-november-15-2021/" rel="alternate" type="text/html"/>
    <title>Tenure-track Faculty at Stanford University (apply by November 15, 2021)</title>
    <summary>Stanford Data Science and the School of Engineering invite applications for a tenure-track appointment at the assistant professor or untenured associate professor level. We welcome candidates engaged in all aspects of data science and applications in one or more engineering disciplines. Application review begins Nov 15, 2021; search will remain open until the position is […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Stanford Data Science and the School of Engineering invite applications for a tenure-track appointment at the assistant professor or untenured associate professor level. We welcome candidates engaged in all aspects of data science and applications in one or more engineering disciplines.</p>
<p>Application review begins Nov 15, 2021; search will remain open until the position is filled.</p>
<p>Website: <a href="https://datascience.stanford.edu/tenure-track-faculty-position-soe">https://datascience.stanford.edu/tenure-track-faculty-position-soe</a><br/>
Email: SDSSoEfacultysearch@stanford.edu</p></div>
    </content>
    <updated>2021-11-11T17:15:22Z</updated>
    <published>2021-11-11T17:15:22Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-11T18:37:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/11/postdoc-at-northwestern-and-ttic-apply-by-january-1-2022/</id>
    <link href="https://cstheory-jobs.org/2021/11/11/postdoc-at-northwestern-and-ttic-apply-by-january-1-2022/" rel="alternate" type="text/html"/>
    <title>Postdoc at Northwestern and TTIC (apply by January 1, 2022)</title>
    <summary>Northwestern University and Toyota Technology Institute at Chicago invite applications for a postdoctoral position. The postdoc will conduct research in approximation algorithms and beyond-worst-case analysis of algorithms, working with Konstantin Makarychev and Yury Makarychev. The position will be based at Northwestern University, but the postdoc will be encouraged to spend some time at TTIC. Website: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Northwestern University and Toyota Technology Institute at Chicago invite applications for a postdoctoral position. The postdoc will conduct research in approximation algorithms and beyond-worst-case analysis of algorithms, working with Konstantin Makarychev and Yury Makarychev. The position will be based at Northwestern University, but the postdoc will be encouraged to spend some time at TTIC.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/20466">https://academicjobsonline.org/ajo/jobs/20466</a><br/>
Email: yury@ttic.edu</p></div>
    </content>
    <updated>2021-11-11T15:12:06Z</updated>
    <published>2021-11-11T15:12:06Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-11T18:37:47Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-2727537320196855723</id>
    <link href="http://blog.computationalcomplexity.org/feeds/2727537320196855723/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/11/20-years-of-algorithmic-game-theory.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/2727537320196855723" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/2727537320196855723" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/11/20-years-of-algorithmic-game-theory.html" rel="alternate" type="text/html"/>
    <title>20 Years of Algorithmic Game Theory</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Twenty years ago DIMACS hosted a <a href="http://dimacs.rutgers.edu/archive/Workshops/gametheory/program.html">Workshop on Computational Issues in Game Theory and Mechanism Design</a>. This wasn't the very beginning of algorithmic game theory, but it was quite the coming out party. From the <a href="http://dimacs.rutgers.edu/archive/Workshops/gametheory/announcement.html">announcement</a></p><p/><blockquote><p>The research agenda of computer science is undergoing significant changes due to the influence of the Internet. Together with the emergence of a host of new computational issues in mathematical economics, as well as electronic commerce, a new research agenda appears to be emerging. This area of research is collectively labeled under various titles, such as "Foundations of Electronic Commerce", Computational Economics", or "Economic Mechanisms in Computation" and deals with various issues involving the interplay between computation, game-theory and economics.</p><p>This workshop is intended to not only summarize progress in this area and attempt to define future directions for it, but also to help the interested but uninitiated, of which there seem many, understand the language, the basis principles and the major issues.</p><p/></blockquote><p>Working at the nearby NEC Research Institute at the time I attended as one of those "interested but unititated."</p><p>The workshop had talks from the current and rising stars in the field in both the theoretical computer science, AI and economics communities. The presentations included some classic early results including <a href="https://doi.org/10.1016/S0304-3975(03)00391-8">Competitive Analysis of Incentive Compatible Online Auctions</a>, <a href="https://doi.org/10.1145/506147.506153">How Bad is Selfish Routing?</a> and the seminal work on <a href="https://doi.org/10.1016/j.geb.2006.02.003">Competitive Auctions</a>. </p><p>Beyond the talks, just having the powerhouse of people at the meeting, established players, like Noam Nisan, Vijay Vazirani, Eva Tardos and Christos Papadimitriou, with several newcomers who are now the established players including Tim Roughgarden and Jason Hartline just to mention a few from theoretical computer science. </p><p>The highlight was a panel discussion on how to overcome the methodological differences between computer scientists and economic game theorists. The panelists were an all-star collection of  John Nash, Andrew Odlyzko, Christos Papadimitriou, Mark Satterthwaite, Scott Shenker and Michael Wellman. The discussion focused on things like competitive analysis though to me, in hindsight, the real difference is between the focus on models (game theory) vs theorems (CS). </p><div>Interest in these connections exploded after the workshop and a new field blossomed.</div></div>
    </content>
    <updated>2021-11-11T12:57:00Z</updated>
    <published>2021-11-11T12:57:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-11-11T15:44:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4588</id>
    <link href="https://lucatrevisan.wordpress.com/2021/11/10/online-optimization-post-7-matrix-multiplicative-weights-update/" rel="alternate" type="text/html"/>
    <title>Online Optimization Post 7: Matrix Multiplicative Weights Update</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This is the seventh in a series of posts on online optimization, where we alternate one post explaining a result from the theory of online convex optimization and one post explaining an “application” in computational complexity or combinatorics. The first … <a href="https://lucatrevisan.wordpress.com/2021/11/10/online-optimization-post-7-matrix-multiplicative-weights-update/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This is the seventh in a series of posts on online optimization, where we alternate one post explaining a result from the theory of online convex optimization and one post explaining an “application” in computational complexity or combinatorics. The first two posts were about the technique of <a href="https://lucatrevisan.wordpress.com/2019/04/24/online-optimization-post-1-multiplicative-weights/">Multiplicative Weights Updates</a> and its application to <a href="https://lucatrevisan.wordpress.com/2019/04/25/online-optimization-post-2-constructing-pseudorandom-sets/">“derandomizing” probabilistic arguments</a> based on combining a Chernoff bound and a union bound. The third and fourth post were about the <a href="https://lucatrevisan.wordpress.com/2019/05/06/online-optimization-post-3-follow-the-regularized-leader/">Follow-the-Regularized-Leader</a> framework, which unifies multiplicative weights and gradient descent, and a <a href="https://lucatrevisan.wordpress.com/2019/05/16/online-optimization-post-4-regularity-lemmas/">“gradient descent view” of the Frieze-Kannan Weak Regularity Lemma</a>. The fifth and sixth post were about the <a href="https://lucatrevisan.wordpress.com/2019/05/20/online-optimization-post-5-bregman-projections-and-mirror-descent/">constrained version of the Follow-the-Regularized-Leader</a> framework, and the <a href="https://lucatrevisan.wordpress.com/2021/10/20/online-optimization-post-6-the-impagliazzo-hard-core-set-lemma/">Impagliazzo Hard-Core Set Lemma</a>. Today we shall see the technique of Matrix Multiplicative Weights Updates.</p>
<p><b>1. Matrix Multiplicative Weights Update </b></p>
<p>In this post we consider the following generalization, introduced and studied by <a href="https://dl.acm.org/doi/10.1145/1250790.1250823">Arora and Kale</a>, of the “learning from expert advice” setting and the multiplicative weights update method. In the “experts” model, we have a repeated game in which, at each time step <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, we have the option of following the advice of one of <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> experts; if we follow the advice of expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, we incur a loss of <img alt="{\ell_t (i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t+%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which is unknown to us (although, at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> we know the loss functions <img alt="{\ell_1(\cdot),\ldots,\ell_{t-1}(\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_1%28%5Ccdot%29%2C%5Cldots%2C%5Cell_%7Bt-1%7D%28%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>). We are allowed to choose a probabilistic strategy, whereby we follow the advice of expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> with probability <img alt="{x_t(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, so that our expected loss at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is <img alt="{\sum_{i=1}^n x_t(i) \ell_t(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Cell_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>In the matrix version, instead of choosing an expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> we are allowed to choose a unit <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-dimensional vector <img alt="{v_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and the loss incurred in choosing the vector <img alt="{v_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is <img alt="{v_t ^T L_t v_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_t+%5ET+L_t+v_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, where <img alt="{L_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is an unknown symmetric <img alt="{n\times n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> matrix. We are also allowed to choose a probabilistic strategy, so that with probability <img alt="{x_t(j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%28j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> we choose the unit vector <img alt="{v_t^{(j)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_t%5E%7B%28j%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and we incur the expected loss</p>
<p align="center"><img alt="\displaystyle  \sum_j x_t (j) \cdot (v_t^{(j)})^T L_t v_t^{(j)} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_j+x_t+%28j%29+%5Ccdot+%28v_t%5E%7B%28j%29%7D%29%5ET+L_t+v_t%5E%7B%28j%29%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p><span id="more-4588"/></p>
<p>The above expression can also be written as</p>
<p align="center"><img alt="\displaystyle  X_t \bullet L_t " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X_t+%5Cbullet+L_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> where <img alt="{X_t = \sum_j x_t(j) v_t^{(j)}(v_t^{(j)})^T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_t+%3D+%5Csum_j+x_t%28j%29+v_t%5E%7B%28j%29%7D%28v_t%5E%7B%28j%29%7D%29%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and we used the Frobenius inner product among square matrices defined as <img alt="{A \bullet B = \sum_{i,j} A_{i,j} B_{i,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+%5Cbullet+B+%3D+%5Csum_%7Bi%2Cj%7D+A_%7Bi%2Cj%7D+B_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The matrices <img alt="{X_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that can be obtained as convex combinations of rank-1 matrices of the form <img alt="{vv^T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bvv%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> where <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a unit vector are called <em>density matrices</em> and can be characterized as the set of positive semidefinite matrices whose trace is 1.</p>
<p>It is possible to see the above game as the “quantum version” of the experts settings. A choice of a unit vector <img alt="{v_t^{(j)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_t%5E%7B%28j%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a <em>pure quantum state</em>, a probability distribution of pure quantum states, described by a density matrix, is a <em>mixed quantum state</em>. If <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a density matrix describing a mixed quantum state, <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a symmetric matrix, and <img alt="{L = \sum_i \lambda_i w_i w_i^T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL+%3D+%5Csum_i+%5Clambda_i+w_i+w_i%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is the spectral decomposition of <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in terms of its eigenvalues <img alt="{\lambda_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and orthonormal eigenvectors <img alt="{w_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, then <img alt="{X \bullet L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX+%5Cbullet+L%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is the expected outcome of a measurement of <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in the basis <img alt="{w_1,\ldots,w_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_1%2C%5Cldots%2Cw_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and such that <img alt="{\lambda_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is the value of the measurement if the outcome is <img alt="{w_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>If you have no idea what the above paragraph means, that is perfectly ok because this view will not be particularly helpful in motivating the algorithm and analysis that we will describe. (Here I am reminded of the joke about the way people from Naples give directions: “How do I get to the post office?”, “Well, you see that road over there? After the a couple of blocks there is a pharmacy, where my uncle used to work, though now he is retired.” “Ok?” “Now, if you turn left after the pharmacy, after a while you get to a square with a big fountain and the church of St. Anthony where my niece got married. It was a beautiful ceremony, but the food at the reception was not great.” “Yes, I know that square”, “Good, don’t go there, the post office is not that way. Now, if you instead take that other road over there …”)</p>
<p>The main point of the above game, and of the Matrix Multiplicative Weights Update (MMWU) algorithm that plays it with bounded regret, is that it provides useful generalizations of the standard “experts” game and of the Multiplicative Weights Update (MWU) algorithm. For example, as we have already seen, MWU can provide a “derandomization” of the Chernoff bound; we will see that MMWU provides a derandomization of the <em>matrix</em> Chernoff bound. MWU can be used to approximate certain Linear Programming problems; MMWU can be used to approximate certain <em>Semidefinite Programming</em> problems.</p>
<p>To define and analyze the MMWU algorithm, we need to introduce certain operations on matrices. We will always work with real-valued symmetric matrices, but everything generalizes to complex-valued Hermitian matrices. If <img alt="{M = \sum_i \lambda_i w_i w_i^T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM+%3D+%5Csum_i+%5Clambda_i+w_i+w_i%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a symmetric matrix, <img alt="{\lambda_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are the eigenvalues of <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and <img alt="{w_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are corresponding orthonormal eigenvectors, then we will define a number of operations and functions on <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that operate on the eigenvalues while leaving the eigenvectors unchanged.</p>
<p>The first operation is <em>matrix exponentiation</em>: we define</p>
<p align="center"><img alt="\displaystyle  e^X := \sum_i e^{\lambda_i} w_i w_i^T " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5EX+%3A%3D+%5Csum_i+e%5E%7B%5Clambda_i%7D+w_i+w_i%5ET+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> The operation always defines a positive definite matrix, and the resulting matrix satisfies a “Taylor expansion”</p>
<p align="center"><img alt="\displaystyle  e^X = \sum_{k=0}^\infty \frac1 {k!} X^k " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5EX+%3D+%5Csum_%7Bk%3D0%7D%5E%5Cinfty+%5Cfrac1+%7Bk%21%7D+X%5Ek+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> Indeed, it is more common to use the above expansion as the definition of the matrix exponential, and then derive the expression in terms of eigenvalues.</p>
<p>We also have the useful bounds</p>
<p align="center"><img alt="\displaystyle  e^X \succeq I + X" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5EX+%5Csucceq+I+%2B+X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> which is true for every <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and</p>
<p align="center"><img alt="\displaystyle  e^X \preceq I + X +X^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++e%5EX+%5Cpreceq+I+%2B+X+%2BX%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> which is true for all <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> such that <img alt="{X \preceq I}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX+%5Cpreceq+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>Analogously, if <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is positive definite, we can define</p>
<p align="center"><img alt="\displaystyle  \log X := \sum_i (\log \lambda_i) \cdot w_i w_i^T " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clog+X+%3A%3D+%5Csum_i+%28%5Clog+%5Clambda_i%29+%5Ccdot+w_i+w_i%5ET+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>and we have a number of identities like <img alt="{\log e^X = X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+e%5EX+%3D+X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="{\log X^k = k \log X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+X%5Ek+%3D+k+%5Clog+X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="{e^{k X} = e^k \cdot X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%5E%7Bk+X%7D+%3D+e%5Ek+%5Ccdot+X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, where <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a scalar. We should be careful, however, not to take the analogy with real numbers too far: for example, if <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are two symmetric matrices, in general it is not trues that <img alt="{e^{A+B} = e^A \cdot e^B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%5E%7BA%2BB%7D+%3D+e%5EA+%5Ccdot+e%5EB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, in fact the above expression is actually always false except when <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> commute, in which case it is trivially true. We have, however, the following extremely useful fact.</p>
<blockquote><p><b>Theorem 1 (Golden-Thompson Inequality)</b> <em> </em></p>
<p><em/></p><em>
<p align="center"><img alt="\displaystyle  {\rm tr}(e^{A+B}) \leq {\rm tr}(e^A \cdot e^B) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+tr%7D%28e%5E%7BA%2BB%7D%29+%5Cleq+%7B%5Crm+tr%7D%28e%5EA+%5Ccdot+e%5EB%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
</em><p><em/><em> </em></p></blockquote>
<p>The Golden-Thompson inequality will be all we need to generalize to this matrix setting everything we have proved about multiplicative weights. See <a href="https://terrytao.wordpress.com/2010/07/15/the-golden-thompson-inequality/">this post by Terry Tao</a> for a proof.</p>
<p>The <em>Von Neumann entropy</em> of a density matrix <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> with eigenvalues <img alt="{\lambda_1,\cdots,\lambda_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_1%2C%5Ccdots%2C%5Clambda_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is defined as</p>
<p align="center"><img alt="\displaystyle  S(X) = \sum_i \lambda_i \log \frac 1 {\lambda_i} = - {\rm tr}(X\log X) = - X \bullet \log X " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S%28X%29+%3D+%5Csum_i+%5Clambda_i+%5Clog+%5Cfrac+1+%7B%5Clambda_i%7D+%3D+-+%7B%5Crm+tr%7D%28X%5Clog+X%29+%3D+-+X+%5Cbullet+%5Clog+X+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> that is, if we view <img alt="{X = \sum_i \lambda_i v_i v_i^T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX+%3D+%5Csum_i+%5Clambda_i+v_i+v_i%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> as the mixed quantum state in which the pure state <img alt="{v_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> has probability <img alt="{\lambda_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, then <img alt="{S(X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is the entropy of the distribution over the pure states. Again, this is not a particularly helpful point of view, and in fact we will be interested in defining <img alt="{S(X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> not just for density matrices <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> but for arbitrary positive definite matrices, and even positive semidefinite (with the convention that <img alt="{0 \log 0 = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0+%5Clog+0+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which is used also in the standard definition of entropy of a distribution).</p>
<p>We will be interested in using Von Neumann entropy as a regularizer, and hence we will want to know what is its Bregman divergence. Some calculations show that the Bregman divergence of the Von Neumann entropy, which is called the quantum relative entropy, is</p>
<p align="center"><img alt="\displaystyle  S(X_1|| X_2) = {\rm tr} (X_1 \cdot ( \log X_1 - \log X_2)) + {\rm tr}(X_2) - {\rm tr}(X_1) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S%28X_1%7C%7C+X_2%29+%3D+%7B%5Crm+tr%7D+%28X_1+%5Ccdot+%28+%5Clog+X_1+-+%5Clog+X_2%29%29+%2B+%7B%5Crm+tr%7D%28X_2%29+-+%7B%5Crm+tr%7D%28X_1%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p align="center"><img alt="\displaystyle  = X_1 \bullet(\log X_1 - \log X_2) + I \bullet (X_2 - X_1) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+X_1+%5Cbullet%28%5Clog+X_1+-+%5Clog+X_2%29+%2B+I+%5Cbullet+%28X_2+-+X_1%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> If <img alt="{X_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{X_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are density matrices, the terms <img alt="{{\rm tr}(X_2) - {\rm tr}(X_1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Crm+tr%7D%28X_2%29+-+%7B%5Crm+tr%7D%28X_1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> cancel out; the above definition is valid for arbitrary positive definite matrices.</p>
<p>We will have to study the minima of various functions that take a matrix as an input, so it is good to understand how to compute the gradient of such functions. For example what is the gradient of the function <img alt="{{\rm tr}(X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Crm+tr%7D%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>? Working through the definition we see that <img alt="{\nabla {\rm tr}(X) = I}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cnabla+%7B%5Crm+tr%7D%28X%29+%3D+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and indeed we always have that the gradient of the function <img alt="{X \rightarrow A\bullet X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX+%5Crightarrow+A%5Cbullet+X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> everywhere. Somewhat less obvious is the calculation of the gradient of the Von Neumann entropy, which is</p>
<p align="center"><img alt="\displaystyle  \nabla (X \bullet \log X) = I + \log X " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+%28X+%5Cbullet+%5Clog+X%29+%3D+I+%2B+%5Clog+X+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p><b>2. Analysis in the Constrained FTRL Framework </b></p>
<p>Suppose that we play that we described above using agile mirror descent and using negative Von Neumann entropy (appropriately scaled) as a regularizer. That is, for some <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that we will choose later, we use the regularizer</p>
<p align="center"><img alt="\displaystyle  R(X) = c X \bullet \log X" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++R%28X%29+%3D+c+X+%5Cbullet+%5Clog+X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> which has the Bregman divergence</p>
<p align="center"><img alt="\displaystyle  D(X_1,X_2) = c S(X_1 || X_2) = c X_1 \bullet (\log X_1 - \log X_2) + cI \bullet (X_2 - X_1) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28X_1%2CX_2%29+%3D+c+S%28X_1+%7C%7C+X_2%29+%3D+c+X_1+%5Cbullet+%28%5Clog+X_1+-+%5Clog+X_2%29+%2B+cI+%5Cbullet+%28X_2+-+X_1%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> and our feasible set is the set of density matrices</p>
<p align="center"><img alt="\displaystyle  \Delta := \{ X\in {\mathbb R}^{n\times n} : X \succeq {\bf 0} \wedge {\rm tr}(X) = 1 \} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CDelta+%3A%3D+%5C%7B+X%5Cin+%7B%5Cmathbb+R%7D%5E%7Bn%5Ctimes+n%7D+%3A+X+%5Csucceq+%7B%5Cbf+0%7D+%5Cwedge+%7B%5Crm+tr%7D%28X%29+%3D+1+%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> To bound the regret, we just have to plug the above definitions into the machinery that we developed <a href="https://lucatrevisan.wordpress.com/2019/05/20/online-optimization-post-5-bregman-projections-and-mirror-descent/">in our fifth post</a>.</p>
<p>At time 1, we play the identity matrix scaled by n, which is a density matrix of maximum Von Neumann entropy <img alt="{\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>:</p>
<p align="center"><img alt="\displaystyle  X_1 := \arg\min_{X \in \Delta} R(X) = \frac 1n I " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X_1+%3A%3D+%5Carg%5Cmin_%7BX+%5Cin+%5CDelta%7D+R%28X%29+%3D+%5Cfrac+1n+I+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> At time <img alt="{t+1\geq 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%2B1%5Cgeq+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, we play the matrix <img alt="{X_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> obtained as</p>
<p align="center"><img alt="\displaystyle  \hat X_{t+1} = \arg\min_{X} D(X,X_{t}) + X\bullet L_{t} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Chat+X_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7BX%7D+D%28X%2CX_%7Bt%7D%29+%2B+X%5Cbullet+L_%7Bt%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p align="center"><img alt="\displaystyle  X_{t+1} = \arg\min_{X\in \Delta} D(X,\hat X_{t+1}) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X_%7Bt%2B1%7D+%3D+%5Carg%5Cmin_%7BX%5Cin+%5CDelta%7D+D%28X%2C%5Chat+X_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> and recall that we proved that, after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> steps,</p>
<p align="center"><img alt="\displaystyle  Regret_T(X) \leq D(X,X_1) + \sum_{t=1}^T D(X_t,\hat X_{t+1}) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Regret_T%28X%29+%5Cleq+D%28X%2CX_1%29+%2B+%5Csum_%7Bt%3D1%7D%5ET+D%28X_t%2C%5Chat+X_%7Bt%2B1%7D%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>If <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a density matrix with eigenvalues <img alt="{\lambda_1,\ldots,\lambda_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_1%2C%5Cldots%2C%5Clambda_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, then the first term is</p>
<p align="center"><img alt="\displaystyle  D \left( X, \frac 1n I \right) = c X \bullet (\log X - \log n^{-1} I) = c \sum_i \lambda_i \log \frac n{\lambda_i} = c \log n - c \sum_i \lambda_i \frac 1 {\lambda_i} \leq c\log n " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D+%5Cleft%28+X%2C+%5Cfrac+1n+I+%5Cright%29+%3D+c+X+%5Cbullet+%28%5Clog+X+-+%5Clog+n%5E%7B-1%7D+I%29+%3D+c+%5Csum_i+%5Clambda_i+%5Clog+%5Cfrac+n%7B%5Clambda_i%7D+%3D+c+%5Clog+n+-+c+%5Csum_i+%5Clambda_i+%5Cfrac+1+%7B%5Clambda_i%7D+%5Cleq+c%5Clog+n+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> To complete the analysis we have to understand <img alt="{\hat X_{t+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Chat+X_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. We need to compute the gradient <img alt="{X \rightarrow D(X,X_{t}) + X\bullet L_{t} }" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX+%5Crightarrow+D%28X%2CX_%7Bt%7D%29+%2B+X%5Cbullet+L_%7Bt%7D+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and set it to zero. The gradient of <img alt="{X\bullet L_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%5Cbullet+L_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is just <img alt="{L_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The gradient of <img alt="{D(X,X_{t})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%28X%2CX_%7Bt%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is</p>
<p align="center"><img alt="\displaystyle  \nabla D(X,X_t) = c \nabla X \bullet \log X - c \nabla X \bullet \log X_t - \nabla c X \bullet I " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cnabla+D%28X%2CX_t%29+%3D+c+%5Cnabla+X+%5Cbullet+%5Clog+X+-+c+%5Cnabla+X+%5Cbullet+%5Clog+X_t+-+%5Cnabla+c+X+%5Cbullet+I+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p align="center"><img alt="\displaystyle  = cI + c \log X - c \log X_t - cI " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+cI+%2B+c+%5Clog+X+-+c+%5Clog+X_t+-+cI+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> Meaning that we want to solve for</p>
<p align="center"><img alt="\displaystyle  c \log X - c\log X_t + L_t = 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++c+%5Clog+X+-+c%5Clog+X_t+%2B+L_t+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> and <img alt="{\hat X_{t+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Chat+X_%7Bt%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> satisfies</p>
<p align="center"><img alt="\displaystyle \log X_t - \log \hat X_{t+1} = \frac 1c L_t " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Clog+X_t+-+%5Clog+%5Chat+X_%7Bt%2B1%7D+%3D+%5Cfrac+1c+L_t+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p align="center"><img alt="\displaystyle  \hat X_{t+1} = e^{\log X_t - \frac 1c L_t } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Chat+X_%7Bt%2B1%7D+%3D+e%5E%7B%5Clog+X_t+-+%5Cfrac+1c+L_t+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> and we can write</p>
<p align="center"><img alt="\displaystyle  D(X_t,\hat X_{t+1} ) = c \cdot \left( X_t \bullet (\log X_t - \log \hat X_{t+1})\right) + c {\rm tr}( \hat X_{t+1} )" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28X_t%2C%5Chat+X_%7Bt%2B1%7D+%29+%3D+c+%5Ccdot+%5Cleft%28+X_t+%5Cbullet+%28%5Clog+X_t+-+%5Clog+%5Chat+X_%7Bt%2B1%7D%29%5Cright%29+%2B+c+%7B%5Crm+tr%7D%28+%5Chat+X_%7Bt%2B1%7D+%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p align="center"><img alt="\displaystyle  = c \cdot X_t \bullet \frac 1c L_t + c \cdot {\rm tr}(e^{\log X_t - \frac 1c L_t}) + c {\rm tr} \hat X_{t+1} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+c+%5Ccdot+X_t+%5Cbullet+%5Cfrac+1c+L_t+%2B+c+%5Ccdot+%7B%5Crm+tr%7D%28e%5E%7B%5Clog+X_t+-+%5Cfrac+1c+L_t%7D%29+%2B+c+%7B%5Crm+tr%7D+%5Chat+X_%7Bt%2B1%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> Then we can use Golden-Thompson and the fact that <img alt="{e^-\frac 1c L_t \preceq I - \frac 1c L_t + \frac 1{c^2} L^2_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%5E-%5Cfrac+1c+L_t+%5Cpreceq+I+-+%5Cfrac+1c+L_t+%2B+%5Cfrac+1%7Bc%5E2%7D+L%5E2_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which holds if <img alt="{L_t \preceq cI}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_t+%5Cpreceq+cI%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, to write</p>
<p align="center"><img alt="\displaystyle  {\rm tr}(e^{\log X_t - \frac 1c L_t}) \leq {\rm tr}(e^{\log X_t} \cdot e^{-\frac 1c L_t} ) = X_t \bullet e^{-\frac 1c L_t} \leq X_t \bullet \left( I - \frac 1c L_t + \frac 1{c^2} L^2_t \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+tr%7D%28e%5E%7B%5Clog+X_t+-+%5Cfrac+1c+L_t%7D%29+%5Cleq+%7B%5Crm+tr%7D%28e%5E%7B%5Clog+X_t%7D+%5Ccdot+e%5E%7B-%5Cfrac+1c+L_t%7D+%29+%3D+X_t+%5Cbullet+e%5E%7B-%5Cfrac+1c+L_t%7D+%5Cleq+X_t+%5Cbullet+%5Cleft%28+I+-+%5Cfrac+1c+L_t+%2B+%5Cfrac+1%7Bc%5E2%7D+L%5E2_t+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> Combining everything together we have</p>
<p align="center"><img alt="\displaystyle  D(X_t,\hat X_{t+1} ) \leq \frac 1c X_t \bullet L_t^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%28X_t%2C%5Chat+X_%7Bt%2B1%7D+%29+%5Cleq+%5Cfrac+1c+X_t+%5Cbullet+L_t%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> and so, provided <img alt="{\lambda_{\max} (L_t) \leq c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda_%7B%5Cmax%7D+%28L_t%29+%5Cleq+c%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>,</p>
<p align="center"><img alt="\displaystyle  Regret_T \leq c \log n + \frac 1c \sum_{t=1}^T X_t \bullet L_t^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Regret_T+%5Cleq+c+%5Clog+n+%2B+%5Cfrac+1c+%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> This is the best bound we can hope for, and it matches Theorem 1 in <a href="https://lucatrevisan.wordpress.com/2019/04/24/online-optimization-post-1-multiplicative-weights/">our first post</a> about the Xultiplicative Weights Update algorithm.</p>
<p>If we have <img alt="{L_t \preceq I}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_t+%5Cpreceq+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, we can simplify it to</p>
<p align="center"><img alt="\displaystyle  Regret_T \leq c \log n + \frac T c = 2 \sqrt{T \log n} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Regret_T+%5Cleq+c+%5Clog+n+%2B+%5Cfrac+T+c+%3D+2+%5Csqrt%7BT+%5Clog+n%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> where the last step comes from optimizing <img alt="{c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>We can also write, under the condition <img alt="{L_t \preceq c I}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_t+%5Cpreceq+c+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>,</p>
<p align="center"><img alt="\displaystyle  Regret_T (X) \leq c \log n + \frac 1c \sum_{t=1}^T (X_t \bullet |L_t| )||L_t|| " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Regret_T+%28X%29+%5Cleq+c+%5Clog+n+%2B+%5Cfrac+1c+%5Csum_%7Bt%3D1%7D%5ET+%28X_t+%5Cbullet+%7CL_t%7C+%29%7C%7CL_t%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> where <img alt="{|L_t|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CL_t%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is the “absolute value” of the matrix <img alt="{L_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> defined in the following way: if <img alt="{X = \sum_i \lambda_i v_i v_i^T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX+%3D+%5Csum_i+%5Clambda_i+v_i+v_i%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a symmetric matrix, then its absolute value is <img alt="{|X| = \sum_i |\lambda_i| \cdot v_i v_i^T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CX%7C+%3D+%5Csum_i+%7C%5Clambda_i%7C+%5Ccdot+v_i+v_i%5ET%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Allen-Zhu, Liao and Orecchia state the analysis in this way in their <a href="https://arxiv.org/abs/1506.04838">on generalizations of Matrix Multiplicative Weights</a>.</p>
<p>Our next post will discuss applications at length, but for now let us gain a bit of intuition about the usefulness of these regret bounds. Recall that, for every symmetric matrix <img alt="{M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, we have</p>
<p align="center"><img alt="\displaystyle  \lambda_{\min} (M) = \min_{X \rm\ density\ matrix} \ \ X \bullet M " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clambda_%7B%5Cmin%7D+%28M%29+%3D+%5Cmin_%7BX+%5Crm%5C+density%5C+matrix%7D+%5C+%5C+X+%5Cbullet+M+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> and so the regret bound can be reintepreted in the following way: if we let <img alt="{L_1,\ldots,L_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_1%2C%5Cldots%2CL_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be the loss functions used in a game played against a MMWU algorithm, and the algorithm selects density matrices <img alt="{X_1,\ldots,X_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_1%2C%5Cldots%2CX_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, then</p>
<p align="center"><img alt="\displaystyle  \sum_{t=1}^T X_t \bullet L_t - \min_{X \rm \ density \ matrix} \ \ X \bullet \sum_{t=1}^T L_t \leq c \log n + \frac 1c \sum_{t=1}^T X_t \bullet L_t^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t+-+%5Cmin_%7BX+%5Crm+%5C+density+%5C+matrix%7D+%5C+%5C+X+%5Cbullet+%5Csum_%7Bt%3D1%7D%5ET+L_t+%5Cleq+c+%5Clog+n+%2B+%5Cfrac+1c+%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> that is,</p>
<p align="center"><img alt="\displaystyle  \sum_{t=1}^T X_t \bullet L_t - \lambda_{\min} \left( \sum_{t=1}^T L_t \right) \leq c \log n + \frac 1c \sum_{t=1}^T X_t \bullet L_t^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t+-+%5Clambda_%7B%5Cmin%7D+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+L_t+%5Cright%29+%5Cleq+c+%5Clog+n+%2B+%5Cfrac+1c+%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p> provided that <img alt="{L_t \preceq cI}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_t+%5Cpreceq+cI%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. For example, switching <img alt="{L_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> with <img alt="{-L_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-L_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, we have <a name="main"/></p>
<p><a name="main"/></p><a name="main">
<p align="center"><img alt="\displaystyle   \lambda_{\max} \left( \sum_{t=1}^T L_t \right) \leq \sum_{t=1}^T X_t \bullet L_t + \frac 1c \sum_{t=1}^T X_t \bullet L_t^2 + c\log n \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Clambda_%7B%5Cmax%7D+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+L_t+%5Cright%29+%5Cleq+%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t+%2B+%5Cfrac+1c+%5Csum_%7Bt%3D1%7D%5ET+X_t+%5Cbullet+L_t%5E2+%2B+c%5Clog+n+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
</a><p><a name="main"/><a name="main"/> provided that <img alt="{L_t \succeq -cI}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_t+%5Csucceq+-cI%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, which means that if we can choose a sequence of loss matrices that make the MMWU have small loss at each step, then we are guaranteed that the sum of such matrices cannot have any large eigenvalue.</p></div>
    </content>
    <updated>2021-11-10T12:11:57Z</updated>
    <published>2021-11-10T12:11:57Z</published>
    <category term="theory"/>
    <category term="matrix multiplicative weights update"/>
    <category term="online optimization"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2021-11-11T18:37:14Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/154</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/154" rel="alternate" type="text/html"/>
    <title>TR21-154 |  Explicit Binary Tree Codes with Sub-Logarithmic Size Alphabet | 

	Gil Cohen, 

	Inbar Ben Yaacov, 

	Tal Yankovitz</title>
    <summary>Since they were first introduced by Schulman (STOC 1993), the construction of tree codes remained an elusive open problem. The state-of-the-art construction by Cohen, Haeupler and Schulman (STOC 2018) has constant distance and $(\log n)^{e}$ colors for some constant $e &gt; 1$ that depends on the distance, where $n$ is the depth of the tree. Insisting on a constant number of colors at the expense of having vanishing distance, Gelles, Haeupler, Kol, Ron-Zewi, and Wigderson (SODA 2016) constructed a distance $\Omega(\frac1{\log n})$ tree code.

In this work we improve upon these prior works and construct a distance-$\delta$ tree code with $(\log{n})^{O(\sqrt{\delta})}$ colors. This is the first construction of a constant distance tree code with sub-logarithmic number of colors. Moreover, as a direct corollary we obtain a tree code with a constant number of colors and distance $\Omega\left(\frac1{(\log\log{n})^{2}}\right)$, exponentially improving upon the above-mentioned work by Gelles et al.</summary>
    <updated>2021-11-10T11:16:50Z</updated>
    <published>2021-11-10T11:16:50Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-11T18:37:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.05321</id>
    <link href="http://arxiv.org/abs/2111.05321" rel="alternate" type="text/html"/>
    <title>Turing-Universal Learners with Optimal Scaling Laws</title>
    <feedworld_mtime>1636502400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nakkiran:Preetum.html">Preetum Nakkiran</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.05321">PDF</a><br/><b>Abstract: </b>For a given distribution, learning algorithm, and performance metric, the
rate of convergence (or data-scaling law) is the asymptotic behavior of the
algorithm's test performance as a function of number of train samples. Many
learning methods in both theory and practice have power-law rates, i.e.
performance scales as $n^{-\alpha}$ for some $\alpha &gt; 0$. Moreover, both
theoreticians and practitioners are concerned with improving the rates of their
learning algorithms under settings of interest. We observe the existence of a
"universal learner", which achieves the best possible distribution-dependent
asymptotic rate among all learning algorithms within a specified runtime (e.g.
$O(n^2)$), while incurring only polylogarithmic slowdown over this runtime.
This algorithm is uniform, and does not depend on the distribution, and yet
achieves best-possible rates for all distributions. The construction itself is
a simple extension of Levin's universal search (Levin, 1973). And much like
universal search, the universal learner is not at all practical, and is
primarily of theoretical and philosophical interest.
</p></div>
    </summary>
    <updated>2021-11-10T22:37:32Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-11-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.05320</id>
    <link href="http://arxiv.org/abs/2111.05320" rel="alternate" type="text/html"/>
    <title>Robust Estimation for Random Graphs</title>
    <feedworld_mtime>1636502400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Acharya:Jayadev.html">Jayadev Acharya</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jain:Ayush.html">Ayush Jain</a>, Gautam Kamath, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Suresh:Ananda_Theertha.html">Ananda Theertha Suresh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Huanyu.html">Huanyu Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.05320">PDF</a><br/><b>Abstract: </b>We study the problem of robustly estimating the parameter $p$ of an
Erd\H{o}s-R\'enyi random graph on $n$ nodes, where a $\gamma$ fraction of nodes
may be adversarially corrupted. After showing the deficiencies of canonical
estimators, we design a computationally-efficient spectral algorithm which
estimates $p$ up to accuracy $\tilde O(\sqrt{p(1-p)}/n + \gamma\sqrt{p(1-p)}
/\sqrt{n}+ \gamma/n)$ for $\gamma &lt; 1/60$. Furthermore, we give an inefficient
algorithm with similar accuracy for all $\gamma &lt;1/2$, the
information-theoretic limit. Finally, we prove a nearly-matching statistical
lower bound, showing that the error of our algorithms is optimal up to
logarithmic factors.
</p></div>
    </summary>
    <updated>2021-11-10T22:44:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.05294</id>
    <link href="http://arxiv.org/abs/2111.05294" rel="alternate" type="text/html"/>
    <title>Lattice structure design optimization under localized linear buckling constraints</title>
    <feedworld_mtime>1636502400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yang:Xingtong.html">Xingtong Yang</a>, Xinzhuo Hu, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhu:Liangchao.html">Liangchao Zhu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Ming.html">Ming Li</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.05294">PDF</a><br/><b>Abstract: </b>An optimization method for the design of multi-lattice structures satisfying
local buckling constraints is proposed in this paper. First, the concept of
free material optimization is introduced to find an optimal elastic tensor
distribution among all feasible elastic continua. By approximating the elastic
tensor under the buckling-containing constraint, a matching lattice structure
is embedded in each macro element. The stresses in local cells are especially
introduced to obtain a better structure. Finally, the present method obtains a
lattice structure with excellent overall stiffness and local buckling
resistance, which enhances the structural mechanical properties.
</p></div>
    </summary>
    <updated>2021-11-10T22:55:11Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-11-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.05281</id>
    <link href="http://arxiv.org/abs/2111.05281" rel="alternate" type="text/html"/>
    <title>Competitive Sequencing with Noisy Advice</title>
    <feedworld_mtime>1636502400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Spyros Angelopoulos, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ars=eacute=nio:Diogo.html">Diogo Arsénio</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kamali:Shahin.html">Shahin Kamali</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.05281">PDF</a><br/><b>Abstract: </b>Several well-studied online resource allocation problems can be formulated in
terms of infinite, increasing sequences of positive values, in which each
element is associated with a corresponding allocation value. Examples include
problems such as online bidding, searching for a hidden target on an unbounded
line, and designing interruptible algorithms based on repeated executions. The
performance of the online algorithm, in each of these problems, is measured by
the competitive ratio, which describes the multiplicative performance loss due
to the absence of full information on the instance.
</p>
<p>We study such competitive sequencing problems in a setting in which the
online algorithm has some (potentially) erroneous information, expressed as a
$k$-bit advice string, for some given $k$. We first consider the untrusted
advice setting of [Angelopoulos et al, ITCS 2020], in which the objective is to
quantify performance considering two extremes: either the advice is either
error-free, or it is generated by a (malicious) adversary. Here, we show a
Pareto-optimal solution, using a new approach for applying the functional-based
lower-bound technique due to [Gal, Israel J. Math. 1972]. Next, we study a
nascent noisy advice setting, in which a number of the advice bits may be
erroneous; the exact error is unknown to the online algorithm, which only has
access to a pessimistic estimate (i.e., an upper bound on this error). We give
improved upper bounds, but also the first lower bound on the competitive ratio
of an online problem in this setting. To this end, we combine ideas from robust
query-based search in arrays, and fault-tolerant contract scheduling. Last, we
demonstrate how to apply the above techniques in robust optimization without
predictions, and discuss how they can be applicable in the context of more
general online problems.
</p></div>
    </summary>
    <updated>2021-11-10T22:53:09Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.05257</id>
    <link href="http://arxiv.org/abs/2111.05257" rel="alternate" type="text/html"/>
    <title>Logarithmic Regret from Sublinear Hints</title>
    <feedworld_mtime>1636502400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bhaskara:Aditya.html">Aditya Bhaskara</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cutkosky:Ashok.html">Ashok Cutkosky</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Ravi.html">Ravi Kumar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Purohit:Manish.html">Manish Purohit</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.05257">PDF</a><br/><b>Abstract: </b>We consider the online linear optimization problem, where at every step the
algorithm plays a point $x_t$ in the unit ball, and suffers loss $\langle c_t,
x_t\rangle$ for some cost vector $c_t$ that is then revealed to the algorithm.
Recent work showed that if an algorithm receives a hint $h_t$ that has
non-trivial correlation with $c_t$ before it plays $x_t$, then it can achieve a
regret guarantee of $O(\log T)$, improving on the bound of $\Theta(\sqrt{T})$
in the standard setting. In this work, we study the question of whether an
algorithm really requires a hint at every time step. Somewhat surprisingly, we
show that an algorithm can obtain $O(\log T)$ regret with just $O(\sqrt{T})$
hints under a natural query model; in contrast, we also show that $o(\sqrt{T})$
hints cannot guarantee better than $\Omega(\sqrt{T})$ regret. We give two
applications of our result, to the well-studied setting of optimistic regret
bounds and to the problem of online learning with abstention.
</p></div>
    </summary>
    <updated>2021-11-10T22:47:03Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.05225</id>
    <link href="http://arxiv.org/abs/2111.05225" rel="alternate" type="text/html"/>
    <title>Helly systems and certificates in optimization</title>
    <feedworld_mtime>1636502400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Basu:Amitabh.html">Amitabh Basu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Tongtong.html">Tongtong Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Conforti:Michele.html">Michele Conforti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jiang:Hongyi.html">Hongyi Jiang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.05225">PDF</a><br/><b>Abstract: </b>Inspired by branch-and-bound and cutting plane proofs in mixed-integer
optimization and proof complexity, we develop a general approach via Hoffman's
Helly systems. This helps to distill the main ideas behind optimality and
infeasibility certificates in optimization. The first part of the paper
formalizes the notion of a certificate and its size in this general setting.
The second part of the paper establishes lower and upper bounds on the sizes of
these certificates in various different settings. We show that some important
techniques existing in the literature are purely combinatorial in nature and do
not depend on any underlying geometric notions.
</p></div>
    </summary>
    <updated>2021-11-10T22:40:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-11-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.05214</id>
    <link href="http://arxiv.org/abs/2111.05214" rel="alternate" type="text/html"/>
    <title>A Topological Data Analysis Based Classifier</title>
    <feedworld_mtime>1636502400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kindelan:Rolando.html">Rolando Kindelan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fr=iacute=as:Jos=eacute=.html">José Frías</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cerda:Mauricio.html">Mauricio Cerda</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hitschfeld:Nancy.html">Nancy Hitschfeld</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.05214">PDF</a><br/><b>Abstract: </b>Topological Data Analysis (TDA) is an emergent field that aims to discover
topological information hidden in a dataset. TDA tools have been commonly used
to create filters and topological descriptors to improve Machine Learning (ML)
methods. This paper proposes an algorithm that applies TDA directly to
multi-class classification problems, without any further ML stage, showing
advantages for imbalanced datasets. The proposed algorithm builds a filtered
simplicial complex on the dataset. Persistent Homology (PH) is applied to guide
the selection of a sub-complex where unlabeled points obtain the label with the
majority of votes from labeled neighboring points. We select 8 datasets with
different dimensions, degrees of class overlap and imbalanced samples per
class. On average, the proposed TDABC method was better than KNN and
weighted-KNN. It behaves competitively with Local SVM and Random Forest
baseline classifiers in balanced datasets, and it outperforms all baseline
methods classifying entangled and minority classes.
</p></div>
    </summary>
    <updated>2021-11-10T22:53:55Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-11-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.05207</id>
    <link href="http://arxiv.org/abs/2111.05207" rel="alternate" type="text/html"/>
    <title>Computing Sparse Jacobians and Hessians Using Algorithmic Differentiation</title>
    <feedworld_mtime>1636502400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bell:Bradley_M=.html">Bradley M. Bell</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kristensen:Kasper.html">Kasper Kristensen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.05207">PDF</a><br/><b>Abstract: </b>Stochastic scientific models and machine learning optimization estimators
have a large number of variables; hence computing large sparse Jacobians and
Hessians is important. Algorithmic differentiation (AD) greatly reduces the
programming effort required to obtain the sparsity patterns and values for
these matrices. We present forward, reverse, and subgraph methods for computing
sparse Jacobians and Hessians. Special attention is given the the subgraph
method because it is new. The coloring and compression steps are not necessary
when computing sparse Jacobians and Hessians using subgraphs. Complexity
analysis shows that for some problems the subgraph method is expected to be
much faster. We compare C++ operator overloading implementations of the methods
in the ADOL-C and CppAD software packages using some of the MINPACK-2 test
problems. The experiments are set up in a way that makes them easy to run on
different hardware, different systems, different compilers, other test problem
and other AD packages. The setup time is the time to record the graph, compute
sparsity, coloring, compression, and optimization of the graph. If the setup is
necessary for each evaluation, the subgraph implementation has similar run
times for sparse Jacobians and faster run times for sparse Hessians.
</p></div>
    </summary>
    <updated>2021-11-10T22:52:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.05197</id>
    <link href="http://arxiv.org/abs/2111.05197" rel="alternate" type="text/html"/>
    <title>A PTAS for the horizontal rectangle stabbing problem</title>
    <feedworld_mtime>1636502400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Arindam Khan, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Subramanian:Aditya.html">Aditya Subramanian</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wiese:Andreas.html">Andreas Wiese</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.05197">PDF</a><br/><b>Abstract: </b>We study rectangle stabbing problems in which we are given $n$ axis-aligned
rectangles in the plane that we want to stab, i.e., we want to select line
segments such that for each given rectangle there is a line segment that
intersects two opposite edges of it.
</p>
<p>In the horizontal rectangle stabbing problem (STABBING), the goal is to find
a set of horizontal line segments of minimum total length such that all
rectangles are stabbed. In general rectangle stabbing problem, also known as
horizontal-vertical stabbing problem (HV-Stabbing), the goal is to find a set
of rectilinear (i.e., either vertical or horizontal) line segments of minimum
total length such that all rectangles are stabbed. Both variants are NP-hard.
Chan, van Dijk, Fleszar, Spoerhase, and Wolff [2018]initiated the study of
these problems by providing constant approximation algorithms. Recently,
Eisenbrand, Gallato, Svensson, and Venzin [2021] have presented a QPTAS and a
polynomial-time 8-approximation algorithm for STABBING but it is was open
whether the problem admits a PTAS.
</p>
<p>In this paper, we obtain a PTAS for STABBING, settling this question. For
HV-Stabbing, we obtain a $(2+\varepsilon)$-approximation. We also obtain PTASes
for special cases of HV-Stabbing: (i) when all rectangles are squares, (ii)
when each rectangle's width is at most its height, and (iii) when all
rectangles are $\delta$-large, i.e., have at least one edge whose length is at
least $\delta$, while all edge lengths are at most 1. Our result also implies
improved approximations for other problems such as generalized minimum
Manhattan network.
</p></div>
    </summary>
    <updated>2021-11-10T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-11-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.05016</id>
    <link href="http://arxiv.org/abs/2111.05016" rel="alternate" type="text/html"/>
    <title>Pattern Matching on Grammar-Compressed Strings in Linear Time</title>
    <feedworld_mtime>1636502400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ganardi:Moses.html">Moses Ganardi</a>, Paweł Gawrychowski <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.05016">PDF</a><br/><b>Abstract: </b>The most fundamental problem considered in algorithms for text processing is
pattern matching: given a pattern $p$ of length $m$ and a text $t$ of length
$n$, does $p$ occur in $t$? Multiple versions of this basic question have been
considered, and by now we know algorithms that are fast both in practice and in
theory. However, the rapid increase in the amount of generated and stored data
brings the need of designing algorithms that operate directly on compressed
representations of data. In the compressed pattern matching problem we are
given a compressed representation of the text, with $n$ being the length of the
compressed representation and $N$ being the length of the text, and an
uncompressed pattern of length $m$. The most challenging (and yet relevant when
working with highly repetitive data, say biological information) scenario is
when the chosen compression method is capable of describing a string of
exponential length (in the size of its representation). An elegant formalism
for such a compression method is that of straight-line programs, which are
simply context-free grammars describing exactly one string. While it has been
known that compressed pattern matching problem can be solved in $O(m+n\log N)$
time for this compression method, designing a linear-time algorithm remained
open. We resolve this open question by presenting an $O(n+m)$ time algorithm
that, given a context-free grammar of size $n$ that produces a single string
$t$ and a pattern $p$ of length $m$, decides whether $p$ occurs in $t$ as a
substring. To this end, we devise improved solutions for the weighted ancestor
problem and the substring concatenation problem.
</p></div>
    </summary>
    <updated>2021-11-10T22:47:52Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.05001</id>
    <link href="http://arxiv.org/abs/2111.05001" rel="alternate" type="text/html"/>
    <title>Parameterized complexity of untangling knots</title>
    <feedworld_mtime>1636502400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Legrand=Duchesne:Cl=eacute=ment.html">Clément Legrand-Duchesne</a>, Ashutosh Rai, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tancer:Martin.html">Martin Tancer</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.05001">PDF</a><br/><b>Abstract: </b>Deciding whether a diagram of a knot can be untangled with a given number of
moves (as a part of the input) is known to be NP-complete. In this paper we
determine the parameterized complexity of this problem with respect to a
natural parameter called defect. Roughly speaking, it measures the efficiency
of the moves used in the shortest untangling sequence of Reidemeister moves.
</p>
<p>We show that the II- moves in a shortest untangling sequence can be
essentially performed greedily. Using that, we show that this problem belongs
to W[P] when parameterized by the defect. We also show that this problem is
W[P]-hard by a reduction from Minimum axiom set.
</p></div>
    </summary>
    <updated>2021-11-10T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-11-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.05000</id>
    <link href="http://arxiv.org/abs/2111.05000" rel="alternate" type="text/html"/>
    <title>Behavioral Strengths and Weaknesses of Various Models of Limited Automata</title>
    <feedworld_mtime>1636502400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yamakami:Tomoyuki.html">Tomoyuki Yamakami</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.05000">PDF</a><br/><b>Abstract: </b>We examine the behaviors of various models of $k$-limited automata, which
naturally extend Hibbard's [Inf. Control, vol. 11, pp. 196--238, 1967] scan
limited automata, each of which is a single-tape linear-bounded automaton
satisfying the $k$-limitedness requirement that the content of each tape cell
should be modified only during the first $k$ visits of a tape head. One central
computation model is a probabilistic $k$-limited automaton (abbreviated as a
$k$-lpa), which accepts an input exactly when its accepting states are
reachable from its initial state with probability more than 1/2 within expected
polynomial time. We also study the behaviors of one-sided-error and
bounded-error variants of such $k$-lpa's as well as the deterministic,
nondeterministic, and unambiguous models of $k$-limited automata, which can be
viewed as natural restrictions of $k$-lpa's. We discuss fundamental properties
of these machine models and obtain inclusions and separations among language
families induced by them. In due course, we study special features -- the blank
skipping property and the closure under reversal -- which are keys to the
robustness of $k$-lpa's.
</p></div>
    </summary>
    <updated>2021-11-10T22:39:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-11-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.04994</id>
    <link href="http://arxiv.org/abs/2111.04994" rel="alternate" type="text/html"/>
    <title>Analysis of Work-Stealing and Parallel Cache Complexity</title>
    <feedworld_mtime>1636502400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gu:Yan.html">Yan Gu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Napier:Zachary.html">Zachary Napier</a>, Yihan Sun <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.04994">PDF</a><br/><b>Abstract: </b>Parallelism has become extremely popular over the past decade, and there have
been a lot of new parallel algorithms and software. The randomized
work-stealing (RWS) scheduler plays a crucial role in this ecosystem. In this
paper, we study two important topics related to the randomized work-stealing
scheduler.
</p>
<p>Our first contribution is a simplified, classroom-ready version of analysis
for the RWS scheduler. The theoretical efficiency of the RWS scheduler has been
analyzed for a variety of settings, but most of them are quite complicated. In
this paper, we show a new analysis, which we believe is easy to understand, and
can be especially useful in education. We avoid using the potential function in
the analysis, and we assume a highly asynchronous setting, which is more
realistic for today's parallel machines.
</p>
<p>Our second and main contribution is some new parallel cache complexity for
algorithms using the RWS scheduler. Although the sequential I/O model has been
well-studied over the past decades, so far very few results have extended it to
the parallel setting. The parallel cache bounds of many existing algorithms are
affected by a polynomial of the span, which causes a significant overhead for
high-span algorithms. Our new analysis decouples the span from the analysis of
the parallel cache complexity. This allows us to show new parallel cache bounds
for a list of classic algorithms. Our results are only a polylogarithmic factor
off the lower bounds, and significantly improve previous results.
</p></div>
    </summary>
    <updated>2021-11-10T22:46:06Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.04958</id>
    <link href="http://arxiv.org/abs/2111.04958" rel="alternate" type="text/html"/>
    <title>Gomory-Hu Tree in Subcubic Time</title>
    <feedworld_mtime>1636502400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Abboud:Amir.html">Amir Abboud</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krauthgamer:Robert.html">Robert Krauthgamer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Jason.html">Jason Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Panigrahi:Debmalya.html">Debmalya Panigrahi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saranurak:Thatchaphol.html">Thatchaphol Saranurak</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Trabelsi:Ohad.html">Ohad Trabelsi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.04958">PDF</a><br/><b>Abstract: </b>In 1961, Gomory and Hu showed that the max-flow values of all $n\choose 2$
pairs of vertices in an undirected graph can be computed using only $n-1$ calls
to any max-flow algorithm. Even assuming a linear-time max-flow algorithm, this
yields a running time of $O(n^3)$ for this problem. We break this 60-year old
barrier by giving an $\tilde{O}(n^{23/8})$-time algorithm for the Gomory-Hu
tree problem. Our result is unconditional, i.e., it does not rely on a
linear-time max-flow algorithm.
</p></div>
    </summary>
    <updated>2021-11-10T22:41:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.04897</id>
    <link href="http://arxiv.org/abs/2111.04897" rel="alternate" type="text/html"/>
    <title>Nearly-Linear Time Approximate Scheduling Algorithms</title>
    <feedworld_mtime>1636502400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Shi.html">Shi Li</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.04897">PDF</a><br/><b>Abstract: </b>We study nearly-linear time approximation algorithms for non-preemptive
scheduling problems in two settings: the unrelated machine setting, and the
identical machine with job precedence constraints setting. The objectives we
study include makespan, weighted completion time, and $L_q$ norm of machine
loads. We develop nearly-linear time approximation algorithms for the studied
problems with $O(1)$-approximation ratios, many of which match the
correspondent best known ratios achievable in polynomial time.
</p>
<p>Our main technique is linear programming relaxation. For problems in the
unrelated machine setting, we formulate mixed packing and covering LP
relaxations of nearly-linear size, and solve them approximately using the
nearly-linear time solver of Young. We show the LP solutions can be rounded
within $O(1)$-factor loss. For problems in the identical machine with
precedence constraints setting, the precedence constraints can not be
formulated as packing or covering constraints. To achieve the claimed running
time, we define a polytope for the constraints, and leverage the multiplicative
weight update (MWU) method with an oracle which always returns solutions in the
polytope.
</p>
<p>Along the way of designing the oracle, we encounter the single-commodity
maximum flow problem over a directed acyclic graph $G = (V, E)$, where sources
and sinks have limited supplies and demands, but edges have infinite
capacities. We develop a $\frac{1}{1+\epsilon}$-approximation for the problem
in time $O\left(\frac{|E|}{\epsilon}\log |V|\right)$, which may be of
independent interest.
</p></div>
    </summary>
    <updated>2021-11-10T22:47:43Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.04888</id>
    <link href="http://arxiv.org/abs/2111.04888" rel="alternate" type="text/html"/>
    <title>Active Sampling for Linear Regression Beyond the $\ell_2$ Norm</title>
    <feedworld_mtime>1636502400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Musco:Cameron.html">Cameron Musco</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Musco:Christopher.html">Christopher Musco</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Woodruff:David_P=.html">David P. Woodruff</a>, Taisuke Yasuda <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.04888">PDF</a><br/><b>Abstract: </b>We study active sampling algorithms for linear regression, which aim to query
only a small number of entries of a target vector $b\in\mathbb{R}^n$ and output
a near minimizer to $\min_{x\in\mathbb{R}^d}\|Ax-b\|$, where $A\in\mathbb{R}^{n
\times d}$ is a design matrix and $\|\cdot\|$ is some loss function.
</p>
<p>For $\ell_p$ norm regression for any $0&lt;p&lt;\infty$, we give an algorithm based
on Lewis weight sampling that outputs a $(1+\epsilon)$ approximate solution
using just $\tilde{O}(d^{\max(1,{p/2})}/\mathrm{poly}(\epsilon))$ queries to
$b$. We show that this dependence on $d$ is optimal, up to logarithmic factors.
Our result resolves a recent open question of Chen and Derezi\'{n}ski, who gave
near optimal bounds for the $\ell_1$ norm, and suboptimal bounds for $\ell_p$
regression with $p\in(1,2)$.
</p>
<p>We also provide the first total sensitivity upper bound of
$O(d^{\max\{1,p/2\}}\log^2 n)$ for loss functions with at most degree $p$
polynomial growth. This improves a recent result of Tukan, Maalouf, and
Feldman. By combining this with our techniques for the $\ell_p$ regression
result, we obtain an active regression algorithm making $\tilde
O(d^{1+\max\{1,p/2\}}/\mathrm{poly}(\epsilon))$ queries, answering another open
question of Chen and Derezi\'{n}ski. For the important special case of the
Huber loss, we further improve our bound to an active sample complexity of
$\tilde O(d^{(1+\sqrt2)/2}/\epsilon^c)$ and a non-active sample complexity of
$\tilde O(d^{4-2\sqrt 2}/\epsilon^c)$, improving a previous $d^4$ bound for
Huber regression due to Clarkson and Woodruff. Our sensitivity bounds have
further implications, improving a variety of previous results using sensitivity
sampling, including Orlicz norm subspace embeddings and robust subspace
approximation. Finally, our active sampling results give the first sublinear
time algorithms for Kronecker product regression under every $\ell_p$ norm.
</p></div>
    </summary>
    <updated>2021-11-10T22:48:02Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.04808</id>
    <link href="http://arxiv.org/abs/2111.04808" rel="alternate" type="text/html"/>
    <title>Locally Testable Codes with constant rate, distance, and locality</title>
    <feedworld_mtime>1636502400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dinur:Irit.html">Irit Dinur</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Evra:Shai.html">Shai Evra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Livne:Ron.html">Ron Livne</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lubotzky:Alexander.html">Alexander Lubotzky</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mozes:Shahar.html">Shahar Mozes</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.04808">PDF</a><br/><b>Abstract: </b>A locally testable code (LTC) is an error-correcting code that has a
property-tester. The tester reads $q$ bits that are randomly chosen, and
rejects words with probability proportional to their distance from the code.
The parameter $q$ is called the locality of the tester.
</p>
<p>LTCs were initially studied as important components of PCPs, and since then
the topic has evolved on its own. High rate LTCs could be useful in practice:
before attempting to decode a received word, one can save time by first quickly
testing if it is close to the code.
</p>
<p>An outstanding open question has been whether there exist "$c^3$-LTCs",
namely LTCs with *c*onstant rate, *c*onstant distance, and *c*onstant locality.
</p>
<p>In this work we construct such codes based on a new two-dimensional complex
which we call a left-right Cayley complex. This is essentially a graph which,
in addition to vertices and edges, also has squares. Our codes can be viewed as
a two-dimensional version of (the one-dimensional) expander codes, where the
codewords are functions on the squares rather than on the edges.
</p></div>
    </summary>
    <updated>2021-11-10T22:37:55Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-11-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2111.04804</id>
    <link href="http://arxiv.org/abs/2111.04804" rel="alternate" type="text/html"/>
    <title>Approximating Fair Clustering with Cascaded Norm Objectives</title>
    <feedworld_mtime>1636502400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Eden Chlamtáč, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Makarychev:Yury.html">Yury Makarychev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vakilian:Ali.html">Ali Vakilian</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2111.04804">PDF</a><br/><b>Abstract: </b>We introduce the $(p,q)$-Fair Clustering problem. In this problem, we are
given a set of points $P$ and a collection of different weight functions $W$.
We would like to find a clustering which minimizes the $\ell_q$-norm of the
vector over $W$ of the $\ell_p$-norms of the weighted distances of points in
$P$ from the centers. This generalizes various clustering problems, including
Socially Fair $k$-Median and $k$-Means, and is closely connected to other
problems such as Densest $k$-Subgraph and Min $k$-Union.
</p>
<p>We utilize convex programming techniques to approximate the $(p,q)$-Fair
Clustering problem for different values of $p$ and $q$. When $p\geq q$, we get
an $O(k^{(p-q)/(2pq)})$, which nearly matches a $k^{\Omega((p-q)/(pq))}$ lower
bound based on conjectured hardness of Min $k$-Union and other problems. When
$q\geq p$, we get an approximation which is independent of the size of the
input for bounded $p,q$, and also matches the recent $O((\log n/(\log\log
n))^{1/p})$-approximation for $(p, \infty)$-Fair Clustering by Makarychev and
Vakilian (COLT 2021).
</p></div>
    </summary>
    <updated>2021-11-10T22:46:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-11-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.12543</id>
    <link href="http://arxiv.org/abs/2005.12543" rel="alternate" type="text/html"/>
    <title>Computing persistent Stiefel-Whitney classes of line bundles</title>
    <feedworld_mtime>1636502400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tinarrage:Rapha=euml=l.html">Raphaël Tinarrage</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.12543">PDF</a><br/><b>Abstract: </b>We propose a definition of persistent Stiefel-Whitney classes of vector
bundle filtrations. It relies on seeing vector bundles as subsets of some
Euclidean spaces. The usual \v{C}ech filtration of such a subset can be endowed
with a vector bundle structure, that we call a \v{C}ech bundle filtration. We
show that this construction is stable and consistent. When the dataset is a
finite sample of a line bundle, we implement an effective algorithm to compute
its persistent Stiefel-Whitney classes. In order to use simplicial
approximation techniques in practice, we develop a notion of weak simplicial
approximation. As a theoretical example, we give an in-depth study of the
normal bundle of the circle, which reduces to understanding the persistent
cohomology of the torus knot (1,2). We illustrate our method on several
datasets inspired by image analysis.
</p></div>
    </summary>
    <updated>2021-11-10T23:17:54Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-11-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/09/christopher-strachey-professorship-of-computing-at-university-of-oxford-apply-by-february-28-2022/</id>
    <link href="https://cstheory-jobs.org/2021/11/09/christopher-strachey-professorship-of-computing-at-university-of-oxford-apply-by-february-28-2022/" rel="alternate" type="text/html"/>
    <title>Christopher Strachey Professorship of Computing at University of Oxford (apply by February 28, 2022)</title>
    <summary>The Strachey Professorship is the oldest chair in the Department of Computer Science, and is named for Christopher Strachey, who founded Oxford’s Programming Research Group in 1965. The Department seeks an internationally recognised research leader who will further the academic and strategic development of the department. Website: http://www.cs.ox.ac.uk/news/1988-full.html Email: head-of-dept@cs.ox.ac.uk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Strachey Professorship is the oldest chair in the Department of Computer Science, and is named for Christopher Strachey, who founded Oxford’s Programming Research Group in 1965. The Department seeks an internationally recognised research leader who will further the academic and strategic development of the department.</p>
<p>Website: <a href="http://www.cs.ox.ac.uk/news/1988-full.html">http://www.cs.ox.ac.uk/news/1988-full.html</a><br/>
Email: head-of-dept@cs.ox.ac.uk</p></div>
    </content>
    <updated>2021-11-09T18:24:49Z</updated>
    <published>2021-11-09T18:24:49Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-11T18:37:47Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/neurips2021/</id>
    <link href="https://differentialprivacy.org/neurips2021/" rel="alternate" type="text/html"/>
    <title>Conference Digest - NeurIPS 2021</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The accepted papers for <a href="https://neurips.cc/Conferences/2020">NeurIPS 2021</a> were recently announced, and there’s a huge amount of differential privacy content. 
We found one relevant workshop and 48 papers.
This is up from 31 papers last year, an over 50% increase!
It looks like there’s huge growth in interest on differentially private machine learning.
Impressively, at the time of this writing, all but five papers are already posted on arXiv!
For the full list of accepted papers, see <a href="https://neurips.cc/Conferences/2021/AcceptedPapersInitial">here</a>.
Please let us know if we missed relevant papers on differential privacy!</p>

<h2 id="workshops">Workshops</h2>

<ul>
  <li><a href="https://priml2021.github.io/">Privacy in Machine Learning (PriML) 2021</a></li>
</ul>

<h2 id="papers">Papers</h2>

<ul>
  <li>
    <p><a href="https://arxiv.org/abs/2103.08721">A Central Limit Theorem for Differentially Private Query Answering</a><br/>
Jinshuo Dong, Weijie Su, Linjun Zhang</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2108.02391">Adapting to function difficulty and growth conditions in private optimization</a><br/>
Hilal Asi, Daniel Levy, John Duchi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.04378">Adaptive Machine Unlearning</a><br/>
Varun Gupta, Christopher Jung, Seth Neel, Aaron Roth, Saeed Sharifi-Malvajerdi, Chris Waites</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2110.13239">An Uncertainty Principle is a Price of Privacy-Preserving Microdata</a><br/>
John Abowd, Robert Ashmead, Ryan Cumings-Menon, Simson Garfinkel, Daniel Kifer, Philip Leclerc, William Sexton, Ashley Simpson, Christine Task, Pavel Zhuravlev</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.03408">Antipodes of Label Differential Privacy: PATE and ALIBI</a><br/>
Mani Malek Esmaeili, Ilya Mironov, Karthik Prasad, Igor Shilov, Florian Tramer</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.13329">Covariance-Aware Private Mean Estimation Without Private Covariance Estimation</a><br/>
Gavin Brown, Marco Gaboardi, Adam Smith, Jonathan Ullman, Lydia Zakynthinou</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.06062">Deep Learning with Label Differential Privacy</a><br/>
Badih Ghazi, Noah Golowich, Ravi Kumar, Pasin Manurangsi, Chiyuan Zhang</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.05855">Differential Privacy Dynamics of Langevin Diffusion and Noisy Gradient Descent</a><br/>
Rishav Chourasia, Jiayuan Ye, Reza Shokri</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.02674">Differentially Private Empirical Risk Minimization under the Fairness Lens</a><br/>
Cuong Tran, My Dinh, Ferdinando Fioretto</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2110.14153">Differentially Private Federated Bayesian Optimization with Distributed Exploration</a><br/>
Zhongxiang Dai, Bryan Kian Hsiang Low, Patrick Jaillet</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1905.03871">Differentially Private Learning with Adaptive Clipping</a><br/>
Galen Andrew, Om Thakkar, Swaroop Ramaswamy, Brendan McMahan</p>
  </li>
  <li>
    <p>Differentially Private Model Personalization<br/>
Prateek Jain, John Rush, Adam Smith, Shuang Song, Abhradeep Guha Thakurta</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.02900">Differentially Private Multi-Armed Bandits in the Shuffle Model</a><br/>
Jay Tenenbaum, Haim Kaplan, Yishay Mansour, Uri Stemmer</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2108.02831">Differentially Private n-gram Extraction</a><br/>
Kunho Kim, Sivakanth Gopi, Janardhan Kulkarni, Sergey Yekhanin</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2111.02516">Differential Privacy Over Riemannian Manifolds</a><br/>
Matthew Reimherr, Karthik Bharath, Carlos Soto</p>
  </li>
  <li>
    <p>Differentially Private Sampling from Distributions<br/>
Sofya Raskhodnikova, Satchit Sivakumar, Adam Smith, Marika Swanberg</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.05585">Differentially Private Stochastic Optimization: New Results in Convex and Non-Convex Settings</a><br/>
Raef Bassily, Cristóbal Guzmán, Michael Menart</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2111.01177">Don’t Generate Me: Training Differentially Private Generative Models with Sinkhorn Divergence</a><br/>
Tianshi Cao, Alex Bie, Arash Vahdat, Sanja Fidler, Karsten Kreis</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2010.09063">Enabling Fast Differentially Private SGD via Just-in-Time Compilation and Vectorization</a><br/>
Pranav Subramani, Nicholas Vadivelu, Gautam Kamath</p>
  </li>
  <li>
    <p>Exact Privacy Guarantees for Markov Chain Implementations of the Exponential Mechanism with Artificial Atoms<br/>
Jeremy Seeman, Matthew Reimherr, Aleksandra Slavković</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.03013">Fast and Memory Efficient Differentially Private-SGD via JL Projections</a><br/>
Zhiqi Bu, Sivakanth Gopi, Janardhan Kulkarni, Yin Tat Lee, Hanwen Shen, Uthaipon Tantipongpipat</p>
  </li>
  <li>
    <p>G-PATE: Scalable Differentially Private Data Generator via Private Aggregation of Teacher Discriminators<br/>
Yunhui Long, Boxin Wang, Zhuolin Yang, Bhavya Kailkhura, Aston Zhang, Carl Gunter, Bo Li</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.03365">Generalized Linear Bandits with Local Differential Privacy</a><br/>
Yuxuan Han, Zhipeng Liang, Yang Wang, Jiheng Zhang</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2008.11193">Individual Privacy Accounting via a Rényi Filter</a><br/>
Vitaly Feldman, Tijana Zrnic</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2104.00979">Information-constrained optimization: can adaptive processing of gradients help?</a><br/>
Jayadev Acharya, Clement Canonne, Prathamesh Mayekar, Himanshu Tyagi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.00463">Instance-optimal Mean Estimation Under Differential Privacy</a><br/>
Ziyue Huang, Yuting Liang, Ke Yi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.07153">Iterative Methods for Private Synthetic Data: Unifying Framework and New Methods</a><br/>
Terrance Liu, Giuseppe Vietri, Steven Wu</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.11845">Learning with User-Level Privacy</a><br/>
Daniel Levy, Ziteng Sun, Kareem Amin, Satyen Kale, Alex Kulesza, Mehryar Mohri, Ananda Theertha Suresh</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.13513">Littlestone Classes are Privately Online Learnable</a><br/>
Noah Golowich, Roi Livni</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2010.07778">Local Differential Privacy for Regret Minimization in Reinforcement Learning</a><br/>
Evrard Garcelon, Vianney Perchet, Ciara Pike-Burke, Matteo Pirotta</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.03940">Locally differentially private estimation of functionals of discrete distributions</a><br/>
Cristina Butucea, Yann Issartel</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2105.10675">Locally private online change point detection</a><br/>
Tom Berrett, Yi Yu</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.10870">Multiclass versus Binary Differentially Private PAC Learning</a><br/>
Satchit Sivakumar, Mark Bun, Marco Gaboardi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.02848">Numerical Composition of Differential Privacy</a><br/>
Sivakanth Gopi, Yin Tat Lee, Lukas Wutschitz</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.11526">On the Sample Complexity of Privately Learning Axis-Aligned Rectangles</a><br/>
Menachem Sadigurschi, Uri Stemmer</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.03645">Photonic Differential Privacy with Direct Feedback Alignment</a><br/>
Ruben Ohana, Hamlet Medina, Julien Launay, Alessandro Cappelli, Iacopo Poli, Liva Ralaivola, Alain Rakotomamonjy</p>
  </li>
  <li>
    <p>Private and Non-private Uniformity Testing for Ranking Data<br/>
Róbert Busa-Fekete, Dimitris Fotakis, Emmanouil Zampetakis</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.07171">Private learning implies quantum stability</a><br/>
Yihui Quek, Srinivasan Arunachalam, John A Smolin</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2103.15352">Private Non-smooth ERM and SCO in Subquadratic Steps</a><br/>
Janardhan Kulkarni, Yin Tat Lee, Daogao Liu</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.02162">Privately Learning Mixtures of Axis-Aligned Gaussians</a><br/>
Ishaq Aden-Ali, Hassan Ashtiani, Christopher Liaw</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2106.00001">Privately Learning Subspaces</a><br/>
Vikrant Singhal, Thomas Steinke</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2111.02281">Privately Publishable Per-instance Privacy</a><br/>
Rachel Redberg, Yu-Xiang Wang</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2109.06153">Relaxed Marginal Consistency for Differentially Private Query Answering</a><br/>
Ryan McKenna, Siddhant Pradhan, Daniel Sheldon, Gerome Miklau</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2103.03279">Remember What You Want to Forget: Algorithms for Machine Unlearning</a><br/>
Ayush Sekhari, Jayadev Acharya, Gautam Kamath, Ananda Theertha Suresh</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2107.08763">Renyi Differential Privacy of The Subsampled Shuffle Model In Distributed Learning</a><br/>
Antonious Girgis, Deepesh Data, Suhas Diggavi</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2102.09159">Robust and differentially private mean estimation</a><br/>
Xiyang Liu, Weihao Kong, Sham Kakade, Sewoong Oh</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2110.04995">The Skellam Mechanism for Differentially Private Federated Learning</a><br/>
Naman Agarwal, Peter Kairouz, Ken Liu</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2110.11208">User-Level Differentially Private Learning via Correlated Sampling</a><br/>
Badih Ghazi, Ravi Kumar, Pasin Manurangsi</p>
  </li>
</ul></div>
    </summary>
    <updated>2021-11-09T15:00:00Z</updated>
    <published>2021-11-09T15:00:00Z</published>
    <author>
      <name>Gautam Kamath</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2021-11-10T23:21:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/09/postdoc-positions-in-quantum-algorithms-at-irif-cnrs-paris-apply-by-december-15-2021/</id>
    <link href="https://cstheory-jobs.org/2021/11/09/postdoc-positions-in-quantum-algorithms-at-irif-cnrs-paris-apply-by-december-15-2021/" rel="alternate" type="text/html"/>
    <title>Postdoc positions in quantum algorithms at IRIF, CNRS (Paris) (apply by December 15, 2021)</title>
    <summary>IRIF (Paris, France) is offering multiple postdoc positions to work on the theory of quantum computing. Emphasis is on the development of quantum algorithms for optimization, machine learning, massive data, and cryptography. You will be working with permanent members S. Apers, I. Kerenidis, S. Laplante and F. Magniez, and a strong team of postdocs and […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>IRIF (Paris, France) is offering multiple postdoc positions to work on the theory of quantum computing. Emphasis is on the development of quantum algorithms for optimization, machine learning, massive data, and cryptography. You will be working with permanent members S. Apers, I. Kerenidis, S. Laplante and F. Magniez, and a strong team of postdocs and PhDs. Starting date: around September 2022.</p>
<p>Website: <a href="https://www.irif.fr/postes/postdoc#quantum-computing">https://www.irif.fr/postes/postdoc#quantum-computing</a><br/>
Email: simon.apers@inria.fr</p></div>
    </content>
    <updated>2021-11-09T14:43:53Z</updated>
    <published>2021-11-09T14:43:53Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-11T18:37:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/153</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/153" rel="alternate" type="text/html"/>
    <title>TR21-153 |  On Hardness Assumptions Needed for &amp;quot;Extreme High-End&amp;quot; PRGs and Fast Derandomization | 

	Ronen Shaltiel, 

	Emanuele Viola</title>
    <summary>The hardness vs.~randomness paradigm aims to explicitly construct pseudorandom generators $G:\{0,1\}^r \to \{0,1\}^m$ that fool circuits of size $m$, assuming the existence of explicit hard functions. A ``high-end PRG'' with seed length $r=O(\log m)$ (implying BPP=P) was achieved in a seminal work of Impagliazzo and Wigderson (STOC 1997), assuming \textsc{the high-end hardness assumption}: there exist constants $0&lt;\beta &lt; 1&lt; B$, and functions computable in time $2^{B \cdot n}$ that cannot be computed by circuits of size $2^{\beta \cdot n}$.

Recently, motivated by fast derandomization of randomized algorithms, Doron et al.~(FOCS 2020) and Chen and Tell (STOC 2021), construct ``extreme high-end PRGs'' with seed length $r=(1+o(1))\cdot \log m$, under qualitatively stronger assumptions.

We study whether extreme high-end PRGs can be constructed from the corresponding hardness assumption in which $\beta=1-o(1)$ and $B=1+o(1)$, which we call \textsc{the extreme high-end hardness assumption}. We give a partial negative answer:

\begin{itemize}
\item The construction of Doron et al. composes a PEG (pseudo-entropy generator) with an extractor.  The PEG is constructed starting from a function that is hard for MA-type circuits.  We show that black-box PEG constructions from \textsc{the extreme high-end hardness assumption} must have large seed length (and so cannot be used to obtain extreme high-end PRGs by applying an extractor).

To prove this, we establish a new property of (general) black-box PRG constructions from hard functions: it is possible to fix many output bits of the construction while fixing few bits of the hard function. This property distinguishes PRG constructions from typical extractor constructions, and this may explain why it is difficult to design PRG constructions.

\item The construction of Chen and Tell composes two PRGs: $G_1:\{0,1\}^{(1+o(1)) \cdot \log m} \to \{0,1\}^{r_2=m^{\Omega(1)}}$ and $G_2:\{0,1\}^{r_2} \to \{0,1\}^m$.  The first PRG is constructed from \textsc{the extreme high-end hardness assumption}, and the second PRG needs to run in time $m^{1+o(1)}$, and is constructed assuming one way functions. We show that in black-box proofs of hardness amplification to $\frac{1}{2}+1/m$, reductions must make $\Omega(m)$ queries, even in the extreme high-end. Known PRG constructions from hard functions are black-box and use (or imply) hardness amplification, and so cannot be used to construct a PRG $G_2$ from \textsc{the extreme high-end hardness assumption}.

The new feature of our hardness amplification result is that it applies even to the extreme high-end setting of parameters, whereas past work does not. Our techniques also improve recent lower bounds of Ron-Zewi, Shaltiel and Varma (ITCS 2021) on the number of queries of local list-decoding algorithms.
\end{itemize}</summary>
    <updated>2021-11-09T11:08:53Z</updated>
    <published>2021-11-09T11:08:53Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-11T18:37:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/152</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/152" rel="alternate" type="text/html"/>
    <title>TR21-152 |  Min-Entropic Optimality | 

	Tomer Grossman, 

	Gal Arnon</title>
    <summary>We introduce the notion of \emph{Min-Entropic Optimality} thereby providing a framework for arguing that a given algorithm computes a function better than any other algorithm. An algorithm is $k(n)$ Min-Entropic Optimal if for every distribution $D$ with min-entropy at least $k(n)$, its expected running time when its input is drawn from $D$ is at most a multiplicative constant larger than the expected running time (also with respect to $D$) of any other algorithm that computes the same function. Min-Entropic Optimality is a relaxation of the well established notion of instance optimality (when $k(n) = 0$). Thereby, Min-Entropic Optimality provides a meaningful notion of optimality, even in scenarios where instance optimality is inherently impossible to achieve (for instance, in the super-linear regime).

We analyze basic properties of this notion and prove that for many values of $k(n)$ there exist functions that have Min-Entropic Optimal algorithms. We further show that some natural search problems, such as $k$-sum, are unlikely to have optimal algorithms under this notion.</summary>
    <updated>2021-11-09T11:01:40Z</updated>
    <published>2021-11-09T11:01:40Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-11T18:37:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=19306</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/11/08/the-artificial-intelligence-historian/" rel="alternate" type="text/html"/>
    <title>The Artificial Intelligence Historian</title>
    <summary>The past actually happened but history is only what someone wrote down—Whitney Brown CMU tribute Pamela McCorduck passed away last month. The New York Times obituary notes her interactions with many builders of the field of Artificial Intelligence from its infancy to its present state. Today we remember her and talk about AI’s near future. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>The past actually happened but history is only what someone wrote down—Whitney Brown</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/11/08/the-artificial-intelligence-historian/mccorduck-obit-700x700-min/" rel="attachment wp-att-19317"><img alt="" class="alignright wp-image-19317" height="156" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/mccorduck-obit-700x700-min.jpeg?resize=125%2C156&amp;ssl=1" width="125"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">CMU <a href="https://www.cmu.edu/news/stories/archives/2021/october/mccorduck-obit.html">tribute</a></font></td>
</tr>
</tbody>
</table>
<p>
Pamela McCorduck passed away last month. The New York Times <a href="https://www.nytimes.com/2021/11/04/technology/pamela-mccorduck-dead.html">obituary</a> notes her interactions with many builders of the field of Artificial Intelligence from its infancy to its present state. </p>
<p>
Today we remember her and talk about AI’s near future.</p>
<p>
I knew McCorduck through her late husband, Joe Traub, who we <a href="https://rjlipton.wpcomstaging.com/2015/08/31/how-joe-traub-beat-the-street/">memorialized</a> in 2015. He became the head of the CS department at Carnegie Mellon University in 1971. She also moved to CMU where she became an English teacher. Per the above quote by Brown, she helped make AI real by writing a number of books on its history. </p>
<p>
The NYT obit quotes something McCorduck wrote in her 2019 <a href="https://press.etc.cmu.edu/index.php/product/this-could-be-important/">memoir</a>, <em>This Could Be Important: My Life and Times With the Artificial Intelligentsia.</em> </p>
<blockquote><p><b> </b> <em> “For 60 years, I’ve lived in AI’s exponential. I’ve watched computers evolve from plodding sorcerer’s apprentices to machines that can best any humans at checkers, then chess, then the guessing game Jeopardy!, and now the deeply complex game of Go.” </em>
</p></blockquote>
<p/><p>
It is hard to project the future of an exponential, however. The best way I can try is to align it with my own field.</p>
<p>
</p><p/><h2> AI Movers and Shakers </h2><p/>
<p/><p>
At CMU McCorduck got to know the AI pioneers like Turing Award recipients Herbert Simon and Allen Newell and Raj Reddy. She already knew Edward Feigenbaum who said:</p>
<blockquote><p><b> </b> <em> She was dumped into this saturated milieu of the great and greatest in AI at Carnegie Mellon—some of the same people whose papers she’d helped us assemble—and decided to write a history of the field. </em>
</p></blockquote>
<p/><p>
The <a href="https://www.routledge.com/Machines-Who-Think-A-Personal-Inquiry-into-the-History-and-Prospects-of/McCorduck/p/book/9781568812052">book</a> was <em>Machines Who Think: A Personal Inquiry Into the History and Prospects of Artificial Intelligence.</em> Said Simon: </p>
<blockquote><p><b> </b> <em> She was interacting with all the movers and shakers of AI. She was in the middle of it, an eyewitness to history. </em>
</p></blockquote>
<p/><p>
I wish I knew more of her thoughts on the movers and shakers in Theory. She was well-versed in complexity of the dynamical-systems kind, and her husband’s work bridged to “our kind” of complexity. The title of her third <a href="https://www.amazon.com/Bounded-Rationality-Novel-Pamela-McCorduck/dp/0865348839">novel</a>, <em>Bounded Rationality</em>, speaks to both kinds of complexity from its setting at the Santa Fe Institute. This has led me to musing on the difference between AI and Theory.</p>
<p>
</p><p/><h2> AI Beats Theory </h2><p/>
<p/><p>
I never have worked on AI problems of any kind. The closest I ever came is I took a class at CMU as graduate student from Newell. He was a fun lecturer and the class was interesting. But I always worked on Theory. I must reflect a bit on why AI is so successful and Theory is less so. </p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/11/08/the-artificial-intelligence-historian/see/" rel="attachment wp-att-19311"><img alt="" class="aligncenter size-full wp-image-19311" height="195" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/11/see.png?resize=258%2C195&amp;ssl=1" width="258"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Pinterest <a href="https://www.pinterest.com/pin/55872851604309287/">source</a></font>
</td>
</tr>
</tbody></table>
<p>
Let’s start by saying that a field of research is determined not by who works in the field. Not by the tools that the field uses. It is determined by the problems that the field works on. AI is different from Theory because of the problems that it studies. This is the fundamental difference:</p>
<blockquote><p><b> </b> <em> AI looks at whole problems; Theory looks at sub-problems. </em>
</p></blockquote>
<p/><p>
What do I mean? AI studies problems that are concrete, that are big, that are as close to real problems as possible. For example, how to play Go or how to recognize images of faces. AI looks at problems that humans actually wish to solve: </p>
<blockquote><p><b> </b> <em> What move to make in this Go position? Or is this an image of X or Y? </em>
</p></blockquote>
<p/><p>
Theory looks at sub-problems. We look at a real problem and then identify some part of the problem that is hard to solve. We then try to invoke clever methods that show that this sub-problem can be done more efficiently that was previous known. This is hard in general. Is fun to work on in general. And leads to a beautiful field of study. One that is deep and rewarding. </p>
<p>
But Theory loses to AI. The issue is that no one really may wish to solve the sub-problem. This is real demand to solve the whole problem, but not the sub-problem. This is the fundamental advantage that AI holds over Theory.</p>
<p>
</p><p/><h2> AI Future </h2><p/>
<p/><p>
Public figures such as Stephen Hawking and Elon Musk have expressed concern that full artificial intelligence (AI) could result in human extinction. The consequences of the technological <a href="https://en.wikipedia.org/wiki/Technological_singularity">singularity</a> and its potential benefit or harm to the human race have been intensely debated.</p>
<p>
For our own part, we have <a href="https://rjlipton.wpcomstaging.com/2011/02/17/are-mathematicians-in-jeopardy/">wondered</a> whether an AI can take over in theory research. This puts a second light on possible meanings of “problems” in another quotation by McCorduck from her memoir, as related <a href="https://computerhistory.org/blog/the-future-humans-and-ai/">here</a>:</p>
<blockquote><p><b> </b> <em> “We can’t now say what living beside other, in some ways superior, intelligences will mean to us. Will it widen and raise our own individual and collective intelligence? In significant ways, it already has. Find solutions to problems we could never solve? Probably. Find solutions to problems we lack the wit even to propose? Maybe. Cause problems? Surely. AI has already shattered some of our fondest myths about ourselves and has shone unwelcome light on others. This will continue.</em></p><em>
<p>
…</p>
</em><p><em>
When people ask me my greatest worry about AI, I say: what we aren’t smart enough even to imagine.” </em>
</p></blockquote>
<p/><p>
Well, we can only talk about things we can imagine now. We can discuss facets of life that already outsource decisions to technology, such as high-speed stock trading and <a href="https://en.wikipedia.org/wiki/2010_flash_crash">several</a> <a href="https://www.technologyreview.com/2016/10/07/244656/algorithms-probably-caused-a-flash-crash-of-the-british-pound/">flash</a>–<a href="https://www.motherjones.com/politics/2013/02/high-frequency-trading-danger-risk-wall-street/">crashes</a> it has caused. </p>
<p>
But looking ahead, what is one near-term application area as a litmus test for the impact of AI? We think many will agree with our looking to <em>self-driving cars</em>. In taking over driving decisions, the AI expressly aims to reduce the evils of impaired or aggressive drivers. There have been <a href="https://www.washingtonpost.com/technology/2021/11/08/tesla-regulation-elon-musk/">mishaps</a> during development, sure, and the algorithms have not yet demonstrated robustness against possible deceptions. That is to say:</p>
<ul>
<li>
We can already see teething problems with this tech and imagine more along the same lines. <p/>
</li><li>
But can we project structural problems with the driverless paradigm whose concrete forms we have not imagined?
</li></ul>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
What are your thoughts on the near future of AI? </p>
<p>
Our condolences go out to Pamela’s family and associates.</p>
<p/></font></font></div>
    </content>
    <updated>2021-11-08T22:23:34Z</updated>
    <published>2021-11-08T22:23:34Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="News"/>
    <category term="People"/>
    <category term="AI"/>
    <category term="artificial intelligence"/>
    <category term="in memoriam"/>
    <category term="Pamela McCorduck"/>
    <category term="singularity"/>
    <category term="Theory"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-11-11T18:37:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/151</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/151" rel="alternate" type="text/html"/>
    <title>TR21-151 |  Locally Testable Codes with constant rate, distance, and locality  | 

	Irit Dinur, 

	Shai Evra, 

	Ron Livne, 

	Alexander Lubotzky, 

	Shahar Mozes</title>
    <summary>A locally testable code (LTC) is an error correcting code that has a property-tester. The tester reads $q$ bits that are randomly chosen, and rejects words with probability proportional to their distance from the code. The parameter $q$ is called the locality of the tester.

LTCs were initially studied as important components of PCPs, and since then the topic has evolved on its own. High rate LTCs could be useful in practice: before attempting to decode a received word, one can save time by first quickly testing if it is close to the code.

An outstanding open question has been whether there exist "$c^3$-LTCs", namely LTCs with *c*onstant rate, *c*onstant distance, and *c*onstant locality.

In this work we construct such codes based on a new two-dimensional complex which we call a left-right Cayley complex. This is essentially a graph which, in addition to vertices and edges, also has squares. Our codes can be viewed as a two-dimensional version of (the one-dimensional) expander codes, where the codewords are functions on the squares rather than on the edges.</summary>
    <updated>2021-11-08T20:01:24Z</updated>
    <published>2021-11-08T20:01:24Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-11T18:37:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8225</id>
    <link href="https://windowsontheory.org/2021/11/08/new-quantum-science-and-engineering-phd-program/" rel="alternate" type="text/html"/>
    <title>New Quantum Science and Engineering PhD Program</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">[Please forward this information to any undergraduate students in your program, or any relevant mailing lists or organizations you are aware of; we also have a postdoctoral fellowship in quantum computing. –Boaz] This year Harvard started a new Ph.D program in Quantum Science and Engineering. We are now accepting application for the first cohort of … <a class="more-link" href="https://windowsontheory.org/2021/11/08/new-quantum-science-and-engineering-phd-program/">Continue reading <span class="screen-reader-text">New Quantum Science and Engineering PhD Program</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>[Please forward this information to any undergraduate students in your program, or any relevant mailing lists or organizations you are aware of; we also have <a href="https://quantum.harvard.edu/external-candidates">a postdoctoral fellowship in quantum computing</a>. –Boaz]</em></p>



<p>This year Harvard started a new <a href="https://gsas.harvard.edu/programs-of-study/all/quantum-science-and-engineering">Ph.D program in Quantum Science and Engineering</a>. We are now accepting application for the first cohort of students which will start in the 2022-2023 academic year.  This program can be an excellent fit for CS majors that are interested in quantum computation and information, and are looking for an interdisciplinary environment, with students from CS, physics, and other backgrounds, and with a curriculum and program that is designed with quantum computing in mind. Students can apply to both the QSE program and the <a href="https://gsas.harvard.edu/programs-of-study/all/computer-science">CS PhD program</a>, in parallel.</p>



<p>Some more information below:</p>



<p>The <strong><a href="https://gsas.harvard.edu/programs-of-study/all/quantum-science-and-engineering">Harvard Quantum Science and Engineering (QSE) PhD program</a></strong> is designed for students like yours, as well as those studying engineering, physics and chemistry. The program brings these students from diverse undergraduate programs together through a world-class, integrated QSE PhD program that uniquely prepares them to become intellectual leaders and innovators in the burgeoning field of QSE. </p>



<p><strong>The curriculum. </strong>The integrated curriculum provides a shared foundation and QSE language that enables students to make discoveries and collaborate fluently beyond traditional disciplinary boundaries. Students enjoy the freedom to broadly explore their interests, specialize in their area of greatest interest, and can choose PhD advisors from across all quantum departments including computer science, physics, engineering, and chemistry.</p>



<p><strong>The community. </strong>The Harvard QSE PhD program is built around a supportive environment and collaborative research community that helps nurture each student and ensure their success. It’s an unprecedented opportunity to work with world leaders in the field of QSE in state-of-the-art educational and computational facilities while participating in cutting-edge research. </p>



<p><strong>The opportunity. </strong>Graduates of the program will be trained as the next generation of world leaders in the field of QSE. With their broad, yet deep educational foundation, guided by their own interests, they’ll be ready to take on exciting roles in industry, academia, and national laboratories. </p>



<p>Deadline to apply is <strong>December 15, 2021</strong>. Apply via <a href="https://gsas.harvard.edu/programs-of-study/all/quantum-science-and-engineering" rel="noreferrer noopener" target="_blank">https://gsas.harvard.edu/programs-of-study/all/quantum-science-and-engineering</a> . Students can also apply in parallel to the CS PhD program via <a href="https://gsas.harvard.edu/programs-of-study/all/computer-science" rel="noreferrer noopener" target="_blank">https://gsas.harvard.edu/programs-of-study/all/computer-science</a> . Email <a href="mailto:qse-admissions@fas.harvard.edu" rel="noreferrer noopener" target="_blank">qse-admissions@fas.harvard.edu</a> with any questions.</p>



<p/></div>
    </content>
    <updated>2021-11-08T15:43:33Z</updated>
    <published>2021-11-08T15:43:33Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-11-11T18:38:03Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-8890204.post-6759625364649289630</id>
    <link href="http://mybiasedcoin.blogspot.com/feeds/6759625364649289630/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=8890204&amp;postID=6759625364649289630" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/6759625364649289630" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/6759625364649289630" rel="self" type="application/atom+xml"/>
    <link href="http://mybiasedcoin.blogspot.com/2021/11/postdoc-call-for-fodsi.html" rel="alternate" type="text/html"/>
    <title>Postdoc call for FODSI</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>As a member of FODSI (Foundations of Data Science Institute -- an NSF funded institute with the aim of advancing theoretical foundations for data science), I'm self-interestedly posting the call for postdocs for this year.  Two of the areas are  a) Sketching, Sampling, and Sublinear-Time Algorithms   and b)  Machine Learning for Algorithms (which includes what I call "Algorithms with Predictions.")  I'd be happy to see postdoc applications in those areas from people who want to spend some time at Harvard, for example.... but of course there are lots of other exciting things going on with FODSI too and you should take a look.</p><p>The call is at <a href="https://academicjobsonline.org/ajo/jobs/20132">https://academicjobsonline.org/ajo/jobs/20132</a>  </p><p>Call text below:</p><table align="center" border="1" class="ads" style="border-collapse: collapse; border: 1px solid rgb(204, 204, 204); color: #2a2a2a; font-family: Verdana, Arial, Helvetica, sans-serif; font-size: 13.3333px; padding: 5px; width: 95%px;"><tbody><tr><td style="border-collapse: collapse; border: 1px solid rgb(204, 204, 204); padding: 5px;">The Foundations of Data Science Institute (FODSI), funded by the National Science Foundation TRIPODS program, is announcing a competitive postdoctoral fellowship. FODSI is a collaboration between UC Berkeley and MIT, partnering with Boston University, Northeastern University, Harvard University, Howard University and Bryn Mawr College. It provides a structured environment for exploring interdisciplinary research in foundations of Data Science spanning Mathematics, Statistics, Theoretical Computer Science and other fields.<p>We are looking for multiple postdoctoral team members who will collaborate with <a href="https://fodsi.us/team.html" style="color: #005f6f; font-weight: 700;">FODSI researchers</a> at one or more of the participating institutions. These positions emphasize strong mentorship, flexibility, and breadth of collaboration opportunities with other team members -- senior and junior faculty, postdocs, and graduate students at various nodes around the country. Furthermore, postdoctoral fellows will be able to participate in workshops and other activities organized by FODSI.</p><p>The fellowship is a one-year full-time appointment, with the possibility of renewal for a second year (based upon mutual agreement) either at the same or at another FODSI institution. The start date is flexible, although most appointments are expected to start in summer 2022. Candidates are encouraged to apply to work with more than one faculty mentor <b>at one or more participating institutions</b> (in-person mentoring is preferred, but remote options will be also considered). The applicants should have an excellent theoretical background and a doctorate in a related field, including Mathematics, Statistics, Computer Science, Electrical Engineering or Economics. We particularly encourage applications from women and minority candidates.</p><p>The review process will start on November 15, 2021 and will continue until positions are filled.</p></td></tr></tbody></table></div>
    </content>
    <updated>2021-11-08T14:45:00Z</updated>
    <published>2021-11-08T14:45:00Z</published>
    <author>
      <name>Michael Mitzenmacher</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/02161161032642563814</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-8890204</id>
      <category term="conferences"/>
      <category term="research"/>
      <category term="society"/>
      <category term="algorithms"/>
      <category term="administration"/>
      <category term="teaching"/>
      <category term="Harvard"/>
      <category term="papers"/>
      <category term="graduate students"/>
      <category term="funding"/>
      <category term="talks"/>
      <category term="blogs"/>
      <category term="codes"/>
      <category term="jobs"/>
      <category term="reviews"/>
      <category term="personal"/>
      <category term="travel"/>
      <category term="undergraduate students"/>
      <category term="books"/>
      <category term="open problems"/>
      <category term="PCs"/>
      <category term="consulting"/>
      <category term="randomness"/>
      <category term="CCC"/>
      <category term="blog book project"/>
      <category term="research labs"/>
      <category term="ISIT"/>
      <category term="tenure"/>
      <category term="comments"/>
      <category term="recommendations"/>
      <category term="outreach"/>
      <category term="students"/>
      <author>
        <name>Michael Mitzenmacher</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06738274256402616703</uri>
      </author>
      <link href="http://mybiasedcoin.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://mybiasedcoin.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">My take on computer science -- <br/> 
algorithms, networking, information theory -- <br/> 
and related items.</div>
      </subtitle>
      <title>My Biased Coin</title>
      <updated>2021-11-10T02:05:19Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6799651120968614396</id>
    <link href="http://blog.computationalcomplexity.org/feeds/6799651120968614396/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/11/reflections-on-trusting-trustlessness.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/6799651120968614396" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/6799651120968614396" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2021/11/reflections-on-trusting-trustlessness.html" rel="alternate" type="text/html"/>
    <title>Reflections on Trusting ``Trustlessness'' in the era of ``Crypto'' Blockchains (Guest Post)</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> </p><p><i>I trust Evangelos Georgiadis to do a guest post on Trust and Blockchain. </i></p><div>Today we have a guest post by Evangelos Georgiadis on Trust. It was written before Lance's post on trust <a href="https://blog.computationalcomplexity.org/2021/08/trusting-scientists.html">here</a> but it can be viewed as a followup to it. </div><div><br/></div><div>And now, here's E.G:</div><div><div><br/></div><div>==========================================================</div><div><br/></div><div>Trust is a funny concept, particularly in the realm of blockchains and "crypto".</div><div><br/></div><div>Do you trust the consensus mechanism of a public blockchain?</div><div><br/></div><div>Do you trust the architects that engineered the consensus mechanism?</div><div><br/></div><div>Do you trust the software engineers that implemented the code for the consensus mechanism?</div><div><br/></div><div>Do you trust the language that the software engineers used?</div><div><br/></div><div>Do you trust the underlying hardware that that the software is running?</div><div><br/></div><div>Theoretical Computer Science provides tools for some of this. But then the question becomes</div><div>Do you trust the program verifier?</div><div>Do you trust the proof of security?</div><div><br/></div><div>I touch on these issues in: </div><div><br/></div><div>                   <i>Reflections on Trusting ‘Trustlessness’ in the era of ”Crypto”/Blockchains</i></div><div><br/></div><div> which is <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/cbit-4-2.pdf">here</a>. Its only 3 pages so enjoy!</div></div></div>
    </content>
    <updated>2021-11-07T20:47:00Z</updated>
    <published>2021-11-07T20:47:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-11-11T15:44:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/150</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/150" rel="alternate" type="text/html"/>
    <title>TR21-150 |  Extractors: Low Entropy Requirements Colliding With Non-Malleability | 

	Eldon Chung, 

	Maciej Obremski, 

	Divesh Aggarwal</title>
    <summary>The known constructions of negligible error (non-malleable) two-source extractors can be broadly classified in three categories:

(1) Constructions where one source has min-entropy rate about $1/2$, the other source can have small min-entropy rate, but the extractor doesn't guarantee non-malleability.
(2) Constructions where one source is uniform, and the other can have small min-entropy rate, and the extractor guarantees non-malleability when the uniform source is tampered.
(3) Constructions where both sources have entropy rate very close to $1$ and the extractor guarantees non-malleability against the tampering of both sources. 

We introduce a new notion of collision resistant extractors and in using it we obtain a strong two source non-malleable extractor where we require the first source to have $0.8$ entropy rate and the other source can have min-entropy polylogarithmic in the length of the source.  

We show how the above extractor can be applied to obtain a non-malleable extractor with output rate $\frac 1 2$, which is optimal. We also show how, by using our extractor and extending the known protocol, one can  obtain a privacy amplification secure against memory tampering where the size of the secret output is almost optimal.</summary>
    <updated>2021-11-07T20:13:16Z</updated>
    <published>2021-11-07T20:13:16Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-11T18:37:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/149</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/149" rel="alternate" type="text/html"/>
    <title>TR21-149 |  On polynomially many queries to NP or QMA oracles | 

	Dorian Rudolph, 

	Sevag Gharibian</title>
    <summary>We study the complexity of problems solvable in deterministic polynomial time with access to an NP or Quantum Merlin-Arthur (QMA)-oracle, such as $P^{NP}$ and $P^{QMA}$, respectively.
The former allows one to classify problems more finely than the Polynomial-Time Hierarchy (PH), whereas the latter characterizes physically motivated problems such as Approximate Simulation (APX-SIM) [Ambainis, CCC 2014].
In this area, a central role has been played by the classes $P^{NP[\log]}$ and $P^{QMA[\log]}$, defined identically to $P^{NP}$ and $P^{QMA}$, except that only logarithmically many oracle queries are allowed. Here, [Gottlob, FOCS 1993] showed that if the adaptive queries made by a $P^{NP}$ machine have a "query graph" which is a tree, then this computation can be simulated in $P^{NP[\log]}$.

 In this work, we first show that for any verification class $C\in\{NP,MA,QCMA,QMA,QMA(2),NEXP,QMA_{\exp}\}$, any $P^C$ machine with a query graph of "separator number" $s$ can be simulated using deterministic time $\exp(s\log n)$ and $s\log n$ queries to a $C$-oracle.
When $s\in O(1)$ (which includes the case of $O(1)$-treewidth, and thus also of trees), this gives an upper bound of $P^{C[\log]}$, and when $s\in O(\log^k(n))$, this yields bound $QP^{C[\log^{k+1}]}$ (QP meaning quasi-polynomial time).
We next show how to combine Gottlob's "admissible-weighting function" framework with the "flag-qubit" framework of [Watson, Bausch, Gharibian, 2020], obtaining a unified approach for embedding $P^C$ computations directly into APX-SIM instances in a black-box fashion.
Finally, we formalize a simple no-go statement about polynomials (c.f. [Krentel, STOC 1986]): Given a multi-linear polynomial $p$ specified via an arithmetic circuit, if one can "weakly compress" $p$ so that its optimal value requires $m$ bits to represent, then $P^{NP}$ can be decided with only $m$ queries to an NP-oracle.</summary>
    <updated>2021-11-07T12:55:09Z</updated>
    <published>2021-11-07T12:55:09Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-11-11T18:37:30Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2021/11/07/ideal-mini-workshop-on-new-directions-on-robustness-in-ml/</id>
    <link href="https://cstheory-events.org/2021/11/07/ideal-mini-workshop-on-new-directions-on-robustness-in-ml/" rel="alternate" type="text/html"/>
    <title>IDEAL mini-workshop on “New Directions on Robustness in ML”</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">November 16, 2021 Virtual https://www.ideal.northwestern.edu/events/mini-workshop-on-new-directions-on-robustness-in-ml/ As machine learning systems are being deployed in almost every aspect of decision-making, it is vital for them to be reliable and secure to adversarial corruptions and perturbations of various kinds. This workshop will explore newer notions of robustness and the different challenges that arise in designing reliable ML algorithms. … <a class="more-link" href="https://cstheory-events.org/2021/11/07/ideal-mini-workshop-on-new-directions-on-robustness-in-ml/">Continue reading <span class="screen-reader-text">IDEAL mini-workshop on “New Directions on Robustness in ML”</span></a></div>
    </summary>
    <updated>2021-11-07T04:05:12Z</updated>
    <published>2021-11-07T04:05:12Z</published>
    <category term="workshop"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2021-11-11T18:38:34Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://ptreview.sublinear.info/?p=1584</id>
    <link href="https://ptreview.sublinear.info/2021/11/news-for-october-2021/" rel="alternate" type="text/html"/>
    <title>News for October 2021</title>
    <summary>The month of September was quite busy, with seven papers, spanning (hyper)graphs, proofs, probability distributions, and sampling. Better Sum Estimation via Weighted Sampling, by Lorenzo Beretta and Jakub Tětek (arXiv). This paper considers the following question: “given a large universe of items, each with an unknown weight, estimate the total weight to a multiplicative \(1\pm […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The month of September was quite busy, with seven papers, spanning (hyper)graphs, proofs, probability distributions, and sampling.</p>



<p><strong>Better Sum Estimation via Weighted Sampling</strong>, by Lorenzo Beretta and Jakub Tětek (<a href="https://arxiv.org/abs/2110.14948">arXiv</a>). This paper considers the following question: “given a large universe of items, each with an unknown weight, estimate the total weight to a multiplicative \(1\pm \varepsilon\).” The key is in the type of access you have to those items: here, the authors consider the setting where items can be sampled proportionally to their unknown weights, and show improved bounds on the sample/query complexity in this model. And there something for everyone: they also discuss connections to edge estimation in graphs (assuming random edge queries) and to distribution testing (specifically, in the “dual” or “probability-revealing” models of Canonne–Rubinfeld and Onak–Sun).</p>



<p>This gives us an easy segue to distribution testing, which is the focus of the next two papers.</p>



<p><strong>As Easy as ABC: Adaptive Binning Coincidence Test for Uniformity Testing</strong>, by Sudeep Salgia, Qing Zhao, and Lang Tong (<a href="https://arxiv.org/abs/2110.06325">arXiv</a>). Most of the work in distribution testing (from the computer science community) focuses on discrete probability distributions, for several reasons. Including a technical one: total variation distance is rather fickle with continuous distributions, unless one makes some assumption on the unknown distribution. This paper does exactly this: assuming the unknown distribution has a Lipschitz density function, it shows how to test uniformity by adaptively discretizing the domain, achieving (near) sample complexity.</p>



<p><strong>Exploring the Gap between Tolerant and Non-tolerant Distribution Testing,</strong> by Sourav Chakraborty, Eldar Fischer, Arijit Ghosh, Gopinath Mishra, and Sayantan Sen (<a href="https://arxiv.org/abs/2110.09972">arXiv</a>). It is known that tolerant testing of distributions can be much harder than “standard” testing – for instance, for identity testing, the sample complexity can blow up by nearly a quadratic factor, from \(\sqrt{n}\) to \(\frac{n}{\log n}\)! But is it the worse that can happen, in general, for other properties? This work explores this question, and answers it in some notable cases of interest, such as for label-invariant (symmetric) properties.</p>



<p>And now, onto graphs!</p>



<p><strong>Approximating the Arboricity in Sublinear Time</strong>, by Talya Eden, Saleet Mossel, and Dana Ron (<a href="https://arxiv.org/abs/2110.15260">arXiv</a>). The arboricity of a graph is the minimal number of spanning forests required to cover all its edges. Many graph algorithms, especially sublinear-time ones, can be parameterized by this quantity: which is very useful, but what do you do if you don’t know the arboricity of your graph? Well, then you estimate it. Which this paper shows how to do efficiently, given degree and neighbor queries. Moreover, the bound they obtain — \(\tilde{O}(n/\alpha)\) queries to obtain a constant-factor approximation of the unknown arboricity \(\alpha\) — is optimal, up to logarithmic factors in the number of vertices \(n\).</p>



<p><strong>Sampling Multiple Nodes in Large Networks: Beyond Random Walks,</strong> by Omri Ben-Eliezer, Talya Eden, Joel Oren, and Dimitris Fotakis (<a href="https://arxiv.org/abs/2110.13324">arXiv</a>). Another thing which one typically wants to do with very large graphs is <em>sample nodes</em> from them, either uniformly or according to some prescribed distribution. This is a core building block in many other algorithms; unfortunately, approaches to do so via random walks will typically require a number of queries scaling with the mixing time \(t_{\rm mix}(G)\) of the graph \(G\), which might be very small for nicely expanding graphs, but not so great in many practical settings. This paper proposes and experimentally evaluates a different algorithm which bypasses this linear dependence on \(t_{\rm mix}(G)\), by first going through a random-walk-based “learning” phase (learn something about the structure of the graph) before using this learned structure to perform faster sampling, focusing on small connected components.</p>



<p>Why stop at graphs? <em>Hypergraphs</em>!</p>



<p><strong>Hypergraph regularity and random sampling,</strong> by Felix Joos, Jaehoon Kim, Daniela Kühn, Deryk Osthus (<a href="https://arxiv.org/abs/2110.01570">arXiv</a>). The main result in this paper is a hypergraph analogue of a result of Alon, Fischer, Newman and Shapira (for graphs), which roughly states that if a hypergraph satisfies some regularity condition, then so does with high probability a randomly sampled sub-hypergraph — and conversely. This in turn has direct implications to characterizing which hypergraph properties are testable: see the <a href="https://arxiv.org/abs/1707.03303">companion paper</a>, <em>b</em>y the same authors.<em><br/>(Note: this paper is a blast from the past, as the result it shows was originally established in the linked companion paper, from 2017; however, the authors split this paper in two this October, leading to this new, standalone paper.)</em></p>



<p>And, to conclude, Arthur, Merlin, and proofs:</p>



<p><strong>Sample-Based Proofs of Proximity,</strong> by Guy Goldberg, Guy Rothblum (<a href="https://eccc.weizmann.ac.il/report/2021/146/">ECCC</a>). Finally, consider the setting of interactive proofs of proximities (IPPs), where the prover is as usual computationally unbounded, but the verifier must run in sublinear time (à la property testing). This has received significant interest in the past years: but what if the verifier didn’t even get to make queries, but only got access to <em>uniformly random location</em>s of the input? These “SIPP” (Sample-based IPPs), and their non-interactive counterpart SAMPs (Sample-based Merlin-Arthur Proofs of Proximity) are the object of study of this paper, which it introduces and motivates in the context, for instance, of delegation of computation for sample-based algorithms.</p></div>
    </content>
    <updated>2021-11-07T02:17:23Z</updated>
    <published>2021-11-07T02:17:23Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Clement Canonne</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2021-11-10T23:19:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/05/postdoc-at-foundations-of-data-science-institute-fodsi-apply-by-november-15-2021/</id>
    <link href="https://cstheory-jobs.org/2021/11/05/postdoc-at-foundations-of-data-science-institute-fodsi-apply-by-november-15-2021/" rel="alternate" type="text/html"/>
    <title>Postdoc at Foundations of Data Science Institute (FODSI) (apply by November 15, 2021)</title>
    <summary>The Foundations of Data Science Institute (FODSI), funded by the National Science Foundation TRIPODS program, is announcing a competitive postdoctoral fellowship. Multiple positions are available. FODSI is a collaboration between UC Berkeley and MIT, partnering with Boston University, Northeastern University, Harvard University, Howard University and Bryn Mawr College. Website: https://academicjobsonline.org/ajo/jobs/20132 Email: See the url</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Foundations of Data Science Institute (FODSI), funded by the National Science Foundation TRIPODS program, is announcing a competitive postdoctoral fellowship. Multiple positions are available. FODSI is a collaboration between UC Berkeley and MIT, partnering with Boston University, Northeastern University, Harvard University, Howard University and Bryn Mawr College.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/20132">https://academicjobsonline.org/ajo/jobs/20132</a><br/>
Email: See the url</p></div>
    </content>
    <updated>2021-11-05T22:17:02Z</updated>
    <published>2021-11-05T22:17:02Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-11T18:37:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/11/05/postdoc-at-computer-science-university-of-victoria-apply-by-november-20-2021/</id>
    <link href="https://cstheory-jobs.org/2021/11/05/postdoc-at-computer-science-university-of-victoria-apply-by-november-20-2021/" rel="alternate" type="text/html"/>
    <title>Postdoc at Computer Science, University of Victoria (apply by November 20, 2021)</title>
    <summary>Bruce Kapron invites applications for a postdoc in CS at the University of Victoria. Applicants with a background or interest in higher-order complexity theory, including models and techniques related to theory of programming languages, feasible analysis, cryptography, and ordinary complexity theory are encouraged. Applicants should have a Ph.D. in CS, Mathematics, Logic or a related […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Bruce Kapron invites applications for a postdoc in CS at the University of Victoria. Applicants with a background or interest in higher-order complexity theory, including models and techniques related to theory of programming languages, feasible analysis, cryptography, and ordinary complexity theory are encouraged. Applicants should have a Ph.D. in CS, Mathematics, Logic or a related field.</p>
<p>Website: <a href="https://www.mathjobs.org/jobs/list/18864">https://www.mathjobs.org/jobs/list/18864</a><br/>
Email: bmkapron@uvic.ca</p></div>
    </content>
    <updated>2021-11-05T16:54:41Z</updated>
    <published>2021-11-05T16:54:41Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-11-11T18:37:47Z</updated>
    </source>
  </entry>
</feed>
