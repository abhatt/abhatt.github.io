<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-06-09T20:23:50Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/086</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/086" rel="alternate" type="text/html"/>
    <title>TR19-086 |  Perfect zero knowledge for quantum multiprover interactive proofs | 

	Alex Bredariol Grilo, 

	William Slofstra, 

	Henry Yuen</title>
    <summary>In this work we consider the interplay between multiprover interactive proofs, quantum
entanglement, and zero knowledge proofs — notions that are central pillars of complexity theory,
quantum information and cryptography. In particular, we study the relationship between the
complexity class MIP$^*$ , the set of languages decidable by multiprover interactive proofs with
quantumly entangled provers, and the class PZK-MIP$^*$ , which is the set of languages decidable
by MIP$^*$ protocols that furthermore possess the perfect zero knowledge property.

Our main result is that the two classes are equal, i.e., MIP$^*$ = PZK-MIP$^*$ . This result provides
a quantum analogue of the celebrated result of Ben-Or, Goldwasser, Kilian, and Wigderson (STOC
1988) who show that MIP = PZK-MIP (in other words, all classical multiprover interactive
protocols can be made zero knowledge). We prove our result by showing that every MIP$^*$
protocol can be efficiently transformed into an equivalent zero knowledge MIP$^*$ protocol in a
manner that preserves the completeness-soundness gap. Combining our transformation with
previous results by Slofstra (Forum of Mathematics, Pi 2019) and Fitzsimons, Ji, Vidick and
Yuen (STOC 2019), we obtain the corollary that all co-recursively enumerable languages (which
include undecidable problems as well as all decidable problems) have zero knowledge MIP$^*$
protocols with vanishing promise gap.</summary>
    <updated>2019-06-09T10:46:57Z</updated>
    <published>2019-06-09T10:46:57Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-06-09T20:21:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3399</id>
    <link href="https://agtb.wordpress.com/2019/06/09/max-plank-summer-school-games-brains-and-distributed-computing/" rel="alternate" type="text/html"/>
    <title>Max Plank Summer School: Games, Brains, and Distributed Computing</title>
    <summary>The 20th Max Planck Advanced Course on the Foundations of Computer Science focused on “Games, Brains, and Distributed Computing” will take place on 19 – 23 August 2019, Saarbrücken, Germany.   The instructors are Christos Papadimitriou, Eva Tardos, and Pierre Fraigniaud. Advertisements</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The 20th <a href="http://resources.mpi-inf.mpg.de/conferences/adfocs/">Max Planck Advanced Course on the Foundations of Computer Science focused on “Games, Brains, and Distributed Computing”</a> will take place on 19 – 23 August 2019, Saarbrücken, Germany.</p>
<p> </p>
<p>The instructors are Christos Papadimitriou, Eva Tardos, and Pierre Fraigniaud.</p></div>
    </content>
    <updated>2019-06-09T07:10:20Z</updated>
    <published>2019-06-09T07:10:20Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>algorithmicgametheory</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2019-06-09T20:21:13Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8872438004336745499</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8872438004336745499/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/06/ray-miller-one-of-our-founders-passes.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8872438004336745499" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8872438004336745499" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/06/ray-miller-one-of-our-founders-passes.html" rel="alternate" type="text/html"/>
    <title>Ray Miller, one of our founders, Passes away at the age of 90</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Ray Miller, one of the founders of our field, passed away recently at the age of 90.<br/>
<br/>
He has associations with both GA Tech and The University of Maryland, so both Lance and I have a connection to him. As does Dick Lipton who has posted about him <a href="https://rjlipton.wordpress.com/2019/06/08/raymond-edward-miller-just-passed-away/">here</a>.<br/>
<br/>
I present two guest blog-posts about him<br/>
<br/>
<br/>
<b>Post One</b>: From<br/>
<br/>
<a href="https://www.cs.umd.edu/people/lin">Ming C Lin</a><br/>
<br/>
Elizabeth Stevinson Chair of Computer Science<br/>
<br/>
University of Maryland at College Park<br/>
<br/>
<br/>
Dear CS Alumni and Friends,<br/>
<br/>
We are deeply saddened to learn that Professor Emeritus Ray Miller passed away two nights ago around 9 pm.<br/>
<br/>
A Memorial Service at St. Andrews Lutheran Church (15300 New Hampshire Ave., Silver Spring MD  20905) for Dr. Miller will be held on Saturday, June 15th at 10:30 am.<br/>
<br/>
Dr. Ray Miller received his Ph.D. from University of Illinois in 1957.  He was a professor and the former Director of the School of Information and Computer Science at the Georgia Institute of Technology before joining our department in 1988 as the Director of the Center of Excellence in Space Data and Information Science (CESDIS).   Dr. Miller was well known for his research on communication protocols, networks, distributed systems, parallel computation, and theory.<br/>
<br/>
In 1970, he became the Fellow of IEEE for the advancement of the theoretical understanding of computation through work in switching theory and theoretical models.<br/>
<br/>
In 1997, he was elevated to be a Fellow of ACM for research contributions to the theory of parallel computation and for his distinguished service to the Computer Science community as an educator and leader.<br/>
<br/>
In 2003, Dr. Miller was designated as a Fellow by the Computer Science Accreditation Board<br/>
<br/>
<i>"in recognition of his outstanding professional volunteer contributions to computing sciences </i><i>and accreditation”.</i><br/>
<br/>
Dr. Miller was also an AAAS Fellow,  and a Charter Member of IEEE Computer Society Golden Core;he received the IEEE Third Millennium Medal in 2000 and ACM Distinguished Service Award in 2002.<br/>
<br/>
Beyond his outstanding research contribution and devotion to education, Dr. Ray Miller has been known for his kindness as a colleague, supportiveness as a mentor, and effectiveness as a leader. Dr. Miller will be forever remembered warmly by his friends, colleagues and students for his dedication and service to our department, the University, and the field of computing at large.<br/>
<br/>
<br/>
<b>Post Two</b>: From<br/>
<br/>
<a href="http://www.cs.umd.edu/users/ben/">Ben Shneiderman</a><br/>
<br/>
Emeritus Professor, University of Maryland at College Park.<br/>
<br/>
I was saddened to hear about the death of Ray Miller at age 90.  He was a dear colleague who contributed a great deal to computer science and to our department.  You can read his 84-page personal memoir at the IEEE Computer Society History Committee website: <a href="https://history.computer.org/pubs/ray-miller.pdf">here</a>.<br/>
<br/>
His memoirs tells his story in detail, describing his research collaborations in computational complexity, parallel algorithms, and program optimization and his leadership roles. You can see more about Ray’s work on his ACM author page: <a href="https://dl.acm.org/author_page.cfm?id=81332515760">here</a><br/>
<br/>
<div>
This is the best source as he had no Google Scholar page or Wikipedia article that I could find. Ray’s quiet and modest style was a valuable asset, but his contributions come through in his memoir. He describes working with Turing Awardees John Cocke, Fran Allen, John Backus, Dick Karp, and other key figures, so maybe Ray should have received that award too.  Ray was also an excellent administrator and leader, who contributed to building the institutions (conferences, ACM, IEEE, etc.) that supported the growth of computer science.</div>
<div>
<div>
<br/></div>
<div>
Ray was especially kind to me in the early 1970s, when I was working on my Phd, developing a graph theoretic model of data structures.  As Assistant Director of the IBM Mathematical Science Department at the T. J. Watson Labs in Yorktown Heights, NY. This legendary IBM Research Lab was equal to Bell Labs and filled with computing pioneers in hardware, software, and applications.<br/>
<br/></div>
<div>
Ray invited me to give a talk about my work, drawing interest from Arnold Rosenberg, who had been developing related ideas. With Ray’s support I returned for monthly visits with Arnie and Ray to refine my esoteric ideas leading to my May 1973 Phd.</div>
<div>
<br/></div>
<div>
Ray’s kindness as a colleague and supportiveness as a mentor will always be remembered warmly. Here are a few photos of Ray giving a talk in the CS Department, probably in 1985: <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/ray1.jpg">here,</a> <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/ray2.jpg">here</a>, and</div>
</div>
<div>
<a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/ray3.jpg">here</a></div></div>
    </content>
    <updated>2019-06-08T18:35:00Z</updated>
    <published>2019-06-08T18:35:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-06-09T15:44:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15969</id>
    <link href="https://rjlipton.wordpress.com/2019/06/08/raymond-edward-miller-just-passed-away/" rel="alternate" type="text/html"/>
    <title>Raymond Edward Miller Just Passed Away</title>
    <summary>Miller just passed away at 90 [ GIT ] Ray Miller just passed away. He had been a researcher and leader at IBM Research, Georgia Tech, and University of Maryland. At all he did important research and also was a leader: a group head, a director, and a chair. Today we remember Ray. For starters […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Miller just passed away at 90</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/06/08/raymond-edward-miller-just-passed-away/ray/" rel="attachment wp-att-15970"><img alt="" class="alignright size-full wp-image-15970" src="https://rjlipton.files.wordpress.com/2019/06/ray.jpeg?w=600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ GIT ]</font></td>
</tr>
</tbody>
</table>
<p>
Ray Miller just passed away. He had been a researcher and leader at IBM Research, Georgia Tech, and University of Maryland. At all he did important research and also was a leader: a group head, a director, and a chair.</p>
<p>
Today we remember Ray.</p>
<p>
For starters you can read <a href="https://cacm.acm.org/news/237370-in-memoriam-raymond-e-ray-miller-1928-2019/fulltext">this</a> or his memoir <a href="https://history.computer.org/pubs/ray-miller.pdf">here</a>. Ray started in the field with an electrical engineering PhD <a href="https://www.ideals.illinois.edu/handle/2142/5131/filter-search">thesis</a> tilted <i>Formal Analysis and Synthesis of Bilateral Switching Networks</i>. See <a href="https://ieeexplore.ieee.org/document/5222582">this</a> for his paper.</p>
<p>
Ray’s memoir does not mention tennis. But we played tennis when I visited IBM, and also when we could while at a conference. Ray was not slim, not fast, not obviously athletic. But he was the best tennis player in our group. He was miles above me. His trick was he could control the ball, especially on his serve so it was untouchable. He could spin it so that it landed and bounced at right angles. He was so good that he hardly ever had to move. When I played him in singles, we had to agree “no spinning serves”. When we played doubles there was a small chance that someone could return his ball, but not much.</p>
<p>
I think this is a fair way to summarize Ray: It was easy to underestimate him. Ray always had a smile on his face. I cannot remember him being anything but happy. This may led some to think he was not serious. But Ray was. He did wonderful research and was a leader in our field. He changed, for the better, all the places he called “home”. I can directly attest to his impact at Tech; others I believe can attest to IBM and Maryland. He was a great editor for <i>Journal of the ACM</i>, and did much more for our field. Thanks Ray.</p>
<p>
</p><p/><h2> Some Thoughts </h2><p/>
<p/><p>
Here are just a few comments from friends of Ray.</p>
<p>
<i>Rich DeMillo</i>:</p>
<blockquote><p><b> </b> <em> Ray was a quiet but strong and effective leader and a good friend to many of us. Nancy Lynch and I recruited him to be School of ICS Director, bringing immediate stature—both internal and external—to computer science at Georgia Tech. Up until that time Information and Computer Science was an odd duck interdisciplinary graduate program in a very traditional engineering school that had not invested in the field despite the early success of the School of Information and Computer Science and a world-class Burroughs installation (Georgia Tech had played a key role in Algol development in the ’60s). Ray was an engineer with impeccable credentials and was taken seriously by the administration in ways that the library scientists, linguists, philosophers, psychologists, cyberneticists, and mathematicians who founded the School never managed to achieve. </em></p><em>
<p>
Dick Lipton and I were long term collaborators on theory research with Ray, and Rays’ presence helped shine a national spotlight on the School. He was the editor-in-chief of the Journal of the ACM and palled around with luminaries like Sam Winograd and Dick Karp. </p>
<p>
In addition to his own work in switching theory and automata, Ray had co-edited the volume in which Karp’s NP-completeness paper appeared, and so his name was forever associated with that ground-breaking paper. From that point on, Atlanta became a mandatory stop on the national research circuit that was the hallmark of theory research in those days. </p>
<p>
One of his first accomplishments was snagging the Computer Science Conference, the large, research-oriented conference for the field. It gave Georgia Tech a chance to showcase its work for the rest of the world. There was a steady rise in rankings and a steady flow of visitors like Michael Rabin, Leslie Lamport, Mike Fisher, Andy Yao, Ravi Kannan, in addition to Karp, Winograd, and Lipton. Ray tried to entice Lipton to Georgia Tech with what was at the time the university’s juiciest startup package. He was ultimately successful, but it took nearly twenty years, and by that time, Tech had replaced the School of ICS with the College of Computing, and Ray was in semi-retirement in Maryland. </p>
</em><p><em>
</em>
</p></blockquote>
<p/><p>
<i>Umakishore Ramachandran</i>:</p>
<blockquote><p><b> </b> <em> I have fond memories of my early years at Tech after being recruited by Ray to join the small but vibrant group of faculty in ICS specializing in theory, systems, and AI. Ray was extremely caring and supportive in nurturing junior faculty. One could say that Ray’s leadership was instrumental in the transformation of CS at Georgia Tech and putting GT on the map to compete with other more established CS departments around the nation.</em></p><em>
<p>
Ray helped create a sense of family in the department. I recall every day he would be at lunch in the faculty club which in those days served coffee at $0.10. He would be there eating a healthy meal featuring a giant sausage and reserving an entire table for the ICS faculty to join him for lunch. It created such a friendly and amicable environment for discussing any issue.</p>
<p>
I feel compelled to share a lighter anecdote when I got hired by Ray. Those were the days of terminals connected to mini computers, DEC VAX 750 and 780, and I wanted Ray to promise me that he will give me a terminal and modem for connecting to the campus computers from home. Even as a grad student at UW-Madison I had a 2400 baud modem at home. When I arrived at Tech, Ray gave me a 300-baud acoustic coupler since I had not specified what speed modem I wanted in my startup package. Ray had a giant infectious smile all the time that made it difficult to get mad at him for anything. Note: <img alt="{300 \ll 2400}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B300+%5Cll+2400%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{300 \ll 2400}"/>.</p>
</em><p><em>
He will be missed by anyone and everyone whose lives he touched. </em>
</p></blockquote>
<p/><p>
<i>Bill Gasarch</i></p>
<p>
Here is a short quote from Bill and see his post for more details.</p>
<blockquote><p><b> </b> <em> I was saddened to hear of Ray Miller’s death. He was at University of Maryland for many years. Since he got his PhD in 1957 and was still active into the 2000’s he had a broad view of computer science. He did theory AND systems AND other things. It was great to talk to him about the early days of computer science before we had these labels. </em>
</p></blockquote>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Ray is missed. Our condolences to his family and his many friends for their loss.</p>
<p/></font></font></div>
    </content>
    <updated>2019-06-08T17:21:41Z</updated>
    <published>2019-06-08T17:21:41Z</published>
    <category term="History"/>
    <category term="News"/>
    <category term="Oldies"/>
    <category term="People"/>
    <category term="Georgia Tech"/>
    <category term="IBM"/>
    <category term="Maryland"/>
    <category term="Ray Miller"/>
    <category term="Switching theory"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-06-09T20:21:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.02511</id>
    <link href="http://arxiv.org/abs/1906.02511" rel="alternate" type="text/html"/>
    <title>On the distribution of runners on a circle</title>
    <feedworld_mtime>1559952000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hrubes:Pavel.html">Pavel Hrubes</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.02511">PDF</a><br/><b>Abstract: </b>Consider $n$ runners running on a circular track of unit length with constant
speeds such that $k$ of the speeds are distinct. We show that, at some time,
there will exist a sector $S$ which contains at least $|S|n+ \Omega(\sqrt{k})$
runners. The result can be generalized as follows. Let $f(x,y)$ be a complex
bivariate polynomial whose Newton polytope has $k$ vertices. Then there exists
$a\in {\mathbb C}\setminus\{0\}$ and a complex sector $S=\{re^{\imath \theta}:
r&gt;0, \alpha\leq \theta \leq \beta\}$ such that the univariate polynomial
$f(x,a)$ contains at least $\frac{\beta-\alpha}{2\pi}n+\Omega(\sqrt{k})$
non-zero roots in $S$ (where $n$ is the total number of such roots and $0\leq
(\beta-\alpha)\leq 2\pi$). This shows that the Real $\tau$-Conjecture of Koiran
implies the conjecture on Newton polytopes of Koiran et al.
</p></div>
    </summary>
    <updated>2019-06-08T23:20:57Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-06-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.02315</id>
    <link href="http://arxiv.org/abs/1906.02315" rel="alternate" type="text/html"/>
    <title>Greed is Not Always Good: On Submodular Maximization over Independence Systems</title>
    <feedworld_mtime>1559952000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuhnle:Alan.html">Alan Kuhnle</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.02315">PDF</a><br/><b>Abstract: </b>In this work, we consider the maximization of submodular functions
constrained by independence systems. Because of the wide applicability of
submodular functions, this problem has been extensively studied in the
literature. When the independence system is a $p$-system, prior literature has
claimed that the greedy algorithm achieves a $1/(p+1)$-approximation if the
submodular function is monotone. We show that, on the contrary, for any
$\epsilon &gt; 0$, the problem is hard to approximate within $(2/n)^{1-\epsilon}$,
where $n$ is the size of the ground set, even when the independence system is a
$1$-system. This result invalidates prior work on constant-factor algorithms
for non-monotone submodular maximization over $p$-systems as well. On the
positive side, we provide the first nearly linear-time algorithm for
maximization of non-monotone submodular functions over $p$-extendible
independence systems, which are a subclass of $p$-systems.
</p></div>
    </summary>
    <updated>2019-06-08T23:21:13Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1906.02229</id>
    <link href="http://arxiv.org/abs/1906.02229" rel="alternate" type="text/html"/>
    <title>Quantum Algorithms for Solving Dynamic Programming Problems</title>
    <feedworld_mtime>1559952000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ronagh:Pooya.html">Pooya Ronagh</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1906.02229">PDF</a><br/><b>Abstract: </b>We present quantum algorithms for solving finite-horizon and infinite-horizon
dynamic programming problems. The infinite-horizon problems are studied using
the framework of Markov decision processes. We prove query complexity lower
bounds for classical randomized algorithms for the same tasks and consequently
demonstrate a polynomial separation between the query complexity of our quantum
algorithms and best-case query complexity of classical randomized algorithms.
Up to polylogarithmic factors, our quantum algorithms provide quadratic
advantage in terms of the number of states $|S|$, and the number of actions
$|A|$, in the Markov decision process when the transition kernels are
deterministic. This covers all discrete and combinatorial optimization problems
solved classically using dynamic programming techniques. In particular, we show
that our quantum algorithm solves the travelling salesperson problem in
$O^*(c^4 \sqrt{2^n})$ where $n$ is the number of nodes of the underlying graph
and $c$ is the maximum edge-weight of it. For stochastic transition kernels the
quantum advantage is again quadratic in terms of the numbers of actions but
less than quadratic (from $|S|^2$ to $|S|^{3/2}$) in terms of the numbers of
states. In all cases, the speed-up achieved is at the expense of appearance of
other polynomial factors in the scaling of the algorithm. Finally we prove
lower bounds for the query complexity of our quantum algorithms and show that
no more-than-quadratic speed-up in either of $|S|$ or $|A|$ can be achieved for
solving dynamic programming and Markov decision problems using quantum
algorithms.
</p></div>
    </summary>
    <updated>2019-06-08T23:21:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-06-07T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/06/07/little-knowledge-can</id>
    <link href="https://11011110.github.io/blog/2019/06/07/little-knowledge-can.html" rel="alternate" type="text/html"/>
    <title>A little knowledge can make the next step harder</title>
    <summary>Suppose you have a fill-in-the-unknowns puzzle, like Sudoku. Can making some deductions and filling in those parts of the puzzle make the whole thing harder to solve than it was before you started? Sometimes, yes!</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Suppose you have a fill-in-the-unknowns puzzle, like Sudoku. Can making some deductions and filling in those parts of the puzzle make the whole thing harder to solve than it was before you started? Sometimes, yes!</p>

<p>I have in mind human-style puzzle-deduction rules, where you see a piece of the puzzle that matches some pattern and use that to deduce what one of the unknowns should be. And by “harder” I mean that a puzzle that was previously possible to solve by some set of deduction rules, after the deduction, stopped being possible for those rules to solve. Of course this is possible if you have a bad set of deduction rules. But normally, at least for the kinds of patterns I think about when solving puzzles, if a pattern matches in a partially completed puzzle, then it or a simplification of it will continue to match no matter how I fill in more of the unknowns. Most of the deduction pattern that I use are monotonic, in this sense. If you had a collection of patterns that was not monotonic, you could add all ways of partially filling them in to your collection, and get a better set of patterns, right?</p>

<p>Wrong! There can be valid deduction patterns for which this extension to monotonic sets would produce invalid patterns. I’m pretty sure this can happen in Sudoku, actually, but the example I have in mind comes from a different puzzle game I’ve been playing lately, part of Simon Tatham’s puzzle collection, where it’s called “map”. It’s based on the problem of <a href="https://en.wikipedia.org/wiki/Precoloring_extension">precoloring extension</a>: you’re given a partially 4-colored planar map, and you have to fill in the rest of the colors.
And it’s trivially NP-complete, by a reduction from planar 3-coloring (augment a 3-coloring instance by extra vertices of the fourth color, preventing any of the given instance vertices from having that color) but the puzzles usually presented by the puzzle app are solvable by hand, even when they’re large and at the highest of its levels of difficulty.</p>

<p style="text-align: center;"><img alt="Screenshot of the map puzzle from Simon Tatham's puzzle collection" src="https://11011110.github.io/blog/assets/2019/map-screenshot.png"/></p>

<p>If that were all, then I think all deduction rules could be made monotonic. But I’m going to tell you one more thing about the puzzle, and this one thing makes it non-monotonic. It is that, like Sudoku, every puzzle has a unique solution.
And <a href="https://11011110.github.io/blog/2005/10/15/assuming-uniqueness-in.html">like Sudoku, the assumption of uniqueness leads to new deduction rules</a>.
You can infer that certain regions have to have certain colors, because if they could be colored anything else then there would be more than one solution.</p>

<p>To see how this works, suppose I had a map like the one shown below (where the white squares have not yet been colored, and I’m only showing a small piece of a larger puzzle):</p>

<p style="text-align: center;"><img alt="Uncolored pocket in a map puzzle" src="https://11011110.github.io/blog/assets/2019/nonmon1.svg"/></p>

<p>There’s a pocket of uncolored squares extending into the colored region on the left. If I colored the square at the mouth of the pocket yellow, the inner square of the puzzle would be ambiguous: it has only yellow and blue neighbors, so it could be either red or black.</p>

<p style="text-align: center;"><img alt="Ambiguously colored pocket in a map puzzle" src="https://11011110.github.io/blog/assets/2019/nonmon2.svg"/></p>

<p>To prevent this ambiguity, the square at the mouth of the pocket must be black. And to force it to be black, the square one step beyond the mouth must be yellow. So from the initial state and the assumption of a unique solution, it’s possible to infer the colors of three previously-blank squares:</p>

<p style="text-align: center;"><img alt="Ambiguously colored pocket in a map puzzle" src="https://11011110.github.io/blog/assets/2019/nonmon3.svg"/></p>

<p>But if I have multiple rules at hand, it’s natural for me to try the weaker and easier ones first. Suppose I had done this, and used a weaker inference rule telling me that the square at the mouth was black.</p>

<p style="text-align: center;"><img alt="Partially colored pocket in a map puzzle" src="https://11011110.github.io/blog/assets/2019/nonmon4.svg"/></p>

<p>Or suppose I had used a rule that produced the valid (but even weaker) inference that the inner square must be red.</p>

<p style="text-align: center;"><img alt="Even more partially colored pocket in a map puzzle" src="https://11011110.github.io/blog/assets/2019/nonmon5.svg"/></p>

<p>Now I can’t use my strong inference rule and color the outer squares! I’ve lost the information about why I colored the inner pocket squares the way I did, and so I’ve lost the ability to make deductions about how the outer squares should be colored to avoid ambiguities. It would not be a valid pattern to see a puzzle in these states and deduce the color of the remaining squares associated with the pocket. So the extension from my initial (valid) rule, which filled in all three squares when they were all blank, to a monotonic rule that fills in the partially-filled-in pocket in the same way, would be invalid. Of course, if the puzzle solution was unique before, it must still be unique after partially filling in the pocket. But with fewer squares colored, my deductive abilities might not be up to the task of reasoning from the remaining parts of the puzzle to its unique solution.</p>

<p>For the same reason, I might not actually want to color yellow the square beyond the mouth, forgetting why it needs to be yellow. Because what I can infer from the initial state is not merely that it should be colored yellow: it’s that the three outer neighbors of this square must have a permutation of the three other colors, so that this square is forced to be yellow, so that the rest of the pocket will have an unambiguous coloring.</p>

<p>I think what this means is that my knowledge representation (consisting only of blank or filled-in puzzle regions) is inadequate. In practice, I actually use a more complex knowledge representation where (either in my head or with markers provided in the puzzle app) I keep track of which colors are still available for the blank regions, but it’s still inadequate, in the same way. It’s not clear to me what the right knowledge representation is, to allow me to keep track of chains of inferences like “one of these squares must be red to prevent this square from becoming yellow to prevent its neighbor from becoming ambiguous” without the complexity of what I remember for each square blowing up to non-constant.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/102234384857906663">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2019-06-07T22:37:00Z</updated>
    <published>2019-06-07T22:37:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-06-08T05:49:29Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:typepad.com,2003:post-6a00d83452383469e20240a48c6624200d</id>
    <link href="https://3dpancakes.typepad.com/ernie/2019/06/my-entry.html" rel="alternate" type="text/html"/>
    <link href="https://3dpancakes.typepad.com/ernie/2019/06/my-entry.html" rel="replies" type="text/html"/>
    <title/>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><br/>
<br/>
<a class="asset-img-link" href="https://3dpancakes.typepad.com/.a/6a00d83452383469e20240a4b0ee61200b-pi" style="display: inline;"> <img alt="It-exists" border="0" class="asset  asset-image at-xid-6a00d83452383469e20240a4b0ee61200b image-full img-responsive" src="https://3dpancakes.typepad.com/.a/6a00d83452383469e20240a4b0ee61200b-800wi" title="It-exists"/> </a><br/></p></div>
    </content>
    <updated>2019-06-07T18:17:11Z</updated>
    <published>2019-06-07T18:17:11Z</published>
    <author>
      <name>Jeff Erickson</name>
    </author>
    <source>
      <id>tag:typepad.com,2003:weblog-6686</id>
      <link href="https://3dpancakes.typepad.com/ernie/atom.xml" rel="self" type="application/atom+xml"/>
      <link href="https://3dpancakes.typepad.com/ernie/" rel="alternate" type="text/html"/>
      <subtitle>Let Σ be a combinatorial surface with n vertices, genus g, and b boundaries.  Amen.</subtitle>
      <title>Ernie's 3D Pancakes</title>
      <updated>2019-06-07T18:17:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15957</id>
    <link href="https://rjlipton.wordpress.com/2019/06/07/a-rank-problem/" rel="alternate" type="text/html"/>
    <title>A Rank Problem</title>
    <summary>More on restricted quantum circuits [ The Daily Grail ] Ken Regan just wrote about his paper with Chaowen Guan (GR). The paper is titled, “Stabilizer Circuits, Quadratic Forms, and Computing Matrix Rank.” Today I thought I would add some additional comments on their result. The post Ken wrote is thorough, detailed, and geared for […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>More on restricted quantum circuits</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/06/07/a-rank-problem/images-191/" rel="attachment wp-att-15961"><img alt="" class="alignright size-full wp-image-15961" src="https://rjlipton.files.wordpress.com/2019/06/images.png?w=600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ The Daily Grail ]</font></td>
</tr>
</tbody>
</table>
<p>
Ken Regan just wrote about his paper with Chaowen Guan (GR). The <a href="https://arxiv.org/abs/1904.00101">paper</a> is titled, “Stabilizer Circuits, Quadratic Forms, and Computing Matrix Rank.”</p>
<p>
Today I thought I would add some additional comments on their result.</p>
<p>
The post Ken wrote is thorough, detailed, and geared for an expert in quantum computation. I think he did a service to the field of quantum complexity theory with his write-up. But I thought too that there some things that he neglected to say: things that could be interesting to a wider community. So with all due respect I hope this short discussion is useful.</p>
<p>
</p><p/><h2> Quantum Circuits </h2><p/>
<p/><p>
In studying classic computations, we can restrict our attention to Boolean circuits. Moreover, these circuits can be assumed to consist of one type of gate. A formal way to say this is: Circuits that use NAND gates are <i>universal</i>. Any computation can be converted into a circuit that only uses this type of gate. It is now viewed as a trivial observation, but was not obvious in the beginning. Note, AND and OR and NOT can be implemented with just NAND gates.</p>
<p>
The understanding of what gate types are universal is not only of interest to theorists. Some technologies support directly some types of gates, while others support different types. For example the dominant CMOS technology efficiently implements NAND gates. </p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/06/07/a-rank-problem/cmos/" rel="attachment wp-att-15958"><img alt="" class="aligncenter size-medium wp-image-15958" height="300" src="https://rjlipton.files.wordpress.com/2019/06/cmos.png?w=150&amp;h=300" width="150"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ Wikipedia ]</font>
</td>
</tr>
</tbody></table>
<p>
In studying quantum computations, we can also restrict our attention to <i>quantum circuits</i>. These circuits need only use gates from a small family. These gate types are more complex than just NAND gates, as you might expect since quantum circuits manipulate qubits and not just bits. I believe that it is fair to say that the fact that there are universal gates for quantum circuits is not trivial. There are many types of gates that are studied: Clifford, NOT, Fredkin, Hadamard, Toffoli, and many others.</p>
<p>
Again the understanding of what gate types are universal is not only of interest to theorists. Quantum circuits are not easy to build, the technology is still evolving. There is yet no CMOS answer to the question: How do we build quantum hardware? Errors may be the limiting factor for large quantum circuits. We will see. One consequence of the difficulty of building universal quantum circuits is the interest in circuits that use gates that are from a family that is <i>not universal</i>.</p>
<p>
The hope is several fold: </p>
<ul>
<li>
Perhaps understanding gates that are not universal will help us understand general quantum computations. <p/>
</li><li>
Perhaps some important problems in chemistry, for example, may be solvable with these non-universal circuits. <p/>
</li><li>
Perhaps the mathematics of these restricted circuits will be interesting in its own right.
</li></ul>
<p>
The third point is where GR’s work falls.</p>
<p>
</p><p/><h2> Stabilizer Circuits </h2><p/>
<p/><p>
The class of quantum circuits that GR studied are the <i>stabilizer circuits</i>. See this for an earlier discussion on these <a href="https://rjlipton.wordpress.com/2012/07/08/grilling-quantum-circuits/">circuits</a>. The key is that these circuits are not universal. And more importantly they can be simulated by classical computers in polynomial time. In the upside-down world of quantum complexity theory, the ability to efficiently simulate them is bad. In order to show that quantum computations are new and exciting, being able to simulate them on your laptop is not good.</p>
<p>
The punch-line is: how efficient is this simulation? There are polynomial algorithms and there are useful polynomial algorithms. The 2004 <a href="https://arxiv.org/abs/quant-ph/0406196">paper</a> by Scott Aaronson and Daniel Gottesman (AG) says that the time complexity is <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2)}"/>. Looks good.</p>
<p>
</p><p/><h2> A Breakthrough? </h2><p/>
<p/><p>
For a few hours, maybe days, GR thought they could show that stabilizer circuits solve the matrix rank problem. In part this was based on a misunderstanding, but it was also based on not needing the full generality of maintaining the system between single-qubit measurements. If they could calculate or even estimate the probability of just one all-qubits measurement in time <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2)}"/> then the same time would apply to computing matrix rank over the field <img alt="{\mathbb{F}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{F}_2}"/>. This would have been an immense result. The best known is that the rank of a matrix can be computed in <img alt="{n^{\omega}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B%5Comega%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^{\omega}}"/> where <img alt="{\omega}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\omega}"/> is the current exponent for multiplying <img alt="{n \times n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n \times n}"/> matrices—over <img alt="{\mathbb{F}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{F}_2}"/> or any field. Today <img alt="{\omega}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\omega}"/> stands at <img alt="{2.3728\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2.3728%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2.3728\dots}"/>. </p>
<p>
Clearly, they were excited. I still recall Ken calling me with the possible news. I thought that it was not a crazy idea. Their plan was: </p>
<ol>
<li>
Reduce matrix rank to a quantum problem; <p/>
</li><li>
Show that problem could be computed by stabilizer circuits; <p/>
</li><li>
Then invoke the simulation theorem for these circuits to get a classical algorithm.
</li></ol>
<p>Wow. This trip from a classic problem, to a quantum one, and back was intriguing. </p>
<p>
</p><p/><h2> Saving Grace </h2><p/>
<p/><p>
Quickly GR figured out the issue. They could in <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2)}"/> total time determine the probability of each individual qubit being <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>. That probability would be either <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> or <img alt="{\frac{1}{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{1}{2}}"/> since their reduction from rank gives a circuit in which the probability of getting all <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>s is guaranteed to be positive. If <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> values are <img alt="{\frac{1}{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{1}{2}}"/> that does not make the answer <img alt="{\frac{1}{2^k}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%5Ek%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{1}{2^k}}"/>, because of entanglements. They had thought that the Aaronson-Gottesman algorithm took care of the entanglement bookkeeping in <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2)}"/> time, but it services only one qubit in that time. Ironically, AG expressly note that they improved Gottesman’s earlier <img alt="{O(n^\omega)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^\omega)}"/> time for this task to <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2)}"/>—exactly what GR thought they would get—but AG still only get <img alt="{O(n^3)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^3)}"/> time for operations involving all the qubits.</p>
<p>
GR worked out the full picture from a more-recent <a href="https://arxiv.org/abs/1712.03554">paper</a> by Héctor García-Ramírez and Igor Markov of Michigan, which describes detailed software for simulating stabilizer circuits. Thus the quantum method would only put matrix rank into cubic time—not new. But then they realized that the goalposts for a result in the <em>other</em> direction were only <img alt="{O(n^3)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^3)}"/>. This they could beat with most of their hard work already done. Thus they can improve <img alt="{O(n^3)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^3)}"/> cases by AG and some subsequent papers to <img alt="{O(n^\omega)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^\omega)}"/>. In theory, that is—the <img alt="{O(n^\omega)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^\omega)}"/> algorithms are galactic. But what GR also have is a pretty result that is not galactic at all:</p>
<blockquote><p><b>Theorem 1</b> <em> If membership in a certain class of undirected <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/>-vertex graphs can be decided in <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{O(n^2)}"/> time, then the following problems all have the same time complexity: </em></p><em>
<ol>
<li>
The strong simulation of quantum stabilizer circuits; <p/>
</li><li>
The computation of the rank of matrices over <img alt="{\mathbb{F}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathbb{F}_2}"/>; <p/>
</li><li>
The counting of solutions to classical quadratic forms modulo <img alt="{4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B4%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{4}"/>.
</li></ol>
</em><p><em/>
</p></blockquote>
<p/><p>
Note that all these problems have upper bounds of <img alt="{n^\omega}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^\omega}"/> by GR; the point is that their bounds would coincide even if matrix rank turns out to be easier than matrix multiplication. The next post will talk about the class of graphs. Just to be clear:</p>
<ul>
<li>
If the graphs are in <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2)}"/> time, this does not mean the problems are all in <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2)}"/> time—they could still need <img alt="{n^\omega}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^\omega}"/> time. <p/>
</li><li>
The graphs are in <img alt="{O(n^\omega)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^\omega)}"/> time. This is for dense graphs with order-<img alt="{n^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^2}"/> edges. <p/>
</li><li>
The reductions from rank to the other problems run in <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2)}"/> time unconditionally—the graphs involved are all bipartite so their status is known.
</li></ul>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
I hope that this discussion helps shed some light on the new result of GR. I hope that raising the covers on how they found their theorem is useful. Research is not smooth, is not a straight-line from start to finish, and their journey is not atypical.</p>
<p>[Edited wrong word: thorough and fixed qubits]</p></font></font></div>
    </content>
    <updated>2019-06-07T12:58:03Z</updated>
    <published>2019-06-07T12:58:03Z</published>
    <category term="Ideas"/>
    <category term="Open Problems"/>
    <category term="Proofs"/>
    <category term="matrix multiplication"/>
    <category term="quantum"/>
    <category term="Simulation"/>
    <category term="stabilizer circuits"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-06-09T20:21:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/085</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/085" rel="alternate" type="text/html"/>
    <title>TR19-085 |  Approximate Degree-Weight and Indistinguishability | 

	Emanuele Viola, 

	Xuangui Huang</title>
    <summary>We prove that the Or function on $n$ bits can be point-wise approximated with error $\eps$ by a polynomial of degree $O(k)$ and weight $2^{O(n \log (1/\eps)/k)}$, for any $k \geq \sqrt{n \log 1/\eps}$.  This result is tight for all $k$.  Previous results were either not tight or had $\eps = \Omega(1)$.  In general we obtain an approximation result for any symmetric function, also tight.  Building on this we also obtain an approximation result for bounded-width CNF.  For these two classes no such result was known.

One motivation for such results comes from the study of indistinguishability.
Two distributions $P$, $Q$ over $n$-bit strings are $(k,\delta)$-indistinguishable if their projections on any $k$ bits have statistical distance at most $\delta$.
The above approximations give values of $(k,\delta)$ that suffice to fool symmetric functions and bounded-width CNF, and the first result is tight.
Finally, we show that any two $(k, \delta)$-indistinguishable distributions are $O(n)^{k/2}\delta$-close to two distributions that are $(k,0)$-indistinguishable, improving the previous bound of $O(n)^k \delta$.</summary>
    <updated>2019-06-06T23:40:41Z</updated>
    <published>2019-06-06T23:40:41Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-06-09T20:21:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://agtb.wordpress.com/?p=3397</id>
    <link href="https://agtb.wordpress.com/2019/06/06/workshop-on-simplicity-and-robustness-in-complex-markets-stony-brook-university-july-11-12-2019/" rel="alternate" type="text/html"/>
    <title>Workshop on Simplicity and Robustness in Complex Markets, Stony Brook University, July 11-12, 2019</title>
    <summary>As part of the 30th Stony Brook International Conference on Game Theory, we will be holding a workshop on Simplicity and Robustness in Complex Markets.  The workshop will be held July 11-12 at Stony Brook University. The goal of this workshop is to bring together researchers from Computer Science, Game Theory, Economics, and Operations Research to focus […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p style="font-weight: 400;">As part of the <a href="http://www.gtcenter.org/?page=Conference.html">30<sup>th</sup> Stony Brook International Conference on Game Theory</a>, we will be holding a workshop on Simplicity and Robustness in Complex Markets.  The workshop will be held July 11-12 at Stony Brook University.</p>
<p style="font-weight: 400;">The goal of this workshop is to bring together researchers from Computer Science, Game Theory, Economics, and Operations Research to focus on issues related to simplicity, robustness, and approximation in economic problems.  For more information, including the workshop program, please visit</p>
<p style="font-weight: 400;"><a href="http://www.gtcenter.org/?page=Workshops.html">http://www.gtcenter.org/?page=Workshops.html</a></p>
<p style="font-weight: 400;">Early registration is open until June 16 at <a href="http://www.gtcenter.org/?page=Registration.html">http://www.gtcenter.org/?page=Registration.html</a></p>
<p style="font-weight: 400;">We hope to see you there!</p>
<p style="font-weight: 400;">Michal Feldman and Brendan Lucier</p></div>
    </content>
    <updated>2019-06-06T18:10:10Z</updated>
    <published>2019-06-06T18:10:10Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>michalfeldman</name>
    </author>
    <source>
      <id>https://agtb.wordpress.com</id>
      <logo>https://secure.gravatar.com/blavatar/52ef314e11e379febf97d1a97547f4cd?s=96&amp;d=https%3A%2F%2Fs0.wp.com%2Fi%2Fbuttonw-com.png</logo>
      <link href="https://agtb.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://agtb.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://agtb.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://agtb.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Computation, Economics, and Game Theory</subtitle>
      <title>Turing's Invisible Hand</title>
      <updated>2019-06-09T20:21:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=357</id>
    <link href="https://tcsplus.wordpress.com/2019/06/06/tcs-talk-wednesday-june-12th-john-wright-mit/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, June 12th — John Wright, MIT</title>
    <summary>The next TCS+ talk—and last of the season!—will take place this coming Wednesday, June 12th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 19:00 Central European Summer Time, 17:00 UTC). John Wright from MIT will speak about “NEEXP in MIP*” (abstract below). Please make sure you reserve a spot for […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk—and last of the season!—will take place this coming Wednesday, June 12th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 19:00 Central European Summer Time, 17:00 UTC). <strong>John Wright</strong> from MIT will speak about “<em>NEEXP in MIP*</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote>
<p style="text-align: justify;">Abstract: A long-standing puzzle in quantum complexity theory is to understand the power of the class <img alt="\textsf{MIP*}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextsf%7BMIP%2A%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="\textsf{MIP*}"/> of multiprover interactive proofs with shared entanglement. This question is closely related to the study of entanglement through non-local games, which dates back to the pioneering work of Bell. In this work we show that <img alt="\textsf{MIP*}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextsf%7BMIP%2A%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="\textsf{MIP*}"/> contains <img alt="\textsf{NEEXP}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextsf%7BNEEXP%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="\textsf{NEEXP}"/> (non-deterministic doubly-exponential time), exponentially improving the prior lower bound of <img alt="\textsf{NEXP}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextsf%7BNEXP%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="\textsf{NEXP}"/> due to Ito and Vidick. Our result shows that shared entanglement exponentially increases the power of these proof systems, as the class <img alt="\textsf{MIP}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextsf%7BMIP%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="\textsf{MIP}"/> of multiprover interactive proofs without shared entanglement is known to be equal to <img alt="\textsf{NEXP}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextsf%7BNEXP%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="\textsf{NEXP}"/>.</p>
</blockquote>
<p> </p></div>
    </content>
    <updated>2019-06-06T16:02:53Z</updated>
    <published>2019-06-06T16:02:53Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2019-06-09T20:22:18Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-1381501092894849452</id>
    <link href="https://blog.computationalcomplexity.org/feeds/1381501092894849452/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/06/what-happened-to-surprising-theorems.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1381501092894849452" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1381501092894849452" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/06/what-happened-to-surprising-theorems.html" rel="alternate" type="text/html"/>
    <title>What Happened to the Surprising Theorems?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Twenty-five years ago Peter Shor presented a polynomial-time factoring algorithms for quantum computers. For Peter, it was a simple translation of a <a href="https://en.wikipedia.org/wiki/Simon%27s_problem">quantum algorithm</a> due to Dan Simon. For the rest of us, it was a shock, while we knew quantum could do some seemingly artificial problems exponentially faster, no one expected a natural problem like factoring to fall so quickly. I remember remarking at the time that Shor bought quantum computing twenty years, now I would say fifty.<br/>
<div>
<br/></div>
<div>
That may have been the last time I was truly shocked by a theorem in theoretical computer science. I've been shocked by proofs, that Primes are in P, Undirected connectivity in Log space, NEXP not in ACC<sup>0</sup>, Graph Isomorphism in quasi-polynomial time. But the theorems themselves all went in the directions we expected.<br/>
<br/>
In the ten years before Shor we had plenty of surprises, interactive proofs, zero-knowledge proofs, probabilistically checkable proofs, nondeterministic space closed under complementation, hardness versus randomness, the permanent hard for the polynomial-time hierarchy. It seemed to come to a hard stop after Shor.<br/>
<br/>
There have been some mild surprises, the Hadamard isn't rigid, holographic algorithms, the complexity of Nash equilibrium, QIP = PSPACE, and many others. But nothing that has made us  rethink the complexity world.<br/>
<br/>
This reflects the maturity of our field. How many shocking theorems have we seen recently in math in general? We're shocked by proofs of the Poincaré conjecture and Fermat's last theorem but both went in the expected direction.<br/>
<br/>
We will have some shocking theorem in the future, maybe Factoring in P or L = NL. To be truly shocked it would have to be something I can't even imagine being true today.</div></div>
    </content>
    <updated>2019-06-06T14:46:00Z</updated>
    <published>2019-06-06T14:46:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-06-09T15:44:55Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://gradientscience.org/robust_apps/</id>
    <link href="http://gradientscience.org/robust_apps/" rel="alternate" type="text/html"/>
    <title>Robustness beyond Security&amp;#58; Computer Vision Applications</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a class="bbutton" href="https://gradientscience.org/robust-apps.pdf" style="float: left;">
<i class="fas fa-file-pdf"/>
    Paper
</a>
<a class="bbutton" href="http://git.io/robust-apps" style="float: left;">
<i class="fab fa-github"/>
   Notebooks
</a>
<a class="bbutton" href="http://bit.ly/robustness_demo" style="float: left;">
<i class="fas fa-code"/>
   Live Demo
</a></p>

<p><i>We discuss our <a href="https://gradientscience.org/robust-apps.pdf">latest paper</a>
on computer vision applications of robust classifiers.
We are able to leverage the features learned by a single classifier to 
develop a rich toolkit for diverse computer vision applications.
Our results suggest the robust classification framework as a viable alternative
to more complex or task-specific approaches.
</i></p>

<p>In our <a href="https://gradientscience.org/robust_reps/">previous post</a>, we saw how robust models 
capture high-level, human-aligned features that can be directly manipulated through 
gradient descent. In this post, we demonstrate how to leverage these robust models 
to perform a wide range of computer vision tasks. In fact, we perform all these tasks 
by simply optimizing the predicted class scores of a <i>single</i> robustly trained 
classifier (per dataset). The resulting toolkit is simple, versatile and 
reliable—to highlight its consistency, in this post we visualize the performance 
of our method using <i>random</i> (not cherry-picked) samples.</p>

<p>Our corresponding paper can be found <a href="https://gradientscience.org/robust-apps.pdf">here</a>, and you 
can reproduce all of our results using open-source IPython notebooks 
<a href="http://git.io/robust-apps">here</a>.
We also have a live <a href="http://bit.ly/robustness_demo">demo</a> where you play with a 
trained robust model like this:</p>

<video align="right" style="width: 90%;">
    <source src="http://gradientscience.org/assets/rf-vision/robust_apps_demo.mp4" type="video/mp4">
    <source src="http://gradientscience.org/assets/rf-vision/robust_apps_demo.webm" type="video/webm">
</source></source></video>

<h3 id="robust-models-as-a-tool-for-input-manipulation">Robust Models as a Tool for Input Manipulation</h3>
<p>The key primitive of our approach is <i>class maximization</i>: maximization of class 
log-probabilities (scores) from an <a href="https://gradientscience.org/robust_opt_pt1/">adversarially robust model</a> 
using gradient descent in input space.</p>

<p>As discussed in our <a href="https://gradientscience.org/robust_reps/">last post</a>, robust models learn 
representations that are more aligned with human perception. As it turns out, 
performing class maximization on these models actually introduces class-relevant 
characteristics in the corresponding input (after all, these class log-probabilities 
are just linear combinations of learned representations). To visualize this, here is 
the result of class maximization for a few random inputs:</p>

<p><img alt="Targeted adversarial examples for a robust model" src="http://gradientscience.org/assets/rf-vision/targeted.jpg"/></p>
<div class="footnote"> For a robust
model, maximizing the predicted probability of a specific class enhances key
features of that class in the input. </div>

<p>These results are also in line with <a href="https://arxiv.org/abs/1805.12152">our previous
work</a>, where we observed that large
adversarial perturbations for robust models often actually resemble natural
examples of the corresponding incorrect class.</p>

<p>In the rest of this post, we will explore how to perform a variety of computer
vision tasks using only class maximization. It turns out that robustness is all
you need!</p>

<h3 id="generation">Generation</h3>
<p>We begin by leveraging robust models to generate diverse,
realistic images. As we saw in the previous section, it is possible to introduce
salient features of a target class into an input through class maximization.
This simple operation alone turns out to also suffice to (class-conditionally)
generate images.</p>

<p>To generate an image, we randomly sample a starting point (seed) and then
execute (starting from that seed) the projected gradient ascent operation
underlying class maximization. The key question here is: how do we sample the
seed input? A natural idea is to just fit a Gaussian distribution to each class
(in image space), and sample seeds from that distribution.. Despite its
simplicity, this approach already leads to fairly diverse and realistic samples:</p>

<div class="widget">
    <div class="choices_one_full" id="gen">
    <span class="widgetheading" id="genclass">Choose an Image</span>
    </div>
    <div style="border-right: 3px white solid;">
        <img class="image-container" id="gen1" style="width: 38%; margin: 8px;"/>
        <img class="image-container" id="gen2" style="width: 60%;"/>
    </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Interactive demo</strong>: select any image in the top two rows to see additional
samples of that class.
</div>

<p>We expect that there is still room for improvement; for example, one could
replace Gaussians with a sophisticated class of distributions for seed sampling.</p>

<h3 id="image-to-image-translation">Image-to-Image translation</h3>

<p>The ability to introduce perceptually meaningful, class-relevant features in
image space (via class maximization) enables a very intuitive approach to
performing <a href="https://arxiv.org/abs/1703.10593">image-to-image translation</a>, the
task of transforming inputs from a source to a target domain (e.g., transforming
horses into zebras in photos). To perform this task, we first train a classifier
to <em>robustly</em> distinguish between the two domains. This process encourages the
classifier to learn key characteristics of each domain. We then perform image
translation on an image from a given domain simply by using class maximization
towards the target domain—this suffices! Here are some instances of our
approach applied to typical datasets:</p>

<div class="widget">
    <div class="choices_one" id="translate">
    <span class="widgetheading">Choose an Image</span>
    </div>
    <span class="widgetheading" id="translatedclass">Translated Image</span>
    <div class="beer-slider selected_one" id="translate_slider">
    <img id="translate1" style="width: 336px;"/>
    <div class="beer-reveal" style="border-right: 3px white solid;">
        <img class="slider_img" id="translate2"/>
    </div>
    </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Interactive demo</strong>: choose an image on the left to see the
result of image-to-image translation.
</div>
<p>This is what a few particularly nice samples produced by our
method look like:</p>

<p><img alt="Select image-to-image translations" src="http://gradientscience.org/assets/rf-vision/translation.jpeg"/></p>

<p>Just as with image generation, we find reasonable solutions to the task using
only class maximization on a robust classifier.</p>

<h3 id="inpainting">Inpainting</h3>

<p>Next, we consider the task of image inpainting—recovering images with large
missing or corrupted regions. At a high level, we would like to fill in the
damaged regions with features that are human-meaningful and consistent with the
rest of the image. Within our framework, the most natural way to do this is to
perform class maximization (towards the original class)  while also penalizing
large changes to the uncorrupted regions of the image. The intuition being that
this process restores the “missing” features while only minimally modifying the
rest of the image. This is how a few random inpainted images produced by our
method look like:</p>

<div class="widget">
    <div class="choices_one" id="inpaint">
    <span class="widgetheading">Choose an Image</span>
    </div>
    <span class="widgetheading">Inpainted Image</span>
    <div class="beer-slider selected_one" id="inpaint_slider">
    <img id="inpaint1" style="width: 336px;"/>
    <div class="beer-reveal" style="border-right: 3px white solid;">
        <img class="slider_img" id="inpaint2"/>
    </div>
    </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Interactive demo</strong>: click on an image on the left to see
an instance of inpainting for that image.
</div>

<p>Interestingly, even when our method produces reconstructions that differ from
the original, they are often still perceptually plausible to a human. Here are a
few select samples:</p>

<p><img alt="Failure modes of inpainting using robust models" src="http://gradientscience.org/assets/rf-vision/inpainting_errors.jpeg"/></p>
<div class="footnote">
Even when inpainting fails to recover the original uncorrupted image, the result is 
often still perceptually plausible to a human.
</div>

<h3 id="superresolution">Superresolution</h3>

<p>To perform inpainting, we used class maximization to restore relevant features
in corrupted images. The exact same intuition applies to the task of
superresolution, i.e., improving the resolution of an image in a human-
meaningful way. Specifically, using class maximization towards the underlying
true class, we can accentuate image features that are distorted in the low-
resolution image. Here we apply this method to images from the CIFAR10 dataset
(32x32 pixels) to a resolution of 224x224 (7-fold upsampling):</p>

<div class="widget">
    <div class="choices_one" id="upsample">
    <span class="widgetheading">Choose an Image</span>
    </div>
    <span class="widgetheading">Upsampled Image</span>
    <div class="beer-slider selected_one" id="upsample_slider">
    <img id="upsample1" style="width: 336px;"/>
    <div class="beer-reveal" style="border-right: 3px white solid;">
        <img class="slider_img" id="upsample2"/>
    </div>
    </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Interactive demo</strong>: click on any of the images on the left to see its
7-fold superresolution.
</div>

<p>Since the starting point of the underlying class maximization comes from a crude
upsampling (i.e., nearest neighbor interpolation) of the low-resolution image,
the final images exhibit some pixelation artifacts. We expect, however, that
combining this approach with a more sophisticated initialization will yield even
more realistic samples.</p>

<h3 id="interactive-image-manipulation">Interactive Image Manipulation</h3>

<p>Finally, using this simple primitive, one can build an interactive toolkit for
performing input space manipulations.</p>

<h4 id="sketch-to-image">Sketch-to-image</h4>

<p>Class maximization with robust classifiers turns out to yield human-meaningful
transformations even for <em>arbitrary</em> inputs. This enables us to use this
primitive to even transform hand-drawn sketches into realistic images. Here is
the result of maximizing a chosen class probability from a very crude sketch:</p>

<div class="widget">
    <div class="choices_one" id="sketch">
    <span class="widgetheading">Choose an Image</span>
    </div>
    <span class="widgetheading" id="sketchclass">Enhanced Image</span>
    <div class="beer-slider selected_one" id="sketch_slider">
    <img id="sketch1" style="width: 336px;"/>
    <div class="beer-reveal" style="border-right: 3px white solid;">
        <img class="slider_img" id="sketch2"/>
    </div>
    </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Interactive demo</strong>: select any of the sketches on the left to see it 
converted into a realistic image.
</div>

<p>You can draw realistic looking images with this method interactively
<a href="http://bit.ly/robustness_demo">here</a>, without any artistic skill!</p>

<h4 id="paint-with-features">Paint-with-Features</h4>

<p>In fact, we can achieve an even more fine-grained level of manipulation if we
directly perform maximization on the <em>representations</em> learned by the robust
model, instead of the class probabilities. (Recall that in the <a href="https://gradientscience.org/robust_reps/">last
post</a> we saw how individual components can
correspond to human-level features such as “stripes”.) By adding a human in the
loop, we can choose particular regions of the image to modify and specific
features to add. This leads to a versatile paint tool (inspired by
<a href="http://gandissect.res.ibm.com/ganpaint.html">GANpaint</a>) that can perform
manipulation such as this:</p>

<div class="widget">
    <div class="choices_one_paint" id="paint_left1">
    <span class="widgetheading">Choose an Image</span>
    </div>
    <div class="selected_three" id="paintclass">
    <!-- <span class="widgetheading">Feature painting</span> -->
    <video id="paint_video" style="width: 100%;">
        <source id="paint_selected_mp4" type="video/mp4">
        <source id="paint_selected_webm" type="video/webm">
    </source></source></video>
    </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Interactive demo</strong>: Choose one of the images on the left to see
a demonstration of painting high-level features onto the image.
</div>

<p>Here, we added a feature to a given region of the image by simply maximize the
corresponding activation (a single component of the robust representation
vector) while penalizing changes to the rest of the image. By successively
performing such <i>activation maximization</i> in different parts of the image,
we can paint with high-level concepts (e.g., grass or stripes).</p>

<h3 id="takeaways">Takeaways</h3>

<p>In this blog post, we applied simple, first-order manipulations of the
representation learned by a <em>single</em> robust classifier to perform a number of
computer vision tasks. This is contrast to prior approaches that
often required specialized
and sophisticated techniques. Crucially, to highlight the potential of the core
methodology itself, we used the same simple toolkit for all tasks and datasets,
and with minimal tuning and no task-specific optimizations. We expect that the
addition of domain knowledge and leveraging more perceptually-aligned notions of
robustness will further boost the performance of this toolkit. Importantly,
the models we use here are truly off-the-shelf and
are trained in a standard (and stable) manner (via
<a href="https://gradientscience.org/robust_opt_pt1/">robust optimization</a>).</p>

<p>Furthermore, our results highlight the utility of the basic classification
toolkit outside of classification tasks. We hope that our framework will expand to
offer ways to perform other vision tasks, on par with the existing
state-of-the-art techniques (e.g., based on generative models). Finally, our
findings highlight the merits of adversarial robustness as a goal that goes
beyond the security and reliability contexts this goal was considered in so far.</p></div>
    </summary>
    <updated>2019-06-06T00:00:00Z</updated>
    <published>2019-06-06T00:00:00Z</published>
    <source>
      <id>http://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="http://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="http://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2019-06-08T23:22:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/084</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/084" rel="alternate" type="text/html"/>
    <title>TR19-084 |  Resolution Lower Bounds for Refutation Statements | 

	Michal Garlik</title>
    <summary>For any unsatisfiable CNF formula we give an exponential lower bound on the size of resolution refutations of a propositional statement that the formula has a resolution refutation. We describe three applications. (1) An open question in [Atserias-Müller,2019] asks whether a certain natural propositional encoding of the above statement is hard for Resolution. We answer by giving an exponential size lower bound. (2) We show exponential resolution size lower bounds for reflection principles, thereby improving a result in [Atserias-Bonet,2004]. (3) We provide new examples of CNFs that exponentially separate Res(2) from Resolution (an exponential separation of these two proof systems was originally proved in [Segerlind-Buss-Impagliazzo,2004]).</summary>
    <updated>2019-06-05T22:34:40Z</updated>
    <published>2019-06-05T22:34:40Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-06-09T20:21:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/06/05/postdoc-position-in-tcs-at-lund-university-apply-by-june-14-2019/</id>
    <link href="https://cstheory-jobs.org/2019/06/05/postdoc-position-in-tcs-at-lund-university-apply-by-june-14-2019/" rel="alternate" type="text/html"/>
    <title>Postdoc position in TCS at Lund University (apply by June 14, 2019)</title>
    <summary>The CS Department at Lund University invites applications for a postdoc position in TCS. The application deadline is June 14, 2019. See https://lu.mynetworkglobal.com/en/what:job/jobID:270663/ for the full announcement with more information and instructions for how to apply. Informal enquiries are welcome and may be sent to jakob.nordstrom@cs.lth.se. Website: https://lu.mynetworkglobal.com/en/what:job/jobID:270663/ Email: jakob.nordstrom@cs.lth.se</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The CS Department at Lund University invites applications for a postdoc position in TCS. The application deadline is June 14, 2019. See <a href="https://lu.mynetworkglobal.com/en/what:job/jobID:270663/">https://lu.mynetworkglobal.com/en/what:job/jobID:270663/</a> for the full announcement with more information and instructions for how to apply. Informal enquiries are welcome and may be sent to jakob.nordstrom@cs.lth.se.</p>
<p>Website: <a href="https://lu.mynetworkglobal.com/en/what:job/jobID:270663/">https://lu.mynetworkglobal.com/en/what:job/jobID:270663/</a><br/>
Email: jakob.nordstrom@cs.lth.se</p></div>
    </content>
    <updated>2019-06-05T14:48:21Z</updated>
    <published>2019-06-05T14:48:21Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-06-09T20:21:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/083</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/083" rel="alternate" type="text/html"/>
    <title>TR19-083 |  Testing Graphs against an Unknown Distribution | 

	Lior Gishboliner, 

	Asaf Shapira</title>
    <summary>The area of graph property testing seeks to understand the relation between the global properties of a graph and its local statistics. In the classical model, the local statistics of a graph is defined relative to a uniform distribution over the graph’s vertex set. A graph property $\mathcal{P}$ is said to be testable if the local statistics of a graph can allow one to distinguish between graphs satisfying $\mathcal{P}$ and those that are far from satisfying it.

Goldreich recently introduced a generalization of this model in which one endows the vertex set of the input graph with an arbitrary and unknown distribution, and asked which of the properties that can be tested in the classical model can also be tested in this more general setting. We completely resolve this problem by giving a (surprisingly ``clean'') characterization of these properties. To this end, we prove a removal lemma for vertex weighted graphs which is of independent interest.</summary>
    <updated>2019-06-04T21:17:16Z</updated>
    <published>2019-06-04T21:17:16Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-06-09T20:21:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7512</id>
    <link href="https://windowsontheory.org/2019/06/04/itcs-20-call-for-papers-guest-post-by-thomas-vidick/" rel="alternate" type="text/html"/>
    <title>ITCS 20 call for papers (guest post by Thomas Vidick)</title>
    <summary>We invite you to submit your papers to the 11th Innovations inTheoretical Computer Science (ITCS). The conference will be held atthe University of Washington in Seattle, Washington from January 12-14,2020. ITCS seeks to promote research that carries a strong conceptual message(e.g., introducing a new concept, model or understanding, opening a newline of inquiry within traditional […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>

We invite you to submit your papers to the 11th Innovations in<br/>Theoretical Computer Science (ITCS). The conference will be held at<br/>the University of Washington in Seattle, Washington from January 12-14,<br/>2020.</p>



<p>ITCS seeks to promote research that carries a strong conceptual message<br/>(e.g., introducing a new concept, model or understanding, opening a new<br/>line of inquiry within traditional or interdisciplinary areas,<br/>introducing new mathematical techniques and methodologies, or new<br/>applications of known techniques). ITCS welcomes both conceptual and<br/>technical contributions whose contents will advance and inspire the<br/>greater theory community.</p>



<p>Submission deadline: September 9, 2019 (05:59pm PDT)<br/>Notification to authors: October 31, 2019<br/>Conference dates: January 12-14, 2020</p>



<p>See the website at <a href="http://itcs-conf.org/itcs20/itcs20-cfp.html" rel="noreferrer noopener" target="_blank">http://itcs-conf.org/itcs20/itcs20-cfp.html</a> for<br/>detailed information regarding submissions.</p>



<p>Program committee</p>



<p>Nikhil Bansal, CWI + TU Eindhoven<br/>Nir Bitansky, Tel-Aviv University<br/>Clement Canonne, Stanford<br/>Timothy Chan, University of Ilinois at Urbana-Champaign<br/>Edith Cohen, Google and Tel-Aviv University<br/>Shaddin Dughmi, University of Southern California<br/>Sumegha Garg, Princeton<br/>Ankit Garg, Microsoft research<br/>Ran Gelles, Bar-Ilan University<br/>Elena Grigorescu, Purdue<br/>Tom Gur, University of Warwick<br/>Sandy Irani, UC Irvine<br/>Dakshita Khurana, University of Illinois at Urbana-Champaign<br/>Antonina Kolokolova, Memorial University of Newfoundland.<br/>Pravesh Kothari, Carnegie Mellon University<br/>Rasmus Kyng, Harvard<br/>Katrina Ligett, Hebrew University<br/>Nutan Limaye, IIT Bombay<br/>Pasin Manurangsi, UC Berkeley<br/>Tamara Mchedlidze, Karlsruhe Institute of Technology<br/>Dana Moshkovitz, UT Austin<br/>Jelani Nelson, UC Berkeley<br/>Merav Parter, Weizmann Institute<br/>Krzysztof Pietrzak, IST Austria<br/>Elaine Shi, Cornell<br/>Piyush Srivastava, Tata Institute of Fundamental Research, Mumbai<br/>Li-Yang Tan, Stanford<br/>Madhur Tulsiani, TTIC<br/>Gregory Valiant, Stanford<br/>Thomas Vidick, California Institute of Technology (chair)<br/>Virginia Vassilevska Williams, MIT<br/>Ronald de Wolf, CWI and University of Amsterdam<br/>David Woodruff, Carnegie Mellon University

</p></div>
    </content>
    <updated>2019-06-04T21:16:57Z</updated>
    <published>2019-06-04T21:16:57Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>windowsontheory</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2019-06-09T20:22:01Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15923</id>
    <link href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/" rel="alternate" type="text/html"/>
    <title>A Quantum Connection For Matrix Rank</title>
    <summary>A new paper with Chaowen Guan Chaowen Guan is a PhD student at Buffalo. After a busy end to the Spring 2019 term at UB, we are getting time to write about our paper, “Stabilizer Circuits, Quadratic Forms, and Computing Matrix Rank.” Today we emphasize new connections we have found between simulating special quantum circuits […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>A new paper with Chaowen Guan</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/chaowenwhiteboard/" rel="attachment wp-att-15934"><img alt="" class="alignright wp-image-15934" height="130" src="https://rjlipton.files.wordpress.com/2019/06/chaowenwhiteboard.jpg?w=180&amp;h=130" width="180"/></a></p>
<p>
Chaowen Guan is a PhD student at Buffalo. After a busy end to the Spring 2019 term at UB, we are getting time to write about our <a href="https://arxiv.org/abs/1904.00101">paper</a>, “Stabilizer Circuits, Quadratic Forms, and Computing Matrix Rank.”</p>
<p>
Today we emphasize new connections we have found between simulating special quantum circuits and computing matrix rank over the field <img alt="{\mathbb{F}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{F}_2}"/>.<span id="more-15923"/></p>
<p>
The quantum circuits involved have been known as polynomial-time solvable <a href="https://en.wikipedia.org/wiki/Gottesman-Knill_theorem">since</a> <a href="https://arxiv.org/abs/quant-ph/9807006v1">1998</a>. They are not universal but form important building blocks of quantum systems people intend to build. They impact the problem of showing quantum circuits are more powerful than classical circuits—the <i>quantum advantage problem</i>—in terms of how much harder quantum stuff must be added to them. </p>
<p>
The question is: How efficiently can we simulate these special circuits? Our answer improves the bound from order-<img alt="{n^3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^3}"/> to <img alt="{n^{\omega}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B%5Comega%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^{\omega}}"/>, where <img alt="{\omega}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\omega}"/> here means the current best-known exponent for multiplying <img alt="{n \times n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n \times n}"/> matrices (over <img alt="{\mathbb{F}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{F}_2}"/> or any field). Today <img alt="{\omega}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\omega}"/> stands at <img alt="{2.3728\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2.3728%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2.3728\dots}"/>. The non-quantum problem of counting solutions to a quadratic polynomial <img alt="{f(x_1,\dots,x_n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x_1,\dots,x_n)}"/> modulo 2 is likewise improved from the <img alt="{O(n^3)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^3)}"/> <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.9881">shown</a> by Andrzej Ehrenfeucht and Marek Karpinski to <img alt="{O(n^\omega)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^\omega)}"/>.</p>
<p>
This comes at a price, however, because the matrix multiplication algorithms that optimize the exponent are <a href="https://en.wikipedia.org/wiki/Galactic_algorithm">galactic</a>. In this post we’ll emphasize what is <em>not</em> galactic: reductions to and from the problem of computing matrix rank that run in linear time—meaning <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2)}"/> time for dense matrices—except for the need to check a yes/no condition in one of them. All this builds on the algebraic methods in our <a href="https://link.springer.com/chapter/10.1007/978-3-662-56499-8_4">paper</a> last year with Amlan Chakrabarti of the University of Calcutta.</p>
<p>
Chaowen has contributed a <a href="https://rjlipton.wordpress.com/2018/07/02/local-hams-in-la-jolla/">post</a> and some other materials for this blog. His work first came up in a <a href="https://rjlipton.wordpress.com/2016/06/29/getting-to-the-roots-of-factoring/">post</a> three years ago that saluted Dick and Kathryn’s wedding. Today is their third anniversary—so this post also comes with happy anniversary wishes.</p>
<p>
</p><p/><h2> Strong Simulation Problems </h2><p/>
<p/><p>
We have <a href="https://rjlipton.wordpress.com/2010/08/02/quantum-algorithms-via-linear-algebra/">covered</a> <a href="https://rjlipton.wordpress.com/2010/08/25/quantum-algorithms-a-different-view-again/">quantum</a> <a href="https://rjlipton.wordpress.com/2011/10/26/quantum-chocolate-boxes/">algorithms</a> <a href="https://rjlipton.wordpress.com/2011/11/14/more-quantum-chocolate-boxes/">several</a> <a href="https://rjlipton.wordpress.com/2015/04/08/a-quantum-two-finger-exercise/">times</a>. We discussed <em>stabilizer circuits</em> in an early <a href="https://rjlipton.wordpress.com/2012/07/08/grilling-quantum-circuits/">post</a> on the work with Amlan and <a href="https://rjlipton.wordpress.com/2017/11/20/a-magic-madison-visit">covered</a> them more recently in connection with the work of Jin-Yi Cai’s group. Suffice it to say that stabilizer circuits—which extend Clifford circuits by allowing intermediate measurement gates—form the most salient case that classical computers can simulate in polynomial time.</p>
<p>
The simulation time is sometimes cited as <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2)}"/> going back to a 2004 <a href="https://arxiv.org/abs/quant-ph/0406196">paper</a> by Scott Aaronson and Daniel Gottesman, but there is a catch: this is only for one measurement of one qubit. For general (non-sparse) instances, all of <a href="https://arxiv.org/abs/quant-ph/0504117">various</a> <a href="https://arxiv.org/abs/1305.6190">other</a> <a href="https://web.eecs.umich.edu/~imarkov/pubs/conf/iccd13-quipu.pdf">algorithms</a> need order-<img alt="{n^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^2}"/> time to re-organize their data structures after each single-qubit measurement. This is so even if one merely wants to measure all <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> qubits in one shot: the time becomes <img alt="{O(n^3)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^3)}"/>. This is one case of what is generally called a <em>strong</em> simulation. It is precisely this time that Chaowen and I improved to <img alt="{O(n^\omega)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^\omega)}"/>.</p>
<p>
In wider contexts, strong simulation of a quantum circuit <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> means the ability to compute the probability of a given output to high precision. When the input and output are both in <img alt="{\{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{0,1\}^n}"/> we may suppose both are <img alt="{0^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0^n}"/> since we can prepend and append <img alt="{\mathsf{NOT}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNOT%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{NOT}}"/> gates to <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. Then strong simulation means computing the amplitude <img alt="{\langle 0^n |C| 0^n \rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\langle 0^n |C| 0^n \rangle}"/> (or computing <img alt="{|\langle 0^n |C| 0^n \rangle|^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|\langle 0^n |C| 0^n \rangle|^2}"/> which is the output probability) to <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-place precision. It doesn’t take much for this to be <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{NP}}"/>-hard, often <img alt="{\mathsf{\#P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{\#P}}"/>-complete. If we take the Clifford generating set </p>
<p align="center"><img alt="\displaystyle  \mathsf{H} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{bmatrix},\quad \mathsf{CZ} = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; -1 \end{bmatrix},\quad \mathsf{S} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; i \end{bmatrix}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BH%7D+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Cbegin%7Bbmatrix%7D+1+%26+1+%5C%5C+1+%26+-1+%5Cend%7Bbmatrix%7D%2C%5Cquad+%5Cmathsf%7BCZ%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%26+0+%5C%5C+0+%26+1+%26+0+%26+0+%5C%5C+0+%26+0+%26+1+%26+0+%5C%5C+0+%26+0+%26+0+%26+-1+%5Cend%7Bbmatrix%7D%2C%5Cquad+%5Cmathsf%7BS%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%5C%5C+0+%26+i+%5Cend%7Bbmatrix%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathsf{H} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{bmatrix},\quad \mathsf{CZ} = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; -1 \end{bmatrix},\quad \mathsf{S} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; i \end{bmatrix}, "/></p>
<p>then we can get universal circuits by adding any any one of the following gates: </p>
<p align="center"><img alt="\displaystyle  \mathsf{T} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; \sqrt{i} \end{bmatrix},\quad \mathsf{CS} = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; i \end{bmatrix},\quad \mathsf{Tof} = \mathit{diag}(1,1,1,1,1,1,\begin{bmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{bmatrix}). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathsf%7BT%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%5C%5C+0+%26+%5Csqrt%7Bi%7D+%5Cend%7Bbmatrix%7D%2C%5Cquad+%5Cmathsf%7BCS%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%26+0+%5C%5C+0+%26+1+%26+0+%26+0+%5C%5C+0+%26+0+%26+1+%26+0+%5C%5C+0+%26+0+%26+0+%26+i+%5Cend%7Bbmatrix%7D%2C%5Cquad+%5Cmathsf%7BTof%7D+%3D+%5Cmathit%7Bdiag%7D%281%2C1%2C1%2C1%2C1%2C1%2C%5Cbegin%7Bbmatrix%7D+0+%26+1+%5C%5C+1+%26+0+%5Cend%7Bbmatrix%7D%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathsf{T} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; \sqrt{i} \end{bmatrix},\quad \mathsf{CS} = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; i \end{bmatrix},\quad \mathsf{Tof} = \mathit{diag}(1,1,1,1,1,1,\begin{bmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{bmatrix}). "/></p>
<p>In the last one we’ve portrayed the <img alt="{8 \times 8}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B8+%5Ctimes+8%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{8 \times 8}"/> matrix of the <em>Toffoli gate</em> as being <em>block-diagonal</em>. We will later consider block-diagonal matrices permuted so that all <img alt="{2 \times 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2+%5Ctimes+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2 \times 2}"/> “blocks” are at upper left.</p>
<p>
There is <a href="https://arxiv.org/abs/1601.07601">much</a> <a href="https://arxiv.org/pdf/1808.00128.pdf">recent</a> <a href="https://arxiv.org/pdf/1712.03554.pdf">literature</a> on trying to simulate circuits with limited numbers of non-Clifford gates, and on how many such gates may be needed for <a href="https://arxiv.org/pdf/1902.04764.pdf">exponential</a> lower bounds—even just to tell whether <img alt="{\langle 0^n |C| 0^n \rangle \neq 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle+%5Cneq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\langle 0^n |C| 0^n \rangle \neq 0}"/>. This plays against a wider context of <a href="https://arxiv.org/abs/1608.00263">efforts</a> <a href="https://arxiv.org/abs/1807.10749">toward</a> <a href="https://arxiv.org/pdf/1905.00444.pdf">quantum</a> <a href="https://arxiv.org/abs/1203.5813">advantage</a>. Chaowen and I have been trying to apply algebraic-geometric techniques for new lower bounds at the high end, but this time we found new upper bounds at the low end.</p>
<p>
</p><p/><h2> From Matrix Rank to Quantum </h2><p/>
<p/><p>
It is not known how to compute the rank <img alt="{rk(A)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Brk%28A%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{rk(A)}"/> of a dense matrix <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> in better than matrix-multiplication time, even over <img alt="{\mathbb{F}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{F}_2}"/>. We may suppose <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> is square and symmetric, since we can always form the block matrix </p>
<p align="center"><img alt="\displaystyle  A' = \begin{bmatrix} 0 &amp; A^\top \\ A &amp; 0 \end{bmatrix} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A%27+%3D+%5Cbegin%7Bbmatrix%7D+0+%26+A%5E%5Ctop+%5C%5C+A+%26+0+%5Cend%7Bbmatrix%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  A' = \begin{bmatrix} 0 &amp; A^\top \\ A &amp; 0 \end{bmatrix} "/></p>
<p>and then <img alt="{rk(A) = \frac{1}{2}rk(A')}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Brk%28A%29+%3D+%5Cfrac%7B1%7D%7B2%7Drk%28A%27%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{rk(A) = \frac{1}{2}rk(A')}"/>. In the case of <img alt="{\mathbb{F}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{F}_2}"/>, <img alt="{A'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A'}"/> is the adjacency matrix <img alt="{A_G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_G}"/> of an undirected bipartite graph <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>. The rank of <img alt="{A_G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_G%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_G}"/> for any undirected graph <img alt="{G = (V,E)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG+%3D+%28V%2CE%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G = (V,E)}"/> must be even. Whereas the rank of the <img alt="{|V| \times |E|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CV%7C+%5Ctimes+%7CE%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|V| \times |E|}"/> vertex-edge incidence matrix always equals <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> minus the number of connected components of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>, less is <a href="http://web.cs.elte.hu/~lovasz/kurzusok/adjrank16.pdf">known</a> about characterizing <img alt="{rk(A_G)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Brk%28A_G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{rk(A_G)}"/>. Our first main theorem brings quantum strong simulation into the picture. Let <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> stand for <img alt="{n^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^2}"/>.</p>
<blockquote><p><b>Theorem 1</b> <em><a name="rank2QC"/> Given any <img alt="{A \in \mathbb{F}_2^{n \times n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA+%5Cin+%5Cmathbb%7BF%7D_2%5E%7Bn+%5Ctimes+n%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A \in \mathbb{F}_2^{n \times n}}"/> we can construct in <img alt="{O(N)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{O(N)}"/> time a stabilizer circuit <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{C}"/> on <img alt="{2n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{2n}"/> qubits such that </em></p><em>
<p align="center"><img alt="\displaystyle  rk(A) = \log_2(|\langle 0^{2n} |C| 0^{2n} \rangle|). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++rk%28A%29+%3D+%5Clog_2%28%7C%5Clangle+0%5E%7B2n%7D+%7CC%7C+0%5E%7B2n%7D+%5Crangle%7C%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  rk(A) = \log_2(|\langle 0^{2n} |C| 0^{2n} \rangle|). "/></p>
</em><p><em/>
</p></blockquote>
<p/><p>
One interpretation is that if you believe matrix rank is a “mildly hard” function (with regard to <img alt="{O(N)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(N)}"/>-time computability) then predicting the result of measuring all the qubits in a stabilizer circuit is also “mildly hard.” Such mild hardness would represent a <em>gap</em> between the <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2)}"/> time for weak simulation and the time for strong simulation. Such gaps have been noted and proved for extensions of stabilizer circuits but those are between “polynomial” and an intractable hardness notion.</p>
<p>
One can also view Theorem <a href="https://rjlipton.wordpress.com/feed/#rank2QC">1</a> as a possible avenue toward computing matrix rank without doing either matrix multiplication or Gaussian elimination. This is the view Chaowen and I have had all along. </p>
<p>
</p><p/><h2> From Quantum to Rank </h2><p/>
<p/><p>
The distinguishing point of our converse reduction to the rank <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> is <em>knowledge of normal forms that depend on <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/></em> where one can use the knowledge <em>to delay or avoid computing them explicitly</em>. The normal forms are for polynomials <img alt="{f_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_C}"/> associated to quantum circuits <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> in our <a href="https://rjlipton.wordpress.com/2012/07/08/grilling-quantum-circuits/">earlier</a> <a href="https://link.springer.com/chapter/10.1007/978-3-662-56499-8_4">work</a>. Stabilizer circuits yield <img alt="{f_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_C}"/> as a <em>classical quadratic form</em> over <img alt="{\mathbb{Z}_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_4}"/>, the integers modulo <img alt="{4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{4}"/>. That is, all cross terms <img alt="{x_i x_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_i+x_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_i x_j}"/> in <img alt="{f_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_C}"/> have even coefficients—here, <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> or <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>. Thus quantum computing enters a debate that occupied Carl Gauss and others over two hundred years ago:</p>
<blockquote><p><b> </b> <em> Should every homogeneous quadratic polynomial <img alt="{f(x_1,\dots,x_n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f(x_1,\dots,x_n)}"/> with integer coefficients be called a <b>quadratic form</b>, or only those whose cross terms <img alt="{c_{i,j}x_i x_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_%7Bi%2Cj%7Dx_i+x_j%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{c_{i,j}x_i x_j}"/> all have even coefficients <img alt="{c_{i,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_%7Bi%2Cj%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{c_{i,j}}"/>? </em>
</p></blockquote>
<p/><p>
The point of even coefficients is that they enable having a symmetric <img alt="{n \times n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n \times n}"/> <em>integer</em> matrix <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> such that </p>
<p align="center"><img alt="\displaystyle  f(x) = x^\top S x " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%29+%3D+x%5E%5Ctop+S+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f(x) = x^\top S x "/></p>
<p>for all <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>. Without that condition, <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> might only be half-integral. This old difference turns out to mirror that between universal quantum computing and classical, because the non-Clifford <img alt="{\mathsf{CS}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BCS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{CS}}"/>-gate noted above yields circuits <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> whose <img alt="{f_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_C}"/> over <img alt="{\mathbb{Z}_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_4}"/> have terms <img alt="{x_i x_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_i+x_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_i x_j}"/> and/or <img alt="{3 x_i x_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3+x_i+x_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3 x_i x_j}"/>. While counting solutions in <img alt="{\mathbb{Z}_4^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_4^n}"/> for those polynomials is in <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{P}}"/>, counting their <em>binary</em> solutions is <img alt="{\mathsf{\#P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{\#P}}"/>-complete—an amazing dichotomy we expounded <a href="https://rjlipton.wordpress.com/2017/11/20/a-magic-madison-visit/">here</a>.</p>
<p>
We hasten to add that for <img alt="{k = 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%3D+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k = 2}"/> the classical forms coincide with those over <img alt="{\mathbb{Z}_{2^k}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_%7B2%5Ek%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_{2^k}}"/> whose nonzero cross terms all have coefficient <img alt="{2^{k-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bk-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{k-1}}"/>. Those are called <em>affine</em> in the work by Jin-Yi and others noted above, and our above-mentioned <a href="https://rjlipton.wordpress.com/2017/11/20/a-magic-madison-visit">post</a> noted his 2017 <a href="https://arxiv.org/abs/1705.00942">paper</a> with Heng Guo and Tyson Williams giving another proof of polynomial-time simulation of stabilizer circuits via <img alt="{f_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_C}"/> being affine. Our work improving the polynomial bounds, however, draws on a 2009 <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.730.2154">paper</a> by Kai-Uwe Schmidt and further theory of classical quadratic forms. This paper uses work going back to 1938 that decomposes a classical (affine) quadratic form <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> over <img alt="{\mathsf{Z}_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{Z}_4}"/> further as <a name="repn"/></p><a name="repn">
<p align="center"><img alt="\displaystyle  f(x) = f_0(x) + 2(x \bullet v) \quad\text{with}\quad f_0(x) = x^\top B x, \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%29+%3D+f_0%28x%29+%2B+2%28x+%5Cbullet+v%29+%5Cquad%5Ctext%7Bwith%7D%5Cquad+f_0%28x%29+%3D+x%5E%5Ctop+B+x%2C+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f(x) = f_0(x) + 2(x \bullet v) \quad\text{with}\quad f_0(x) = x^\top B x, \ \ \ \ \ (1)"/></p>
</a><p><a name="repn"/> for binary arguments <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>. Here <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/> is a binary vector with <img alt="{v_i = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_i+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_i = 1}"/> if <img alt="{S[i,i] = 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%5Bi%2Ci%5D+%3D+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S[i,i] = 2}"/> or <img alt="{S[i,i] = 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%5Bi%2Ci%5D+%3D+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S[i,i] = 3}"/>, <img alt="{v_i =0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_i+%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_i =0}"/> otherwise, and the operations including the inner product <img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> are mod-2 except that the final <img alt="{+}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%2B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{+}"/> is in <img alt="{\mathbb{Z}_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_4}"/>. Then <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> is <em>alternating</em> if the diagonal of <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> is all-zero, <em>non-alternating</em> otherwise. Now take <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> to be the rank of <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/>. The key normal-form lemma is:</p>
<blockquote><p><b>Lemma 2</b> <em> There is a change of basis to <img alt="{y_1,\dots,y_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By_1%2C%5Cdots%2Cy_n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{y_1,\dots,y_n}"/> such that if <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f}"/> is non-alternating then <img alt="{f_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f_0}"/> is transformed to </em></p><em>
<p align="center"><img alt="\displaystyle  f'_0(y) = y_1 + y_2 + \cdots + y_r, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%27_0%28y%29+%3D+y_1+%2B+y_2+%2B+%5Ccdots+%2B+y_r%2C+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  f'_0(y) = y_1 + y_2 + \cdots + y_r, "/></p>
<p>whereas if <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f}"/> is alternating then <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{r}"/> is even and <img alt="{f_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f_0}"/> is transformed to </p>
<p align="center"><img alt="\displaystyle  f'_0(y) = 2y_1 y_2 + 2y_3 y_4 + \cdots + 2y_{r-1} y_r. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%27_0%28y%29+%3D+2y_1+y_2+%2B+2y_3+y_4+%2B+%5Ccdots+%2B+2y_%7Br-1%7D+y_r.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  f'_0(y) = 2y_1 y_2 + 2y_3 y_4 + \cdots + 2y_{r-1} y_r. "/></p>
</em><p><em>In either case, there is a binary vector <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{w}"/> so that <img alt="{f(y) = f'_0(y) + 2(y \bullet w)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28y%29+%3D+f%27_0%28y%29+%2B+2%28y+%5Cbullet+w%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f(y) = f'_0(y) + 2(y \bullet w)}"/> for all <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{y}"/>. </em>
</p></blockquote>
<p/><p>
The point is that to evaluate the quantum circuit <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>, we don’t need to evaluate <img alt="{f_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_C}"/>, but can make inferences about the structure of the solution sets to <img alt="{f_C(x) = a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_C%28x%29+%3D+a%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_C(x) = a}"/> for <img alt="{a = 0,1,2,3 \pmod{4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%3D+0%2C1%2C2%2C3+%5Cpmod%7B4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a = 0,1,2,3 \pmod{4}}"/>, where <img alt="{x \in \{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cin+%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x \in \{0,1\}^n}"/>. Given the knowledge of <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/>, the normal form goes a long way to this. The vector <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> is also needed, but the fact of its having only <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> bits gives hope of finding it in <img alt="{O(n^2) = O(N)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29+%3D+O%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2) = O(N)}"/> time. That—plus an analysis of the normal form <img alt="{f'_0,w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%27_0%2Cw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f'_0,w}"/> itself of course—would complete an <img alt="{O(N)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(N)}"/>-time reduction from computing the amplitude to computing <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/>.</p>
<p>
</p><p/><h2> The Needed Piece—For Now </h2><p/>
<p/><p>
Chaowen took the lead all through the Fall 2018 term in trying multiple attacks. In the non-alternating case, the change of basis converts <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> into a diagonal matrix <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> over <img alt="{\mathbb{F}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{F}_2}"/>. In the alternating case, the same process makes <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> a block-diagonal matrix of the kind we mentioned above. The conversion <img alt="{D = Q B Q^\top}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD+%3D+Q+B+Q%5E%5Ctop%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D = Q B Q^\top}"/> in both cases also yields <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/>. Of course <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q}"/> can be computed by Gaussian elimination in <img alt="{O(n^3)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E3%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^3)}"/> time, but this is what we wanted to avoid.</p>
<p>
After poring over older literature on <img alt="{n^\omega}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E%5Comega%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^\omega}"/>-time methods, including a 1974 <a href="https://www.ams.org/journals/mcom/1974-28-125/S0025-5718-1974-0331751-8/">paper</a> by James Bunch and John Hopcroft (see also <a href="http://renatoppl.com/blog/2014/08/12/solving-linear-systems-and-inverting-a-matrix-is-equivalent-to-matrix-multiplication/">this</a>), we found a <a href="https://arxiv.org/abs/1802.10453">paper</a> from last year by Jean-Guillaume Dumas and Clément Pernet that gives exactly what we needed: an LDU-type decomposition that yields <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> in <img alt="{O(n^\omega)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^\omega)}"/> time. We only needed to apply the change-of-basis analysis in Schmidt’s paper to this decomposition and combine with the normal-form analysis to establish our algorithm for computing the amplitude <img alt="{\langle 0^n |C| 0^n \rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\langle 0^n |C| 0^n \rangle}"/>: </p>
<ol>
<li>
Convert <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> to the classical quadratic form <img alt="{f_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_C}"/> with matrix <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> over <img alt="{\mathbb{Z}_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_4}"/> and associate the <img alt="{n \times n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n \times n}"/> matrix <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> over <img alt="{\mathbb{Z}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_2}"/> as above. This needs only <img alt="{O(N)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(N)}"/> time. <p/>
</li><li>
Compute the Dumas-Pernet decomposition <img alt="{B = PLDL^\top P^\top}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB+%3D+PLDL%5E%5Ctop+P%5E%5Ctop%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B = PLDL^\top P^\top}"/> over <img alt="{\mathbb{Z}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_2}"/> where <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> is a permutation matrix, <img alt="{L}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L}"/> is lower-triangular, and <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> is block-diagonal with blocks that are either <img alt="{1 \times 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%5Ctimes+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 \times 1}"/> or <img alt="{2 \times 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2+%5Ctimes+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2 \times 2}"/>. Of course, this involves computing the rank <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> of <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> and takes <img alt="{O(n^\omega)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^\omega)}"/> time. Think of it as <img alt="{D = QBQ^\top}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD+%3D+QBQ%5E%5Ctop%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D = QBQ^\top}"/>. This takes <img alt="{O(n^\omega)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^\omega)}"/> time—indeed, <img alt="{O(n^2 r^{\omega - 2})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2+r%5E%7B%5Comega+-+2%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2 r^{\omega - 2})}"/> time according to Dumas and Pernet. <p/>
</li><li>
Compute <img alt="{D' = Q S Q^\top}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%27+%3D+Q+S+Q%5E%5Ctop%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D' = Q S Q^\top}"/> over <img alt="{\mathbb{Z}_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_4}"/>. This, too, takes <img alt="{O(n^\omega)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^\omega)}"/> time. <p/>
</li><li>
If any diagonal <img alt="{1 \times 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%5Ctimes+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 \times 1}"/> block of the original <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> has become <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/> in <img alt="{D'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D'}"/>, output <img alt="{\langle 0^n |C| 0^n \rangle = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\langle 0^n |C| 0^n \rangle = 0}"/>. Else, <img alt="{\langle 0^n |C| 0^n \rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\langle 0^n |C| 0^n \rangle}"/> is nonzero and we have enough information about <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> and <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> to find it—in only <img alt="{O(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n)}"/> time, in fact.
</li></ol>
<p>
This proves our main theorem:</p>
<blockquote><p><b>Theorem 3</b> <em><a name="rank2QC"/> For stabilizer circuits <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{C}"/>, <img alt="{\langle 0^n |C| 0^n \rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\langle 0^n |C| 0^n \rangle}"/> is computable in <img alt="{O(n^\omega)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%5Comega%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{O(n^\omega)}"/> time. So is counting binary solutions to a classical quadratic form over <img alt="{\mathbb{Z}_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_4}"/>, or any quadratic polynomial mod 2. </em>
</p></blockquote>
<p/><p>
Because we use the decomposition, the above is not a clean <img alt="{O(N)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(N)}"/>-time reduction to computing <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/>. It does not make Theorem <a href="https://rjlipton.wordpress.com/feed/#rank2QC">3</a> into a linear-time equivalence. By further analysis, however, we show that the only impediment is needing <img alt="{D'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D'}"/> in step 4 of our algorithm to tell whether <img alt="{\langle 0^n |C| 0^n \rangle = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\langle 0^n |C| 0^n \rangle = 0}"/>. If we are <a href="https://rjlipton.wordpress.com/2010/09/05/promise-problems-and-twopaths/">promised</a> that it is nonzero, then we obtain the probability <img alt="{|\langle 0^n |C| 0^n \rangle|^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%5Clangle+0%5En+%7CC%7C+0%5En+%5Crangle%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|\langle 0^n |C| 0^n \rangle|^2}"/> in <img alt="{O(N)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(N)}"/> time from <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> alone. This is actually where the power of Chaowen’s analysis of the normal forms is brightest and neatest. We will devote further posts to this and to illuminating further connections in graph and matroid theory.</p>
<p>
</p><p/><h2> A Three-Part Example </h2><p/>
<p/><p>
Consider the following quantum circuit <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. OK, this is a very low-tech drawing. Besides the six Hadamard gates it has two <img alt="{\mathsf{CZ}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BCZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{CZ}}"/> gates, which are shown as simple bars since they are symmetric:</p>
<p>
<a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/c1/" rel="attachment wp-att-15927"><img alt="" class="aligncenter wp-image-15927" height="112" src="https://rjlipton.files.wordpress.com/2019/06/c1.png?w=240&amp;h=112" width="240"/></a></p>
<p>
By the rules given <a href="https://rjlipton.wordpress.com/2012/07/08/grilling-quantum-circuits/">here</a>, the three Hadamard gates at left introduce “nondeterministic variables” <img alt="{x_1,x_2,x_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_1%2Cx_2%2Cx_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_1,x_2,x_3}"/>. The three Hadamard gates at right also give nondeterministic variables, but they are immediately equated to the output variables <img alt="{z_1,z_2,z_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz_1%2Cz_2%2Cz_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z_1,z_2,z_3}"/> so we skip them. The polynomial <img alt="{q_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_C}"/> is </p>
<p align="center"><img alt="\displaystyle  2u_1 x_1 + 2u_2 x_2 + 2u_3 x_3 + 2x_1 x_2 + 2 x_2 x_3 + 2 x_1 z_1 + 2 x_2 z_2 + 2 x_3 z_3. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2u_1+x_1+%2B+2u_2+x_2+%2B+2u_3+x_3+%2B+2x_1+x_2+%2B+2+x_2+x_3+%2B+2+x_1+z_1+%2B+2+x_2+z_2+%2B+2+x_3+z_3.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  2u_1 x_1 + 2u_2 x_2 + 2u_3 x_3 + 2x_1 x_2 + 2 x_2 x_3 + 2 x_1 z_1 + 2 x_2 z_2 + 2 x_3 z_3. "/></p>
<p>Upon substituting <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> for all of <img alt="{u_1,u_2,u_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu_1%2Cu_2%2Cu_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u_1,u_2,u_3}"/> and <img alt="{z_1,z_2,z_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz_1%2Cz_2%2Cz_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z_1,z_2,z_3}"/> this gives simply <img alt="{f(x) = 2x_1 x_2 + 2 x_2 x_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29+%3D+2x_1+x_2+%2B+2+x_2+x_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x) = 2x_1 x_2 + 2 x_2 x_3}"/>. This is an alternating form with </p>
<p align="center"><img alt="\displaystyle  S = B = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S+%3D+B+%3D+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  S = B = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}, "/></p>
<p>which is the adjacency matrix of the path graph of length 2 on <img alt="{n = 3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n = 3}"/> vertices. Gaussian elimination does not need any prior swaps, so the permutation matrix <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> in the decomposition is the identity and we get </p>
<p align="center"><img alt="\displaystyle  Q = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix}, \quad\text{giving}\quad D = QBQ^\top = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{bmatrix} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5Cend%7Bbmatrix%7D%2C+%5Cquad%5Ctext%7Bgiving%7D%5Cquad+D+%3D+QBQ%5E%5Ctop+%3D+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+0+%5C%5C+0+%26+0+%26+0+%5Cend%7Bbmatrix%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  Q = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix}, \quad\text{giving}\quad D = QBQ^\top = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{bmatrix} "/></p>
<p>as the block-diagonal matrix over <img alt="{\mathbb{Z}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_2}"/>. Now we re-compute the products over <img alt="{\mathbb{Z}_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_4}"/> to get </p>
<p align="center"><img alt="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^\top \!=\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 2 &amp; 0 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 2 \\ 0 &amp; 2 &amp; 0 \end{bmatrix} \!=\! D'. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q+S+Q%5E%7B%5Ctop%7D+%5C%21%5C%21%3D%5C%21%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot+Q%5E%5Ctop+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+2+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5C%5C+0+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+2+%5C%5C+0+%26+2+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+D%27.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^\top \!=\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 2 &amp; 0 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 2 \\ 0 &amp; 2 &amp; 0 \end{bmatrix} \!=\! D'. "/></p>
<p>Now <img alt="{D'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D'}"/> has entries that are <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/> but they are off-diagonal, and hence cancel when <img alt="{y^\top D' y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%5E%5Ctop+D%27+y%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y^\top D' y}"/> is computed in the <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/>-basis. Since <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> is likewise the zero vector, this gives the transformed form as </p>
<p align="center"><img alt="\displaystyle  f'_0(y_1,y_2,y_3) = 2y_1 y_2. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%27_0%28y_1%2Cy_2%2Cy_3%29+%3D+2y_1+y_2.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f'_0(y_1,y_2,y_3) = 2y_1 y_2. "/></p>
<p>It is easy to compute that <img alt="{f'_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%27_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f'_0}"/> has six values of 0 and two values of 2, which gives the amplitude as the difference <img alt="{6 - 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B6+-+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{6 - 2}"/> divided by the square root of <img alt="{2^6}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E6%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^6}"/>, so <img alt="{\frac{1}{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{1}{2}}"/>, The probability of getting <img alt="{000}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B000%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{000}"/> as the result of the measurement is <img alt="{\frac{1}{4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{1}{4}}"/>.</p>
<p>
Now suppose we insert a <img alt="{\mathsf{Z}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{Z}}"/>-gate <img alt="{\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -1 \end{bmatrix}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbegin%7Bbmatrix%7D+1+%26+0+%5C%5C+0+%26+-1+%5Cend%7Bbmatrix%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -1 \end{bmatrix}}"/> on the first qubit to make a new circuit <img alt="{C_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_2}"/>. Since <img alt="{\mathsf{Z}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{Z}}"/> and <img alt="{\mathsf{CZ}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BCZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{CZ}}"/> are diagonal in the standard basis it does not matter where between the Hadamard gates it goes, say:</p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/c2/" rel="attachment wp-att-15928"><img alt="" class="aligncenter wp-image-15928" height="106" src="https://rjlipton.files.wordpress.com/2019/06/c2.png?w=240&amp;h=106" width="240"/></a></p>
<p/><p><br/>
After substituting zeroes the form over <img alt="{\mathbb{Z}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}}"/> is <img alt="{g = 2x_1 x_2 + 2 x_2 x_3 + 2x_1^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg+%3D+2x_1+x_2+%2B+2+x_2+x_3+%2B+2x_1%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g = 2x_1 x_2 + 2 x_2 x_3 + 2x_1^2}"/>. This gives </p>
<p align="center"><img alt="\displaystyle  S = \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix},\quad v = (1,0,0), \quad B = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S+%3D+%5Cbegin%7Bbmatrix%7D+2+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D%2C%5Cquad+v+%3D+%281%2C0%2C0%29%2C+%5Cquad+B+%3D+%5Cbegin%7Bbmatrix%7D+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  S = \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix},\quad v = (1,0,0), \quad B = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}. "/></p>
<p>The matrix <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> is the same as in the first example, hence so are the matrices <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q}"/> and <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> and the alternating status of <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/>. The difference made by <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/> and the resulting <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> makes itself felt when we re-compute over <img alt="{\mathbb{Z}_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_4}"/>: </p>
<p align="center"><img alt="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^\top \!=\! \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 2 &amp; 2 &amp; 0 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 2 &amp; 1 &amp; 2 \\ 1 &amp; 0 &amp; 2 \\ 2 &amp; 2 &amp; 2 \end{bmatrix} \!=\! D'. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q+S+Q%5E%7B%5Ctop%7D+%5C%21%5C%21%3D%5C%21%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+0+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21+%5Cbegin%7Bbmatrix%7D+2+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot+Q%5E%5Ctop+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+2+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+2+%26+2+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5C%5C+0+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+2+%26+1+%26+2+%5C%5C+1+%26+0+%26+2+%5C%5C+2+%26+2+%26+2+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+D%27.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^\top \!=\! \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 2 &amp; 2 &amp; 0 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 2 &amp; 1 &amp; 2 \\ 1 &amp; 0 &amp; 2 \\ 2 &amp; 2 &amp; 2 \end{bmatrix} \!=\! D'. "/></p>
<p>Well, <img alt="{D'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D'}"/> is far from diagonal—perhaps we shouldn’t use that name—but again the off-diagonal <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>s are innocuous so we really have </p>
<p align="center"><img alt="\displaystyle  D'' = \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 2 \end{bmatrix}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%27%27+%3D+%5Cbegin%7Bbmatrix%7D+2+%26+1+%26+0+%5C%5C+1+%26+0+%26+0+%5C%5C+0+%26+0+%26+2+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  D'' = \begin{bmatrix} 2 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 2 \end{bmatrix}. "/></p>
<p>The <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/> at upper left does not zero out the amplitude, because it is within a <img alt="{2 \times 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2+%5Ctimes+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2 \times 2}"/> block. The <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/> at lower right, however, constitutes a <img alt="{1 \times 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%5Ctimes+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 \times 1}"/> block of <img alt="{D''}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%27%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D''}"/>, so it signifies that <img alt="{000}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B000%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{000}"/> is not a possible measurement outcome. Essentially what has happened is that in the <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/>-basis the form has become </p>
<p align="center"><img alt="\displaystyle  g'(y) = 2y_1^2 + 2y_1 y_2 + 2y_3^2. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%27%28y%29+%3D+2y_1%5E2+%2B+2y_1+y_2+%2B+2y_3%5E2.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  g'(y) = 2y_1^2 + 2y_1 y_2 + 2y_3^2. "/></p>
<p>The isolated term in <img alt="{y_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y_3}"/> contributes <img alt="{+2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%2B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{+2}"/> mod <img alt="{4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{4}"/> to half the <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>–<img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> assignments so as to cancel the other half, leaving a difference of <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> in the numerator of the amplitude.</p>
<p>
For the third example, let us insert a phase gate <img alt="{\mathsf{S}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{S}}"/> after the <img alt="{\mathsf{Z}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{Z}}"/> to make a circuit <img alt="{C_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_3}"/>:</p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2019/06/04/a-quantum-connection-for-matrix-rank/c3/" rel="attachment wp-att-15929"><img alt="" class="aligncenter wp-image-15929" height="90" src="https://rjlipton.files.wordpress.com/2019/06/c3.png?w=240&amp;h=90" width="240"/></a></p>
<p/><p><br/>
The <img alt="{\mathsf{ZS}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{ZS}}"/> combination is the same as <img alt="{\mathsf{S^*}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BS%5E%2A%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{S^*}}"/>, the adjoint (and inverse) of <img alt="{\mathsf{S}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{S}}"/>. Now after substitutions we have <img alt="{h_{C_3}(x) = 2x_1 x_2 + 2 x_2 x_3 + 3x_1^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh_%7BC_3%7D%28x%29+%3D+2x_1+x_2+%2B+2+x_2+x_3+%2B+3x_1%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h_{C_3}(x) = 2x_1 x_2 + 2 x_2 x_3 + 3x_1^2}"/>, giving: </p>
<p align="center"><img alt="\displaystyle  S = \begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix},\quad v = (1,0,0), \quad B = \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++S+%3D+%5Cbegin%7Bbmatrix%7D+3+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D%2C%5Cquad+v+%3D+%281%2C0%2C0%29%2C+%5Cquad+B+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  S = \begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix},\quad v = (1,0,0), \quad B = \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix}. "/></p>
<p>Note that <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> is still a 0-1 matrix. This <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> has full rank. Again it helps our exposition that <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> is diagonalizable without swaps (and that the inverse of an invertible lower-triangular matrix is lower-triangular), so we can find <img alt="{QBQ^\top = D = I}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQBQ%5E%5Ctop+%3D+D+%3D+I%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{QBQ^\top = D = I}"/> with </p>
<p align="center"><img alt="\displaystyle  Q = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \end{bmatrix}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+1+%26+1+%26+0+%5C%5C+1+%26+1+%26+1+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  Q = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \end{bmatrix}. "/></p>
<p>In the <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/>-basis we get <img alt="{h'(y) = y_1 + y_2 + y_3 + 2(y \bullet w)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%27%28y%29+%3D+y_1+%2B+y_2+%2B+y_3+%2B+2%28y+%5Cbullet+w%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h'(y) = y_1 + y_2 + y_3 + 2(y \bullet w)}"/> for some <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/>. To test for zero amplitude—before we know what <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> is—we compute in <img alt="{\mathbb{Z}_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_4}"/>: </p>
<p align="center"><img alt="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \end{bmatrix} \!\cdot\!\begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^{\top} \!=\! \begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 2 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 3 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 2 \\ 0 &amp; 2 &amp; 3 \end{bmatrix} \!=\! D'. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Q+S+Q%5E%7B%5Ctop%7D+%5C%21%5C%21%3D%5C%21%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+1+%26+1+%26+0+%5C%5C+0+%26+1+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21%5Cbegin%7Bbmatrix%7D+3+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot+Q%5E%7B%5Ctop%7D+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+3+%26+1+%26+0+%5C%5C+0+%26+1+%26+1+%5C%5C+0+%26+2+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%5Ccdot%5C%21+%5Cbegin%7Bbmatrix%7D+1+%26+1+%26+1+%5C%5C+0+%26+1+%26+1+%5C%5C+0+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+%5Cbegin%7Bbmatrix%7D+3+%26+0+%26+0+%5C%5C+0+%26+1+%26+2+%5C%5C+0+%26+2+%26+3+%5Cend%7Bbmatrix%7D+%5C%21%3D%5C%21+D%27.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  Q S Q^{\top} \!\!=\!\! \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \end{bmatrix} \!\cdot\!\begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \!\cdot Q^{\top} \!=\! \begin{bmatrix} 3 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 2 &amp; 1 \end{bmatrix} \!\cdot\! \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \!=\! \begin{bmatrix} 3 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 2 \\ 0 &amp; 2 &amp; 3 \end{bmatrix} \!=\! D'. "/></p>
<p>Again we can ignore the off-diagonal <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>‘s. There is no <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/> on the main diagonal, so we know the amplitude is non-zero. To compute it, we only need the information on the diagonal, which tells us <img alt="{h'_0(y) = y_1 + y_2 + y_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%27_0%28y%29+%3D+y_1+%2B+y_2+%2B+y_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h'_0(y) = y_1 + y_2 + y_3}"/> and <img alt="{w = (1,0,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw+%3D+%281%2C0%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w = (1,0,1)}"/> in the transformed basis. Note that we could have written <img alt="{h'_0(y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%27_0%28y%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h'_0(y)}"/> down the moment we learned that <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> has rank <img alt="{3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3}"/> over <img alt="{\mathbb{F}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BF%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{F}_2}"/>, so <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> is the only rigmarole. The final analysis—using a recursion detailed in the appendix of our paper—gives the amplitude as </p>
<p align="center"><img alt="\displaystyle  \frac{2 - 2i}{8} = \frac{1 - i}{4}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B2+-+2i%7D%7B8%7D+%3D+%5Cfrac%7B1+-+i%7D%7B4%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac{2 - 2i}{8} = \frac{1 - i}{4}, "/></p>
<p>and so the probability of the output <img alt="{000}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B000%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{000}"/> is <img alt="{\frac{1}{8}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B8%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{1}{8}}"/>. </p>
<p>
We remark finally that <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w}"/> is generally not the same as <img alt="{Qv}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Qv}"/>. To see where it comes from, let us now compute <img alt="{Q B Q^{\top}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ+B+Q%5E%7B%5Ctop%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q B Q^{\top}}"/> (not <img alt="{QSQ^\top}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQSQ%5E%5Ctop%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{QSQ^\top}"/>) over <img alt="{\mathbb{Z}_4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_4}"/> to get <img alt="{QBQ^\top = D + 2U}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQBQ%5E%5Ctop+%3D+D+%2B+2U%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{QBQ^\top = D + 2U}"/>. Then </p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}  f(x) &amp;=&amp; x^\top B x + 2x^\top v = x^\top Q^{-1} (D+2U) (Q^\top)^{-1} x + 2x^\top (Q^{-1} Q) v\\ &amp;=&amp; y^\top (D + 2U) y + 2 y^\top Qv, \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++f%28x%29+%26%3D%26+x%5E%5Ctop+B+x+%2B+2x%5E%5Ctop+v+%3D+x%5E%5Ctop+Q%5E%7B-1%7D+%28D%2B2U%29+%28Q%5E%5Ctop%29%5E%7B-1%7D+x+%2B+2x%5E%5Ctop+%28Q%5E%7B-1%7D+Q%29+v%5C%5C+%26%3D%26+y%5E%5Ctop+%28D+%2B+2U%29+y+%2B+2+y%5E%5Ctop+Qv%2C+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}  f(x) &amp;=&amp; x^\top B x + 2x^\top v = x^\top Q^{-1} (D+2U) (Q^\top)^{-1} x + 2x^\top (Q^{-1} Q) v\\ &amp;=&amp; y^\top (D + 2U) y + 2 y^\top Qv, \end{array} "/></p>
<p>where <img alt="{y = (Q^\top)^{-1} x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By+%3D+%28Q%5E%5Ctop%29%5E%7B-1%7D+x%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y = (Q^\top)^{-1} x}"/>. Now off-diagonal elements in <img alt="{2U}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2U%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2U}"/> will cancel when taking <img alt="{2 y^\top U y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2+y%5E%5Ctop+U+y%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2 y^\top U y}"/> modulo 4, so we need only retain the diagonal <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u}"/> of <img alt="{U}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BU%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{U}"/> as a binary vector. Since <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> is binary, <img alt="{y^\top \mathit{diag}(u) y = y^\top u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%5E%5Ctop+%5Cmathit%7Bdiag%7D%28u%29+y+%3D+y%5E%5Ctop+u%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y^\top \mathit{diag}(u) y = y^\top u}"/>. This finally gives </p>
<p align="center"><img alt="\displaystyle  f(x) = y^\top D y + 2y^\top (u + Qv) = y^\top D y + 2(y \bullet w) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f%28x%29+%3D+y%5E%5Ctop+D+y+%2B+2y%5E%5Ctop+%28u+%2B+Qv%29+%3D+y%5E%5Ctop+D+y+%2B+2%28y+%5Cbullet+w%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f(x) = y^\top D y + 2y^\top (u + Qv) = y^\top D y + 2(y \bullet w) "/></p>
<p>with <img alt="{w = u + Qv}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw+%3D+u+%2B+Qv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w = u + Qv}"/>. In the third example we have <img alt="{Qv = (1,1,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQv+%3D+%281%2C1%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Qv = (1,1,1)}"/> and </p>
<p align="center"><img alt="\displaystyle  QBQ^{\top} = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \cdot Q^\top = \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 2 &amp; 1 &amp; 1 \\ 2 &amp; 2 &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} = \begin{bmatrix} 1 &amp; 2 &amp; 2 \\ 2 &amp; 3 &amp; 0 \\ 2 &amp; 0 &amp; 1 \end{bmatrix}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++QBQ%5E%7B%5Ctop%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+0+%26+0+%5C%5C+1+%26+1+%26+0+%5C%5C+0+%26+1+%26+1+%5Cend%7Bbmatrix%7D+%5Ccdot+%5Cbegin%7Bbmatrix%7D+1+%26+1+%26+0+%5C%5C+1+%26+0+%26+1+%5C%5C+0+%26+1+%26+0+%5Cend%7Bbmatrix%7D+%5Ccdot+Q%5E%5Ctop+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+1+%26+0+%5C%5C+2+%26+1+%26+1+%5C%5C+2+%26+2+%26+1+%5Cend%7Bbmatrix%7D+%5Ccdot+%5Cbegin%7Bbmatrix%7D+1+%26+1+%26+1+%5C%5C+0+%26+1+%26+1+%5C%5C+0+%26+0+%26+1+%5Cend%7Bbmatrix%7D+%3D+%5Cbegin%7Bbmatrix%7D+1+%26+2+%26+2+%5C%5C+2+%26+3+%26+0+%5C%5C+2+%26+0+%26+1+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  QBQ^{\top} = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \cdot Q^\top = \begin{bmatrix} 1 &amp; 1 &amp; 0 \\ 2 &amp; 1 &amp; 1 \\ 2 &amp; 2 &amp; 1 \end{bmatrix} \cdot \begin{bmatrix} 1 &amp; 1 &amp; 1 \\ 0 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix} = \begin{bmatrix} 1 &amp; 2 &amp; 2 \\ 2 &amp; 3 &amp; 0 \\ 2 &amp; 0 &amp; 1 \end{bmatrix}. "/></p>
<p>The diagonal gives <img alt="{u = (0,1,0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu+%3D+%280%2C1%2C0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u = (0,1,0)}"/> and so <img alt="{w = u + Qv \pmod{2} = (1,0,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw+%3D+u+%2B+Qv+%5Cpmod%7B2%7D+%3D+%281%2C0%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w = u + Qv \pmod{2} = (1,0,1)}"/>. This agrees with what we read off above by comparing <img alt="{D'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D'}"/> with <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/>. There is a different worked-out example for the triangle graph on three vertices in the paper.</p>
<p>
</p><p/><h2> Looking Ahead </h2><p/>
<p/><p>
Chaowen and I continue to be interested in shortcuts to computing the amplitude and/or probability. Here we take a cue from how Volker Strassen titled his famous 1969 <a href="https://eudml.org/doc/131927">paper</a> on matrix multiplication:</p>
<blockquote><p><b> </b> <em> “Gaussian Elimination is not Optimal.” </em>
</p></blockquote>
<p/><p>
We would like to find cases where we can say, “Matrix Multiplication is not Optimal.” In view of recent papers blunting efforts to show <img alt="{\omega = 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Comega+%3D+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\omega = 2}"/>—see this <a href="https://rjlipton.wordpress.com/2018/08/30/limits-on-matrix-multiplication/">post</a>—the question may shift to which computations may not need the full power of matrix multiplication and be achievable in <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2)}"/> time after all. This applies to computing the rank (over <img alt="{\mathbb{Z}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BZ%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{Z}_2}"/>) itself, and the question extends to sparse cases like those considered in the <a href="http://www-scf.usc.edu/~hoyeeche/papers/matrix-rank.pdf">paper</a>, “Fast Matrix Rank Algorithms and Applications,” by Ho Yee Cheung, Tsz Chiu Kwok, and Lap Chi Lau.</p>
<p>
The second circuit in the above example corresponds to a graph with a self-loop at node 1—or, depending on how one counts incidence of self-loops in undirected graphs, one could call it a double self-loop. It exemplifies circuits used to create quantum <a href="https://en.wikipedia.org/wiki/Graph_state">graph states</a>, and those circuits are representative of stabilizer circuits in general. The third circuit can be said to have a “triple loop,” or maybe better, a “3/2-loop”—while if the original <img alt="{\mathsf{Z}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BZ%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{Z}}"/>-gate were a single <img alt="{\mathsf{S}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{S}}"/>-gate giving the form <img alt="{x_1^2 + 2x_1 x_2 + 2 x_2 x_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_1%5E2+%2B+2x_1+x_2+%2B+2+x_2+x_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_1^2 + 2x_1 x_2 + 2 x_2 x_3}"/>, we would face the ambiguity of calling it a “loop” or a “half-loop.” Sorting this out properly needs going beyond graph theory. In upcoming posts, Chaowen and I will say more about how all this yields new problems in graph theory and new connections between quantum computing and <em>matroid theory</em>.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
What do our results say about the problem of computing the rank of a matrix, and possibly separating it from dependence on matrix multiplication?</p>
<p>
We hope that we have begun to convey how our paper uncovers a lot of fun computational mathematics. We are grateful for communications from people we’ve approached (some acknowledged in our paper) about possible known connections, but there may be more we don’t know. Our next posts will say more about combinatorial aspects of quantum circuits.</p>
<p>
[fixed name]</p></font></font></div>
    </content>
    <updated>2019-06-04T19:55:03Z</updated>
    <published>2019-06-04T19:55:03Z</published>
    <category term="algorithms"/>
    <category term="All Posts"/>
    <category term="News"/>
    <category term="Proofs"/>
    <category term="Results"/>
    <category term="algebra"/>
    <category term="Chaowen Guan"/>
    <category term="concrete complexity"/>
    <category term="counting"/>
    <category term="linear algebra"/>
    <category term="polynomial simulation"/>
    <category term="quadratic forms"/>
    <category term="quantum"/>
    <category term="solution sets"/>
    <category term="stabilizer circuits"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-06-09T20:21:15Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1500</id>
    <link href="https://theorydish.blog/2019/06/04/itcs20-call-for-papers/" rel="alternate" type="text/html"/>
    <title>ITCS’20 Call for Papers</title>
    <summary>ITCS is one of my favorite (if not my favorite) conferences, with not only great and insightful papers but also a friendly atmosphere. This year should be no exception! tl;dr: the ITCS’20 CFP has been posted. Read it, and submit your work there!   We invite you to submit your papers to the 11th Innovations in Theoretical Computer Science (ITCS). The conference will be held at the University of Washington in Seattle, Washington from January 12-14, 2020. ITCS seeks to promote research that carries a strong conceptual message (e.g., introducing a new concept, model or understanding, opening a new line of inquiry within traditional or interdisciplinary areas, introducing new mathematical techniques and methodologies, or new applications of known techniques). ITCS welcomes both conceptual and technical contributions whose contents will advance and inspire the greater theory community. Important dates Submission deadline: September 9, 2019 (05:59pm PDT) Notification to authors: October 31, 2019 Conference dates: January 12-14, 2020 See the website at http://itcs-conf.org/itcs20/itcs20-cfp.html for detailed information regarding submissions. Program committee Nikhil Bansal, CWI + TU Eindhoven Nir Bitansky, Tel-Aviv University Clement Canonne, Stanford Timothy Chan, University of Ilinois at Urbana-Champaign Edith Cohen, Google and Tel-Aviv University Shaddin Dughmi, University of Southern California Sumegha Garg, [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p style="text-align: justify;"><em>ITCS is one of my favorite (if not my favorite) conferences, with not only great and insightful papers but also a friendly atmosphere. This year should be no exception!</em></p>
<p><em><strong>tl;dr:</strong> the ITCS’20 CFP has been <a href="http://itcs-conf.org/itcs20/itcs20-cfp.html" rel="noopener" target="_blank">posted</a>. Read it, and submit your work there!</em></p>
<hr/>
<p> </p>
<p style="text-align: justify;">We invite you to submit your papers to the <a href="http://itcs-conf.org/" rel="noopener" target="_blank">11th Innovations in</a> <a href="http://itcs-conf.org/" rel="noopener" target="_blank">Theoretical Computer Science</a> (ITCS). The conference will be held at the University of Washington in Seattle, Washington from January 12-14, 2020.</p>
<p style="text-align: justify;">ITCS seeks to promote research that carries a strong conceptual message (e.g., introducing a new concept, model or understanding, opening a new line of inquiry within traditional or interdisciplinary areas, introducing new mathematical techniques and methodologies, or new applications of known techniques). ITCS welcomes both conceptual and technical contributions whose contents will advance and inspire the<br/>
greater theory community.</p>
<p><strong>Important dates</strong></p>
<ul>
<li><em>Submission deadline:</em> September 9, 2019 (05:59pm PDT)</li>
<li><em>Notification to authors:</em> October 31, 2019</li>
<li><em>Conference dates:</em> January 12-14, 2020</li>
</ul>
<p>See the website at <a href="http://itcs-conf.org/itcs20/itcs20-cfp.html">http://itcs-conf.org/itcs20/itcs20-cfp.html</a> for detailed information regarding submissions.</p>
<p><strong>Program committee</strong></p>
<p>Nikhil Bansal, CWI + TU Eindhoven<br/>
Nir Bitansky, Tel-Aviv University<br/>
Clement Canonne, Stanford<br/>
Timothy Chan, University of Ilinois at Urbana-Champaign<br/>
Edith Cohen, Google and Tel-Aviv University<br/>
Shaddin Dughmi, University of Southern California<br/>
Sumegha Garg, Princeton<br/>
Ankit Garg, Microsoft research<br/>
Ran Gelles, Bar-Ilan University<br/>
Elena Grigorescu, Purdue<br/>
Tom Gur, University of Warwick<br/>
Sandy Irani, UC Irvine<br/>
Dakshita Khurana, University of Illinois at Urbana-Champaign<br/>
Antonina Kolokolova, Memorial University of Newfoundland.<br/>
Pravesh Kothari, Carnegie Mellon University<br/>
Rasmus Kyng, Harvard<br/>
Katrina Ligett, Hebrew University<br/>
Nutan Limaye, IIT Bombay<br/>
Pasin Manurangsi, UC Berkeley<br/>
Tamara Mchedlidze, Karlsruhe Institute of Technology<br/>
Dana Moshkovitz, UT Austin<br/>
Jelani Nelson, UC Berkeley<br/>
Merav Parter, Weizmann Institute<br/>
Krzysztof Pietrzak, IST Austria<br/>
Elaine Shi, Cornell<br/>
Piyush Srivastava, Tata Institute of Fundamental Research, Mumbai<br/>
Li-Yang Tan, Stanford<br/>
Madhur Tulsiani, TTIC<br/>
Gregory Valiant, Stanford<br/>
Thomas Vidick, California Institute of Technology (chair)<br/>
Virginia Vassilevska Williams, MIT<br/>
Ronald de Wolf, CWI and University of Amsterdam<br/>
David Woodruff, Carnegie Mellon University</p></div>
    </content>
    <updated>2019-06-04T18:58:43Z</updated>
    <published>2019-06-04T18:58:43Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>ccanonne</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2019-06-09T20:22:42Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3116995407343145604</id>
    <link href="https://blog.computationalcomplexity.org/feeds/3116995407343145604/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/06/imus-non-controversial-changing-name-of.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3116995407343145604" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3116995407343145604" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/06/imus-non-controversial-changing-name-of.html" rel="alternate" type="text/html"/>
    <title>IMU's non-controversial changing the name of the Nevanlinna Prize</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">(I want to thank Alexander Soifer for supplying me with some of the documents I point to in this post. We should all thank him for getting the ball rolling on changing the name of the Nevanlinna Prize.)<br/>
<br/>
The <i>Nevanlinna Prize </i>was essentially a Fields Medal for Theoretical Computer Science.  I do not know why it is a<i> Prize </i>instead of a <i>Medal.</i><br/>
<div>
<br/></div>
<div>
It has been renamed <i>The Abacus Medal. </i>If you want to know why the IMU (International Mathematics Union) thinks the new name is good <i>but do not </i><i>care even a little about why the original name was bad</i> then see this article: <a href="https://www.heidelberg-laureate-forum.org/blog/imu-abacus-medal/">here</a>.</div>
<div>
<br/></div>
<div>
So why is <i>The Nevanlinna Prize</i> a bad name? In brief, Rolf Nevanlinna was an enthusiastic Nazi sympathizer. How enthused? He served as the chair of the Finish SS recruitment committee.<br/>
<br/>
That would seem like enough to get the name changed. In fact, it makes one wonder why the prize originally had the name.<br/>
<br/>
1) Why the change now?  It began when Alexander Soifer came across this information about Nevanlinna while working on his book<br/>
<br/>
<i>The Scholar and the State: In Search of Van der Waerdan</i> (see <a href="https://amzn.to/2WnfDYh">here</a> to buy it, see <a href="https://mathcs.clarku.edu/~fgreen/SIGACTReviews/bookrev/47-1.pdf">here</a> for a book review column that includes my review of it).<br/>
<br/>
He then wrote a letter to the IMU which sponsors the <i>Nevanlinna Prize</i>. The letter is <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/letterToImu.pdf">here</a>. Note that Alexander offered to pay for the prize ($15,000 every four years) if that will help get the name changed.<br/>
<br/>
After a response that lamely said (I paraphrase): <i>Gee, we didn't know. Oh well</i>. Alex wrote another letter which is <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/letterToImu2.pdf">here</a>.<br/>
<br/>
The story has a happy ending: the name was changed.  (No, Alexander is not paying for the award.)<br/>
<br/>
2) For a full summary of why the award was originally named Nevanlinna  and why it was changed see the article, <i>Yes We Can,  </i>by Alexander Soifer,<i> </i>in an issue of the journal <i>Mathematical Competition</i>s, see <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/yeswecan.pdf">here</a>.</div>
<div>
<br/></div>
<div>
3) When is change possible?<br/>
<br/></div>
<div>
 Assume Y did X and X is awful (e.g., I assume for most of my readers believing and spreading Nazi propaganda). Assume there is a Y-prize. What does it take to have the name changed?<br/>
<br/></div>
<div>
<br/></div>
<div>
a) You need someone pushing hard for it. Kudos to Alexander Soifer who started this.</div>
<div>
<br/></div>
<div>
b) There is no really good reason to use that name in the first place. </div>
<div>
<br/></div>
<div>
What was Nevanlinna's contribution to mathematical aspects of computer science? The IMU (International Mathematics Union) internet page answers:</div>
<div>
<br/></div>
<div>
<i>The prize was named in honors of Rolf Nevanlinna ... who in the 1950's had taken the initiative to the computer organization at Finnish Universities. </i></div>
<div>
<i><br/></i></div>
<div>
That's all. If there was a Gauss Prize (actually there IS a Gauss Prize) and we later found out that Gauss was X, I doubt we would change the name of the award. Gauss's name is on it since he is a great mathematician. </div>
<div>
<br/></div>
<div>
c) The person on the award is not the one giving the money. If we found out that Nobel was an X,  I doubt the name would change since he is paid for it. </div>
<div>
<br/></div>
<div>
d) If the award name is well known then it might not change. Nobel is a good example. I think the Nevanlinna prize is mostly unknown to the public. The Field's medal is better known, though still not that well known. The general public became briefly aware of the Field's medal twice: when it was mentioned in the movie <i>Good Will Hunting,</i> and when Perelman turned it down. Fame is fleeting for both prizes and people.</div>
<div>
<br/></div>
<div>
e) Organizations don't like to change things. Hence X would need to be particularly bad to warrant a name change. </div>
<div>
<br/></div>
<div>
OTHER THOUGHTS</div>
<div>
<br/></div>
<div>
1) Why <i>The Abacus Medal</i>? Perhaps they are worried that if they name it after someone and that someone turns out to be an X they'll have to change it again. I find the explanation given <a href="https://www.heidelberg-laureate-forum.org/blog/imu-abacus-medal/">here</a> to be unsatisfying. I find the fact that they make <b>NO MENTION</b> of why they are no longer naming it <i>The</i> <i>Nevanlinna prize </i>appalling and insulting.</div>
<div>
<br/></div>
<div>
2) Lets turn to people who get the awards. If someone solved two Millennium problems and clearly deserved a Field's Medal, but was an X, should they be denied the prize on that basis. I would tend to think no (that is, they should get the prize) but it does trouble me. What would happen?  I honestly don't know.  </div>
<div>
<br/></div>
<div>
3) X will change over time.</div>
<div>
<br/></div></div>
    </content>
    <updated>2019-06-04T15:53:00Z</updated>
    <published>2019-06-04T15:53:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-06-09T15:44:55Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-32902056.post-2757759457141834790</id>
    <link href="http://paulwgoldberg.blogspot.com/feeds/2757759457141834790/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=32902056&amp;postID=2757759457141834790" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/32902056/posts/default/2757759457141834790" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/32902056/posts/default/2757759457141834790" rel="self" type="application/atom+xml"/>
    <link href="http://paulwgoldberg.blogspot.com/2019/06/plans-for-wine-conferences.html" rel="alternate" type="text/html"/>
    <title>plans for WINE conferences</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div dir="ltr" style="text-align: left;">Update on the annual Conference on Web and Internet Economics (I am on the steering committee).<br/><br/><a href="http://wine2019.cs.columbia.edu/">WINE 2019</a> (the 15th) will be at Columbia University, December 10-12. We could not avoid the clash with NeurIPS, due to Columbia’s exam schedule. Submission deadline is July 15th.<br/><br/>The plan is for WINE 2020 to take place at Peking University, following the tradition to rotate between Europe, USA, and Asia.<br/><br/>WINE 2021 is under discussion; one idea it to hold it in Addis Ababa, Ethiopia, which is not as novel as it may seem at first sight, it would be following ICLR 2020 (see <a href="https://venturebeat.com/2018/11/19/major-ai-conference-is-moving-to-africa-in-2020-due-to-visa-issues/">this link</a> (noting the visa issues) and others). The rationale is that Africa has a burgeoning AI community including people who are interested in algorithmic game theory, and for them, Ethiopia is an easy destination (administratively, in particular). WINE 2018 (at Oxford, UK) had (I think) 5 African participants, and about 10 more would have liked to come but were denied visas. These participants brought home to me the point that there is this developing AI community in Africa. At WINE 2018, Eric Sodomka (from Facebook) gave a well-received presentation on the idea of holding a future WINE in Africa. Are there other places in Africa we should be thinking of? I welcome feedback and comments!<br/><br/></div></div>
    </content>
    <updated>2019-06-04T10:46:00Z</updated>
    <published>2019-06-04T10:46:00Z</published>
    <category scheme="http://www.blogger.com/atom/ns#" term="aggregator"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="conferences"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="game theory"/>
    <author>
      <name>Paul Goldberg</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/10952445127830395305</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-32902056</id>
      <category term="aggregator"/>
      <category term="UK academia"/>
      <category term="politics"/>
      <category term="meetings"/>
      <category term="research"/>
      <category term="research directions"/>
      <category term="conferences"/>
      <category term="funding"/>
      <category term="people"/>
      <category term="rant"/>
      <category term="economics"/>
      <category term="academia"/>
      <category term="forecasts"/>
      <category term="internet"/>
      <category term="advertisement"/>
      <category term="game theory"/>
      <category term="trips"/>
      <category term="University of Liverpool"/>
      <category term="editorial"/>
      <category term="times higher"/>
      <category term="announcements"/>
      <category term="publications"/>
      <category term="teaching"/>
      <category term="technical"/>
      <category term="work"/>
      <category term="postgraduate research"/>
      <category term="technology"/>
      <category term="books"/>
      <category term="social choice"/>
      <category term="talks"/>
      <category term="CACM"/>
      <category term="liverpool"/>
      <category term="open problems"/>
      <category term="problems"/>
      <category term="research assessment"/>
      <category term="tongue in cheek"/>
      <category term="administration"/>
      <category term="architecture"/>
      <category term="email"/>
      <category term="environment"/>
      <category term="games"/>
      <category term="mechanism design"/>
      <category term="puzzles"/>
      <category term="web sites"/>
      <category term="URLs"/>
      <category term="USS"/>
      <category term="education"/>
      <category term="higher education"/>
      <category term="oxford"/>
      <category term="schools"/>
      <category term="UCU"/>
      <category term="go"/>
      <category term="intellectual property"/>
      <category term="jobs"/>
      <category term="league table"/>
      <category term="math education"/>
      <category term="money"/>
      <category term="products"/>
      <category term="science"/>
      <category term="students"/>
      <category term="Liberal democrats"/>
      <category term="UK"/>
      <category term="XJTLU"/>
      <category term="behavioural economics"/>
      <category term="china"/>
      <category term="current affairs"/>
      <category term="diversity"/>
      <category term="epsrc"/>
      <category term="family"/>
      <category term="geography"/>
      <category term="holidays"/>
      <category term="joke"/>
      <category term="misc"/>
      <category term="nerd humour"/>
      <category term="pensions"/>
      <category term="predictions"/>
      <category term="proposals"/>
      <category term="psephology"/>
      <category term="region"/>
      <category term="student finance"/>
      <category term="visits"/>
      <category term="warwick"/>
      <category term="web"/>
      <category term="weekends"/>
      <category term="wikipedia"/>
      <category term="writing"/>
      <author>
        <name>Paul Goldberg</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/10952445127830395305</uri>
      </author>
      <link href="http://paulwgoldberg.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/32902056/posts/default/-/aggregator" rel="self" type="application/atom+xml"/>
      <link href="http://paulwgoldberg.blogspot.com/search/label/aggregator" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/32902056/posts/default/-/aggregator/-/aggregator?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>theoretical computer science, economics, and academic life in general. Writing in personal capacity, not representing my employer or other colleagues</subtitle>
      <title>Paul Goldberg</title>
      <updated>2019-06-04T13:05:01Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://offconvex.github.io/2019/06/03/trajectories/</id>
    <link href="http://offconvex.github.io/2019/06/03/trajectories/" rel="alternate" type="text/html"/>
    <title>Is Optimization a Sufficient Language for Understanding Deep Learning?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In this Deep Learning era, machine learning usually boils down to defining a suitable objective/cost function for the learning task at hand, and then optimizing this function using some variant of gradient descent (implemented via backpropagation).  Little wonder that hundreds of ML papers each year are devoted to various aspects of optimization. Today I will suggest that if our goal is mathematical understanding of deep learning, then  the optimization viewpoint is potentially insufficient —at least in the conventional view:</p>

<blockquote>
  <p><strong>Conventional View (CV) of Optimization</strong>: Find a solution of minimum possible value of the objective, as fast as possible.</p>
</blockquote>

<p>Note that <em>a priori</em> it is not obvious if all learning should involve optimizing a single objective. Whether or not this is true for  learning in the brain is a longstanding open question in neuroscience. Brain components appear to have been repurposed/cobbled together through various accidents of evolution and the whole assemblage may or may not boil down to optimization of an objective. See <a href="https://arxiv.org/pdf/1606.03813.pdf">this survey by Marblestone et al</a>.</p>

<p>I am suggesting that deep learning algorithms also have important properties that are not always reflected in the objective value. Current deep nets, being vastly overparametrized, have multiple optima. They are trained until the objective is almost zero (i.e., close to optimality) and training is said to succeed if the optimum (or near-optimum) model thus found also performs well on unseen/held-out data —i.e., <em>generalizes.</em> The catch here is that the value of the objective may imply nothing about generalization (see <a href="https://arxiv.org/abs/1611.03530">Zhang et al.</a>).</p>

<p>Of course experts will now ask: “Wasn’t generalization theory invented precisely for this reason as the “second leg” of machine learning,  where optimization is the first leg?” For instance this theory shows how to add regularizers to the training objective to ensure the solution generalizes. Or that <em>early stopping</em> (i.e., stopping before reaching the optimum) or even adding noise to the gradient (e.g. by playing with batch sizes and learning rates) can be preferable to perfect optimization, even in simple settings such as regression.</p>

<p>However, in practice explicit regularizers  and noising tricks can’t prevent deep nets from attaining low training objective even on data with random labels; see <a href="https://arxiv.org/abs/1611.03530">Zhang et al.</a>. Current generalization theory is designed to give <em>post hoc</em> explanations for why a particular model generalized. It is agnostic about <em>how</em> the solution was obtained, and thus makes few prescriptions —apart from recommending some regularization— for optimization.   (See my earlier <a href="http://www.offconvex.org/2017/12/08/generalization1/">blog post</a>, which explains the distinction between descriptive and prescriptive methods, and  that generalization theory is primarily descriptive.) The fundamental mystery is:</p>

<blockquote>
  <p>Even vanilla gradient descent (GD) is good at finding models with reasonable generalization. Furthermore, methods to speed up gradient descent (e.g., acceleration or adaptive regularization) can sometimes lead to worse generalization.</p>
</blockquote>

<p>In other words, GD has an innate bias towards finding solutions with good generalization. Magic happens along the GD trajectory and is not captured in the objective value per se. We’re reminded of the old adage.</p>

<blockquote>
  <p>The journey matters more than the destination.</p>
</blockquote>

<p>I will illustrate this viewpoint by sketching new  rigorous analyses of gradient descent in two simple but suggestive settings. I  hope more  detailed writeups will appear in future blog posts.</p>

<p>Acknowledgements: My views on this topic were initially shaped by the excellent papers from TTI Chicago group regarding the implicit bias of gradient descent (<a href="https://arxiv.org/pdf/1709.01953.pdf">Behnam Neyshabur’s thesis</a> is a good starting point), and then of course by  various coauthors.</p>

<h2 id="computing-with-infinitely-wide-deep-nets">Computing with Infinitely Wide Deep Nets</h2>

<p>Since overparametrization does not appear to hurt deep nets too much, researchers have wondered what happens in the infinite limit of overparametrization: use a fixed training set such as CIFAR10 to train a classic deep net architecture like AlexNet or VGG19 whose “width” —namely, number of channels in the convolutional filters, and number of nodes in fully connected internal layers—- is allowed to increase to <strong>infinity</strong>. Note that initialization (using sufficiently small Gaussian weights) and training makes sense for any finite width, no matter how large. We assume $\ell_2$ loss at the output.</p>

<p>Understandably, such questions can seem hopeless and pointless: all the computing in the world is insufficient to train an infinite net, and we theorists already have our hands full trying to figure out finite nets.  But sometimes in math/physics one can derive insight into questions by studying them in the infinite limit.  Here where an infinite net is training on a finite dataset like CIFAR10, the number of optima is infinite and we are trying to understand what GD does.</p>

<p>Thanks to insights in recent papers on provable learning by overparametrized deep nets (some of the key papers are: <a href="https://arxiv.org/abs/1811.04918">Allen-Zhu et al 1</a>, <a href="https://arxiv.org/abs/1811.03962">Allen-Zhu et al 2</a> <a href="https://arxiv.org/abs/1811.03804">Du et al</a>, <a href="https://arxiv.org/abs/1811.08888">Zou et al</a>) researchers have realized that a nice limiting structure emerges:</p>

<blockquote>
  <p>As width $\rightarrow \infty$, trajectory approaches the trajectory of GD for a kernel regression problem, where the (fixed) kernel in question is the so-called  <em>Neural Tangent Kernel</em> (NTK). (For convolutional nets the kernel is <em>Convolutional NTK or CNTK.</em> )</p>
</blockquote>

<p>The kernel was identified and named by <a href="https://arxiv.org/abs/1806.07572">Jacot et al.</a>, and also implicit in some of the above-mentioned papers on overparametrized nets, e.g. <a href="https://arxiv.org/abs/1810.02054">Du et al</a>.</p>

<p>The definition of this fixed kernel uses the infinite net at its random initialization. For  two inputs $x_i$ and $x_j$ the kernel inner product  $K(x_i, x_j)$  is the inner product of the gradient $\nabla_x$ of the output with respect to the input, evaluated at $x=x_i$, and $x= x_j$ respectively. As the net size increases to infinity this kernel inner product can be shown to converge to a limiting value (there is a technicality about how to define the limit, and the series of new papers have improved the formal statement here; eg <a href="https://arxiv.org/abs/1902.04760">Yang2019</a> and our paper below.).</p>

<p>Our <a href="https://arxiv.org/abs/1904.11955">new paper with Simon Du, Wei Hu, Zhiyuan Li, Russ Salakhutdinov and Ruosang Wang</a> shows that the CNTK can be efficiently computed via dynamic programming, giving us a way to efficiently compute the answer of the trained net for any desired input,  <em>even though training the infinite net directly is of course computationally infeasible.</em> (Aside: Please do not confuse these new results with some earlier papers which view infinite nets as kernels or Gaussian Processes —see citations/discussion in our paper—  since they correspond to training only the top layer while freezing the lower layers to a random initialization.) Empirically we find that this infinite net (aka kernel regression with respect to the NTK) yields better performance on CIFAR10 than any previously known kernel —not counting kernels that were  hand-tuned or designed by training on image data. For instance we can compute the kernel corresponding to a 10-layer convolutional net (CNN) and obtain 77.4% success rate on CIFAR10.</p>

<h2 id="deep-matrix-factorization-for-solving-matrix-completion">Deep Matrix Factorization for solving Matrix Completion</h2>

<p><a href="https://en.wikipedia.org/wiki/Matrix_completion">Matrix completion</a>, motivated by design of recommender systems, is well-studied for over a decade: given $K$ random entries of an unknown matrix, we wish to recover the unseen entries. Solution is not unique in general. But if the unknown matrix is low rank or approximately low rank and satisfies some additional technical assumptions (eg <em>incoherence</em>) then various algorithms can recover the unseen entries approximately or even exactly. A famous algorithm  based upon <a href="https://en.wikipedia.org/wiki/Matrix_norm#Schatten_norms">nuclear/trace norm</a>  minimization is as follows: find matrix that fits all the known observations and has minimum nuclear norm. (Note that nuclear norm is a convex relaxation of rank.) It is also possible to rephrase this as a single objective in the form required by the Conventional View as follows where $S$ is the subset of indices of revealed entries,  $\lambda$ is a multiplier:</p>



<p>In case you didn’t know about nuclear norms, you will like the interesting suggestion made by <a href="http://papers.nips.cc/paper/7195-implicit-regularization-in-matrix-factorization">Gunasekar et al. 2017</a>: let us just forget about the nuclear norm penalty term  altogether. Instead try to recover the missing entries by  simply training (via simple gradient descent/backpropagation) a linear net with two layers on the first term in the loss. This linear net is just a multiplication of two $n\times n $ matrices (you can read about linear deep nets in this <a href="http://www.offconvex.org/2018/03/02/acceleration-overparameterization/">earlier blog post by Nadav Cohen</a>) so we obtain the following  where $e_i$ is the vector with all entries $0$ except for $1$ in the $i$th position:</p>



<p>The “data” now corresponds to indices $(i, j) \in S$, and the training loss captures how well the end-to-end model $M_2M_1$ fits the revealed entries.  Since $S$ was chosen randomly among all entries,  “generalization” corresponds exactly to doing well at predicting the remaining entries. Empirically, soving matrix completion this way via deep learning  (i.e., gradient descent to solve for $M_1, M_2$, and entirely forgetting about ensuring low rank) works as well as the classic algorithm, leading to the following conjecture, which if true would imply that the implicit regularization effect of gradient descent in this case is captured exactly by the nuclear norm.</p>

<blockquote>
  <p>(Conjecture by Gunasekar et al.; Rough Statement) When solving matrix completion as above using a depth-$2$ linear net, the solution obtained is exactly the  one obtained by the nuclear norm minimization method.</p>
</blockquote>

<p>But as you may have already guessed, this turns out to be too simplistic. In <a href="https://arxiv.org/abs/1905.13655">a new paper with Nadav Cohen, Wei Hu and Yuping Luo</a>, we report new experiments suggesting that the above conjecture is false. (I hedge by saying “suggest” because some fine print in the conjecture statement makes it pretty hard to refute definitively.) More interesting, we find that if we overparametrize the problem by further increasing the number of layers from two to $3$ or even higher —which we call Deep Matrix Factorization—then this empirically solves matrix completion even better than nuclear norm minimization. (Note that we’re working in the regime where $S$ is slightly smaller than what it needs to be for nuclear norm algorithm to exactly recover the matrix. Inductive bias is most important precisely in such data-poor settings!) We provide partial analysis for this improved performance of depth $N$ nets by analysing —surprise surprise!—the trajectory of gradient descent and showing how it biases strongly toward finding solutions of low rank, and this bias is stronger than simple nuclear norm. Furthermore our analysis suggests that this bias toward low rank  cannot be captured by nuclear norm or any obvious Schatten quasi-norm of the end-to-end matrix.</p>

<p>NB: Empirically we find that Adam, the celebrated  acceleration method for deep learning, speeds up optimization a lot here as well, but slightly hurts generalization. This relates to what I said above about the  Conventional View being insufficient to capture generalization.</p>

<h2 id="conclusionstakeways">Conclusions/Takeways</h2>

<p>Though the above settings are simple, they suggest that to understand deep learning we have to go beyond the Conventional View of optimization, which focuses only on the value of the objective and the rate of convergence.</p>

<p>(1): Different optimization strategies —GD, SGD, Adam, AdaGrad etc. —-lead to different learning algorithms. They induce different trajectories, which may lead to solutions with different generalization properties.</p>

<p>(2) We need to develop a new vocabulary (and mathematics) to reason about trajectories. This goes beyond the usual “landscape view” of stationary points, gradient norms, Hessian norms, smoothness etc. Caution: trajectories depend on initialization!</p>

<p>(3): I wish I had learnt a few tricks about ODEs/PDEs/Dynamical Systems/Lagrangians in college, to be in better shape to reason about trajectories!</p></div>
    </summary>
    <updated>2019-06-03T10:00:00Z</updated>
    <published>2019-06-03T10:00:00Z</published>
    <source>
      <id>http://offconvex.github.io/</id>
      <author>
        <name>Off the Convex Path</name>
      </author>
      <link href="http://offconvex.github.io/" rel="alternate" type="text/html"/>
      <link href="http://offconvex.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Algorithms off the convex path.</subtitle>
      <title>Off the convex path</title>
      <updated>2019-06-09T01:22:11Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://gradientscience.org/robust_reps/</id>
    <link href="http://gradientscience.org/robust_reps/" rel="alternate" type="text/html"/>
    <title>Robustness beyond Security&amp;#58; Representation Learning</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a class="bbutton" href="https://arxiv.org/abs/1906.00945" style="float: left;">
<i class="fas fa-file-pdf"/>
    Read the paper
</a>
<a class="bbutton" href="http://git.io/robust-reps" style="float: right;">
<i class="fab fa-github"/>
   Download the notebooks
</a></p>

<p><i>This post discusses our <a href="https://arxiv.org/abs/1906.00945">latest paper</a>
on deep network representations—while representations of standard
networks are brittle and thus not fully reflective of the input geometry,
we find that the representations of robust networks are amenable to all
sorts of manipulation, and can truly be thought of (and dealt with) as just
high-level feature representations. Our work suggests that robustness might
be more broadly useful than just protection against adversarial examples.
</i></p>

<p>One of the most promising aspects of deep neural networks is their
potential to learn high-level <i>features</i> that are useful beyond the
classification task at hand. Our mental model of deep
learning classifiers is often similar to the following diagram, in which
the networks learns progressively higher-level features until the final
layer, which acts as a linear classifier over these high-level features:</p>

<p><img alt="" src="http://gradientscience.org/assets/rf1_images/visualization.png"/></p>
<div class="footnote">
A conceptual picture of our understanding of modern deep neural networks
(NVIDIA).
</div>

<p>This picture is consistent with the surprising versatility of deep neural
network <i>feature representations</i>—learned representations for one
task are useful for many others
(as in <a href="https://papers.nips.cc/paper/959-learning-many-related-tasks-at-the-same-time-with-backpropagation.pdf">transfer learning</a>), and
distance in representation space has often been proposed as a perceptual
metric on natural images (as in <a href="http://arxiv.org/abs/1801.03924">VGG distance</a>).</p>

<div class="footnote">
<strong>Note:</strong> In <a href="https://arxiv.org/abs/1906.00945">our paper</a> and in this blog post, we refer to the
<i>representation</i> $R(x)$ of an input $x$ for a network as the
values of the penultimate layer in the network for that input.
</div>

<p>But to what extent is this picture accurate? It turns out that it is
rather simple to (consistently) construct images that are <i>completely</i>
different to a human, but share very similar representations:</p>

<p><img alt="Standard representations are brittle" src="http://gradientscience.org/assets/rf1_images/standard_brittleness.png"/></p>
<div class="footnote">
The above two images, despite seeming completely different to humans, share very
similar representations.
</div>

<p>This phenomenon is somewhat troubling for our conceptual picture: if
feature representations actually encode high-level, human-meaningful
features, we should not be able to find two images with totally different
features that the model “sees” as very similar.</p>

<p>The phenomenon at play here turns out to be more fundamental than just
pairs of images with similar representations. Indeed, the
representations of neural networks seem to be <i>pervasively brittle</i>:
they can be manipulated arbitrarily without meaningful change to the input.
(In fact, this brittleness is similar to the phenomenon that we exploit
when making <a href="https://gradientscience.org/intro_adversarial">adversarial examples</a>.)</p>

<p>Clearly, this brittleness precludes standard representations from acting
how we want them to—in particular, distance in representation space is
not fully <i>aligned</i> with our human perception of distance in feature
space. So, how might we go about fixing this issue?</p>

<h3 id="adversarial-robustness-as-a-feature-prior">Adversarial Robustness as a Feature Prior</h3>

<p>Unfortunately, we don’t have a way to explicitly control which features
models learn (or in what way they learn them). We can, however,
disincentivize models from using features that humans <i>definitely</i>
don’t use by imposing a <i>prior</i> during training. In our paper, we
explore a very simple prior: namely, that imperceptible changes in the
input should not cause large changes in the model’s prediction (i.e.,
models should not rely on brittle features):</p>



<p>Note that this stability is a necessary, but not sufficient property: all
features that humans use certainly obey this property (for reasonably small
), but not every feature obeying this property is one that we
want our models to rely on.</p>

<p>How should we enforce this prior? Well, observe that the condition
$\eqref{eq:robustcond}$ above is actually <i>precisely</i> $\ell_2$-<a href="https://gradientscience.org/intro_adversarial">adversarial
robustness</a>! Thus, a natural method to employ is robust optimization,
which, as we discussed in a <a href="https://gradientscience.org/robust_opt_pt1">previous post</a>, provides reasonable
robustness to adversarial perturbations. Concretely, instead of just
minimizing loss, we opt to minimize <i>adversarial loss</i>:</p>



<h3 id="inverting-representations">Inverting representations</h3>

<p>Now, given a network trained in this manner, what happens if we look for
images with the same representations? Concretely, fixing some image $x$,
what happens if we look for an image $x’$ that has a matching
representation:</p>



<p>(Note that we found the image pairs presented earlier for standard networks
by solving exactly the above problem.) It turns out that when our model is
<i>robust</i>, we end up with an image that is remarkably similar to the
original:</p>

<div class="widget">
    <div class="choices_one" id="left2">
	<span class="widgetheading">Choose an Image</span>
    </div>
    <span class="widgetheading">Reconstructed Image</span>
    <div class="beer-slider selected_one" id="inv_slider">
	<img id="selectedinv1"/>
	<div class="beer-reveal" style="border-right: 3px white solid;">
	    <img class="slider_img" id="selectedinv2"/>
	</div>
    </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Interactive demo</strong>: click on any of the images on the left to see its
reconstruction via the representation of a robust network. The top row
contains random images from the test set, and the bottom row has random
<i>out-of-distribution</i> inputs (images without a correct class).
</div>

<p>Indeed, instead of being able to manipulate feature representations
arbitrarily within a small radius, we now find that matching the
representation of an image leads to (approximately) matching the image
itself.</p>

<h2 id="what-can-we-do-with-these-representations">What can we do with these representations?</h2>
<p>We just saw that the learned representation of a robust deep classifier
suffices to reconstruct its input pretty accurately (at least in terms of
human perception). This highlights two crucial properties of these
representations: a) optimizing for closeness in representation space leads
to perceptually similar images, b) representations contain a large amount
of information about the high-level features of the inputs. These
properties are very desirable and prompt us to further explore the
structure and potential of these representations.  <i>What we find is that
the representations of robust networks can truly be thought of as
high-level feature representations, and thus (in stark contrast to standard
networks) are naturally amenable to various types of manipulation.</i></p>

<p>In the following sections, we explore these “robust representations” in
more depth. A crucial theme in our exploration is
<i>model-faithfulness</i>. Though significant work has been done in
manipulating and interpreting standard (non-robust) models, it seems as
though getting anything meaningful from standard networks requires
enforcing <i>priors</i> into the visualization process
(see this excerpt <a href="https://distill.pub/2017/feature-visualization/#enemy-of-feature-vis">“The Enemy of Feature Visualization”</a>
for a discussion and illustration of this). This comes at the cost of either hiding
vital signals the model utilizes or introducing information that was not
already present in the model—thus blurring the line between what
information the model actually has, versus what information we introduced
when interacting with it. In contrast, throughout our exploration we will
rely on only direct optimization over representation space, without
introducing any priors or extra information.</p>

<h3 id="feature-visualization">Feature visualization</h3>

<p>We begin our exploration of robust representations by trying to understand
the features captured by their individual components. We visualize these
components in the simplest possible way: we perform gradient descent to
find inputs that maximally activate individual components of the
representation. This is how a few <em>random</em> visualizations look like:</p>

<p><img src="http://gradientscience.org/assets/rf1_images/features.png" style="margin: 0;"/></p>
<div class="footnote">
   Inputs maximizing various coordinates (separated by column) of a robust network, found via gradient descent starting from the "seed" image on the far left.
</div>

<p>We see a surprising alignment with human concepts. For instance, the last
component above seems to correspond to “anemone” and the second-last component to
“flowers”. In fact, these names are consistent with the test images
maximally activating these neurons—here are the images corresponding to
each component:</p>

<div class="widget">
    <div class="choices_one" id="left_maxact">
	<span class="widgetheading">Choose a Coordinate (Feature)</span>
    </div>
    <span class="widgetheading">Top Images</span>
    <div class="selected_one" id="maxact_selected"/>
</div>
<div style="clear: both;"/>
<div class="footnote">
    <strong>Interactive demo</strong>: On the left are components of the representation of a robust network
    (the thumbnails are a visualization of the components maximized from
    noise). On the right are the images from the test set that maximally activate the corresponding components.
</div>

<p>These visualizations might look familiar. Indeed, similar results have been
produced in prior work using non-robust models (e.g.
<a href="https://distill.pub/2017/feature-visualization/">here</a> or
<a href="https://distill.pub/2018/building-blocks/">here</a>). The difference is that
the images above are generated by directly maximizing representation
components with gradient descent in input space—we do not enforce any
priors or regularization. For standard networks, the same process is
unfruitful—to circumvent this, prior work imposes priors on the
optimization process.</p>

<h3 id="feature-manipulation">Feature Manipulation</h3>

<p>So far, we have seen that matching the representation of an image starting
from random noise, recovers the high-level features of the image itself. At
the same time, we saw that individual representation components correspond
to high-level human-meaningful concepts. These findings suggests an
intriguing possibility: perhaps we can directly modify high-level features
of an image by manipulating the corresponding representation over the input
space.</p>

<p>This turns out to yield remarkably plausible results! Here we visualize the
results of increasing a few select components via gradient descent over the
image space for a few <em>random</em> (not cherry-picked) inputs:</p>

<div class="widget">
    <div class="choices_right" id="right1">
	<span class="widgetheading">Choose a Coordinate</span>
    </div>
    <div class="selected_two">
	<span class="widgetheading">Feature Addition</span>
	<div class="beer-slider" id="manipulation_slider">
	    <img id="man_selected1"/> 
	    <div class="beer-reveal" style="border-right: 3px white solid;">
		<img class="slider_img" id="man_selected2"/> 
	    </div>
	</div>
    </div>
    <div class="choices_left" id="left1">
	<span class="widgetheading">Choose a Source Image</span>
    </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Interactive demo</strong>: On the left are randomly selected source images,
and on the right are components of the representation of a robust network
(the thumbnails are a visualization of the components maximized from
noise). In the middle is the feature from the selected component, "added"
to the selected image.
</div>

<p>These images end up actually exhibiting the relevant features in a way that
is plausible to humans (for example, stripes appear mostly on animals
instead of the background).</p>

<p>This opens up a wide range of fine-grained manipulations that one can
perform by leveraging the learned representations (in fact, stay tuned for
some applications in our next blog post).</p>

<h3 id="input-interpolation">Input interpolation</h3>
<p>In fact, this outlook can be pushed even further—robust models can be
leveraged as a tool for another kind of manipulation: input-to-input
interpolation. That is, if we think of robust representations as encoding
the high-level features of an input in a sensible manner, an intuitive way
to interpolate between any two inputs is to linearly interpolate their
representations. More precisely, given any two inputs, we can try to
construct an interpolation between them by linearly interpolating their
representations and then constructing inputs to match these
representations.</p>

<p>This rather intuitive way of dealing with representations turns out to work
reasonably well—we can interpolate between arbitrary images. Randomly
sampled interpolations are shown below:</p>

<div class="widget">
    <div class="choices_left" id="int_left1">
	<span class="widgetheading">Choose a Source Image</span>
    </div>
    <div class="selected_two">
	<span class="widgetheading">Interpolation</span>
	<video style="width: 100%;">
	    <source id="int_selected" type="video/mp4">
	</source></video>
    </div>
    <div class="choices_right" id="int_right1">
	<span class="widgetheading">Choose a Destination Image</span>
    </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<strong>Interactive demo</strong>: On the left are randomly selected source images,
and on the right are randomly selected target images.
In the middle is the feature interpolation from the selected source image
to the selected target.
</div>

<p>As we can see, the interpolations appears perceptually plausible. Note
that, in contrast to approaches based on generative models
(e.g. <a href="https://arxiv.org/abs/1511.06434">here</a> or
<a href="https://arxiv.org/abs/1809.11096">here</a>), this approach can interpolate
between arbitrary inputs and not only between those produced by the
generative model.</p>

<h3 id="insight-into-model-predictions">Insight into model predictions</h3>
<p>Expanding on our view of deep classifiers as simple linear classifiers on
top of the learned representations, there is also a simple way to gain
insight into predictions of (robust) models. In particular, for incorrect
predictions, we can identify the component most heavily contributing to the
incorrect class (in the same way we would for a linear classifier) and then
directly manipulate the input to increase the value of that component (with
input-space gradient descent). Here we perform this visualization for a few
random misclassified inputs:</p>

<p><img src="http://gradientscience.org/assets/rf1_images/misclassification_IN.jpg" style="margin: 0;"/></p>

<p>The resulting images could provide insight into the model’s incorrect
decision. For instance, we see the bug becoming a dog eye or negative space
becoming the face of a dog. At a high level, these inputs demonstrate which
parts of the image the incorrect prediction was most sensitive to.</p>

<p>Still, as a word of caution, it is important to note that just as with all
saliency methods (e.g. heatmaps, occlusion studies, etc.), visualizing
features and studying misclassification only gives insights into a “local”
sense of model behaviour. Deep neural networks are complex, highly
non-linear models and it’s important to keep in mind that <i>local
sensitivity does not necessarily entail causality</i>.</p>

<h2 id="towards-better-learned-representations">Towards better learned representations</h2>
<p>As we discussed, robust feature representations possess properties that
make them desirable from a broader point of view. In particular, we found
these representations to be better aligned with a perceptual notion of
distance, while allowing us to perform direct input manipulations in a
model-faithful way. These are properties that are fundamental to any “truly
human-level” representation. One can thus view adversarial robustness as a
very potent prior for obtaining representations that are more aligned with
human perception beyond the standard goals of security and reliability.</p></div>
    </summary>
    <updated>2019-06-03T00:00:00Z</updated>
    <published>2019-06-03T00:00:00Z</published>
    <source>
      <id>http://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="http://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="http://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2019-06-08T23:22:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4199</id>
    <link href="https://www.scottaaronson.com/blog/?p=4199" rel="alternate" type="text/html"/>
    <link href="http://www.fields.utoronto.ca/video-archive/static/2019/05/2774-20557/mergedvideo.ogv" length="339811750" rel="enclosure" type="video/ogg"/>
    <link href="https://www.scottaaronson.com/blog/?p=4199#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4199" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">NP-complete Problems and Physics: A 2019 View</title>
    <summary xml:lang="en-US">If I want to get back to blogging on a regular basis, given the negative amount of time that I now have for such things, I’ll need to get better at dispensing with pun-filled titles, jokey opening statements, etc. etc., and resigning myself to a less witty, more workmanline blog. So in that spirit: a […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>If I want to get back to blogging on a regular basis, given the negative amount of time that I now have for such things, I’ll need to get better at dispensing with pun-filled titles, jokey opening statements, etc. etc., and resigning myself to a less witty, more workmanline blog.</p>



<p>So in that spirit: a few weeks ago I gave a talk at the Fields Institute in Toronto, at a <a href="http://www.fields.utoronto.ca/activities/18-19/NP50">symposium</a> to celebrate Stephen Cook and the 50th anniversary (or actually more like 48th anniversary) of the discovery of NP-completeness.  Thanks so much to the organizers for making this symposium happen.</p>



<p>You can <a href="http://www.fields.utoronto.ca/video-archive/static/2019/05/2774-20557/mergedvideo.ogv">watch the video of my talk here</a> (or <a href="https://www.scottaaronson.com/talks/npphys-toronto.ppt">read the PowerPoint slides here</a>).  The talk, on whether NP-complete problems can be efficiently solved in the physical universe, covers much the same ground as <a href="https://www.scottaaronson.com/papers/npcomplete.pdf">my 2005 survey article</a> on the same theme (not to mention dozens of earlier talks), but this is an updated version and I’m happier with it than I was with most past iterations.</p>



<p>As I explain at the beginning of the talk, I wasn’t going to fly to Toronto at all, due to severe teaching and family constraints—but my wife Dana uncharacteristically <em>urged me to go</em> (“don’t worry, I’ll watch the kids!”).  Why?  Because in her view, it was the risks that Steve Cook took 50 years ago, as an untenured assistant professor at Berkeley, that gave birth to the field of computational complexity that Dana and I both now work in.</p>



<p>Anyway, be sure to <a href="http://www.fields.utoronto.ca/video-archive//event/2774/2019">check out the other talks as well</a>—they’re by an assortment of random nobodies like Richard Karp, Avi Wigderson, Leslie Valiant, Michael Sipser, Alexander Razborov, Cynthia Dwork, and Jack Edmonds.  I found the talk by Edmonds particularly eye-opening: he explains how he thought about (the objects that we now call) P and NP∩coNP when he first defined them in the early 60s, and how it was similar to and different from the way we think about them today.</p>



<p>Another memorable moment came when Edmonds interrupted Sipser’s talk—about the history of P vs. NP—to deliver a booming diatribe about how what really matters is not mathematical proof, but just how quickly you can solve problems in the real world.  Edmonds added that, from a practical standpoint, P≠NP is “true today but might become false in the future.”  In response, Sipser asked “what does a mathematician like me care about the real world?,” to roars of approval from the audience.  I might’ve picked a different tack—about how for every practical person I meet for whom it’s blindingly obvious that “in real life, P≠NP,” I meet another for whom it’s equally obvious that “in real life, P=NP” (for all the usual reasons: because SAT solvers work so well in practice, because physical systems so easily relax as their ground states, etc).  No wonder it took 25+ years of smart people thinking about operations research and combinatorial optimization before the P vs. NP question was even explicitly posed.</p>



<hr/>

<p><font color="red"><strong>Unrelated Announcement:</strong></font> The Texas Advanced Computing Center (TACC), a leading supercomputing facility in North Austin that’s part of the University of Texas, is seeking to hire a Research Scientist focused on quantum computing.  Such a person would be a full participant in our <a href="https://www.cs.utexas.edu/~qic/">Quantum Information Center</a> at UT Austin, with plenty of opportunities for collaboration.  <a href="https://utaustin.wd1.myworkdayjobs.com/UTstaff/job/PICKLE-RESEARCH-CAMPUS/Research-Scientist_R_00003442">Check out their posting!</a></p>



<p/></div>
    </content>
    <updated>2019-06-02T13:52:31Z</updated>
    <published>2019-06-02T13:52:31Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Adventures in Meatspace"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2019-06-03T13:29:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/082</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/082" rel="alternate" type="text/html"/>
    <title>TR19-082 |  Approximate degree, secret sharing, and concentration phenomena | 

	Andrej Bogdanov, 

	Nikhil Mande, 

	Justin Thaler, 

	Christopher Williamson</title>
    <summary>The $\epsilon$-approximate degree $\widetilde{\text{deg}}_\epsilon(f)$ of a Boolean function $f$ is the least degree of a real-valued polynomial that approximates $f$ pointwise to error $\epsilon$.  The approximate degree of $f$ is at least $k$ iff there exists a pair of probability distributions, also known as a dual polynomial, that are perfectly $k$-wise indistinguishable, but are distinguishable by $f$ with advantage $1 - \epsilon$.  Our contributions are:

We give a simple new construction of a dual polynomial for the AND function, certifying that $\widetilde{\text{deg}}_\epsilon(f) \geq \Omega(\sqrt{n \log 1/\epsilon})$.  This construction is the first to extend to the notion of weighted degree, and yields the first explicit certificate that the $1/3$-approximate degree of any read-once DNF is $\Omega(\sqrt{n})$.

We show that any pair of symmetric distributions on $n$-bit strings that are perfectly $k$-wise indistinguishable are also statistically $K$-wise indistinguishable with error at most $K^{3/2} \cdot \exp(-\Omega(k^2/K))$ for all $k \leq K \leq n/64$.
This implies that any symmetric function $f$ is a reconstruction function with constant advantage for a ramp secret sharing scheme that is secure against size-$K$ coalitions with statistical error $K^{3/2} \exp(-\Omega(\widetilde{\text{deg}}_{1/3}(f)^2/K))$ for all values of $K$ up to $n/64$ simultaneously.
Previous secret sharing schemes required that $K$ be determined in advance, and only worked for $f=$ AND.  

Our analyses draw new connections between approximate degree and concentration phenomena.

As a corollary, we show that for any $d \leq n/64$, any degree $d$ polynomial approximating a symmetric function $f$ to error $1/3$
must have $\ell_1$-norm at least $K^{-3/2} \exp({\Omega(\widetilde{\text{deg}}_{1/3}(f)^2/d)})$, which we also show to be tight for any $d &gt; \widetilde{\text{deg}}_{1/3}(f)$.
These upper and lower bounds were also previously only known in the case $f=$ AND.</summary>
    <updated>2019-06-02T04:15:39Z</updated>
    <published>2019-06-02T04:15:39Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-06-09T20:21:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/081</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/081" rel="alternate" type="text/html"/>
    <title>TR19-081 |  Channels of Small Log-Ratio Leakage and Characterization of Two-Party Differentially Private Computation | 

	Iftach Haitner, 

	Noam Mazor, 

	Ronen Shaltiel, 

	Jad Silbak</title>
    <summary>Consider a PPT two-party protocol ?=(A,B) in which the parties get no private inputs and obtain outputs O^A,O^B?{0,1}, and let V^A and V^B denote the parties’ individual views. Protocol ? has ?-agreement if Pr[O^A=O^B]=1/2+?. The leakage of ? is the amount of information a party obtains about the event {O^A=O^B}; that is, the leakage ? is the maximum, over P?{A,B}, of the distance between V^P|OA=OB and V^P|OA!=OB. Typically, this distance is measured in statistical distance, or, in the computational setting, in computational indistinguishability. For this choice, Wullschleger [TCC ’09] showed that if ?&gt;&gt;? then the protocol can be transformed into an OT protocol.

We consider measuring the protocol leakage by the log-ratio distance (which was popularized by its use in the differential privacy framework). The log-ratio distance between X,Y over domain ? is the minimal ??0 for which, for every v??, log(Pr[X=v]/Pr[Y=v])? [??,?]. In the computational setting, we use computational indistinguishability from having log-ratio distance ?. We show that a protocol with (noticeable) accuracy ???(?^2) can be transformed into an OT protocol (note that this allows ?&gt;&gt;?). We complete the picture, in this respect, showing that a protocol with ??o(?^2) does not necessarily imply OT. Our results hold for both the information theoretic and the computational settings, and can be viewed as a “fine grained” approach to “weak OT amplification”.

We then use the above result to fully characterize the complexity of differentially private two-party computation for the XOR function, answering the open question put by Goyal, Khurana, Mironov, Pandey, and Sahai [ICALP ’16] and Haitner, Nissim, Omri, Shaltiel, and Silbak [FOCS ’18]. Specifically, we show that for any (noticeable) ???(?^2), a two-party protocol that computes the XOR function with ?-accuracy and ?-differential privacy can be transformed into an OT protocol. This improves upon Goyal et al. that only handle ???(?), and upon Haitner et al. who showed that such a protocol implies (infinitely-often) key agreement (and not OT). Our characterization is tight since OT does not follow from protocols in which ??o(?^2), and extends to functions (over many bits) that “contain” an “embedded copy” of the XOR function.</summary>
    <updated>2019-06-02T04:13:14Z</updated>
    <published>2019-06-02T04:13:14Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-06-09T20:21:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/080</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/080" rel="alternate" type="text/html"/>
    <title>TR19-080 |  On List Recovery of High-Rate Tensor Codes | 

	Noga Ron-Zewi, 

	Swastik Kopparty, 

	Shubhangi Saraf, 

	Nicolas Resch, 

	Shashwat Silas</title>
    <summary>We continue the study of list recovery properties of high-rate tensor codes, initiated by Hemenway, Ron-Zewi, and Wootters (FOCS'17). In that work it was shown that the tensor product of an efficient (poly-time) high-rate globally list recoverable code is {\em approximately}  locally list recoverable, as well as globally list recoverable in {\em probabilistic} near-linear time. This was used in turn to give the first capacity-achieving list decodable codes with (1) local list decoding algorithms, and with (2)  {\em probabilistic} near-linear time  global list decoding algorithms. This was also yielded constant-rate codes approaching the Gilbert-Varshamov bound  with  {\em probabilistic}  near-linear time global   unique decoding algorithms.

In the current work we obtain the following results:
1. The tensor product of an efficient (poly-time) high-rate globally list recoverable code is globally list recoverable in {\em deterministic} near-linear time. This yields in turn the first capacity-achieving list decodable codes with {\em deterministic} near-linear time global  list decoding algorithms. It also gives constant-rate codes approaching the Gilbert Varshamov bound with {\em deterministic} near-linear time global unique decoding algorithms.

2. If the base code is additionally locally correctable, then the tensor product is (genuinely) locally list recoverable. This yields in turn constant-rate codes approaching the Gilbert-Varshamov bound that are {\em locally correctable} with query complexity and running time $N^{o(1)}$. This improves over prior work by Gopi et. al. (SODA'17; IEEE Transactions on Information Theory'18) that only gave query complexity $N^{\epsilon}$ with rate that is exponentially small in $1/\epsilon$.

3. A nearly-tight combinatorial lower bound on output list size for list recovering high-rate tensor codes. This bound implies in turn a nearly-tight lower bound of $N^{\Omega(1/\log \log N)}$ on the product of  query complexity and output list size  for locally list recovering high-rate tensor codes.</summary>
    <updated>2019-06-01T16:43:20Z</updated>
    <published>2019-06-01T16:43:20Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-06-09T20:21:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/079</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/079" rel="alternate" type="text/html"/>
    <title>TR19-079 |  Average Bias and Polynomial Sources | 

	Arnab Bhattacharyya, 

	Philips George John, 

	Suprovat Ghoshal, 

	Raghu Meka</title>
    <summary>We identify a new notion of pseudorandomness for randomness sources, which we call the average bias. Given a distribution $Z$ over $\{0,1\}^n$, its average bias is: $b_{\text{av}}(Z) =2^{-n} \sum_{c \in \{0,1\}^n} |\mathbb{E}_{z \sim Z}(-1)^{\langle c, z\rangle}|$. A source with average bias at most $2^{-k}$ has min-entropy at least $k$, and so low average bias is a stronger condition than high min-entropy. We observe that the inner product function is an extractor for any source with average bias less than $2^{-n/2}$.

  The notion of average bias especially makes sense for polynomial sources, i.e., distributions sampled by low-degree $n$-variate polynomials over $\mathbb{F}_2$. For the well-studied case of affine sources, it is easy to see that min-entropy $k$ is exactly equivalent to average bias of $2^{-k}$. We show that for quadratic sources, min-entropy $k$ implies that the average bias is at most $2^{-\Omega(\sqrt{k})}$. We use this relation to design dispersers for separable quadratic sources with a min-entropy guarantee.</summary>
    <updated>2019-06-01T16:39:21Z</updated>
    <published>2019-06-01T16:39:21Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-06-09T20:21:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/078</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/078" rel="alternate" type="text/html"/>
    <title>TR19-078 |  Pseudo-Mixing Time of Random Walks | 

	Itai Benjamini, 

	Oded Goldreich</title>
    <summary>We introduce the notion of pseudo-mixing time of a graph define as the number of steps in a random walk that suffices for generating a vertex that looks random to any polynomial-time observer, where, in addition to the tested vertex, the observer is also provided with oracle access to the incidence function of the graph. 

Assuming the existence of one-way functions,
we show that the pseudo-mixing time of a graph can be much smaller than its mixing time.
Specifically, we present bounded-degree $N$-vertex Cayley graphs that have pseudo-mixing time $t$ for any $t(N)=\omega(\log\log N)$. 
Furthermore, the vertices of these graphs can be represented by string of length $2\log_2N$, and the incidence function of these graphs can be computed by Boolean circuits of size $poly(\log N)$.</summary>
    <updated>2019-06-01T07:34:30Z</updated>
    <published>2019-06-01T07:34:30Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-06-09T20:21:05Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/05/31/linkage</id>
    <link href="https://11011110.github.io/blog/2019/05/31/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>No maths for Europe (). Sadly, the EU parliament has passed up a chance to find a nice (or even not-so-nice) formula for its apportionment of seats to countries, instead opting for back-room deals and numbers pulled out of a hat.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://plus.maths.org/content/democratic-dilemmas">No maths for Europe</a> (<a href="https://mathstodon.xyz/@11011110/102109693915830408"/>). Sadly, the EU parliament has passed up a chance to find a nice (or even not-so-nice) <a href="https://en.wikipedia.org/wiki/Highest_averages_method">formula for its apportionment</a> of seats to countries, instead opting for back-room deals and numbers pulled out of a hat.</p>
  </li>
  <li>
    <p>Prominent cryptographers <a href="https://en.wikipedia.org/wiki/Adi_Shamir">Adi Shamir</a> and <a href="https://en.wikipedia.org/wiki/Ross_J._Anderson">Ross J. Anderson</a> were both <a href="https://www.schneier.com/blog/archives/2019/05/why_are_cryptog.html">denied visas to travel to the US</a> for a conference and a book awards ceremony respectively (<a href="https://mathstodon.xyz/@11011110/102112360619663485"/>, <a href="https://boingboing.net/2019/05/17/denying-cryptographers-problem.html">see also</a>). Bruce Schneier mentions “two other prominent cryptographers who are in the same boat”. Odd and troubling.</p>
  </li>
  <li>
    <p><a href="https://mathlesstraveled.com/2019/05/09/computing-the-euler-totient-function-part-1/">Three</a> <a href="https://mathlesstraveled.com/2019/05/18/computing-the-euler-totient-function-part-2-seeing-phi-is-multiplicative/">new</a> <a href="https://mathlesstraveled.com/2019/05/27/computing-the-euler-totient-function-part-3-proving-phi-is-multiplicative/">blog posts</a> by Brent Yorgey concern the <a href="https://en.wikipedia.org/wiki/Euler%27s_totient_function">Euler totient function</a> (<a href="https://mathstodon.xyz/@11011110/102118180704402052"/>). Computing it quickly would break RSA; Brent describes using factoring to do better than brute force. The problem is clearly in , and I think it may be a natural candidate for being -intermediate. Igor Pak (who asked me for -intermediate problems when I recently visited UCLA) <a href="https://cstheory.stackexchange.com/q/43954/95">thinks the prime-counting function may be another</a>, but neither function is very combinatorial. <a href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html">In a recent blog post I found a couple of combinatorial candidates</a>, but others would be interesting.</p>
  </li>
  <li>
    <p>The image below (<a href="https://commons.wikimedia.org/wiki/File:Gr%C3%BCnbaum-Rigby_configuration,_vector_graphics.svg">as redrawn by Brammers</a>) is the 
<a href="https://en.wikipedia.org/wiki/Gr%C3%BCnbaum%E2%80%93Rigby_configuration">Grünbaum–Rigby configuration</a> (<a href="https://mathstodon.xyz/@11011110/102119858635464298"/>) with 21 points and lines, 4 points per line, and 4 lines per point. Klein studied it in the complex projective plane in 1879, but it wasn’t known to have this nice real heptagonal realization until Grünbaum and Rigby (1990). The new Wikipedia article on it was started by “Tomo” (whose real-world identity Wikipedia’s arcane outing rules bar me from disclosing, but he just turned 70, so if you figure it out wish him a happy birthday).</p>

    <p style="text-align: center;"><img alt="The Gr&#xFC;nbaum&#x2013;Rigby configuration" src="https://11011110.github.io/blog/assets/2019/grunrig.svg" width="60%"/></p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Garden_of_Eden_(cellular_automaton)">Garden of Eden</a> (<a href="https://mathstodon.xyz/@11011110/102129199678568606"/>). Now a Good Article on Wikipedia.</p>
  </li>
  <li>
    <p><a href="https://www.pnas.org/content/early/2019/05/20/1902572116. Via https://mathstodon.xyz/@helger/102138884170343694">Ono et al prove that almost all Jensen-Pólya polynomials have only real roots</a> (<a href="https://mathstodon.xyz/@11011110/102143915068349382"/>, <a href="https://mathstodon.xyz/@helger/102138884170343694">via</a>). The Riemann Hypothesis is equivalent to the statement that they all do. The same thing works for similar families of polynomials associated with partition functions and proves a conjecture of Chen. See also <a href="https://phys.org/news/2019-05-mathematicians-revive-abandoned-approach-riemann.html">a popularized account</a> and <a href="http://people.oregonstate.edu/~petschec/ONTD/Talk1.pdf">Ono’s talk slides</a>.</p>
  </li>
  <li>
    <p><a href="https://scilogs.spektrum.de/hlf/imu-abacus-medal/">The International Mathematical Union is renaming</a> its <a href="https://en.wikipedia.org/wiki/Nevanlinna_Prize">Nevanlinna Prize</a> to be the IMU Abacus Medal (<a href="https://mathstodon.xyz/@11011110/102149453984232922"/>). The prize is given every four years for major accomplishments in theoretical computer science. The article doesn’t say why rename but it’s because Nevanlinna was a Nazi sympathizer and collaborator. The prize was named after him in the early 1980s because its funding came from Finland, but Nevanlinna also never had much to do with TCS.</p>
  </li>
  <li>
    <p><a href="https://blog.computationalcomplexity.org/2019/05/notorious-lah-or-notorious-lah-or-you.html">Gasarch on proofreading</a> (<a href="https://mathstodon.xyz/@11011110/102154856271421940"/>). Just as in programming, there’s always one more bug.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Ellen_Fetter">Ellen Fetter</a> and <a href="https://en.wikipedia.org/wiki/Margaret_Hamilton_(scientist)">Margaret Hamilton</a>: <a href="https://www.quantamagazine.org/hidden-heroines-of-chaos-ellen-fetter-and-margaret-hamilton-20190520/">Uncredited collaborators with Edward Lorenz at the birth of chaos theory</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/102163812229252010"/>).</span></p>
  </li>
  <li>
    <p><a href="https://www.nytimes.com/2019/05/17/science/math-physics-knitting-matsumoto.html">Elisabetta Matsumoto is studying the mathematics of knitting</a> (<a href="https://mathstodon.xyz/@11011110/102177647389031957"/>, <a href="https://twitter.com/Sabetta_">via</a>), with the hope that it can lead to new programmable metamaterials.</p>
  </li>
  <li>
    <p><a href="https://www.sciencemag.org/news/2019/05/ieee-major-science-publisher-bans-huawei-scientists-reviewing-papers">IEEE bans Huawei employees from reviewing submissions to its journals</a> (<a href="https://mathstodon.xyz/@11011110/102183137967208347"/>, <a href="https://news.ycombinator.com/item?id=20046771">via</a>), saying it is forced to do so by US government sanctions.</p>
  </li>
  <li>
    <p>The UCI University Club (which in other places might be called a faculty club) is next door to the building I work in, and has a bustling side business hosting weddings. Here’s the view that greeted me as I left the office this evening, looking across their lawn towards the gazebo (<a href="https://mathstodon.xyz/@11011110/102188200347058355"/>).</p>

    <p style="text-align: center;"><a href="https://www.ics.uci.edu/~eppstein/pix/uclub/index.html"><img alt="UCI University Club lawn" src="https://www.ics.uci.edu/~eppstein/pix/uclub/uclub-m.jpg" style="border-style: solid; border-color: black;"/></a></p>
  </li>
  <li>
    <p>Line arrangements in architecture (<a href="https://mathstodon.xyz/@11011110/102193430755771327"/>): the beams of <a href="https://en.wikipedia.org/wiki/Mathematical_Bridge">Cambridge’s Mathematical Bridge</a> form tangent lines to its arch and then extend through and support its trusswork, while another set of radial lines tie the structure together. The bridge just looks like a wood truss bridge in real life but <a href="https://commons.wikimedia.org/wiki/File:Mathematical_Bridge_tangents.jpg">this artificially-colored image</a> makes the underlying structure clearer.</p>

    <p style="text-align: center;"><img alt="Cambridge's Mathematical Bridge" src="https://11011110.github.io/blog/assets/2019/cambridgebridge.jpg" style="border-style: solid; border-color: black;" width="80%"/></p>
  </li>
</ul></div>
    </content>
    <updated>2019-05-31T21:40:00Z</updated>
    <published>2019-05-31T21:40:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-06-08T05:49:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/077</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/077" rel="alternate" type="text/html"/>
    <title>TR19-077 |  Consistency of circuit lower bounds with bounded theories | 

	Jan Bydzovsky, 

	Jan  Krajicek, 

	Igor Carboni Oliveira</title>
    <summary>Proving that there are problems in $P^{NP}$ that require boolean circuits of super-linear size is a major frontier in complexity theory. While such lower bounds are known for larger complexity classes, existing results only show that the corresponding problems are hard on infinitely many input lengths. For instance, proving almost-everywhere circuit lower bounds is open even for problems in MAEXP. Giving the notorious difficulty of proving lower bounds that hold for all large input lengths, we ask the following question: 

Can we show that a large set of techniques cannot prove that NP is easy infinitely often? 

Motivated by this and related questions about the interaction between mathematical proofs and computations, we investigate circuit complexity from the perspective of logic.

  Among other results, we prove that for any parameter $k \geq 1$ it is consistent with theory $T$ that computational class $C$ is not contained infinitely often in SIZE$(n^k)$, where $(T, C)$ is one of the pairs:

  $T = T^1_2\;$ and $\;C = P^{NP}$, $\quad T = S^1_2\;$ and $\;C = NP$, $\quad T =~ $PV$\;$ and $C = P$.

  In other words, these theories cannot establish infinitely often circuit upper bounds for the corresponding problems. This is of interest because the weaker theory PV already formalizes sophisticated arguments, such as a proof of the PCP Theorem (Pich, 2015). These consistency statements are unconditional and improve on earlier theorems of Krajicek and Oliveira (2017) and Bydzovsky and Muller (2018) on the consistency of lower bounds with PV.</summary>
    <updated>2019-05-30T10:45:15Z</updated>
    <published>2019-05-30T10:45:15Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-06-09T20:21:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://grigory.github.io/blog/theory-jobs-2019</id>
    <link href="http://grigory.github.io/blog/theory-jobs-2019/" rel="alternate" type="text/html"/>
    <title xml:lang="en">Theory Jobs 2019</title>
    <content type="xhtml" xml:lang="en"><div xmlns="http://www.w3.org/1999/xhtml"><p><img src="http://grigory.github.io/blog/pics/theory-jobs-2019.png"/>
Apparently, it’s a busy life being an assistant prof so there were no posts here all year. However, while some of us are decompressing after the NeurIPS deadline, <a href="https://docs.google.com/spreadsheets/d/1Oegc0quwv2PqoR_pzZlUIrPw4rFsZ4FKoKkUvmLBTHM/edit?usp=sharing">here is a link</a> to a crowdsourced spreadsheet created to collect information about theory jobs this year. 
Congratulations to both job seekers and departments/labs who are done with their searches!</p>

<p>In the past my academic uncle Lance Fortnow set this spreadsheet up (check <a href="https://blog.computationalcomplexity.org/2017/06/theory-jobs-2016.html">this link</a> to his post from two years ago which also has links to all the previous years). This year the first entry is Lance himself who is moving back to Chicago to be the Dean of the College of Science at the Illinois Institute of Technology. Did Lance get the idea from his advisor <a href="https://en.wikipedia.org/wiki/Michael_Sipser">Michael Sipser</a> who is also a Dean of Science but at MIT? In any case, great to see theoretical computer scientists stepping up to be the deans of science, congratulations!</p>

<p>Rules about the spreadsheet have been copied from last years and all edits to the document are anonymized. Please, post a comment if you have any suggestions about the rules.</p>
<ul>
 <li>Separate sheets for faculty, industry and postdocs/visitors. </li>
 <li>People should be connected to theoretical computer science, broadly defined.</li>
 <li>Only add jobs that you are absolutely sure have been offered and accepted. This is not the place for speculation and rumors. </li>
 <li>You are welcome to add yourself, or people your department has hired. </li>
</ul>

<p>This document will continue to grow as more jobs settle.</p>




  <p><a href="http://grigory.github.io/blog/theory-jobs-2019/">Theory Jobs 2019</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on May 30, 2019.</p></div>
    </content>
    <updated>2019-05-30T00:00:00Z</updated>
    <published>2019-05-30T00:00:00Z</published>
    <author>
      <name>Grigory Yaroslavtsev</name>
      <email>grigory@grigory.us</email>
      <uri>http://grigory.github.io/blog</uri>
    </author>
    <source>
      <id>http://grigory.github.io/blog/</id>
      <author>
        <name>Grigory Yaroslavtsev</name>
        <email>grigory@grigory.us</email>
        <uri>http://grigory.github.io/blog/</uri>
      </author>
      <link href="http://grigory.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="http://grigory.github.io/blog" rel="alternate" type="text/html"/>
      <title xml:lang="en">The Big Data Theory</title>
      <updated>2019-05-30T19:18:10Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-1040381893569171546</id>
    <link href="https://blog.computationalcomplexity.org/feeds/1040381893569171546/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/nsf-panels.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1040381893569171546" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1040381893569171546" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/nsf-panels.html" rel="alternate" type="text/html"/>
    <title>NSF Panels</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The government shut down in January led to delays at the National Science Foundation and only recently announcing decisions on grants submitted last fall. For those who successfully received awards, congratulations! For those who didn't, don't take it personally, buckle up and try again.<br/>
<br/>
For those who don't know how the process works, for each grant program, the program directors organize one or more panels which typically meets in person at NSF headquarters in Alexandria, Virginia. A typical panel has about a dozen panelists and twenty or so proposals. Before the panels, each proposal gets at least three reviews by the panelists. Discussions ensue over a day or two, proposals get sorted into categories: Highly Competitive, Competitive, Low Competitive and Not Competitive and then ranked ordered in the top categories.<br/>
<br/>
There are tight rules for Conflict-of-Interest and those who are conflicted have to leave the room during the discussions on those papers.<br/>
<br/>
If you do get asked to serve on a panel, you should definitely do so. You get to see how the process works and help influence funding and research directions in your field. You can't reveal when you serve on a particular panel but you can say "Served on NSF Panels" on your CV.<br/>
<br/>
Panels tend to take proposals that will likely make progress and not take ones less risky. Funding risky proposals is specifically mentioned to the panel but when push comes to shove and there is less funding than worthy proposals, panelists gravitate towards proposals that don't take chances.<br/>
<br/>
Panels are not unlike conference program committees. It didn't always work this way, it used to be more like journal publications. I remember when the program director would send out proposals for outside reviews and then make funding decisions. That gave the program director more discretion to fund a wider variety of proposals.<br/>
<br/>
The NSF budget for computing goes up slowly while the number of academic computer scientists grows at a much larger clip. Until this changes, we'll have more and more worthy proposals unfunded, particularly proposals of bold risky projects. That's the saddest part of all.</div>
    </content>
    <updated>2019-05-29T20:09:00Z</updated>
    <published>2019-05-29T20:09:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-06-09T15:44:55Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-25562705.post-831739446833439686</id>
    <link href="http://aaronsadventures.blogspot.com/feeds/831739446833439686/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=25562705&amp;postID=831739446833439686" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/25562705/posts/default/831739446833439686" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/25562705/posts/default/831739446833439686" rel="self" type="application/atom+xml"/>
    <link href="http://aaronsadventures.blogspot.com/2019/05/individual-notions-of-fairness-you-can.html" rel="alternate" type="text/html"/>
    <title>Individual Notions of Fairness You Can Use</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div style="text-align: center;"><span style="font-size: x-large;"><u>Individual Notions of Fairness You Can Use</u></span></div><br/>Our group at Penn has been thinking about when <i>individual </i>notions of fairness might be practically achievable for awhile, and we have <a href="https://arxiv.org/abs/1905.10660">two</a> <a href="https://arxiv.org/abs/1905.10607">new</a> approaches.<br/><br/><span style="font-size: large;"><u>Background</u>:</span><br/><u>Statistical Fairness</u><br/>I've written about this before, <a href="http://aaronsadventures.blogspot.com/2017/11/between-statistical-and-individual.html">here</a>. But briefly: there are two families of definitions in the fairness in machine learning literature. The first group of definitions, which I call <i>statistical</i> fairness notions, is far and away the most popular. If you want to come up with your own statistical fairness notion, you can follow this recipe:<br/><ol><li>Partition the world into a small number of "protected sub-groups". You will probably be thinking along the lines of race or gender or something similar when you do this.</li><li>Pick your favorite error/accuracy metric for a classifier. This might literally be classification error, or false positive or false negative rate, or positive predictive value, or something else. Lots of options here. </li><li>Ask that this metric be approximately equalized across your protected groups.</li><li>Finally, enjoy your new statistical fairness measure! Congratulations!</li></ol><div>These definitions are far and away the most popular in this literature, in large part (I think) because they are so immediately actionable. Because they are defined as conditions on a small number of expectations, you can easily check whether your classifier is "fair" according to these metrics, and (although there are some interesting computational challenges) go and try and learn classifiers subject to these constraints. </div><div><br/></div><div>Their major problem is related to the reason for their success: they are defined as conditions on a small number of expectations or <i>averages</i> over people, and so they don't promise much to particular individuals. I'll borrow an example from our <a href="https://arxiv.org/abs/1711.05144">fairness gerrymandering</a> paper from a few years ago to put this in sharp relief. Imagine that we are building a system to decide who to incarcerate, and we want to be "fair" with respect to both gender (men and women) and race (green and blue people). We decide that in our scenario, it is the false positives who are harmed (innocent people sent to jail), and so to be fair, we decide should equalize the false positive rate: across men and women, and across greens and blues. But one way to do this is to jail all green men and blue women. This does indeed equalize the false positive rate (at 50%) across all four of the groups we specified, but is cold comfort if you happen to be a green man --- since then you will be jailed with certainty. The problem was our fairness constraint was never a promise to an individual to begin with, just a promise about the average behavior of our classifier over a large group. And although this is a toy example constructed to make a point, things like this happen in real data too. </div><br/><u>Individual Fairness</u><br/>Individual notions of fairness, on the other hand, really do correspond to promises made to individuals. There are at least two kinds of individual fairness definitions that have been proposed: <a href="https://dl.acm.org/citation.cfm?id=2090255">metric fairness</a>, and <a href="http://papers.nips.cc/paper/6355-fairness-in-learning-classic-and-contextual-bandits">weakly meritocratic fairness</a>. Metric fairness proposes that the learner will be handed a <i>task specific similarity metric</i>, and requires that individuals who are close together in the metric should have a similar probability of being classified as positive. Weakly meritocratic fairness, on the other hand, takes the (unknown) labels of an individual as a measure of merit, and requires that individuals who have a higher probability of really having a positive label should have only a higher probability of being classified as positive. This in particular implies that false positive and false negative rates should be equalized <i>across individuals</i>, where now the word <i>rate</i> is averaging over only the randomness of the classifier, not over people. What makes both of these <i>individual</i> notions of fairness is that they impose constraints that bind on all pairs of individuals and not just over averages of people.<br/><br/>Definitions like this have the advantage of strong individual-level semantics, which the statistical definitions don't have. But they also have big problems: for metric fairness, the obvious question is: <i>where does the metric come from</i>? Even granting that fairness should be some Lipschitz condition on a metric, it seems hard to pin down what the metric is, and different people will disagree: coming up with the metric seems to encapsulate a large part of the original problem of defining fairness. For weakly meritocratic fairness, the obvious problem is that we don't know what the labels are. Its possible to do non-trivial things if you make assumptions about the label generating process, but its not at all clear you can do any non-trivial learning subject to this constraint if you don't make strong assumptions.<br/><br/><span style="font-size: large;"><u>Two New Approaches:</u></span><br/>We have two new approaches, building off of metric fairness and weakly meritocratic fairness respectively. Both have the advantages of statistical notions of fairness in that they can be put into practice without making unrealistic assumptions about the data, and without needing to wait on someone to hand us a metric. But they continue to make meaningful promises to individuals.<br/><br/><u>Subjective Individual Fairness</u><br/>Lets start with our variant of metric fairness, which we call subjective individual fairness. (This is joint work with Michael Kearns, our PhD students Chris Jung and Seth Neel, our former PhD student Steven Wu, and Steven's student (our grand student!) Logan Stapleton). The paper is here: <a href="https://arxiv.org/abs/1905.10660">https://arxiv.org/abs/1905.10660</a>. We stick with the premise that "similar people should be treated similarly", and that whether or not it is correct/just/etc., it is at least fair to treat two people the same way, in the sense that we classify them as positive with the same probability. But we don't want to assume anything else.<br/><br/>Suppose I were to create a machine learning fairness panel: I could recruit "AI Ethics" experts, moral philosophers, hyped up consultants, people off the street, toddlers, etc. I would expect that there would be as many different conceptions of fairness as there were people on the panel, and that none of them could precisely quantify what they meant by fairness --- certainly not in the form of a "fairness metric". But I could still ask these people, in particular cases, if they thought it was fair that two particular individuals be treated differently or not.<br/><br/>Of course, I would have no reason to expect that the responses that I got from the different panelists would be consistent with one another --- or possibly even internally consistent (we won't assume, e.g. that the responses satisfy any kind of triangle inequality). Nevertheless, once we fix a data distribution and a group of people who have opinions about fairness, we have a well defined tradeoff we can hope to manage: any classifier we could choose will have both:<br/><ol><li>Some error rate, and</li><li>Some frequency with which it makes a pair of decisions that someone in the group finds unfair. </li></ol><div>We can hope to find classifiers that optimally trade off 1 and 2: note this is a coherent tradeoff even though we haven't forced the people to try and express their conceptions of fairness into some consistent metric. What we show is that you can do this. </div><div><br/></div><div>Specifically, given a set of pairs that we have determined should be treated similarly, there is an <i>oracle efficient </i>algorithm that can find the optimal classifier subject to the constraint that no pair of individuals that has been specified as a constraint should have a substantially different probability of positive classification. Oracle efficiency means that what we can do is reduce the "fair learning" problem to a regular old learning problem, without fairness constraints. If we can solve the regular learning problem, we can also solve the fair learning problem. This kind of fairness constraint also generalizes in the standard way: if you ask your fairness panel about a reasonably small number of pairs, and then solve the in-sample problem subject to these constraints, the classifier you learn will also satisfy the fairness constraints out of sample. And it works: we implement the algorithm and try it out on the COMPAS data set, with fairness constraints that we elicited from 43 human (undergrad) subjects. The interesting thing is that once you have an algorithm like this, it isn't only a tool to create "fair" machine learning models: its also a new instrument to investigate human conceptions of fairness. We already see quite a bit of variation among our 43 subjects in our preliminary experiments. We plan to pursue this direction more going forward.</div><div><br/></div><div><u>Average Individual Fairness</u></div><div>Next, our variant of weakly meritocratic fairness. This is joint work with Michael Kearns and our student Saeed Sharifi. The paper is here: <a href="https://arxiv.org/abs/1905.10607">https://arxiv.org/abs/1905.10607</a>. In certain scenarios, it really does seem tempting to think about fairness in terms of false positive rates. Criminal justice is a great example, in the sense that it is clear that everyone agrees on which outcome they <i>want</i> (they would like to be released from jail), and so the people we are being unfair to really do seem to be the false positives: the people who should have been released from jail, but who were mistakenly incarcerated for longer. So in our "fairness gerrymandering" example above, maybe the problem with thinking about false positive rates wasn't a problem with <i>false positives</i>, but with <i>rates</i>: i.e. the problem was that the word rate averaged over many people, and so it didn't promise <i>you</i> anything. Our idea is to redefine the word rate. </div><div><br/></div><div>In some (but certainly not all) settings, people are subject to not just one, but many classification tasks. For example, consider online advertising: you might be shown thousands of targeted ads each month. Or applying for schools (a process that is centralized in cities like New York): you apply not just to one school, but to many. In situations like this, we can model the fact that we have not just a distribution over people, but also a distribution over (or collection of) problems. </div><div><br/></div><div>Once we have a distribution over problems, we can define the error rate, or false positive rate, or any other rate you like <i>for individuals. </i>It is now sensible to talk about Alice's false positive rate, or Bob's error rate, because rate has been redefined as an average over problems, for a particular individual. So we can now ask for individual fairness notions in the spirit of the statistical notions of fairness we discussed above! We no longer need to define protected groups: we can now ask that the false positive rates, or error rates, be equalized across all pairs of people. </div><div><br/></div><div>It turns out that given a reasonably sized sample of people, and a reasonably sized sample of problems, it is tractable to find the optimal classifier subject to constraints like this in sample, and that these guarantees generalize out of sample. The in-sample algorithm is again an oracle-efficient algorithm, or in other words, a reduction to standard, unconstrained learning. The generalization guarantee here is a little interesting, because now we are talking about simultaneous generalization in two different directions: to people we haven't seen before, and also to problems we haven't seen before. This requires thinking a little bit about what kind of object we are even trying to output: a mapping from new problems to classifiers. The details are in the paper (spoiler --- the mapping is defined by the optimal dual variables for the empirical risk minimization problem): here, I'll just point out that again, the algorithm is practical to implement, and we perform some simple experiments with it. </div><div><br/></div><div><br/></div><br/><br/><br/></div>
    </content>
    <updated>2019-05-28T10:47:00Z</updated>
    <published>2019-05-28T10:47:00Z</published>
    <author>
      <name>Aaron</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/09952936358739421126</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-25562705</id>
      <category term="game theory"/>
      <category term="news"/>
      <author>
        <name>Aaron</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/09952936358739421126</uri>
      </author>
      <link href="http://aaronsadventures.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/25562705/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://aaronsadventures.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/25562705/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <title>Adventures in Computation</title>
      <updated>2019-06-03T07:25:04Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/05/28/one-or-more-phd-stipends-in-machine-learning-for-wireless-communications-8-19015-at-department-of-electronic-systems-aalborg-university-apply-by-june-10-2019/</id>
    <link href="https://cstheory-jobs.org/2019/05/28/one-or-more-phd-stipends-in-machine-learning-for-wireless-communications-8-19015-at-department-of-electronic-systems-aalborg-university-apply-by-june-10-2019/" rel="alternate" type="text/html"/>
    <title>One or more PhD Stipends in Machine Learning for Wireless Communications (8-19015) at Department of Electronic Systems, Aalborg University (apply by June 10, 2019)</title>
    <summary>WINDMILL Early Stage Researcher 9: Optimizing URLLC metadata/data flows using machine learning Aalborg University is seeking to hire an Early Stage Researcher (ESR) to join the Marie Skłodowska-Curie Innovative Training Network on “Integrating Wireless Communication ENgineering and MachIne Learning”.(WindMill). More details are included https://windmill-itn.eu/ Website: https://www.stillinger.aau.dk/vis-stilling/?vacancy=1029705 Email: edc@es.aau.dk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>WINDMILL Early Stage Researcher 9: Optimizing URLLC metadata/data flows using machine learning</p>
<p>Aalborg University is seeking to hire an Early Stage Researcher (ESR) to join the Marie Skłodowska-Curie Innovative Training Network on “Integrating Wireless Communication ENgineering and MachIne Learning”.(WindMill). More details are included <a href="https://windmill-itn.eu/">https://windmill-itn.eu/</a></p>
<p>Website: <a href="https://www.stillinger.aau.dk/vis-stilling/?vacancy=1029705">https://www.stillinger.aau.dk/vis-stilling/?vacancy=1029705</a><br/>
Email: edc@es.aau.dk</p></div>
    </content>
    <updated>2019-05-28T09:14:08Z</updated>
    <published>2019-05-28T09:14:08Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-06-09T20:21:20Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/05/28/one-or-more-phd-stipends-in-machine-learning-for-wireless-communications-8-19014-at-department-of-electronic-systems-aalborg-university-apply-by-june-10-2019/</id>
    <link href="https://cstheory-jobs.org/2019/05/28/one-or-more-phd-stipends-in-machine-learning-for-wireless-communications-8-19014-at-department-of-electronic-systems-aalborg-university-apply-by-june-10-2019/" rel="alternate" type="text/html"/>
    <title>One or more PhD Stipends in Machine Learning for Wireless Communications (8-19014) at Department of Electronic Systems, Aalborg University (apply by June 10, 2019)</title>
    <summary>Aalborg University is seeking to hire an Early Stage Researcher (ESR) to join the Marie Skłodowska-Curie Innovative Training Network on “Integrating Wireless Communication ENgineering and MachIne Learning”.(WindMill). More details are included https://windmill-itn.eu/ Website: https://www.stillinger.aau.dk/vis-stilling/?vacancy=1029703 Email: edc@es.aau.dk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Aalborg University is seeking to hire an Early Stage Researcher (ESR) to join the Marie Skłodowska-Curie Innovative Training Network on “Integrating Wireless Communication ENgineering and MachIne Learning”.(WindMill). More details are included <a href="https://windmill-itn.eu/">https://windmill-itn.eu/</a></p>
<p>Website: <a href="https://www.stillinger.aau.dk/vis-stilling/?vacancy=1029703">https://www.stillinger.aau.dk/vis-stilling/?vacancy=1029703</a><br/>
Email: edc@es.aau.dk</p></div>
    </content>
    <updated>2019-05-28T09:11:31Z</updated>
    <published>2019-05-28T09:11:31Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-06-09T20:21:20Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality</id>
    <link href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html" rel="alternate" type="text/html"/>
    <title>Shattering and quasipolynomiality</title>
    <summary>An inadequately-explained phenomenon in computational complexity theory is that there are so few natural candidates for -intermediate problems, problems in but neither in nor -complete. Of course, if there are none, and the dichotomy theorem implies that there are no intermediate Boolean constraint satisfaction problems. But there are a lot of other types of problems in , and a theorem of Ladner1 shows that there should be an infinite hierarchy of degrees of hardness within . So where are all the members of this hierarchy, and why are they so shy? Ladner, Richard (1975), “On the structure of polynomial time reducibility”, J. ACM 22 (1): 155–171. ↩</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>An inadequately-explained phenomenon in computational complexity theory is that there are so few natural candidates for <a href="https://en.wikipedia.org/wiki/NP-intermediate">-intermediate problems</a>, problems in  but neither in  nor -complete. Of course, if  there are none, and the <a href="https://en.wikipedia.org/wiki/Schaefer%27s_dichotomy_theorem">dichotomy theorem</a> implies that there are no intermediate Boolean constraint satisfaction problems. But there are a lot of other types of problems in , and a theorem of Ladner<sup id="fnref:l"><a class="footnote" href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fn:l">1</a></sup> shows that there should be an infinite hierarchy of degrees of hardness within . So where are all the members of this hierarchy, and why are they so shy?</p>

<p>The same thing happens not just for  but for other related complexity classes like <a href="https://en.wikipedia.org/wiki/%E2%99%AFP"/>. There should be many -intermediate classes but we know even fewer than for . <a href="https://mathstodon.xyz/@11011110/102118180704402052">I recently posted</a> about a discussion I had with Igor Pak on this issue, in which we suggested to each other two number-theoretic candidates for being -intermediate, the <a href="https://en.wikipedia.org/wiki/Euler%27s_totient_function">Euler totient function</a> and the <a href="https://en.wikipedia.org/wiki/Prime-counting_function">prime-counting function</a> (see also <a href="https://cstheory.stackexchange.com/q/43954/95">Igor’s StackExchange question on this</a>). But although they’re in , neither of these functions is very combinatorial.</p>

<p>So anyway, the point of all this is to discuss more candidates for being -intermediate that are, I think, natural and combinatorial. They’re part of a family of problems that include a couple of related candidates for being -intermediate, and even a candidate for being -intermediate. These problems come from computational learning theory, or alternatively they can be seen as coming from mathematical logic, hereditary graph theory, and the theory of the <a href="https://en.wikipedia.org/wiki/Rado_graph">Rado graph</a>. And they’re all at what is in some sense the shallow end of the intermediate problems: they’re solvable in quasi-polynomial time, meaning , but not known to be solvable in polynomial time. So this is pretty strong evidence that they’re not complete for their respective complexity classes, but weaker evidence than usual that they’re not polynomial.</p>

<p>In learning theory, a family of sets  is said to <em>shatter</em> another set  (not necessarily belonging to ) if every subset of , including the empty set and  itself, can be obtained by intersecting  with some member of . The <a href="https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension">Vapnik–Chervonenkis dimension</a> of  is just the size of the largest set that is shattered by  . If we let  (the number of sets in the family) and  (the number of distinct elements in those sets), then the dimension is clearly at most , because sets of size larger than  have too many subsets for them all to be formed by intersection with a member of . Therefore, the following problem can be solved in quasipolynomial time, by a brute-force search of the  small-enough subsets of :<sup id="fnref:lmr"><a class="footnote" href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fn:lmr">2</a></sup></p>

<dl>
  <dt>VC-dimension (largest shattered set)</dt>
  <dd>Input: family of sets , number 

    <p>Output: true if  shatters a set of size , false otherwise.</p>
  </dd>
</dl>

<p>The same quasipolynomial time bound applies to the following related problems,
the first of which is also in  and the second of which is in :</p>

<dl>
  <dt>Smallest non-shattered set</dt>
  <dd>Input: family of sets , number 

    <p>Output: True if there exists a subset  of 
of size  that is not shattered by , false otherwise.</p>
  </dd>
  <dt>Number of shattered sets</dt>
  <dd>Input: family of sets 

    <p>Output: the number of sets shattered by .</p>
  </dd>
</dl>

<p>For the first two problems, being non--complete hinges on the assumption that , but for the number of shattered sets, being non--complete (under <a href="https://en.wikipedia.org/wiki/Polynomial-time_counting_reduction">counting reductions</a>) is unconditional: the output doesn’t provide enough bits of information to encode the answers to all other  problems.
The VC-dimension is hard to approximate under a form of the <a href="https://en.wikipedia.org/wiki/Exponential_time_hypothesis">exponential time hypothesis</a>, strongly suggesting that it cannot be computed exactly in polynomial time.<sup id="fnref:mr"><a class="footnote" href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fn:mr">3</a></sup></p>

<p>To see that the two existence problems can sometimes both have answers that are logarithmic, it’s helpful to turn to the theory of random graphs, and of <em>the</em> random graph, the <a href="https://en.wikipedia.org/wiki/Rado_graph">Rado graph</a>. This graph obeys a collection of <em>extension axioms</em> according to which, for every two disjoint finite subsets of vertices, there exists another vertex adjacent to everything in the first subset and to nothing in the second subset. Using these axioms, we can build up induced copies of any finite or countable subgraph, one vertex at a time, using a greedy algorithm. Based on this property, let’s define a subset  of the vertices in an undirected graph to be <em>extensible</em> if, for every partition of  into two disjoint subsets, there exists another vertex outside  that is adjacent to everything in the first subset and to nothing in the second subset. This is nothing more than being shattered by the neighborhoods of the vertices outside . So we have the following corresponding problems.</p>

<dl>
  <dt>Largest extensible set</dt>
  <dd>Input: Undirected graph , number 

    <p>Output: true if  has an extensible set of size , false otherwise.</p>
  </dd>
  <dt>Smallest non-extensible set</dt>
  <dd>Input: Undirected graph , number 

    <p>Output: true if  has a non-extensible set of size , false otherwise.</p>
  </dd>
  <dt>Smallest missing induced subgraph</dt>
  <dd>Input: Undirected graph , number 

    <p>Output: true if there is a graph  on at most  vertices that
is not an induced subgraph of , false otherwise.</p>
  </dd>
  <dt>Number of extensible sets</dt>
  <dd>Input: Undirected graph 

    <p>Output: The number of extensible sets of vertices of .</p>
  </dd>
</dl>

<p>The smallest missing induced subgraph size naturally falls into the complexity class  of problems for which you can guess a solution (the missing subgraph) but then verifying it involves solving a co- problem (is this subgraph missing).
It is greater than the size of the smallest non-extensible set, because if you try to build up a given induced subgraph by adding one vertex at a time greedily you can only get stuck at a non-extensible set. There must be a missing induced subgraph of size at most , because there are  isomorphism classes of -vertex labeled graphs and fewer than  ways of choosing which of the  labeled vertices correspond to vertices of , so for larger values of  than this bound there are more labeled graphs than placements of them as induced subgraphs. Another way of thinking about the smallest missing induced subgraph problem is that we are asking for the largest  for which  is <a href="https://en.wikipedia.org/wiki/Universal_graph">-universal</a>: it contains all graphs on at most  vertices as induced subgraphs.</p>

<p>The smallest non-extensible set and the smallest missing subgraph are both easy on any hereditary class of graphs, because these classes always have a missing subgraph of size . On the other hand, if  is chosen uniformly at random among the -vertex graphs, then any small subset of its vertices is extensible with high probability, so the smallest non-extensible set has expected size .</p>

<p>If these problems are not - and -complete, what are they? Papadimitriou and Yannakakis<sup id="fnref:py"><a class="footnote" href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fn:py">4</a></sup> define a complexity class , and show that VC-dimension is -complete. Presumably, because it’s so similar, the same is true for the largest extensible set. Maybe it’s possible to prove completeness for the smallest missing induced subgraph in an analogue of  at the level of , and to prove completeness for the number of shattered sets and number of extensible sets in an analogue of  at this level.</p>

<div class="footnotes">
  <ol>
    <li id="fn:l">
      <p>Ladner, Richard (1975), “<a href="https://doi.org/10.1145/321864.321877">On the structure of polynomial time reducibility</a>”, <em>J. ACM</em> 22 (1): 155–171. <a class="reversefootnote" href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fnref:l">↩</a></p>
    </li>
    <li id="fn:lmr">
      <p>Linial, Nathan, Mansour, Yishay, and Rivest, Ronald L. (1991), “<a href="https://doi.org/10.1016/0890-5401(91)90058-A">Results on learnability and the Vapnik–Chervonenkis dimension</a>”, <em>Inf. Comput.</em> 90 (1): 33–49. <a class="reversefootnote" href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fnref:lmr">↩</a></p>
    </li>
    <li id="fn:mr">
      <p>Manurangsi, Pasin, and Rubinstein, Aviad (2017), “<a href="http://proceedings.mlr.press/v65/manurangsi17a.html">Inapproximability of VC dimension and Littlestone’s dimension</a>”, <em>Proc. 2017 Conf. Learning Theory (COLT 2017)</em>, Proceedings of Machine Learning Research 65, pp. 1432–1460. <a class="reversefootnote" href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fnref:mr">↩</a></p>
    </li>
    <li id="fn:py">
      <p>Papadimitriou, Christos H., and Yannakakis, Mihalis (1996), “<a href="https://doi.org/10.1006/jcss.1996.0058">On limited nondeterminism and the complexity of the V–C dimension</a>”, <em>J. Comput. Syst. Sci.</em> 53 (2): 161–170. <a class="reversefootnote" href="https://11011110.github.io/blog/2019/05/27/shattering-quasipolynomiality.html#fnref:py">↩</a></p>
    </li>
  </ol>
</div>

<p>(<a href="https://mathstodon.xyz/@11011110/102170815471019923">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2019-05-27T17:07:00Z</updated>
    <published>2019-05-27T17:07:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-06-08T05:49:29Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3247584017741087776</id>
    <link href="https://blog.computationalcomplexity.org/feeds/3247584017741087776/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/separating-fact-from-fiction-with-56-of.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3247584017741087776" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3247584017741087776" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/separating-fact-from-fiction-with-56-of.html" rel="alternate" type="text/html"/>
    <title>separating fact from fiction with the 56% of Americans say Arabic Numerals should not be taught in school</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><br/>
On the excellent TV show Veep there was a subplot about a political candidate (who himself had failed algebra in HS) objecting to Algebra since it was invented by the Muslims. I don't recall the exact line, but he said something like `Math teachers are terrorists'<br/>
This was, of course, fiction.<br/>
<br/>
The same week I read that 56% of survey respondents say `<u><i>Arabic Numerals' shouldn't be taught in</i></u> <i><u>schools'</u></i> Obviously also a fiction. Perhaps a headline from <i>The Onion</i>.<br/>
<br/>
No. The story is true.<br/>
<br/>
See snopes entry on this: <a href="https://www.snopes.com/fact-check/teaching-arabic-numerals/">here</a><br/>
<br/>
but also see many FALSE but FUNNY websites:<br/>
<br/>
Sarah Palin wants Arabic Numerals out of the schools: <a href="http://nationalreport.net/sarah-palin-wants-arabic-numerals-banned-americas-schools/">here</a> Funny but false.<br/>
<br/>
Jerry Brown is forcing students in California to learn Arabic Numerals as part of multi-culturism False by funny:  <a href="https://me.me/i/sharia-law-must-be-stopped-under-gov-brown-students-in-20990368">here</a><br/>
<br/>
A website urging us to use Roman Numerals (which Jesus used!) False but funny:  <a href="http://freedomnumerals.com/">here</a><br/>
<br/>
OKAY, what to make of the truth that really, really, 56% of Americans are against Arab Numerals<br/>
<br/>
1) Bigotry combined with ignorance.<br/>
<br/>
2) Some of the articles I read about this say its a problem with polls and people. There may be some of that, but still worries me.<br/>
<br/>
3) In Nazi Germany (WOW- Goodwin's law popped up rather early!) they stopped teaching relativity because Albert Einstein was Jewish (the story is more complicated than that, see <a href="https://www.scientificamerican.com/article/how-2-pro-nazi-nobelists-attacked-einstein-s-jewish-science-excerpt1/">her</a>e). That could of course never happen in America now (or could it, see <a href="https://www.tabletmag.com/jewish-news-and-politics/50097/time-warp">here</a> and <a href="https://www.conservapedia.com/index.php?title=Counterexamples_to_Relativity">here</a>).<br/>
<br/>
4) There is no danger that we will dump Arabic Numerals. I wonder if we will change there name to Freedom Numerals.<br/>
<br/>
5) Ignorance of science is a more immediate problem with the anti-vax people. See <a href="https://www.thedailybeast.com/measles-outbreak-grows-with-60-new-cases-across-26-states?ref=home">here</a><br/>
<br/>
<br/></div>
    </content>
    <updated>2019-05-27T15:12:00Z</updated>
    <published>2019-05-27T15:12:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-06-09T15:44:55Z</updated>
    </source>
  </entry>
</feed>
