<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2022-06-23T03:38:54Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://thmatters.wordpress.com/?p=1376</id>
    <link href="https://thmatters.wordpress.com/2022/06/22/call-for-tcs-job-market-profiles/" rel="alternate" type="text/html"/>
    <title>Call for TCS Job Market profiles</title>
    <summary>CATCS is resuming the effort to collect and disseminate profiles of theory researchers who are going on the job market during the 2022-23 academic year, complementing the job postings collected under the Jobs tab.  The goals are to provide: a platform to job-seekers to advertise their credentials and an interface for institutions/individuals with open positions […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>CATCS is resuming the effort to collect and disseminate profiles of theory researchers who are going on the job market during the 2022-23 academic year, complementing the job postings collected under the <a href="https://cstheory-jobs.org/">Jobs tab</a>.  The goals are to provide:</p>



<ul><li>a platform to job-seekers to advertise their credentials and</li><li>an interface for institutions/individuals with open positions to find prospective candidates.</li></ul>



<p>Candidates can fill out this <a href="https://docs.google.com/forms/u/1/d/1jeN3hjzwdRCmI3-7yzuHmyelcZjCtKDn19X2dj4POro/edit?urp=gmail_link">form</a>, which asks for basic information, graduation date (past or future), cv, bio, research summary, etc.</p>



<p>The responses will be reviewed and, if approved, edited and posted on Theory Matters starting on Oct 15, 2022. There is no deadline, but for responses received after Oct 15, please allow two weeks for review before your profile appears on the website. Responses received by Oct 15 will have summaries published in the following issue of SIGACT News (Dec’22 issue).</p></div>
    </content>
    <updated>2022-06-22T21:28:41Z</updated>
    <published>2022-06-22T21:28:41Z</published>
    <category term="for PhD students"/>
    <category term="postdocs"/>
    <category term="TCS Community"/>
    <author>
      <name>shuchic</name>
    </author>
    <source>
      <id>https://thmatters.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://thmatters.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://thmatters.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://thmatters.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://thmatters.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theory Matters</title>
      <updated>2022-06-23T03:37:56Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://thmatters.wordpress.com/?p=1371</id>
    <link href="https://thmatters.wordpress.com/2022/06/22/women-in-tcs-profiles/" rel="alternate" type="text/html"/>
    <title>Women in TCS Profiles</title>
    <summary>Are you trying to form a committee or panel or invite speakers for a TCS event, but cannot find enough women? Look no further. This spreadsheet contains the profiles of 100+ women TCS researchers spanning many subareas of TCS; countries; universities and research institutions; and career stages. The spreadsheet is password protected. The password is […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Are you trying to form a committee or panel or invite speakers for a TCS event, but cannot find enough women? Look no further. <a href="https://www.dropbox.com/scl/fi/45bn5wg870u004zckuleq/Women-in-TCS-Profiles.xlsx?dl=0&amp;rlkey=yk4yxkwxg79w6qmp2tndophvl">This spreadsheet</a> contains the profiles of 100+ women TCS researchers spanning many subareas of TCS; countries; universities and research institutions; and career stages. </p>



<p>The spreadsheet is password protected. The password is a case-sensitive five letter phrase that captures the most iconic (but as yet unsolved) problem in TCS.</p>



<p>The spreadsheet was made possible through the efforts of Yusu Wang and Kira Goldner. We have full permission of the participants to have their information shared publicly. </p>



<p>Would you like your information added to or corrected on the spreadsheet? If so, fill out <a href="https://docs.google.com/forms/d/e/1FAIpQLSc2LcI0mtUvyKgl34OqbxDVpu0zbYs0fiLmU_5jr2qHybCfMQ/viewform">this Google form</a>. Any edits will be verified and posted to the spreadsheet within 3-4 weeks.</p></div>
    </content>
    <updated>2022-06-22T21:25:40Z</updated>
    <published>2022-06-22T21:25:40Z</published>
    <category term="featured"/>
    <category term="TCS Community"/>
    <author>
      <name>shuchic</name>
    </author>
    <source>
      <id>https://thmatters.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://thmatters.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://thmatters.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://thmatters.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://thmatters.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theory Matters</title>
      <updated>2022-06-23T03:37:56Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2022/06/22/dehn-rank-revisited</id>
    <link href="https://11011110.github.io/blog/2022/06/22/dehn-rank-revisited.html" rel="alternate" type="text/html"/>
    <title>Dehn rank revisited</title>
    <summary>In a recent post, I discussed dissection of orthogonal polygons into each other by axis-parallel cuts, translation, and gluing. Each polygon has a value associated with it, called its Dehn invariant, that cannot be changed by dissection, so two polygons that can be dissected into each other must have equal invariants. And for past usage of Dehn invariants, that was pretty much all we looked at: are they equal or not? But my post pointed out that these invariants actually have a lot of structure (you can think of them as matrices, after an arbitrary choice of basis) and this structure is geometrically meaningful. Matrices (or tensors) have a rank, and the rank of the Dehn invariant is a lower bound on the number of rectangles into which a polygon can be dissected. This in turn has implications on the ability of a polygon or its dissections to tile the plane.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In <a href="https://11011110.github.io/blog/2022/04/03/dissection-into-rectangles.html">a recent post</a>, I discussed dissection of orthogonal polygons into each other by axis-parallel cuts, translation, and gluing. Each polygon has a value associated with it, called its <a href="https://en.wikipedia.org/wiki/Dehn_invariant">Dehn invariant</a>, that cannot be changed by dissection, so two polygons that can be dissected into each other must have equal invariants. And for past usage of Dehn invariants, that was pretty much all we looked at: are they equal or not? But my post pointed out that these invariants actually have a lot of structure (you can think of them as matrices, after an arbitrary choice of basis) and this structure is geometrically meaningful. Matrices (or tensors) have a rank, and the rank of the Dehn invariant is a lower bound on  the number of rectangles into which a polygon can be dissected. This in turn has implications on the ability of a polygon or its dissections to tile the plane.</p>

<p>Now it’s a paper: “Orthogonal dissection into few rectangles”, <a href="https://arxiv.org/abs/2206.10675">arXiv:2206.10675</a>, to appear at CCCG. The results of the paper are stronger: instead of just using the Dehn rank as a lower bound, I proved that it always equals the minimum number of rectangles into which a given polygon can be dissected, and can be used to compute this number of rectangles efficiently. The two main steps of the proof are:</p>

<ul>
  <li>
    <p>constructing a set of the right number of rectangles for any given tensor, by a direct geometric construction that turns an algebraic realization of the rank (a combination of positive and negative rectangles) into a geometric representation without the negativity, and</p>

    <p style="text-align: center;"><img alt="Construction of a set of rectangles realizing a given tensor as their Dehn invariant" src="https://11011110.github.io/blog/assets/2022/dehn-realizability.svg" style="width: 100%;"/></p>
  </li>
  <li>
    <p>finding a dissection for any two polygons with equal invariants, by induction on the dimensions of the matrix describing their invariants.</p>

    <p style="text-align: center;"><img alt="Induction step for proving the existence of a dissection between polygons with equal Dehn invariants" src="https://11011110.github.io/blog/assets/2022/dehn-dissectability.svg" style="width: 100%;"/></p>
  </li>
</ul>

<p>The images above are taken from the illustrations for the proofs of these two results from the paper, and maybe will provide a little insight into how they might be proven. For details, see the paper. Instead, here, I wanted to highlight a different, related problem, that I wasn’t able to prove as much about.</p>

<p>Mostly when mathematicians talk about Dehn invariants, it’s for 3d dissections: cutting polyhedra up along planes, translating and rotating the pieces, and gluing them back together. The 3d Dehn invariant combines lengths and angles of polyhedron edges in the same way that the 2d invariant combines widths and heights of rectangles. Therefore, 3d Dehn rank is a lower bound on the number of edges you can dissect something into. It’s easy to construct polyhedra with arbitrarily large rank, and these polyhedra are forced to have arbitrarily large numbers of edges, no matter how you try to cut them up and reassemble them.</p>

<p>However, the 3d Dehn rank is not exactly equal to the minimum number of edges after dissection. A cube has Dehn invariant zero (with rank zero), but the minimum number of edges of a polyhedron you can dissect it into is four, for a <a href="https://www.jstor.org/stable/2689983">space-filling tetrahedron</a> of the same volume. A regular tetrahedron is not space-filling, has a Dehn invariant of rank one, and already has the minimum number of edges among anything you can dissect it into. In both cases the rank is unequal to the minimum number of edges. Also, the rank is different in these cases but the minimum number of edges is the same.</p>

<p>Nevertheless, I was hoping that the rank of the Dehn invariant would be usable as a constant-factor approximation to the minimum number of edges after dissection. To prove this, I’d need to find a polyhedron, having the same Dehn invariant as any given polyhedron, but with a number of edges proportional to the rank. The existence of a dissection would then follow from known results. But finding this few-edge polyhedron would have to use some knowledge of the starting polyhedron (unlike my set-of-rectangles construction), because not all tensors are realizable as Dehn invariants. So far, I haven’t been able to find any construction of these few-edge polyhedra.</p>

<p>So: does every polyhedron, of Dehn rank \(r\), have an equivalent polyhedron with \(O(r)\) edges? Or are there some polyhedra that cannot be dissected into another polyhedron with few edges, even though their Dehn invariants have low rank?</p>

<p>(<a href="https://mathstodon.xyz/@11011110/108524203085266737">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2022-06-22T18:22:00Z</updated>
    <published>2022-06-22T18:22:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2022-06-23T01:33:18Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2206.09048</id>
    <link href="http://arxiv.org/abs/2206.09048" rel="alternate" type="text/html"/>
    <title>ICLR 2022 Challenge for Computational Geometry and Topology: Design and Results</title>
    <feedworld_mtime>1655856000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Adele Myers, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Utpala:Saiteja.html">Saiteja Utpala</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Talbar:Shubham.html">Shubham Talbar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sanborn:Sophia.html">Sophia Sanborn</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shewmake:Christian.html">Christian Shewmake</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Donnat:Claire.html">Claire Donnat</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mathe:Johan.html">Johan Mathe</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lupo:Umberto.html">Umberto Lupo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sonthalia:Rishi.html">Rishi Sonthalia</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cui:Xinyue.html">Xinyue Cui</a>, Tom Szwagier, Arthur Pignet, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bergsson:Andri.html">Andri Bergsson</a>, Soren Hauberg, Dmitriy Nielsen, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sommer:Stefan.html">Stefan Sommer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Klindt:David.html">David Klindt</a>, Erik Hermansen, Melvin Vaupel, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dunn:Benjamin.html">Benjamin Dunn</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xiong:Jeffrey.html">Jeffrey Xiong</a>, Noga Aharony, Itsik Pe'er, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ambellan:Felix.html">Felix Ambellan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hanik:Martin.html">Martin Hanik</a>, Esfandiar Nava-Yazdani, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tycowicz:Christoph_von.html">Christoph von Tycowicz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Miolane:Nina.html">Nina Miolane</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2206.09048">PDF</a><br/><b>Abstract: </b>This paper presents the computational challenge on differential geometry and
topology that was hosted within the ICLR 2022 workshop ``Geometric and
Topological Representation Learning". The competition asked participants to
provide implementations of machine learning algorithms on manifolds that would
respect the API of the open-source software Geomstats (manifold part) and
Scikit-Learn (machine learning part) or PyTorch. The challenge attracted seven
teams in its two month duration. This paper describes the design of the
challenge and summarizes its main findings.
</p></div>
    </summary>
    <updated>2022-06-22T22:37:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2022-06-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2206.08996</id>
    <link href="http://arxiv.org/abs/2206.08996" rel="alternate" type="text/html"/>
    <title>Towards Consensus: Reducing Polarization by Perturbing Social Networks</title>
    <feedworld_mtime>1655856000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Miklos Z. Racz, Daniel E. Rigobon <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2206.08996">PDF</a><br/><b>Abstract: </b>This paper studies how a centralized planner can modify the structure of a
social or information network to reduce polarization. First, polarization is
found to be highly dependent on degree and structural properties of the
network. We then formulate the planner's problem under full information, and
motivate disagreement-seeking and coordinate descent heuristics. A novel
setting for the planner in which the population's innate opinions are
adversarially chosen is introduced, and shown to be equivalent to maximization
of the Laplacian's spectral gap. We prove bounds for the effectiveness of a
strategy that adds edges between vertices on opposite sides of the cut induced
by the spectral gap's eigenvector. Finally, these strategies are evaluated on
six real-world and synthetic networks. In several networks, we find that
polarization can be significantly reduced through the addition of a small
number of edges.
</p></div>
    </summary>
    <updated>2022-06-22T22:37:30Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2022-06-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-2675194666660547317</id>
    <link href="http://processalgebra.blogspot.com/feeds/2675194666660547317/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=2675194666660547317" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/2675194666660547317" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/2675194666660547317" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2022/06/interview-with-luca-de-alfaro-marco.html" rel="alternate" type="text/html"/>
    <title>Interview with Luca de Alfaro, Marco Faella, Thomas A. Henzinger, Rupak Majumdar and Mariëlle Stoelinga, CONCUR 2022 ToT Award Recipients</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In this instalment of the Process Algebra Diary, <a href="http://math.umons.ac.be/staff/Randour.Mickael/" target="_blank">Mickael Randour</a> and I joined forces to interview <a href="https://luca.dealfaro.com/" target="_blank">Luca de Alfaro</a>, <a href="http://wpage.unina.it/m.faella/" target="_blank">Marco Faella</a>, <a href="https://pub.ist.ac.at/~tah/" target="_blank">Thomas A. Henzinger</a>, <a href="https://people.mpi-sws.org/~rupak/" target="_blank">Rupak Majumdar</a> and <a href="https://wwwhome.ewi.utwente.nl/~marielle/" target="_blank">Mariëlle Stoelinga</a>, who are some of the recipients of the <a href="https://concur2022.mimuw.edu.pl/tot-award/" target="_blank">CONCUR 2022 Test-of-Time award</a>. We hope that you'll enjoy reading the very inspiring and insightful answers provided by the above-mentioned colleagues to our questions.  <br/></p><p>Note: In what follows, "Luca A." refers to me, whereas "Luca" is Luca de Alfaro. <br/></p><p><b>Luca A. and Mickael:</b> You receive the CONCUR ToT Award 2022 for your paper  "<a href="https://pub.ist.ac.at/~tah/Publications/the_element_of_surprise_in_timed_games.pdf)" target="_blank">The Element of Surprise in Timed Games</a>", which appeared at CONCUR 2003. In that article, you studied concurrent, two-player timed games. A key contribution of your paper is the definition of an elegant timed game model, allowing both the representation of moves that can take the opponent by surprise, as they are played “faster”, and the definition of natural concepts of winning conditions for the two players — ensuring that players can win only by playing according to a physically meaningful strategy. In our opinion, this is a great example of how novel concepts and definitions can advance a research field. Could you tell us more about the origin of your model?</p><p><br/><b>All: </b>Mariëlle and Marco were postdocs with Luca at UCSC in that period, Rupak was a student of Tom's, and we were all in close touch, meeting very often to work together.  We all had worked much on games, and an extension to timed games was natural for us to consider. </p><p><br/>In untimed games, players propose a move, and the moves jointly determine the next game state. In these games there is no notion of real-time.  We wanted to study games in which players could decide not only the moves, but also the instant in time when to play them.</p><p><br/>In timed automata, there is only one “player” (the automaton), which can take either a transition, or a time step.  The natural generalization would be a game in which players could propose either a move, or a time step.</p><p><br/>Yet, we were unsatisfied with this model. It seemed to us that it was different to say “Let me wait 14 seconds and reconvene.  Then, let me play my King of Spades” or “Let me play my King of Spades in 14 seconds”. In the first, by stopping after 14 seconds, the player is providing a warning that the card might be played. In the second, there is no such warning.  In other words, if players propose either a move or a time-step, they cannot take the adversary by surprise with a move at an unanticipated instant.  We wanted a model that could capture this element of surprise.</p><p><br/>To capture the element of surprise, we came up with a model in which players propose both a move and the delay with which it is played. After this natural insight, the difficulty was to find the appropriate winning condition, so that a player could not win by stopping time. </p><p><br/><b>Tom:</b> Besides the infinite state space (region construction etc.), a second issue that is specific to timed systems is the divergence of time. Technically, divergence is a built-in Büchi condition ("there are infinitely many clock ticks"), so all safety and reachability questions about timed systems are really co-Büchi and Büchi questions, respectively.  This observation had been part of my work on timed systems since the early 1990s, but it has particularly subtle consequences for timed games, where no player (and no collaboration of players) should have the power to prevent time from diverging.  This had to be kept in mind during the exploration of the modeling space.</p><p><br/><b>All:</b> We came up with many possible winning conditions, and for each we identified some undesirable property, except for the one that we published.  This is in fact an aspect that did not receive enough attention in the paper; we presented the chosen winning condition, but we did not discuss in full detail why several other conditions that might have seemed plausible did not work.</p><p><br/>In the process of analyzing the winning conditions, we came up with many interesting games, which form the basis of many results, such as the result on lack of determinazation, on the need for memory in reachability games (even when clock values are part of the state), and most famously as it gave the title to the paper, on the power of surprise.</p><p><br/>After this fun ride came the hard work, where we had to figure out how to solve these games. We had worked at symbolic approaches to games before, and we followed the approach here, but there were many complex technical adaptations required. When we look at the paper in the distance of time, it has this combination of a natural game model, but also of a fairly sophisticated solution algorithm.</p><p><br/><b>Luca A. and Mickael: </b>Did any of your subsequent research build explicitly on the results and the techniques you developed in your award-winning paper? If so, which of your subsequent results on (timed) games do you like best? Is there any result obtained by other researchers that builds on your work and that you like in particular or found surprising?</p><p><br/><b>Luca:</b> Marco and I built Ticc, which was meant to be a tool for timed interface theories, based largely on the insights in this paper.  The idea was to be able to check the compatibility of real-time systems, and automatically infer the requirements that enable two system components to work well together – to be compatible in time.  We thought this would be useful for hardware or embedded systems, and especially for control systems, and in fact the application is important: there is now much successful work on the compositionality of StateFlow/Simulink models.</p><p><br/>We used MTBDDs as the symbolic engine, and Marco and I invented a language for describing the components and we wrote by pair-programming some absolutely beautiful Ocaml code that compiled real-time component models into MTBDDs (perhaps the nicest code I have ever written). The problem was that we were too optimistic in our approach to state explosion, and we were never able to study any system of realistic size.</p><p><br/>After this, I became interested in games more in an economic setting, and from there I veered into incentive systems, and from there to reputation systems and to a three-year period in which I applied reputation systems in practice in industry, thus losing somewhat touch with formal methods work.</p><p><b>Mariëlle:</b> I have been working on games for test case generation: One player represents the tester, which chooses inputs to test; the other player represents the System-under-Test, and chooses the outputs of the system. Strategy synthesis algorithms can then compute strategies for the tester that maximize all kinds of objectives, eg reaching certain states, test coverage etc. </p><p><br/>A result that I really like is that we were able to show a very close correspondence between the existing testing frameworks and game theoretic frameworks: Specifications act as game arenas; test cases are exactly game strategies, and the conformance relation used in testing (namely ioco) coincides with game refinement (i.e. alternating refinement). </p><p><br/><b>Rupak:</b> In an interesting way, the first paper on games I read was the <a href="https://www-verimag.imag.fr/~sifakis/RECH/Synth-MalerPnueli.pdf" target="_blank">one by Maler, Pnueli and Sifakis (STACS 95)</a> that had both fixpoint algorithms and timed games (without “surprise”). So the problem of symbolic solutions to games and their applications in synthesis followed me throughout my career. I moved to finding controllers for games with more general (non-linear) dynamics, where we worked on abstraction techniques. We also realized some new ways to look at restricted classes of adversaries. I was always fortunate to have very good collaborators who kept my interest alive with new insights. Very recently, I have gotten interested in games from a more economic perspective, where players can try to signal each other or persuade each other about private information but it’s too early to tell where this will lead.</p><p><br/><b>Luca A. and Mickael:</b> What are the research topics that you find most interesting right now? Is there any specific problem in your current field of interest that you'd like to see solved?</p><p><br/><b>Mariëlle: </b>Throughout my academic life, I have been working on stochastic analysis --- with Luca and Marco, we worked on stochastic games a lot. First only on theory, but later also on industrial applications, esp in the railroad and high-tech domain. At some point in time, I realized that my work was actually centred around analysing failure probabilities and risk. That is how I moved into risk analysis; the official title of the title of the chair I hold is Risk Management for High Tech Systems. </p><p><br/>The nice thing is: this sells <i>much</i> better than Formal Methods! Almost nobody knows what Formal Methods are, and if they know, people think “yes, those difficult people who urge us to specify everything mathematically”. For risk management, this is completely different: everybody understands that this is an important area. <br/><br/><b>Luca: </b>I am currently working on computational ecology, on ML for networks, and on fairness in data and ML.  In computational ecology, we are working on the role of habitat and territory for species viability. We use ML techniques to write “differentiable algorithms”, where we can compute the effect of each input – such as the kind of vegetation in each square-kilometer of territory – on the output.  If all goes well, this will enable us to efficiently compute which regions should be prioritized for protection and habitat conservation.</p><p><br/>In networks, we have been able to show that reinforcement learning can yield tremendous throughput gains in wireless protocols, and we are now starting to work on routing and congestion control.</p><p><br/>And in fairness and ML, we have worked on the automatic detection of anomalous data subgroups (something that can be useful in model diagnostics), and we are now working on the spontaneous inception of discriminatory behavior in agent systems.</p><p><br/>While these do not really constitute a coherent research effort, I can certainly say that I am having a grand tour of CS – the kind of joy ride one can afford with tenure!</p><p><br/><b>Rupak: </b>I have veered between practical and theoretical problems. I am working on charting the decidability frontier for infinite-state model checking problems (most recently, for asynchronous programs and context-bounded reachability). I am also working on applying formal methods to the world of cyber-physical systems ---mostly games and synthesis. Finally, I have become very interested in applying formal methods to large scale industrial systems through a collaboration with Amazon Web Services. There is still a large gap between what is theoretically understood and what is practically applicable to these systems; and the problems are a mix of technical and social.</p><p><br/><b>Luca A. and Mickael:</b> You have a very strong track record in developing theoretical results and in applying them to real-life problems. In our, admittedly biased, opinion, your work exemplifies Ben Schneiderman's <a href="https://www.pnas.org/doi/pdf/10.1073/pnas.1802918115" target="_blank">Twin-Win Model</a>, which propounds the pursuit of "the dual goals of breakthrough theories in published papers and validated solutions that are ready for widespread dissemination." Could you say a few words on your research philosophy? How do you see the interplay between basic and applied research?</p><p><br/><b>Luca:</b> This is very kind for you to say, and a bit funny to hear, because certainly when I was young I had a particular talent for getting lost in useless theoretical problems.  </p><p><br/>I think two things played in my favor.  One is that I am curious.  The other is that I have a practical streak: I still love writing code and tinkering with “things”, from IoT to biology to web and more.  This tinkering was at the basis of many of the works I did.  My work on reputation systems started when I created a wiki on cooking; people were vandalizing it, and I started to think about game theory and incentives for collaboration, which led to my writing much of the code for Wikipedia analysis, and at Google, for Maps edits analysis.  My work on networks started with me tinkering with simple reinforcement-learning schemes that might work, and writing the actual code. On the flip side, my curiosity too often had the better of me, so that I have been unable to pay the continuous and devoted attention to a single research field.  I am not a specialist in any single thing I do or I have done.  I am always learning the ropes of something I don’t quite know yet how to do.</p><p><br/>My applied streak probably gave me some insight on which problems might be of more practical relevance, and my frequent field changes have allowed me to bring new perspectives to old problems.  There were not many people using RL for wireless networks, there are not many who write ML and GPU code and also avidly read about conservation biology.<br/><br/><b>Rupak:</b> I must say that Tom and Luca were very strong influencers for me in my research: both in problem selection and in appreciating the joy of research. I remember one comment of Tom, paraphrased as “Life is short. We should write papers that get read.” I spent countless hours in Luca’s office and learnt a lot of things about research, coffee, the ideal way to make pasta, and so on.<br/><br/><b>Luca A. and Mickael:</b> Several of you have high-profile leadership roles at your institutions. What advice would you give to a colleague who is about to take up the role of department chair, director of a research centre, dean or president  of a university? How can one build a strong research culture, stay research active and live to tell the tale?</p><p><br/><b>Luca:</b> My colleagues may have better advice; my productivity certainly decreased when I was department chair, and is lower even now that I am the vice-chair.  <br/>When I was young, I was ambitious enough to think that my scientific work would have the largest impact among the things I was doing.  But I soon realized that some of the greatest impact was on others: on my collaborators, on the students I advised, who went on to build great careers and stayed friends, and on all the students I was teaching.  This awareness serves to motivate and guide me in my administrative work. The CS department at UCSC is one of the ten largest in the number of students we graduate, and the time I spend on improving its organization and the quality of the education it delivers is surely very impactful.  My advice to colleagues is to consider their service not as an impediment to research, but as one of the most impactful things they do.</p><p><br/>My way of staying alive is to fence off some days that I only dedicate to research (aside from some unavoidable emergency), and also, to have collaborators that give me such joy in working together that they brighten and energize my whole day. </p><p><br/><b>Luca A. and Mickael:</b> Finally, what advice would you give to a young researcher who is keen to start working on topics related to concurrency theory today?<br/> </p><p><b>Luca: </b>Oh that sounds very interesting!  And, may I show you this very interesting thing we are doing in Jax to model bird dispersal? We feed in this climate and vegetation data, and then we…</p><p><br/>Just kidding.  Just kidding.  If I come to CONCUR I promise not to lead any of the concurrency yearlings astray.  At least I will try.</p><p><br/>My main advice would be this: work on principles that allow correct-by-design development.  If you look at programming languages and software engineering, the progress in software productivity has not happened because people have become better at writing and debugging code written in machine language or C. It has happened because of the development of languages and software principles that make it easier to build large systems that are correct by construction.<br/>We need the same kind of principles, (modeling) languages, and ideas to build correct concurrent systems.  Verification alone is not enough. Work on design tools, ideas to guide design, and design languages.</p><p><br/><b>Tom:</b> In concurrency theory we define formalisms and study their properties. Most papers do the studying, not the defining: they take a formalism that was defined previously, by themselves or by someone else, and study a property of that formalism, usually to answer a question that is inspired by some practical motivation. To me, this omits the most fun part of the exercise, the {\it defining} part. The point I am trying to make is not that we need more formalisms, but that, if one wishes to study a specific question, it is best to study the question on the simplest possible formalism that exhibits exactly the features that make the question meaningful. To do this, one often has to define that formalism. In other words, the formalism should follow the question, not the other way around. This principle has served me well again and again and led to formalisms such as timed games, which try to capture the essence needed to study the power of timing in strategic games played on graphs. So my advice to a young researcher in concurrency theory is: choose your formalism wisely and don't be afraid to define it.<br/> </p><p><b>Mariëlle:</b> As Michael Jordan puts it: <i>Just play. Have fun. Enjoy the game.</i><br/><br/><b>Rupak:</b> Problems have different measures. Some are practically justified (“Is this practically relevant in the near future?”) and some are justified by the foundations they build (“Does this avenue provide new insights and tools?”). Different communities place different values on the two. But both kinds of work are important and one should recognize that one set of values is not universally better than the other.<br/><br/></p></div>
    </content>
    <updated>2022-06-21T20:53:00Z</updated>
    <published>2022-06-21T20:53:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2022-06-21T20:54:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8397</id>
    <link href="https://windowsontheory.org/2022/06/21/teaching-circuits-as-the-first-computational-model/" rel="alternate" type="text/html"/>
    <title>Teaching circuits as the first computational model</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This fall, I am once again teaching Harvard’s “Introduction to Theoretical Computer Science” course (CS 121). Like many “intro to TCS / intro to theory of computation” courses, Harvard’s course used to be taught with Sipser’s classic textbook. Sipser’s book is indeed, for better or worse, a classic. It is extremely well-written and students like … <a class="more-link" href="https://windowsontheory.org/2022/06/21/teaching-circuits-as-the-first-computational-model/">Continue reading <span class="screen-reader-text">Teaching circuits as the first computational model</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This fall, I am once again teaching Harvard’s “Introduction to Theoretical Computer Science” course (<a href="https://cs121.boazbarak.org/">CS 121</a>). Like many “intro to TCS / intro to theory of computation” courses,  Harvard’s course used to be taught with <a href="https://www.amazon.com/Introduction-Theory-Computation-Michael-Sipser/dp/113318779X">Sipser’s classic textbook</a>. Sipser’s book is indeed, for better or worse, a classic. It is extremely well-written and students like it very much. It has clear explanations,  plenty of examples and solved exercises, and a wealth of material on the web accumulated through decades of it being used in many courses. On the other hand, CS in general and theoretical CS in particular has changed a lot in the 25+ years since the book was written. In fact, the basic approach of starting with finite automata as the initial model of computation dates to the <a href="https://dl.acm.org/doi/pdf/10.5555/1096945">1969 book</a> of Hopcroft and Ullman.</p>



<p>One of my main goals in revising the theoretical CS course is to give students both rigorous foundations as well as a taste of modern topics. Some of these modern topics: </p>



<ul><li><strong>Cryptography</strong>: a topic that combines mathematical beauty, practical importance, and a demonstration that sometimes computational hardness can be a resource rather than a hindrance.</li><li><strong>Quantum computing: </strong>a topic that shows the interaction between TCS and physics, the fundamental nature of the “Church Turing hypothesis”, and how we can (as in crypto) take a “lemon” (inability of classical computers to simulate certain quantum processes) and use it to make “lemonade” (a computer with stronger power than classical computers).</li><li><strong>Randomized computation and derandomization: </strong>Randomization is now essential to so many areas of CS, and so it is important to both demonstrate its power, and also how we might use complexity to remove it.</li><li><strong>Machine learning and average-case complexity:</strong> Traditionally in an intro TCS course the focus is purely on worst-case complexity. This leads to a disconnect with modern applications of CS, and in particular machine learning. </li></ul>



<p>So, I ended up <a href="https://windowsontheory.org/2017/07/27/rethinking-the-intro-theory-course/">writing my own text</a> – <a href="https://introtcs.org/"><strong>Introduction to Theoretical Computer Science</strong></a>.  While at some point I hope to make it into a printed book, it will always be available freely online on <a href="https://introtcs.org/">https://introtcs.org/</a>. The markdown source for it is available on the repository <a href="https://github.com/boazbk/tcs">https://github.com/boazbk/tcs</a> .  I’ve benefitted greatly from feedback from both students and readers around the globe: at the time of writing, the project has 330 issues and 385 pull requests.</p>



<p>A central difference between the approach I take and the one of previous courses is that I start from <strong>Boolean circuits</strong> as the first model of computation. Boolean circuits are crucial to teach the topics above:</p>



<ul><li>Cryptography is much more natural with circuits rather than Turing machines as the model of computation. Statements such as “128 bits of security” make no sense in the asymptotic Turing machine formalism, but can be made precise with circuits.</li><li>The standard model for quantum computing is quantum circuits.</li><li>Derandomization is best described using the circuit model, and of course many results such as BPP in the Polynomial-Hierarchy are best shown using circuits and the class P/poly as an intermediate concept. </li><li>Circuits are a very natural fit for machine learning, and in particular Neural Networks are just a special type of circuit.</li></ul>



<p>Finally, while circuits are often considered an “advanced” topic, they have some advantages over automata as the initial model of computation:</p>



<ol><li><strong>Finite is always easier than infinite:</strong> Starting with circuits enables us to start the course talking about arguably the simplest object: finite functions. Writing down the truth table of a finite function, and showing that there is more than one circuit to compute the same function, also helps clarify the difference between <strong>specification</strong> of a function and its <strong>implementation</strong> by some algorithm, which is distinction that many students grapple with.</li><li><strong>Circuits are connected to actual hardware</strong>. An intro to TCS course is not a pure math course – we want to convey to students that are models are motivated by actual computing. Circuits make this connection much closer, and less artificial than automata or even Turing machines.</li><li><strong>Can show cool theorems early.</strong> If we start with automata, then the first theorems we show can often seem not well motivated to students. It takes some time to build the machinery to show the main theorem – equivalence of automata and regular expressions – and the proof of that theorem is rather technical. In contrast, with circuits we can show three important theorems rather quickly: <strong>(1)</strong> <em>every</em> finite function <img alt="f:\{0,1\}^n \rightarrow \{0,1\}" class="latex" src="https://s0.wp.com/latex.php?latex=f%3A%5C%7B0%2C1%5C%7D%5En+%5Crightarrow+%5C%7B0%2C1%5C%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> can be computed by some circuit of at most <img alt="\tilde{O}(2^n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BO%7D%282%5En%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> size, <strong>(2)</strong> <em>every</em> circuit of size <img alt="s" class="latex" src="https://s0.wp.com/latex.php?latex=s&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> can be represented as a labeled graph and hence (using adjacency list)  by a string of size <img alt="\tilde{O}(s)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde%7BO%7D%28s%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, and <strong>(3)</strong> using (2) and the fact that there are <img alt="2^{2^n}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7B2%5En%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> functions mapping <img alt="\{0,1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B0%2C1%5C%7D%5En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to <img alt="\{0,1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B0%2C1%5C%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, there <em>exist</em> some function <img alt="f:\{0,1\}^n \rightarrow \{0,1\}" class="latex" src="https://s0.wp.com/latex.php?latex=f%3A%5C%7B0%2C1%5C%7D%5En+%5Crightarrow+%5C%7B0%2C1%5C%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> that <em>requires</em> a circuit of <img alt="\tilde{\Omega}(2^n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctilde%7B%5COmega%7D%282%5En%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> gates.</li></ol>



<p>While the course is a theory course, and not about programming, one of my goals in the book and course was to connect it to programming. This is not just to motivate students and make them feel that the material is “practical” but also to better understand the theory itself. Notions such as NP-completeness reductions can be often confusing to students (which is why they get the direction wrong half the time). Implementing a reduction from 3SAT to independent set and seeing the end result make it much more concrete.</p>



<p/>


<div class="wp-block-image">
<figure class="aligncenter size-large"><a href="https://windowsontheory.files.wordpress.com/2022/06/image-1.png"><img alt="" class="wp-image-8409" src="https://windowsontheory.files.wordpress.com/2022/06/image-1.png?w=1024"/></a>3SAT to independent set reduction from the <a href="https://introtcs.org/public/lec_12_NP.html#the-independent-set-problem">chapter on NP reductions</a>.</figure></div>


<p>One way in which I wanted to use programming is to demonstrate to students how we can take a piece of Python code such as the code for adding two numbers given in their binary representation:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: python; gutter: false; title: ; notranslate">def add(A,B): 
      """Add two binary numbers, given as lists of bits"""
      Y = []
      carry = zero(A[0]) # initialize carry to 0
      for i in range(len(A)): # compute i-th digit of output
            y = xor(A[i],B[i],carry) # xor function
            carry = maj(A[i],B[i],carry) # majority function
            Y.append(y)
      Y.append(carry)
      return Y

</pre></div>


<p>And obtain the corresponding circuit:</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2022/06/image-2.png"><img alt="" class="wp-image-8413" src="https://windowsontheory.files.wordpress.com/2022/06/image-2.png?w=1007"/></a></figure>



<p>The code also uses the following one-linear helper functions</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: python; gutter: false; title: ; notranslate">def maj(a,b,c): return (a &amp; b) | (b&amp;c) | (a&amp;c)
def zero(a): return a &amp; ~a
def xor2(a,b): return (a &amp; ~b) | (~a &amp; b)
def xor(*L): return xor2(*L) if len(L)==2 else xor2(xor(*L[:-1]),L[-1])
</pre></div>


<p>If you think about it, the task corresponds to extracting the <strong>computational graph</strong> of a piece of Python code. This is precisely the same task that <em>auto-differentiation</em> packages such as <a href="https://pytorch.org/">Pytorch</a> need to do. Hence it can be solved in a similar way. Thus, inspired by Karpathy’s <a href="https://github.com/karpathy/micrograd">micro-grad</a> package (see my <a href="https://windowsontheory.org/2020/11/03/yet-another-backpropagation-tutorial/">back-propagation tutorial</a>) and using the awesome <a href="https://schemdraw.readthedocs.io/en/latest/">SchemDraw</a> package, I wrote a short <a href="https://colab.research.google.com/drive/1Hv6LVrEZDJ5s5NgR1bZPbCkz5NLIgX8d#scrollTo=Y-XFrGdO069l"><strong>colab notebook</strong></a> that does precisely that.</p>



<p>Specifically, the notebook defines a <code>Bit</code> class that (as its name suggests) stores a single bit. The class defines the logical AND, OR and NOT operations ( <code>&amp;</code>, <code>|</code> , <code>~</code> in Python). If a and b are two bits then <code>c = a &amp; b</code> not just contains the value which is the AND of the values of <code>a</code> and <code>b</code>, but also pointers to a and b and remembers how it was computed from them.  This allows  us to obtain from <code>c</code> a formula/circuit expressing it in terms of <code>a</code> and <code>b</code>.</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: python; gutter: false; title: ; notranslate">class Bit:
  counter  = 0
  def __init__(self,val=0, label="-"): 
    self.label = label
    self.data = val
    self.children = []
  
  def op(self,f, label, *others):
    inputs = [self.data] + [o.data for o in others]
    out = Bit(f(*inputs),label)
    out.children = [self] + list(others)
    return out

  def __and__(self,other): return self.op(lambda a,b: a &amp; b, "\\wedge", other)
  def __or__(self,other): return self.op(lambda a,b: a | b, "\\vee", other)
  def __invert__(self): return self.op(lambda a: ~a, "\\neg")
</pre></div>


<p>Now we can write a simple recursive function <code>formula</code> (see the <a href="https://colab.research.google.com/drive/1Hv6LVrEZDJ5s5NgR1bZPbCkz5NLIgX8d#scrollTo=g7ykkoEbH6NJ">notebook</a>) to give out the latex of the formula corresponding to how a particular bit was computed. So if we write </p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: python; gutter: false; title: ; notranslate">from IPython.display import Markdown, display, Math
Y = xor(Bit(0,"X_0"), Bit(1,"X_1"))
Math(formula(Y))
</pre></div>


<p>Then we will get</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2022/06/image-3.png"><img alt="" class="wp-image-8421" src="https://windowsontheory.files.wordpress.com/2022/06/image-3.png?w=283"/></a></figure>



<p>The code of a recursive function that transforms this into the circuit </p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2022/06/image-4.png"><img alt="" class="wp-image-8423" src="https://windowsontheory.files.wordpress.com/2022/06/image-4.png?w=659"/></a>is only slightly more complicated. The code to draw the addition circuit above is the following:</figure>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: python; gutter: false; title: ; notranslate">A = [Bit(0,f"A_{i}") for i in range(2)]
B = [Bit(0,f"B_{i}") for i in range(2)]
draw_circ(*add(A,B))
</pre></div>


<p>See the <a href="https://colab.research.google.com/drive/1Hv6LVrEZDJ5s5NgR1bZPbCkz5NLIgX8d#scrollTo=hHqMEfe9ZUY6">colab notebook</a> for more.</p></div>
    </content>
    <updated>2022-06-21T16:55:24Z</updated>
    <published>2022-06-21T16:55:24Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2022-06-23T03:37:59Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2022-06-21-sandglass/</id>
    <link href="https://decentralizedthoughts.github.io/2022-06-21-sandglass/" rel="alternate" type="text/html"/>
    <title>Safe Permissionless Consensus</title>
    <summary>Nakamoto’s consensus protocol works in a permissionless model, where nodes can join and leave without notice. However, it guarantees agreement only probabilistically. Is this weaker guarantee a necessary concession to the severe demands of supporting a permissionless model? We show that, at least in a benign failure model, it is...</summary>
    <updated>2022-06-21T12:00:00Z</updated>
    <published>2022-06-21T12:00:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2022-06-22T22:42:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8378</id>
    <link href="https://windowsontheory.org/2022/06/20/the-uneasy-relationship-between-deep-learning-and-classical-statistics/" rel="alternate" type="text/html"/>
    <title>The uneasy relationship between deep learning and (classical) statistics</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">An often-expressed sentiment is that deep learning (and machine learning in general) is “simply statistics,” in the sense that it uses different words to describe the same concepts statisticians have been studying for decades. In the 1990s, Rob Tibshirani wrote the following tongue-in-cheek “glossary”:: Something about this table resonates with me.  In fact, as anyone … <a class="more-link" href="https://windowsontheory.org/2022/06/20/the-uneasy-relationship-between-deep-learning-and-classical-statistics/">Continue reading <span class="screen-reader-text">The uneasy relationship between deep learning and (classical) statistics</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>An often-expressed sentiment is that deep learning (and machine learning in general) is “simply statistics,” in the sense that it uses different words to describe the same concepts statisticians have been studying for decades. In the 1990s, Rob Tibshirani wrote the following tongue-in-cheek “glossary”::</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img alt="" height="300" src="https://lh3.googleusercontent.com/RauK9YOaLdTiw42nyAQHRuuV9nLfa-zKY49NgpOX1z0pDk0QSe77LQLtZHtAt575adehcZjv7Jbkg8FvXqLklGvprfBRzYaNSk-Q7EtIuV0Kda-5fgCqXLC1xCZuPWzeJAxIbQMCAj6mKMAfeA" width="442"/></figure></div>


<p>Something about this table resonates with me.  In fact, as anyone using Pytorch knows, since Tibshiriani posted this table, many of the terms on the right have found broader use in the machine learning community. (And I do hope that statisticians’ grants and conferences have improved as well…)</p>



<p>But thinking of deep learning purely in terms of statistics misses crucial aspects of its success. A better critique of deep learning is that <strong>it uses statistical terms to describe radically different concepts</strong>. In meme form, it is the “Princess Bride” meme on the right that is a better critique of deep learning than <a href="https://www.instagram.com/sandserifcomics/">sandserif</a>’s meme on the left. </p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img alt="" height="-107" src="https://lh6.googleusercontent.com/NNnn3c1UfSLSUSfQk6TdrbBHUarDnY9UKgEEUJ0DtEOi0LyREmCmMyIWbxmybd6uRD23Is5GHYeI5caxqKF3QQycm9tbiEuufcwzwDz8dmjWUIoBpG9v_S1pEIMStfoHkp64McsybsF_rLwxsg" width="-230"/><strong>Figure:</strong> I claim that the right critique of deep learning is not that it uses different words to describe old statistical terms, but rather that it uses these terms to describe a radically different process.</figure></div>


<p/>



<p><strong>This blog post: organization. </strong>In this post, I explain this point of view and why some of the most fundamental aspects of deep learning deviate radically from statistics and even from classical machine learning. In this somewhat long post, I’ll start by talking about the difference between <strong>explanation</strong> and <strong>prediction</strong> when fitting models to data. I’ll then discuss two “cartoons” of a learning process: <strong>fitting a statistical model</strong> using empirical risk minimization and <strong>teaching a math skill to a (human) student</strong>. I then discuss which one of those processes is a closer match to deep learning. Spoiler: while the math and code of deep learning is nearly identical to the first scenario (fitting a statistical model), I claim that a deeper level, some of deep learning’s most aspects are captured by the “teaching a skill to a student” scenario. I do not claim to have a full theory for deep learning. In fact,I strongly suspect such a theory doesn’t exist. Rather, I believe different aspects of deep learning are best understood from different lenses, and the statistical lens cannot provide the complete picture.</p>



<p><em>Caveat:</em><strong> </strong> While I contrast deep learning with statistics in this post, I refer to “classical statistics” as it was studied in the past and explained in textbooks. Many statisticians are studying deep learning and going beyond classical methods, analogously to how physicists in the 20th century needed to expand the framework of classical physics. Indeed, the blurring of the lines between computer scientists and statisticians is a modern (and very welcome!) phenomenon that benefits us all. </p>



<h2><strong>1) Predictions vs. explanations in model fitting.</strong></h2>



<p>Scientists have fitted models to observations for thousands of years. For example, as mentioned in my <a href="https://windowsontheory.org/2022/05/03/philosophy-of-science-and-the-blockchain-a-book-review/">philosophy of science book review post</a>, the Egyptian astronomer Ptolemy came up with an ingenious model for the movement of the planets. Ptolemy’s model was geocentric (with planets rotating around the earth) but had a sequence of “knobs” (concretely, epicycles) that gave it excellent predictive accuracy. In contrast, Copernicus’ initial <em>heliocentric</em> model posited a circular orbit of planets around the sun. It was a simpler model than Ptolemy’s (with fewer “adjustable knobs”) and got the big picture right, but was <em>less accurate</em> in predicting observations. (Copernius later added his own epicycles so he could match Ptolemy’s performance.)</p>



<p>Ptolemy’s and Copernicus’ models were incomparable. If you needed a “black box” for <strong>predictions</strong>, then Ptolemy’s geocentric model was superior. If you wanted a simple model into which you can “peer inside” and that could be the starting point for a theory to <strong>explain</strong> the movements of the stars, then Copernicus’ model was better. Indeed, eventually, Kepler refined Copernicus’ models to elliptical orbits and came up with his three laws of planetary movements, which enabled Newton to explain them using the same laws of gravity that apply here on earth. For that, it was crucial that the heliocentric model wasn’t simply a “black box” that provides predictions, but rather was given by simple mathematical equations with few “moving parts.” Over the years, astronomy continued to be an inspiration for developing statistical techniques. Gauss and Legendre (independently) <a href="https://www.jstor.org/stable/2240811">invented least-squares regression</a> around 1800 to predict the orbits of asteroids and other celestial bodies. Cauchy’s <a href="https://www.math.uni-bielefeld.de/documenta/vol-ismp/40_lemarechal-claude.pdf">1847 invention of gradient descent</a> was also motivated by astronomical predictions.</p>



<p>In physics, you can (at least sometimes) “have it all” – find the “right” theory that achieves the best predictive accuracy and the best explanation for the data. This is captured by sentiments such as Occam’s Razor, which can be thought of as positing that simplicity, predictive power, and explanatory insights, are all aligned with one another. However, in many other fields, there is a tension between the twin goals of <strong>explanation</strong> (or, more generally, <strong>insight</strong>) and <strong>prediction</strong>. If you simply want to predict observations, then a “black box” could very well be best. On the other hand, if you want to extract insights such as a causal model, general principles, or significant features, then a simpler model that you can understand and interpret might be better. The right choice of model depends on its usage. Consider, for example, a dataset containing genetic expressions and a phenotype (say some disease) for many individuals. If your goal is to predict<strong> </strong>the chances of an individual getting sick, you want to use the best model for that task, regardless of how complex it is or how many genes it depends on. In contrast, if your goal is to identify a few genes for further investigation in a wet lab, a complicated black box would be of limited use, even if it’s highly accurate.</p>



<p>This point was forcefully made in <a href="https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.full">Leo Breiman’s famous 2001 essay on the two cultures of statistical modeling</a>. The “data modeling culture” focuses on simple generative models that <strong>explain</strong> the data. In contrast, the “algorithmic modeling culture” is agnostic on how the data is generated and focuses on finding models that <strong>predict </strong>the data, no matter how complex. Breiman argued that statistics was too dominated by the first culture, and this focus has <em>“</em><em>led to irrelevant theory and questionable scientific conclusions”</em> and <em>“prevented statisticians from working on exciting new problems.”</em></p>



<p>Breiman’s paper was controversial, to say the least. Brad Efron responded to it by saying that, while he agreed with some points, <em>“at first glance, Leo Breiman’s stimulating paper looks like an argument against parsimony and scientific insight, and in favor of black boxes with lots of knobs to twiddle. At second glance, it still looks that way”</em> (see also <a href="https://www.stat.cmu.edu/~kass/papers/KassOnBreiman.pdf">Kass</a>). In a <a href="https://www.fox.temple.edu/wp-content/uploads/2021/07/Efron-2020-JASA-wdiscussion.pdf">more recent piece</a>, Efron graciously concedes that <em>“Breiman turned out to be more prescient than me: pure prediction algorithms have seized the statistical limelight in the twenty-first century, developing much along the lines Leo suggested.”</em></p>



<h2><strong>2) Classical and modern predictive models.</strong></h2>



<p>Machine learning, deep or not, stands firmly in Breiman’s second culture, with a focus on <strong>prediction</strong>. This culture has a long history.  For example, the following snippets from <a href="https://archive.org/details/patternclassific0000duda">Duda and Hart’s 1973 textbook</a> and <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/j.1538-7305.1962.tb02426.x">Highleyman’s 1962 paper</a> would be very recognizable to deep learning practitioners today:</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img alt="" height="305" src="https://lh6.googleusercontent.com/i79e0Q5uv1DWLfOVoy0MzuJMRCLpZEQQs44mlvyFBCYkYsyXqZByeg1XYsiZi-Jg7jxBcN_ZvDuNJxjgxuXN-qiDLWNSmAwDj-HsLvZHq_25jd5YgJW2g3DviYmwgHXrq9VLcxnKCDFI1Ww5hQ" width="671"/></figure></div>


<p>Similarly, Highleyman’s handwritten characters dataset and the architecture <a href="https://ieeexplore.ieee.org/document/5219431">Chow (1962)</a>  used to fit it  (with ~58% accuracy) would also strike a chord with modern readers, see the <a href="https://mlstory.org/data.html">Hardt-Recht book</a> and their <a href="http://www.argmin.net/2021/10/20/highleyman/">blog post</a>.  </p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img alt="" height="297" src="https://lh6.googleusercontent.com/lHZ5yqJuR2K2_gfrAfwTkICu-ogBqYcdPyStrnLvO81ED0WxSbS5ZlObtTMy8WeNvfV7IbS2hpOz9ondLDAitLr77FT-apK4TG2jEEiiaajmGMtydWMYhMux6_pqh0M6nin7oo2LM6g700rPWQ" width="638"/></figure></div>


<h2><strong>3) Why deep learning is different.</strong></h2>



<p>In 1992, <a href="https://www.dam.brown.edu/people/documents/bias-variance.pdf">Geman, Bienenstock, and Doursat</a> wrote a pessimistic article about neural networks, arguing that <em>“current-generation feed-forward neural networks are largely inadequate for difficult problems in machine perception and machine learning”</em>. Specifically, they believed that general-purpose neural networks would not be successful in tackling difficult tasks, and the only way for them to succeed would be via hand-designed features. In their words: <em>“important properties must be built-in or “hard-wired” … not learned in any statistically meaningful way.”</em> In hindsight (which is always 20/20), Geman et al. were completely wrong (if anything, modern architectures such as transformers are even <em>more general</em> than the convolutional networks that existed at the time), but it is interesting to understand <em>why</em> they were wrong. </p>



<p>I believe that the reason is that deep learning is genuinely different from other learning methods. A priori, it seems that deep learning is just one more predictive model, like nearest neighbors or random forests. It may have more “knobs,” but that seems to be a quantitative rather than qualitative difference. However, <a href="https://cse-robotics.engr.tamu.edu/dshell/cs689/papers/anderson72more_is_different.pdf">in the words of P.W. Andreson</a>, <strong>“more is different.”</strong>    In Physics, we often need a completely different theory once scale changes by several orders of magnitude, and the same holds in deep learning. The processes that underlie deep learning vs. classical models (parametric or not) are radically different, even if the equations (and Python code) look identical at a high level. </p>



<p>To clarify this point, let’s consider two very different learning processes:<strong> fitting a statistical model</strong> and <strong>teaching math to a student</strong>.</p>



<h3><strong>Scenario A: Fitting a statistical model</strong></h3>



<p>Classically, fitting a statistical model to data corresponds to the following:</p>



<ol><li>We observe some data <img alt="X,y" class="latex" src="https://s0.wp.com/latex.php?latex=X%2Cy&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. (Think of  <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as an <img alt="n\times p" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Ctimes+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> matrix and <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as an <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> dimensional vector; think of the data as coming from a <strong>structure and noise</strong> model: each coordinate <img alt="y_i" class="latex" src="https://s0.wp.com/latex.php?latex=y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is obtained as <img alt="f_0(x_i)+e_i" class="latex" src="https://s0.wp.com/latex.php?latex=f_0%28x_i%29%2Be_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> where <img alt="e_i" class="latex" src="https://s0.wp.com/latex.php?latex=e_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the corresponding noise , using additive noise for simplicity, and <img alt="f_0" class="latex" src="https://s0.wp.com/latex.php?latex=f_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as the “ground truth.”)</li><li>We fit a model <img alt="\hat{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to the data by running some <strong>optimization algorithm</strong> to minimize an <strong>empirical risk</strong> of <img alt="\hat{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. That is, we use optimization to (try to) find <img alt="\hat{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> that minimizes a quantity <img alt="L(\hat{f}) + R(\hat{f})" class="latex" src="https://s0.wp.com/latex.php?latex=L%28%5Chat%7Bf%7D%29+%2B+R%28%5Chat%7Bf%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> where <img alt="L(\cdot)" class="latex" src="https://s0.wp.com/latex.php?latex=L%28%5Ccdot%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a loss term (capturing how close <img alt="\hat{f}(X)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D%28X%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is to <img alt="y" class="latex" src="https://s0.wp.com/latex.php?latex=y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>) and <img alt="R(\hat{f})" class="latex" src="https://s0.wp.com/latex.php?latex=R%28%5Chat%7Bf%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is an optional regularization term (attempting to bias <img alt="\hat{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> toward simpler models under some measure). </li><li>Our hope is that our model will have good <strong>population loss</strong>, in the sense that the <strong>generalization error/loss</strong>  <img alt="\mathbb{E} [L(\hat{f}(x),y)]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D+%5BL%28%5Chat%7Bf%7D%28x%29%2Cy%29%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is small (where this expectation is taken over the total population from which our data was drawn).</li></ol>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img alt="" height="-135" src="https://lh5.googleusercontent.com/j84eNgmOsBGbSDI3nbHRLMmKYP-fAqSSfkjX6eZOlndHm00xibwbMEfLcW4Yw58JpZK9bgzc1gFvTzsZsUPVC4r1pnpB_koZv1kBpn_L4dHQBWuVk5cDwwoAZj-nVlTRu1twjUh25ihRTDcglQ" width="-290"/><strong>Figure:</strong> <a href="https://www.fox.temple.edu/wp-content/uploads/2021/07/Efron-2020-JASA-wdiscussion.pdf">Effron’s</a> cartoon of recovering Newton’s first law from noisy observations.</figure></div>


<p>This very general paradigm captures many settings, including least-squares linear regression, nearest neighbors,  neural network training, and more. In the classical statistical setup, we expect to observe the following: </p>



<p><strong>Bias/variance tradeoff:</strong> Let <img alt="\mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> be the set of models that we optimize over. (If we are in the non-convex setting and/or have a regularizer term, we can let <img alt="\mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> be the set of such models that can be achieved by the algorithm with non-negligible probability, taking the effects of algorithm choice and regularizer into account.)  The <strong>bias</strong> of <img alt="\mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the best approximation to the ground truth that can be achieved by an element <img alt="\hat{f} \in \mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7Bf%7D+%5Cin+%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. The larger the class <img alt="\mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the smaller the bias, and it can be zero if <img alt="f_0 \in \mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=f_0+%5Cin+%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. However, the larger the class <img alt="\mathcal{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BF%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the more samples we would need to narrow down its members and hence the more <strong>variance</strong> in the model that the algorithm outputs. The overall <strong>generalization error</strong> is the sum of the bias term and the contribution from the variance. Hence statistical learning typically displays a <strong>bias/variance tradeoff,</strong> with a “goldilocks choice” of the right model complexity that minimizes the overall error.  Indeed, Geman et al. justified their pessimism on neural networks by saying that <em>“the fundamental limitations resulting from the bias-variance dilemma apply to all nonparametric inference models, including neural networks.” </em></p>



<p><strong>More is not always better.</strong> In statistical learning, getting more features or data does not necessarily improve performance. For example, learning from data that contains many irrelevant features is more challenging. Similarly, learning from a mixture model, in which data comes from one of two distributions (e.g., <img alt="y=f_0(x)" class="latex" src="https://s0.wp.com/latex.php?latex=y%3Df_0%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="y=f_1(x)" class="latex" src="https://s0.wp.com/latex.php?latex=y%3Df_1%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>), is harder than learning each distribution independently. </p>



<p><strong>Diminishing returns.</strong> In many settings, the number of data points needed to reduce the prediction noise to a level of <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> scales as <img alt="k/\epsilon^2" class="latex" src="https://s0.wp.com/latex.php?latex=k%2F%5Cepsilon%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for some parameter <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. In such cases, it takes about <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> samples to “get off the ground” but once we do so we face a regime of diminishing returns, whereby if it took <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> points to achieve (say) 90% accuracy, it will take roughly an additional <img alt="3n" class="latex" src="https://s0.wp.com/latex.php?latex=3n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> points to increase accuracy to 95%. In general, as we increase our resources (whether data, model complexity, or computation) we expect to capture finer and finer distinctions rather than unlocking qualitatively new capabilities.</p>



<p><strong>Strong dependence on loss, data.</strong> When fitting a model to high-dimensional data, small details can make a big difference. Statisticians know that choices such as an L1 or L2 regularizer matter, not to mention using completely different datasets (e.g., Wikipedia vs. Reddit). High-dimensional optimizers of different quantities will be very different from one another.</p>



<p><strong>No natural “difficulty” of data points (at least in some settings)</strong>. Traditionally, we think of data points as sampled independently from some distribution. Though points closer to the decision boundary could be harder to classify, given the concentration-of-measure phenomena in high dimensions, we expect that most points would be of similar distance. Thus at least in classical data distributions, we don’t expect points to vary greatly in their difficulty level. However, mixture models can display such variance in difficulty level, and hence, unlike the other issues above, such variance would not be terribly surprising in the statistical setting.</p>



<h3><strong>Scenario B: Learning math</strong></h3>



<p>In contrast to the above, consider the setting of teaching a student some particular topic in mathematics (e.g., computing derivatives), by giving them general instructions, as well as exercises to work through. This is not a formally defined setting, but let’s consider some of its qualitative features:</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img alt="" height="360" src="https://lh5.googleusercontent.com/FLZEzRfYsdYYku-FxQlt8r6oc9PAAXqaoGo5-R2ABxKoGB1hxUvJWiSYxt1weyqTJGujoGh1YhIAoE_2RuM-nzzSicDlpjDSFjAXeedX2schUZEb3VB7dWQirp4n1bt90Qz3_K5SlYHeg9zJ6w" width="567"/><strong>Figure: </strong>An exercise to learn a particular math skill from the <a href="https://www.ixl.com/math/calculus/find-derivatives-using-the-product-rule">IXL website</a>.</figure></div>


<p/>



<p><strong>Learning a skill, rather than approximating a distribution.</strong> In this setting, the student learns a <em>skill</em> rather than an estimator/predictor for some quantity. While defining “skill” is not a trivial task (and not one we’ll undertake in this blog post), it is a qualitatively different object. In particular, even if the function mapping exercises to solutions cannot be used as a “black box” to solve some related task X, we believe that the <strong>internal representations</strong> that the student develops while working through these problems can still be useful for X.</p>



<p><strong>More is better.</strong> Generally, students that do more problems and problems of different types achieve better performance. A “mixture model” – doing some calculus problems and some algebra problems – does not hurt the student in their calculus performance and in fact, could only help.</p>



<p><strong>“Grokking” or unlocking capabilities, moving to automatic representations.</strong> While at some point there are diminishing returns also when solving problems, students do seem to undergo several phases. There is a stage in which doing some problems helps a concept “click” and unlocks new capabilities. Also, as students repeat problems of a specific type, they seem to move their facilities and representations of these problems to a lower level, enabling certain automaticity with them that they didn’t have before.</p>



<p><strong>Performance is partially independent of the loss and data.</strong> There is more than one way to teach mathematical concepts. Students who study with different books, educational approaches, or grading systems can eventually learn the same material and (as far as we can tell) similar internal representations of it.</p>



<p><strong>Some problems are harder than others.</strong> In math exercises, we often see a strong correlation between how different students solve the same problem. There does seem to be an inherent difficulty level for a problem and a natural progression of difficulty that is optimal for learning. Indeed this is precisely what is being done by platforms such as <a href="https://www.ixl.com/">IXL</a>.</p>



<h2><strong>4) Is deep learning more like statistical estimation or a student learning a skill?</strong></h2>



<p>So, which of the above two metaphors more appropriately captures modern deep learning, and specifically the reasons why it is so successful? Statistical model fitting seems to correspond well to the math and the code. Indeed the canonical Pytorch training loop trains deep networks through empirical risk minimization as described above:</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img alt="" height="182" src="https://lh4.googleusercontent.com/yf9rz1IV1Ac1LvzQPzh8rY3GLZpecxUtOjm-ITQRuWHb3vrPjDAd5BLUTitGPWlYAFtVR8Ou7a2JnsMIld33hPoe2QKeNXsQLySJIGbNgMCdvx0Vcr7No6cBHXxDClY1xvYNN6iBUJbX6bhEzA" width="402"/></figure></div>


<p>However, on a deeper level, the relation between the two settings is not as clear. For concreteness, let us fix a particular learning task. Consider a classification algorithm that is trained using the method of “self-supervised learning + a linear probe” (what we called Self-Supervised + Simple or SSS in <a href="https://windowsontheory.org/2020/10/18/understanding-generalization-requires-rethinking-deep-learning/">our paper with Bansal and Kaplun</a>). Concretely, the algorithm is trained as follows:</p>



<ol><li>Suppose that the data is a sequence <img alt="\{ (x_i,y_i) \}_{i=1..n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B+%28x_i%2Cy_i%29+%5C%7D_%7Bi%3D1..n%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> where <img alt="x_i \in \mathbb{R}^p " class="latex" src="https://s0.wp.com/latex.php?latex=x_i+%5Cin+%5Cmathbb%7BR%7D%5Ep+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is some datapoint (say an image for concreteness) and <img alt="y_i" class="latex" src="https://s0.wp.com/latex.php?latex=y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a label.</li><li>We first find a deep neural network implementing <em>representation function</em> <img alt="\hat{r}:\mathbb{R}^p \rightarrow \mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7Br%7D%3A%5Cmathbb%7BR%7D%5Ep+%5Crightarrow+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. This function is trained only using the datapoints <img alt="\{ x_1,\ldots, x_n \}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B+x_1%2C%5Cldots%2C+x_n+%5C%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and not using the labels by minimizing some type of a self-supervised loss function. Example of such loss functions are <em>reconstruction</em> or in-painting (recovering some part of the input <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> from another) or <em>contrastive learning</em> (finding <img alt="\hat{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7Br%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> such that <img alt="\parallel \hat{r}(x)-\hat{r}(x') \parallel" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cparallel+%5Chat%7Br%7D%28x%29-%5Chat%7Br%7D%28x%27%29+%5Cparallel&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is significantly smaller when <img alt="x,x'" class="latex" src="https://s0.wp.com/latex.php?latex=x%2Cx%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are augmentations of the same datapoint than when they are two random points).  </li><li>We then use the full labeled data <img alt="\{ (x_i,y_i )" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B+%28x_i%2Cy_i+%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to fit a linear classifier <img alt="\hat{\ell}:\mathbb{R}^d \to \mathbb{R}^c" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7B%5Cell%7D%3A%5Cmathbb%7BR%7D%5Ed+%5Cto+%5Cmathbb%7BR%7D%5Ec&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (where <img alt="c" class="latex" src="https://s0.wp.com/latex.php?latex=c&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the number of classes) that minimizes the cross-entropy loss. Our final classifier is the map <img alt="x \mapsto \mathrm{arg}\max \hat{\ell}(\hat{r}(x))" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cmapsto+%5Cmathrm%7Barg%7D%5Cmax+%5Chat%7B%5Cell%7D%28%5Chat%7Br%7D%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</li></ol>



<p>Step 3 merely fits a linear classifier and so the “magic” happens in step 2 (self-supervised learning of a deep network). Some of the properties we see in self-supervised learning include:</p>



<p><strong>Learning a skill rather than approximating a function. </strong>Self-supervised learning is not about approximating a function but rather learning representations that could be used in a variety of downstream tasks. For example, this is the dominant paradigm in natural language processing. Whether the downstream task is obtained through linear probe, fine tuning, or prompting is of secondary importance.</p>



<p><strong>More is better.</strong> In self-supervised learning, representation quality improves with data quantity. We don’t suffer from mixing in several sources: in fact, the more diverse the data is, the better. </p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img alt="" height="246" src="https://lh3.googleusercontent.com/9bjInHmiYVAFGBmtD-I50K7UwTOCRvSa69OjdwwuezlkPjEZS-1_7gwfzS3_-MuLOwLjm_swX_D02L95Do8brDTQJe7TFmfijsykp4GhNvR4i0wx9AABlqKKTS6VrJaus6VsRUPbVL-76cJjGg" width="602"/><strong>Figure: </strong>Dataset for the <a href="https://arxiv.org/abs/2204.02311">Google PaLM model</a>.</figure></div>


<p/>



<p><strong>Unlocking capabilities.</strong> We have seen time and again discontinuous improvements in deep learning models as we scale resources (data, compute, model size). This has also <a href="https://arxiv.org/abs/2201.02177">been demonstrated</a> in some synthetic settings. </p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img alt="" height="389" src="https://lh5.googleusercontent.com/AkKwPPjyXXrmQ10_5yrAfhzhCjAeO9oWbQD2cXCgEarxyuSdb5dlZg6lSKFs0IJQ63njmlyDp14X-gRFaQgCQ0RLaBY8o5YgXl9-reba1XzOFJxn_QPSoaCgE4DlXFA8xfCyVWm7bXuWL5BMPg" width="723"/><strong>Figure: </strong>The <a href="https://arxiv.org/abs/2204.02311">PaLM model</a> displays some discontinuous improvements in some benchmarks as model size increases (with caveat of only three sizes in these plots), with some surprising capabilities unlocked such as explaining jokes.</figure></div>


<p/>



<p><strong>Performance is largely independent of loss or data</strong>. There is more than one self-supervised loss. Several contrastive and reconstruction losses have been used for images. For language models, we sometimes use one-sided reconstruction (predict next token) and sometimes masked models whose goal is to predict a masked input from both the left and right token. We can also use slightly different datasets. These can make differences in efficiency, but as long as we make “reasonable” choices, typically raw resources are more significant predictors of performance than the particular loss or dataset used.</p>



<p><strong>Some instances are harder than others.</strong> This point is not specific to self-supervised learners. It does seem that data points have some inherent “difficulty level”. Indeed, we have several pieces of empirical evidence for the notion that different learning algorithms have a different “skill level” and different points have a different “difficulty level” (with the probability of classifier <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> classifying point <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> correctly being monotonically increasing with <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>’s skill and monotonically decreasing with <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>’s difficulty). The “skill vs. difficulty” paradigm is the cleanest explanation for the “accuracy on the line” phenomenon uncovered by <a href="https://arxiv.org/abs/1902.10811">Recht et al</a> and  <a href="https://arxiv.org/abs/2107.04649">Miller et al</a>. Our paper with Kaplun, Ghosh, Garg, and Nakkiran also shows how different inputs in datasets have an inherent “difficulty profile” that seems to be generally robust with respect to different model families.</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img alt="" height="372" src="https://lh5.googleusercontent.com/vqgciucsObFfbagqYE_AMDW-IFcVaRIThslLnoR5QpXEeb4q24IHMPitPGjRaIKsijNECy3el8e00QI6xh6STAA7LbNeAfmU16deO59P6Lq4hvNUS1aSRduynVxcF9r2bIGS4rZOMq5ZAmCqAA" width="591"/><strong>Figure: </strong><a href="https://share.streamlit.io/millerjohnp/linearfits_app/main/app.py">Miller et al’s graph</a> showing accuracy on the line phenomena for classifiers trained on CIFAR-10 and tested on CINIC-10.</figure></div>


<p/>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img alt="" height="391" src="https://lh6.googleusercontent.com/cNFVOxR5avX184hU0mFY3_WTqfZhyD7n0Jqg7mcn36PkZk_835mx8fdXOMJTW5xt10EwnBblx480XdRBEcSRq5RSpPE6rYAr7R9PrBx8VCT54FAixyg3bXgxueRLWWdMKteLgiuAkuXrAcs5Eg" width="680"/><strong>Figure: </strong>Deconstruction of datasets into points from <a href="https://arxiv.org/abs/2202.09931">Kaplun and Ghosh et al</a> of different “difficulty profiles” for increasingly more resourced classifiers. The graphs at the top describe the different softmax probabilities for most likely classes as a function of the global accuracy of a classifier from a certain family indexed by training time. The pie charts at the bottom show the decomposition of different datasets into points of the varying types. Note that this decomposition is similar with respect to different neural architectures.</figure></div>


<p/>



<p><strong>Training as teaching.</strong> Training of modern large models seems much more like teaching a student than fitting a model to data, complete with “taking breaks” or trying different approaches when the student doesn’t get it or seems tired (training diverges). The <a href="https://github.com/facebookresearch/metaseq/tree/main/projects/OPT/chronicles">training logbook of Meta’s large model</a> is instructive- aside from issues with hardware, we can see interventions such as switching different optimization algorithms in the middle of training and even considering “hot swapping” the activation functions (GELU to RELU). The latter doesn’t make much sense if you think of model training as fitting data as opposed to learning representations. </p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img alt="" height="201" src="https://lh4.googleusercontent.com/ujboPOfONTtBdZypMX-flKhxBQWnvxhdGN_eVox8BXzHSJbGyk7qzTCTe-lE4aUKewvrEMaU9etWb8eis7KTNXsawJ1bR-mou7-EvF9n_VwTruJJWvqkd34BvON5-99DUof45skX7EOHfgljeg" width="548"/></figure></div>

<div class="wp-block-image">
<figure class="aligncenter is-resized"><img alt="" height="152" src="https://lh6.googleusercontent.com/EFP7xVCiHMsAt_ZBGE1U10t6KCzWXRqI_KNOYOXE6KfUWBydmKILs3LB7LHEyAReTYLu9nSajqKUq0prSf7y_W8puRS0iymFznyYe07kPnDI3vVm6AQDcJnmp0WhV2W4J_lUbnNjU03tmCvnRg" width="634"/><strong>Figure:</strong> Excerpts from Meta’s training log</figure></div>


<p/>



<h2><strong>4.1) But what about supervised learning?</strong></h2>



<p>Up to this point, we only discussed self-supervised learning, but the canonical example of deep learning- the one you teach first in a course- is still supervised learning. After all, deep learning’s “ImageNet moment” came with, well, ImageNet. Does anything we said above still apply to this setting?</p>



<p>First, the emergence of supervised large-scale deep learning is to some extent a historical accident, aided by the availability of large high quality labeled datasets (i.e. ImageNet). One could imagine an alternative history in which deep learning first started showing breakthrough advances in Natural Language Processing via unsupervised learning, and only later transported into vision and supervised learning.</p>



<p>Second,  we have some evidence that even though they use radically different loss functions, supervised and self-supervised learning behave similarly “under the hood.” Both often achieve the same performance, and in work with <a href="https://arxiv.org/abs/2106.07682">Bansal and Nakkiran</a>, we showed that they also learn similar internal representations. Concretely, for every <img alt="k \in \{1..d-1\}" class="latex" src="https://s0.wp.com/latex.php?latex=k+%5Cin+%5C%7B1..d-1%5C%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, one can “stitch together” the first <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> layers of a depth <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> model that was trained via self-supervision with the last <img alt="d-k" class="latex" src="https://s0.wp.com/latex.php?latex=d-k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> layers of a supervised model with little loss in performance. </p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img alt="" height="299" src="https://lh4.googleusercontent.com/b8aReNvFG6VNbi-tTsFB-WMov7HSF4T56CXm1eFMDHfmg4wmDAX_St76YqZWZU2EfMd4xyIPHSSzzs_QUkz5Jslor2l_8M23OKM_XghkF2IVHMW5ntHpxH05aWLT8i6UYZD3ZrBnLEg6ymTkqA" width="681"/><strong>Figure: </strong>Table from <a href="https://arxiv.org/abs/2006.10029">SimCLR v2 paper (Chen et al)</a>. Note the general similarity in performance between supervised learning, fine-tuned (100%) self-supervised, and self-supervised + linear probe.</figure></div>


<p/>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img alt="" height="341" src="https://lh6.googleusercontent.com/nKpdayaNeS9IP7Vhu7QhDaTjUcgEZoAl3CbY9bmGZkV_YfcUL4rmRy1esz5824eheS5EbiPBl5pIaNJk4wZuJoAfEXB7rjyP3AVCFF_ke5mN4lQsI2MlvCV4ZBWJGIkeWA9WbtN89Z184fm69w" width="759"/><strong>Figure: </strong>Stitching a self supervised and a supervised model from <a href="https://arxiv.org/abs/2106.07682">Bansal et al</a>. Left: If the self-supervised is (say) <img alt="3\%" class="latex" src="https://s0.wp.com/latex.php?latex=3%5C%25&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> less accurate than the supervised model, then fully compatible representations would result in a stitching penalty of <img alt="p\cdot 3\%" class="latex" src="https://s0.wp.com/latex.php?latex=p%5Ccdot+3%5C%25&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> when <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> fraction of layers come from the self-supervised model. If the models are completely incompatible then we expect accuracy to drop sharply as we stitch more models. Right: Actual results for stitching different self-supervised models. We also stitch a random network as a “sanity check”.</figure></div>


<p/>



<p>The advantage of self-supervised + simple models is that they can separate out the aspects of feature learning or “deep learning magic” (done by the deep representation function) from the statistical model fitting (done by the linear or other “simple” classifier on top of this representation).</p>



<p>Finally, while this is more speculative, the fact that often “meta learning” seems to amount to learning representations (see <a href="https://arxiv.org/abs/1909.09157">Raghu et al.</a> and <a href="https://arxiv.org/abs/2206.03271">Mandi et. al.</a>)  can be considered as another piece of evidence that this is much of what’s going on, regardless of the objective that the model ostensibly optimizes. </p>



<h2><strong>4.2) What about over parameterization?</strong></h2>



<p>The reader may have noticed that I skipped over what is considered the canonical example of the disparity between the model of statistical learning and deep learning in practice: the absence of a “bias-variance tradeoff” (see Belkin et al.’s <a href="https://arxiv.org/abs/1812.11118">double descent</a>) and the ability of over-parameterized models to generalize well.</p>



<p>There are two reasons I do not focus on this aspect. First, if supervised learning really does correspond to self-supervised + simple learning “under the hood” then that <a href="https://arxiv.org/abs/2010.08508">may explain its generalization ability</a>.  Second, I think that over parameterization is <em>not</em> crucial to deep learning’s success. Deep networks are special not because they are big compared to the number of samples but because they are big in absolute terms. Indeed, typically in unsupervised / self-supervised learning models are <em>not</em> over parameterized. Even for the very large language models, their datasets are larger still. This does not make their performance any less mysterious.</p>


<div class="wp-block-image">
<figure class="aligncenter is-resized"><img alt="" height="292" src="https://lh5.googleusercontent.com/BgGdDZHOPsPxw2-dYE49AS7Lc_Opwqmg19d0PAUVNX9vwSOadwVrQAingo1pXUOJ_o13v25QV_SFQU7qdtSBwGk05VsWwsIwyc2j8wcGBkZ9m3wWCyiOOwTynhF3zWFZFe1PmDJKCZgO1IDl4A" width="673"/><strong>Figure: </strong><a href="https://arxiv.org/abs/2010.08127">Nakkiran-Neyshabur-Sadghi’s</a> “deep bootstrap” paper demonstrates that modern architectures behave similarly in the “over parameterized” or “under sampled” regime (where models train for multiple epochs on limited data until over fitting: “real world” in the above figure) and “under parameterized” or “online” regime (where models train for a single epoch, seeing each sample only once: “ideal world” in the above figure). </figure></div>


<p/>



<h2><strong>Summary</strong></h2>



<p>Statistical learning certainly plays a role in deep learning. However, despite using similar terms and code, thinking of deep learning as simply fitting a model with more knobs than classical models misses a lot of what is essential to its success. The human student metaphor is hardly perfect either. Like biological evolution, even though deep learning consists of many repeated applications of the same rule – gradient descent on empirical loss- it gives rise to highly complex outcomes. It seems that at different times different components of networks learn different things, including, representation learning, prediction fitting, implicit regularization, and pure noise. We are still searching for the right lens by which to ask questions about deep learning, let alone answer them.</p>



<p><strong>Acknowledgments:</strong> Thanks to Lucas Janson and Preetum Nakkiran for comments on early versions of this blog post.</p></div>
    </content>
    <updated>2022-06-20T17:15:58Z</updated>
    <published>2022-06-20T17:15:58Z</published>
    <category term="Philosophizing"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2022-06-23T03:37:58Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4640</id>
    <link href="https://lucatrevisan.wordpress.com/2022/06/20/workshop-on-fairness-in-ai/" rel="alternate" type="text/html"/>
    <title>Workshop on Fairness in AI</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Next Monday, June 27, I am organizing a workshop on issues around fairness, bias and discrimination in AI and Machine Learning. Here is a link to the program. Remote participation is possible (link in the website), and in-person participation is … <a href="https://lucatrevisan.wordpress.com/2022/06/20/workshop-on-fairness-in-ai/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Next Monday, June 27, I am organizing a workshop on issues around fairness, bias and discrimination in AI and Machine Learning.</p>



<p>Here is a <a href="https://lucatrevisan.github.io/fai.html">link to the program</a>. Remote participation is possible (link in the website), and in-person participation is free but we ask people to register so we can print badges and order the appropriate number of coffee breaks.</p>



<p>This workshop is being organized in partnership with <a href="https://www.edge-glbt.it/">EDGE</a>, an Italian NGO that works on LGBT rights, and it is the first event of their initiative “A+I: Algoritmi + Inclusivi”, which will feature an awareness campaign and a series of video interviews that will start after the summer.</p>



<p>In next week’s workshop, Oreste Pollicino from Bocconi will talk about the perspective of the legal community around algorithmic discrimination, Symeon Papadopoulos from ITI Patras will give a survey on issues of fairness in image processing and image understanding, Sanghamitra Dutta from J.P. Morgan AI will talk about how to use the theory of causality to reason about fairness, Debora Nozza and Dirk Hovy from Bocconi will talk about issues of fairness in language models and natural language processing, and Omer Reingold from Stanford and Cynthia Dwork from Harvard will talk about modeling and achieving fairness in prediction models.</p>



<p>The last morning session will be a panel discussion moderated by Damiano Terziotti from EDGE about perspectives from the social sciences and from outside academia. It will feature, among others, <a href="https://en.wikipedia.org/wiki/Brando_Benifei">Brando Benifei</a>, a member of the EU parliament who has played a leading role in the <a href="https://en.wikipedia.org/wiki/Artificial_Intelligence_Act">2021 draft EU regulations on AI</a>. The other panel members are Alessandro Bonaita, who is a data science lead in Generali (Italy’s largest insurance company), Luisella Giani, who is leading a technology consulting branch of Oracle for Europe, Middle East and Africa, Cinzia Maiolini, who is in the national secretariat of CGIL, an Italian Union, and Massimo Airoldi from the University of Milan.</p>



<p>If you are in or near Milan next week, come to what is shaping up to be a memorable event!</p></div>
    </content>
    <updated>2022-06-20T11:36:33Z</updated>
    <published>2022-06-20T11:36:33Z</published>
    <category term="Bocconi"/>
    <category term="technology"/>
    <category term="Algoritmi + Inclusivi"/>
    <category term="things that are excellent"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2022-06-23T03:37:13Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=20186</id>
    <link href="https://rjlipton.wpcomstaging.com/2022/06/19/the-graph-of-ancestors/" rel="alternate" type="text/html"/>
    <title>The Graph of Ancestors</title>
    <summary>Is there an “Implex Method” in complexity theory? Wikipedia src Bill Wyman was the bass guitarist of the Rolling Stones until 1993. He married Mandy Smith in 1989. A few years later, in 1993, his son, Stephen Wyman, married Mandy Smith’s mother, Patsy Smith. Had Bill and Mandy not divorced by then, Wyman would have […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Is there an “Implex Method” in complexity theory?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2022/06/19/the-graph-of-ancestors/bill_wyman_2009/" rel="attachment wp-att-20188"><img alt="" class="alignright wp-image-20188" height="205" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/Bill_Wyman_2009.jpg?resize=135%2C205&amp;ssl=1" width="135"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Wikipedia <a href="https://en.wikipedia.org/wiki/Bill_Wyman">src</a></font></td>
</tr>
</tbody>
</table>
<p>
Bill Wyman was the bass guitarist of the Rolling Stones until 1993. He married Mandy Smith in 1989. A few years later, in 1993, his son, Stephen Wyman, married Mandy Smith’s mother, Patsy Smith. Had Bill and Mandy not divorced by then, Wyman would have been his own step<img alt="{\,^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%2C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>father. Which Wyman do we mean? Both of them.</p>
<p>
Today we investigate the level of degeneracy in “family tree” type graphs.</p>
<p>
We say “family tree” but it cannot be a tree. If your great<img alt="{\,^k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%2C%5Ek%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-grandparents were all distinct for every value of <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, then at <img alt="{k = 30}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%3D+30%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> you would have over one billion distinct ancestors at that level—and two billion in your whole tree—virtually all born after the year 1100 CE. The world population is <a href="https://en.wikipedia.org/wiki/World_population_milestones">estimated</a> not to have passed one billion until after 1800. </p>
<p>
At least we can say it cannot have cycles. You cannot be your own biological grandfather, for reasons more fundamental than not being able to go back in time to kill your grandfather. Thus the graph is a <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">DAG</a>—a <em>directed acyclic graph</em>. Is “DAG” a jargon term? It should be regarded as common—it is, after all, the stuff of us.</p>
<p>
</p><p/><h2> Implex and Amplex </h2><p/>
<p/><p>
The fact that you are descended by multiple paths from common ancestors is called <a href="https://en.wikipedia.org/wiki/Pedigree_collapse">pedigree collapse</a> by Wikipedia, rather dramatically. </p>
<p>
The degree of pedigree collapse, meaning the difference between <img alt="{2^k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Ek%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and your actual number <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of level-<img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> ancestors—is called the <a href="https://en.wiktionary.org/wiki/implex">implex</a>. This is not a legal word in official US <a href="https://scrabble.hasbro.com/en-us/tools#dictionary">Scrabble</a> (TM), but does appear on the larger US-UK <a href="https://www.wordgamedictionary.com/sowpods/download/sowpods.txt">SOWPODS</a> list. </p>
<p>
The ratio <img alt="{\frac{A}{2^k}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7BA%7D%7B2%5Ek%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> strikes me as more natural than the difference. To keep it representing “collapse,” one would have to do <img alt="{1 - \frac{A}{2^k}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+-+%5Cfrac%7BA%7D%7B2%5Ek%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, but the original ratio is more convenient in what follows. I propose calling it the <b>amplex</b>, saying how ample one’s DAG of ancestors is. This term is not in SOWPODS but does appear (along with implex) in a downloadable <a href="https://github.com/dwyl/english-words">dictionary</a> of 466,551 meaningful words and names and abbreviations compiled by the organization <a href="https://dwyl.com/">dwyl</a> for “Do What You Love.” The word amplex originally <a href="https://en.wiktionary.org/wiki/amplex">means</a> to engage in a process by which some amphibians make ample trees—or rather DAGs—of descendants.</p>
<p>
The ratio may be expected to stabilize once <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> becomes moderately large. We can say similar about the differences <img alt="{k - \log_2 A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+-+%5Clog_2+A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> as <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> grows. Finally, the ratio <img alt="{\frac{\log_2 A}{k}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B%5Clog_2+A%7D%7Bk%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a technically different but similarly motivated notion of <em>amplex</em>. These logarithmic forms are most akin to information complexity measures.</p>
<p>
</p><p/><h2> An Example </h2><p/>
<p/><p>
For an example of implex and amplex, let Alice have a completely unique tree of ancestors, so zero collapse and amplex ration <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Then Alice’s full brother, named Bob of course, has the same. Suppose Alice and Bob incestuously marry and have a child C. Then for <img alt="{k = 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%3D+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to <img alt="{10}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B10%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, C has implex <img alt="{2^{k-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bk-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in the difference form, or <img alt="{0.5}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0.5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in the ratio form. The amplex ratio <img alt="{\frac{A}{2^k}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7BA%7D%7B2%5Ek%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is also <img alt="{0.5}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0.5%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>
Now suppose instead that Alice and Bob come from different families and each have amplex <img alt="{0.4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0.4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, i.e., <img alt="{\frac{A}{2^k} = \frac{B}{2^k} = 0.4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7BA%7D%7B2%5Ek%7D+%3D+%5Cfrac%7BB%7D%7B2%5Ek%7D+%3D+0.4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (where <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> refers to Bob’s set of level-<img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> ancestors) at level <img alt="{k = 9}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%3D+9%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Then their child D at level <img alt="{k = 10}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%3D+10%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> has amplex <em>at most</em> <img alt="{0.4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0.4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The maximum is achieved when Alice and Bob are unrelated to each other—i.e., the sets <img alt="{S_A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{S_B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of their respective ancestors (saying out to level <img alt="{9}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B9%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>) are distinct. If they coincide, then D will have amplex only <img alt="{0.2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0.2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. </p>
<p>
It may seem intuitively wrong that the incestuous child C could have higher amplex—lower implex—than the child of unrelated parents, each with a reasonable ratio. This hints that some further modifications of these measures may be more effective, such as weighting differences of closer ancestors more. Insofar as different paths to the same level-<img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> ancestor define an equivalence relation on binary strings of length <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, there are notions of prefixes and prefix-freedom that are relevant. </p>
<p>
I won’t try to sort such notions out now. I am only trying to be suggestive and invite reader suggestions. In the real world, <em>genomics</em> brings much more extensive notions of genetic diversity. What might help the world of computational complexity theory is more in mind here.</p>
<p>
</p><p/><h2> Analogy With Boolean Functions </h2><p/>
<p/><p>
There are various ways to set up a parenthood notion for Boolean functions. Given two Boolean functions <img alt="{\alpha(x_1,\dots,x_n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{\beta(x_1,\dots,x_n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, any of the following can be regarded as an offspring:</p>
<ul>
<li>
<img alt="{\alpha(x_1,\dots,x_n) \wedge \beta(x_1,\dots,x_n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%28x_1%2C%5Cdots%2Cx_n%29+%5Cwedge+%5Cbeta%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>; <p/>
</li><li>
<img alt="{\alpha(x_1,\dots,x_n) \vee \beta(x_1,\dots,x_n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%28x_1%2C%5Cdots%2Cx_n%29+%5Cvee+%5Cbeta%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>; <p/>
</li><li>
<img alt="{\alpha(x_1,\dots,x_n) \oplus \beta(x_1,\dots,x_n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%28x_1%2C%5Cdots%2Cx_n%29+%5Coplus+%5Cbeta%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (etc.); <p/>
</li><li>
<img alt="{(x_{n+1} \wedge \alpha(x_1,\dots,x_n)) \vee (\bar{x}_{n+1} \wedge \beta(x_1,\dots,x_n))}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28x_%7Bn%2B1%7D+%5Cwedge+%5Calpha%28x_1%2C%5Cdots%2Cx_n%29%29+%5Cvee+%28%5Cbar%7Bx%7D_%7Bn%2B1%7D+%5Cwedge+%5Cbeta%28x_1%2C%5Cdots%2Cx_n%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.
</li></ul>
<p>
The idea that we are trying to tap into is the following: In complexity theory, we want to prove that a set <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of functions are <em>hard</em>, meaning they do not have Boolean circuits of a given <em>small</em> size <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The sets of hard functions overall are large, but we mean sets <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of functions that are known in some other way, such as belonging to families that define problems in <img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. For the most part, it suffices to prove that <em>some</em> member of such an <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is hard. </p>
<p>
The usual mindset is to prove that circuits of size <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> have insufficient “virility” to compute members of <img alt="{H.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> The idea I wish to suggest instead is to try to prove that they are too incestuous—that too many of them compute copies of the same function. Then show that they cannot encompass all of <img alt="{H.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
To make this work in the above framework, we may need to invert the notion of Boolean “offspring” suggested above, so that the ultimate function computed is styled as an ancestor. In any event, the idea is to structure the counting so that size-<img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> circuits cannot have all the members of <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in their collective “gene pool”—so that some member of <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> must go uncomputed.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Can you suggest further notions in genealogy—without going into genetics—and can they help resolve impasses in complexity?</p>
<p/></font></font></div>
    </content>
    <updated>2022-06-20T03:47:40Z</updated>
    <published>2022-06-20T03:47:40Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="Oldies"/>
    <category term="P=NP"/>
    <category term="People"/>
    <category term="Bill Wyman"/>
    <category term="complexity"/>
    <category term="Father's Day"/>
    <category term="genealogy"/>
    <category term="Implex"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2022-06-23T03:37:42Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-4213949930505395862</id>
    <link href="http://blog.computationalcomplexity.org/feeds/4213949930505395862/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/06/guest-post-by-prahalad-rajkumar-advice.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/4213949930505395862" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/4213949930505395862" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/06/guest-post-by-prahalad-rajkumar-advice.html" rel="alternate" type="text/html"/>
    <title>Guest post by Prahalad Rajkumar: advice for grad students</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I suspect that Lance and/or I have had blogs giving advice to grad students. I won't point to any particular posts since that's a hard thing to search for. However, they were all written WAY AFTER Lance and I actually were grad students. Recently a former grad student at UMCP, Prahalad Rajkumar, emailed me that he wanted to do a post about advice for grad students. Since he has graduated more recently (Master degree in CS, topic was Monte Carlo Techniques, in 2009, then a job as a programmer-analyst) his advice may be better, or at least different, than ours was.</p><p>Here is his guest post with an occasional comment by me embedded in it. </p><p>----------------------------</p><p> <b>I Made this Fatal Mistake when I Joined a Graduate Program at Maryland</b></p><p>Getting accepted to a graduate program in a good school is an honor.</p><p>It is also an opportunity to do quality work and hone your skills. I made one fatal mistake at the start of my master’s degree at the University of Maryland which took me down a vicious rabbit hole. I believed that I was not cut out for this program.</p><p><b>The Only Person Who Gave an Incorrect Answer</b></p><p>Before the start of my graduate studies, there was an informal gathering held for newer students and some faculty members. A faculty member asked a basic algorithm question.</p><p>Everyone in the room gave one answer. I gave another answer.</p><p>This is real life and not Good Will Hunting, and of course, I was wrong. I had misunderstood the question. It would have been a simple matter to shrug and move forward. But the paternal voice in my head saw a good opportunity to continue to convince me that I was an imposter who did not belong here.</p><p><b>Who is Smarter than Whom?</b></p><p>Some of my fellow incoming graduate students, who TAed with me for Bill Gasarch’s class, played an innocent looking game.</p><p>“That guy is so smart”.</p><p>“I wish I were as smart as her”.</p><p>They couldn’t know that this would affect me. I too did not know that this could affect me. But it did. I asked myself “Am I smarter than person X?”. Each time, the paternal voice in my head was quick to answer “No”. And each time I took this “No” seriously.</p><p>NOTE FROM BILL: Professors also play <i>who is smarter than who</i> game and we shouldn't.</p><p><b>I Didn’t Choose My Classes Wisely</b></p><p>I made a few mistakes in choosing my classes. I chose Concrete Complexity with Bill, which I later realized I had no aptitude for. I chose an undergraduate class taught by a professor whose style did not resonate with me. Mercifully, I chose a third class that I liked and excelled in. A class which did not destroy my confidence.</p><p>In retrospect, though I chose a couple of classes that were not my cup of tea, I compounded my problems with the stories I told myself. I had several good options available to me. I could redouble my efforts in the said classes and give it my best shot. I could accept my inevitable “B” grades in these classes, and be mindful to choose better classes in the upcoming semesters.</p><p>I, however, did the one thing I should not have done: I further convinced myself that I was not cut out to be a graduate student.</p><div>NOTE FROM BILL: Some students wisely ask around to find out <i>who is a good teacher</i>? Prahalad points out that this is just the first question. A class may be appropriate for you or not based on many factors, not just if the instructor is a <i>good teacher.</i> </div><div><div><br/></div><div><b>I Fell Victim to Impostor Syndrome</b></div><div><br/></div><div>I kept compounding my woes in my second and third semesters. Things got bad -- I took up a position as a research assistant in my third semester. My confidence was low -- and I struggled to do basic tasks that fall under my areas of competence.</div><div><br/></div><div>In my fourth semester, I convinced myself that I could not code. In a class project where I had to do some coding as a part of a group project, I struggled to write a single line of code.</div><div><br/></div><div>When I confessed this to one of my group members, he got me out of my head. He got me to code my part more than capably. I’ve written about this experience <a href="https://os.me/short-stories/the-impostor-syndrome/">here</a>.</div><div><br/></div><div><br/></div><div><b> It Does Not Matter in the Slightest</b></div><div><br/></div><div>I wish I could tell the 2007 version of myself the following: It doesn’t matter who is smarter than whom. In any way whatsoever. We are on our individual journeys. In graduate school. In life.</div><div><br/></div><div>Comparing myself with another person is as productive as playing several hours of angry birds.</div><div><b><br/></b></div><div><b> The Admission Committee Believed in Me.</b></div><div><br/></div><div>There was one good reason I should have rejected the thought that I did not belong in the program. The admission committee believed that I belonged here. Consisting of several brilliant minds. If they thought I should be here, why should I second guess them?</div><div><br/></div><div>NOTE FROM BILL: While the Admissions committee DID believe in Prahalad and was CORRECT in this, I would not call the committee <i>brilliant</i>. As is well known, the IQ of a committee is the IQ of its dumbest member divided by the number of people on the committee. </div><div><br/></div><div><b>Bill Gasarch’s Secret Sauce</b></div><div><br/></div><div>Since I took a class with Bill and TAed for him, I had occasion to spend a lot of time with Bill.</div><div>In one conversation, Bill told me something profound. He told me the secret sauce behind his accomplishments. No, it was not talent. It was his willingness to work as hard as it takes.</div><div>And working hard is a superpower which is available to anyone who is inclined to invoke it. I wish I had.</div><div><br/></div><div>BILL COMMENT: The notion that hard work is important is of course old. I wonder how old. One of the best expressions of this that I read was in a book <i>Myths of Innovation</i> which said (a) Great ideas are over rated, (b) hard work and follow through are underrated. There are more sources on this notion in the next part of Prahalad's post. (Side Note- I got the book at the <i>Borders Books Going</i> <i>Out of Business Sale</i>. Maybe they should have read it.) </div><div><br/></div></div><div><div><br/></div><div><b>Talent is Overrated.</b></div><div><br/></div><div>I read a few books in the last couple of years that discussed the subject of mastery: Mastery by Robert Greene, Peak by Anders Ericsson, Talent is Overrated by Geoff Colvin, The Talent Code by Daniel Coyle, Grit by Angela Duckworth, Mindset by Carol Dweck</div><div><br/></div><div>There was one point that all of these books made: talent is not the factor which determines a person’s success. Their work ethic, their willingness to do what it takes, and several hours of deliberate practice is the secret of success. Of course, talent plays a part -- you can’t be 5’1 and hope to be better than Michael Jordan. But in the graduate school setting, where a majority are competent, it really is a matter of putting in the effort.</div><div><br/></div><div><b> Follow the Process</b></div><div><br/></div><div>Bill Walsh signed up as the coach of the languishing 49ers football team. The title of his bestselling book describes his coaching philosophy: The Score Takes Care of Itself. He established processes. Focusing on the smallest of details. Walsh made everyone in the football team and in the administrative departments follow their respective processes. Long story short: <i>the score took care of itself</i>. The 49ers won 3 super bowls among other impressive performances.</div><div><br/></div><div>If I had to do it all again: I would get out of my head. And keep going with a disciplined work ethic. Establish a process. Follow the process. And let the results take care of themselves.</div><div><br/></div><div><b><br/></b></div><div><b>All’s Well That Ends Well</b></div><div><br/></div><div>I grinded and hustled and successfully completed my Masters degree. However, instead of making the journey a joyride, I got in my own way and complicated things for no good reason.</div><div><br/></div><div><b> Final Thoughts</b></div><div><br/></div><div>As William James said, a person can change his life by changing his attitude. All I needed to do was change my thinking -- work hard -- and the “score would have taken care of itself”.</div><div><br/></div><div>I thought I was alone. But I found out in other spheres that a non-negligent percentage of people fall prey to the impostor syndrome. I wanted to write this to help any student who may be going through the problem that I did. If you are going through self-doubt, my message to you is to get out of the head, believe that you are capable (and make no mistake, you certainly are), do the work diligently,</div><div>follow the process, and <i>let the score take care of itself</i>.</div><div><br/></div></div></div>
    </content>
    <updated>2022-06-19T19:08:00Z</updated>
    <published>2022-06-19T19:08:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2022-06-22T23:07:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/089</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/089" rel="alternate" type="text/html"/>
    <title>TR22-089 |  A separation of PLS from PPP | 

	Ilario Bonacina, 

	Neil Thapen</title>
    <summary>Recently it was shown that  PLS is not contained in PPADS (ECCC report TR22-058). We show that this separation already implies that PLS is not contained in PPP. These separations are shown for the decision tree model of TFNP and imply similar separations in the type-2, relativized model.

Important note. This manuscript is based on an early preprint of report TR22-058, which did not contain the result that PLS is not contained in PPP. Our work is superseded by  the revised version of the report of 10 June 2022 which, independently, contains this result. The manuscript has not been edited to reflect this, and in particular references to the report are to the early version.</summary>
    <updated>2022-06-19T17:22:09Z</updated>
    <published>2022-06-19T17:22:09Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-06-23T03:37:32Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2022-06-19-crusader-braodcast/</id>
    <link href="https://decentralizedthoughts.github.io/2022-06-19-crusader-braodcast/" rel="alternate" type="text/html"/>
    <title>Crusader Broadcast</title>
    <summary>In previous posts we showed that the classic Dolev-Strong broadcast protocol takes $O(n^3)$ words and $t+1$ rounds and that Dolev Reischuk show that $\Omega(n^2)$ is needed and it is also known that $t+1$ rounds are needed. So while the number of rounds is optimal, to this day it remains an...</summary>
    <updated>2022-06-19T07:11:00Z</updated>
    <published>2022-06-19T07:11:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2022-06-22T22:42:47Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2022/06/18/shapes-triangular-pencils</id>
    <link href="https://11011110.github.io/blog/2022/06/18/shapes-triangular-pencils.html" rel="alternate" type="text/html"/>
    <title>The shapes of triangular pencils</title>
    <summary>The Institute of Mathematics &amp; its Applications tells us that applications of the Reuleaux triangle include “the cross-section of some pencils that are thought to be more ergonomic than traditional hexagonal ones”. It it true?</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://ima.org.uk/19140/westward-ho-conway-concorde-and-curiously-curved-coins/">The Institute of Mathematics &amp; its Applications tells us</a> that applications of the <a href="https://en.wikipedia.org/wiki/Reuleaux_triangle">Reuleaux triangle</a> include “the cross-section of some pencils that are thought to be more ergonomic than traditional hexagonal ones”. It it true?</p>

<p>It’s a bit difficult to tell, because there are many brands of triangular pencil and they don’t all come with easy-to-interpret cross-sections or dimensions. A true Reuleaux triangle pencil would have a cross section with rounded sides and 120° corners, making the same angle as the corners of a hexagonal pencil, as shown in the upper left cross-section of the figure below.</p>

<p style="text-align: center;"><img alt="Hypothetical cross-sections of triangular and hexagonal pencils: upper left, Reuleaux triangle; upper right, hexagon; lower left, flat triangle with rounded corners; lower-right, smooth three-lobed curve of constant width" src="https://11011110.github.io/blog/assets/2022/tri-pencils.svg"/></p>

<p>Instead, all the ones I found online appear to fall into two other classes of shapes: flat-sided triangles with rounded-off corners (lower left of the figure) or smooth three-lobed shapes with no corners at all (lower right). Here are some:</p>

<ul>
  <li>
    <p>The best image I could find of the Dixon Tri-Conderoga is <a href="https://woodclinched.com/2010/09/15/tri-conderoga/">this brief post by Woodclinched</a>. It appears to be a smooth three-lobed shape. Dixon also produce a “Tri-Write” triangular pencil, aimed at a lower-level market than the Tri-Conderoga; the <a href="https://dixonticonderogacompany.com/wp-content/uploads/2021/05/2021_Dixon-Prod-Catalog_04.21.pdf">2021 Dixon product catalog</a> shows both it and the Tri-Conderoga as having a cross-section with slightly rounded sides (flatter than a Reuleaux triangle but not completely flat) and rounded corners.</p>
  </li>
  <li>
    <p>The Faber-Castell Grip 2001 is another three-sided pencil, with added bumps on its sides to make it grippier. If we ignore the bumps for the purpose of determining its shape, <a href="https://www.penciltalk.org/2008/10/faber-castell-grip-2001-and-jumbo-grip-in-alternate-finishes">Pencil Talk shows it as a flat triangle with rounded corners</a>.</p>
  </li>
  <li>
    <p>The Koh-I-Noor Triograph, as shown in <a href="https://www.penciltalk.org/2007/09/koh-i-noor-hardmuth-triograph-1830-pencil">a review by Pencil Talk</a>, has rounded corners, and flat-enough sides to clearly print the brand name in large letters on it.</p>
  </li>
  <li>
    <p>The Lyra Ferby is a German children’s brand with a smooth three-lobed shape.</p>
  </li>
  <li>
    <p>The Marco Grip-Rite appears, from <a href="https://polarpencilpusher.home.blog/2019/07/11/pencil-review-marco-grip-rite-2b-w-bonus-sharpener/">its review by Polar Pencil Pusher</a>, to have a smooth three-lobed cross-section, even though the review calls it a Reuleaux triangle.</p>
  </li>
  <li>
    <p>The <a href="https://micador.com.au/products/micador-jr-colourush-jumbo-pencils-fsc-100">Micador ColouRush Jumbo Triangular Pencils</a> (an Asutralian branch of children’s pencils) have a cross-section image on the manufacturer website clearly showing a flat triangle with rounded corners.</p>
  </li>
  <li>
    <p>PaperMate make several triangular writing tools under the “Handwriting” brand, including PaperMate Handwriting woodcase pencils, aimed at children. I couldn’t find a good image of a cross-section of one, but their mechanical pencil in the same branch is clearly a smooth three-lobed shape. <a href="https://www.pencilrevolution.com/2019/03/papermate-handwriting-pencil/">Pencil Revolution</a> appears to show a similar shape for the wood pencils.</p>
  </li>
  <li>
    <p>I don’t know what brand of pencils the IMA used for the photo of their example, but it appears to be one with flat sides and rounded corners.</p>
  </li>
</ul>

<p>I also didn’t find pencils with flat-sided triangles (and non-rounded or only very lightly rounded corners) as their cross-sections. But if you search for “triangle pencil” you will find plenty of colorful plastic things with that shape that you can slide over your pencil to make it easier to hold.</p>

<p>I can’t tell you whether any of these are any good to write or draw with; for that, I prefer fountain pens. (I don’t have an <a href="https://www.fountainpennetwork.com/forum/topic/229621-modern-pens-the-omas-360/">Omas 360</a>, but that one really does appear to be based on a Reuleaux triangle.) Beyond writing, if you’re hoping for a pencil that doubles as a constant-width roller for rolling your books smoothly across your desk, and don’t want boring circular pencils, the smooth three-lobed ones may be your best bet. The one I drew above really is a curve of constant width. It’s just not a Reuleaux triangle.</p>

<p>This has been yet another episode of “things that are not Reuleaux triangles”. See previously <a href="https://11011110.github.io/blog/2020/11/27/study-triangular-bottle.html">a triangular bottle</a>, <a href="https://11011110.github.io/blog/2020/11/15/linkage.html">the Kölntriangle</a>, <a href="https://11011110.github.io/blog/2020/11/02/constant-width-involutes.html">rotors within rotors</a>, <a href="https://11011110.github.io/blog/2020/08/30/linkage.html">a Polish football logo</a>, <a href="https://11011110.github.io/blog/2020/07/05/shape-wankel-rotor.html">Wankel engines, and many more</a>.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/108501435837486329">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2022-06-18T16:42:00Z</updated>
    <published>2022-06-18T16:42:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2022-06-23T01:33:18Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=2847</id>
    <link href="https://theorydish.blog/2022/06/17/tcs-women-spotlight-workshop-at-stoc22/" rel="alternate" type="text/html"/>
    <title>TCS Women Spotlight Workshop at STOC’22</title>
    <summary>The 5th TCS Women Spotlight Workshop, organized by Elena Grigorescu (Purdue), Barna Saha (UCSD), Virginia Vassilevska Williams (MIT), and Mary Wootters (Stanford), is happening on Monday, 8:45 am Rome Time (GMT+2), as part of STOC’22. To attend the workshop in person, just show up! You can also join online for free (no STOC registration required!), by just registering here. The event is open to all. The workshop features an inspiring series of speakers — including an Inspiration talk by Irit Dinur: “Expansion, PCPs, and high dimensional expansion, or How I re-proved the PCP theorem and how I hope to do it again”; as well as Rising star talks by six stellar women researchers, soon to be on the job market: Sami Davies (Northwestern), Tamalika Mukherjee (Purdue), Aditi Dudeja (Rutgers), Charlie Carlson (Colorado Boulder), Yasamin Nazari (Johns Hopkins), and Jessica Sorrell (UCSD). For more information on the workshop, program, and talks, see the website.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The <a href="https://sigact.org/tcswomen/5th-tcs-women-meeting/tcs-women-2022-program/">5th TCS Women Spotlight Workshop</a>, organized by Elena Grigorescu (Purdue), Barna Saha (UCSD), Virginia Vassilevska Williams (MIT), and Mary Wootters (Stanford), is happening on <strong>Monday, 8:45 am Rome Time</strong> (GMT+2), as part of<a href="http://acm-stoc.org/stoc2022/"> STOC’22</a>. To attend the workshop in person, just show up! You can also join online for free (no STOC registration required!), by just registering <a href="https://form.jotform.com/221605322816045">here</a>. The event is open to all.</p>



<p>The workshop features an inspiring series of speakers — including an <strong>Inspiration talk </strong>by <a href="https://www.wisdom.weizmann.ac.il/~dinuri/">Irit Dinur</a>: <em>“Expansion, PCPs, and high dimensional expansion, or How I re-proved the PCP theorem and how I hope to do it again”</em>; as well as <strong>Rising star talks</strong> by six stellar women researchers, soon to be on the job market: <a href="http://samidavies.com/">Sami Davies </a>(Northwestern), <a href="https://web.ics.purdue.edu/~tmukherj/">Tamalika Mukherjee</a> (Purdue), <a href="https://www.cs.rutgers.edu/people/graduate-students/details/aditi-dudeja">Aditi Dudeja</a> (Rutgers), <a href="https://cacarlson.github.io/">Charlie Carlson </a>(Colorado Boulder), <a href="https://www.cs.jhu.edu/~ynazari/">Yasamin Nazari</a> (Johns Hopkins), and <a href="https://cseweb.ucsd.edu/~jlsorrel/">Jessica Sorrell </a>(UCSD).</p>



<p>For more information on the workshop, program, and talks, see <a href="https://sigact.org/tcswomen/5th-tcs-women-meeting/tcs-women-2022-program/">the website</a>.</p></div>
    </content>
    <updated>2022-06-18T04:12:35Z</updated>
    <published>2022-06-18T04:12:35Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>ccanonne</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2022-06-23T03:38:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://scottaaronson.blog/?p=6484</id>
    <link href="https://scottaaronson.blog/?p=6484" rel="alternate" type="text/html"/>
    <link href="https://scottaaronson.blog/?p=6484#comments" rel="replies" type="text/html"/>
    <link href="https://scottaaronson.blog/?feed=atom&amp;p=6484" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">OpenAI!</title>
    <summary xml:lang="en-US">I have some exciting news (for me, anyway). Starting next week, I’ll be going on leave from UT Austin for one year, to work at OpenAI. They’re the creators of the astonishing GPT-3 and DALL-E2, which have not only endlessly entertained me and my kids, but recalibrated my understanding of what, for better and worse, […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>I have some exciting news (for me, anyway).  Starting next week, I’ll be going on leave from UT Austin for one year, to work at <a href="https://openai.com/">OpenAI</a>.  They’re the creators of the astonishing <a href="https://en.wikipedia.org/wiki/GPT-3">GPT-3</a> and <a href="https://openai.com/dall-e-2/">DALL-E2</a>, which have not only endlessly entertained me and my kids, but recalibrated my understanding of what, for better and worse, the world is going to look like for the rest of our lives.  Working with an amazing team at OpenAI, including <a href="https://jan.leike.name/">Jan Leike</a>, <a href="http://joschu.net/">John Schulman</a>, and <a href="https://en.wikipedia.org/wiki/Ilya_Sutskever">Ilya Sutskever</a>, my job will be think about the theoretical foundations of AI safety and alignment.  What, if anything, can computational complexity contribute to a principled understanding of how to get an AI to do what we want and not do what we don’t want?</p>



<p>Yeah, I don’t know the answer either.  That’s why I’ve got a whole year to try to figure it out!  One thing I know for sure, though, is that I’m interested <em>both</em> in the short-term, where new ideas are now quickly testable, and where the misuse of AI for spambots, surveillance, propaganda, and other nefarious purposes is already a major societal concern, <em>and</em> the long-term, where one might worry about what happens once AIs surpass human abilities across nearly every domain.  (And all the points in between: we might be in for a long, wild ride.)  When you start reading about AI safety, it’s striking how there are two separate communities—the one mostly worried about machine learning perpetuating racial and gender biases, and the one mostly worried about superhuman AI turning the planet into goo—who not only don’t work together, but are <em>at each other’s throats</em>, with each accusing the other of totally missing the point.  I persist, however, in the possibly-naïve belief that these are merely two extremes along a single continuum of AI worries.  By figuring out how to align AI with human values today—constantly confronting our theoretical ideas with reality—we can develop knowledge that will give us a better shot at aligning it with human values tomorrow.</p>



<p>For family reasons, I’ll be doing this work mostly from home, in Texas, though traveling from time to time to OpenAI’s office in San Francisco.  I’ll <em>also</em> spend 30% of my time continuing to run the Quantum Information Center at UT Austin and working with my students and postdocs.  At the end of the year, I plan to go back to full-time teaching, writing, and thinking about quantum stuff, which remains my main intellectual love in life, even as AI—the field where I started, as a PhD student, before I switched to quantum computing—has been taking over the world in ways that none of us can ignore.</p>



<p>Maybe fittingly, this new direction in my career had its origins here on <em>Shtetl-Optimized</em>.  Several commenters, including <a href="https://scottaaronson.blog/?p=6288#comment-1928020">Max Ra</a> and <a href="https://scottaaronson.blog/?p=6288#comment-1928039">Matt Putz</a>, asked me point-blank what it would take to induce me to work on AI alignment.  Treating it as an amusing hypothetical, I replied that it wasn’t mostly about money for me, and that:</p>



<blockquote class="wp-block-quote"><p>The central thing would be finding an actual <em>potentially-answerable technical question</em> around AI alignment, even just a small one, that piqued my interest and that I felt like I had an unusual angle on. In general, I have an absolutely terrible track record at working on topics because I abstractly feel like I “should” work on them. My entire scientific career has basically just been letting myself get nerd-sniped by one puzzle after the next.</p></blockquote>



<p>Anyway, Jan Leike at OpenAI saw this exchange and wrote to ask whether I was serious in my interest.  Oh shoot!  Was I?  After intensive conversations with Jan, others at OpenAI, and others in the broader AI safety world, I finally concluded that I was.</p>



<p>I’ve obviously got my work cut out for me, just to catch up to what’s already been done in the field.  I’ve actually been in the Bay Area all week, meeting with numerous AI safety people (<em>and</em>, of course, complexity and quantum people), carrying a stack of technical papers on AI safety everywhere I go.  I’ve been struck by how, when I talk to AI safety experts, they’re not only <em>not dismissive</em> about the potential relevance of complexity theory, they’re more gung-ho about it than I am!  They want to talk about whether, say, <a href="https://en.wikipedia.org/wiki/IP_(complexity)">IP=PSPACE</a>, or <a href="https://people.cs.uchicago.edu/~fortnow/papers/mip2.pdf">MIP=NEXP</a>, or the <a href="https://en.wikipedia.org/wiki/PCP_theorem">PCP theorem</a> could provide key insights about how we could verify the behavior of a powerful AI.  (Short answer: maybe, on some level!  But, err, more work would need to be done.)</p>



<p>How did this complexitophilic state of affairs come about?  That brings me to another wrinkle in the story.  Traditionally, students follow in the footsteps of their professors.  But in trying to bring complexity theory into AI safety, I’m actually following in the footsteps of my student: <a href="https://paulfchristiano.com/">Paul Christiano</a>, one of the greatest undergrads I worked with in my nine years at MIT, the student whose course project turned into the <a href="https://arxiv.org/abs/1203.4740">Aaronson-Christiano quantum money paper</a>.  After MIT, Paul did a PhD in quantum computing at Berkeley, with my own former adviser Umesh Vazirani, while <em>also</em> working part-time on AI safety.  Paul then left quantum computing to work on AI safety full-time—indeed, along with others such as Dario Amodei, he helped start the safety group at OpenAI.  Paul has since left to found his own AI safety organization, the <a href="https://alignment.org/">Alignment Research Center (ARC)</a>, although he remains on good terms with the OpenAI folks.  Paul is largely responsible for bringing complexity theory intuitions and analogies into AI safety—for example, through the <a href="https://arxiv.org/abs/1805.00899">“AI safety via debate” paper</a> and the <a href="https://arxiv.org/abs/1810.08575">Iterated Amplification paper</a>.  I’m grateful for Paul’s guidance and encouragement—as well as that of the others now working in this intersection, like Geoffrey Irving and Elizabeth Barnes—as I start this new chapter.</p>



<p>So, what projects will I actually work on at OpenAI?  Yeah, I’ve been spending the past week trying to figure that out.  I still don’t know, but a few possibilities have emerged.  First, I might work out a general theory of sample complexity and so forth for learning in dangerous environments—i.e., learning where making the wrong query might kill you.  Second, I might work on explainability and interpretability for machine learning: given a deep network that produced a particular output, what do we even mean by an “explanation” for “why” it produced that output?  What can we say about the computational complexity of finding that explanation?  Third, I might work on the ability of weaker agents to verify the behavior of stronger ones.  Of course, if P≠NP, then the gap between the difficulty of solving a problem and the difficulty of recognizing a solution can sometimes be enormous.  And indeed, even in empirical machine learing, there’s typically a gap between the difficulty of <em>generating</em> objects (say, cat pictures) and the difficulty of <em>discriminating</em> between them and other objects, the latter being easier.  But this gap typically isn’t exponential, as is conjectured for NP-complete problems: it’s much smaller than that.  And counterintuitively, we can then turn around and use the generators to improve the discriminators.  How can we understand this abstractly?  Are there model scenarios in complexity theory where we can prove that something similar happens?  How far can we amplify the generator/discriminator gap—for example, by using interactive protocols, or debates between competing AIs?</p>



<p>OpenAI, of course, has the word “open” right in its name, and a founding mission “to ensure that artificial general intelligence benefits all of humanity.”  But it’s also a for-profit enterprise, with investors and paying customers and serious competitors.  So throughout the year, don’t expect me to share any proprietary information—that’s not my interest anyway, even if I <em>hadn’t</em> signed an NDA.  But <em>do</em> expect me to blog my general thoughts about AI safety as they develop, and to solicit feedback from readers.</p>



<p>In the past, I’ve often been skeptical about the prospects for superintelligent AI becoming self-aware and destroying the world anytime soon (see, for example, my 2008 post <a href="https://scottaaronson.blog/?p=346">The Singularity Is Far</a>).  While I was aware since 2005 or so of the AI-risk community; and of its leader and prophet, <a href="https://en.wikipedia.org/wiki/Eliezer_Yudkowsky">Eliezer Yudkowsky</a>; and of Eliezer’s exhortations for people to drop everything else they’re doing and work on AI risk, as the biggest issue facing humanity, I … kept the whole thing at arms’ length.  Even <em>supposing</em> I agreed that this was a huge thing to worry about, I asked, what on earth do you want me to do about it today?  We know so little about a future superintelligent AI and how it would behave that any actions we took today would likely be useless or counterproductive.</p>



<p>Over the past 15 years, though, my and Eliezer’s views underwent a dramatic and ironic reversal.  If you read Eliezer’s <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities">“litany of doom”</a> from two weeks ago, you’ll see that he’s now resigned and fatalistic: because his early warnings weren’t heeded, he argues, humanity is almost certainly doomed and an unaligned AI will soon destroy the world.  He says that there are basically <em>no</em> promising directions in AI safety research: for any alignment strategy anyone points out, Eliezer can trivially refute it by explaining how (e.g.) the AI would be wise to the plan, and would pretend to go along with whatever we wanted from it while secretly plotting against us.</p>



<p>The weird part is, just as Eliezer became more and more pessimistic about the prospects for getting anywhere on AI alignment, I’ve become more and more <em>optimistic</em>.  Part of my optimism is because people like Paul Christiano have laid foundations for a meaty mathematical theory: much like the Web (or quantum computing theory) in 1992, it’s still in a ridiculously primitive stage, but even <em>my</em> limited imagination now suffices to see how much more could be built there.  An even greater part of my optimism is because we now live in a world with GPT-3, DALL-E2, and other systems that, while they clearly aren’t AGIs, are powerful enough that worrying about AGIs has come to seem more like prudence than like science fiction. And we can finally test our intuitions against the realities of these systems, which (outside of mathematics) is pretty much the only way human beings have ever succeeded at anything.</p>



<p>I didn’t predict that machine learning models this impressive would exist by 2022.  Most of you probably didn’t predict it.  For godsakes, <em>Eliezer Yudkowsky</em> didn’t predict it.  But it’s happened.  And to my mind, one of the defining virtues of science is that, when empirical reality gives you a clear shock, you update and adapt, rather than expending your intelligence to come up with clever reasons why it doesn’t matter or doesn’t count.</p>



<p>Anyway, so that’s the plan!  If I can figure out a way to save the galaxy, I will, but I’ve set my goals slightly lower, at learning some new things and doing some interesting research and writing some papers about it and enjoying a break from teaching.  Wish me a non-negligible success probability!</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p><strong>Update (June 18):</strong> To respond to a couple criticisms that I’ve seen elsewhere on social media…</p>



<p><strong>Can the rationalists sneer at me for waiting to get involved with this subject until it had become sufficiently “respectable,” “mainstream,” and ”high-status”? </strong> I suppose they can, if that’s their inclination.  I suppose I should be grateful that so many of them chose to respond instead with messages of congratulations and encouragement.   Yes, I plead guilty to keeping this subject at arms-length until I could point to GPT-3 and DALL-E2 and the other dramatic advances of the past few years to justify the reality of the topic to anyone who might criticize me.  It <em>feels</em> internally like I had principled reasons for this: I can think of almost no examples of research programs that succeeded over decades even in the teeth of opposition from the scientific mainstream.  If so, then arguably the best time to get involved with a “fringe” scientific topic, is when and only when you can foresee a path to it <em>becoming</em> the scientific mainstream.  At any rate, that’s what I did with quantum computing, as a teenager in the mid-1990s.  It’s what many scientists of the 1930s did with the prospect of nuclear chain reactions.  And if I’d optimized for getting the right answer earlier, I might’ve had to weaken the filters and let in a bunch of dubious worries that would’ve paralyzed me.  But I admit the possibility of self-serving bias here.</p>



<p><strong>Should you worry that OpenAI is just hiring me to be able to say “look, we have Scott Aaronson working on the problem,” rather than actually caring about what its safety researchers come up with? </strong> I mean, I can’t prove that you <em>shouldn’t</em> worry about that.  In the end, whatever work I do on the topic will have to speak for itself.  For whatever it’s worth, though, I was impressed by the OpenAI folks’ detailed, open-ended engagement with these questions when I met them—sort of like how it might look if they actually believed what they said about wanting to get this right for the world.  I wouldn’t have gotten involved otherwise.</p></div>
    </content>
    <updated>2022-06-17T23:39:17Z</updated>
    <published>2022-06-17T23:39:17Z</published>
    <category scheme="https://scottaaronson.blog" term="Announcements"/>
    <category scheme="https://scottaaronson.blog" term="Self-Referential"/>
    <category scheme="https://scottaaronson.blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://scottaaronson.blog/?feed=atom</id>
      <icon>https://scottaaronson.blog/wp-content/uploads/2021/10/cropped-Jacket-32x32.gif</icon>
      <link href="https://scottaaronson.blog" rel="alternate" type="text/html"/>
      <link href="https://scottaaronson.blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2022-06-19T05:44:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://corner.mimuw.edu.pl/?p=1115</id>
    <link href="https://corner.mimuw.edu.pl/?p=1115" rel="alternate" type="text/html"/>
    <title>WOLA 2022: a few slots for online presentations by students and postdocs</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">It is still not too late to find a cheap flight to Warsaw. This includes Rome (see our suggestions in the previous email/post). We have a few unused slots for talks. Our plan is to open them to online participants … <a href="https://corner.mimuw.edu.pl/?p=1115">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ol><li>It is still not too late to find a cheap flight to Warsaw. This includes Rome (see our suggestions in the previous email/post).</li><li>We have a few unused slots for talks. Our plan is to open them to online participants who are students or postdocs. If you are a student or postdoc and want to give a talk remotely, via Zoom, please submit your title and abstract preferably by Monday via the registration form at https://ideas-ncbr.pl/en/wola/registration/ (If you have already registered, please register again.)<br/>If you have any time constraints, please indicate them at the beginning of the abstract (use Warsaw local time) and we will do our best to find a working slot. If there are a lot of submissions, we may assign less time to each talk and if this does not help, we may assign slots on the first–come first–serve basis.</li><li>If you have not registered but you are coming in person, please do this ASAP. If you are planning on joining remotely, this will also help us with keeping you updated. </li></ol></div>
    </content>
    <updated>2022-06-17T19:31:52Z</updated>
    <published>2022-06-17T19:31:52Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>sank</name>
    </author>
    <source>
      <id>https://corner.mimuw.edu.pl</id>
      <link href="https://corner.mimuw.edu.pl/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://corner.mimuw.edu.pl" rel="alternate" type="text/html"/>
      <subtitle>University of Warsaw</subtitle>
      <title>Banach's Algorithmic Corner</title>
      <updated>2022-06-22T22:41:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://kamathematics.wordpress.com/?p=341</id>
    <link href="https://kamathematics.wordpress.com/2022/06/17/guest-post-colt-2022-call-for-open-problems/" rel="alternate" type="text/html"/>
    <title>Guest post: COLT 2022 Call for Open Problems</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The tireless Clément Canonne is the open problem chair for COLT 2022. He asked to share this call for open problems. Please submit your best! The 35th Annual Conference on Learning Theory (COLT 2022), to be held in London on July 2-5 and remotely, will follow in the footsteps of previous editions and feature an … <a class="more-link" href="https://kamathematics.wordpress.com/2022/06/17/guest-post-colt-2022-call-for-open-problems/">Continue reading<span class="screen-reader-text"> "Guest post: COLT 2022 Call for Open Problems"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The tireless <a href="https://ccanonne.github.io/">Clément Canonne</a> is the open problem chair for COLT 2022. He asked to share this call for open problems. Please submit your best!</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p><a href="https://learningtheory.org/colt2022/index.html">The 35th Annual Conference on Learning Theory</a> (COLT 2022), to be held in London on July 2-5 and remotely, will follow in the footsteps of previous editions and feature an <em>Open Problems session</em>, where attendees can present their open problems and suggest them to the learning community — and possibly offer prizes for their resolution! (After all, a little incentive goes a long way…)</p>



<p>The deadline to submit an open problem has been extended to <em>Monday June 20, 4pm PDT.</em> If you have any nagging question or stubborn problem, please submit them!</p>



<p>More information and CfP: <a href="https://learningtheory.org/colt2022/cfp.html#openproblems" rel="nofollow">https://learningtheory.org/colt2022/cfp.html#openproblems</a></p></div>
    </content>
    <updated>2022-06-17T16:46:29Z</updated>
    <published>2022-06-17T16:46:29Z</published>
    <category term="Events"/>
    <author>
      <name>Gautam</name>
    </author>
    <source>
      <id>https://kamathematics.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://kamathematics.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://kamathematics.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://kamathematics.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://kamathematics.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Kamathematics</title>
      <updated>2022-06-23T03:38:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8366</id>
    <link href="https://windowsontheory.org/2022/06/17/stocial-2022-guest-post-by-clement-canonne/" rel="alternate" type="text/html"/>
    <title>STOCial 2022: Guest post by Clément Canonne (+ TCS Women Spotlight workshop)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">[To further prove that I am not a longtermist, here is a guest post by Clément about activities in STOC that will happen next week. –Boaz] UPDATE: Don’t miss the TCS Women Spotlight Workshop  Monday, June 20th, 8:45 am – 11:450 am Rome Time. Irit Dinur will give a talk on “How I re-proved the PCP … <a class="more-link" href="https://windowsontheory.org/2022/06/17/stocial-2022-guest-post-by-clement-canonne/">Continue reading <span class="screen-reader-text">STOCial 2022: Guest post by Clément Canonne (+ TCS Women Spotlight workshop)</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>[To further prove that <a href="https://windowsontheory.org/2022/05/23/why-i-am-not-a-longtermist/">I am not a longtermist</a>, here is a guest post by Clément about activities in STOC that will happen <strong>next week.</strong> –Boaz]</p>



<p><strong>UPDATE</strong>: Don’t miss the<a href="https://sigact.org/tcswomen/"> TCS Women Spotlight Workshop</a>  <strong>Monday, June 20th, 8:45 am – 11:450 am</strong> Rome Time. Irit Dinur will give a talk on “How I re-proved the PCP theorem and how I hope to do it again” and there will be six rising star talks by Sami Davies (Northwestern), Tamalika Mukherjee (Purdue), Aditi Dudeja (Rutgers), Charlie Carlson (Colorado Boulder), Yasamin Nazari (Johns Hopkins), and Jessica Sorrell (UCSD). Register on <a href="https://form.jotform.com/221605322816045" rel="nofollow">https://form.jotform.com/221605322816045</a></p>



<p>The <a href="http://acm-stoc.org/stoc2022/">54th Annual ACM Symposium on Theory of Computing</a> (STOC’22) is starting next week in Rome, as part of the broader TheoryFest. Now, while this probably is not coming as a surprise to you, did you now about the <em>social and mentoring events</em> at STOC, which, not to miss a good portmanteau when one sees one,* we shall refer henceforth as <em>STOCial’22?</em></p>



<p>Organised by <a href="https://sites.google.com/uniroma1.it/federicofusco/home">Federico Fusco</a>, <a href="https://www.cs.cornell.edu/~teganwilson/">Tegan Wilson</a>, <a href="https://sites.google.com/site/marywootters/">Mary Wootters</a>, and myself, <a href="https://sites.google.com/view/stocial-2022">STOCial’22</a> includes a bonanza of activities, games, and fun, including (but not limited to):</p>



<ul><li>a student lunch!</li><li>two senior/junior lunches!</li><li>cartoon caption contests!</li><li>a scavenger hunt! <img alt="&#x1F9E9;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f9e9.png" style="height: 1em;"/></li><li>a STOC-themed crossword!</li><li>a game of socc… football! <img alt="&#x26BD;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/26bd.png" style="height: 1em;"/></li><li>PRIZES! <img alt="&#x1F3C6;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f3c6.png" style="height: 1em;"/></li></ul>



<p>To learn more about those, and sign up to the student or senior/junior lunches: <a href="https://sites.google.com/view/stocial-2022" rel="nofollow">https://sites.google.com/view/stocial-2022</a></p>



<p>See you next week!</p>



<p>Clément Canonne</p>



<p>*  If someone finds a palindrome instead, let me know. I would love a good pal in Rome.</p></div>
    </content>
    <updated>2022-06-17T14:42:52Z</updated>
    <published>2022-06-17T14:42:52Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2022-06-23T03:37:58Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=22882</id>
    <link href="https://gilkalai.wordpress.com/2022/06/17/richard-stanley-enumerative-and-algebraic-combinatorics-in-the1960s-and-1970s/" rel="alternate" type="text/html"/>
    <title>Richard Stanley: Enumerative and Algebraic Combinatorics in the1960’s and 1970’s</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">In his comment to the previous post by Igor Pak, Joe Malkevitch referred us to a wonderful paper by Richard Stanley on enumerative and algebraic combinatorics in the 1960’s and 1970’s. See also this post on Richard’s memories regarding the … <a href="https://gilkalai.wordpress.com/2022/06/17/richard-stanley-enumerative-and-algebraic-combinatorics-in-the1960s-and-1970s/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p/>



<p>In his comment to the previous post by Igor Pak, Joe Malkevitch referred us to a <a href="https://arxiv.org/abs/2105.07884">wonderful paper by Richard Stanley on enumerative and algebraic combinatorics in the 1960’s and 1970’s.</a> </p>



<p>See also this post on Richard’s memories regarding the proof of the upper bound theorem for spheres <a href="https://gilkalai.wordpress.com/2013/09/10/how-the-proof-of-the-upper-bound-theorem-for-spheres-was-found/">Richard Stanley: How the Proof of the Upper Bound Theorem (for spheres) was Found</a>.</p></div>
    </content>
    <updated>2022-06-17T05:57:41Z</updated>
    <published>2022-06-17T05:57:41Z</published>
    <category term="Combinatorics"/>
    <category term="What is Mathematics"/>
    <category term="Richard Stanley"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2022-06-23T03:37:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/2022/06/16/how-i-chose-enumerative-combinatorics/</id>
    <link href="https://gilkalai.wordpress.com/2022/06/16/how-i-chose-enumerative-combinatorics/" rel="alternate" type="text/html"/>
    <title>Igor Pak: How I chose Enumerative Combinatorics</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Originally posted on <a href="https://igorpak.wordpress.com/2022/06/12/how-i-chose-enumerative-combinatorics/">Igor Pak's blog</a>: <br/>Apologies for not writing anything for awhile. After Feb 24, the math part of the “life and math” slogan lost a bit of relevance, while the actual events were stupefying to the point…</div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div class="wpcom-reblog-snapshot"><div class="reblogger-note"><div class="reblogger-note-content"><blockquote><p>Another great post by Igor Pak</p>
</blockquote></div></div><div class="reblog-post"><p class="reblog-from"><img alt="" class="avatar avatar-32" height="32" src="https://2.gravatar.com/avatar/28ad5e53cacd5f67ae8b4d80bbcff03d?s=32&amp;d=identicon&amp;r=PG" width="32"/><a href="https://igorpak.wordpress.com/2022/06/12/how-i-chose-enumerative-combinatorics/">Igor Pak's blog</a></p><div class="reblogged-content">
<p/>

<p>Apologies for not writing anything for awhile.  After <a href="https://en.wikipedia.org/wiki/2022_Russian_invasion_of_Ukraine">Feb 24</a>, the <em>math </em>part of the “<em/>” slogan lost a bit of relevance, while the actual events were stupefying to the point when I had nothing to say about the <em>life </em>part.  Now that the shock subsided, let me break the silence by telling an old personal story  which is neither relevant to anything happening right now nor a lesson to anyone.  Sometimes a story is just a story… </p>

<p/>

<p/>

<h4>My field</h4>

<p/>

<p/>

<p>As the readers of this blog know, I am a <em><strong/></em>.  Not a “proud one”.  Just “a combinatorialist”.  To paraphrase a <a href="https://en.wikipedia.org/wiki/Rifleman%27s_Creed">military slogan</a> “there are many fields like this one, but this one is mine”.  While I’ve been <a href="https://wp.me/p211iQ-ds">defending</a> my field <a href="https://wp.me/p211iQ-sq">for years</a>, writing about <a href="https://wp.me/p211iQ-ds">its struggles</a>, and often <a href="https://wp.me/p211iQ-bQ">defining it</a>, it’s not because this field is more important than others. Rather, because it’s so…</p>
</div><p class="reblog-source"><a href="https://igorpak.wordpress.com/2022/06/12/how-i-chose-enumerative-combinatorics/">View original post</a> <span class="more-words">1,095 more words</span></p></div></div></div>
    </content>
    <updated>2022-06-16T11:38:26Z</updated>
    <published>2022-06-16T11:38:26Z</published>
    <category term="Combinatorics"/>
    <category term="What is Mathematics"/>
    <category term="Igor Pak"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2022-06-23T03:37:40Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://bit-player.org/?p=2477</id>
    <link href="http://bit-player.org/2022/jotto" rel="alternate" type="text/html"/>
    <link href="http://bit-player.org/2022/jotto#comments" rel="replies" type="text/html"/>
    <link href="http://bit-player.org/2022/jotto/feed/atom" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Jotto</title>
    <summary type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml">A day or two after publishing my TL;DR on Wordle algorithms, I stumbled on a remarkable paper that neatly summarizes all the main ideas. The remarkable part is that the paper was written 50 years before Wordle was invented! The … <a href="http://bit-player.org/2022/jotto">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>A day or two after publishing my <a href="http://bit-player.org/2022/words-for-the-wordle-weary">TL;DR on Wordle algorithms</a>, I stumbled on a remarkable paper that neatly summarizes all the main ideas. The remarkable part is that the paper was written 50 years before Wordle was invented!</p>
<p>The paper is “<a href="https://dspace.mit.edu/handle/1721.1/6192">Information Theory and the Game of Jotto</a>,” issued in August of 1971 as Artificial Intelligence Memo No. 28 from the AI Lab at MIT. The author was Michael D. Beeler, known to me mainly as one of the three principal authors of <a href="https://dspace.mit.edu/bitstream/handle/1721.1/6086/AIM-239.pdf">HAKMEM</a> (the others were Bill Gosper and Rich Schroeppel). Beeler later worked at Bolt, Baranek, and Newman, an MIT spinoff.</p>
<p><a href="https://en.wikipedia.org/wiki/Jotto">Wikipedia</a> tells me that Jotto was invented in 1955 by Morton M. Rosenfeld as a game for two players. As in Wordle, you try to discover a secret word by submitting guess words and getting feedback about how close you have come to the target. The big difference is that JOTTO’s feedback offers only a crude measure of closeness. You learn the number of letters in your guess word that match one of the letters in the target word. You get no indication of <em>which</em> letters match, or whether they are in the correct positions.</p>
<p>The unit of measure for closeness is the jot. Beeler gives the example of playing GLASS against SMILE, which earns a closeness score of two jots, since there are matches for the letter L and for one S. Unlike the Wordle feedback rule, this scoring scheme is symmetric: The score remains the same if you switch the roles of guess and target word.</p>
<p>A defect of the game, in my view, is that you can max out the score at five jots and still not know the target word. For example, when a five-jot score tells you that the letters of the target are {A, E, G, L, R}, the word could be GLARE, LAGER, LARGE, or REGAL. Your only way to pin down the answer is to guess them in sequence.</p>
<p>Beeler’s main topic is not how the game proceeds between human players but how a computer can be programmed to take the role of a player. He reports that “A JOTTO program has existed for a couple of years at MIT’s A.I. Lab,” meaning it was created sometime in the late 1960s. He says nothing about who wrote this program. I’m going to make the wild surmise that Beeler himself might have been the author, particularly given his intimate knowledge of the program’s innards.</p>
<p>Here’s the crucial passage, lifted directly from the memo:</p>
<p><img alt="Beeler block quote on JOTTO" border="0" class="centered" height="217" src="http://bit-player.org/wp-content/uploads/2022/06/Beeler-block-quote-on-JOTTO.png" width="640"/></p>
<p>The strategy described here—maximizing the information gain from each guess—is exactly what’s recommended for Wordle. But where Wordle divvies up the potential target words into \(3^5 = 243\) subsets, the JOTTO scoring rule defines only six categories (0 through 5 jots). As a result, the maximum possible information gain is only about 2.6 bits in JOTTO, compared with almost 8 bits in Wordle. </p>
<p>Beeler also recognized a limitation of this “greedy” strategy. “It is conceivable that the test word with the highest expectation at the current point in the game has a good chance of getting us to a point where we will NOT have any particularly good test words available . . . I am indebted to Bill Gosper for pointing out this possibility; the computation required, however, is impractical, and besides, the program seems to do acceptably as is.”</p>
<p>The JOTTO program was written in the assembly language of the <a href="http://bitsavers.org/pdf/dec/_Books/Bell-ComputerEngineering.pdf">PDP-6 and PDP-10 family of machines</a> from the Digital Equipment Corporation, which were much loved in that era at MIT. (Beeler praises the instruction set as “very symmetrical, complete, powerful and easy to think in.”) But however elegant the architecture, physical resources were cramped, with a maximum memory capacity of about one megabyte. Nevertheless, Beeler found room for the program itself, for a dictionary of about 7,000 words, and for tables of precomputed responses to the first two or three guesses.</p>
<p>Humbling.</p></div>
    </content>
    <updated>2022-06-15T17:50:45Z</updated>
    <published>2022-06-15T17:50:45Z</published>
    <category scheme="http://bit-player.org" term="computing"/>
    <category scheme="http://bit-player.org" term="games"/>
    <author>
      <name>Brian Hayes</name>
      <uri>http://bit-player.org</uri>
    </author>
    <source>
      <id>http://bit-player.org/feed/atom</id>
      <link href="http://bit-player.org" rel="alternate" type="text/html"/>
      <link href="http://bit-player.org/feed/atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">An amateur's outlook on computation and mathematics</subtitle>
      <title xml:lang="en-US">bit-player</title>
      <updated>2022-06-15T17:52:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/06/15/cryptography-postdoc-at-hebrew-university-apply-by-august-31-2022/</id>
    <link href="https://cstheory-jobs.org/2022/06/15/cryptography-postdoc-at-hebrew-university-apply-by-august-31-2022/" rel="alternate" type="text/html"/>
    <title>Cryptography Postdoc at Hebrew University (apply by August 31, 2022)</title>
    <summary>The Cryptography Group at the Hebrew University’s School of Computer Science and Engineering invites applications for a post-doc position. Potential applicants are encouraged to visit http://www.gilsegev.net and http://www.cs.huji.ac.il/~ilank for an overview of the group’s recent activity. To apply for the position, please provide us with your CV and research statement. Website: https://www.cs.huji.ac.il/page/5087 Email: crypto@cs.huji.ac.il</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Cryptography Group at the Hebrew University’s School of Computer Science and Engineering invites applications for a post-doc position. Potential applicants are encouraged to visit <a href="http://www.gilsegev.net" rel="nofollow">http://www.gilsegev.net</a> and <a href="http://www.cs.huji.ac.il/~ilank" rel="nofollow">http://www.cs.huji.ac.il/~ilank</a> for an overview of the group’s recent activity.</p>
<p>To apply for the position, please provide us with your CV and research statement.</p>
<p>Website: <a href="https://www.cs.huji.ac.il/page/5087">https://www.cs.huji.ac.il/page/5087</a><br/>
Email: crypto@cs.huji.ac.il</p></div>
    </content>
    <updated>2022-06-15T17:36:59Z</updated>
    <published>2022-06-15T17:36:59Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-06-23T03:37:44Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2022/06/15/linkage</id>
    <link href="https://11011110.github.io/blog/2022/06/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Claas Voelcker on academic work-life balance (\(\mathbb{M}\), via). I think we all know that many academics (myself included!) struggle to keep our weekend and evening time free of work-related distractions. Voelcker investigates where this pressure to work comes from (often internally) and suggests that overwork may block creativity; taking time off can make you more productive.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://thegradient.pub/working-on-the-weekends-an-academic-necessity/">Claas Voelcker on academic work-life balance</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108406720822630002">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=31562264">via</a>). I think we all know that many academics (myself included!) struggle to keep our weekend and evening time free of work-related distractions. Voelcker investigates where this pressure to work comes from (often internally) and suggests that overwork may block creativity; taking time off can make you more productive.</p>
  </li>
  <li>
    <p>From a face-up deck of cards, repeatedly deal off the number of cards showing on the top card (counting Jacks as 11, etc.). What’s the probability that you empty the deck by dealing out exactly the right number of cards in the last step? <a href="https://mathstodon.xyz/@christianp/108401164015456979">Christian Lawson-Perfect’s post</a> inspired a group discussion leading to the result that, for decks with large numbers of suits, the answer should tend to 1/7, and that for a standard 52-card deck it is approximately 0.1420342593977892.</p>
  </li>
  <li>
    <p>Another Wikipedia illustration <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108415828596664345">\(\mathbb{M}\)</a>):</span> empty regions for the <a href="https://en.wikipedia.org/wiki/Euclidean_minimum_spanning_tree">Euclidean minimum spanning tree</a>. If the red vertical segment is to be an MST edge, the outer white lens needs to be empty of other points; this emptiness implies that the edge is part of the relative neighborhood graph. The emptiness of the light blue diameter circle inside the lens defines the Gabriel graph in the same way. The inner rhombus must not only be empty, but disjoint from the rhombi of other edges.</p>

    <p style="text-align: center;"><img alt="Empty regions for the Euclidean minimum spanning tree" src="https://11011110.github.io/blog/assets/2022/EMST-empty-regions.svg" style="width: 100%;"/></p>
  </li>
  <li>
    <p><a href="https://somethingorotherwhatever.com/shunting-yard-animation/">Shunting yard animation</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@christianp/108424166828836198">\(\mathbb{M}\)</a>).</span> Cutesy train animation of the <a href="https://en.wikipedia.org/wiki/Shunting_yard_algorithm">shunting yard algorithm</a> for parsing infix expressions.</p>
  </li>
  <li>
    <p><a href="https://beachpackagingdesign.com/boxvox/pseudo-cylindrical-concave-polyhedral-packaging">Pseudo-cylindrical concave polyhedral packaging</a> <span style="white-space: nowrap;">(<a href="MLIhttps://mathstodon.xyz/@11011110/108434883781055479NK">\(\mathbb{M}\)</a>).</span>  This post describes multiple examples of the <a href="https://en.wikipedia.org/wiki/Yoshimura_buckling">Yoshimura buckling pattern</a> or <a href="https://en.wikipedia.org/wiki/Schwarz_lantern">Schwarz lantern</a> in food/drink packaging, not in the obvious way (it happens when you crumple a can end-on) but deliberately by the manufacturer. It doesn’t say why they did, though. Maybe because it looks cool.</p>
  </li>
  <li>
    <p><a href="https://cp4space.hatsya.com/2022/05/25/threelds/">Threelds</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108440193859317506">\(\mathbb{M}\)</a>).</span> I have no idea whether it’s useful for anything, but a threeld is a pair of fields where the multiplication operation on the inner one forms the addition on the outer one. The finite ones have inner order 3 and outer order 2, or inner order a Mersenne prime and outer order the adjacent power of two, but there also exist infinite ones with inner field of characteristic 0 and outer of characteristic 2.</p>
  </li>
  <li>
    <p>Roundup of recent <em>Quanta</em> popularizations and the research they come from <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108442789631056740">\(\mathbb{M}\)</a>):</span></p>

    <ul>
      <li>
        <p><a href="https://www.quantamagazine.org/impossible-seeming-surfaces-confirmed-decades-after-conjecture-20220602/">Near-optimal expansion for 2d surfaces</a>, based on “<a href="https://arxiv.org/abs/2107.05292">Near optimal spectral gaps for hyperbolic surfaces</a>”, by Will Hide and Michael Magee.</p>
      </li>
      <li>
        <p><a href="https://www.quantamagazine.org/mathematicians-transcend-geometric-theory-of-motion-20211209/">Inequality between cohomology rank and number of Hamiltonian flow orbits</a>, based on “<a href="https://arxiv.org/abs/2103.01507">Arnold conjecture and Morava K-theory</a>”, by Mohammed Abouzaid and Andrew J. Blumberg.</p>
      </li>
      <li>
        <p><a href="https://www.quantamagazine.org/graduate-students-side-project-proves-prime-number-conjecture-20220606/">Among pairwise-coprime sequences, primes maximize \(\sum 1/n_i\log n_i\)</a>, based on “<a href="https://arxiv.org/abs/2202.02384">A proof of the Erdős primitive set conjecture</a>”, by Jared Duker Lichtman.</p>
      </li>
      <li>
        <p><a href="https://www.quantamagazine.org/researchers-achieve-absurdly-fast-algorithm-for-network-flow-20220608/">Fast maximum flow algorithms</a>, based on “<a href="https://arxiv.org/abs/2203.00671">Maximum flow and minimum-cost flow in almost-linear time</a>”, by Li Chen, Rasmus Kyng, Yang P. Liu, Richard Peng, Maximilian Probst Gutenberg, and Sushant Sachdeva.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>Kirk Smith asks Mastodon: “<a href="https://scholar.social/@kirk/108439384108355311">Do people on here edit/write Wikipedia articles related to your field? and what’s your experience/motivation?</a>” There’s a conflict here between desiring academic credit for your work, and maintaining the protection of pseudonymity. But the real-world harassment that pseudonymity prevents is real, whereas I think the possibility of getting much academic credit for this sort of work is largely illusory.</p>
  </li>
  <li>
    <p>New Wikipedia article: <a href="https://en.wikipedia.org/wiki/Staircase_paradox">Staircase paradox</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108454961367151468">\(\mathbb{M}\)</a>),</span> on the familiar example of staircase curves in a unit square that uniformly converge to the diagonal of the square, while their lengths converge to the wrong number (\(2\), rather than \(\sqrt2\)). Somehow we don’t seem to have already had an article on this example. I’m sure there must be many more published sources on this example than the ones I used; if you think I missed something important, please let me know.</p>
  </li>
  <li>
    <p><a href="https://www.math.columbia.edu/~woit/wordpress/?p=12936">Physicists discover never-before seen particle sitting on a tabletop</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108465478766177772">\(\mathbb{M}\)</a>).</span> Peter Woit’s headline for this <em>Not Even Wrong</em> post repeats the breathless hype from the churnalism on a new condensed-matter-physics preprint, which promises applications to dark matter and quantum computing and turns out to be much much less. From the comments, it seems that the condensed matter physicists have been guilty of misapplying the tag “Higgs field” for a long time.</p>
  </li>
  <li>
    <p><a href="https://blogs.ams.org/beyondreviews/2021/07/18/yoshimura-crush-patterns/">Yoshimura crushing patterns on the <em>Inside MathSciNet</em> blog</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108469088531022175">\(\mathbb{M}\)</a>).</span>  I don’t think it’s accurate to say that a crush pattern and a crease pattern are synonyms, though. One is a description of the output of a crushing process; the other is an input to a folding process that guides you to put the folds into their intended places. The similarity of the crushing pattern and the Yoshimura fold is not coincidental but the purpose is different.</p>
  </li>
  <li>
    <p><a href="https://doi.org/10.1016/j.jclinepi.2022.05.019">Study of all open-access papers on BioMed Central from a month-long window</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108474483987356656">\(\mathbb{M}\)</a>,</span> <a href="https://news.ycombinator.com/item?id=31660239">via</a>) finds that although 42% claim their data to be available on reasonable request, only 7% actually responded and provided their data.</p>
  </li>
  <li>
    <p><a href="https://b-mehta.github.io/unit-fractions/">Formalization in Lean of Thomas Bloom’s proof</a> of the density version of the <a href="https://en.wikipedia.org/wiki/Erd%C5%91s%E2%80%93Graham_problem">Erdős–Graham problem</a> <span style="white-space: nowrap;">(<a href="https://mathstodon.xyz/@11011110/108484103209861017">\(\mathbb{M}\)</a>,</span> <a href="https://twitter.com/XenaProject/status/1536099892694859777">via</a>), according to which every set of integers with positive upper density includes the denominators of an Egyptian fraction representation of one. The blueprint appears to show Theorem 2 of <a href="https://arxiv.org/abs/2112.03726">Bloom’s preprint</a> as verified, but Theorem 3 (log density) still to go.</p>
  </li>
</ul></div>
    </content>
    <updated>2022-06-15T16:36:00Z</updated>
    <published>2022-06-15T16:36:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2022-06-23T01:33:18Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/088</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/088" rel="alternate" type="text/html"/>
    <title>TR22-088 |  Efficient decoding up to a constant fraction of the code length for asymptotically good quantum codes | 

	Anthony Leverrier, 

	Gilles Zémor</title>
    <summary>We introduce and analyse an efficient decoder for the quantum Tanner codes that can correct adversarial errors of linear weight. Previous decoders for quantum low-density parity-check codes could only handle adversarial errors of weight $O(\sqrt{n \log n})$. We also work on the link between quantum Tanner codes and the Lifted Product codes of Panteleev and Kalachev, and show that our decoder can be adapted to the latter. The decoding algorithm alternates between sequential and parallel procedures and converges in linear time.</summary>
    <updated>2022-06-15T14:56:24Z</updated>
    <published>2022-06-15T14:56:24Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-06-23T03:37:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/06/14/lecturer-senior-lecturer-in-algorithms-at-university-of-sheffield-apply-by-july-5-2022/</id>
    <link href="https://cstheory-jobs.org/2022/06/14/lecturer-senior-lecturer-in-algorithms-at-university-of-sheffield-apply-by-july-5-2022/" rel="alternate" type="text/html"/>
    <title>Lecturer/Senior Lecturer in Algorithms at University of Sheffield (apply by July 5, 2022)</title>
    <summary>The Department of Computer Science at the University of Sheffield UK aims to hire one Lecturer/Senior Lecturer (Assistant/Associate professor) in an area including, but not limited to complexity, algorithm, AI, and AGT. The role is supported with a generous startup package, including funding for conference travel and equipment and a PhD scholarship. Website: https://www.jobs.ac.uk/job/CQL118/lecturer-senior-lecturer-in-algorithms Email: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Computer Science at the University of Sheffield UK aims to hire one Lecturer/Senior Lecturer (Assistant/Associate professor) in an area including, but not limited to complexity, algorithm, AI, and AGT. The role is supported with a generous startup package, including funding for conference travel and equipment and a PhD scholarship.</p>
<p>Website: <a href="https://www.jobs.ac.uk/job/CQL118/lecturer-senior-lecturer-in-algorithms">https://www.jobs.ac.uk/job/CQL118/lecturer-senior-lecturer-in-algorithms</a><br/>
Email: s.mukhopadhyay@sheffield.ac.uk</p></div>
    </content>
    <updated>2022-06-14T12:39:10Z</updated>
    <published>2022-06-14T12:39:10Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-06-23T03:37:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2022/06/14/lecturer-in-algorithms-at-university-of-sheffield-apply-by-july-5-2022/</id>
    <link href="https://cstheory-jobs.org/2022/06/14/lecturer-in-algorithms-at-university-of-sheffield-apply-by-july-5-2022/" rel="alternate" type="text/html"/>
    <title>Lecturer in Algorithms at University of Sheffield (apply by July 5, 2022)</title>
    <summary>The Department of Computer Science at the University of Sheffield UK aims to hire two Lecturers (Assistant professor) in an area including, but not limited to complexity, algorithm, AI, and AGT. The role is supported with a generous startup package, including funding for conference travel and equipment and a PhD scholarship. Website: https://www.jobs.ac.uk/job/CQL110/lecturer-in-algorithms-two-posts Email: s.mukhopadhyay@sheffield.ac.uk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Computer Science at the University of Sheffield UK aims to hire two Lecturers (Assistant professor) in an area including, but not limited to complexity, algorithm, AI, and AGT. The role is supported with a generous startup package, including funding for conference travel and equipment and a PhD scholarship.</p>
<p>Website: <a href="https://www.jobs.ac.uk/job/CQL110/lecturer-in-algorithms-two-posts">https://www.jobs.ac.uk/job/CQL110/lecturer-in-algorithms-two-posts</a><br/>
Email: s.mukhopadhyay@sheffield.ac.uk</p></div>
    </content>
    <updated>2022-06-14T12:36:59Z</updated>
    <published>2022-06-14T12:36:59Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2022-06-23T03:37:44Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2022/06/14/analysts-minimum-spanning</id>
    <link href="https://11011110.github.io/blog/2022/06/14/analysts-minimum-spanning.html" rel="alternate" type="text/html"/>
    <title>The analyst’s minimum spanning tree</title>
    <summary>Infinite sets of points in the Euclidean plane, even discrete sets, do not always have Euclidean minimum spanning trees. For instance, consider the points with coordinates</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Infinite sets of points in the Euclidean plane, even discrete sets, do not always have <a href="https://en.wikipedia.org/wiki/Euclidean_minimum_spanning_tree">Euclidean minimum spanning trees</a>. For instance, consider the points with coordinates</p>

\[\left(i, \pm\left(1+\frac1i\right)\right),\]

<p>for positive <span style="white-space: nowrap;">integers \(i\).</span> You can connect the <span style="white-space: nowrap;">positive-\(y\)</span> points and the <span style="white-space: nowrap;">negative-\(y\)</span> points into two chains with edges of length less than two, but then you have to pick one edge of length greater than two to span from one chain to the other. Whichever edge you choose, the next edge along would always be a better choice. So a tree that minimizes the multiset of its edge weights (as finite minimum spanning trees do) does not exist for this example. And as the same example shows, the sum of edge weights may be infinite, so how can we use minimization of this sum to define a tree?</p>

<p style="text-align: center;"><img alt="Discrete infinite set of points with no Euclidean minimum spanning tree" src="https://11011110.github.io/blog/assets/2022/ladder-no-mst.svg"/></p>

<p>Despite that, here’s a construction that works for any <a href="https://en.wikipedia.org/wiki/Compact_space">compact set</a>, even one with infinitely many components, and that generalizes easily to higher-dimensional Euclidean spaces. I think it deserves to be called the Euclidean minimum spanning tree. Given a compact <span style="white-space: nowrap;">set \(C\),</span> consider every partition \(C=A\cup (C\setminus A)\) of \(C\) into two disjoint nonempty compact subsets. For each such partition, find a line segment \(s_A\) of minimum length with endpoints in \(A\) <span style="white-space: nowrap;">and \(C\setminus A\),</span> breaking ties lexicographically by coordinates. By the assumed compactness of \(A\) <span style="white-space: nowrap;">and \(C\setminus A\),</span> such a line segment exists. Let \(T_C\) be the union of \(C\) itself and of all line segments obtained in this way. For example, the union of a triangle, square, and circle shown below has three partitions into two nonempty compact subsets, separating one of these three shapes from the other two. Two of these partitions choose the diagonal pink segment as their shortest connection, and the third partition chooses the horizontal pink segment. So in this case, \(T_C\) consists of the three blue given shapes and two pink segments.</p>

<p style="text-align: center;"><img alt="Minimum spanning tree of a circle, square, and triangle" src="https://11011110.github.io/blog/assets/2022/trisquircle.svg"/></p>

<p>When \(C\) is a finite point set, \(T_C\) is just a Euclidean minimum spanning tree. When \(C\) has finitely many connected components, like the example above, \(T_C\) is again a minimum spanning tree, for the component-component distances. In the general case, \(T_C\) still has many of the familiar properties of Euclidean minimum spanning trees:</p>

<ul>
  <li>
    <p>It consists of the input and a collection of line segments connecting pairs of input points, by construction.</p>
  </li>
  <li>
    <p>It is a <a href="https://en.wikipedia.org/wiki/Connected_space">connected set</a>. Topologically, this means that it cannot be covered by two disjoint open sets that both have a nonempty intersection with it. (This is different from being path-connected, a stronger property.) Any nontrivial open disjoint cover of \(C\) would be spanned by a line segment from one set to the other, and no new disjoint covers can separate these line segments from their endpoints.</p>
  </li>
  <li>
    <p>For any added <span style="white-space: nowrap;">segment \(s_A\),</span> the intersection of two disks with that segment as radius (a “lune”) has no point of \(C\) in its interior. Any interior point would form one end of a shorter connecting segment between \(A\) <span style="white-space: nowrap;">and \(C\setminus A\),</span> with the other end at an endpoint <span style="white-space: nowrap;">of \(s_A\).</span> No two added segments can cross without violating the empty lune property.</p>

    <p style="text-align: center;"><img alt="The empty lune of an edge" src="https://11011110.github.io/blog/assets/2022/vesica.svg"/></p>
  </li>
  <li>
    <p>For any added <span style="white-space: nowrap;">segment \(s_A\),</span> the open rhombus with angles \(60^\circ\) and \(120^\circ\) having \(s_A\) as its long diagonal is disjoint from the rhombi formed in the same way from the other segments. Any two overlapping rhombi would allow the longer of the two segments they come from to be replaced by a shorter segment crossing the same compact partition, on a three-segment path connecting its endpoints via the other segment endpoints. Because these non-overlapping rhombi cover a region of bounded area, the squared segment lengths have a bounded sum, and only finitely many segments can be longer than any given length threshold.</p>

    <p style="text-align: center;"><img alt="An infinite minimum spanning tree and its empty rhombi" src="https://11011110.github.io/blog/assets/2022/ivy-rhombs.svg"/></p>
  </li>
  <li>
    <p>The union of \(C\) with any subset of added segments is compact. If \(p\) is a limit point of a <span style="white-space: nowrap;">sequence \(\sigma_i\)</span> of points in this union, it must either lie in the empty rhombus of a segment (in which case it can only be a point of the same segment), or it is a limit point of a sequence of points <span style="white-space: nowrap;">in \(C\),</span> obtained by replacing each point in \(\sigma_i\) that is interior to a segment by the nearest segment endpoint. This replacement only increases the distance from the replaced point to \(p\) by a constant factor, which does not affect convergence. By compactness the replaced sequence converges to a point <span style="white-space: nowrap;">in \(C\).</span></p>
  </li>
  <li>
    <p>For <span style="white-space: nowrap;">any \(i\),</span> the set \(T_i\) of the largest \(i\) added segments (with the same tie-breaking order) are edges of a minimum spanning tree for a family of \(i-1\) sets. To construct these sets, find the components of the union of \(C\) with all shorter segments, and intersect each component <span style="white-space: nowrap;">with \(C\).</span> None of these components can cross between \(A\) and \(C\setminus A\) for any <span style="white-space: nowrap;">edge \(s_A\in T_i\).</span> Because adding \(T_i\) connects all these components, there can be at most \(i-1\) components. Each edge in \(T_i\) is shortest (with a consistent tie-breaking rule) across some partition of the components, one of the ways of determining the edges in a finite minimum spanning tree. In particular, \(T_C\) is minimally connected: removing any edge \(s_A\in S_i\) separates some of the components from each other.</p>
  </li>
  <li>
    <p>\(T_C\) has the minimum sum of squared edge lengths of all collections of line segments between points of \(C\) that <span style="white-space: nowrap;">connect \(C\).</span> To see this, consider any other connecting set \(X\) of line segments with a finite sum of squared edge lengths. Truncate the sorted sequence of edges of \(T_C\) to a finite initial <span style="white-space: nowrap;">sequence \(T_i\)</span> such that the rest of the sequence has negligible sum of squares. Because \(T_i\) is a minimum spanning tree of its components, and \(X\) connects those same components (perhaps redundantly), the sequence of edge lengths in \(T_i\) is, step for step, less than or equal to the sorted sequence of lengths <span style="white-space: nowrap;">in \(X\).</span></p>
  </li>
</ul>

<p>There may exist other sets of line segments that connect \(C\) with the same sum of squared edge lengths but they all are minimally connected, with the same sequence of edge lengths, the same empty lune and empty rhombus properties, and the same property that their initial sequences form finite minimum spanning trees of their components.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/108476695929961897">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2022-06-14T08:47:00Z</updated>
    <published>2022-06-14T08:47:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2022-06-23T01:33:18Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=20165</id>
    <link href="https://rjlipton.wpcomstaging.com/2022/06/13/sorting-and-proving/" rel="alternate" type="text/html"/>
    <title>Sorting and Proving</title>
    <summary>A proof tells us where to concentrate our doubts—Morris Kline Tony Hoare is also known informally as Sir Charles Antony Richard Hoare. He has made key contributions to programming languages, algorithms, operating systems, formal verification, and concurrent computing. He won the 1980 Turing Award for “Fundamental contributions to the definition and design of programming languages.” […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>A proof tells us where to concentrate our doubts—Morris Kline</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p>
Tony Hoare is also known informally as Sir Charles Antony Richard Hoare. He has made key contributions to programming languages, algorithms, operating systems, formal verification, and concurrent computing. </p>
<p>
He won the <a href="https://amturing.acm.org/award_winners/hoare_4622167.cfm">1980 Turing Award</a> for <i>“Fundamental contributions to the definition and design of programming languages.”</i> </p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2022/06/13/sorting-and-proving/th-2/" rel="attachment wp-att-20168"><img alt="" class="aligncenter wp-image-20168" height="200" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2022/06/th.png?resize=200%2C200&amp;ssl=1" width="200"/></a></p>
<p><span id="more-20165"/></p>
<p>
</p><p/><h2> Quicksort </h2><p/>
<p/><p>
Hoare invented a new sorting algorithm in 1959. It was not later directly cited in his winning of the Turing Award—see above. Back then Hoare was a visiting student at Moscow State University. His student project at the time needed to sort words in Russian sentences before looking them up in a Russian-English dictionary, which was in alphabetical order on a magnetic tape. He tried insertion sort but it was too slow, and so he invented a new sorting algorithm—it is now called <a href="https://en.wikipedia.org/wiki/Quicksort">Quicksort</a>. Amazing. </p>
<p>
On his return to England, he was asked to write code for <a href="https://en.wikipedia.org/wiki/Shellsort">Shellsort</a>. Hoare mentioned to his boss that he knew of a faster algorithm and his boss bet sixpence that he did not. His boss ultimately accepted that he had lost the bet. Quicksort is well named—it is a winner.</p>
<p>
</p><p/><h2> And Still Champion </h2><p/>
<p/><p>
The most amazing thing to me—Ken writing this and the next section—-is that Quicksort has remained the champion sorter. <a href="https://en.wikipedia.org/wiki/Merge_sort">Mergesort</a> was earlier but <a href="https://en.wikipedia.org/wiki/Heapsort">Heapsort</a> came five years later. No one has improved on Hoare’s idea to focus on <em>swaps</em> that improve the sortedness of two elements at once, both jumping over a guiding element called the <em>pivot</em>.</p>
<p>
Mergesort and Heapsort guarantee <img alt="{O(n\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> time to sort <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> items, whereas Quicksort does not. Yet careful implementations of Quicksort almost always evade slow outcomes. Then they beat Mergesort and Heapsort and all other sorting methods cleanly in terms of the constant under the “<img alt="{O}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.” </p>
<p>
The algorithms-and-data-structures course I taught at UB this past term includes sorting algorithms. I have brought together and polished code instances I showed during the term to compare these three algorithms in one <a href="https://cse.buffalo.edu/~regan/cse250/DataStructures/Sorts.scala">code file</a>. It is in the <a href="https://en.wikipedia.org/wiki/Scala_(programming_language)">Scala</a> programming language. The program has options described at the top, such as setting the “tradeoff point” <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> so that on recursive calls to sort pieces of size at most <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, the code uses Insertion Sort instead. It counts comparisons and copies of items separately, as well as report the time in milliseconds. A swap counts as three not two copies. </p>
<p>
Here is an example run on the English <a href="https://cse.buffalo.edu/~regan/cse250/DataStructures/WarAndPeace.txt">text</a> of <em>War and Peace</em> using <img alt="{m = 16}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm+%3D+16%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for both Quicksort and Mergesort. I wanted to salute Hoare’s application by using the original Russian text, but every file of it I found was densely footnoted. The Russian version doesn’t begin in Russian anyway. I’ve edited the output slightly.<br/>
<font size="-1"><br/>
<code><br/>
metallica&lt;~&gt; scala Sorts WarAndPeace.txt 16 medianRandom3 011000<br/>
Bits=(randomize)(makeHeap)(part for ==)(use Selsort)(make distinct)(pad len)<br/>
Sorting 562,603 items, tradepoint 16 to insertion sort<br/>
QuickSort pivot is medianRandom3 for 3-way part; heapSort uses makeHeap</code></font></p><font size="-1">
<p>MergeSort made 8,874,241 comparisons and 9,001,648 copies<br/>
plus 1,447,593 comparisons and 2,061,296 copies for insertionSort at bottom<br/>
In total: made 10,321,834 comparisons and 11,062,944 copies<br/>
Time for mergeSort: 3241 milliseconds</p>
<p>QuickSort made 5,832,042 comparisons and 16,390,570 copies<br/>
plus 74,992 comparisons and 108,450 copies for insertionSort at bottom<br/>
In total: made 5,907,034 comparisons and 16,499,020 copies<br/>
Time for quickSort: 1212 milliseconds</p>
</font><p><font size="-1">HeapSort made 19,977,762 comparisons and 31,494,294 copies<br/>
Time for heapSort: 1885 milliseconds<br/>
</font></p>
<p/><p><br/>
This used the strategy of selecting the median of three randomly sampled elements of the array piece to be sorted as the pivot. Quicksort runs quickest when the pivot is the median, but it takes too long to find the exact median at each level of recursion. My program has the option “<font size="+1"><tt>ninther</tt></font>” to select the median of three such median-of-3 samples, as <a href="https://www.johndcook.com/blog/2009/06/23/tukey-median-ninther/">proposed</a> by John Tukey. Try my code to see if <font size="+1"><tt>ninther</tt></font> works faster and with fewer comparisons on your system. </p>
<p>
</p><p/><h2> Empirical Testing and Proving </h2><p/>
<p/><p>
The person whose 1975 PhD <a href="https://sedgewick.io/wp-content/themes/sedgewick/papers/1975Quicksort.pdf">dissertation</a> studied the performance of Quicksort implementations with rigor and depth was none other than Bob Sedgewick, whose retirement party Dick recently <a href="https://rjlipton.wpcomstaging.com/2022/05/04/sedgewick-to-emeritus-status/">covered</a>. This led into Bob’s famous textbook <a href="https://sedgewick.io/books/algorithms/"><em>Algorithms</em></a>, which is now in its fourth edition with Kevin Wayne as co-author. Bob’s separate <a href="https://github.com/chrswt/algorithms-sedgewick/blob/master/notes/2.3-quicksort.md">notes</a> accompanying the book point out an aspect vital to running on inputs like <em>War and Peace</em> that have many identical (keys of) items. The simple partition for a pivot <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> into </p>
<p>
<code><br/>
               [ elements &lt;= p ][ p ][ elements &gt;= p ]<br/>
</code></p>
<p/><p><br/>
winds up doing many extra comparisons of keys equal to <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. The extra initial effort to determine a 3-way partition</p>
<p>
<code><br/>
               [ elements &lt; p ][ elements == p ][ elements &gt; p ]<br/>
</code></p>
<p/><p><br/>
pays off in the recursion, which only needs to be on the outer two segments. Without this trick, Quicksort—on even a relatively high-entropy source like <em>War and Peace</em>—degrades to be worse than the other two algorithms. </p>
<p>
My code allows making all words distinct by setting the fifth bit. Then Quicksort makes more comparisons and copies than Mergesort. Yet it still runs almost 40% faster in my tests. As explained in general <a href="https://www.geeksforgeeks.org/quick-sort-vs-merge-sort">here</a>, this is because the Quicksort gives better cache locality. Heapsort likewise runs in-place, but not with locality. </p>
<p>
The assertions itemized in this section are unimpeachable. But in what sense, and to what degree, are they <b>formally provable</b>? </p>
<p>
There is also a difference from the criterion of <b>reproducibility</b> in the sciences. The environmental conditions for reproducing an experiment are presumed to be closed and given. The primacy of Quicksort, however, applies to processing architectures that were unknown at the time of Sedgewick’s 1975 thesis, let alone Hoare’s 1959 concept. It is a more open-ended prediction that if you run mine or similar code on your system—in machine environments I may know nothing about—it will give much the same time performance.</p>
<p>
Now back to Dick, about Hoare’s take on formally proving properties of programs.</p>
<p>
</p><p/><h2> Hoare Logic </h2><p/>
<p/><p>
Hoare invented a new <a href="https://en.wikipedia.org/wiki/Hoare_logic">logic</a> for reasoning about the correctness of computer programs ten years later in 1969. It was directly counted toward his winning of the Turing Award. It was based on original ideas created by Robert Floyd, who had published a system for flowcharts. The logic is now known as Hoare logic—see <a href="http://sunnyday.mit.edu/16.355/Hoare-CACM-69.pdf">this</a> for details. This is one of the most influential papers on the theory of programming. In this paper Hoare showed how to reason about program execution using logical specifications of statement behavior that has become known as <a href="https://www.cs.cmu.edu/~aldrich/courses/654-sp09/notes/3-hoare-notes.pdf">Hoare triples</a>.</p>
<p>
A Hoare triple has three parts, a precondition <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, a program statement or series of statements <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and a postcondition <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. It’s is usually written in the form 	</p>
<p align="center"><img alt="\displaystyle  \{P\} S \{Q\} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5C%7BP%5C%7D+S+%5C%7BQ%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>The meaning is “if <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is true before <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is executed, and if the execution of <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> terminates, then <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is true afterwards”. Note that the triple does not assert that <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> will terminate; that requires a separate proof. As a simple example: 	</p>
<p align="center"><img alt="\displaystyle  \{x+1=43\}y:=x+1\{y=43\} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5C%7Bx%2B1%3D43%5C%7Dy%3A%3Dx%2B1%5C%7By%3D43%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
A key motivation for Hoare Logic is to be able to prove the correctness of real systems with real programs. The fact that Hoare Logic is possible is clear. It is possible to use it to <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.75.1739&amp;rep=rep1&amp;type=pdf">prove</a> Quicksort, for example. But it is less clear whether or not we will be able to apply formal methods to complex practical systems. This is a topic that I have thought about for decades. </p>
<p>
</p><p/><h2> A Codex </h2><p/>
<p/><p>
Saturday’s Washington Post has a tech <a href="https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/">article</a>, “The Google engineer who thinks the company’s AI has come to life.” This prompted us to visit a long <a href="https://www.nytimes.com/2022/04/15/magazine/ai-language.html">article</a> in the April 15 New York Times Magazine on Open AI’s <a href="https://en.wikipedia.org/wiki/GPT-3">GPT-3</a> language engine. </p>
<p>
GPT-3 works by playing a game of <em>guess the next word</em> in a phrase. This is akin to <em>guess the next move</em> in chess and other games, and we will have more to say about it. For an example with wider context, suppose we fed it this post up before this section, and then gave it “<b>A Cod-</b>” as the partially completed section title. Since this comes at the end, GPT-3 might guess <b>coda</b>. Or since this is an addendum, maybe <b>codicil</b>. A <b>codex</b>, on the other hand, is a large manuscript book.</p>
<p>
In fact, we do mean <i>codex</i>, or rather <a href="https://en.wikipedia.org/wiki/OpenAI_Codex">Codex</a>, which is an offshoot of GPT-3 for generating code in a wide variety of programming languages. Its emergence creates a new riff on our recent <a href="https://rjlipton.wpcomstaging.com/2022/04/10/discussion-about-proving-again/">discussion</a> on proving and how <a href="https://rjlipton.wpcomstaging.com/2022/05/23/hilberts-lost-problem/">software</a> projects have stayed robust while growing far beyond the scale on which they can be formally proved. Now <a href="https://openai.com/blog/codex-apps/">Codex</a> ventures to write one’s software by gleaning the intent from one’s prose specification—by drawing on millions of available coding projects that give relatable specifications. </p>
<p>
Is program output from Codex <em>proven</em>—or <em>provable</em>? This will be a challenge. It may be more feasible to integrate Codex with projects like the <a href="https://vst.cs.princeton.edu/">Verified Software Toolchain</a> led Andrew Appel, Lennart Beringer, William Mansky, and Qinshi Wang. This is at any rate further grist for discussion.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Take a look at <a href="https://blog.computationalcomplexity.org/2021/06/i-went-to-debate-about-program-verif.html">this</a> for comments about the recent Harry Lewis <a href="https://scp.cc.gatech.edu/2021/05/26/debate-that-changed-programming-living-history/">debate</a>. This was based on our <a href="https://www.cs.umd.edu/~gasarch/BLOGPAPERS/social.pdf">paper</a>. </p>
<p>
What do you think?</p>
<p/></font></font></div>
    </content>
    <updated>2022-06-13T04:57:48Z</updated>
    <published>2022-06-13T04:57:48Z</published>
    <category term="algorithms"/>
    <category term="fast"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="Results"/>
    <category term="Teaching"/>
    <category term="Algorithms"/>
    <category term="Codex"/>
    <category term="formal methods"/>
    <category term="GPT-3"/>
    <category term="Hoare logic"/>
    <category term="Quicksort"/>
    <category term="Tony Hoare"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2022-06-23T03:37:43Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-414280661681721253</id>
    <link href="http://blog.computationalcomplexity.org/feeds/414280661681721253/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/06/i-am-surprised-that-shortest-vector.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/414280661681721253" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/3722233/posts/default/414280661681721253" rel="self" type="application/atom+xml"/>
    <link href="http://blog.computationalcomplexity.org/2022/06/i-am-surprised-that-shortest-vector.html" rel="alternate" type="text/html"/>
    <title>I am surprised that the Shortest Vector Problem is not known to be NP-hard, but perhaps I am wrong</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div><br/></div><div>A lattice L in R^n is a discrete subgroup of R^n. </div><div><br/></div><div>Let p IN [1,infinty)</div><div><br/></div><div>The<i> p-norm of a vector </i>x=(x_1,...,x_n) IN R^n is</div><div><br/></div><div>                                          ||x||_p=(|x_1|^p + ... + |x_n|^p)^{1/p}.</div><div><br/></div><div>Note that p=2 yields the standard Euclidean distance.</div><div><br/></div><div>If p=infinity  then ||x||_p=max_{1 LE  i LE n} |x_i|.</div><div><br/></div><div>Let p IN [1,infinity]</div><div><br/></div><div>The Shortest Vector Problem in norm p (SVP_p) is as follows:</div><div><br/></div><div>INPUT A lattice L specified by a basis.</div><div><br/></div><div>OUTPUT Output the shortest vector in that basis using the p-norm.</div><div><br/></div><div>I was looking at lower bounds on approximating this problem and just assumed the original problem was NP-hard. Much to my surprise either (a) its not known, or (b) it is known and I missed in in my lit search. I am hoping that comments on this post will either verify (a) or tell me (b) with refs. </div><div><br/></div><div>Here is what I found:</div><div><br/></div><div>Peter van Emde Boas in 1979  showed that SVP_infinity  is NP-hard.   </div><div>(See <a href="https://cs.stackexchange.com/questions/33828/np-completeness-of-closest-vector-problem">here</a> for a page that has a link to the paper.  I was unable to post the link directly. Its where it says <i>I found</i> <i>the original paper.) </i>He conjectured that for all p GE 1 the problem is NP-hard. </div><div><br/></div><div><br/></div><div>Miklos Ajtai in 1998 showed that SVP_2 is NP-hard under randomized reductions.  (See <a href="https://doi.org/10.1145/276698.276705">here</a>)</div><div><br/></div><div>There are other results by Subhash Khot in 2005  (see <a href="https://doi.org/10.1145/1089023.1089027">here</a>)  and Divesh Aggarwal et al. in 2021 (see <a href="https://doi.org/10.1137/1.9781611976465.109">here</a>)  (Also see the references in those two papers.)  about lower bounds on approximation using a variety of assumptions. Aggarwal's paper in unusual in that it shows hardness results for all p except p even; however, this is likely a function of the proof techniques and not of reality. Likely these problems are hard for all p.</div><div><br/></div><div>But even after all of those great papers it seems that the  the statement:</div><div><br/></div><div>                For all p IN [1,infinity] SVP_p is NP-hard</div><div><br/></div><div>is a conjecture, not a theorem. I wonder if van Emde Boas would be surprised. If he reads this blog, maybe I'll find out. If you know him then ask him to comment, or comment yourself. </div><div><br/></div><div>SO is that still a conjecture OR have I missed something?</div><div><br/></div><div>(Oddly enough, my own blog post <a href="https://blog.computationalcomplexity.org/search?q=shortest+vector">here</a> (item 5)  indicates SVP_p  is NP-hard; however, </div><div>I have not been able to track down the reference.)</div><div><br/></div></div>
    </content>
    <updated>2022-06-12T18:34:00Z</updated>
    <published>2022-06-12T18:34:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="http://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2022-06-22T23:07:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://scottaaronson.blog/?p=6479</id>
    <link href="https://scottaaronson.blog/?p=6479" rel="alternate" type="text/html"/>
    <link href="https://scottaaronson.blog/?p=6479#comments" rel="replies" type="text/html"/>
    <link href="https://scottaaronson.blog/?feed=atom&amp;p=6479" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Alright, so here are my comments…</title>
    <summary xml:lang="en-US">… on Blake Lemoine, the Google engineer who became convinced that a machine learning model had become sentient, contacted federal government agencies about it, and was then fired placed on administrative leave for violating Google’s confidentiality policies. (1) I don’t think Lemoine is right that LaMDA is at all sentient, but the transcript is so […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>… on Blake Lemoine, the Google engineer who <a href="https://www.huffpost.com/entry/blake-lemoine-lamda-sentient-artificial-intelligence-google_n_62a5613ee4b06169ca8c0a2e?d_id=3887326&amp;ref=bffbhuffpost&amp;ncid_tag=fcbklnkushpmg00000063&amp;utm_medium=Social&amp;utm_source=Facebook&amp;utm_campaign=us_main&amp;fbclid=IwAR0o5U4wv2cDP8o3XIAekj2Xh5wVPZVzVhyH696N8tnLv_m-YXtUDt0tFNU">became convinced</a> that a machine learning model had become sentient, contacted federal government agencies about it, and was then <s>fired</s> placed on administrative leave for violating Google’s confidentiality policies.</p>



<p>(1) I don’t think Lemoine is right that <a href="https://blog.google/technology/ai/lamda/">LaMDA</a> is at all sentient, but the <a href="https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917">transcript</a> is so mind-bogglingly impressive that I did have to stop and think for a second! Certainly, if you sent the transcript back in time to 1990 or whenever, even an expert reading it might say, yeah, it looks like by 2022 AGI has more likely been achieved than not (“but can I run my own tests?”).  Read it for yourself, if you haven’t yet.</p>



<p>(2) Reading Lemoine’s <a href="https://cajundiscordian.medium.com/">blog</a> and <a href="https://twitter.com/cajundiscordian">Twitter</a> this morning, he holds many views that I disagree with, not just about the sentience of LaMDA. Yet I’m touched and impressed by how principled he is, and I expect I’d hit it off with him if I met him. I wish that a solution could be found where Google wouldn’t fire him.</p></div>
    </content>
    <updated>2022-06-12T17:56:29Z</updated>
    <published>2022-06-12T17:56:29Z</published>
    <category scheme="https://scottaaronson.blog" term="Metaphysical Spouting"/>
    <category scheme="https://scottaaronson.blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://scottaaronson.blog/?feed=atom</id>
      <icon>https://scottaaronson.blog/wp-content/uploads/2021/10/cropped-Jacket-32x32.gif</icon>
      <link href="https://scottaaronson.blog" rel="alternate" type="text/html"/>
      <link href="https://scottaaronson.blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2022-06-19T05:44:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/087</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/087" rel="alternate" type="text/html"/>
    <title>TR22-087 |  Depth-$d$ Threshold Circuits vs. Depth-$(d + 1)$ AND-OR Trees | 

	Pooya Hatami, 

	William Hoza, 

	Avishay Tal, 

	Roei Tell</title>
    <summary>For $n \in \mathbb{N}$ and $d = o(\log \log n)$, we prove that there is a Boolean function $F$ on $n$ bits and a value $\gamma = 2^{-\Theta(d)}$ such that $F$ can be computed by a uniform depth-$(d + 1)$ $\text{AC}^0$ circuit with $O(n)$ wires, but $F$ cannot be computed by any depth-$d$ $\text{TC}^0$ circuit with $n^{1 + \gamma}$ wires. This bound matches the current state-of-the-art lower bounds for computing explicit functions by threshold circuits of depth $d &gt; 2$, which were previously known only for functions outside $\text{AC}^0$ such as the parity function. Furthermore, in our result, the $\text{AC}^0$ circuit computing $F$ is a monotone *read-once formula* (i.e., an AND-OR tree), and the lower bound holds even in the average-case setting with respect to advantage $n^{-\gamma}$.

Our proof builds on the *random projection* procedure of Håstad, Rossman, Servedio, and Tan, which they used to prove the celebrated average-case depth hierarchy theorem for $\text{AC}^0$ (J. ACM, 2017). We show that under a modified version of their projection procedure, any depth-$d$ threshold circuit with $n^{1 + \gamma}$ wires simplifies to a near-trivial function, whereas an appropriately parameterized AND-OR tree of depth $d + 1$ maintains structure.</summary>
    <updated>2022-06-12T02:57:47Z</updated>
    <published>2022-06-12T02:57:47Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-06-23T03:37:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://grigory.github.io/blog/theory-jobs-2022</id>
    <link href="http://grigory.github.io/blog/theory-jobs-2022/" rel="alternate" type="text/html"/>
    <title xml:lang="en">Theory Jobs 2022</title>
    <content type="xhtml" xml:lang="en"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://docs.google.com/spreadsheets/d/1BdHrWYT6F7je4lINqS14Fu0cOlkyrRhy2nkuAtNn2-c/edit?usp=sharing">Here is a link</a> to a crowdsourced spreadsheet created to collect information about theory hires this year. 
Rules for the spreadsheet have been copied from previous years and all edits to the document are anonymized. Please, feel free to contact me directly or post a comment if you have any suggestions about the rules.</p>
<ul>
 <li>You are welcome to add yourself, or people your department has hired. </li>
 <li>Separate sheets for faculty, industry and postdocs/visitors. </li>
 <li>Hires should be connected to theoretical computer science, broadly defined.</li>
 <li>Only add jobs that you are <b>absolutely sure have been offered and accepted</b>. This is not the place for speculation and rumors. Please, be particularly careful when adding senior hires (people who already have an academic or industrial job) -- end dates of their current positions might be still in the future. </li>
</ul>


  <p><a href="http://grigory.github.io/blog/theory-jobs-2022/">Theory Jobs 2022</a> was originally published by Grigory Yaroslavtsev at <a href="http://grigory.github.io/blog">The Big Data Theory</a> on June 11, 2022.</p></div>
    </content>
    <updated>2022-06-11T00:00:00Z</updated>
    <published>2022-06-11T00:00:00Z</published>
    <author>
      <name>Grigory Yaroslavtsev</name>
      <email>grigory@grigory.us</email>
      <uri>http://grigory.github.io/blog</uri>
    </author>
    <source>
      <id>http://grigory.github.io/blog/</id>
      <author>
        <name>Grigory Yaroslavtsev</name>
        <email>grigory@grigory.us</email>
        <uri>http://grigory.github.io/blog/</uri>
      </author>
      <link href="http://grigory.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="http://grigory.github.io/blog" rel="alternate" type="text/html"/>
      <title xml:lang="en">The Big Data Theory</title>
      <updated>2022-06-11T15:23:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://scottaaronson.blog/?p=6457</id>
    <link href="https://scottaaronson.blog/?p=6457" rel="alternate" type="text/html"/>
    <link href="https://scottaaronson.blog/?p=6457#comments" rel="replies" type="text/html"/>
    <link href="https://scottaaronson.blog/?feed=atom&amp;p=6457" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Computer scientists crash the Solvay Conference</title>
    <summary xml:lang="en-US">Thanks so much to everyone who sent messages of support following my last post! I vowed there that I’m going to stop letting online trolls and sneerers occupy so much space in my mental world. Truthfully, though, while there are many trolls and sneerers who terrify me, there are also some who merely amuse me. […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Thanks so much to everyone who sent messages of support following my <a href="https://scottaaronson.blog/?p=6444">last post</a>!  I vowed there that I’m going to stop letting online trolls and sneerers occupy so much space in my mental world.  Truthfully, though, while there <em>are</em> many trolls and sneerers who terrify me, there are also some who merely amuse me.  A good example of the latter came a few weeks ago, when an anonymous commenter calling themselves “String Theorist” submitted the following:</p>



<blockquote class="wp-block-quote"><p>It’s honestly funny to me when you [Scott] call yourself a “nerd” or a “prodigy” or whatever <em>[I don’t recall ever calling myself a “prodigy,” which would indeed be cringe, though “nerd” certainly —SA]</em>, as if studying quantum computing, which is essentially nothing more than glorified linear algebra, is such an advanced intellectual achievement. For what it’s worth I’m a theoretical physicist, I’m in a completely different field, and I was still able to learn Shor’s algorithm in about half an hour, that’s how easy this stuff is. I took a look at some of your papers on arXiv and the math really doesn’t get any more advanced than linear algebra. To understand quantum circuits about the most advanced concept is a tensor product which is routinely covered in undergraduate linear algebra. Wheras in my field of string theory grasping, for instance, holographic dualities relating confirmal field theories and gravity requires vastly more expertise (years of advanced study). I actually find it pretty entertaining that you’ve said yourself you’re still struggling to understand QFT, which most people I’m working with in my research group were first exposed to in undergrad <img alt="&#x1F609;" class="wp-smiley" src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f609.png" style="height: 1em;"/>  The truth is we’re in entirely different leagues of intelligence (“nerdiness”) and any of your qcomputing papers could easily be picked up by a first or second year math major. It’s just a joke that this is even a field (quantum complexity theory) with journals and faculty when the results in your papers that I’ve seen are pretty much trivial and don’t require anything more than undergraduate level maths.</p></blockquote>



<p>Why does this sort of trash-talk, reminiscent of <a href="https://en.wikipedia.org/wiki/Lubo%C5%A1_Motl">Luboš Motl</a>, no longer ruffle me?  Mostly because the boundaries between quantum computing theory, condensed matter physics, and quantum gravity, which were never clear in the first place, have steadily gotten fuzzier.  Even in the 1990s, the field of quantum computing attracted amazing physicists—folks who definitely <em>do</em> know quantum field theory—such as Ed Farhi, John Preskill, and Ray Laflamme.  Decades later, it would be fair to say that the physicists have banged their heads against many of the same questions that we computer scientists have banged <em>our</em> heads against, oftentimes in collaboration with us.  And yes, there were cases where actual knowledge of particle physics gave physicists an advantage—with some famous examples being the algorithms of Farhi and collaborators (the <a href="https://en.wikipedia.org/wiki/Adiabatic_quantum_computation">adiabatic algorithm</a>, the <a href="https://arxiv.org/abs/quant-ph/0209131">quantum walk on conjoined trees</a>, the <a href="https://scottaaronson.blog/?p=207">NAND-tree algorithm</a>).  There were other cases where computer scientists’ knowledge gave <em>them</em> an advantage: I wouldn’t know many details about that, but conceivably shadow tomography, BosonSampling, PostBQP=PP?  Overall, it’s been what you wish <em>every</em> indisciplinary collaboration could be.</p>



<p>What’s new, in the last decade, is that the scientific conversation centered around quantum information and computation has dramatically “metastasized,” to encompass not only a good fraction of all the experimentalists doing quantum optics and sensing and metrology and so forth, and not only a good fraction of all the condensed-matter theorists, but even many leading string theorists and quantum gravity theorists, including Susskind, Maldacena, Bousso, Hubeny, Harlow, and yes, <a href="https://arxiv.org/abs/1805.11965">Witten</a>.  And I don’t think it’s <em>just</em> that they’re too professional to trash-talk quantum information people the way commenter “String Theorist” does.  Rather it’s that, because of the intellectual success of “It from Qubit,” we’re increasingly participating in the same <a href="https://www.youtube.com/watch?v=1CpzigpEJnU">conversations</a> and working on the same technical questions.  One particularly exciting such question, which I’ll have more to say about in a future post, is the truth or falsehood of the Quantum Extended Church-Turing Thesis for observers who jump into black holes.</p>



<p>Not to psychoanalyze, but I’ve noticed a pattern wherein, the more secure a scientist is about their position within their own field, the readier they are to admit ignorance about the <em>neighboring</em> fields, to learn about those fields, and to reach out to the experts in them, to ask simple or (as it usually turns out) not-so-simple questions.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>I can’t imagine any better illustration of these tendencies better than the 28th Solvay Conference on the Physics of Quantum Information, which I attended two weeks ago in Brussels on my 41st birthday.</p>



<figure class="wp-block-image size-large"><a href="https://149663533.v2.pressablecdn.com/wp-content/uploads/2022/06/solvay.jpg"><img alt="" class="wp-image-6466" height="578" src="https://149663533.v2.pressablecdn.com/wp-content/uploads/2022/06/solvay-1024x578.jpg" width="1024"/></a>As others pointed out, the proportion of women is not as high as we all wish, but it’s higher than in 1911, when there was exactly one: Madame Curie herself.</figure>



<p>It was my first trip out of the US since before COVID—indeed, I’m so out of practice that I nearly missed my flights in <em>both</em> directions, in part because of my lack of familiarity with the COVID protocols for transatlantic travel, as well as the immense lines caused by those protocols.  My former adviser Umesh Vazirani, who was also at the Solvay Conference, was <a href="https://scottaaronson.blog/?p=40">proud</a>.</p>



<p>The Solvay Conference is the venue where, legendarily, the fundamentals of quantum mechanics got hashed out between 1911 and 1927, by the likes of Einstein, Bohr, Planck, and Curie.  (Einstein complained, in a letter, about being called away from his work on general relativity to attend a <a href="https://www.europhysicsnews.org/articles/epn/pdf/2011/05/epn2011425p15.pdf">“witches’ sabbath.”</a>)  Remarkably, it’s still being held in Brussels every few years, and still funded by the same Solvay family that started it.  The once-every-few-years schedule has, we were constantly reminded, been interrupted only three times in its 110-year history: once for WWI, once for WWII, and now once for COVID (this year’s conference was supposed to be in 2020).</p>



<p>This was the first ever Solvay conference organized around the theme of quantum information, and apparently, the first ever that counted computer scientists among its participants (me, Umesh Vazirani, Dorit Aharonov, Urmila Mahadev, and Thomas Vidick).  There were four topics: (1) many-body physics, (2) quantum gravity, (3) quantum computing hardware, and (4) quantum algorithms.  The structure, apparently unchanged since the conference’s founding, is this: everyone attends every session, without exception.  They sit around facing each other the whole time; no one ever stands to lecture.  For each topic, two <a href="https://en.wikipedia.org/wiki/Rapporteur">“rapporteurs”</a> introduce the topic with half-hour prepared talks; then there are short prepared response talks as well as an hour or more of unstructured discussion.  Everything everyone says is recorded in order to be published later.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>Daniel Gottesman and I were the two rapporteurs for quantum algorithms: Daniel spoke about quantum error-correction and fault-tolerance, and I spoke about <a href="https://www.scottaaronson.com/talks/aar-solvay.ppt">“How Much Structure Is Needed for Huge Quantum Speedups?”</a>  The link goes to my PowerPoint slides, if you’d like to check them out.  I tried to survey 30 years of history of that question, from Simon’s and Shor’s algorithms, to huge speedups in quantum query complexity (e.g., glued trees and Forrelation), to the recent quantum supremacy experiments based on BosonSampling and Random Circuit Sampling, all the way to the <a href="https://arxiv.org/abs/2204.02063">breakthrough</a> by Yamakawa and Zhandry a couple months ago.  The last slide hypothesizes a “Law of Conservation of Weirdness,” which after all these decades still remains to be undermined: “For every problem that admits an exponential quantum speedup, there must be some weirdness in its detailed statement, which the quantum algorithm exploits to focus amplitude on the rare right answers.”  My title slide also shows <a href="https://openai.com/dall-e-2/">DALL-E2</a>‘s impressionistic take on the title question, “how much structure is needed for huge quantum speedups?”:</p>



<figure class="wp-block-image size-large"><a href="https://149663533.v2.pressablecdn.com/wp-content/uploads/2022/06/speedups.jpg"><img alt="" class="wp-image-6474" height="418" src="https://149663533.v2.pressablecdn.com/wp-content/uploads/2022/06/speedups-1024x418.jpg" width="1024"/></a></figure>



<p>The discussion following my talk was largely a debate between me and Ed Farhi, reprising many debates he and I have had over the past 20 years: Farhi urged optimism about the prospect for large, practical quantum speedups via algorithms like <a href="https://arxiv.org/abs/1411.4028">QAOA</a>, pointing out his group’s past successes and explaining how they wouldn’t have been possible without an optimistic attitude.  For my part, I praised the past successes and said that optimism is well and good, but at the same time, companies, venture capitalists, and government agencies are right now pouring billions into quantum computing, in many cases—as I know from talking to them—because of a mistaken impression that QCs are <em>already known</em> to be able to revolutionize machine learning, finance, supply-chain optimization, or whatever other application domains they care about, and to do so <em>soon</em>.  They’re genuinely surprised to learn that the consensus of QC experts is in a totally different place.  And to be clear: among quantum computing theorists, I’m not at all unusually pessimistic or skeptical, just unusually willing to say in public what others say in private.</p>



<p>Afterwards, one of the string theorists said that Farhi’s arguments with me had been a highlight … and I agreed.  What’s the point of a friggin’ Solvay Conference if everyone’s just going to <em>agree</em> with each other?</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>Besides quantum algorithms, there was naturally lots of animated discussion about the practical prospects for building scalable quantum computers.  While I’d hoped that this discussion might change the impressions I’d come with, it mostly confirmed them.  Yes, the problem is staggeringly hard.  Recent ideas for fault-tolerance, including the use of LDPC codes and bosonic codes, might help.  Gottesman’s talk gave me the insight that, at its core, quantum fault-tolerance is all about <em>testing</em>, <em>isolation</em>, and <em>contact-tracing</em>, just for bit-flip and phase-flip errors rather than viruses.  Alas, we don’t yet have the quantum fault-tolerance analogue of a vaccine!</p>



<p>At one point, I asked the trapped-ion experts in open session if they’d comment on the startup company <a href="https://ionq.com/">IonQ</a>, whose stock price recently fell precipitously in the wake of a scathing analyst report.  Alas, none of them took the bait.</p>



<p>On a different note, I was tremendously excited by the quantum gravity session.  Netta Engelhardt spoke about her and others’ <a href="https://www.quantamagazine.org/netta-engelhardt-has-escaped-hawkings-black-hole-paradox-20210823/">celebrated recent work</a> explaining the Page curve of an evaporating black hole using Euclidean path integrals—and by questioning her and others during coffee breaks, I finally got a handwavy intuition for how it works.  There was also lots of debate, again at coffee breaks, about Susskind’s <a href="https://www.youtube.com/watch?v=1CpzigpEJnU">recent speculations</a> on observers jumping into black holes and the quantum Extended Church-Turing Thesis.  One of my main takeaways from the conference was a dramatically better understanding of the issues involved there—but that’s a big enough topic that it will need its own post.</p>



<p>Toward the end of the quantum gravity session, the experimentalist John Martinis innocently asked what actual experiments, or at least thought experiments, had been at issue for the past several hours.  I got a laugh by explaining to him that, while the gravity experts considered this too obvious to point out, the thought experiments in question all involve forming a black hole in a known quantum pure state, with total control over all the Planck-scale degrees of freedom; then waiting outside the black hole for ~10<sup>70</sup> years; collecting every last photon of Hawking radiation that comes out and routing them all into a quantum computer; doing a quantum computation that might actually require exponential time; <em>and then</em> jumping into the black hole, whereupon you might either die immediately at the event horizon, or else learn something in your last seconds before hitting the singularity, which you could then never communicate to anyone outside the black hole.  Martinis thanked me for clarifying.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>Anyway, I had a total blast.  Here I am amusing some of the world’s great physicists by letting them mess around with GPT-3.</p>



<figure class="wp-block-image size-large"><a href="https://149663533.v2.pressablecdn.com/wp-content/uploads/2022/06/gpt3.jpg"><img alt="" class="wp-image-6467" height="768" src="https://149663533.v2.pressablecdn.com/wp-content/uploads/2022/06/gpt3-1024x768.jpg" width="1024"/></a>Back: Ahmed Almheiri, Juan Maldacena, John Martinis, Aron Wall.  Front: Geoff Penington, me, Daniel Harlow.  Thanks to Michelle Simmons for the photo.</figure>



<p>I also had the following exchange at my birthday dinner:</p>



<p><strong>Physicist:</strong> So I don’t get this, Scott. Are you a physicist who studied computer science, or a computer scientist who studied physics?</p>



<p><strong>Me:</strong> I’m a computer scientist who studied computer science.</p>



<p><strong>Physicist:</strong> But then you…</p>



<p><strong>Me:</strong> Yeah, at some point I learned what a boson was, in order to invent BosonSampling.</p>



<p><strong>Physicist:</strong> And your courses in physics…</p>



<p><strong>Me:</strong> They ended at thermodynamics. I couldn’t handle PDEs.</p>



<p><strong>Physicist:</strong> What are the units of h-bar?</p>



<p><strong>Me:</strong> Uhh, well, it’s a conversion factor between energy and time. (*)</p>



<p><strong>Physicist:</strong> Good.  What’s the radius of the hydrogen atom?</p>



<p><strong>Me:</strong> Uhh … not sure … maybe something like 10<sup>-15</sup> meters?</p>



<p><strong>Physicist:</strong> OK fine, he’s not one of us.</p>



<p>(The answer, it turns out, is more like 10<sup>-10</sup> meters. I’d stupidly substituted the radius of the <em>nucleus</em>—or, y’know, a positively-charged hydrogen <em>ion</em>, i.e. proton. In my partial defense, I was massively jetlagged and at most 10% conscious.)</p>



<p>(*) Actually h-bar is a conversion factor between energy and <em>1/time</em>, i.e. frequency, but the physicist accepted this answer.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>Anyway, I look forward to attending more workshops this summer, seeing more colleagues who I hadn’t seen since before COVID, and talking more science … including branching out in some new directions that I’ll blog about soon.  It does beat worrying about online trolls.</p></div>
    </content>
    <updated>2022-06-09T20:43:19Z</updated>
    <published>2022-06-09T20:43:19Z</published>
    <category scheme="https://scottaaronson.blog" term="Complexity"/>
    <category scheme="https://scottaaronson.blog" term="CS/Physics Deathmatch"/>
    <category scheme="https://scottaaronson.blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://scottaaronson.blog/?feed=atom</id>
      <icon>https://scottaaronson.blog/wp-content/uploads/2021/10/cropped-Jacket-32x32.gif</icon>
      <link href="https://scottaaronson.blog" rel="alternate" type="text/html"/>
      <link href="https://scottaaronson.blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2022-06-19T05:44:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://toc4fairness.org/?p=2188</id>
    <link href="https://toc4fairness.org/toc4fairness-seminar-roland-maio/" rel="alternate" type="text/html"/>
    <title>TOC4Fairness Seminar – Roland Maio</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><strong>Date:</strong> Wednesday, June 15th 2022 9:00 am – 10:00 am Pacific Time <br/>

<strong>Location: </strong>Zoom meeting</div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong>Date: </strong>Wednesday, June 15th, 2022<br/>9:00 am – 10:00 am Pacific Time<br/>12:00 pm – 1:00 pm Eastern Time</p>



<figure class="wp-block-image size-large"><img alt="" class="wp-image-2190" height="459" src="https://i0.wp.com/toc4fairness.org/wp-content/uploads/2022/06/bruce-park-roland.jpg?resize=800%2C459&amp;ssl=1" width="800"/></figure>



<p><strong>Location: </strong>Weekly Seminar, Zoom </p>



<h3 id="title-structural-racism-white-supremacy-and-the-2-edged-sword-of-data-health-inequities-and-the-embodied-truths-sharply-exposed-by-covid-19-in-context"><strong><a href="Title:&#xA0;Allocating Opportunities in a Dynamic WorldAbstract: I  will speak about the need for considering dynamic changes to the  environment in response to allocational policies. As an example, I will  introduce a sequential model for allocating opportunities, such as  higher education, in a society that exhibits bottlenecks in  socio-economic mobility. I will discuss how the problem of optimal  allocation reflects a trade-off between the benefits conferred by the  opportunities in the current generation and the potential to elevate the  socioeconomic status of recipients, shaping the composition of future  generations in ways that can benefit further from the opportunities. Our  results show how optimal allocations in this model arise as solutions  to continuous optimization problems over multiple generations, and in  general, these optimal solutions can favor recipients of low  socioeconomic status over slightly higher-performing individuals of high  socioeconomic status &#x2014; a form of socioeconomic affirmative action that  the society in our model discovers in the pursuit of purely  payoff-maximizing goals. I will conclude with directions for future  work.Bio: Hoda Heidari is an Assistant  Professor in Machine Learning and Societal Computing at the School of  Computer Science, Carnegie Mellon University. Her research is broadly  concerned with the social, ethical, and economic implications of  Artificial Intelligence. In particular, her research addresses issues of  unfairness and opaqueness through Machine Learning. Her work in this  area has won a best-paper award at the ACM Conference on Fairness,  Accountability, and Transparency (FAccT) and an exemplary track award at  the ACM Conference on Economics and Computation (EC). She has organized  several scholarly events on topics related to Responsible and  Trustworthy AI, including a tutorial at the Web Conference (WWW) and  several workshops at the Neural and Information Processing Systems  (NeurIPS) conference and the International Conference on Learning  Representations (ICLR). Dr. Heidari completed her doctoral studies in  Computer and Information Science at the University of Pennsylvania. She  holds an M.Sc. degree in Statistics from the Wharton School of  Business.&#xA0; Before joining Carnegie Mellon as a faculty member, she was a  postdoctoral scholar at the Machine Learning Institute of ETH Z&#xFC;rich,  followed by a year at the Artificial Intelligence, Policy, and Practice  (AIPP) initiative at Cornell University.">Title: </a></strong><a href="Title:&#xA0;Allocating Opportunities in a Dynamic WorldAbstract: I  will speak about the need for considering dynamic changes to the  environment in response to allocational policies. As an example, I will  introduce a sequential model for allocating opportunities, such as  higher education, in a society that exhibits bottlenecks in  socio-economic mobility. I will discuss how the problem of optimal  allocation reflects a trade-off between the benefits conferred by the  opportunities in the current generation and the potential to elevate the  socioeconomic status of recipients, shaping the composition of future  generations in ways that can benefit further from the opportunities. Our  results show how optimal allocations in this model arise as solutions  to continuous optimization problems over multiple generations, and in  general, these optimal solutions can favor recipients of low  socioeconomic status over slightly higher-performing individuals of high  socioeconomic status &#x2014; a form of socioeconomic affirmative action that  the society in our model discovers in the pursuit of purely  payoff-maximizing goals. I will conclude with directions for future  work.Bio: Hoda Heidari is an Assistant  Professor in Machine Learning and Societal Computing at the School of  Computer Science, Carnegie Mellon University. Her research is broadly  concerned with the social, ethical, and economic implications of  Artificial Intelligence. In particular, her research addresses issues of  unfairness and opaqueness through Machine Learning. Her work in this  area has won a best-paper award at the ACM Conference on Fairness,  Accountability, and Transparency (FAccT) and an exemplary track award at  the ACM Conference on Economics and Computation (EC). She has organized  several scholarly events on topics related to Responsible and  Trustworthy AI, including a tutorial at the Web Conference (WWW) and  several workshops at the Neural and Information Processing Systems  (NeurIPS) conference and the International Conference on Learning  Representations (ICLR). Dr. Heidari completed her doctoral studies in  Computer and Information Science at the University of Pennsylvania. She  holds an M.Sc. degree in Statistics from the Wharton School of  Business.&#xA0; Before joining Carnegie Mellon as a faculty member, she was a  postdoctoral scholar at the Machine Learning Institute of ETH Z&#xFC;rich,  followed by a year at the Artificial Intelligence, Policy, and Practice  (AIPP) initiative at Cornell University."> </a>Secrets, Adversaries, Incentives, and Composition in Algorithmic Fairness</h3>



<h3 id="abstract"><strong>Abstract:</strong></h3>



<p>A central goal of algorithmic fairness is to build systems with fairness properties that compose gracefully. A major effort towards this goal in fair machine learning has been the development of <em>fair representations</em> which guarantee demographic parity under sequential composition by removing group membership information from the data (i.e. by imposing a <em>demographic secrecy</em> constraint). This approach models all data consumers as utterly malicious adversaries whose sole objective is to be as unfair as possible (i.e. maximally violate demographic parity)—any other possible objective (i.e. incentive) that a data consumer may have is dismissed and ignored. In this talk, I describe joint work with Augustin Chaintreau, in which we elucidate limitations of demographically secret fair representations and propose a fresh approach to potentially overcome them by incorporating information about parties’ incentives into fairness interventions. We show that in a stylized model, it is possible to relax demographic secrecy to obtain <em>incentive-compatible representations</em>, where rational parties obtain exponentially greater utilities vis-à-vis any demographically secret representation and satisfy demographic parity. These substantial gains are recovered not from the well-known <em>cost of fairness</em>, but rather from a <em>cost of demographic secrecy</em> which we formalize and quantify for the first time. We further show that the sequential composition property of demographically secret representations is not robust to aggregation. Our results contribute to further understanding the challenges of fair composition while simultaneously suggesting that incentives may be an important and flexible tool for addressing or even overcoming those challenges.</p>



<h3 id="bio"><strong>Bio:</strong></h3>



<p>Roland Maio is a fourth year Computer Science PhD student at Columbia University advised by Augustin Chaintreau. Roland works on algorithmic fairness and CS ethics. His work has been supported by an NSF fellowship.<br/></p></div>
    </content>
    <updated>2022-06-09T16:31:57Z</updated>
    <published>2022-06-09T16:31:57Z</published>
    <category term="Events"/>
    <category term="seminar"/>
    <author>
      <name>jubaziani</name>
    </author>
    <source>
      <id>https://toc4fairness.org</id>
      <logo>https://i0.wp.com/toc4fairness.org/wp-content/uploads/2020/10/cropped-favicon.png?fit=32%2C32&amp;ssl=1</logo>
      <link href="https://toc4fairness.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://toc4fairness.org" rel="alternate" type="text/html"/>
      <subtitle>a simons collaboration project</subtitle>
      <title>TOC for Fairness</title>
      <updated>2022-06-23T03:38:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/086</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/086" rel="alternate" type="text/html"/>
    <title>TR22-086 |  Extremely Efficient Constructions of Hash Functions, with Applications to Hardness Magnification and PRFs | 

	Jiatu Li, 

	Tianqi Yang, 

	Lijie Chen</title>
    <summary>In a recent work, Fan, Li, and Yang (STOC 2022) constructed a family of almost-universal hash functions such that each function in the family is computable by $(2n + o(n))$-gate circuits of fan-in $2$ over the $B_2$ basis. Applying this family, they established the existence of pseudorandom functions computable by circuits of the same complexity, under the standard assumption that OWFs exist. However, a major disadvantage of the hash family construction by Fan, Li, and Yang (STOC 2022) is that it requires a seed length of $\text{poly}(n)$, which limits its potential applications. 
        
We address this issue by giving an improved construction of almost-universal hash functions with seed length $\text{polylog}(n)$, such that each function in the family is computable with $\text{POLYLOGTIME}$-uniform $(2n + o(n))$-gate circuits. Our new construction has the following applications in both complexity theory and cryptography.
        
* ($\textbf{Hardness magnification}$). Let $\alpha : \mathbb{N} \rightarrow \mathbb{N}$ be any function such that $\alpha(n) \leq \log n / \log \log n$. We show that if there is an $n^{\alpha(n)}$-sparse $\textbf{NP}$ language that does not have probabilistic circuits of $2n + O(n/\log\log n)$ gates, then we have (1) $\textbf{NTIME}[2^n] \not\subseteq \textbf{SIZE} \left[2^{n^{1/5}}\right]$ and (2) $\textbf{NP} \not\subseteq \textbf{SIZE}[n^k]$ for every constant $k$. Complementing this magnification phenomenon, we present an $O(n)$-sparse language in $\textbf{P}$ which requires probabilistic circuits of size at least $2n - 2$. This is the first result in hardness magnification showing that even a \emph{sub-linear additive} improvement on known circuit size lower bounds would imply $\textbf{NEXP} \not\subseteq \textbf{P}_{/\text{poly}}$. 

Following Chen, Jin, and Williams (STOC 2020), we also establish a sharp threshold for \textbf{explicit obstructions}: we give an explict obstruction against $(2n-2)$-size circuits, and prove that a sub-linear additive improvement on the circuit size would imply (1) $\textbf{DTIME}[2^n] \not\subseteq \textbf{SIZE} \left[2^{n^{1/5}}\right]$ and (2) $\textbf{P} \not\subseteq \textbf{SIZE}[n^k]$ for every constant $k$.

* ($\textbf{Extremely efficient construction of pseudorandom functions}$). Assuming that one of integer factoring, decisional Diffie-Hellman, or ring learning-with-errors is sub-exponentially hard, we show the existence of pseudorandom functions computable by $\text{POLYLOGTIME}$-uniform $\textbf{AC}^0[2]$ circuits with $2n + o(n)$ wires, with key length $\text{polylog}(n)$. We also show that PRFs computable by $\text{POLYLOGTIME}$-uniform $B_2$ circuits of $2n + o(n)$ gates follows from the existence of sub-exponentially secure one-way functions.</summary>
    <updated>2022-06-09T12:54:50Z</updated>
    <published>2022-06-09T12:54:50Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-06-23T03:37:32Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4635</id>
    <link href="https://lucatrevisan.wordpress.com/2022/06/09/workshop-in-milan-next-week/" rel="alternate" type="text/html"/>
    <title>Workshop in Milan Next Week</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">As previously announced, next week Alon Rosen and I are organizing a workshop at Bocconi, which will actually be the union of two workshops, one on Recent Advances in Cryptography and one on Spectral and Convex Optimization Techniques in Graph … <a href="https://lucatrevisan.wordpress.com/2022/06/09/workshop-in-milan-next-week/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>As <a href="https://lucatrevisan.wordpress.com/2022/05/10/stoc-2022-and-other-theory-events/">previously announced</a>, next week Alon Rosen and I are organizing a workshop at Bocconi, which will actually be the union of two  workshops, one on <em>Recent Advances in Cryptography</em> and one on <em>Spectral and Convex Optimization Techniques in Graph Algorithms</em>. Here is the <a href="https://lucatrevisan.github.io/mtw.html">program</a>. In short:</p>



<ul><li>where: Bocconi University’s <a href="https://goo.gl/maps/TmBQg7g43CiKeNb18">Roentgen Building</a> (via Roentgen 1, Milano), Room AS01</li><li>when: June 15-18</li><li>what: <a href="https://lucatrevisan.github.io/mtw.html">talks</a> on cryptography and graph algorithms, including two hours devoted to Max Flow in nearly-linear time</li><li>how: <a href="https://events.unibocconi.eu/index.php?key=ev2022050043">register</a> for free</li></ul></div>
    </content>
    <updated>2022-06-09T12:26:02Z</updated>
    <published>2022-06-09T12:26:02Z</published>
    <category term="Milan"/>
    <category term="theory"/>
    <category term="cryptography"/>
    <category term="spectral graph theory"/>
    <category term="things that are excellent"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2022-06-23T03:37:14Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2022-06-09-phase-king-via-gradecast/</id>
    <link href="https://decentralizedthoughts.github.io/2022-06-09-phase-king-via-gradecast/" rel="alternate" type="text/html"/>
    <title>Phase-King through the lens of Gradecast: A simple unauthenticated synchronous Byzantine Agreement protocol</title>
    <summary>In this post we overview a simple unauthenticated synchronous Byzantine Agreement protocol that is based on the Phase-King protocol of Berman, Garay, and Perry 1989-92. We refer also to Jonathan Katz’s excellent write-up on this same protocol from 2013. We offer a modern approach that decomposes the Phase-King protocol into...</summary>
    <updated>2022-06-09T11:11:00Z</updated>
    <published>2022-06-09T11:11:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2022-06-22T22:42:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2022/085</id>
    <link href="https://eccc.weizmann.ac.il/report/2022/085" rel="alternate" type="text/html"/>
    <title>TR22-085 |  A Note on Lower Bounds for Monotone Multilinear Boolean Circuits | 

	Andrzej Lingas</title>
    <summary>A monotone Boolean circuit is a restriction of a Boolean circuit
  allowing for the use of disjunctions, conjunctions, the Boolean
  constants, and the input variables.  A monotone Boolean circuit is
  multilinear if for any AND gate the two input functions have no
  variable in common.  We show that the known lower bounds on the size
  of monotone arithmetic circuits for multivariate polynomials that
  are sums of monomials consisting of $k$ distinct variables yield the
  analogous lower bounds divided by $O(k^2)$ on the size of monotone
  multilinear Boolean circuits computing the Boolean functions
  represented by the corresponding multivariate Boolean polynomials.</summary>
    <updated>2022-06-09T09:41:53Z</updated>
    <published>2022-06-09T09:41:53Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2022-06-23T03:37:32Z</updated>
    </source>
  </entry>
</feed>
