<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-01-09T22:22:01Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42183</id>
    <link href="https://cstheory.stackexchange.com/questions/42183/new-subset-sum-approach-tc-results" rel="alternate" type="text/html"/>
    <title>new subset sum approach TC/results</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I have been working on a new approach for a subset sum exact solver, and the current state provides an algorithm operating on <span class="math-container">$O{n/2 \choose n/4}$</span>, demonstrating as well the hardest target value is not <span class="math-container">$\approx \frac{sum(S)}{2}$</span> but <span class="math-container">$\approx\frac{sum(S)}{4}$</span> for dense instances (<span class="math-container">$d \approx 1$</span>) in all cases.</p>

<p>I asked several people about their opinion and I got mixed feedback, some people though it was worth it to keep pushing the approach before publishing (to try to obtain an improved result, given this is likely possible, then publish), while other people encouraged me to publish right away given the TC improvement over the <span class="math-container">$O(2^{\frac{n}{2}})$</span> approach and the new characterization for the hardest target values not demonstrated before in the available literature, then continue working looking to improve these results.</p>

<p>What are your suggestions/opinions? </p></div>
    </summary>
    <updated>2019-01-09T21:51:34Z</updated>
    <published>2019-01-09T21:51:34Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="np-complete"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="subset-sum"/>
    <author>
      <name>John Seppard</name>
      <uri>https://cstheory.stackexchange.com/users/47335</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-09T22:21:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2019/01/09/mixed-integer-nonlinear-optimization-meets-data-science/</id>
    <link href="https://cstheory-events.org/2019/01/09/mixed-integer-nonlinear-optimization-meets-data-science/" rel="alternate" type="text/html"/>
    <title>Mixed-Integer Nonlinear Optimization meets Data Science</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">June 25, 2018 – June 28, 2019 Ischia, Italy http://www.iasi.cnr.it/minoa/big-data-school/ CNR-IASI, as part of the MINOA project, announces the school for PhD students and post-docs on the theme Mixed Integer Non linear Optimization meets Data Science. The school will cover the following topics: Deep learning for AI Clustering for Big Data Machine Learning for Combinatorial … <a class="more-link" href="https://cstheory-events.org/2019/01/09/mixed-integer-nonlinear-optimization-meets-data-science/">Continue reading <span class="screen-reader-text">Mixed-Integer Nonlinear Optimization meets Data Science</span></a></div>
    </summary>
    <updated>2019-01-09T20:51:39Z</updated>
    <published>2019-01-09T20:51:39Z</published>
    <category term="school"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2019-01-09T22:21:48Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://adamsheffer.wordpress.com/?p=5374</id>
    <link href="https://adamsheffer.wordpress.com/2019/01/09/the-baruch-distinguished-mathematics-lecture-series/" rel="alternate" type="text/html"/>
    <title>The Baruch Distinguished Mathematics Lecture Series</title>
    <summary>I am happy to announce the beginning of the Baruch Distinguished Mathematics Lecture Series. In this series we will bring established mathematicians to give talks to a general mathematical audience. Our first Distinguished Lecture, by Bjorn Poonen, will be “Undecidability in Number Theory”. Click here for the full details. The talk is open to everyone, […]</summary>
    <updated>2019-01-09T20:43:09Z</updated>
    <published>2019-01-09T20:43:09Z</published>
    <category term="Math events"/>
    <author>
      <name>Adam Sheffer</name>
    </author>
    <source>
      <id>https://adamsheffer.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://adamsheffer.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://adamsheffer.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://adamsheffer.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://adamsheffer.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Discrete geometry and other typos</subtitle>
      <title>Some Plane Truths</title>
      <updated>2019-01-09T22:21:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://blogs.princeton.edu/imabandit/?p=1350</id>
    <link href="https://blogs.princeton.edu/imabandit/2019/01/09/nemirovskis-acceleration/" rel="alternate" type="text/html"/>
    <title>Nemirovski’s acceleration</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I will describe here the very first (to my knowledge) acceleration algorithm for smooth convex optimization, which is due to Arkadi Nemirovski (dating back to the end of the 70’s). The algorithm relies on a -dimensional plane-search subroutine (which, in … <a href="https://blogs.princeton.edu/imabandit/2019/01/09/nemirovskis-acceleration/">Continue reading <span class="meta-nav">→</span></a></p></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I will describe here the very first (to my knowledge) acceleration algorithm for smooth convex optimization, which is due to <a class="liinternal" href="https://en.wikipedia.org/wiki/Arkadi_Nemirovski" rel="nofollow">Arkadi Nemirovski</a> (dating back to the end of the 70’s). The algorithm relies on a <img alt="2" class="ql-img-inline-formula " height="12" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-bc2da46d9824359f6ac8d33c5fb882dd_l3.png?resize=8%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="8"/>-dimensional plane-search subroutine (which, in theory, can be implemented in <img alt="\log(1/\epsilon)" class="ql-img-inline-formula " height="19" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-38bfa3c1131fbae41cb358b8b685dc56_l3.png?resize=61%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="61"/> calls to a first-order oracle). He later improved it to only require a <img alt="1" class="ql-img-inline-formula " height="13" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-21b5b4cbe9a10b6d847eeb4265b99898_l3.png?resize=7%2C13&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="7"/>-dimensional line-search in 1981, but of course the breakthrough that everyone knows about came a year after with the famous 1982 paper by <a class="liinternal" href="https://en.wikipedia.org/wiki/Yurii_Nesterov" rel="nofollow">Nesterov</a> that gets rid of this extraneous logarithmic term altogether (and in addition is based on the <a class="liinternal" href="https://blogs.princeton.edu/imabandit/2018/11/21/a-short-proof-for-nesterovs-momentum/">deep insight</a> of modifying Polyak’s momentum).</p>
<p>Let <img alt="f" class="ql-img-inline-formula " height="16" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c7d97b919a3b73617cf2fbb375fff3b1_l3.png?resize=10%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="10"/> be a <img alt="1" class="ql-img-inline-formula " height="13" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-21b5b4cbe9a10b6d847eeb4265b99898_l3.png?resize=7%2C13&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="7"/>-smooth function. Denote <img alt="x^{+} = x - \nabla f(x)" class="ql-img-inline-formula " height="19" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d69559ebcb4e4ecdf7454cab91bf526b_l3.png?resize=125%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="125"/>. Fix a sequence <img alt="(\lambda_t)_{t \in \N}" class="ql-img-inline-formula " height="18" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ba9ca1ebe088d1befda0acb3c4644727_l3.png?resize=43%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="43"/>, to be optimized later. We consider the “conjugate” point <img alt="\sum_{s =1}^t \lambda_s \nabla f(x_s)" class="ql-img-inline-formula " height="23" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d22daf1993fbb2397de0b21ab0ea87ee_l3.png?resize=119%2C23&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="119"/>. The algorithm simply returns the optimal combination of the conjugate point and the gradient descent point, that is:</p>
<p class="ql-center-displayed-equation" style="line-height: 54px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[ x_{t+1} = \argmin_{x \in P_t} f(x) \, \text{where} \, P_t = \mathrm{span}\left(x_t^+, \sum_{s =1}^t \lambda_s \nabla f(x_s)\right) \,. \]" class="ql-img-displayed-equation " height="54" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-360399a70097a55997eaeacb3ec02615_l3.png?resize=424%2C54&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="424"/></p>
<p>Let us denote <img alt="g_s = \nabla f(x_s)" class="ql-img-inline-formula " height="18" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e614d7a76c02a9a308f9898af91df8ff_l3.png?resize=95%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="95"/> and <img alt="\delta_s = f(x_s) - f(x^*)" class="ql-img-inline-formula " height="18" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-629dcf91ea83c14ea854c74de1069acd_l3.png?resize=143%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="143"/> for shorthand. The key point is that <img alt="g_{t+1} \in P_t^{\perp}" class="ql-img-inline-formula " height="20" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9d21b904e2ccc5df54959aa117a37b98_l3.png?resize=77%2C20&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="77"/>, and in particular <img alt="\|\sum_{s \leq t} \lambda_s g_s\|^2 = \sum_{s \leq t} \lambda_s^2 \|g_s\|^2" class="ql-img-inline-formula " height="22" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-4e57279bc2d0342e42087a51af61fb76_l3.png?resize=231%2C22&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="231"/>. Now recognize that <img alt="\|g_s\|^2" class="ql-img-inline-formula " height="20" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b728147d95760e42ef7cdbb706a8cfd1_l3.png?resize=38%2C20&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="38"/> is a lower bound on the improvement <img alt="\delta_s - \delta_{s+1}" class="ql-img-inline-formula " height="17" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-0808cda8024eb11ab7ce37176832a9fe_l3.png?resize=68%2C17&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="68"/> (here we use that <img alt="x_{s+1}" class="ql-img-inline-formula " height="13" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b0878f455a78407f8618e726e941aea6_l3.png?resize=33%2C13&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="33"/> is better than <img alt="x_s^+" class="ql-img-inline-formula " height="19" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5acc4c663fcf2f647eb177ebb24bc154_l3.png?resize=20%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="20"/>). Thus we get:</p>
<p class="ql-center-displayed-equation" style="line-height: 40px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[ \|\sum_{s \leq t} \lambda_s g_s\|^2 \leq \sum_{s \leq t} \lambda_s^2 (\delta_s - \delta_{s+1}) \leq \sum_{s \leq t} \delta_s (\lambda_s^2 - \lambda_{s-1}^2) \,. \]" class="ql-img-displayed-equation " height="40" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-a12bda17f9c1f0f0e13ef03c9a1c9d2c_l3.png?resize=404%2C40&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="404"/></p>
<p>In other words if the sequence <img alt="\lambda" class="ql-img-inline-formula " height="12" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ab48baf331239642a00255b86324280a_l3.png?resize=10%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="10"/> is chosen such that <img alt="\lambda_s = \lambda_s^2 - \lambda_{s-1}^2" class="ql-img-inline-formula " height="21" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-2f241a8315da6dd3f7735135d1d2b7ae_l3.png?resize=114%2C21&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="114"/> then we get</p>
<p class="ql-center-displayed-equation" style="line-height: 40px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[ \|\sum_{s \leq t} \lambda_s g_s\|^2 \leq \sum_{s \leq t} \lambda_s \delta_s \,. \]" class="ql-img-displayed-equation " height="40" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-8e959561f27eec9096b81bd7148a4a75_l3.png?resize=180%2C40&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="180"/></p>
<p>This is good because roughly the reverse inequality also holds true by convexity (and the fact that <img alt="x_s \in P_s" class="ql-img-inline-formula " height="15" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-212cae06b1b9b8b6af498b589bb15865_l3.png?resize=56%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="56"/> so <img alt="g_s \cdot x_s = 0" class="ql-img-inline-formula " height="16" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-cf91835fdc91be361d1c7e89f867c5db_l3.png?resize=78%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="78"/>):</p>
<p class="ql-center-displayed-equation" style="line-height: 40px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[ \sum_{s \leq t} \lambda_s \delta_s \leq \sum_{s \leq t} \lambda_s g_s \cdot (x_s - x^*) \leq \|x^*\| \cdot \| \sum_{s \leq t} \lambda_s g_s\| \,. \]" class="ql-img-displayed-equation " height="40" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9d1ee450c97dfc04bca3a123fb68daa8_l3.png?resize=391%2C40&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="391"/></p>
<p>So finally we get <img alt="\sum_{s \leq t} \lambda_s g_s \leq \|x^*\|^2" class="ql-img-inline-formula " height="22" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9a824c3ceaa5f687a8751a121d4ef8a4_l3.png?resize=144%2C22&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="144"/>, and it just remains to realize that <img alt="\lambda_s" class="ql-img-inline-formula " height="15" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-f100f89f751713a1814b3938a510009b_l3.png?resize=16%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="16"/> is of order <img alt="s" class="ql-img-inline-formula " height="8" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3bcfb3f0b6b04be3b598743cd774dd78_l3.png?resize=8%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="8"/> so that <img alt="\delta_t \leq \|x^*\|^2 / t^2" class="ql-img-inline-formula " height="20" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3ff4a3e78d1ced5a05f33eb077194504_l3.png?resize=103%2C20&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="103"/>.</p></div>
    </content>
    <updated>2019-01-09T18:51:19Z</updated>
    <published>2019-01-09T18:51:19Z</published>
    <category term="Optimization"/>
    <author>
      <name>Sebastien Bubeck</name>
    </author>
    <source>
      <id>https://blogs.princeton.edu/imabandit</id>
      <link href="https://blogs.princeton.edu/imabandit/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://blogs.princeton.edu/imabandit" rel="alternate" type="text/html"/>
      <subtitle>Random topics in optimization, probability, and statistics. By Sébastien Bubeck</subtitle>
      <title>I’m a bandit</title>
      <updated>2019-01-09T19:21:34Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42180</id>
    <link href="https://cstheory.stackexchange.com/questions/42180/coordinate-descent-in-integer-programing-when-does-it-work" rel="alternate" type="text/html"/>
    <title>Coordinate descent in integer programing: when does it work?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Denote <span class="math-container">$N_i=\{0,1,\dots,\bar{n}_i\}$</span> and define <span class="math-container">$N=N_1\times \dots \times N_I$</span>. I want to minimize a function <span class="math-container">$f:N\rightarrow \mathbb{R}$</span>. It is very easy to minimize <span class="math-container">$f$</span> coordinate by coordinate so one natural algorithm is to iterate on a mapping <span class="math-container">$T$</span> for which the <span class="math-container">$i$</span>th element is defined as
<span class="math-container">$$(Tn)_i=\arg\min_{\tilde{n}_i\in N_i} f\left( \left\{ n_1,\dots,\tilde{n}_i,\dots,n_I\right\}\right)$$</span>
until we have convergence. This is essentially a <a href="https://en.wikipedia.org/wiki/Coordinate_descent" rel="nofollow noreferrer">coordinate descent</a> algorithm but in a discrete space.</p>

<p>My question is: under what conditions does this approach yield the true global minimum of <span class="math-container">$f$</span>? For instance, is <span class="math-container">$f$</span> strictly convex a sufficient condition for this procedure to work? Also, if anybody has a reference on the topic that would be highly appreciated.</p></div>
    </summary>
    <updated>2019-01-09T18:38:14Z</updated>
    <published>2019-01-09T18:38:14Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="ds.algorithms"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="reference-request"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="optimization"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="integer-programming"/>
    <author>
      <name>user_lambda</name>
      <uri>https://cstheory.stackexchange.com/users/51700</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-09T22:21:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://emanueleviola.wordpress.com/?p=606</id>
    <link href="https://emanueleviola.wordpress.com/2019/01/09/my-last-3-5-years/" rel="alternate" type="text/html"/>
    <title>My last 3.5 years</title>
    <summary>I haven’t breathed (freely) since 3.5 years ago.  Precisely since the day before I left my Cambridge flat, when the Pods guy told me he couldn’t park. I had to vacate within 24 hours, had no place to put all the stuff I had never used since moving there in 2008, and also happened to […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p style="text-align: justify;">I haven’t breathed (freely) since 3.5 years ago.  Precisely since the day before I left my Cambridge flat, when the Pods guy told me he couldn’t park. I had to vacate within 24 hours, had no place to put all the stuff I had never used since moving there in 2008, and also happened to have a 3-hour CPR course planned long ago, starting in minutes.  I took that life-saving course on the edge of the seat, each 5-minute break dashing out to call movers who might have had an unlikely last minute cancellation in the busiest day of the year (August 31).</p>
<p style="text-align: justify;">Oh the times I wished that the fireproof storage where the things eventually went burned down to the ground.  Instead I was going to have to move my never used belongings a million times up and down stairs.</p>
<p style="text-align: justify;">Anyway, after Cambridge I went to the <a href="https://emanueleviola.wordpress.com/2015/11/16/from-the-simons-institute/">Simons institute</a>. Even with all the help from the staff, finding housing was atrocious, and I had to change it during the semester. I didn’t have a place to come back, and from Berkeley I eventually found a short-term rental in Needham, MA.  The idea was to buy a house in that short term.  This proved <a href="https://emanueleviola.wordpress.com/2015/12/15/how-to-buy-a-house/">impossible</a>.  So we had to find another rental.  In the process, I was discriminated against three times.  One time the landlord rejected in writing my application claiming that they did not want to rent to families. The other two times the landlord simply rejected my application, and then lowered the price. I thought these moves made them dumb, but maybe they are actually much smarter than me, because after toying with the idea I did not, in fact, sue.</p>
<p style="text-align: justify;">Eventually we found another longer-term rental.  From there, with more excruciating difficulties I <a href="https://emanueleviola.wordpress.com/2017/12/19/how-to-buy-a-house-ii/">wrote about earlier</a>, I bought a house, which however required 1 year of renovations (not exactly cosmetic — more about this later).  These were completed just in time to store my useless stuff there: I left for another semester at the Simons institute.</p>
<p style="text-align: justify;">My second visit to the institute was also great.  In fact I enjoyed it even more than the semester on fine-grained: I was there for the program on lower bounds, which are exactly the problems I went into computer science to study. I had the best time, and lots of research exchanges.</p>
<p style="text-align: justify;">But again, the housing situation in Berkeley was desperate.  Twice I lost a house for 1 hour. Meaning, the landlord called to make the deal, I couldn’t pick up the phone, and when I called back 1 hour later the place was gone.  I still think it would be better if the institute bought a block of houses, and also provided computers.  Even better if they make it easier to print, rather than having to stand in a corner or go through a complicated set up.</p>
<p style="text-align: justify;">Another interesting pattern is that during my first visit there was a heat wave and the AC broke, and it was hot.  This time there was a rather serious wildfire, causing very unhealthy conditions in the bay area, and at times they couldn’t run the heating systems to avoid sucking in the smoke, and it was cold.</p>
<p style="text-align: justify;">Berkeley isn’t Princeton, but it’s hard for me not to compare the logistics of my visits to Simons and the IAS in Princeton.  In the latter I was put in a house steps from the Institute, with minimal effort and at a fraction of the price.  In my office there was already a working computer, connected to a printer.</p>
<p>Here’s the meaning of cloud computing, remote desktop, telnet, etc in 2019, here’s the progress, the sustainability, the sharing economy: everybody brings their own laptop.</p>
<p style="text-align: justify;">Back from Simons, I can’t help but be surprised that I still have an office.  In fact this happens every time I go up the stairs, turn the corner and see my name on the tag, and it says “Professor”. Really? Under <em>my name</em>? I have a startle each time.  I know this feeling is irrational, but is there.  Coming back from California, the feeling is intense.</p>
<p style="text-align: justify;">Back to business, I am now teaching algorithms.  I am running an online section, for which I am making videos on my <a href="https://www.youtube.com/channel/UChbOQ1Q8Fv44LbrQMvTPoEQ">youtube channel</a>. It’s the future.</p></div>
    </content>
    <updated>2019-01-09T16:36:12Z</updated>
    <published>2019-01-09T16:36:12Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Emanuele</name>
    </author>
    <source>
      <id>https://emanueleviola.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://emanueleviola.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://emanueleviola.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://emanueleviola.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://emanueleviola.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>By Emanuele Viola</subtitle>
      <title>Thoughts</title>
      <updated>2019-01-09T22:21:30Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42178</id>
    <link href="https://cstheory.stackexchange.com/questions/42178/approximate-multi-covering-with-randomized-rounding" rel="alternate" type="text/html"/>
    <title>Approximate Multi Covering with Randomized Rounding</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In the set multicover problem we are given a set <span class="math-container">$N$</span> of <span class="math-container">$n$</span> elements and a set <span class="math-container">$S$</span> of <span class="math-container">$m$</span> subsets of <span class="math-container">$N$</span>. Additionally, each element has a coverage requirement (the number of times it has to be covered) and each set has a weight. The question is to cover <span class="math-container">$N$</span> with the minimum weight subsets from <span class="math-container">$S$</span>. I'm aware of the approximation algorithm for this problem using (Rajagopalan &amp; Vazirani) .</p>

<p>But I am interested in finding an algorithm for this problem that uses the randomized rounding and study its approximation factor.</p>

<p>Thanks in advance!</p></div>
    </summary>
    <updated>2019-01-09T16:01:30Z</updated>
    <published>2019-01-09T16:01:30Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="approximation-algorithms"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="randomized-algorithms"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="set-cover"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="covering-problems"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="iterated-rounding"/>
    <author>
      <name>user2404626</name>
      <uri>https://cstheory.stackexchange.com/users/51697</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-09T22:21:10Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42177</id>
    <link href="https://cstheory.stackexchange.com/questions/42177/maximum-minimum-satisfiability" rel="alternate" type="text/html"/>
    <title>Maximum-minimum satisfiability</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In <a href="https://en.wikipedia.org/wiki/Maximum_satisfiability_problem" rel="nofollow noreferrer">MAX-SAT</a>, given a formula, we want to maximize the number of satisfied clauses: given a formula <span class="math-container">$\phi = c_1 \cap \cdots \cap c_n$</span>, where each <span class="math-container">$c_i$</span> is a disjunction, we want to find the largest <span class="math-container">$k\in\{1,\ldots,n\}$</span> such that, for some assignment, some <span class="math-container">$k$</span> clauses <span class="math-container">$c_{i1},\ldots,c_{ik}$</span> are true.</p>

<p>In <strong>MAX-MIN-SAT</strong>, given two different formulas, we want to maximize the minimum number of satisfied clauses in both.
I.e., given <span class="math-container">$\phi_a = a_1 \cap \cdots \cap a_n$</span> and <span class="math-container">$\phi_b = b_1 \cap \cdots \cap b_n$</span>,  where each <span class="math-container">$a_i$</span> and each <span class="math-container">$b_i$</span> is a disjunction, find the largest <span class="math-container">$k$</span> such that, for some assignment, some <span class="math-container">$k$</span> clauses <span class="math-container">$a_{i1},\ldots,a_{ik}$</span> and some <span class="math-container">$k$</span> clauses <span class="math-container">$b_{j1},\ldots,b_{jk}$</span> are true.</p>

<p>To illustrate the difference between the problems, suppose we have two assignments: one assignment satisfies 10 clauses in <span class="math-container">$\phi_a$</span> and 1 clause in <span class="math-container">$\phi_b$</span>, while another assignment satisfies 5 clauses in <span class="math-container">$\phi_a$</span> and 4 clauses in <span class="math-container">$\phi_b$</span>. Then, MAX-SAT (on <span class="math-container">$\phi_a \cap \phi_b$</span>) would prefer the first assignment since it satisfies <span class="math-container">$11&gt;9$</span> clauses overall, while MAX-MIN-SAT would prefer the second assignment since it satisfies at least <span class="math-container">$4&gt;1$</span> clauses in both formulas.</p>

<p>This problem is obviously NP-hard, so I am looking for reasonable approximations.</p>

<p>As a first approximation, suppose each formula is a conjunction of <span class="math-container">$n$</span> clauses, and each clause is a disjunction of <span class="math-container">$l$</span> variables. Suppose we set each variable randomly. Then, each clause is unsatisfied with probability <span class="math-container">$2^{-l}$</span>. So the expected number of unsatisfied clauses in each formula is <span class="math-container">$2^{-l}n$</span>. So the expected number of unsatisfied clauses in both formulas is <span class="math-container">$2^{1-l}n$</span>. So there exists an assignment in which the total number of unsatisfied clauses is at most <span class="math-container">$2^{1-l}n$</span>. In that assignment, in each formula, at least  <span class="math-container">$(1-2^{1-l})n$</span> clauses are satisfied. 
So we have a constant-factor <span class="math-container">$(1-2^{1-l})$</span> approximation to MAX MIN SAT. </p>

<p>Is there a better approximation? </p>

<p><sub><a href="https://cs.stackexchange.com/q/100375/1342">Posted some weeks ago in cs.SE,</a> with no replies</sub></p></div>
    </summary>
    <updated>2019-01-09T15:31:20Z</updated>
    <published>2019-01-09T15:31:20Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="approximation-algorithms"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="optimization"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="sat"/>
    <author>
      <name>Erel Segal-Halevi</name>
      <uri>https://cstheory.stackexchange.com/users/9453</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-09T22:21:10Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-2727898493587029341</id>
    <link href="https://blog.computationalcomplexity.org/feeds/2727898493587029341/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/01/search-versus-decision.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/2727898493587029341" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/2727898493587029341" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/01/search-versus-decision.html" rel="alternate" type="text/html"/>
    <title>Search versus Decision</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Shockingly I've never done a post on search versus decision, one of the more interesting dualities in complexity. In short: Decision: Is there a needle in the haystack? Search: Find the needle.<br/>
<br/>
In Satisfiability, or any other NP-complete problem, the two problems are essentially equivalent. If you can decided SAT you can find a solution (good homework problem) or even the best solution. Often people mix up the two, where people say finding the shortest Traveling Salesman Tour is NP-complete, <a href="https://blog.computationalcomplexity.org/2014/01/is-traveling-salesman-np-complete.html">usually</a> without getting into too much trouble.<br/>
<br/>
Decision is always at least as easy as search: If you have a solution you know there is one. What about the other direction? We can't actually prove search is hard without separating P and NP, but we have our conjectures.<br/>
<br/>
Sometimes both are easy. We can easily find the maximum weighted matching.<br/>
<br/>
Sometimes decision is easy and search is supposedly hard: Composite Numbers. The search version is factoring.<br/>
<br/>
Sometimes decision is trivial (i.e. they always exist) and search is still hard. Nash Equilibria. <a href="https://blog.computationalcomplexity.org/2006/05/dispersing-ramsey-graphs.html">Ramsey Graphs</a>.<br/>
<br/>
Often we ask whether search reduces to decision? If you have some oracle (magic black box) that answered decision questions, can you solve the search problem efficiently? SAT has this property, as does Matching (for trivial reasons). Nash Equilibrium and Composite Numbers likely don't.<br/>
<br/>
Graph Isomorphism does, i.e., given an oracle for graph isomorphism you can find the isomorphism (another good homework problem).<br/>
<br/>
There's also an interesting non-adaptive version. Given a SAT formula can you find an assignment with questions to a SAT oracle that all have to be asked at the same time?<br/>
<br/>
Here we get a probable yes. If the formula has one solution you can find it by asking for each bit of the solution. <a href="https://blog.computationalcomplexity.org/2006/09/favorite-theorems-unique-witnesses.html">Randomly you can reduce SAT to several formulas</a>, one of which is likely to have a single assignment that is also an assignment of the original formula. With standard hardness assumptions <a href="https://blog.computationalcomplexity.org/2006/07/full-derandomization.html">you can eliminate the randomness</a>.<br/>
<br/>
Is the same true for graph isomorphism? I think that's still open.</div>
    </content>
    <updated>2019-01-09T13:10:00Z</updated>
    <published>2019-01-09T13:10:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>https://plus.google.com/101693130490639305932</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>https://plus.google.com/101693130490639305932</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-01-09T21:43:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.02441</id>
    <link href="http://arxiv.org/abs/1901.02441" rel="alternate" type="text/html"/>
    <title>Lower bounds for maximal matchings and maximal independent sets</title>
    <feedworld_mtime>1546992000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Balliu:Alkida.html">Alkida Balliu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brandt:Sebastian.html">Sebastian Brandt</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hirvonen:Juho.html">Juho Hirvonen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Olivetti:Dennis.html">Dennis Olivetti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rabie:Mika=euml=l.html">Mikaël Rabie</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Suomela:Jukka.html">Jukka Suomela</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.02441">PDF</a><br/><b>Abstract: </b>There are distributed graph algorithms for finding maximal matchings and
maximal independent sets in $O(\Delta + \log^* n)$ communication rounds; here
$n$ is the number of nodes and $\Delta$ is the maximum degree. The lower bound
by Linial (1992) shows that the dependency on $n$ is optimal: these problems
cannot be solved in $o(\log^* n)$ rounds even if $\Delta = 2$.
</p>
<p>However, the dependency on $\Delta$ is a long-standing open question, and
there is currently an exponential gap between the upper and lower bounds.
</p>
<p>We prove that the upper bounds are tight. We show that maximal matchings and
maximal independent sets cannot be found in $o(\Delta + \log \log n / \log \log
\log n)$ rounds. Our lower bound holds for deterministic and randomized
distributed algorithms in the LOCAL model of distributed computing.
</p></div>
    </summary>
    <updated>2019-01-09T02:20:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-01-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.02393</id>
    <link href="http://arxiv.org/abs/1901.02393" rel="alternate" type="text/html"/>
    <title>Fair Algorithms for Clustering</title>
    <feedworld_mtime>1546992000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bera:Suman_K=.html">Suman K. Bera</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chakrabarty:Deeparnab.html">Deeparnab Chakrabarty</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Negahbani:Maryam.html">Maryam Negahbani</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.02393">PDF</a><br/><b>Abstract: </b>We study clustering problems under the lens of {\em algorithmic fairness}
inspired by the disparate impact doctrine. Given a collection of points
containing many {\em protected groups}, the goal is to find good clustering
solutions where each cluster {\em fairly represents} each group. We allow the
user to specify the parameters that define fair representation, and this
flexibility makes our model significantly more general than the recent models
of Chierichetti et al. (NIPS 2017) and R\"osner and Schmidt (ICALP 2018). Our
main result is a simple algorithm that, for any $\ell_p$-norm including the
$k$-center, $k$-median, and $k$-means objectives, transforms any clustering
solution to a fair one with only a slight loss in quality.
</p></div>
    </summary>
    <updated>2019-01-09T02:22:25Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.02209</id>
    <link href="http://arxiv.org/abs/1901.02209" rel="alternate" type="text/html"/>
    <title>Subset Feedback Vertex Set in Chordal and Split Graphs</title>
    <feedworld_mtime>1546992000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Philip:Geevarghese.html">Geevarghese Philip</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rajan:Varun.html">Varun Rajan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saurabh:Saket.html">Saket Saurabh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tale:Prafullkumar.html">Prafullkumar Tale</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.02209">PDF</a><br/><b>Abstract: </b>In the \textsc{Subset Feedback Vertex Set (Subset-FVS)} problem the input is
a graph $G$, a subset \(T\) of vertices of \(G\) called the `terminal'
vertices, and an integer $k$. The task is to determine whether there exists a
subset of vertices of cardinality at most $k$ which together intersect all
cycles which pass through the terminals. \textsc{Subset-FVS} generalizes
several well studied problems including \textsc{Feedback Vertex Set} and
\textsc{Multiway Cut}. This problem is known to be \NP-Complete even in split
graphs. Cygan et al. proved that \textsc{Subset-FVS} is fixed parameter
tractable (\FPT) in general graphs when parameterized by $k$ [SIAM J. Discrete
Math (2013)]. In split graphs a simple observation reduces the problem to an
equivalent instance of the $3$-\textsc{Hitting Set} problem with same solution
size. This directly implies, for \textsc{Subset-FVS} \emph{restricted to split
graphs}, (i) an \FPT algorithm which solves the problem in $\OhStar(2.076^k)$
time \footnote{The \(\OhStar()\) notation hides polynomial factors.}% for
\textsc{Subset-FVS} in Chordal % Graphs [Wahlstr\"om, Ph.D. Thesis], and (ii) a
kernel of size $\mathcal{O}(k^3)$. We improve both these results for
\textsc{Subset-FVS} on split graphs; we derive (i) a kernel of size
$\mathcal{O}(k^2)$ which is the best possible unless $\NP \subseteq \coNP/{\sf
poly}$, and (ii) an algorithm which solves the problem in time
$\mathcal{O}^*(2^k)$. Our algorithm, in fact, solves \textsc{Subset-FVS} on the
more general class of \emph{chordal graphs}, also in $\mathcal{O}^*(2^k)$ time.
</p></div>
    </summary>
    <updated>2019-01-09T02:22:02Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.02166</id>
    <link href="http://arxiv.org/abs/1901.02166" rel="alternate" type="text/html"/>
    <title>K-Core Minimization: A Game Theoretic Approach</title>
    <feedworld_mtime>1546992000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Medya:Sourav.html">Sourav Medya</a>, Tiyani Ma, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silva:Arlei.html">Arlei Silva</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singh:Ambuj.html">Ambuj Singh</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.02166">PDF</a><br/><b>Abstract: </b>$K$-cores are maximal induced subgraphs where all vertices have degree at
least $k$. These dense patterns have applications in community detection,
network visualization and protein function prediction. However, $k$-cores can
be quite unstable to network modifications, which motivates the question: How
resilient is the k-core structure of a network, such as the Web or Facebook, to
edge deletions? We investigate this question from an algorithmic perspective.
More specifically, we study the problem of computing a small set of edges for
which the removal minimizes the $k$-core structure of a network.
</p>
<p>This paper provides a comprehensive characterization of the hardness of the
$k$-core minimization problem (KCM), including innaproximability and
fixed-parameter intractability. Motivated by such a challenge in terms of
algorithm design, we propose a novel algorithm inspired by Shapley value---a
cooperative game-theoretic concept--- that is able to leverage the strong
interdependencies in the effects of edge removals in the search space. As
computing Shapley values is also NP-hard, we efficiently approximate them using
a randomized algorithm with probabilistic guarantees. Our experiments, using
several real datasets, show that the proposed algorithm outperforms competing
solutions in terms of $k$-core minimization while being able to handle large
graphs. Moreover, we illustrate how KCM can be applied in the analysis of the
$k$-core resilience of networks.
</p></div>
    </summary>
    <updated>2019-01-09T02:21:45Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.02070</id>
    <link href="http://arxiv.org/abs/1901.02070" rel="alternate" type="text/html"/>
    <title>Convolutional Neural Networks on non-uniform geometrical signals using Euclidean spectral transformation</title>
    <feedworld_mtime>1546992000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Chiyu "Max" Jiang, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Dequan.html">Dequan Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Jingwei.html">Jingwei Huang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Marcus:Philip.html">Philip Marcus</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nie=szlig=ner:Matthias.html">Matthias Nießner</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.02070">PDF</a><br/><b>Abstract: </b>Convolutional Neural Networks (CNN) have been successful in processing data
signals that are uniformly sampled in the spatial domain (e.g., images).
However, most data signals do not natively exist on a grid, and in the process
of being sampled onto a uniform physical grid suffer significant aliasing error
and information loss. Moreover, signals can exist in different topological
structures as, for example, points, lines, surfaces and volumes. It has been
challenging to analyze signals with mixed topologies (for example, point cloud
with surface mesh). To this end, we develop mathematical formulations for
Non-Uniform Fourier Transforms (NUFT) to directly, and optimally, sample
nonuniform data signals of different topologies defined on a simplex mesh into
the spectral domain with no spatial sampling error. The spectral transform is
performed in the Euclidean space, which removes the translation ambiguity from
works on the graph spectrum. Our representation has four distinct advantages:
(1) the process causes no spatial sampling error during the initial sampling,
(2) the generality of this approach provides a unified framework for using CNNs
to analyze signals of mixed topologies, (3) it allows us to leverage
state-of-the-art backbone CNN architectures for effective learning without
having to design a particular architecture for a particular data structure in
an ad-hoc fashion, and (4) the representation allows weighted meshes where each
element has a different weight (i.e., texture) indicating local properties. We
achieve results on par with the state-of-the-art for the 3D shape retrieval
task, and a new state-of-the-art for the point cloud to surface reconstruction
task.
</p></div>
    </summary>
    <updated>2019-01-09T02:23:02Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-01-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/01/08/postdoc-at-saint-louis-university-apply-by-january-21-2019/</id>
    <link href="https://cstheory-jobs.org/2019/01/08/postdoc-at-saint-louis-university-apply-by-january-21-2019/" rel="alternate" type="text/html"/>
    <title>Postdoc at Saint Louis University (apply by January 21, 2019)</title>
    <summary>I have a openings for 2 postdocs, both starting in 2019: One is flexible in focus, fitting under the broad categories of computational topology/geometry and algorithms. The other is for a shape simplification project, jointly supervised by Dr. David Letscher, focusing on designing and implementing algorithms that use persistent homology as well as other tools […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I have a openings for 2 postdocs, both starting in 2019: One is flexible in focus, fitting under the broad categories of computational topology/geometry and algorithms. The other is for a shape simplification project, jointly supervised by Dr. David Letscher, focusing on designing and implementing algorithms that use persistent homology as well as other tools from computational topology.</p>
<p>Website: <a href="http://cs.slu.edu/~chambers/">http://cs.slu.edu/~chambers/</a><br/>
Email: erin.chambers@slu.edu</p></div>
    </content>
    <updated>2019-01-08T18:05:57Z</updated>
    <published>2019-01-08T18:05:57Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-01-09T22:20:53Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42176</id>
    <link href="https://cstheory.stackexchange.com/questions/42176/minimal-dfa-corresponding-to-complement-of-a-language" rel="alternate" type="text/html"/>
    <title>minimal DFA corresponding to complement of a language [on hold]</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>is it necessary that number of states in minimal DFA for a language corresponding to L' is equal to the number of states in minimal DFA of the language corresponding to L. </p>

<p>here L' is the complement of the language L.</p>

<p>for example,
number of states in the minimal DFA for the language</p>

<p>L={set of all string containing 01 and 011 as the substring over the alphabet {0,1}}</p>

<p>is 4. </p>

<p>if i'm making the DFA for the complement of this language, i'm getting 3. 
but is it true that it should be 4?</p>

<p>please help!! which one is correct-- 3 or 4?</p></div>
    </summary>
    <updated>2019-01-08T14:41:22Z</updated>
    <published>2019-01-08T14:41:22Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="automata-theory"/>
    <author>
      <name>aambazinga</name>
      <uri>https://cstheory.stackexchange.com/users/51682</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-09T22:21:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.01968</id>
    <link href="http://arxiv.org/abs/1901.01968" rel="alternate" type="text/html"/>
    <title>A semi-structured approach to curvilinear mesh generation around streamlined bodies</title>
    <feedworld_mtime>1546905600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Julian Marcon, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peir=oacute=:Joaquim.html">Joaquim Peiró</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moxey:David.html">David Moxey</a>, Nico Bergemann, Henry Bucklow, Mark Gammon <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.01968">PDF</a><br/><b>Abstract: </b>We present an approach for robust high-order mesh generation specially
tailored to streamlined bodies. The method is based on a semi-sructured
approach which combines the high quality of structured meshes in the near-field
with the flexibility of unstructured meshes in the far-field. We utilise medial
axis technology to robustly partition the near-field into blocks which can be
meshed coarsely with a linear swept mesher. A high-order mesh of the near-field
is then generated and split using an isoparametric approach which allows us to
obtain highly stretched elements aligned with the flow field. Special treatment
of the partition is performed on the wing root juntion and the trailing edge
--- into the wake --- to obtain an H-type mesh configuration with anisotropic
hexahedra ideal for the strong shear of high Reynolds number simulations. We
then proceed to discretise the far-field using traditional robust tetrahedral
meshing tools. This workflow is made possible by two sets of tools: CADfix,
focused on CAD system, the block partitioning of the near-field and the
generation of a linear mesh; and NekMesh, focused on the curving of the
high-order mesh and the generation of highly-stretched boundary layer elements.
We demonstrate this approach on a NACA0012 wing attached to a wall and show
that a gap between the wake partition and the wall can be inserted to remove
the dependency of the partitioning procedure on the local geometry.
</p></div>
    </summary>
    <updated>2019-01-08T23:26:43Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-01-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.01944</id>
    <link href="http://arxiv.org/abs/1901.01944" rel="alternate" type="text/html"/>
    <title>A Compact Representation of Raster Time Series</title>
    <feedworld_mtime>1546905600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cruces:Nataly.html">Nataly Cruces</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Seco:Diego.html">Diego Seco</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guti=eacute=rrez:Gilberto.html">Gilberto Gutiérrez</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.01944">PDF</a><br/><b>Abstract: </b>The raster model is widely used in Geographic Information Systems to
represent data that vary continuously in space, such as temperatures,
precipitations, elevation, among other spatial attributes. In applications like
weather forecast systems, not just a single raster, but a sequence of rasters
covering the same region at different timestamps, known as a raster time
series, needs to be stored and queried. Compact data structures have proven
successful to provide space-efficient representations of rasters with query
capabilities. Hence, a naive approach to save space is to use such a
representation for each raster in a time series. However, in this paper we show
that it is possible to take advantage of the temporal locality that exists in a
raster time series to reduce the space necessary to store it while keeping
competitive query times for several types of queries.
</p></div>
    </summary>
    <updated>2019-01-08T23:21:39Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.01926</id>
    <link href="http://arxiv.org/abs/1901.01926" rel="alternate" type="text/html"/>
    <title>An in-place, subquadratic algorithm for permutation inversion</title>
    <feedworld_mtime>1546905600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Grzegorz Guśpiel <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.01926">PDF</a><br/><b>Abstract: </b>We assume the permutation $\pi$ is given by an $n$-element array in which the
$i$-th element denotes the value $\pi(i)$. Constructing its inverse in-place
(i.e. using $O(\log{n})$ bits of additional memory) can be achieved in linear
time with a simple algorithm. Limiting the numbers that can be stored in our
array to the range $[1...n]$ still allows a straightforward $O(n^2)$ time
solution. The time complexity can be improved using randomization, but this
only improves the expected, not the pessimistic running time. We present a
deterministic algorithm that runs in $O(n^{3/2})$ time.
</p></div>
    </summary>
    <updated>2019-01-08T23:23:19Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.01870</id>
    <link href="http://arxiv.org/abs/1901.01870" rel="alternate" type="text/html"/>
    <title>Coresets for $(k,l)$-Clustering under the Fr\'echet Distance</title>
    <feedworld_mtime>1546905600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rohde:Dennis.html">Dennis Rohde</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.01870">PDF</a><br/><b>Abstract: </b>Clustering is the task of partitioning a given set of geometric objects. This
is thoroughly studied when the objects are points in the euclidean space. There
are also several approaches for points in general metric spaces. In this thesis
we consider clustering polygonal curves, i.e., curves composed of line
segments, under the Fr\'echet distance. We obtain clusterings by minimizing an
objective function, which yields a set of centers that induces a partition of
the input.
</p>
<p>The objective functions we consider is the so called $(k,l)$-\textsc{center},
where we are to find the $k$ center-curves that minimize the maximum distance
between any input-curve and a nearest center-curve and the so called
$k$-\textsc{median}, where we are to find the $k$ center-curves that minimize
the sum of the distances between the input-curves and a nearest center-curve.
</p>
<p>Given a set of $n$ polygonal curves, we are interested in reducing this set
to an $\epsilon$-coreset, i.e., a notably smaller set of curves that has a very
similar clustering-behavior. We develop a construction method for such
$\epsilon$-coresets for the $(k,l)$-\textsc{center}, that yields
$\epsilon$-coresets of size of a polynomial of $\frac{1}{\epsilon}$, in time
linear in $n$ and a polynomial of $\frac{1}{\epsilon}$, for line segments.
Also, we develop a construction technique for the $(k,l)$-\textsc{center} that
yields $\epsilon$-coresets of size exponential in $m$ with basis
$\frac{1}{\epsilon}$, in time sub-quadratic in $n$ and exponential in $m$ with
basis $\frac{1}{\epsilon}$, for general polygonal curves. Finally, we develop a
construction method for the $k$-\textsc{median}, that yields
$\epsilon$-coresets of size polylogarithmic in $n$ and a polynomial of
$\frac{1}{\epsilon}$, in time linear in $n$ and a polynomial of
$\frac{1}{\epsilon}$.
</p></div>
    </summary>
    <updated>2019-01-08T23:27:02Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-01-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.01825</id>
    <link href="http://arxiv.org/abs/1901.01825" rel="alternate" type="text/html"/>
    <title>Bloom Multifilters for Multiple Set Matching</title>
    <feedworld_mtime>1546905600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Concas:Francesco.html">Francesco Concas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Pengfei.html">Pengfei Xu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoque:Mohammad_A=.html">Mohammad A. Hoque</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tarkoma:Sasu.html">Sasu Tarkoma</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lu:Jiaheng.html">Jiaheng Lu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.01825">PDF</a><br/><b>Abstract: </b>Bloom filter is a space-efficient probabilistic data structure for checking
elements' membership in a set. Given multiple sets, however, a standard Bloom
filter is not sufficient when looking for the items to which an element or a
set of input elements belong to. In this article, we solve multiple set
matching problem by proposing two efficient Bloom Multifilters called Bloom
Matrix and Bloom Vector. Both of them are space efficient and answer queries
with a set of identifiers for multiple set matching problems. We show that the
space efficiency can be optimized further according to the distribution of
labels among multiple sets: Uniform and Zipf. While both of them are space
efficient, Bloom Vector can efficiently exploit Zipf distribution of data for
further space reduction. Our results also highlight that basic ADD and LOOKUP
operations on Bloom Matrix are faster than on Bloom Vector. However, Bloom
Matrix does not meet the theoretical false positive rate of less than $10^{-2}$
for LOOKUP operations if the represented data is not uniformly distributed
among multiple sets. Consequently, we introduce Bloom Test to determine which
structure is suitable for an arbitrary input dataset.
</p></div>
    </summary>
    <updated>2019-01-08T23:23:20Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.01763</id>
    <link href="http://arxiv.org/abs/1901.01763" rel="alternate" type="text/html"/>
    <title>Approximate Discontinuous Trajectory Hotspots</title>
    <feedworld_mtime>1546905600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rudi:Ali_Gholami.html">Ali Gholami Rudi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.01763">PDF</a><br/><b>Abstract: </b>A hotspot is an axis-aligned square of fixed side length $s$, the duration of
the presence of an entity moving in the plane in which is maximised. An exact
hotspot of a polygonal trajectory with $n$ edges can be found in $O(n^2)$.
Defining a $c$-approximate hotspot as an axis-aligned square of side length
$cs$, in which the duration of the entity's presence is no less than that of an
exact hotspot, in this paper we present an algorithm to find a $(1 +
\epsilon)$-approximate hotspot of a polygonal trajectory with the time
complexity $O({n\phi \over \epsilon} \log {n\phi \over \epsilon})$, where
$\phi$ is the ratio of average trajectory edge length to $s$.
</p></div>
    </summary>
    <updated>2019-01-08T23:27:06Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-01-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.01710</id>
    <link href="http://arxiv.org/abs/1901.01710" rel="alternate" type="text/html"/>
    <title>Approximate-Closed-Itemset Mining for Streaming Data Under Resource Constraint</title>
    <feedworld_mtime>1546905600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yamamoto:Yoshitaka.html">Yoshitaka Yamamoto</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tabei:Yasuo.html">Yasuo Tabei</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Iwanuma:Koji.html">Koji Iwanuma</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.01710">PDF</a><br/><b>Abstract: </b>Here, we present a novel algorithm for frequent itemset mining for streaming
data (FIM-SD). For the past decade, various FIM-SD methods in one-pass
approximation settings have been developed to approximate the frequency of each
itemset. These approaches can be categorized into two approximation types:
parameter-constrained (PC) mining and resource-constrained (RC) mining. PC
methods control the maximum error that can be included in the frequency based
on a pre-defined parameter. In contrast, RC methods limit the maximum memory
consumption based on resource constraints. However, the existing PC methods can
exponentially increase the memory consumption, while the existing RC methods
can rapidly increase the maximum error. In this study, we address this problem
by introducing the notion of a condensed representation, called a
$\Delta$-covered set, to the RC approximation. This notion is regarded as an
extension of the closedness compression and when $\Delta = 0$, the solution
corresponds to an ordinary closed itemset. The algorithm searches for such
approximate closed itemsets that can restore the frequent itemsets and their
frequencies under resource constraint while the maximum error is bounded by an
integer, $\Delta$. We first propose a one-pass approximation algorithm to find
the condensed solution. Then, we improve the basic algorithm by introducing a
unified PC-RC approximation approach. Finally, we empirically demonstrate that
the proposed algorithm significantly outperforms the state-of-the-art PC and RC
methods for FIM-SD.
</p></div>
    </summary>
    <updated>2019-01-08T23:21:16Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.01665</id>
    <link href="http://arxiv.org/abs/1901.01665" rel="alternate" type="text/html"/>
    <title>Communication cost of consensus for nodes with limited memory</title>
    <feedworld_mtime>1546905600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Giulia Fanti, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Holden:Nina.html">Nina Holden</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peres:Yuval.html">Yuval Peres</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ranade:Gireeja.html">Gireeja Ranade</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.01665">PDF</a><br/><b>Abstract: </b>Motivated by applications in blockchains and sensor networks, we consider a
model of $n$ nodes trying to reach consensus on their majority bit. Each node
$i$ is assigned a bit at time zero, and is a finite automaton with $m$ bits of
memory (i.e., $2^m$ states) and a Poisson clock. When the clock of $i$ rings,
$i$ can choose to communicate, and is then matched to a uniformly chosen node
$j$. The nodes $j$ and $i$ may update their states based on the state of the
other node. Previous work has focused on minimizing the time to consensus and
the probability of error, while our goal is minimizing the number of
communications. We show that when $m&gt;3 \log\log\log(n)$, consensus can be
reached at linear communication cost, but this is impossible if
$m&lt;\log\log\log(n)$. We also study a synchronous variant of the model, where
our upper and lower bounds on $m$ for achieving linear communication cost are
$2\log\log\log(n)$ and $\log\log\log(n)$, respectively. A key step is to
distinguish when nodes can become aware of knowing the majority bit and stop
communicating. We show that this is impossible if their memory is too low.
</p></div>
    </summary>
    <updated>2019-01-08T23:21:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.01651</id>
    <link href="http://arxiv.org/abs/1901.01651" rel="alternate" type="text/html"/>
    <title>Tooth morphometry using quasi-conformal theory</title>
    <feedworld_mtime>1546905600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Choi:Gary_P=_T=.html">Gary P. T. Choi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chan:Hei_Long.html">Hei Long Chan</a>, Robin Yong, Sarbin Ranjitkar, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brook:Alan.html">Alan Brook</a>, Grant Townsend, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Ke.html">Ke Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lui:Lok_Ming.html">Lok Ming Lui</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.01651">PDF</a><br/><b>Abstract: </b>Shape analysis is important in anthropology, bioarchaeology and forensic
science for interpreting useful information from human remains. In particular,
teeth are morphologically stable and hence well-suited for shape analysis. In
this work, we propose a framework for tooth morphometry using quasi-conformal
theory. Landmark-matching Teichm\"uller maps are used for establishing a 1-1
correspondence between tooth surfaces with prescribed anatomical landmarks.
Then, a quasi-conformal statistical shape analysis model based on the
Teichm\"uller mapping results is proposed for building a tooth classification
scheme. We deploy our framework on a dataset of human premolars to analyze the
tooth shape variation among genders and ancestries. Experimental results show
that our method achieves much higher classification accuracy with respect to
both gender and ancestry when compared to the existing methods. Furthermore,
our model reveals the underlying tooth shape difference between different
genders and ancestries in terms of the local geometric distortion and
curvatures.
</p></div>
    </summary>
    <updated>2019-01-08T23:54:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-01-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.01637</id>
    <link href="http://arxiv.org/abs/1901.01637" rel="alternate" type="text/html"/>
    <title>Fine-grained quantum supremacy of the one-clean-qubit model</title>
    <feedworld_mtime>1546905600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Morimae:Tomoyuki.html">Tomoyuki Morimae</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tamaki:Suguru.html">Suguru Tamaki</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.01637">PDF</a><br/><b>Abstract: </b>The one-clean-qubit model (or the DQC1 model) is a restricted model of
quantum computing where all but a single input qubits are maximally mixed. It
is known that output probability distributions of the DQC1 model cannot be
classically sampled in polynomial-time unless the polynomial-time hierarchy
collapses. In this paper, we show that even superpolynomial-time and
exponential-time classical samplings are impossible under certain fine-grained
complexity conjectures. We also show similar fine-grained quantum supremacy
results for the Hadamard-classical circuit with one-qubit (HC1Q) model, which
is another sub-universal model with a classical circuit sandwiched by two
Hadamard layers.
</p></div>
    </summary>
    <updated>2019-01-08T23:20:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-01-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.01630</id>
    <link href="http://arxiv.org/abs/1901.01630" rel="alternate" type="text/html"/>
    <title>Smaller Cuts, Higher Lower Bounds</title>
    <feedworld_mtime>1546905600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Abboud:Amir.html">Amir Abboud</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Censor=Hillel:Keren.html">Keren Censor-Hillel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Khoury:Seri.html">Seri Khoury</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Paz:Ami.html">Ami Paz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.01630">PDF</a><br/><b>Abstract: </b>This paper proves strong lower bounds for distributed computing in the
CONGEST model, by presenting the bit-gadget: a new technique for constructing
graphs with small cuts.
</p>
<p>The contribution of bit-gadgets is twofold. First, developing careful sparse
graph constructions with small cuts extends known techniques to show a
near-linear lower bound for computing the diameter, a result previously known
only for dense graphs. Moreover, the sparseness of the construction plays a
crucial role in applying it to approximations of various distance computation
problems, drastically improving over what can be obtained when using dense
graphs.
</p>
<p>Second, small cuts are essential for proving super-linear lower bounds, none
of which were known prior to this work. In fact, they allow us to show
near-quadratic lower bounds for several problems, such as exact minimum vertex
cover or maximum independent set, as well as for coloring a graph with its
chromatic number. Such strong lower bounds are not limited to NP-hard problems,
as given by two simple graph problems in P which are shown to require a
quadratic and near-quadratic number of rounds. All of the above are optimal up
to logarithmic factors. In addition, in this context, the complexity of the
all-pairs-shortest-paths problem is discussed.
</p>
<p>Finally, it is shown that graph constructions for CONGEST lower bounds
translate to lower bounds for the semi-streaming model, despite being very
different in its nature.
</p></div>
    </summary>
    <updated>2019-01-08T23:22:39Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.01504</id>
    <link href="http://arxiv.org/abs/1901.01504" rel="alternate" type="text/html"/>
    <title>Walking the Dog Fast in Practice: Algorithm Engineering of the Fr\'echet Distance</title>
    <feedworld_mtime>1546905600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bringmann:Karl.html">Karl Bringmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/K=uuml=nnemann:Marvin.html">Marvin Künnemann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nusser:Andr=eacute=.html">André Nusser</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.01504">PDF</a><br/><b>Abstract: </b>The Fr\'echet distance provides a natural and intuitive measure for the
popular task of computing the similarity of two (polygonal) curves. While a
simple algorithm computes it in near-quadratic time, a strongly subquadratic
algorithm cannot exist unless the Strong Exponential Time Hypothesis fails.
Still, fast practical implementations of the Fr\'echet distance, in particular
for realistic input curves, are highly desirable. This has even lead to a
designated competition, the ACM SIGSPATIAL GIS Cup 2017: Here, the challenge
was to implement a near-neighbor data structure under the Fr\'echet distance.
The bottleneck of the top three implementations turned out to be precisely the
decision procedure for the Fr\'echet distance.
</p>
<p>In this work, we present a fast, certifying implementation for deciding the
Fr\'echet distance, in order to (1) complement its pessimistic worst-case
hardness by an empirical analysis on realistic input data and to (2) improve
the state of the art for the GIS Cup challenge. We experimentally evaluate our
implementation on a large benchmark consisting of several data sets (including
handwritten characters and GPS trajectories). Compared to the winning
implementation of the GIS Cup, we obtain running time improvements of up to
more than two orders of magnitude for the decision procedure and of up to a
factor of 30 for queries to the near-neighbor data structure.
</p></div>
    </summary>
    <updated>2019-01-08T23:52:18Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-01-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.01476</id>
    <link href="http://arxiv.org/abs/1901.01476" rel="alternate" type="text/html"/>
    <title>Maximum Matchings and Minimum Blocking Sets in $\Theta_6$-Graphs</title>
    <feedworld_mtime>1546905600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Therese Biedl, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Biniaz:Ahmad.html">Ahmad Biniaz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Irvine:Veronika.html">Veronika Irvine</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jain:Kshitij.html">Kshitij Jain</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kindermann:Philipp.html">Philipp Kindermann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lubiw:Anna.html">Anna Lubiw</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.01476">PDF</a><br/><b>Abstract: </b>$\Theta_6$-Graphs are important geometric graphs that have many applications
especially in wireless sensor networks. They are equivalent to Delaunay graphs
where empty equilateral triangles take the place of empty circles. We
investigate lower bounds on the size of maximum matchings in these graphs. The
best known lower bound is $n/3$, where $n$ is the number of vertices of the
graph. Babu et al. (2014) conjectured that any $\Theta_6$-graph has a perfect
matching (as is true for standard Delaunay graphs). Although this conjecture
remains open, we improve the lower bound to $(3n-8)/7$.
</p>
<p>We also relate the size of maximum matchings in $\Theta_6$-graphs to the
minimum size of a blocking set. Every edge of a $\Theta_6$-graph on point set
$P$ corresponds to an empty triangle that contains the endpoints of the edge
but no other point of $P$. A blocking set has at least one point in each such
triangle. We prove that the size of a maximum matching is at least $\beta(n)/2$
where $\beta(n)$ is the minimum, over all $\Theta_6$-graphs with $n$ vertices,
of the minimum size of a blocking set. In the other direction, lower bounds on
matchings can be used to prove bounds on $\beta$, allowing us to show that
$\beta(n)\geq 3n/4-2$.
</p></div>
    </summary>
    <updated>2019-01-08T23:52:59Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-01-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.01412</id>
    <link href="http://arxiv.org/abs/1901.01412" rel="alternate" type="text/html"/>
    <title>New Algorithms and Lower Bounds for All-Pairs Max-Flow in Undirected Graphs</title>
    <feedworld_mtime>1546905600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Abboud:Amir.html">Amir Abboud</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krauthgamer:Robert.html">Robert Krauthgamer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Trabelsi:Ohad.html">Ohad Trabelsi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.01412">PDF</a><br/><b>Abstract: </b>We investigate the time-complexity of the $\textbf{All-Pairs Max-Flow}$
problem: Given a graph with $n$ nodes and $m$ edges, compute for all pairs of
nodes the maximum-flow value between them. If $\textbf{Max-Flow}$ (the version
with a given source-sink pair $s,t$) can be solved in time $T(m)$, then an
$O(n^2) \cdot T(m)$ is a trivial upper bound. But can we do better?
</p>
<p>For directed graphs, recent results in fine-grained complexity suggest that
this time bound is essentially optimal. In contrast, for undirected graphs with
edge capacities, a seminal algorithm of Gomory and Hu (1961) runs in much
faster time $O(n)\cdot T(m)$. Under the plausible assumption that
$\textbf{Max-Flow}$ can be solved in near-linear time $m^{1+o(1)}$, this
half-century old algorithm yields an $nm^{1+o(1)}$ bound. Several other
algorithms have been designed through the years, including $\tilde{O}(mn)$ time
for unit-capacity edges (unconditionally), but none of them break the $O(mn)$
barrier. Meanwhile, no super-linear lower bound was shown for undirected
graphs.
</p>
<p>We design the first hardness reductions for $\textbf{All-Pairs Max-Flow}$ in
undirected graphs, giving an essentially optimal lower bound for the
$\textit{node-capacities}$ setting. For edge capacities, our efforts to prove
similar lower bounds have failed, but we have discovered a surprising new
algorithm that breaks the $O(mn)$ barrier for graphs with unit-capacity edges!
Assuming $T(m)=m^{1+o(1)}$, our algorithm runs in time $m^{3/2 +o(1)}$ and
outputs a cut-equivalent tree (similarly to the Gomory-Hu algorithm). Even with
current $\textbf{Max-Flow}$ algorithms we improve state-of-the-art in many
density regimes.
</p></div>
    </summary>
    <updated>2019-01-08T23:25:38Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.01341</id>
    <link href="http://arxiv.org/abs/1901.01341" rel="alternate" type="text/html"/>
    <title>Sheaves: A Topological Approach to Big Data</title>
    <feedworld_mtime>1546905600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vepstas:Linas.html">Linas Vepstas</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.01341">PDF</a><br/><b>Abstract: </b>This document develops general concepts useful for extracting knowledge
embedded in large graphs or datasets that have pair-wise relationships, such as
cause-effect-type relations. Almost no underlying assumptions are made, other
than that the data can be presented in terms of pair-wise relationships between
objects/events. This assumption is used to mine for patterns in the dataset,
defining a reduced graph or dataset that boils-down or concentrates information
into a more compact form. The resulting extracted structure or set of patterns
are manifestly symbolic in nature, as they capture and encode the graph
structure of the dataset in terms of a (generative) grammar. This structure is
identified as having the formal mathematical structure of a sheaf. In essence,
this paper introduces the basic concepts of sheaf theory into the domain of
graphical datasets.
</p></div>
    </summary>
    <updated>2019-01-08T23:25:34Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-01-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1901.01255</id>
    <link href="http://arxiv.org/abs/1901.01255" rel="alternate" type="text/html"/>
    <title>Generic Primitive Detection in Point Clouds Using Novel Minimal Quadric Fits</title>
    <feedworld_mtime>1546905600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Birdal:Tolga.html">Tolga Birdal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Busam:Benjamin.html">Benjamin Busam</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Navab:Nassir.html">Nassir Navab</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Ilic:Slobodan.html">Slobodan Ilic</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sturm:Peter.html">Peter Sturm</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1901.01255">PDF</a><br/><b>Abstract: </b>We present a novel and effective method for detecting 3D primitives in
cluttered, unorganized point clouds, without axillary segmentation or type
specification. We consider the quadric surfaces for encapsulating the basic
building blocks of our environments - planes, spheres, ellipsoids, cones or
cylinders, in a unified fashion. Moreover, quadrics allow us to model higher
degree of freedom shapes, such as hyperboloids or paraboloids that could be
used in non-rigid settings.
</p>
<p>We begin by contributing two novel quadric fits targeting 3D point sets that
are endowed with tangent space information. Based upon the idea of aligning the
quadric gradients with the surface normals, our first formulation is exact and
requires as low as four oriented points. The second fit approximates the first,
and reduces the computational effort. We theoretically analyze these fits with
rigor, and give algebraic and geometric arguments. Next, by re-parameterizing
the solution, we devise a new local Hough voting scheme on the null-space
coefficients that is combined with RANSAC, reducing the complexity from
$O(N^4)$ to $O(N^3)$ (three points). To the best of our knowledge, this is the
first method capable of performing a generic cross-type multi-object primitive
detection in difficult scenes without segmentation. Our extensive qualitative
and quantitative results show that our method is efficient and flexible, as
well as being accurate.
</p></div>
    </summary>
    <updated>2019-01-08T23:27:11Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-01-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42171</id>
    <link href="https://cstheory.stackexchange.com/questions/42171/minimum-relevant-variables-in-linear-system-additive-approximation" rel="alternate" type="text/html"/>
    <title>Minimum relevant variables in linear system - additive approximation</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In the problem <a href="https://en.wikipedia.org/wiki/Minimum_relevant_variables_in_linear_system" rel="nofollow noreferrer">Minimum Relevant Variables in Linear System</a> (Min-RVLS), the input is a linear system, e.g.:</p>

<p><span class="math-container">$$ A x = b $$</span></p>

<p>and the goal is to find a solution <span class="math-container">$x$</span> with as few nonzero variables as possible. </p>

<p>The problem is known to be NP-hard and hard to approximate to within a constant multiplicative factor (see the wikipedia page for details). </p>

<p>My question is: is anything known about <em>additive</em> approximations? In particular: what is the complexity of finding a solution that has at most <span class="math-container">$\text{OPT}+d$</span> nonzero variables, where <span class="math-container">$\text{OPT}$</span> is the smallest number of nonzero variables in a solution, and <span class="math-container">$d$</span> is some constant?</p></div>
    </summary>
    <updated>2019-01-07T16:08:53Z</updated>
    <published>2019-01-07T16:08:53Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="approximation-algorithms"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="optimization"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="linear-programming"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="approximation-hardness"/>
    <author>
      <name>Erel Segal-Halevi</name>
      <uri>https://cstheory.stackexchange.com/users/9453</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-09T22:21:10Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42169</id>
    <link href="https://cstheory.stackexchange.com/questions/42169/optimal-algorithm-to-compare-lines-of-different-files-without-repetition" rel="alternate" type="text/html"/>
    <title>Optimal algorithm to compare lines of different files without repetition [on hold]</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I have 1600 ASCII files with 1000 lines in each file. Each line has only one entry and is a floating point number e.g. 1.67923.
Let's denote the line1 of file1 with <code>L(1,1)</code>, line2 of file1 with <code>L(1,2)</code> and so forth to ...<code>L(1,1000)</code>. Similarly, line1 of file2 will be <code>L(2,1)</code> and the last line of file1600 will thus be <code>L(1600,1000)</code>.
My task is to come up with a memory efficient algorithm to compare all lines between each file and the lines within each file. Since, I have 1600 files and 1000 lines in each file, it will take approx. <code>10^12</code> calculations. These first comparisons will look like this:</p>

<pre><code>1. {L(1,1)-L(1,2)}, {L(1,1)-L(1,3)},....,{L(1,1)-L(1,1000)}
2. {L(1,1)-L(2,1)}, {L(1,1)-L(2,2)},....,{L(1,1)-L(2,1000)}
3. {L(1,1)-L(3,1)}, {L(1,1)-L(3,2)},....,{L(1,1)-L(3,1000)}
.
.
. 
</code></pre>

<p>Please note that I don't want repetitions i.e <code>{L(1,1)-L(2,1)} = {L(2,1)-L(1,1)}</code>.
I need to code this problem in Fortran but any help on a general scheme as to how the problem needs to be approached will be useful.
Thank you in advance!  </p></div>
    </summary>
    <updated>2019-01-07T15:12:49Z</updated>
    <published>2019-01-07T15:12:49Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="ds.algorithms"/>
    <author>
      <name>Abedin Y. Abedin</name>
      <uri>https://cstheory.stackexchange.com/users/51670</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-09T22:21:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://thmatters.wordpress.com/?p=1259</id>
    <link href="https://thmatters.wordpress.com/2019/01/07/catcs-mailing-list-and-sign-up-link/" rel="alternate" type="text/html"/>
    <title>CATCS mailing list and sign-up link</title>
    <summary>CATCS is starting up a new mailing list to send out annual newsletters. Messages will be sent out 1-2 times every year describing recent projects undertaken by the committee, funding opportunities, links to useful resources, etc. Anyone interested in hearing about our activities is welcome to sign up at this link. You do not have […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>CATCS is starting up a new mailing list to send out annual newsletters. Messages will be sent out 1-2 times every year describing recent projects undertaken by the committee, funding opportunities, links to useful resources, etc. Anyone interested in hearing about our activities is welcome to sign up at <a href="https://groups.google.com/forum/#!forum/catcs-news">this link</a>. You do not have to be a member of SIGACT to sign up.<span style="color: #000000; font-family: Arial, sans-serif;"><br/>
</span></p>
<div/></div>
    </content>
    <updated>2019-01-07T08:42:09Z</updated>
    <published>2019-01-07T08:42:09Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shuchic</name>
    </author>
    <source>
      <id>https://thmatters.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://thmatters.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://thmatters.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://thmatters.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://thmatters.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theory Matters</title>
      <updated>2019-01-09T22:21:08Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42167</id>
    <link href="https://cstheory.stackexchange.com/questions/42167/decomposition-for-a-certain-class-of-graphs" rel="alternate" type="text/html"/>
    <title>Decomposition for a certain class of graphs</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Suppose a graph, <span class="math-container">$G = (V,E)$</span> is characterized as a lattice/network of cliques as in the picture below. Does there exist some decomposition principle (i.e. on the right) for <span class="math-container">$G$</span>, that yields some special structure that may be used to explain efficiencies experienced with what are supposed to be combinatorial hard problems?</p>

<p><a href="https://i.stack.imgur.com/FTbx8.png" rel="nofollow noreferrer"><img alt="enter image description here" src="https://i.stack.imgur.com/FTbx8.png"/></a></p></div>
    </summary>
    <updated>2019-01-07T06:19:24Z</updated>
    <published>2019-01-07T06:19:24Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="graph-theory"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="graph-algorithms"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="co.combinatorics"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="treewidth"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="integer-lattice"/>
    <author>
      <name>Student</name>
      <uri>https://cstheory.stackexchange.com/users/51578</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-09T22:21:10Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42166</id>
    <link href="https://cstheory.stackexchange.com/questions/42166/algorithm-for-k-best-non-perfect-bipartite-matchings" rel="alternate" type="text/html"/>
    <title>Algorithm for K-best NON perfect bipartite matchings</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I was reading this great article: <a href="https://core.ac.uk/download/pdf/82129717.pdf" rel="nofollow noreferrer">https://core.ac.uk/download/pdf/82129717.pdf</a></p>

<p>It solves a generalization of the maximum sum assignment problem by finding the k best assignments and not only the best.
However, it only looks at perfect matchings. I'm am especially interested in bipartite matchings.</p>

<p>In particular, for the bipartite graphs, the Theorem 1 p. 161 uses the fact that the matchings are considered perfect.</p>

<p>How can I solve the k-best assignment problem for general bipartite graphs?</p></div>
    </summary>
    <updated>2019-01-06T23:47:08Z</updated>
    <published>2019-01-06T23:47:08Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="graph-algorithms"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="matching"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="bipartite-graphs"/>
    <author>
      <name>Labo</name>
      <uri>https://cstheory.stackexchange.com/users/43172</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-09T22:21:10Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-4355005625360509962</id>
    <link href="https://blog.computationalcomplexity.org/feeds/4355005625360509962/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/01/when-is-kilogram-not-kilogram.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4355005625360509962" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/4355005625360509962" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/01/when-is-kilogram-not-kilogram.html" rel="alternate" type="text/html"/>
    <title>When is a kilogram not a kilogram?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">A long long time ago the standards for meter's, kilograms, etc was an actual physical object.<br/>
<br/>
Those days are long gone of course. For example, the meter is defined is the length of the path traveled by light in 1/299,792,458 th of a second. Why such an odd number (can fractions be odd?)? Because they retrofitted it to what that the meter is.  Rather than go to France and compare my stick to the one under a glass case I can just measure the speed of light. Oh. That sounds hard!<br/>
<br/>
It matters a bit since the weight of what was the standard kilogram did increase over time, though of course not by much. When did the measurements for stuff STOP being based on physical objects and was all done based on constants of the universe?<br/>
<br/>
The answer surprised me:<br/>
<br/>
On Nov 16, 2018 (yes, you read that light) they decided that by May 20, 2019, the Kilogram will be defined in terms of Plank's constant. I have not been able to find out how they will use Plank, maybe they don't know yet (they do and its known -- see the first comment) .With that, there are no more standards based on physical objects. Read about it <a href="https://www.wired.com/story/new-kilogram-definition-based-on-planck-constant/">here</a>.<br/>
<br/>
Why did it take so long? I honestly don't know and I am tossing that question out to my readers. You can leave serious or funny answers, and best if I can't tell which is which!<br/>
<br/>
<br/>
<br/>
<br/></div>
    </content>
    <updated>2019-01-06T21:35:00Z</updated>
    <published>2019-01-06T21:35:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>https://plus.google.com/101693130490639305932</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-01-09T21:43:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15562</id>
    <link href="https://rjlipton.wordpress.com/2019/01/06/predictions-for-2019/" rel="alternate" type="text/html"/>
    <title>Predictions For 2019</title>
    <summary>The problem of predicting ‘when’ not just ‘what’ Cropped from Toronto Star source Isaac Asimov was a prolific writer of science fiction and nonfiction. Thirty-five years ago, on the eve of the year 1984, he noted that 35 years had passed since the publication of George Orwell’s 1984. He wrote an exclusive feature for the […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>The problem of predicting ‘when’ not just ‘what’</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2019/01/asimovtorontostar.jpg"><img alt="" class="alignright wp-image-15564" height="167" src="https://rjlipton.files.wordpress.com/2019/01/asimovtorontostar.jpg?w=180&amp;h=167" width="180"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from Toronto Star <a href="https://www.thestar.com/news/world/2018/12/27/35-years-ago-isaac-asimov-was-asked-by-the-star-to-predict-the-world-of-2019-here-is-what-he-wrote.html">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Isaac Asimov was a prolific writer of science fiction and nonfiction. Thirty-five years ago, on the eve of the year 1984, he noted that 35 years had passed since the publication of George Orwell’s <em>1984</em>. He wrote an exclusive <a href="https://www.thestar.com/news/world/2018/12/27/35-years-ago-isaac-asimov-was-asked-by-the-star-to-predict-the-world-of-2019-here-is-what-he-wrote.html">feature</a> for the Toronto Star newspaper predicting what the world would be like 35 years hence, that is, in 2019.</p>
<p>
Today we give our take on his predictions and make our own for the rest of 2019.</p>
<p>
Asimov’s essay began by presupposing the absence of nuclear holocaust without predicting it. It then focused on two subjects: computerization and use of outer space. On the spectrum of evaluations subtended by this laudatory BBC <a href="https://www.bbc.com/news/technology-46736024">piece</a> and this critical <a href="https://www.thestar.com/news/world/2018/12/27/isaac-asimov-you-were-no-nostradamus.html">column</a> in the Toronto Star itself, we’re closer to the latter. On space he predicted we’d be mining the Moon by now; instead nothing more landed on the Moon until the Chinese <a href="https://en.wikipedia.org/wiki/Chang'e_3">Chang’e 3</a> mission in 2013 and <a href="https://en.wikipedia.org/wiki/Chang'e_4">Chang’e 4</a> happening now. His 35-year span should be lengthened to over a century.</p>
<p>
On computerization and robotics he was mostly right except again for the timespan: he said the transition would be “about over” by 2019 whereas it may be entering its period of greatest flux only now. However, for the end of 1983 we think the “whats” of his predictions were easy. Personal computers had already been around for almost a decade. Computer systems for business were plentiful. The Internet was already a proclaimed goal and the text-based <a href="https://en.wikipedia.org/wiki/Usenet">Usenet</a> was already operating. Asimov’s essay seems to miss how the combination of these three would soon move points of control outward to end-users. </p>
<p>
We still think what he wrote about space and robots will happen. This shows the problem of predictions is not just ‘what’ but ‘when.’ For another instance of being wrong on ‘when’ too soon, Ken told a Harvard Law graduate who visited him in Oxford in 1984 that what we now call <a href="https://en.wikipedia.org/wiki/Deepfake">deepfake</a> videos were imminent. We’ll make the rest of this post more about ‘when’ than ‘what.’</p>
<p>
</p><p/><h2> Predictions in Past Years </h2><p/>
<p/><p>
Here are some predictions that we have made before. Seems we did not make any new predictions last year—oh well—but see <a href="https://rjlipton.wordpress.com/2018/01/02/predictions-we-didnt-make/">this</a>.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <em>No circuit lower bound of <img alt="{1000n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1000n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1000n}"/> or better will be proved for SAT.</em> Well that’s a freebie.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <em>A computer scientist will win a Nobel Prize.</em> No—indeed, less close than other years.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <em>At least five claims that <img alt="{\mathsf{P}=\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%3D%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{P}=\mathsf{NP}}"/> and five that <img alt="{\mathsf{P} \neq \mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D+%5Cneq+%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{P} \neq \mathsf{NP}}"/> will be made.</em> </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> A “provably” secure crypto-system will be broken. For this one we don’t have to check any claims. We just pocket the ‘yes’ answer. Really, could you ever prove the opposite? How about the <a href="https://cacm.acm.org/magazines/2019/1/233523-imperfect-forward-secrecy/abstract">attack</a> on Diffie-Hellman in the current CACM?</p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> <em>An Earth-sized planet will be detected orbiting within the habitable zone of its single star.</em> The “when” for this one came in 2017 already. We are retiring it.</p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> <em>A Clay problem will be solved, or at least notable progress made.</em> Again we sense that the answer on progress is “no.” This includes saying that nothing substantial seems to have emerged from Sir Michael Atiyah’s <a href="https://aperiodical.com/2018/09/atiyah-riemann-hypothesis-proof-final-thoughts/">claim</a> of proving the Riemann Hypothesis. However, we note <a href="https://gilkalai.wordpress.com/2018/12/25/amazing-karim-adiprasito-proved-the-g-conjecture-for-spheres/">via</a> Gil Kalai’s blog that a longstanding problem called the <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/>-conjecture for spheres has been <a href="https://arxiv.org/abs/1812.10454">solved</a> by Karim Adiprasito.</p>
<p>
</p><p/><h2> Predictions This Year </h2><p/>
<p/><p>
We will add some new predictions—it seems unfair to keep repeating sure winners. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <em>Deep learning methods will be found able to solve integer factoring.</em> This will place current cryptography is trouble.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <em>Deep learning methods will be found to help prove that factoring is hard.</em></p>
<p>
These may not be as contradictory as they seem. There is a long-known <a href="http://www.cs.sfu.ca/~kabanets/papers/natural-learning-short.pdf">connection</a> between certain learning algorithms and the <a href="https://en.wikipedia.org/wiki/Natural_proof">natural</a> <a href="https://rjlipton.wordpress.com/2009/03/25/whos-afraid-of-natural-proofs/">proofs</a> of Alexander Razborov and Stephen Rudich. The hardness predicate at the core of a natural proof is a classifier to distinguish (succinct) hard Boolean functions from easy ones. There is a duality between upper and lower bounds that in particular leads to the unconditional result that the discrete log problem, which is related to factoring and equally amenable to Peter Shor’s famous polynomial-time quantum algorithm, does not have natural proofs of hardness—because their existence would make discrete log relatively easy. </p>
<p>
Talking about quantum, we predict:</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <em>Quantum supremacy will be proved—finally.</em> But be careful: there is a problem with this whole direction. See the next section.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <em>An algorithm originating in a theoretical model will be enshrined in law.</em> </p>
<p>
There are several near-term opportunities for this. The Supreme Court yesterday agreed to <a href="https://www.cnn.com/2019/01/04/politics/supreme-court-gerrymandering-cases/index.html">hear</a> two cases on partisan gerrymandering, at least one of which promises to codify an algorithmic criterion for excessive vote dilution. Maine adopted a automatic-runoff voting system whose dependence on computer implementation gave grounds for an unsuccessful <a href="https://www.americanthinker.com/blog/2018/11/maine_gop_rep_sues_to_stop_counting_ranked_choice_ballots.html">lawsuit</a>. Algorithmic fairness is a burgeoning area which we <a href="https://rjlipton.wordpress.com/2017/11/20/a-magic-madison-visit/">discussed</a> a year-plus ago. <a href="https://www.sciencemag.org/news/2019/01/can-set-equations-keep-us-census-data-private">Use</a> of differential privacy by the U.S. Census could involve legislation. We distinguish legal provisions from the myriad problematic uses of algorithmic models in public and private <em>policy</em> ranging from credit evaluations to parole decisions to college admissions and much else.</p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> <em>The lines between heuristically solvable and really hard problems will become clearer.</em> We have <a href="https://rjlipton.wordpress.com/2016/07/10/the-world-turned-upside-down/">previously</a> <a href="https://rjlipton.wordpress.com/2014/02/28/practically-pnp/">opined</a> that the great success of SAT solvers in particular renders the <img alt="{\mathsf{P=NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%3DNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{P=NP}}"/> question moot for many purposes. Well, now we say the opposite: SAT solvers will hit a wall.</p>
<p>
</p><p/><h2> Quantum Supremacy and Advantage </h2><p/>
<p/><p>
Ken recently attended a workshop in central New York that aimed to bring together researchers in many fields working on quantum devices. Materials for the workshop led off with the question of building quantum computers and highlighted Gil Kalai’s skeptical position in particular. An <a href="https://rjlipton.wordpress.com/2012/01/30/perpetual-motion-of-the-21st-century/">eight</a>–<a href="https://rjlipton.wordpress.com/2012/02/15/nature-does-not-conspire/">part</a> <a href="https://rjlipton.wordpress.com/2012/06/20/can-you-hear-the-shape-of-a-quantum-computer/">debate</a> between him and Aram Harrow which we hosted in 2012 <a href="https://rjlipton.wordpress.com/2012/03/05/the-quantum-super-pac/">involved</a> also John Preskill and <a href="https://rjlipton.wordpress.com/2012/10/03/quantum-supremacy-or-classical-control/">ended</a> with a discussion of quantum <a href="https://en.wikipedia.org/wiki/Quantum_supremacy">supremacy</a>, a term advanced that year by Preskill. The workshop preferred the term quantum <em>advantage</em>. We interpret these terms as having the following distinction:</p>
<ul>
<li>
(a) Quantum <em>supremacy</em> means that a quantum device can perform general-purpose computations that no classical program or device can emulate in comparably feasible time. <p/>
</li><li>
(b) Quantum <em>advantage</em> means that some particular practical task can be achieved by available quantum devices at lower costs than near-term available classical devices.
</li></ul>
<p>
As theoreticians we tend to think about (a) but many businesses and public-sector organizations would be ecstatic to have (b) in important applications. </p>
<p>
A new angle on (a) was shown by the new construction by Ran Raz and Avishay Tal of an oracle <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> such that <img alt="{\mathsf{BQP}^A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BBQP%7D%5EA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{BQP}^A}"/> is not in <img alt="{\mathsf{PH}^A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BPH%7D%5EA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{PH}^A}"/>. This was <a href="https://blog.computationalcomplexity.org/2018/12/complexity-year-in-review-2018.html">hailed</a> as the “result of the year” by Lance Fortnow (his second and our first is this <a href="https://eccc.weizmann.ac.il/report/2018/006/">progress</a> on the Unique Games Conjecture), and Scott Aaronson furnished a great <a href="https://www.scottaaronson.com/blog/?p=3827">discussion</a> of its genesis and further ramifications in complexity theory. <a href="https://www.quantamagazine.org/finally-a-problem-that-only-quantum-computers-will-ever-be-able-to-solve-20180621/">Several</a> <a href="https://cacm.acm.org/magazines/2019/1/233514-quantum-leap/fulltext">popular</a> <a href="https://www.thehindu.com/sci-tech/science/quantum-computers-have-an-edge-over-classical-ones-says-the-oracle/article24420375.ece">articles</a> tried to pump this as non-oracle evidence for (a). But there is the over-arching problem:</p>
<blockquote><p><b> </b> <em> We know <img alt="{\mathsf{P \subseteq BPP \subseteq BQP \subseteq PP \subseteq P^{\#P} \subseteq PSPACE}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%5Csubseteq+BPP+%5Csubseteq+BQP+%5Csubseteq+PP+%5Csubseteq+P%5E%7B%5C%23P%7D+%5Csubseteq+PSPACE%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathsf{P \subseteq BPP \subseteq BQP \subseteq PP \subseteq P^{\#P} \subseteq PSPACE}}"/> but we don’t know <img alt="{\mathsf{P \neq PSPACE}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP+%5Cneq+PSPACE%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\mathsf{P \neq PSPACE}}"/>. </em>
</p></blockquote>
<p/><p>
So how are we ever going to be able to <em>prove</em> any form of supremacy? Even if we replace ‘polynomial time’ as our definition of ‘feasible’ by something more concrete, how can we prove that successful classical heuristics <em>do not exist</em>? On a certain practical problem of general import, Ewin Tang, a teenager in Texas advised by Scott, <a href="https://arxiv.org/abs/1807.04271">designed</a> an improved classical algorithm for low-rank matrix completion that <a href="https://www.quantamagazine.org/teenager-finds-classical-alternative-to-quantum-recommendation-algorithm-20180731/">eliminated</a> a previous quantum exponential advantage in the time dependence on the rank parameter. It is not just a case of <em>whether</em> we can prove supremacy, but judging <em>when</em> general quantum computers will be built to realize it.</p>
<p>
Whereas, the <em>when</em> involved in (b) is <em>now</em>. If a quantum device can do something useful now that classical methods are not delivering now, then it does not matter if the latter could be improved at greater hardware and development cost to work a year from now. This has been the gung-ho tenor of many responses to the recently-<a href="https://www.fedscoop.com/trump-signs-national-quantum-initiative-law/">signed</a> National Quantum Initiative Act. We do, however, still need to find and build said devices…</p>
<p>
As for the status of (a), we don’t know any better thought for January than the Janus-like title of this <a href="https://arxiv.org/abs/1807.10749">paper</a> by Igor Markov, Aneeqa Fatima, Sergei Isakov, and Sergio Boixo: </p>
<blockquote><p><b> </b> <em> “Quantum Supremacy Is Both Closer and Farther than It Appears.” </em>
</p></blockquote>
<p>
</p><p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
What are your predictions for 2019? What are the most important matters we’ve left unsaid?</p>
<p>
[added some words to end of intro]</p></font></font></div>
    </content>
    <updated>2019-01-06T19:03:39Z</updated>
    <published>2019-01-06T19:03:39Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Oldies"/>
    <category term="2018"/>
    <category term="2019"/>
    <category term="Isaac Asimov"/>
    <category term="New Year's"/>
    <category term="predictions"/>
    <category term="quantum advantage"/>
    <category term="quantum supremacy"/>
    <category term="year-in-review"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-01-09T22:20:51Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://cstheory.stackexchange.com/q/42163</id>
    <link href="https://cstheory.stackexchange.com/questions/42163/immutable-space-model" rel="alternate" type="text/html"/>
    <title>Immutable Space Model</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I have heard it said that time is more precious than space because we can reuse space but not time.  What if we treat space with this much reverence?</p>

<h3>What is generally known about models of computation in which space is immutable?</h3>

<p>I would expect such models to initialize each memory cell to some "blank" state and then only allow the writing of some "non-blank" value to each cell at most once.</p>

<p>The study of <a href="https://en.wikipedia.org/wiki/Persistent_data_structure" rel="noreferrer">persistent data structures</a> seems to me like a possible way to answer this question.</p>

<p>I thought of this question while studying functional programming, which highly values immutability.</p></div>
    </summary>
    <updated>2019-01-06T15:37:04Z</updated>
    <published>2019-01-06T15:37:04Z</published>
    <category scheme="https://cstheory.stackexchange.com/tags" term="reference-request"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="ds.data-structures"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="functional-programming"/>
    <category scheme="https://cstheory.stackexchange.com/tags" term="space-complexity"/>
    <author>
      <name>Tyson Williams</name>
      <uri>https://cstheory.stackexchange.com/users/3964</uri>
    </author>
    <source>
      <id>https://cstheory.stackexchange.com/feeds/</id>
      <link href="https://cstheory.stackexchange.com/feeds/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory.stackexchange.com/questions" rel="alternate" type="text/html"/>
      <link href="http://www.creativecommons.org/licenses/by-sa/3.0/rdf" rel="license"/>
      <subtitle>most recent 30 from cstheory.stackexchange.com</subtitle>
      <title>Recent Questions - Theoretical Computer Science Stack Exchange</title>
      <updated>2019-01-09T22:21:10Z</updated>
    </source>
  </entry>
</feed>
