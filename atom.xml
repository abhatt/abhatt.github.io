<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-05-09T06:22:53Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=430</id>
    <link href="https://tcsplus.wordpress.com/2020/05/08/430/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, May 13 — Sahil Singla, Princeton University and IAS</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, May 13th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Sahil Singla from Princeton University and IAS will speak about “Online Vector Balancing and Geometric Discrepancy” (abstract below). You can reserve a spot as an individual or a […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, May 13th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Sahil Singla</strong> from Princeton University and IAS will speak about “<em>Online Vector Balancing and Geometric Discrepancy</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our website</a> on the day of the talk, so people who did not sign up will still be able to watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: We consider an online vector balancing question where <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=fff&amp;fg=444444&amp;s=0" title="T"/> vectors, chosen from an arbitrary distribution over <img alt="[-1,1]^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5B-1%2C1%5D%5En&amp;bg=fff&amp;fg=444444&amp;s=0" title="[-1,1]^n"/>, arrive one-by-one and must be immediately given a <img alt="\{+1,-1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%2B1%2C-1%5C%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="\{+1,-1\}"/> sign. The goal is to keep the discrepancy—the <img alt="\ell_{\infty}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cell_%7B%5Cinfty%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="\ell_{\infty}"/>-norm of any signed prefix-sum—as small as possible. A concrete example of this question is the online interval discrepancy problem where <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=fff&amp;fg=444444&amp;s=0" title="T"/> points are sampled one-by-one uniformly in the unit interval <img alt="[0,1]" class="latex" src="https://s0.wp.com/latex.php?latex=%5B0%2C1%5D&amp;bg=fff&amp;fg=444444&amp;s=0" title="[0,1]"/>, and the goal is to immediately color them <img alt="\{+1,-1\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B%2B1%2C-1%5C%7D&amp;bg=fff&amp;fg=444444&amp;s=0" title="\{+1,-1\}"/> such that every sub-interval remains always nearly balanced. As random coloring incurs <img alt="\Omega(T^{1/2})" class="latex" src="https://s0.wp.com/latex.php?latex=%5COmega%28T%5E%7B1%2F2%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\Omega(T^{1/2})"/> discrepancy, while the worst-case offline bounds are <img alt="\Theta(\sqrt{n \log (T/n)})" class="latex" src="https://s0.wp.com/latex.php?latex=%5CTheta%28%5Csqrt%7Bn+%5Clog+%28T%2Fn%29%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\Theta(\sqrt{n \log (T/n)})"/> for vector balancing and 1 for interval balancing, a natural question is whether one can (nearly) match the offline bounds in the online setting for these problems. One must utilize the stochasticity as in the worst-case scenario it is known that discrepancy is <img alt="\Omega(T^{1/2})" class="latex" src="https://s0.wp.com/latex.php?latex=%5COmega%28T%5E%7B1%2F2%7D%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\Omega(T^{1/2})"/> for any online algorithm.</p>
<p>In this work, we introduce a new framework that allows us to handle online vector balancing even when the input distribution has dependencies across coordinates. In particular, this lets us obtain a <img alt="\textrm{poly}(n, \log T)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7Bpoly%7D%28n%2C+%5Clog+T%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\textrm{poly}(n, \log T)"/> bound for online vector balancing under arbitrary input distributions, and a <img alt="\textrm{poly}\log (T)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7Bpoly%7D%5Clog+%28T%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\textrm{poly}\log (T)"/> bound for online interval discrepancy. Our framework is powerful enough to capture other well-studied geometric discrepancy problems; e.g., we obtain a <img alt="\textrm{poly}(\log^d (T))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctextrm%7Bpoly%7D%28%5Clog%5Ed+%28T%29%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\textrm{poly}(\log^d (T))"/> bound for the online <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=fff&amp;fg=444444&amp;s=0" title="d"/>-dimensional Tusnády’s problem. All our bounds are tight up to polynomial factors.</p>
<p>A key new technical ingredient in our work is an anti-concentration inequality for sums of pairwise uncorrelated random variables, which might also be of independent interest.</p>
<p>Based on joint works with Nikhil Bansal, Haotian Jiang, Janardhan Kulkarni, and Makrand Sinha. Part of this work appears in STOC 2020 and is available at <a href="https://arxiv.org/abs/1912.03350" rel="nofollow">https://arxiv.org/abs/1912.03350</a></p></blockquote>
<p> </p></div>
    </content>
    <updated>2020-05-09T01:20:02Z</updated>
    <published>2020-05-09T01:20:02Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-05-09T06:21:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4786</id>
    <link href="https://www.scottaaronson.com/blog/?p=4786" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4786#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4786" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Announcements</title>
    <summary xml:lang="en-US">(1) I’ll be giving an online talk at SlateStarCodex (actually, in a VR room where you can walk around with your avatar, mingle, and even try to get “front-row seating”), this coming Sunday at 10:30am US Pacific time = 12:30pm US Central time (i.e., me) = 1:30pm US Eastern time = … Here’s the abstract: […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>(1) I’ll be <a href="https://cephalopods.blog/2020/04/12/april-26th-ssc-lesswrong-meetup/">giving an online talk at SlateStarCodex</a> (actually, in a VR room where you can walk around with your avatar, mingle, and even try to get “front-row seating”), this coming Sunday at 10:30am US Pacific time = 12:30pm US Central time (i.e., me) = 1:30pm US Eastern time = …  Here’s the abstract:</p>



<p><strong>Schrödinger’s Cat and Quantum Necromancy</strong></p>



<p><em>I’ll try, as best I can, to give a 10-minute overview of the century-old measurement problem of quantum mechanics.  I’ll then</em> <em>discuss a new result, by me and Yosi Atia, that might add a new</em> <em>wrinkle to the problem.  Very roughly, our result says that if you had</em> <em>the technological ability, as measured by (say) quantum circuit</em> <em>complexity, to prove that a cat was in a coherent superposition of the</em> <em>alive and dead states, then you’d necessarily also have the</em> <em>technological ability to bring a dead cat back to life.  Of course, this raises the question of in what sense such a cat was ever “dead” in the first place.</em></p>



<p>(2) Robin Kothari has a <a href="https://www.microsoft.com/en-us/research/blog/quantum-speedups-for-unstructured-problems-solving-two-twenty-year-old-problems/">beautiful blog post</a> about a <a href="https://arxiv.org/abs/2004.13231">new paper</a> by me, him, Avishay Tal, and Shalev Ben-David, which uses Huang’s recent breakthrough proof of the Sensitivity Conjecture to show that D(f)=O(Q(f)<sup>4</sup>) for all total Boolean functions f, where D(f) is the deterministic query complexity of f and Q(f) is the quantum query complexity—thereby resolving another longstanding open problem (the best known relationship since 1998 had been D(f)=O(Q(f)<sup>6</sup>)).  Check out his post!</p>



<p>(3) For all the people who’ve been emailing me, and leaving blog comments, about Stephen Wolfram’s new model of fundamental physics (his <em>new</em> new kind of science?)—Adam Becker now has an excellent article for <em>Scientific American</em>, entitled <a href="https://www.scientificamerican.com/article/physicists-criticize-stephen-wolframs-theory-of-everything/">Physicists Criticize Stephen Wolfram’s “Theory of Everything.”</a>  The article quotes me, friend-of-the-blog Daniel Harlow, and several others.  The only thing about Becker’s piece that I disagreed with was the amount of space he spent on <em>process</em> (e.g. Wolfram’s flouting of traditional peer review).  Not only do I care less and less about such things, but I worry that harping on them feeds directly into Wolfram’s misunderstood-genius narrative.  Why not use the space to explain how Wolfram makes a hash of quantum mechanics—e.g., never really articulating how he proposes to get unitarity, or the Born rule, or even a Hilbert space?  Anyway, given the demand, I guess I’ll do a separate blog post about this when I have time.  (Keep in mind that, with my kids home from school, I have approximately 2 working hours per day.)</p>



<p>(4) Oh yeah, I forgot!  Joshua Zelinsky pointed me to a <a href="https://turingmachinesimulator.com/shared/vgimygpuwi?fbclid=IwAR2inmxK7p5nuf2rQfDP58kUH7-yC0XvVabybIY2CN1LjQizScK4KGRgdu0">website by Martin Ugarte</a>, which plausibly claims to construct a Turing machine with only 748 states whose behavior is independent of ZF set theory—beating the previous claimed record of 985 states due to Stefan O’Rear (see <a href="https://github.com/sorear/metamath-turing-machines/blob/ab448a2d/sample_out/zf2.tm">O’Rear’s GitHub page</a>), which in turn beat the 8000 states of me and Adam Yedidia (see my <a href="https://www.scottaaronson.com/blog/?p=2725">2016 blog post about this</a>).  I should caution that, to my knowledge, the new construction hasn’t been peer-reviewed, let alone proved correct in a machine-checkable way (well, the latter hasn’t yet been done for <em>any</em> of these constructions).  For that matter, while an absolutely beautiful interface is provided, I couldn’t even find documentation for the new construction.  Still, Turing machine and Busy Beaver aficionados will want to check it out!</p></div>
    </content>
    <updated>2020-05-08T23:05:23Z</updated>
    <published>2020-05-08T23:05:23Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Bell's Theorem? But a Flesh Wound!"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-05-09T05:57:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/05/08/14-phd-positions-in-stochastics-and-algorithmics-cofund-at-networks-apply-by-may-31-2020/</id>
    <link href="https://cstheory-jobs.org/2020/05/08/14-phd-positions-in-stochastics-and-algorithmics-cofund-at-networks-apply-by-may-31-2020/" rel="alternate" type="text/html"/>
    <title>14 PhD Positions in Stochastics and Algorithmics (COFUND) at NETWORKS (apply by May 31, 2020)</title>
    <summary>The research project NETWORKS is looking for 14 international PhD students in mathematics and computer science. Are you interested in the stochastics and algorithmics behind network problems? And would you like to be part of this project with its many activities? Then we invite you to apply for one of these positions. Website: https://www.thenetworkcenter.nl/Open-Positions/openposition/29/14-PhD-Positions-in-Stochastics-and-Algorithmics-COFUND- Email: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The research project NETWORKS is looking for 14 international PhD students in mathematics and computer science. Are you interested in the stochastics and algorithmics behind network problems? And would you like to be part of this project with its many activities? Then we invite you to apply for one of these positions.</p>
<p>Website: <a href="https://www.thenetworkcenter.nl/Open-Positions/openposition/29/14-PhD-Positions-in-Stochastics-and-Algorithmics-COFUND-">https://www.thenetworkcenter.nl/Open-Positions/openposition/29/14-PhD-Positions-in-Stochastics-and-Algorithmics-COFUND-</a><br/>
Email: info@thenetworkcenter.nl</p></div>
    </content>
    <updated>2020-05-08T13:12:32Z</updated>
    <published>2020-05-08T13:12:32Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-05-09T06:21:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.03584</id>
    <link href="http://arxiv.org/abs/2005.03584" rel="alternate" type="text/html"/>
    <title>Simulating Population Protocols in Sub-Constant Time per Interaction</title>
    <feedworld_mtime>1588896000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Berenbrink:Petra.html">Petra Berenbrink</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hammer:David.html">David Hammer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kaaser:Dominik.html">Dominik Kaaser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meyer:Ulrich.html">Ulrich Meyer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Penschuck:Manuel.html">Manuel Penschuck</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tran:Hung.html">Hung Tran</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.03584">PDF</a><br/><b>Abstract: </b>We consider the problem of efficiently simulating population protocols. In
the population model, we are given a distributed system of $n$ agents modeled
as identical finite-state machines. In each time step, a pair of agents is
selected uniformly at random to interact. In an interaction, agents update
their states according to a common transition function. We empirically and
analytically analyze two classes of simulators for this model.
</p>
<p>First, we consider sequential simulators executing one interaction after the
other. Key to the performance of these simulators is the data structure storing
the agents' states. For our analysis, we consider plain arrays, binary search
trees, and a novel Dynamic Alias Table data structure.
</p>
<p>Secondly, we consider batch processing to efficiently update the states of
multiple independent agents in one step. For many protocols considered in
literature, our simulator requires amortized sub-constant time per interaction
and is fast in practice: given a fixed time budget, the implementation of our
batched simulator is able to simulate population protocols several orders of
magnitude larger compared to the sequential competitors, and can carry out
$2^{50}$ interactions among the same number of agents in less than 400s.
</p></div>
    </summary>
    <updated>2020-05-08T22:37:13Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.03552</id>
    <link href="http://arxiv.org/abs/2005.03552" rel="alternate" type="text/html"/>
    <title>Online Algorithms to Schedule a Proportionate Flexible Flow Shop of Batching Machines</title>
    <feedworld_mtime>1588896000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hertrich:Christoph.html">Christoph Hertrich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wei=szlig=:Christian.html">Christian Weiß</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Ackermann:Heiner.html">Heiner Ackermann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Heydrich:Sandy.html">Sandy Heydrich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krumke:Sven_O=.html">Sven O. Krumke</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.03552">PDF</a><br/><b>Abstract: </b>This paper is the first to consider online algorithms to schedule a
proportionate flexible flow shop of batching machines (PFFB). The scheduling
model is motivated by manufacturing processes of individualized medicaments,
which are used in modern medicine to treat some serious illnesses. We provide
two different online algorithms, proving also lower bounds for the offline
problem to compute their competitive ratios. The first algorithm is an
easy-to-implement, general local scheduling heuristic. It is 2-competitive for
PFFBs with an arbitrary number of stages and for several natural scheduling
objectives. We also show that for total/average flow time, no deterministic
algorithm with better competitive ratio exists. For the special case with two
stages and the makespan or total completion time objective, we describe an
improved algorithm that achieves the best possible competitive ratio
$\varphi=\frac{1+\sqrt{5}}{2}$, the golden ratio. All our results also hold for
proportionate (non-flexible) flow shops of batching machines (PFB) for which
this is also the first paper to study online algorithms.
</p></div>
    </summary>
    <updated>2020-05-08T22:42:46Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.03394</id>
    <link href="http://arxiv.org/abs/2005.03394" rel="alternate" type="text/html"/>
    <title>Scheduling with a processing time oracle</title>
    <feedworld_mtime>1588896000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dufoss=eacute=:Fanny.html">Fanny Dufossé</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/D=uuml=rr:Christoph.html">Christoph Dürr</a>, Noël Nadal, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Trystram:Denis.html">Denis Trystram</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/V=aacute=squez:=Oacute=scar_C=.html">Óscar C. Vásquez</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.03394">PDF</a><br/><b>Abstract: </b>In this paper we study a single machine scheduling problem on a set of
independent jobs whose execution time is not known, but guaranteed to be either
short or long, for two given processing times. At every time step, the
scheduler has the possibility either to test a job, by querying a processing
time oracle, which reveals its processing time, and occupies one time unit on
the schedule. Or the scheduler can execute a job, might it be previously tested
or not. The objective value is the total completion time over all jobs, and is
compared with the objective value of an optimal schedule, which does not need
to test. The resulting competitive ratio measures the price of hidden
processing time.
</p>
<p>Two models are studied in this paper. In the non-adaptive model, the
algorithm needs to decide before hand which jobs to test, and which jobs to
execute untested. However in the adaptive model, the algorithm can make these
decisions adaptively to the outcomes of the job tests. In both models we
provide optimal polynomial time two-phase algorithms, which consist of a first
phase where jobs are tested, and a second phase where jobs are executed
untested. Experiments give strong evidence that optimal algorithms have this
structure. Proving this property is left as an open problem.
</p></div>
    </summary>
    <updated>2020-05-08T22:38:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.03256</id>
    <link href="http://arxiv.org/abs/2005.03256" rel="alternate" type="text/html"/>
    <title>Critique of Boyu Sima's Proof that ${\rm P}\neq{\rm NP}$</title>
    <feedworld_mtime>1588896000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Brendon Pon <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.03256">PDF</a><br/><b>Abstract: </b>We review and critique Boyu Sima's paper, "A solution of the P versus NP
problem based on specific property of clique function," (<a href="http://export.arxiv.org/abs/1911.00722">arXiv:1911.00722</a>)
which claims to prove that ${\rm P}\neq{\rm NP}$ by way of removing the gap
between the nonmonotone circuit complexity and the monotone circuit complexity
of the clique function. We first describe Sima's argument, and then we describe
where and why it fails. Finally, we present a simple example that clearly
demonstrates the failure.
</p></div>
    </summary>
    <updated>2020-05-08T22:37:09Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.03246</id>
    <link href="http://arxiv.org/abs/2005.03246" rel="alternate" type="text/html"/>
    <title>Fast multivariate empirical cumulative distribution function with connection to kernel density estimation</title>
    <feedworld_mtime>1588896000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Langren=eacute=:Nicolas.html">Nicolas Langrené</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Warin:Xavier.html">Xavier Warin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.03246">PDF</a><br/><b>Abstract: </b>This paper revisits the problem of computing empirical cumulative
distribution functions (ECDF) efficiently on large, multivariate datasets.
Computing an ECDF at one evaluation point requires $\mathcal{O}(N)$ operations
on a dataset composed of $N$ data points. Therefore, a direct evaluation of
ECDFs at $N$ evaluation points requires a quadratic $\mathcal{O}(N^2)$
operations, which is prohibitive for large-scale problems. Two fast and exact
methods are proposed and compared. The first one is based on fast summation in
lexicographical order, with a $\mathcal{O}(N{\log}N)$ complexity and requires
the evaluation points to lie on a regular grid. The second one is based on the
divide-and-conquer principle, with a $\mathcal{O}(N\log(N)^{(d-1){\vee}1})$
complexity and requires the evaluation points to coincide with the input
points. The two fast algorithms are described and detailed in the general
$d$-dimensional case, and numerical experiments validate their speed and
accuracy. Secondly, the paper establishes a direct connection between
cumulative distribution functions and kernel density estimation (KDE) for a
large class of kernels. This connection paves the way for fast exact algorithms
for multivariate kernel density estimation and kernel regression. Numerical
tests with the Laplacian kernel validate the speed and accuracy of the proposed
algorithms. A broad range of large-scale multivariate density estimation,
cumulative distribution estimation, survival function estimation and regression
problems can benefit from the proposed numerical methods.
</p></div>
    </summary>
    <updated>2020-05-08T22:43:38Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.03192</id>
    <link href="http://arxiv.org/abs/2005.03192" rel="alternate" type="text/html"/>
    <title>Trains, Games, and Complexity: 0/1/2-Player Motion Planning through Input/Output Gadgets</title>
    <feedworld_mtime>1588896000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Joshua Ani, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Demaine:Erik_D=.html">Erik D. Demaine</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hendrickson:Dylan_H=.html">Dylan H. Hendrickson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lynch:Jayson.html">Jayson Lynch</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.03192">PDF</a><br/><b>Abstract: </b>We analyze the computational complexity of motion planning through local
"input/output" gadgets with separate entrances and exits, and a subset of
allowed traversals from entrances to exits, each of which changes the state of
the gadget and thereby the allowed traversals. We study such gadgets in the 0-,
1-, and 2-player settings, in particular extending past
motion-planning-through-gadgets work to 0-player games for the first time, by
considering "branchless" connections between gadgets that route every gadget's
exit to a unique gadget's entrance. Our complexity results include containment
in L, NL, P, NP, and PSPACE; as well as hardness for NL, P, NP, and PSPACE. We
apply these results to show PSPACE-completeness for certain mechanics in
Factorio, [the Sequence], and a restricted version of Trainyard, improving
prior results. This work strengthens prior results on switching graphs and
reachability switching games.
</p></div>
    </summary>
    <updated>2020-05-08T22:22:51Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.03185</id>
    <link href="http://arxiv.org/abs/2005.03185" rel="alternate" type="text/html"/>
    <title>Determinantal Point Processes in Randomized Numerical Linear Algebra</title>
    <feedworld_mtime>1588896000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Michał Dereziński, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mahoney:Michael_W=.html">Michael W. Mahoney</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.03185">PDF</a><br/><b>Abstract: </b>Randomized Numerical Linear Algebra (RandNLA) uses randomness to develop
improved algorithms for matrix problems that arise in scientific computing,
data science, machine learning, etc. Determinantal Point Processes (DPPs), a
seemingly unrelated topic in pure and applied mathematics, is a class of
stochastic point processes with probability distribution characterized by
sub-determinants of a kernel matrix. Recent work has uncovered deep and
fruitful connections between DPPs and RandNLA which lead to new guarantees and
improved algorithms that are of interest to both areas. We provide an overview
of this exciting new line of research, including brief introductions to RandNLA
and DPPs, as well as applications of DPPs to classical linear algebra tasks
such as least squares regression, low-rank approximation and the Nystr\"om
method. For example, random sampling with a DPP leads to new kinds of unbiased
estimators for least squares, enabling more refined statistical and inferential
understanding of these algorithms; a DPP is, in some sense, an optimal
randomized algorithm for the Nystr\"om method; and a RandNLA technique called
leverage score sampling can be derived as the marginal distribution of a DPP.
We also discuss recent algorithmic developments, illustrating that, while not
quite as efficient as standard RandNLA techniques, DPP-based algorithms are
only moderately more expensive.
</p></div>
    </summary>
    <updated>2020-05-08T22:40:59Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2005.03176</id>
    <link href="http://arxiv.org/abs/2005.03176" rel="alternate" type="text/html"/>
    <title>A Parameterized Perspective on Attacking and Defending Elections</title>
    <feedworld_mtime>1588896000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Kishen N. Gowda, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Misra:Neeldhara.html">Neeldhara Misra</a>, Vraj Patel <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2005.03176">PDF</a><br/><b>Abstract: </b>We consider the problem of protecting and manipulating elections by
recounting and changing ballots, respectively. Our setting involves a
plurality-based election held across multiple districts, and the problem
formulations are based on the model proposed recently by~[Elkind et al, IJCAI
2019]. It turns out that both of the manipulation and protection problems are
NP-complete even in fairly simple settings. We study these problems from a
parameterized perspective with the goal of establishing a more detailed
complexity landscape. The parameters we consider include the number of voters,
and the budgets of the attacker and the defender. While we observe
fixed-parameter tractability when parameterizing by number of voters, our main
contribution is a demonstration of parameterized hardness when working with the
budgets of the attacker and the defender.
</p></div>
    </summary>
    <updated>2020-05-08T22:36:32Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-05-08T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=19821</id>
    <link href="https://gilkalai.wordpress.com/2020/05/08/to-cheer-you-up-in-difficult-times-3-a-guest-post-by-noam-lifshitz-on-the-new-hypercontractivity-inequality-of-peter-keevash-noam-lifshitz-eoin-long-and-dor-minzer/" rel="alternate" type="text/html"/>
    <title>To cheer you up in difficult times 3: A guest post by Noam Lifshitz on the new hypercontractivity inequality of Peter Keevash, Noam Lifshitz, Eoin Long and Dor Minzer</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This is a guest post kindly contributed by Noam Lifshitz. My short introduction: There is nothing like a new hypercontractivity inequality to cheer you up in difficult times and this post describes an amazing new hypercontractivity inequality.  The post describes … <a href="https://gilkalai.wordpress.com/2020/05/08/to-cheer-you-up-in-difficult-times-3-a-guest-post-by-noam-lifshitz-on-the-new-hypercontractivity-inequality-of-peter-keevash-noam-lifshitz-eoin-long-and-dor-minzer/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>This is a guest post kindly contributed by Noam Lifshitz</em>.</p>
<p>My short introduction: There is nothing like a new hypercontractivity inequality to cheer you up in difficult times and this post describes an amazing new hypercontractivity inequality.  The post describes a recent hypercontractive inequality by Peter Keevash, Noam Lifshitz, Eoin Long and Dor Minzer (KLLM) from their paper: <a href="https://arxiv.org/abs/1906.05568">Hypercontractivity for global functions and sharp thresholds</a>. (We reported on this development in <a href="https://gilkalai.wordpress.com/2018/10/30/exciting-beginning-of-the-year-activities-and-seminars/">this post</a>. By now, there are quite a few important applications.) And for Talagrand’s generic chaining inequality, see this <a href="https://lucatrevisan.wordpress.com/2020/05/03/talagrands-generic-chaining/">beautiful blog post</a> by Luca Trevisan.</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/04/barrysimon.jpg"><img alt="" class="alignnone size-full wp-image-19825" src="https://gilkalai.files.wordpress.com/2020/04/barrysimon.jpg?w=640"/></a></p>
<p><span style="color: #ff0000;">Barry Simon coined the term “hypercontractivity” in the 70s.  (We asked about it here and Nick Read was the first to answer.) A few months ago Barry told us about the early history of hypercontractivity inequalities, and, in particular, the very entertaining story on William Beckner’s Ph. D. qualifying exam.</span></p>
<p>And now to Noam Lifshitz’s guest post.</p>
<h2>Hypercontractivity on product spaces</h2>
<p>Analysis of Boolean functions (ABS) is a very rich subject. There are many works whose concern is generalising some of the results on analysis of Boolean functions to other (product) settings, such as functions on the multicube <img alt="{\left[m\right]^{n},}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%5Bm%5Cright%5D%5E%7Bn%7D%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\left[m\right]^{n},}"/> where <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> is very large. However, in some of these cases the fundemental tools of AOBF seem to be false for functions on the multicube <img alt="{f\colon\left[m\right]^{n}\rightarrow\mathbb{R}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Ccolon%5Cleft%5Bm%5Cright%5D%5E%7Bn%7D%5Crightarrow%5Cmathbb%7BR%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f\colon\left[m\right]^{n}\rightarrow\mathbb{R}.}"/> However, in the recent work of Keevash, Long, Minzer, and I. We introduce the notion of global functions. These are functions that do not strongly depend on a small set of coordinates. We then show that most of the rich classical theory of AOBF can in fact be generalised to these global functions. Using our machinery we were able to strengthen an isoperimetric stability result of Bourgain, and to make progress on some Erdos-Ko-Rado type open problem.</p>
<p>We now discuss some background on the Fourier analysis on functions on the multicube <img alt="{f\colon\left\{ 1,\ldots,m\right\} ^{n}\rightarrow\mathbb{R}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Ccolon%5Cleft%5C%7B+1%2C%5Cldots%2Cm%5Cright%5C%7D+%5E%7Bn%7D%5Crightarrow%5Cmathbb%7BR%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f\colon\left\{ 1,\ldots,m\right\} ^{n}\rightarrow\mathbb{R}.}"/></p>
<h3>Derivatives and Laplacians</h3>
<p>There are two fundemental types of operators on Boolean functions <img alt="{f\colon\left\{ 0,1\right\} ^{n}\rightarrow\mathbb{R}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Ccolon%5Cleft%5C%7B+0%2C1%5Cright%5C%7D+%5E%7Bn%7D%5Crightarrow%5Cmathbb%7BR%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f\colon\left\{ 0,1\right\} ^{n}\rightarrow\mathbb{R}.}"/> The first ones are the discrete derivatives, defined by <img alt="{D_{i}[f]=\frac{f_{i\rightarrow1}-f_{i\rightarrow0}}{2},}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_%7Bi%7D%5Bf%5D%3D%5Cfrac%7Bf_%7Bi%5Crightarrow1%7D-f_%7Bi%5Crightarrow0%7D%7D%7B2%7D%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_{i}[f]=\frac{f_{i\rightarrow1}-f_{i\rightarrow0}}{2},}"/> where <img alt="{f_{i\rightarrow x}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_%7Bi%5Crightarrow+x%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_{i\rightarrow x}}"/> denotes the we plug in the value <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> for the <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>th coordinate. The other closely related ones are the laplacians defined by <img alt="{L_{i}f\left(x\right):=f\left(x\right)-\mathbb{E}f\left(\mathbf{y}\right),}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7Bi%7Df%5Cleft%28x%5Cright%29%3A%3Df%5Cleft%28x%5Cright%29-%5Cmathbb%7BE%7Df%5Cleft%28%5Cmathbf%7By%7D%5Cright%29%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{i}f\left(x\right):=f\left(x\right)-\mathbb{E}f\left(\mathbf{y}\right),}"/> where <img alt="{\mathbf{y}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7By%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{y}}"/> is obtained from <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> by resampling its <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>th coordinate.</p>
<p>The laplacians and the derivatives are closely related. In fact, when we plug in <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> in the <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>th coordinate, we obtain <img alt="{L_{i}[f]_{i\rightarrow1}=D_{i}[f]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7Bi%7D%5Bf%5D_%7Bi%5Crightarrow1%7D%3DD_%7Bi%7D%5Bf%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{i}[f]_{i\rightarrow1}=D_{i}[f]}"/>, and when we plug in <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> in it, we obtain <img alt="{L_{i}[f]_{i\rightarrow0}=-D_{i}[f].}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7Bi%7D%5Bf%5D_%7Bi%5Crightarrow0%7D%3D-D_%7Bi%7D%5Bf%5D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{i}[f]_{i\rightarrow0}=-D_{i}[f].}"/></p>
<p>The 2-norm of the <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>th derivative is called the <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>th influence of <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> as it measures the impact of the <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>th coordinate on the value of <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/>. It’s usually denoted by <img alt="{\mathrm{Inf}_{i}[f]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BInf%7D_%7Bi%7D%5Bf%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathrm{Inf}_{i}[f]}"/>.</p>
<h3>Generalisation to functions on the multicube</h3>
<p>For functions on the multicube we don’t have a very good notion of a discrete derivative, but it turns out that it will be enough to talk about the laplacians and their restrictions. The Laplacians are again defined via <img alt="{L_{i}f\left(x\right):=f\left(x\right)-\mathbb{E}f\left(\mathbf{y}\right),}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7Bi%7Df%5Cleft%28x%5Cright%29%3A%3Df%5Cleft%28x%5Cright%29-%5Cmathbb%7BE%7Df%5Cleft%28%5Cmathbf%7By%7D%5Cright%29%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{i}f\left(x\right):=f\left(x\right)-\mathbb{E}f\left(\mathbf{y}\right),}"/> where <img alt="{\mathbf{y}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7By%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{y}}"/> is obtained from <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> by resampling its <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>th coordinate. It turns out that in the continuous cube it’s not enough to talk about Laplacians of coordinate, and we will also have to concern ourselves with Laplacians of sets. We define the generalised Laplacians of a set <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> by composing the laplacians corresponding to each coordinate in <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> <img alt="{L_{\left\{ i_{1},i_{2},\ldots,i_{r}\right\} }\left[f\right]:=L_{i_{1}}\circ\cdots\circ L_{i_{r}}\left[f\right].}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_%7B%5Cleft%5C%7B+i_%7B1%7D%2Ci_%7B2%7D%2C%5Cldots%2Ci_%7Br%7D%5Cright%5C%7D+%7D%5Cleft%5Bf%5Cright%5D%3A%3DL_%7Bi_%7B1%7D%7D%5Ccirc%5Ccdots%5Ccirc+L_%7Bi_%7Br%7D%7D%5Cleft%5Bf%5Cright%5D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_{\left\{ i_{1},i_{2},\ldots,i_{r}\right\} }\left[f\right]:=L_{i_{1}}\circ\cdots\circ L_{i_{r}}\left[f\right].}"/></p>
<p>We now need to convince ourselves that these laplacians have something to do with the impact of <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> on the outcome of <img alt="{f.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f.}"/> In fact, the following notions are equivalent</p>
<ol>
<li>For each <img alt="{x,y\in\left[m\right]^{S}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%5Cin%5Cleft%5Bm%5Cright%5D%5E%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x,y\in\left[m\right]^{S}}"/>we have <img alt="{\|f_{S\rightarrow x}-f_{S\rightarrow y}\|_{2}&lt;\delta_{1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7Cf_%7BS%5Crightarrow+x%7D-f_%7BS%5Crightarrow+y%7D%5C%7C_%7B2%7D%3C%5Cdelta_%7B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\|f_{S\rightarrow x}-f_{S\rightarrow y}\|_{2}&lt;\delta_{1}}"/></li>
<li>For each <img alt="{x\in\left[m\right]^{S}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin%5Cleft%5Bm%5Cright%5D%5E%7BS%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x\in\left[m\right]^{S}}"/> we have <img alt="{\|L_{S}[f]_{S\rightarrow x}\|_{2}&lt;\delta_{2},}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7CL_%7BS%7D%5Bf%5D_%7BS%5Crightarrow+x%7D%5C%7C_%7B2%7D%3C%5Cdelta_%7B2%7D%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\|L_{S}[f]_{S\rightarrow x}\|_{2}&lt;\delta_{2},}"/></li>
</ol>
<p>in the sense that if (1) holds then (2) holds with <img alt="{\delta_{2}=C^{\left|S\right|}\delta_{1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta_%7B2%7D%3DC%5E%7B%5Cleft%7CS%5Cright%7C%7D%5Cdelta_%7B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta_{2}=C^{\left|S\right|}\delta_{1}}"/> and conversely if (2) holds, then (1) holds with <img alt="{\delta_{1}=C^{\left|S\right|}\delta_{2}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta_%7B1%7D%3DC%5E%7B%5Cleft%7CS%5Cright%7C%7D%5Cdelta_%7B2%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta_{1}=C^{\left|S\right|}\delta_{2}.}"/></p>
<p>The main theme of our work is that one can understand <strong>global</strong> function on the continuous cube, and these are functions that satisfy the above equivalent notions for all small sets <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>.</p>
<h3>Noise operator, hypercontractivity, and small set expansion</h3>
<p>For <img alt="{\rho\in\left(0,1\right),}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crho%5Cin%5Cleft%280%2C1%5Cright%29%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\rho\in\left(0,1\right),}"/> the noise operator is given by <img alt="{\mathrm{T}_{\rho}\left[f\right]\left(x\right)=\mathbb{E}f\left(\mathbf{y}\right)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7BT%7D_%7B%5Crho%7D%5Cleft%5Bf%5Cright%5D%5Cleft%28x%5Cright%29%3D%5Cmathbb%7BE%7Df%5Cleft%28%5Cmathbf%7By%7D%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathrm{T}_{\rho}\left[f\right]\left(x\right)=\mathbb{E}f\left(\mathbf{y}\right)}"/> when <img alt="{\mathbf{y}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7By%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{y}}"/> is obtained from <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> by independently setting each coordinate <img alt="{\mathbf{y}_{i}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7By%7D_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{y}_{i}}"/> to be <img alt="{\mathbf{x}_{i}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7Bx%7D_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{x}_{i}}"/> with probability <img alt="{\rho}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crho%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\rho}"/> and resampling it with uniformly out of <img alt="{\left\{ -1,1\right\} }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%5C%7B+-1%2C1%5Cright%5C%7D+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\left\{ -1,1\right\} }"/> otherwise. The process which given <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> outputs <img alt="{\mathbf{y}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7By%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{y}}"/> is called the <img alt="{\rho}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crho%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\rho}"/>-noisy process, and we write <img alt="{\mathbf{y}\sim N_{\rho}\left(x\right).}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7By%7D%5Csim+N_%7B%5Crho%7D%5Cleft%28x%5Cright%29.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{y}\sim N_{\rho}\left(x\right).}"/></p>
<p>The Bonami hypercontractivity theorem, which was then generalised by Gross and Beckner states that the noise operator <img alt="{T_{\frac{1}{\sqrt{3}}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_%7B%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_{\frac{1}{\sqrt{3}}}}"/> is a contraction from <img alt="{L^{2}\left(\left\{ 0,1\right\} ^{n}\right)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%5E%7B2%7D%5Cleft%28%5Cleft%5C%7B+0%2C1%5Cright%5C%7D+%5E%7Bn%7D%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L^{2}\left(\left\{ 0,1\right\} ^{n}\right)}"/> to <img alt="{L^{4}\left(\left\{ 0,1\right\} ^{n}\right),}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%5E%7B4%7D%5Cleft%28%5Cleft%5C%7B+0%2C1%5Cright%5C%7D+%5E%7Bn%7D%5Cright%29%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L^{4}\left(\left\{ 0,1\right\} ^{n}\right),}"/> i.e.</p>
<blockquote><p><img alt="\displaystyle \|\mathrm{T}_{\frac{1}{\sqrt{3}}}f\|_{4}\le\|f\|_{2} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5C%7C%5Cmathrm%7BT%7D_%7B%5Cfrac%7B1%7D%7B%5Csqrt%7B3%7D%7D%7Df%5C%7C_%7B4%7D%5Cle%5C%7Cf%5C%7C_%7B2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \|\mathrm{T}_{\frac{1}{\sqrt{3}}}f\|_{4}\le\|f\|_{2} "/><br/>
for any function <img alt="{f.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f.}"/></p></blockquote>
<p>One consequence of the hypercontractivity theorem is the small set expansion theorem of KKL. It concerns fixed <img alt="{\rho\in\left(0,1\right)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crho%5Cin%5Cleft%280%2C1%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\rho\in\left(0,1\right)}"/> and a sequence of sets <img alt="{A_{n}\subseteq\{0,1\}^{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7Bn%7D%5Csubseteq%5C%7B0%2C1%5C%7D%5E%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_{n}\subseteq\{0,1\}^{n}}"/> with <img alt="{\left|A_{n}\right|=o\left(2^{n}\right).}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%7CA_%7Bn%7D%5Cright%7C%3Do%5Cleft%282%5E%7Bn%7D%5Cright%29.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\left|A_{n}\right|=o\left(2^{n}\right).}"/> The small set expansion theorem states that if we choose <img alt="{\mathbf{x}\sim A_{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7Bx%7D%5Csim+A_%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{x}\sim A_{n}}"/> uniformly and a noisy <img alt="{\mathbf{y}\sim N_{\rho}\left(\mathbf{x}\right),}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7By%7D%5Csim+N_%7B%5Crho%7D%5Cleft%28%5Cmathbf%7Bx%7D%5Cright%29%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{y}\sim N_{\rho}\left(\mathbf{x}\right),}"/> then <img alt="{\mathbf{y}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7By%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{y}}"/> will reside outside of <img alt="{A_{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_{n}}"/> almost surely.</p>
<h3>The Generalisation to the multicube:</h3>
<p>The small set expansion theorem and the hypercontractivity theorem both fail for function on the multicube that are of a very local nature. For instance, let <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> be the set of all <img alt="{x\in\left\{ 1,\ldots,m\right\} ^{n},}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin%5Cleft%5C%7B+1%2C%5Cldots%2Cm%5Cright%5C%7D+%5E%7Bn%7D%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x\in\left\{ 1,\ldots,m\right\} ^{n},}"/> such that <img alt="{x_{1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_%7B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_{1}}"/> is <img alt="{m.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m.}"/> Then <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> is of size <img alt="{m^{n-1},}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%5E%7Bn-1%7D%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m^{n-1},}"/> which is <img alt="{o\left(m^{n}\right)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bo%5Cleft%28m%5E%7Bn%7D%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{o\left(m^{n}\right)}"/> if we allow <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> to be a growing function of <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>. However, the <img alt="{\rho}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crho%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\rho}"/>-noisy process from the set stays within the set with probability <img alt="{\rho.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crho.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\rho.}"/> For a similar reason the hypercontractivity theorem fails as is for functions on <img alt="{\left\{ 1,\ldots,m\right\} ^{n}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%5C%7B+1%2C%5Cldots%2Cm%5Cright%5C%7D+%5E%7Bn%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\left\{ 1,\ldots,m\right\} ^{n}.}"/> However we were able to generalise the hypercontractivity theorem by taking the globalness of <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> into consideration.</p>
<p>Our main hypercontractive inequality is the following</p>
<p><strong>Theorem 1.</strong></p>
<blockquote><p><img alt="\displaystyle \|\mathrm{T}_{\frac{1}{100}}f\|_{4}^{4}\le\sum_{S\subseteq\left[n\right]}\mathbb{E}_{\mathbf{x}\sim\left\{ 1,\ldots,m\right\} ^{m}}\left(\|L_{S}\left[f\right]_{S\rightarrow\mathbf{x}}\|_{2}^{4}\right). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5C%7C%5Cmathrm%7BT%7D_%7B%5Cfrac%7B1%7D%7B100%7D%7Df%5C%7C_%7B4%7D%5E%7B4%7D%5Cle%5Csum_%7BS%5Csubseteq%5Cleft%5Bn%5Cright%5D%7D%5Cmathbb%7BE%7D_%7B%5Cmathbf%7Bx%7D%5Csim%5Cleft%5C%7B+1%2C%5Cldots%2Cm%5Cright%5C%7D+%5E%7Bm%7D%7D%5Cleft%28%5C%7CL_%7BS%7D%5Cleft%5Bf%5Cright%5D_%7BS%5Crightarrow%5Cmathbf%7Bx%7D%7D%5C%7C_%7B2%7D%5E%7B4%7D%5Cright%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \|\mathrm{T}_{\frac{1}{100}}f\|_{4}^{4}\le\sum_{S\subseteq\left[n\right]}\mathbb{E}_{\mathbf{x}\sim\left\{ 1,\ldots,m\right\} ^{m}}\left(\|L_{S}\left[f\right]_{S\rightarrow\mathbf{x}}\|_{2}^{4}\right). "/></p></blockquote>
<p>The terms <img alt="{\|L_{S}\left[f\right]_{S\rightarrow x}\|_{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7CL_%7BS%7D%5Cleft%5Bf%5Cright%5D_%7BS%5Crightarrow+x%7D%5C%7C_%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\|L_{S}\left[f\right]_{S\rightarrow x}\|_{2}}"/> appearing on the right hand side are small whenever <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> has a small dependency on <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/> and it turns out that you have the following corrolary of it, which looks a bit more similar to the hypercontractive intequality.</p>
<p> </p>
<p><strong>Corollary 2.</strong></p>
<p>Let <img alt="{f\colon\left\{ 1,\ldots,m\right\} ^{n}\rightarrow\mathbb{R}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Ccolon%5Cleft%5C%7B+1%2C%5Cldots%2Cm%5Cright%5C%7D+%5E%7Bn%7D%5Crightarrow%5Cmathbb%7BR%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f\colon\left\{ 1,\ldots,m\right\} ^{n}\rightarrow\mathbb{R}}"/>, and uppose that <img alt="{\|L_{S}[f]_{S\rightarrow x}\|_{2}\le4^{\left|S\right|}\|f\|_{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7CL_%7BS%7D%5Bf%5D_%7BS%5Crightarrow+x%7D%5C%7C_%7B2%7D%5Cle4%5E%7B%5Cleft%7CS%5Cright%7C%7D%5C%7Cf%5C%7C_%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\|L_{S}[f]_{S\rightarrow x}\|_{2}\le4^{\left|S\right|}\|f\|_{2}}"/> for all sets <img alt="{S.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S.}"/></p>
<p>Then <img alt="{\mathrm{\|\mathrm{T}_{\frac{1}{1000}}f\|_{4}\le\|f\|_{2}.}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathrm%7B%5C%7C%5Cmathrm%7BT%7D_%7B%5Cfrac%7B1%7D%7B1000%7D%7Df%5C%7C_%7B4%7D%5Cle%5C%7Cf%5C%7C_%7B2%7D.%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathrm{\|\mathrm{T}_{\frac{1}{1000}}f\|_{4}\le\|f\|_{2}.}}"/></p>
<p>Finally, one might ask wonder why this globalness notion appears only when we look at large values of <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> and not when <img alt="{m=2.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%3D2.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m=2.}"/> I think the corollary is a good explanation for that as <img alt="{\|f\|_{2}^{2}\ge\left(\frac{1}{2}\right)^{\left|S\right|}\|f_{S\rightarrow x}\|_{2}^{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7Cf%5C%7C_%7B2%7D%5E%7B2%7D%5Cge%5Cleft%28%5Cfrac%7B1%7D%7B2%7D%5Cright%29%5E%7B%5Cleft%7CS%5Cright%7C%7D%5C%7Cf_%7BS%5Crightarrow+x%7D%5C%7C_%7B2%7D%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\|f\|_{2}^{2}\ge\left(\frac{1}{2}\right)^{\left|S\right|}\|f_{S\rightarrow x}\|_{2}^{2}}"/> holds trivially for any Boolean function <img alt="{f\colon\left\{ 0,1\right\} ^{n}\rightarrow\mathbb{R}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%5Ccolon%5Cleft%5C%7B+0%2C1%5Cright%5C%7D+%5E%7Bn%7D%5Crightarrow%5Cmathbb%7BR%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f\colon\left\{ 0,1\right\} ^{n}\rightarrow\mathbb{R}.}"/></p></div>
    </content>
    <updated>2020-05-07T21:01:42Z</updated>
    <published>2020-05-07T21:01:42Z</published>
    <category term="Analysis"/>
    <category term="Combinatorics"/>
    <category term="Computer Science and Optimization"/>
    <category term="Guest post"/>
    <category term="Poetry"/>
    <category term="Probability"/>
    <category term="Barry Simon"/>
    <category term="Dor Minzer"/>
    <category term="Eoin Long"/>
    <category term="Noam Lifshitz"/>
    <category term="Peter Keevash"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2020-05-09T06:20:51Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-7656443173792446291</id>
    <link href="https://blog.computationalcomplexity.org/feeds/7656443173792446291/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/05/vidcast-on-conferences.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7656443173792446291" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7656443173792446291" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/05/vidcast-on-conferences.html" rel="alternate" type="text/html"/>
    <title>Vidcast on Conferences</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Bill and Lance have another socially-distanced <a href="https://youtu.be/VwvuTnE66xQ">vidcast</a>, this time with Lance telling the story of two conferences (<a href="http://ec20.sigecom.org/">ACM Economics and Computation</a> and the Game Theory Congress). As mentioned in the video the Game Theory Congress has been <a href="http://gametheorysociety.org/6th-world-congress-of-the-game-theory-society-in-budapest-july-13-17-2020/">postponed to next year</a>. Also mentioned in the video, for a limited time you can read Lance's <a href="https://goldenticket.fortnow.com/">book</a> on P v NP on <a href="https://muse.jhu.edu/book/36432">Project Muse</a>.<div><br/></div><div><br/></div></div>
    </content>
    <updated>2020-05-07T14:28:00Z</updated>
    <published>2020-05-07T14:28:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-05-08T21:30:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/075</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/075" rel="alternate" type="text/html"/>
    <title>TR20-075 |  Rigid Matrices From Rectangular PCPs | 

	Amey Bhangale, 

	Prahladh Harsha, 

	Orr Paradise, 

	Avishay Tal</title>
    <summary>We introduce a variant of PCPs, that we refer to as *rectangular* PCPs, wherein proofs are thought of as square matrices, and the random coins used by the verifier can be partitioned into two disjoint sets, one determining the *row* of each query and the other determining the *column*.

We construct PCPs that are *efficient*, *short*, *smooth* and (almost-)*rectangular*. As a key application, we show that proofs for hard languages in NTIME$(2^n)$, when viewed as matrices, are rigid infinitely often. This strengthens and considerably simplifies a recent result of Alman and Chen [FOCS, 2019] constructing explicit rigid matrices in FNP. Namely, we prove the following theorem:
- There is a constant $\delta \in (0,1)$ such that there is an FNP-machine that, for infinitely many $N$, on input $1^N$ outputs $N \times N$ matrices with entries in $\mathbb{F}_2$ that are $\delta N^2$-far (in Hamming distance) from matrices of rank at most $2^{\log  N/\Omega(\log \log N)}$.

Our construction of rectangular PCPs starts with an analysis of how randomness yields queries in the Reed--Muller-based outer PCP of Ben-Sasson, Goldreich, Harsha, Sudan and Vadhan [SICOMP, 2006; CCC, 2005]. We then show how to preserve rectangularity under PCP composition and a smoothness-inducing transformation. This warrants refined and stronger notions of rectangularity, which we prove for the outer PCP and its transforms.</summary>
    <updated>2020-05-06T19:28:27Z</updated>
    <published>2020-05-06T19:28:27Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-09T06:20:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/074</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/074" rel="alternate" type="text/html"/>
    <title>TR20-074 |  Depth-First Search in Directed Graphs, Revisited | 

	Eric Allender, 

	Archit Chauhan, 

	Samir Datta</title>
    <summary>We present an algorithm for constructing a depth-first search tree in planar digraphs; the algorithm can be implemented in the complexity class UL, which is contained in nondeterministic logspace NL, which in turn lies in NC^2. Prior to this (for more than a quarter-century), the fastest uniform deterministic parallel algorithm for this problem was O(log^10 n) (corresponding to the complexity class AC^10, which is contained in NC^11).

We also consider the problem of computing depth-first search trees in other classes of graphs, and obtain additional new upper bounds.</summary>
    <updated>2020-05-06T18:15:38Z</updated>
    <published>2020-05-06T18:15:38Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-09T06:20:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/073</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/073" rel="alternate" type="text/html"/>
    <title>TR20-073 |  Lower Bounds on OBDD Proofs with Several Orders | 

	Dmitry Itsykson, 

	Sam Buss, 

	Dmitry Sokolov, 

	Alexander Knop, 

	Artur Riazanov</title>
    <summary>This paper is motivated by seeking lower bounds on OBDD($\land$, weakening, reordering) refutations, namely OBDD refutations that allow weakening and arbitrary reorderings. We first work with 1-NBP($\land$) refutations based on read-once nondeterministic branching programs. These generalize OBDD($\land$, reordering) refutations. There are polynomial size 1-NBP($\land$) refutations of the pigeonhole principle, hence 1-NBP($\land$) is strictly stronger than OBDD($\land$, reordering). There are also formulas that have polynomial size tree-like resolution refutations but require exponential size 1-NBP($\land$) refutations. As a corollary, OBDD($\land$, reordering) does not simulate tree-like resolution, answering a previously open question.

The system 1-NBP($\land$, $\exists$) uses projection inferences instead of weakening. 1-NBP($\land$, $\exists_k$) is the system restricted to projection on at most $k$ distinct variables. We construct explicit constant degree graphs $G_n$ on $n$ vertices and an $\epsilon &gt; 0$, such that 1-NBP($\land$, $\exists_{\epsilon n}$) refutations of the Tseitin formula for $G_n$ require exponential size.

Second, we study the proof system OBDD($\land$, weakening, reordering$_\ell$) which allows $\ell$ different variable orders in a refutation. We prove an exponential lower bound on the complexity of tree-like OBDD($\land$, weakening, reordering$_\ell$) refutations for $\ell = \epsilon \log n$, where $n$ is the number of variables and $\epsilon &gt; 0$ is a constant. The lower bound is based on multiparty communication complexity.</summary>
    <updated>2020-05-05T18:13:59Z</updated>
    <published>2020-05-05T18:13:59Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-09T06:20:44Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-5609460581142399437</id>
    <link href="https://blog.computationalcomplexity.org/feeds/5609460581142399437/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/05/why-is-there-no-dn-grid-for-hilberts.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5609460581142399437" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5609460581142399437" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/05/why-is-there-no-dn-grid-for-hilberts.html" rel="alternate" type="text/html"/>
    <title>Why is there no (d,n) grid for Hilbert's Tenth Problem?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><br/>
Hilbert's 10th problem, in modern language is:<br/>
<br/>
Find an algorithm that will, given a poly over Z in many variables, determine if it has a solution in Z.<br/>
<br/>
This problem was proven undecidable through the work of Davis, Putnam, Robinson and then<br/>
Matiyasevich supplied the last crucial part of the proof.<br/>
<br/>
Let H10(d,n) be the problem with degree d and n variables.<br/>
<br/>
I had assumed that somewhere on the web would be a grid where the dth row, nth col has<br/>
<br/>
U if  H10(d,n) is undecidable<br/>
<br/>
D if H10(d,n) is decidable<br/>
<br/>
? if the status of H10(d,n) was unknown.<br/>
<br/>
I found no grid. I then collected up all the results I could find <a href="http://www.cs.umd.edu/~gasarch/BLOGPAPERS/h10.pdf">here</a><br/>
<br/>
This lead to the (non-math) question: Why is there no grid out there? Here are my speculations.<br/>
<br/>
1) Logicians worked on proving particular (d,n) are undecidable. They sought solutions in N. By contrast number theorists worked on proving particular (d,n) decidable. They sought solutions in Z.. Hence a grid would need to reconcile these two related problems.<br/>
<br/>
<div>
<div>
2) Logicians and number theorists didn't talk to each other. Websites and books on Hilbert's Tenth problem do not mention any solvable cases of it.</div>
</div>
<div>
<br/></div>
<div>
<div>
3) There is a real dearth of positive results, so a grid would not be that interesting. Note that we do not even know if the following is decidable: given k in Z does there exists x,y,z in Z such that</div>
<div>
<br/></div>
<div>
x^3 +y^3+ z^3 = k. I blogged about that <a href="https://blog.computationalcomplexity.org/2019/04/x-3-y-3-z-3-33-has-solution-in-z-and.html">here</a></div>
</div>
<div>
<br/></div>
<div>
4) For an undecidable result for (d,n) if you make n small then all of the results make d very large.</div>
<div>
<br/></div>
<div>
For example</div>
<div>
<br/></div>
<div>
n=9, d= 1.6 x 10^{45}  is undecidable. The status of n=9, d=1.6 x 10^{45} -1 is unknown.</div>
<div>
<br/></div>
<div>
Hence the grid would be hard to draw.</div>
<div>
<br/></div>
<div>
Frankly I don't really want a grid. I really want a sense of what open problems might be solved. I think progress has gone in other directions- H10 over other domains. Oh well, I want to know about</div>
<div>
<br/></div>
<div>
n=9 and d=1.6 x 10^{45}-1. (parenthesis ambiguous but either way would be an advance.)</div>
<div>
<br/></div>
<div>
<br/></div>
<div>
<br/></div></div>
    </content>
    <updated>2020-05-05T04:39:00Z</updated>
    <published>2020-05-05T04:39:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-05-08T21:30:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://dstheory.wordpress.com/?p=48</id>
    <link href="https://dstheory.wordpress.com/2020/05/05/friday-may-15-amin-karbasi-from-yale-university/" rel="alternate" type="text/html"/>
    <title>Friday, May 15 — Amin Karbasi from Yale University</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The fourth Foundations of Data Science virtual talk will take place on Friday, May 15th at 11:00 AM Pacific Time (2:00 pm Eastern Time, 20:00 Central European Time, 19:00 UTC).  Amin Karbasi from Yale University will speak about “User-Friendly Submodular Maximization”. Abstract: Submodular functions model the intuitive notion of diminishing returns. Due to their far-reaching applications, they<a class="more-link" href="https://dstheory.wordpress.com/2020/05/05/friday-may-15-amin-karbasi-from-yale-university/">Continue reading <span class="screen-reader-text">"Friday, May 15 — Amin Karbasi from Yale University"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The fourth Foundations of Data Science virtual talk will take place on Friday, May 15th at 11:00 AM Pacific Time (2:00 pm Eastern Time, 20:00 Central European Time, 19:00 UTC).  <strong>Amin Karbasi </strong>from Yale University will speak about “<em>User-Friendly Submodular Maximization</em>”.</p>



<p class="has-text-align-left"><strong>Abstract</strong>: Submodular functions model the intuitive notion of diminishing returns. Due to their far-reaching applications, they have been rediscovered in many fields such as information theory, operations research, statistical physics, economics, and machine learning. They also enjoy computational tractability as they can be minimized exactly or maximized approximately.</p>



<p>The goal of this talk is simple. We see how a little bit of randomness, a little bit of greediness, and the right combination can lead to pretty good methods for offline, streaming, and distributed solutions. I do not assume any background on submodularity and try to explain all the required details during the talk.</p>



<p><a href="https://sites.google.com/view/dstheory" rel="noreferrer noopener" target="_blank">Please register here to join the virtual talk.</a></p>



<p>The series is supported by the <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1934846&amp;HistoricalAwards=false">NSF HDR TRIPODS Grant 1934846</a>.</p></div>
    </content>
    <updated>2020-05-05T01:30:02Z</updated>
    <published>2020-05-05T01:30:02Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>dstheory</name>
    </author>
    <source>
      <id>https://dstheory.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://dstheory.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://dstheory.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://dstheory.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://dstheory.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Foundation of Data Science – Virtual Talk Series</title>
      <updated>2020-05-09T06:22:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/072</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/072" rel="alternate" type="text/html"/>
    <title>TR20-072 |  Locally testable codes via high-dimensional expanders | 

	Irit Dinur, 

	Prahladh Harsha, 

	Yotam Dikstein, 

	Noga Ron-Zewi</title>
    <summary>Locally testable codes (LTC) are error-correcting codes that have a local tester which can distinguish valid codewords from words that are far from all codewords, by probing a given word only at a very small (sublinear, typically constant) number of locations. Such codes form the combinatorial backbone of PCPs. A major open problem is whether there exist LTCs with positive rate, constant relative distance and testable with a constant number of queries. 

In this paper, we present a new approach towards constructing such LTCs using the machinery of high-dimensional expanders. 
To this end, we consider the Tanner representation of a code, which is specified by a graph and a base code. Informally, our result states that if this graph is part of an {\em agreement expander} then the local testability of the code follows from the local testability of the base code. Agreement expanders allow one to stitch together many mostly-consistent local functions into a single global function. High-dimensional expanders are known to yield agreement expanders with constant degree. 

This work unifies and generalizes the known results on testability of the Hadamard, Reed-Muller and lifted codes, all of which are proved via a single round of local self-correction: the corrected value at a vertex v depends on the values of all vertices that share a constraint with v. In the above codes this set includes all of the vertices. In contrast, in our setting the degree of a vertex might be a constant, so we cannot hope for one-round self-correction. We overcome this technical hurdle by performing iterative self correction with logarithmically many rounds and tightly controlling the error in each iteration using properties of the agreement expander.

Given this result, the missing ingredient towards constructing a constant-query LTC with positive rate and constant relative distance is an instantiation of a base code and a constant-degree agreement expander that interact well with each other.</summary>
    <updated>2020-05-05T00:09:11Z</updated>
    <published>2020-05-05T00:09:11Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-09T06:20:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/05/04/postdoc-position-at-university-of-alberta-apply-by-december-31-2020/</id>
    <link href="https://cstheory-jobs.org/2020/05/04/postdoc-position-at-university-of-alberta-apply-by-december-31-2020/" rel="alternate" type="text/html"/>
    <title>postdoc position at University of Alberta (apply by December 31, 2020)</title>
    <summary>The Theory Group in the Dept. of Computing Science at U. of Alberta invites applications for TWO postdoc positions. The successful applicants are expected to work closely with Zachary Friggstad and Mohammad Salavatipour in the areas of: approx algorithms, hardness of approximation, combinatorial optimization. For details see https://webdocs.cs.ualberta.ca/~mreza/pdf-ad7.pdf Email: mrs@ualberta.ca Website: http://www.cs.ualberta.ca Email: mrs@ualberta.ca</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Theory Group in the Dept. of Computing Science at U. of Alberta invites applications for TWO postdoc positions. The successful applicants are expected to work closely with Zachary Friggstad and Mohammad Salavatipour in the areas of: approx algorithms, hardness of approximation, combinatorial optimization. For details see <a href="https://webdocs.cs.ualberta.ca/~mreza/pdf-ad7.pdf">https://webdocs.cs.ualberta.ca/~mreza/pdf-ad7.pdf</a><br/>
Email: mrs@ualberta.ca</p>
<p>Website: <a href="http://www.cs.ualberta.ca">http://www.cs.ualberta.ca</a><br/>
Email: mrs@ualberta.ca</p></div>
    </content>
    <updated>2020-05-04T18:43:57Z</updated>
    <published>2020-05-04T18:43:57Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2020-05-09T06:21:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/071</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/071" rel="alternate" type="text/html"/>
    <title>TR20-071 |  A Tight Lower Bound on Adaptively Secure Full-Information Coin Flip | 

	Iftach Haitner, 

	Yonatan Karidi-Heller</title>
    <summary>In a distributed coin-flipping protocol, Blum [ACM Transactions on Computer Systems '83],
the parties try to output a common (close to) uniform bit, even when some adversarially chosen parties try to bias the common output. In an adaptively secure full-information coin flip, Ben-Or and Linial [FOCS '85], the parties communicate over a broadcast channel and a computationally unbounded adversary can choose which parties to corrupt during the protocol execution. Ben-Or and Linial proved that the $n$-party majority protocol is resilient to $o(\sqrt{n})$ corruptions (ignoring log factors), and conjectured this is a tight upper bound for any $n$-party protocol (of any round complexity). Their conjecture was proved to be correct for single-turn (each party sends a single message) single-bit (a message is one bit) protocols, Lichtenstein, Linial, and Saks [Combinatorica '89], symmetric protocols Goldwasser, Kalai, and Park [ICALP '15], and recently for (arbitrary message length) single-turn protocols Tauman Kalai, Komargodski, and Raz [DISC '18]. Yet, the question for many-turn (even single-bit) protocols was left completely open.

In this work we close the above gap, proving that no $n$-party protocol (of any round complexity) is resilient to $O(\sqrt{n})$ (adaptive) corruptions.</summary>
    <updated>2020-05-04T14:14:22Z</updated>
    <published>2020-05-04T14:14:22Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-09T06:20:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/070</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/070" rel="alternate" type="text/html"/>
    <title>TR20-070 |  On the list recoverability of randomly punctured codes | 

	Ben Lund, 

	Aditya Potukuchi</title>
    <summary>We show that a random puncturing of a code with good distance is list recoverable beyond the Johnson bound.
In particular, this implies that there are Reed-Solomon codes that are list recoverable beyond the Johnson bound.
It was previously known that there are Reed-Solomon codes that do not have this property. 
As an immediate corollary to our main theorem, we obtain better degree bounds on unbalanced expanders that come from Reed-Solomon codes.</summary>
    <updated>2020-05-04T09:06:23Z</updated>
    <published>2020-05-04T09:06:23Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-09T06:20:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://ptreview.sublinear.info/?p=1297</id>
    <link href="https://ptreview.sublinear.info/?p=1297" rel="alternate" type="text/html"/>
    <title>News for April 2020</title>
    <summary>April is now behind us, and we hope you and your families are all staying safe and healthy. We saw six seven property papers appear online last month, so at least there is some reading ahead of us! A mixture of privacy, quantum, high-dimensional distributions, and juntas (juntæ?). A lot of distribution testing, overall. Connecting […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>April is now behind us, and we hope you and your families are all staying safe and healthy. We saw <s>six</s> seven property papers appear online last month, so at least there is some reading ahead of us! A mixture of privacy, quantum, high-dimensional distributions, and juntas (juntæ?). A lot of distribution testing, overall.</p>



<p><strong>Connecting Robust Shuffle Privacy and Pan-Privacy</strong>, by Victor Balcer, Albert Cheu, Matthew Joseph, and Jieming Mao (<a href="https://arxiv.org/abs/2004.09481">arXiv</a>). This paper considers a recent notion of differential privacy called<em> shuffle privacy</em>, where users have sensitive data, a central untrusted server wants to do something with that data (for instance, say… testing its distribution), and a trusted middle-man/entity shuffles the users’ messages u.a.r. to bring in a bit more anonymity. As it turns out, testing uniformity (or identity) of distributions in the shuffle privacy model is (i) much harder than without privacy constraints; (ii) much harder than with ‘usual’ (weaker) differential privacy (iii) much easier than with local privacy; (iv) related to the sample complexity under another privacy notion, <em>pan-privacy</em>. It’s a brand exciting new world out there!</p>



<p><em>(Note: for the reader interested in keeping track of identity/uniformity testing of probability distributions under various privacy models, I wrote a very short summary of the current results <a href="https://github.com/ccanonne/probabilitydistributiontoolbox/blob/master/private-goodness-of-fit.pdf">here</a>.)</em></p>



<p><strong>Entanglement is Necessary for Optimal Quantum Property Testing, </strong>by Sebastien Bubeck, Sitan Chen, and Jerry Li (<a href="https://arxiv.org/abs/2004.07869">arXiv</a>). The analogue of uniformity testing, in the quantum world, is testing whether a quantum state is equal (or far from) the maximally mixed state. It’s known that this task  has “quantum sample complexity” (number of measurements) \(\Theta(d/\varepsilon^2)\) (i.e., square root dependence on  the dimension of the state, \(d^2\)). But this requires <em>entangled</em> measurements, which may be tricky to get (or, in my case, understand): what happens if the measurements can be adaptive, but not entangled? In this work, the authors show that, under this weaker access model \(\Omega(d^{4/3}/\varepsilon^2)\) measurements are necessary: adaptivity alone won’t cut it. It may still help though: without either entanglement <em>nor</em> adaptivity, the authors also show a \(\Omega(d^{3/2}/\varepsilon^2)\) measurements lower bound.</p>



<p><strong>Testing Data Binnings</strong>, by Clément Canonne and Karl Wimmer (<a href="https://eccc.weizmann.ac.il/report/2020/062/">ECCC</a>). More identity testing! Not private and not quantum for this one, but… not <em>quite</em> identity testing either. To paraphrase the abstract: this paper introduces (and gives near matching bounds for)  the related question of <em>identity up to binning</em>, where the reference distribution \(q\) is over \(k \ll n\) elements: the question is then whether there exists a suitable binning of the domain \([n]\) into \(k\) intervals such that, <em>once binned</em>, \(p\) is equal to \(q\).” </p>



<p><strong>Hardness of Identity Testing for Restricted Boltzmann Machines and Potts models</strong>, by Antonio Blanca, Zongchen Chen, Daniel Štefankovič, and Eric Vigoda (<a href="https://arxiv.org/abs/2004.10805">arXiv</a>). Back to identity testing of distributions, but for high-dimensional structured ones this one. Specifically, this paper focuses on the undirected graphical models known as <em>restricted Boltzmann machines, </em>and provides efficient algorithms for identity testing and conditional hardness lower bounds depending on the type of correlations allowed in the graphical models.</p>



<p><strong>Robust testing of low-dimensional functions</strong>, by Anindya De, Elchanan Mossel, and Joe Neeman (<a href="https://arxiv.org/abs/2004.11642">arXiv</a>). Junta testing is a classical, central problem in property testing, with motivations and applications in machine learning and complexity. The related (and equally well-motivated) question of junta testing of functions on \(\mathbb{R}^d\) (instead of the Boolean hypercube) was recently studied by the same authors; and the related (and, again, equally well-motivated) question of <em>tolerant</em> junta testing on the Boolean hypercube was also recently studied (among other works) by the same authors. Well, this paper does it all, and tackles the challenging (and, for a change, equally well-motivated!) question of <em>tolerant</em> testing of juntas  on \(\mathbb{R}^d\).</p>



<p><strong>Differentially Private Assouad, Fano, and Le Cam</strong>, by Jayadev Acharya, Ziteng Sun, and Huanyu Zhang (<a href="https://arxiv.org/abs/2004.06830">arXiv</a>). Back to probability distributions and privacy. This paper provides differentially private analogues of the classical eponymous statistical inference results (Assouad’s lemma, Fano’s inequality, and Le Cam’s method). In particular, it gives ready-to-use, blackbox tools to prove testing and learning lower bounds for distributions in the differentially private setting, and shows how to use them to easily derive, and rederive, several lower bounds.</p>



<p><strong>Edit: </strong>We missed one!</p>



<p><strong>Learning and Testing Junta Distributions with Subcube Conditioning</strong>, by Xi Chen, Rajesh Jayaram, Amit Levi, Erik Waingarten (<a href="https://arxiv.org/abs/2004.12496">arXiv</a>). This paper focuses on the <em>subcube conditioning</em> model of (high-dimensional) distribution testing, where the algorithm can fix some variables to values of its choosing and get samples conditioned on those variables. Extending and refining techniques from <a href="https://ptreview.sublinear.info/?p=1227">a previous work by a (sub+super)set of the authors</a>, the paper shows how to optimally learn and test <a href="http://proceedings.mlr.press/v49/aliakbarpour16.html">junta distributions</a> in this framework—with exponential savings with respect to the usual i.i.d. sampling model.</p></div>
    </content>
    <updated>2020-05-04T01:52:43Z</updated>
    <published>2020-05-04T01:52:43Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Clement Canonne</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2020-05-08T22:45:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://blog.simons.berkeley.edu/?p=164</id>
    <link href="https://blog.simons.berkeley.edu/2020/05/fine-grained-hardness-of-lattice-problems-open-questions/" rel="alternate" type="text/html"/>
    <title>Fine-grained hardness of lattice problems: Open questions</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">1 Introduction 1.1 Lattices and lattice-based cryptography Lattices are classically-studied geometric objects that in the past few decades have found a multitude of applications in computer science. The most important application area is lattice-based cryptography, the design of cryptosystems whose … <a href="https://blog.simons.berkeley.edu/2020/05/fine-grained-hardness-of-lattice-problems-open-questions/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h1>1 Introduction</h1>
<h2>1.1 Lattices and lattice-based cryptography</h2>
<p>Lattices are classically-studied geometric objects that in the past few decades have found a multitude of applications in computer science. The most important application area is <em>lattice-based cryptography</em>, the design of cryptosystems whose security is based on the apparent intractability of computational problems on lattices, even for quantum computers. Indeed, lattice-based cryptography has revolutionized the field because of its apparent quantum resistance and its other attractive security, functionality, and efficiency properties.</p>
<p>Intuitively, a lattice is a regular ordering of points in some (typically high-dimensional) space. More precisely, a <em>lattice</em> \( {{\cal{L}}}\) of rank \( {n}\) is the set of all integer linear combinations of some linearly independent vectors \( {\mathbf{b}_1, \ldots, \mathbf{b}_n}\), which are called a <em>basis</em> of \( {{\cal{L}}}\). We will be primarily interested in analyzing the running times of lattice algorithms as functions of the lattice’s rank \( {n}\).</p>
<h2>1.2. Computational lattice problems</h2>
<p>The two most important computational problems on lattices are the Shortest Vector Problem (SVP) and the Closest Vector Problem (CVP). SVP asks, given a basis of a lattice \( {{\cal{L}}}\) as input, to find a shortest non-zero vector in \( {{\cal{L}}}\). CVP, which can be viewed as an inhomogeneous version of SVP, asks, given a basis of a lattice \( {{\cal{L}}}\) and a target point \( {\mathbf{t}}\) as input, to find a closest vector in \( {{\cal{L}}}\) to \( {\mathbf{t}}\).</p>
<p>Algorithms for solving SVP form the core of the best known attacks on lattice-based cryptography both in theory and in practice. Accordingly, it is critical to understand the precise complexity of SVP as well as possible. The best provably correct algorithms for both SVP and CVP run in \( {2^{n + o(n)}}\)-time [<a href="https://arxiv.org/abs/1412.7994">ADRS15</a>, <a href="https://arxiv.org/abs/1504.01995">ADS15</a>, <a href="https://arxiv.org/abs/1709.01535">AS18a</a>]. The best heuristic algorithms for SVP run in \( {2^{cn + o(n)}}\)-time for \( {c = 0.292}\) classically [<a href="https://eprint.iacr.org/2015/1128">BDGL16</a>] and \( {c = 0.265}\) using quantum speedups [<a href="http://www.thijs.com/docs/phd-final.pdf">Laa15</a>] (see also [<a href="https://eprint.iacr.org/2019/1016">KMPR19</a>]), and most real-world lattice-based cryptosystems assume that these algorithms are close to optimal. Indeed, many of these cryptosystems assume what Bos et al. [<a href="https://eprint.iacr.org/2016/659">B+16</a>] call a “paranoid” worst-case estimate of \( {c = 0.2075}\) (based on the kissing number and assuming that sieving algorithms are optimal) as the fastest hypothetical running time for SVP algorithms when choosing parameters. (See also Albrecht et al. [<a href="https://estimate-all-the-lwe-ntru-schemes.github.io/paper.pdf">A+18</a>], which surveys the security assumptions made in a wide range of lattice-based cryptosystems.) Accordingly, the difference in being able to solve SVP in \( {2^{0.2075n}}\) versus \( {2^{n/20}}\) versus \( {2^{\sqrt{n}}}\) time may mean the difference between lattice-based cryptosystems being secure, insecure with current parameters, or effectively broken in practice.</p>
<p>There is a rank-preserving reduction from SVP to CVP [<a href="https://cseweb.ucsd.edu/~daniele/papers/GMSS.pdf">GMSS99</a>], so any algorithm for CVP immediately gives an essentially equally fast algorithm for SVP. In other words, CVP is at least as hard as SVP (and probably a bit harder). Indeed, historically, almost all lower bounds for SVP are proven via reduction from CVP (and nearly all algorithmic progress on CVP uses ideas originally developed for SVP).</p>
<h2>1.3. Fine-grained hardness</h2>
<p>The field of fine-grained complexity works to give strong, quantitative lower bounds on computational problems assuming standard complexity-theoretic assumptions. Proving such a (conditional) lower bound for an \( {{\mathsf{NP}}}\)-hard problem generally works by (1) assuming a stronger hardness assumption than \( {{\mathsf{P}} \neq {\mathsf{NP}}}\) about the complexity of \( {k}\)-SAT (such as ETH or SETH, defined below), and (2) giving a highly efficient reduction from \( {k}\)-SAT to the problem. The most important hardness assumptions for giving lower bounds on \( {{\mathsf{NP}}}\)-hard problems are the Exponential Time Hypothesis (ETH) and the Strong Exponential Time Hypothesis (SETH) of Impagliazzo and Paturi [<a href="https://cseweb.ucsd.edu/~paturi/myPapers/pubs/ImpagliazzoPaturi_2001_jcss.pdf">IP01</a>]. ETH asserts that there is no \( {2^{o(n)}}\)-time algorithm for \( {3}\)-SAT, and SETH asserts that for every \( {\epsilon &gt; 0}\) there exists \( {k \in {\mathbb Z}^+}\) such that there is no \( {2^{(1 – \epsilon)n}}\)-time algorithm for \( {k}\)-SAT, where \( {n}\) denotes the number of variables in the SAT instance.</p>
<p>Here by “highly efficient” reductions we mean linear ones, i.e., reductions that map a \( {3}\)-SAT or \( {k}\)-SAT formula on \( {n}\) variables to an SVP or CVP instance of rank \( {C n + o(n)}\) for some absolute constant \( {C &gt; 0}\). Indeed, by giving a reduction from \( {3}\)-SAT (respectively, \( {k}\)-SAT for any \( {k \in {\mathbb Z}^+}\)) instances on \( {n}\) variables to SVP or CVP instances of rank \( {C n + o(n)}\), we can conclude that there is no \( {2^{o(n)}}\)-time (resp., \( {2^{(1-\epsilon)n/C}}\)-time for any \( {\epsilon &gt; 0}\)) algorithm for the corresponding problem assuming ETH (resp., SETH). Note that the smaller the value of \( {C}\) for which one can show such a reduction, the stronger the conclusion. In particular, a reduction mapping \( {k}\)-SAT instances on \( {n}\) variables to SVP or CVP instances of rank \( {n + o(n)}\) would imply an essentially tight lower bound on the corresponding problem assuming SETH — as mentioned above, the best provably correct algorithms for both SVP and CVP run in time \( {2^{n + o(n)}}\).</p>
<h2>1.4. Fine-grained hardness of CVP (and SVP)</h2>
<p>It is relatively easy to show that CVP is “ETH-hard,” i.e., to show that a \( {2^{o(n)}}\)-time algorithm for CVP would imply a \( {2^{o(n)}}\)-time algorithm for \( {3}\)-SAT instances with \( {n}\) variables. This would falsify ETH. (It’s a nice exercise to show that the Subset Sum problem on a set of size \( {n}\) reduces to CVP on a lattice of rank \( {n}\), which implies the result.)</p>
<p>With some work, Divesh Aggarwal and Noah extended this to SVP [<a href="https://arxiv.org/abs/1712.00942">AS18b</a>]. In particular, we showed a reduction from CVP to SVP that only increases the rank of the lattice by some constant multiplicative factor. (Formally, the reduction only works with certain minor constraints on the CVP instance. The reduction originally relied on a geometric conjecture, which was open for decades. But, Serge Vlăduţ proved the conjecture [<a href="https://arxiv.org/abs/1802.00886">Vlă19</a>] shortly after we published!)</p>
<p>So, unless ETH is false, there is no \( {2^{o(n)}}\)-time algorithm for CVP or SVP. But, for cryptographic applications, even, say, a \( {2^{n/20}}\)-time algorithm would be completely devastating. If such an algorithm were found, cryptographic schemes that we currently think are secure against absurdly powerful attackers straight out of science fiction (say, one with a computer the size of the sun running until the heat death of the universe) would turn out to be easily broken (e.g., in seconds on our laptops).</p>
<p>In [<a href="https://arxiv.org/abs/1704.03928">BGS17</a>, <a href="https://arxiv.org/abs/1911.02440">ABGS20</a>], we <em>almost</em> showed that CVP is “SETH-hard,” i.e., that a \( {2^{(1-\epsilon)n}}\)-time algorithm for CVP would imply such an algorithm for \( {k}\)-SAT for <em>any</em> constant \( {k}\). This would falsify SETH. So, we <em>almost</em> showed that the [<a href="https://arxiv.org/abs/1504.01995">ADS15</a>] algorithm is optimal. The “almost” is because our proof works with \( {\ell_p}\) norms, that is, we show hardness for the version of CVP in which the distance from the target to a lattice vector is defined in terms of the \( {\ell_p}\) norm,</p>
<p align="center">\( \displaystyle \|\mathbf{x}\|_p := (|x_1|^p + \cdots + |x_d|^p)^{1/p} \; . \)</p>
<p>We call the corresponding problem \( {{\mathrm{CVP}}_p}\). In fact, our proof works for all \( {\ell_p}\) norms <em>except</em> when \( {p}\) is an even integer. (To see why this might happen, notice \( {\|\mathbf{x}\|_p^p}\) is a polynomial in the \( {x_i}\) if and only if \( {p}\) is an even integer. In fact, there’s some sense in which “\( {\ell_2}\) is the easiest norm,” because for any \( {p}\), there is a linear map \( {A \in {\mathbb R}^{d \times m}}\) such that \( {m}\) is not too large and \( {\|\mathbf{x}\|_2 \approx \|A \mathbf{x}\|_p}\).) Of course, we are most interested in the case \( {p= 2}\) (the only case for which the [<a href="https://arxiv.org/abs/1504.01995">ADS15</a>] algorithm works), which is an even integer! Indeed, for all \( {p \neq 2}\), the fastest known algorithm for CVP is still Ravi Kannan’s \( {n^{O(n)}}\)-time algorithm from 1987 [<a href="https://kilthub.cmu.edu/articles/Minkowski_s_convex_body_theorem_and_integer_programming/6607328/files/12097865.pdf">Kan87</a>]. (For SVP and for constant-factor approximate CVP, \(2^{O(n)}\)-time algorithms are known [<a href="https://arxiv.org/abs/1011.5666">DPV11</a>].)</p>
<p>In fact, we showed that for \( {p = 2}\), no “natural” reduction can rule out a \( {2^{3n/4}}\)-time algorithm for CVP under SETH. A “natural” reduction is one with a fixed bijection between witnesses. In particular, any “natural” reduction from \( {3}\)-SAT to CVP must reduce to a lattice with rank at least roughly \( {4n/3}\). So, new ideas will be needed to prove stronger hardness of CVP in the \( {\ell_2}\) norm.</p>
<h1>2. Open problems</h1>
<p>We now discuss some of the problems that we left open in [<a href="https://arxiv.org/abs/1704.03928">BGS17</a>, <a href="https://arxiv.org/abs/1911.02440">ABGS20</a>]. For simplicity, we ask for specific results (e.g., “prove that problem \( {A}\) is \( {T}\)-hard under hypothesis \( {B}\)“), but of course any similar results would be very interesting (e.g., “\( {A}\) is \( {T’}\)-hard under hypothesis \( {B’}\)“).</p>
<h2>2.1. Hardness in the \(\ell_2\) norm</h2>
<p>The most obvious question that we left open is, of course, to prove similar \( {2^n}\)-time hardness results for \( {{\mathrm{CVP}}_2}\) (and more generally for \( {{\mathrm{CVP}}_p}\) for even integers \( {p}\)).</p>
<blockquote>
<p><b>Open problem 1.</b> Show that there is no \( {2^{0.99 n}}\)-time algorithm for \( {{\mathrm{CVP}}_2}\) assuming SETH.</p>
</blockquote>
<p>Remember that we showed that any proof of such a strong result would have to use an “unnatural” reduction. So, a fundamentally different approach is needed. One potentially promising direction would be to find a Cook reduction, as our limitations only apply to Karp reductions.</p>
<p>Alternatively, one might try for a different result that gets around this “natural” reduction limitations. E.g., even the following much weaker result would be very interesting.</p>
<blockquote>
<p><b>Open problem 2.</b> Show an efficient reduction from \( {3}\)-SAT on \( {n}\) variables to \( {{\mathrm{CVP}}_2}\) on a lattice of rank \( {\approx 10n}\).</p>
</blockquote>
<p>Such a reduction to \( {{\mathrm{CVP}}_2}\) on a lattice of rank \( {Cn}\) for some large constant \( {C}\) is known by applying the Sparsification Lemma [<a href="https://cseweb.ucsd.edu/~russell/ipz.pdf">IPZ01</a>] to \( {3}\)-SAT, but showing such a reduction for any reasonably small \( {C}\) or even any explicit \( {C}\) using a different proof technique would be interesting.</p>
<p>Also, our limitations only apply to reductions that map satisfying assignments to <em>exact</em> closest vectors. So, one might try to get around our limitation by working directly with approximate versions of \( {3}\)-SAT and \( {{\mathrm{CVP}}_2}\). (In [<a href="https://arxiv.org/abs/1911.02440">ABGS20</a>], we show such reductions from Gap-\( {k}\)-SAT to constant-factor approximate \( {{\mathrm{CVP}}_p}\) for all \( {p \notin 2{\mathbb Z}}\) as well as all \( {k \leq p}\). We also show reductions from Gap-\( {k}\)-Parity that achieve relatively large approximation factors.)</p>
<blockquote>
<p><b>Open problem 3.</b> Show an efficient reduction from Gap-\( {3}\)-SAT on \( {n}\) variables to approximate \( {{\mathrm{CVP}}_2}\) on a lattice of rank \( {n}\).</p>
</blockquote>
<h2>2.2. Hardness in \(\ell_p\) norms</h2>
<p>Intuitively, one reason that we are able to prove such strong results for \( {\ell_p}\) norms for \( {p \neq 2}\) is because we can use lattices with large ambient dimension \( {d}\) but low rank \( {n}\). In other words, while our reductions produce lattices \( {{\cal{L}}}\) that live in some \( {n}\)-dimensional subspace of \( {\ell_p}\)-space, the ambient space itself has large dimension \( {d}\) relative to \( {n}\). Of course, any subspace of the \( {\ell_2}\) norm is an \( {\ell_2}\) subspace (i.e., every slice of a ball is a lower-dimensional ball), so in the \( {\ell_2}\) norm, one can assume without loss of generality that \( {d = n}\). In particular, if we were able to prove \( {2^n}\)-hardness for the \( {\ell_2}\) norm, then we would actually prove \( {2^d}\)-hardness for free. However, a potentially easier problem would be to improve the \( {2^n}\)-hardness of \( {{\mathrm{CVP}}_p}\) shown in [<a href="https://arxiv.org/abs/1704.03928">BGS17</a>, <a href="https://arxiv.org/abs/1911.02440">ABGS20</a>]  to \( {2^d}\)-hardness for some \( p \neq 2 \).</p>
<blockquote>
<p><b>Open problem 4.</b> Show that there is no \( {2^{0.99 d}}\)-time algorithm for \( {{\mathrm{CVP}}_p}\) (for some \( {p}\)) assuming SETH.</p>
</blockquote>
<p>More generally, it would be very interesting to settle the fine-grained complexity of \( {{\mathrm{CVP}}_p}\) for some \( {p \neq 2}\) (either in terms of rank \( {n} \) or dimension \( {d} \)). This could take the form either of showing improved algorithms (currently the fastest algorithms for \( {{\mathrm{CVP}}_p}\) for general \( {p}\) run in \( {n^{O(n)}}\)-time [<a href="https://kilthub.cmu.edu/articles/Minkowski_s_convex_body_theorem_and_integer_programming/6607328/files/12097865.pdf">Kan87</a>], and \( {2^{O(n)}}\)-time for a constant approximation factor [<a href="https://arxiv.org/abs/1011.5666">DPV11</a>]), or showing super-\( {2^n}\) hardness, or both.</p>
<blockquote>
<p><b>Open problem 5.</b> Show matching upper bounds and lower bounds (under SETH) for \( {{\mathrm{CVP}}_p}\) for some \( {p}\) (possibly with a constant approximation factor).</p>
</blockquote>
<p>The case where \( {p = \infty}\) is especially interesting. Indeed, because the kissing number in the \( {\ell_\infty}\) norm is \( {3^n-1}\), one might guess that the fastest algorithms for \( {{\mathrm{CVP}}_\infty}\) and \( {{\mathrm{SVP}}_\infty}\) actually run in time \( {3^{n + o(n)}}\) or perhaps \( {3^{d + o(d)}}\). (See [<a href="https://arxiv.org/pdf/1801.02358">AM18</a>], which essentially achieves this.) We therefore ask whether stronger lower bounds can be proven in this special case.</p>
<blockquote>
<p><b>Open problem 6.</b> Show that \( {{\mathrm{CVP}}_\infty}\) cannot be solved in time \( {3^{0.99n}}\) (under SETH).</p>
</blockquote>
<h2>2.3. Hardness closer to crypto</h2>
<p>The most relevant problem to cryptography is approximate \( {{\mathrm{SVP}}_2}\) with an approximation factor that is polynomial in the rank \( {n}\). Our fastest algorithms to solve this problem work via a reduction to exact (or near exact) \( {{\mathrm{SVP}}_2}\) with some lower rank \( {n’ = \Theta(n)}\), so that even for these polynomial approximation factors, our fastest algorithms run in time \( {2^{\Omega(n)}}\) (where the hidden constant depends on the polynomial; see <a href="https://blog.simons.berkeley.edu/2020/04/lattice-blog-reduction-part-i-bkz/">Michael’s post</a> for more on this topic). And, hardness results for exact SVP rule out attacks on cryptography that use such reductions. We currently only know how to rule out \( {2^{o(n)}}\)-time algorithms for \( {{\mathrm{SVP}}_2}\) (under the Gap-ETH assumption). We ask whether we can do better. (In [<a href="https://arxiv.org/abs/1712.00942">AS18b</a>], we proved the stronger result below for \( {\ell_p}\) norms for large enough \( {p \notin 2{\mathbb Z}}\).)</p>
<blockquote>
<p><b>Open problem 7.</b> Prove that there is no \( {2^{n/10}}\)-time algorithm for \( {{\mathrm{SVP}}_2}\) (under SETH).</p>
</blockquote>
<p>Of course, we would ideally like to directly rule out faster algorithms for approximate \( {{\mathrm{SVP}}_2}\) with the approximation factors that are most directly relevant to cryptography. There are serious complexity-theoretic barriers to overcome to get all the way there (e.g., \( {{\mathrm{CVP}}_p}\) and \( {{\mathrm{SVP}}_p}\) are known to be in \( {{\mathsf{NP}}} \cap {{\mathsf{coNP}}}\) for large enough polynomial approximation factors. But, we can still hope to get as close as possible, by proving stronger hardness results for approximate \( {{\mathrm{CVP}}_p}\) and approximate \( {{\mathrm{SVP}}_p}\). Indeed, a beautiful sequence of works showed hardness for approximation factors up to \( {n^{c/\log \log n}}\) (so “nearly polynomial) [<a href="http://www.wisdom.weizmann.ac.il/~dinuri/mypapers/cvpjournal.pdf">DKRS03</a>, <a href="https://arxiv.org/abs/1806.04087">HR12</a>], but these results are not fine grained.</p>
<p>The best <em>fine-grained</em> hardness of approximation results known rule out algorithms for small constant-factor approximations for \( {{\mathrm{CVP}}_p}\) with \( {p \notin 2{\mathbb Z}}\) in time \( {2^{0.99n}}\) for \( {{\mathrm{CVP}}_p}\) and \( {{\mathrm{SVP}}_p}\) for any \( {p}\) in time \( {2^{o(n)}}\). We ask whether we can do better.</p>
<blockquote>
<p><b>Open problem 8.</b> Prove that there is no \( {2^{0.99 n}}\)-time algorithm for \( {2}\)-approximate \( {{\mathrm{CVP}}_p}\) (under some form of Gap-SETH, see below).</p>
</blockquote>
<blockquote>
<p><b>Open problem 9.</b> Prove that there is no \( {2^{o(n)}}\)-time algorithm for \( {\gamma}\)-approximate \( {{\mathrm{CVP}}_p}\) for superconstant \( {\gamma = \omega(1)}\) (under Gap-ETH).</p>
</blockquote>
<h2>2.4. Gap-SETH?</h2>
<p>One issue that arose in our attempts to prove fine-grained hardness of approximation results is that we don’t even know the “right” complexity-theoretic assumption about approximate CSPs to use as a starting point. For fine-grained hardness of exact problems, ETH and SETH are very well established hypotheses, and they are in some sense “the weakest possible” assumptions of their form. E.g., it is easy to see that \( {k}\)-SAT is \( {2^{Cn}}\) hard if any \( {k}\)-CSP is. But, for hardness of approximation, the situation is less clear.</p>
<p>The analogue of ETH in the regime of hardness of approximation is the beautiful Gap-ETH assumption, which was defined independently by Irit Dinur [<a href="https://eccc.weizmann.ac.il/report/2016/128/">Din16</a>] and Pasin Manurangsi and Prasad Raghavendra [<a href="https://arxiv.org/pdf/1607.02986.pdf">MR17</a>]. This assumption says that there exists some constant approximation factor \( {\delta \neq 1}\) such that \( {\delta}\)-Gap-\( {3}\)-SAT cannot be solved in time \( {2^{o(n)}}\). (Formally, both Dinur and Manurangsi and Raghavendra say that there is no \( {2^{o(n)}}\)-time algorithm that distinguishes a satisfiable formula from a formula for which no assignment satisfies more than a \( {(1-\epsilon)}\) fraction of the clauses, but we ignore this requirement of perfect completeness here.) It is easy to see that this hypothesis is equivalent to a similar hypothesis about any \( {3}\)-CSP (or, indeed, any \( {k}\)-CSP for any constant\( {k}\)).</p>
<p>However, to prove hardness of approximation with the finest of grains, we need some “gap” analogue of SETH, i.e., we would like to assume that for large enough \( {k}\), some Gap-\( {k}\)-CSP is hard to approximate up to some constant factor \( {\delta \neq 1}\) in better than \( {2^{0.99n}}\)-time. (Formally, we should add an additional variable \( {\epsilon &gt; 0}\) and have such a hypothesis for every running time \( {2^{(1-\epsilon)n}}\), but we set \( {\epsilon = 0.01}\) here to keep things relatively simple.)</p>
<p>An issue arises here concerning the dependence of the approximation factor \( {\delta}\) on the arity \( {k}\). In particular, recall that \( {k}\)-SAT can be trivially approximated up to a factor of \( {1-2^{-k}}\) (since a random assignment satisfies a \( {1-2^{-k}}\) fraction of the clauses in expectation). So, if we define Gap-SETH in terms of Gap-\( {k}\)-SAT, then we must choose \( {\delta = \delta(k) \geq 1-2^{-k}}\) that converges to one as \(k\) increases. Manurangsi proposed such a version of Gap-SETH in his thesis [<a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2019/EECS-2019-49.html">Man19</a>, Conjecture 12.1], specifically that for every large enough constant \( {k}\) there exists a constant \( {\delta = \delta(k) \neq 1}\) such that Gap-\( {k}\)-SAT cannot be approximated up to a factor of \( {\delta}\) in time \( {2^{0.99n}}\). (Again, we are leaving out an additional variable, \( {\epsilon}\).)</p>
<p>If we rely on this version of Gap-SETH, then our current techniques seem to get stuck at proving hardness of approximation for, say, \( {\gamma}\)-approximate \( {{\mathrm{CVP}}_p}\) for some non-explicit constant \( {\gamma_p &gt; 1}\) (and, if one works out the numbers, one can see immediately that \( {\gamma_p}\) must be really quite close to one). However, other Gap-\(k\)-CSPs are known to be (\(\mathsf{NP}\)-)hard to approximate up to much better approximation factors. E.g., for any \( {k}\), Gap-\(k\)-Parity is \( {{\mathsf{NP}}}\)-hard to approximate up to any constant approximation factor \( {1/2 &lt; \delta \leq 1}\) [<a href="http://kiosk.nada.kth.se/theory/projects/publications/optimaljh.pdf">Hås01</a>], and Gap-\( {k}\)-AND is \( {{\mathsf{NP}}}\)-hard to approximate for any constant approximation factor \( {\Omega(k/2^k) \leq \delta \leq 1}\) [<a href="https://eccc.weizmann.ac.il/report/2012/110/">Cha16</a>]. Indeed, Gap-\( {k}\)-AND is a quite natural problem to consider in this context since there is a fine-grained, approximation-factor preserving reduction from any Gap-\( {k}\)-CSP to Gap-\( {k}\)-AND. This generality motivates understanding the precise complexity of Gap-\( {k}\)-AND.</p>
<blockquote>
<p><b>Open problem 10.</b> What is the fine-grained complexity of the \( {\delta}\)-Gap-\( {k}\)-AND problem in terms of \( {n}\), \( {k}\), and \( {\delta}\)? In particular, if</p>
<p align="center">\( \displaystyle C_{k,\delta} := \inf \{ C &gt; 0 \ : \ \text{there is a $2^{C_{k,\delta}}$-time algorithm for algorithm for $\delta$-Gap-$k$-AND}\}\)</p>
<p>then what is the behavior of \( {C_{k,\delta}}\) as \( {k \rightarrow \infty}\) (for various functions \( {\delta = \delta(k)}\) of \( {k}\))?</p>
</blockquote>
<p>In particular, if one were to hypothesize sufficiently strong hardness of \( {\delta}\)-Gap-\( {k}\)-AND — i.e., to define an appropriate variant of Gap-SETH based on Gap-\( {k}\)-AND — then one might be able to use this hypothesis to prove very strong fine-grained hardness of approximation results. There is a fine-grained (but non-approximation preserving) reduction from Gap-\( {k}\)-AND to Gap-\( {k}\)-SAT, and so Manurangsi’s Gap-SETH is equivalent to the conjecture that there exists some non-explicit \( {\delta(k)}\) such that \( {\lim_{k \rightarrow \infty} C_{k,\delta} = 1}\).</p>


<ul><li>[<a href="https://arxiv.org/abs/1911.02440">ABGS20</a>] Aggarwal, Bennett, Golovnev, Stephens-Davidowitz. Fine-grained hardness of CVP(P)— Everything that we can prove (and nothing else)</li><li>[<a href="https://estimate-all-the-lwe-ntru-schemes.github.io/paper.pdf">A+18</a>] Albrecht, Curtis, Deo, Davidson, Player, Postlethwaite, Virdia, Wunderer. Estimate all the {LWE, NTRU} schemes! <em>SCN</em>, 2019.</li><li>[<a href="https://arxiv.org/abs/1412.7994">ADRS15</a>] Aggarwal, Dadush, Regev, Stephens-Davidowitz. Solving the Shortest Vector Problem in \(2^n\) time via discrete Gaussian sampling. <em>STOC</em>, 2015.</li><li>[<a href="https://arxiv.org/abs/1504.01995">ADS15</a>]  Aggarwal, Dadush, Stephens-Davidowitz. Solving the Closest Vector Problem in \(2^n\) time–The discrete Gaussian strikes again! <em>FOCS</em>, 2015.</li><li>[<a href="https://arxiv.org/pdf/1801.02358">AM18</a>] Aggarwal, Mukhopadhyay. Faster algorithms for SVP and CVP in the \(\ell_\infty\) norm. <em>ISAAC</em>, 2018.</li><li>[<a href="https://arxiv.org/abs/1709.01535">AS18a</a>] Aggarwal, Stephens-Davidowitz. Just take the average! An embarrassingly simple \(2^n\)-time algorithm for SVP (and CVP). <em>SOSA</em>, 2018.</li><li>[<a href="https://arxiv.org/abs/1712.00942">AS18b</a>] Aggarwal, Stephens-Davidowitz. (Gap/S)ETH hardness of SVP. <em>STOC</em>, 2018.</li><li>[<a href="https://eprint.iacr.org/2016/659">B+16</a>] Bos, Costello, Ducas, Mironov, Naehrig, Nikolaenko, Raghunathan, Stebila. Frodo: Take off the ring! Practical, Quantum-Secure Key Exchange from LWE. <em>CCS,</em> 2016.</li><li>[<a href="https://eprint.iacr.org/2015/1128">BDGL16</a>] Becker, Ducas, Gama, Laarhoven. New directions in nearest neighbor searching with applications to lattice sieving. <em>SODA</em>, 2016.</li><li>[<a href="https://arxiv.org/abs/1704.03928">BGS17</a>] Bennett, Golovnev, Stephens-Davidowitz. On the quantitative hardness of CVP. <em>FOCS</em>, 2017.</li><li>[<a href="https://eccc.weizmann.ac.il/report/2012/110/">Cha16</a>] Chan. Approximation resistance from pairwise-independent subgroups. <em>J. ACM</em>, 2016.</li><li>[<a href="https://eccc.weizmann.ac.il/report/2016/128/">Din16</a>] Dinur. Mildly exponential reduction from gap 3SAT to polynomial-gap label-cover.</li><li>[<a href="http://www.wisdom.weizmann.ac.il/~dinuri/mypapers/cvpjournal.pdf">DKRS03</a>] Dinur, Kindler, Raz, Safra. Approximating CVP to within almost-polynomial factors is NP-hard. <em>Combinatorica</em>, 2003.</li><li>[<a href="https://arxiv.org/abs/1011.5666">DPV11</a>] Dadush, Peikert, Vempala. Enumerative lattice algorithms in any norm via \(M\)-ellipsoid coverings. <em>FOCS</em>, 2011.</li><li>[<a href="https://cseweb.ucsd.edu/~daniele/papers/GMSS.pdf">GMSS99</a>] Goldreich, Micciancio, Safra, Seifert. Approximating shortest lattice vectors is not harder than approximating closest lattice vectors. <em>IPL</em>, 1999.</li><li>[<a href="http://kiosk.nada.kth.se/theory/projects/publications/optimaljh.pdf">Hås01</a>] Håstad. Some optimal inapproximability results. <em>J. ACM</em>, 2001.</li><li>[<a href="https://arxiv.org/abs/1806.04087">HR12</a>] Haviv, Regev. Tensor-based hardness of the Shortest Vector Problem to within almost polynomial factors. <em>TOC</em>, 2012.</li><li>[<a href="https://cseweb.ucsd.edu/~paturi/myPapers/pubs/ImpagliazzoPaturi_2001_jcss.pdf">IP01</a>] Impagliazzo, Paturi. On the complexity of \(k\)-SAT. <em>JCSS</em>, 2001.</li><li>[<a href="https://cseweb.ucsd.edu/~russell/ipz.pdf">IPZ01</a>] Impagliazzo, Paturi, Zane. Which problems have strongly exponential complexity? <em>JCSS</em>, 2001.</li><li>[<a href="http://www.thijs.com/docs/phd-final.pdf">Laa15</a>] Laarhoven. Search problems in cryptography. Ph.D thesis, 2015.</li><li>[<a href="https://kilthub.cmu.edu/articles/Minkowski_s_convex_body_theorem_and_integer_programming/6607328/files/12097865.pdf">Kan87</a>] Kannan. Minkowski’s convex body theorem and Integer Programming. <em>MOR</em>, 1987.</li><li>[<a href="https://eprint.iacr.org/2019/1016">KMPR19</a>] Kirshanova, Mårtensson, Postlethwaite, Roy Moulik. Quantum algorithms for the approximate \(k\)-list problem and their application to lattice sieving. <em>Asiacrypt</em>, 2019.</li><li>[<a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2019/EECS-2019-49.html">Man19</a>] Manurangsi. Approximation and Hardness: Beyond P and NP.</li><li>[<a href="https://arxiv.org/pdf/1607.02986.pdf">MR17</a>] Manurangsi, Raghavendra. A Birthday Repetition Theorem and Complexity of Approximating Dense CSPs. <em>ICALP</em>, 17.</li><li>[<a href="https://arxiv.org/abs/1802.00886">Vlă19</a>] Vlăduţ. Lattices with exponentially large kissing numbers. <em>Moscow J. of Combinatorics and Number Theory</em>, 2019<em>.</em></li></ul></div>
    </content>
    <updated>2020-05-04T01:07:28Z</updated>
    <published>2020-05-04T01:07:28Z</published>
    <category term="General"/>
    <author>
      <name>Huck Bennett</name>
    </author>
    <source>
      <id>https://blog.simons.berkeley.edu</id>
      <link href="https://blog.simons.berkeley.edu/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://blog.simons.berkeley.edu" rel="alternate" type="text/html"/>
      <subtitle>What's New at the Simons Institute for the Theory of Computing.</subtitle>
      <title>Calvin Café: The Simons Institute Blog</title>
      <updated>2020-05-08T22:45:28Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/05/03/hanoi-vs-sierpinski</id>
    <link href="https://11011110.github.io/blog/2020/05/03/hanoi-vs-sierpinski.html" rel="alternate" type="text/html"/>
    <title>Hanoi vs Sierpiński</title>
    <summary>The Hanoi graphs and Sierpiński graphs both look like the Sierpiński triangle, and have a very similar recursive construction from triples of smaller graphs of the same type, but they are not quite the same graphs as each other. The Sierpiński graphs (left, below) are the graphs of the vertices and boundary edges of partially-constructed Sierpiński triangles; they can also be formed from three smaller Sierpiński graphs by identifying pairs of extreme vertices (the vertices of degree two at the three corners of the triangular layout). The Hanoi graphs (right, below) are the state spaces of the tower of Hanoi puzzle, in which rings of different size are moved one at a time between three pegs, only allowing moves that keep the rings sorted on each peg. They also have a construction from three smaller Hanoi graphs, but where the pairs of extreme vertices are connected by an edge rather than identified.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Hanoi graphs and Sierpiński graphs both look like the <a href="https://en.wikipedia.org/wiki/Sierpi%C5%84ski_triangle">Sierpiński triangle</a>, and have a very similar recursive construction from triples of smaller graphs of the same type, but they are not quite the same graphs as each other. The Sierpiński graphs (left, below) are the graphs of the vertices and boundary edges of partially-constructed Sierpiński triangles; they can also be formed from three smaller Sierpiński graphs by identifying pairs of extreme vertices (the vertices of degree two at the three corners of the triangular layout). The <a href="https://en.wikipedia.org/wiki/Hanoi_graph">Hanoi graphs</a> (right, below) are the state spaces of the tower of Hanoi puzzle, in which rings of different size are moved one at a time between three pegs, only allowing moves that keep the rings sorted on each peg. They also have a construction from three smaller Hanoi graphs, but where the pairs of extreme vertices are connected by an edge rather than identified.</p>

<p style="text-align: center;"><img alt="Hanoi graphs and Sierpi&#x144;ski graphs" src="https://11011110.github.io/blog/assets/2020/hanoi-vs-sierpinski.svg"/></p>

<p>The difference between them comes out much more strongly when you generalize them to higher dimensions. The Sierpiński triangle generalizes to tetrahedra (a popular shape for kites) and higher-dimensional simplices; <a href="https://commons.wikimedia.org/wiki/File:Alexander_Graham_Bell_facing_his_wife,_Mabel_Hubbard_Gardiner_Bell,_who_is_standing_in_a_tetrahedral_kite,_Baddeck,_Nova_Scotia.tif">the photo below</a> shows <a href="https://en.wikipedia.org/wiki/Mabel_Gardiner_Hubbard">Mabel Bell</a> and <a href="https://en.wikipedia.org/wiki/Alexander_Graham_Bell">Alexander Graham Bell</a>, seemingly about to kiss, in a three-dimensional Sierpiński graph, the framework for a kite.</p>

<p style="text-align: center;"><img alt="Mabel Bell and Alexander Graham Bell kissing in a Sierpi&#x144;ski tetrahedron kite frame, from https://commons.wikimedia.org/wiki/File:Alexander_Graham_Bell_facing_his_wife,_Mabel_Hubbard_Gardiner_Bell,_who_is_standing_in_a_tetrahedral_kite,_Baddeck,_Nova_Scotia.tif" src="https://11011110.github.io/blog/assets/2020/bell-kite-kiss.jpg"/></p>

<p>Again, the -dimensional Sierpiński graph has a recursive construction from  smaller graphs of the same type, identified at extreme vertices (the vertices of degree  at the  corners of the layout). Because the number of vertices separating the subgraphs at each level of the recursion is so small, these graphs have bounded <a href="https://en.wikipedia.org/wiki/Treewidth">treewidth</a>, and a few years ago on the TCS stackexchange <a href="https://cstheory.stackexchange.com/q/36542">I calculated the treewidth of the Sierpiński triangle graphs explicitly as being four</a>. The same bound transfers easily enough to the three-peg Hanoi graphs.</p>

<p>The analogue of higher dimensions for the Hanoi graphs is to use more pegs. The Hanoi graph with  pegs and  rings has  states, more or less the same as the Sierpiński graph for -dimensional Sierpiński fractals with  levels of recursion. Here’s the one with two rings; each state is described by a pair of letters, using a capital letter for the peg holding the larger ring and a lowercase letter for the peg holding the smaller ring.</p>

<p style="text-align: center;"><img alt="Hanoi graph for two rings on four pegs" src="https://11011110.github.io/blog/assets/2020/Hanoi-4-2.svg"/></p>

<p>The recursive construction for these graphs combines  copies of a smaller graph of the same type: one copy for each position where the largest ring can be placed, and a smaller graph describing the placements of the smaller rings once the largest ring has been placed. These copies of the smaller graph are connected together by edges describing the movements of the largest ring. But I’ve only drawn an example for two rings because these graphs get messy and hard to draw very quickly. The reason is not the exponential number of total vertices, but the large number of connections from one recursive subgraph to another. Two recursive subgraphs are connected whenever the largest ring can move from its peg in one subgraph to its peg in the other, and this is allowed whenever these two pegs have no smaller rings on them. So in a Hanoi graph with  pegs,  rings, and  vertices, each pair of recursive subgraphs has  edges between them, one for each placement of the smaller rings on the  remaining pegs.</p>

<p>The recursive subdivision with  edges between subgraphs leads to a tree-decomposition with treewidth , and this naturally raises the question of whether this is tight or whether some other less-intuitive recursive decomposition has smaller cuts between its recursive subgraphs. This is the question studied in my newest preprint, “On the treewidth of Hanoi graphs” (<a href="https://arxiv.org/abs/2005.00179">arXiv:2005.00179</a>), with UCI student Daniel Frishberg and Oregon State student Will Maxwell, to appear at <a href="https://sites.google.com/view/fun2020/home">FUN 2020</a> (supposedly to be held in person in September in Italy after being rescheduled from June, but I’m not holding my breath). We don’t get a precise answer, but we succeed in proving bounds on the treewidth of the form <span style="white-space: nowrap;">.</span> This is nearly tight for fixed  and variable : we get the same exponential function of  as the upper bound, and are smaller than the upper bound by only a much lower-order polynomial factor. But the exact treewidth remains elusive.</p>

<p>To put it in a possibly more familiar form, when one of these graphs (for a fixed number of pegs and variable number of rings) has  vertices, it has separators of size , where . For the four-peg Hanoi graphs, this means separators of size , more or less the same as for planar graphs (although these graphs seem far from planar). But that nice exponent is just a coincidence caused by the fact that  is a power of . For other choices of , that doesn’t happen and we get a transcendental exponent . So these graphs don’t even act like -dimensional graphs, for which a reasonable separator exponent might be the rational number . And they certainly don’t act like the Sierpiński graphs, for which the exponent is zero.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/104108481482094736">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-05-03T22:03:00Z</updated>
    <published>2020-05-03T22:03:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-05-04T05:19:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/069</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/069" rel="alternate" type="text/html"/>
    <title>TR20-069 |  Optimal Error Pseudodistributions for Read-Once Branching Programs | 

	Eshan Chattopadhyay, 

	Jyun-Jie Liao</title>
    <summary>In a seminal work, Nisan (Combinatorica'92) constructed a pseudorandom generator for length $n$ and width $w$ read-once branching programs  with seed length $O(\log n\cdot \log(nw)+\log n\cdot\log(1/\varepsilon))$ and  error $\varepsilon$. It remains a central question  to reduce the seed length to $O(\log (nw/\varepsilon))$, which would prove that $\mathbf{BPL}=\mathbf{L}$. However, there has been no improvement on Nisan's construction for the case $n=w$, which is most relevant to space-bounded derandomization.




Recently, in a beautiful work, Braverman, Cohen and Garg (STOC'18) introduced the notion of a \emph{pseudorandom pseudo-distribution} (PRPD) and gave an explicit construction of a PRPD with seed length $\tilde{O}(\log n\cdot \log(nw)+\log(1/\varepsilon))$. A PRPD is a relaxation of a pseudorandom generator, which suffices for derandomizing $\mathbf{BPL}$ and also implies a hitting set. Unfortunately, their construction is quite involved and complicated. Hoza and Zuckerman (FOCS'18) later constructed a much simpler hitting set generator with seed length $O(\log n\cdot \log(nw)+\log(1/\varepsilon))$, but their techniques are restricted to hitting sets.

In this work, we construct a PRPD with seed length 
$$O(\log n\cdot \log (nw)\cdot \log\log(nw)+\log(1/\varepsilon)).$$
This improves upon the construction in \cite{BCG18} by a $O(\log\log(1/\varepsilon))$ factor, and is optimal in the small error regime. In addition, we believe our construction and analysis to be   simpler than the work of Braverman, Cohen and Garg.</summary>
    <updated>2020-05-03T22:01:57Z</updated>
    <published>2020-05-03T22:01:57Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-09T06:20:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/068</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/068" rel="alternate" type="text/html"/>
    <title>TR20-068 |  One-Sided Error Testing of Monomials and Affine Subspaces | 

	Oded Goldreich, 

	Dana Ron</title>
    <summary>We consider the query complexity of three versions of the problem of testing monomials and affine (and linear) subspaces with one-sided error, and obtain the following results: 
\begin{itemize}
\item The general problem, in which the arity of the monomial (resp., co-dimension of the subspace) is not specified, has query complexity ${\widetilde{O}}(1/\epsilon)$, where $\epsilon$ denotes the proximity parameter. 
\item The bounded problem, in which the arity of the monomial (resp., co-dimension of the subspace) is upper bounded by a fixed parameter, has query complexity ${\widetilde{O}}(1/\epsilon)$.
\item The exact problem, in which the arity of the monomial (resp., co-dimension of the subspace) is required to equal a fixed parameter (e.g., equals~2), has query complexity ${\widetilde{\Omega}}(\log n)$, where $n$ denotes the length of the argument for the tested function.  
\end{itemize}
The running time of the testers in the positive results is linear in their query complexity.</summary>
    <updated>2020-05-03T20:04:44Z</updated>
    <published>2020-05-03T20:04:44Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-09T06:20:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4379</id>
    <link href="https://lucatrevisan.wordpress.com/2020/05/03/talagrands-generic-chaining/" rel="alternate" type="text/html"/>
    <title>Talagrand’s Generic Chaining</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Welcome to phase two of in theory, in which we again talk about math. I spent last Fall teaching two courses and getting settled, I mostly traveled in January and February, and I have spent the last two months on … <a href="https://lucatrevisan.wordpress.com/2020/05/03/talagrands-generic-chaining/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
 Welcome to phase two of <em>in theory</em>, in which we again talk about math. I spent last Fall teaching two courses and getting settled, I mostly traveled in January and February, and I have spent the last two months on my sofa catching up on TV series. Hence I will reach back to last Spring, when I learned about Talagrand’s machinery of generic chaining and majorizing measures from Nikhil Bansal, in the context of our work with Ola Svensson on <a href="https://arxiv.org/abs/1905.01495">graph and hypergraph sparsification</a>. Here I would like to record what I understood about the machinery, and in a follow-up post I plan to explain the application to hypergraph sparsification.</p>
<p>
<span id="more-4379"/></p>
<p>
</p><p><b>1. A Concrete Setting </b></p>
<p/><p>
Starting from a very concrete setting, suppose that we have a subset <img alt="{T \subseteq {\mathbb R}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT+%5Csubseteq+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T \subseteq {\mathbb R}^n}"/>, we pick a random Gaussian vector <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> from <img alt="{N({\bf 0}, I)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%28%7B%5Cbf+0%7D%2C+I%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N({\bf 0}, I)}"/>, and we are interested in the random variable</p>
<p>
<a name="eqg"/></p><a name="eqg">
<p align="center"><img alt="\displaystyle   \sup_{t\in T}\ \langle g, t \rangle \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Csup_%7Bt%5Cin+T%7D%5C+%5Clangle+g%2C+t+%5Crangle+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle   \sup_{t\in T}\ \langle g, t \rangle \ \ \ \ \ (1)"/></p>
</a><p><a name="eqg"/> In theoretical computer science, for example, a random variable like <a href="https://lucatrevisan.wordpress.com/feed/#eqg">(1)</a> comes up often in the study of rounding algorithms for semidefinite programming, but this is a problem of much broader interest. </p>
<p>
We will be interested both in bounds on the expectations of <a href="https://lucatrevisan.wordpress.com/feed/#eqg">(1)</a> and on its tail, but in this post we will mostly reason about its expectation. </p>
<p>
A first observation is that each <img alt="{\langle g,t \rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+g%2Ct+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\langle g,t \rangle}"/> is Gaussian with mean zero and variance <img alt="{|| t||^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C+t%7C%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|| t||^2}"/>. If <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> is finite, we can use a union bound to estimate the tail of <img alt="{\sup_{t\in T} \langle t,g \rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csup_%7Bt%5Cin+T%7D+%5Clangle+t%2Cg+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sup_{t\in T} \langle t,g \rangle}"/> as</p>
<p/><p align="center"><img alt="\displaystyle  \Pr \left[ \sup_{t\in T}\ \langle g, t \rangle &gt; \ell \right] \leq |T| \cdot e^{-\ell^2 / 2 \sup_{t\in T} ||t||^2} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPr+%5Cleft%5B+%5Csup_%7Bt%5Cin+T%7D%5C+%5Clangle+g%2C+t+%5Crangle+%3E+%5Cell+%5Cright%5D+%5Cleq+%7CT%7C+%5Ccdot+e%5E%7B-%5Cell%5E2+%2F+2+%5Csup_%7Bt%5Cin+T%7D+%7C%7Ct%7C%7C%5E2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \Pr \left[ \sup_{t\in T}\ \langle g, t \rangle &gt; \ell \right] \leq |T| \cdot e^{-\ell^2 / 2 \sup_{t\in T} ||t||^2} "/></p>
<p> and we can compute the upper bound <a name="equb"/></p><a name="equb">
<p align="center"><img alt="\displaystyle  \mathop{\mathbb E}_{g\sim N({\bf 0}, I)} \ \ \sup_{t\in T}\ \langle g, t \rangle \leq O\left(\sqrt{\log |T|} \cdot \sup_{t\in T} || t|| \right) \ \ \ \ \ (2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D_%7Bg%5Csim+N%28%7B%5Cbf+0%7D%2C+I%29%7D+%5C+%5C+%5Csup_%7Bt%5Cin+T%7D%5C+%5Clangle+g%2C+t+%5Crangle+%5Cleq+O%5Cleft%28%5Csqrt%7B%5Clog+%7CT%7C%7D+%5Ccdot+%5Csup_%7Bt%5Cin+T%7D+%7C%7C+t%7C%7C+%5Cright%29+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E}_{g\sim N({\bf 0}, I)} \ \ \sup_{t\in T}\ \langle g, t \rangle \leq O\left(\sqrt{\log |T|} \cdot \sup_{t\in T} || t|| \right) \ \ \ \ \ (2)"/></p>
</a><p><a name="equb"/> The above bound can be tight, but it is poor if the points of <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> are densely clustered, and it is useless if <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> is infinite. </p>
<p>
It is useful to note that, if we fix, arbitrarily, an element <img alt="{t_0 \in T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt_0+%5Cin+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t_0 \in T}"/>, then we have <a name="eqgtzero"/></p><a name="eqgtzero">
<p align="center"><img alt="\displaystyle   \mathop{\mathbb E}_{g\sim N({\bf 0}, I)} \ \ \sup_{t\in T}\ \langle g, t \rangle = \mathop{\mathbb E}_{g\sim N({\bf 0}, I)} \ \ \sup_{t\in T}\ \langle g, t - t_0 \rangle \ \ \ \ \ (3)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+++%5Cmathop%7B%5Cmathbb+E%7D_%7Bg%5Csim+N%28%7B%5Cbf+0%7D%2C+I%29%7D+%5C+%5C+%5Csup_%7Bt%5Cin+T%7D%5C+%5Clangle+g%2C+t+%5Crangle+%3D+%5Cmathop%7B%5Cmathbb+E%7D_%7Bg%5Csim+N%28%7B%5Cbf+0%7D%2C+I%29%7D+%5C+%5C+%5Csup_%7Bt%5Cin+T%7D%5C+%5Clangle+g%2C+t+-+t_0+%5Crangle+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle   \mathop{\mathbb E}_{g\sim N({\bf 0}, I)} \ \ \sup_{t\in T}\ \langle g, t \rangle = \mathop{\mathbb E}_{g\sim N({\bf 0}, I)} \ \ \sup_{t\in T}\ \langle g, t - t_0 \rangle \ \ \ \ \ (3)"/></p>
</a><p><a name="eqgtzero"/> because <img alt="{\mathop{\mathbb E} \langle g, t_0 \rangle = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D+%5Clangle+g%2C+t_0+%5Crangle+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathop{\mathbb E} \langle g, t_0 \rangle = 0}"/>. The latter expression is nicer to work with because it makes it more explicit that what we are trying to compute is invariant under shifts of <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>, and only depends on pairwise distances of the elements of <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>, rather than their norm. </p>
<p>
In the cases in which <a href="https://lucatrevisan.wordpress.com/feed/#equb">(2)</a> gives a poor bound, a natural approach is to reason about an <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>-net <img alt="{T'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T'}"/> of <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>, that is, a subset <img alt="{T'\subseteq T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%27%5Csubseteq+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T'\subseteq T}"/> such that for every <img alt="{t\in T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%5Cin+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t\in T}"/> there is an element <img alt="{\pi(t) \in T'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%28t%29+%5Cin+T%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\pi(t) \in T'}"/> such that <img alt="{||t - \pi(t) ||\leq \epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7Ct+-+%5Cpi%28t%29+%7C%7C%5Cleq+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{||t - \pi(t) ||\leq \epsilon}"/>. Then we can say that</p>
<p/><p align="center"><img alt="\displaystyle  \mathop{\mathbb E} \sup_t \langle t - t_0, g \rangle = \mathop{\mathbb E} \sup_{t\in T} \langle t - \pi(t) , g \rangle + \langle \pi(t) -t_0, g \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_t+%5Clangle+t+-+t_0%2C+g+%5Crangle+%3D+%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%5Cin+T%7D+%5Clangle+t+-+%5Cpi%28t%29+%2C+g+%5Crangle+%2B+%5Clangle+%5Cpi%28t%29+-t_0%2C+g+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E} \sup_t \langle t - t_0, g \rangle = \mathop{\mathbb E} \sup_{t\in T} \langle t - \pi(t) , g \rangle + \langle \pi(t) -t_0, g \rangle "/></p>
<p align="center"><img alt="\displaystyle  \leq \left( \mathop{\mathbb E} \sup_{t\in T} \langle t - \pi(t) , g \rangle \right) + \left( \mathop{\mathbb E} \sup_{t\in T}\langle \pi(t) -t_0, g \rangle \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+%5Cleft%28+%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%5Cin+T%7D+%5Clangle+t+-+%5Cpi%28t%29+%2C+g+%5Crangle+%5Cright%29+%2B+%5Cleft%28+%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%5Cin+T%7D%5Clangle+%5Cpi%28t%29+-t_0%2C+g+%5Crangle+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \leq \left( \mathop{\mathbb E} \sup_{t\in T} \langle t - \pi(t) , g \rangle \right) + \left( \mathop{\mathbb E} \sup_{t\in T}\langle \pi(t) -t_0, g \rangle \right) "/></p>
<p align="center"><img alt="\displaystyle  \leq O( \sqrt{\log |T|} ) \cdot \sup_{t\in T} ||t-\pi(t) || + \left( \mathop{\mathbb E} \sup_{t'\in T'}\langle t' -t_0, g \rangle \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+O%28+%5Csqrt%7B%5Clog+%7CT%7C%7D+%29+%5Ccdot+%5Csup_%7Bt%5Cin+T%7D+%7C%7Ct-%5Cpi%28t%29+%7C%7C+%2B+%5Cleft%28+%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%27%5Cin+T%27%7D%5Clangle+t%27+-t_0%2C+g+%5Crangle+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \leq O( \sqrt{\log |T|} ) \cdot \sup_{t\in T} ||t-\pi(t) || + \left( \mathop{\mathbb E} \sup_{t'\in T'}\langle t' -t_0, g \rangle \right) "/></p>
<p align="center"><img alt="\displaystyle  \leq O\left( \sqrt{\log |T|} \cdot \epsilon + \sqrt {\log |T'|} \cdot \sup_{t'\in T'} || t' - t_0|| \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+O%5Cleft%28+%5Csqrt%7B%5Clog+%7CT%7C%7D+%5Ccdot+%5Cepsilon+%2B+%5Csqrt+%7B%5Clog+%7CT%27%7C%7D+%5Ccdot+%5Csup_%7Bt%27%5Cin+T%27%7D+%7C%7C+t%27+-+t_0%7C%7C+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \leq O\left( \sqrt{\log |T|} \cdot \epsilon + \sqrt {\log |T'|} \cdot \sup_{t'\in T'} || t' - t_0|| \right) "/></p>
<p>
which can be a much tighter bound. Notice that we used <a href="https://lucatrevisan.wordpress.com/feed/#equb">(2)</a> to bound </p>
<p align="center"><img alt="\displaystyle  \mathop{\mathbb E} \sup_{t\in T'} \langle t' - t_0 , g\rangle \ , " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%5Cin+T%27%7D+%5Clangle+t%27+-+t_0+%2C+g%5Crangle+%5C+%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E} \sup_{t\in T'} \langle t' - t_0 , g\rangle \ , "/></p>
<p> but it might actually be better to find an <img alt="{\epsilon'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon'}"/>-net <img alt="{T''}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%27%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T''}"/> of <img alt="{T'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T'}"/>, and so on. In general, a tighter analysis would be to choose a sequence of nested sets <img alt="{T_0 \subseteq T_1 \subseteq \cdots \subseteq T_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_0+%5Csubseteq+T_1+%5Csubseteq+%5Ccdots+%5Csubseteq+T_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_0 \subseteq T_1 \subseteq \cdots \subseteq T_k}"/>, where <img alt="{T_0 = \{ t_0 \}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_0+%3D+%5C%7B+t_0+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_0 = \{ t_0 \}}"/>, <img alt="{T_k = T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_k+%3D+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_k = T}"/>, and we have that <img alt="{T_{i-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_%7Bi-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_{i-1}}"/> is an <img alt="{\epsilon_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon_i}"/>-net of <img alt="{T_{i}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_{i}}"/>, that is, for every element <img alt="{t_{i}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t_{i}}"/> of <img alt="{T_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_i}"/> there is an element <img alt="{\pi_i (t_i) \in T_{i-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi_i+%28t_i%29+%5Cin+T_%7Bi-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\pi_i (t_i) \in T_{i-1}}"/> such that <img alt="{|| t_i - \pi_i (t_i) || \leq \epsilon_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C+t_i+-+%5Cpi_i+%28t_i%29+%7C%7C+%5Cleq+%5Cepsilon_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|| t_i - \pi_i (t_i) || \leq \epsilon_i}"/>. Then, by generalizing the above reasoning, we get </p>
<p align="center"><img alt="\displaystyle  \mathop{\mathbb E} \sup_{t\in T} \langle t-t_0,g \rangle \leq O(1) \cdot \sum_{i=1}^k \sqrt{\log |T_i | } \cdot \sup_{t_i \in T_i} || t_i - \pi_i(t_i) || \leq O(1) \cdot \sum_{i=1}^n \epsilon_i \sqrt{\log |T_i | } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%5Cin+T%7D+%5Clangle+t-t_0%2Cg+%5Crangle+%5Cleq+O%281%29+%5Ccdot+%5Csum_%7Bi%3D1%7D%5Ek+%5Csqrt%7B%5Clog+%7CT_i+%7C+%7D+%5Ccdot+%5Csup_%7Bt_i+%5Cin+T_i%7D+%7C%7C+t_i+-+%5Cpi_i%28t_i%29+%7C%7C+%5Cleq+O%281%29+%5Ccdot+%5Csum_%7Bi%3D1%7D%5En+%5Cepsilon_i+%5Csqrt%7B%5Clog+%7CT_i+%7C+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E} \sup_{t\in T} \langle t-t_0,g \rangle \leq O(1) \cdot \sum_{i=1}^k \sqrt{\log |T_i | } \cdot \sup_{t_i \in T_i} || t_i - \pi_i(t_i) || \leq O(1) \cdot \sum_{i=1}^n \epsilon_i \sqrt{\log |T_i | } "/></p>
<p>
Finally, if the cardinality of the <img alt="{T_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_i}"/> grows sufficiently fast, namely, if we have <img alt="{|T_i| &gt; |T_{i-1}|^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CT_i%7C+%3E+%7CT_%7Bi-1%7D%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|T_i| &gt; |T_{i-1}|^2}"/>, it is possible to refine the estimate to </p>
<p align="center"><img alt="\displaystyle  \mathop{\mathbb E} \sup_{t\in T} \langle t-t_0,g \rangle \leq O(1) \cdot \sup_{t\in T} \sum_{i=1}^k \sqrt{\log |T_i | } \cdot || t - \pi_i (t) || " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%5Cin+T%7D+%5Clangle+t-t_0%2Cg+%5Crangle+%5Cleq+O%281%29+%5Ccdot+%5Csup_%7Bt%5Cin+T%7D+%5Csum_%7Bi%3D1%7D%5Ek+%5Csqrt%7B%5Clog+%7CT_i+%7C+%7D+%5Ccdot+%7C%7C+t+-+%5Cpi_i+%28t%29+%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E} \sup_{t\in T} \langle t-t_0,g \rangle \leq O(1) \cdot \sup_{t\in T} \sum_{i=1}^k \sqrt{\log |T_i | } \cdot || t - \pi_i (t) || "/></p>
<p> where <img alt="{\pi_i(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi_i%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\pi_i(t)}"/> is the closest element to <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> in <img alt="{T_{i-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_%7Bi-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_{i-1}}"/>. This is done by avoiding to write the expectation of the sup of a sum as a sum of expectations of sups and then using <a href="https://lucatrevisan.wordpress.com/feed/#equb">(2)</a>, but by bounding the tail of the sup of the sum directly.</p>
<p>
At this point, we do not even need to assume that <img alt="{T_k= T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_k%3D+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_k= T}"/>, that <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> is finite, or that the sequence of sets is finite, and we have the following result.</p>
<blockquote><p><b>Theorem 1 (Talagrand’s generic chaining inequality)</b> <em> Let <img alt="{T \subseteq {\mathbb R}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT+%5Csubseteq+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T \subseteq {\mathbb R}^n}"/> be an arbitrary set, let <img alt="{T_0 \subseteq T_1 \subseteq \cdots T_k \subseteq \cdots \subseteq T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_0+%5Csubseteq+T_1+%5Csubseteq+%5Ccdots+T_k+%5Csubseteq+%5Ccdots+%5Csubseteq+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_0 \subseteq T_1 \subseteq \cdots T_k \subseteq \cdots \subseteq T}"/> be a countable sequence of finite subsets of <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> such that <img alt="{|T_0| = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CT_0%7C+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|T_0| = 1}"/> and <img alt="{|T_k| \leq 2^{2^k}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CT_k%7C+%5Cleq+2%5E%7B2%5Ek%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|T_k| \leq 2^{2^k}}"/>. Then </em></p><em>
<p align="center"><img alt="\displaystyle  \mathop{\mathbb E} \sup_{t\in T} \langle t, g \rangle \leq O(1) \cdot \sup_{t\in T} \sum_{k=1}^\infty 2^{k/2} || t - \pi_k(t)|| " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%5Cin+T%7D+%5Clangle+t%2C+g+%5Crangle+%5Cleq+O%281%29+%5Ccdot+%5Csup_%7Bt%5Cin+T%7D+%5Csum_%7Bk%3D1%7D%5E%5Cinfty+2%5E%7Bk%2F2%7D+%7C%7C+t+-+%5Cpi_k%28t%29%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E} \sup_{t\in T} \langle t, g \rangle \leq O(1) \cdot \sup_{t\in T} \sum_{k=1}^\infty 2^{k/2} || t - \pi_k(t)|| "/></p>
</em><p><em> where <img alt="{\pi_k(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi_k%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\pi_k(t)}"/> is the element of <img alt="{T_{k-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_%7Bk-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_{k-1}}"/> closest to <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>. </em></p></blockquote>
<p/><p>
A short complete proof is in these <a href="https://tcsmath.wordpress.com/2010/06/15/the-generic-chaining/">notes by James Lee</a>.</p>
<p>
While the above Theorem has a very simple proof, the amazing thing, which is rather harder to prove, is that it is <em>tight</em>, in the sense that for every <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> there is a sequence of sets such that the bound of the above theorem has a matching lower bound, up to an absolute constant. This is why it is called <em>generic chaining</em>. <em>Chaining</em> because the projection <img alt="{\langle t-t_0,g\rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clangle+t-t_0%2Cg%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\langle t-t_0,g\rangle}"/> of <img alt="{t-t_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt-t_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t-t_0}"/> on <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> is estimated based on the “chain” </p>
<p align="center"><img alt="\displaystyle  \sum_k \langle \pi_{k+1} (t) - \pi_k(t) , g \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_k+%5Clangle+%5Cpi_%7Bk%2B1%7D+%28t%29+-+%5Cpi_k%28t%29+%2C+g+%5Crangle&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_k \langle \pi_{k+1} (t) - \pi_k(t) , g \rangle"/></p>
<p> of projections of the intermediate steps of a path that goes from <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> to <img alt="{t_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t_0}"/> passing through the <img alt="{\pi_k(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi_k%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\pi_k(t)}"/>. <em>Generic</em> because this upper bound technique works as well as any other possible upper bound, up to an absolute constant.</p>
<p>
</p><p><b>2. An Abstract Setting </b></p>
<p/><p>
Let now <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> be a completely arbitrary set, and suppose that we have a distribution <img alt="{\cal D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\cal D}"/> over functions <img alt="{f: T \rightarrow {\mathbb R}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%3A+T+%5Crightarrow+%7B%5Cmathbb+R%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f: T \rightarrow {\mathbb R}}"/> and we want to upper bound </p>
<p align="center"><img alt="\displaystyle  \mathop{\mathbb E}_{F \sim {\cal D}} \ \ \sup_{t\in T} \ F(t) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D_%7BF+%5Csim+%7B%5Ccal+D%7D%7D+%5C+%5C+%5Csup_%7Bt%5Cin+T%7D+%5C+F%28t%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E}_{F \sim {\cal D}} \ \ \sup_{t\in T} \ F(t) "/></p>
<p> That is, we have a random optimization problem with a fixed feasible set <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>, and we want to know the typical value of the optimum. For example, <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> could be the set of cuts of a vertex set <img alt="{V}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BV%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{V}"/>, and <img alt="{\cal D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\cal D}"/> describe a distribution of random graphs <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> such that <img alt="{F(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(t)}"/> is the number of edges cut in a random graph by the cut <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>. Then the above problem is to estimate the average value of the max cut in the random graphs of the distribution. Or <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> could be the unit sphere <img alt="{\{ x \in {\mathbb R}^N : ||x|| = 1 \}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+x+%5Cin+%7B%5Cmathbb+R%7D%5EN+%3A+%7C%7Cx%7C%7C+%3D+1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ x \in {\mathbb R}^N : ||x|| = 1 \}}"/> and <img alt="{\cal D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\cal D}"/> describe a distribution of random Hermitian matrices such that <img alt="{F(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(t)}"/> is the quadratic form of a random matrix evaluated at <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>. In this case, the above problem is to estimate the average value of the largest eigenvalue of such a random matrix.</p>
<p>
We will call the collection of random variables <img alt="{\{ F(t) \}_{t\in T}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ F(t) \}_{t\in T}}"/> a <em>random process</em>, where <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> is a random variable distributed according to <img alt="{\cal D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\cal D}"/>.</p>
<p>
If every <img alt="{F(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(t)}"/>, and every finite linear combination <img alt="{\sum_{i=1}^k \alpha_i F(t_i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5Ek+%5Calpha_i+F%28t_i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_{i=1}^k \alpha_i F(t_i)}"/>, has a Gaussian distribution, then we say that <img alt="{\{ F(t) \}_{t\in T}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ F(t) \}_{t\in T}}"/> is a <em>Gaussian process</em>, and if, in addition, <img alt="{\mathop{\mathbb E} F(t) = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D+F%28t%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathop{\mathbb E} F(t) = 0}"/> for every <img alt="{t\in T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%5Cin+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t\in T}"/> then we say that it is a <em>centered Gaussian process</em>.</p>
<p>
If <img alt="{T\subseteq {\mathbb R}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%5Csubseteq+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T\subseteq {\mathbb R}^n}"/>, and we define <img alt="{F(t) = \langle t,g\rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28t%29+%3D+%5Clangle+t%2Cg%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(t) = \langle t,g\rangle}"/> for a random standard Gaussian <img alt="{g\sim N({\bf 0},I)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%5Csim+N%28%7B%5Cbf+0%7D%2CI%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g\sim N({\bf 0},I)}"/>, then <img alt="{\{ F(t) \}_{t\in T}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ F(t) \}_{t\in T}}"/> is a centered Gaussian process and, in this case, upper bounding <img alt="{\mathop{\mathbb E} \sup F(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D+%5Csup+F%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathop{\mathbb E} \sup F(t)}"/> is precisely the problem we studied before. </p>
<p>
If <img alt="{T\subseteq {\mathbb R}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%5Csubseteq+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T\subseteq {\mathbb R}^n}"/> and <img alt="{F(t) = \langle t,g \rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28t%29+%3D+%5Clangle+t%2Cg+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(t) = \langle t,g \rangle}"/> for a random standard Gaussian <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/>, then, for every <img alt="{t_1,t_2 \in T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt_1%2Ct_2+%5Cin+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t_1,t_2 \in T}"/>, we have</p>
<p/><p align="center"><img alt="\displaystyle  \mathop{\mathbb E} \left(F(t_1) - F(t_2)\right)^2 = \mathop{\mathbb E}_{g \sim N({\bf 0},I)} \ \ \langle t_1-t_2,g \rangle^2 = ||t_1 - t_2 ||^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Cleft%28F%28t_1%29+-+F%28t_2%29%5Cright%29%5E2+%3D+%5Cmathop%7B%5Cmathbb+E%7D_%7Bg+%5Csim+N%28%7B%5Cbf+0%7D%2CI%29%7D+%5C+%5C+%5Clangle+t_1-t_2%2Cg+%5Crangle%5E2+%3D+%7C%7Ct_1+-+t_2+%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E} \left(F(t_1) - F(t_2)\right)^2 = \mathop{\mathbb E}_{g \sim N({\bf 0},I)} \ \ \langle t_1-t_2,g \rangle^2 = ||t_1 - t_2 ||^2 "/></p>
<p> and, by analogy, if <img alt="{\{ F(t) \}_{t\in T}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ F(t) \}_{t\in T}}"/> is a centered Gaussian process, we will define the following distance function on <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>: </p>
<p align="center"><img alt="\displaystyle  d(t_1,t_2 ) := \sqrt{ \mathop{\mathbb E} (F(t_1) - F(t_2))^2} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++d%28t_1%2Ct_2+%29+%3A%3D+%5Csqrt%7B+%5Cmathop%7B%5Cmathbb+E%7D+%28F%28t_1%29+-+F%28t_2%29%29%5E2%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  d(t_1,t_2 ) := \sqrt{ \mathop{\mathbb E} (F(t_1) - F(t_2))^2} "/></p>
<p> If <img alt="{\{ F(t) \}_{t\in T}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ F(t) \}_{t\in T}}"/> is a centered Gaussian process then one can prove that the above distance function is a semi-metric on <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>.</p>
<p>
We will not need this fact, but if <img alt="{\{ F(t) \}_{t\in T}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ F(t) \}_{t\in T}}"/> is a centered Gaussian process and <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> is finite, then there is an embedding <img alt="{h: T \rightarrow {\mathbb R}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%3A+T+%5Crightarrow+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h: T \rightarrow {\mathbb R}^n}"/>, for some <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>, such that the process can be equivalently defined as picking <img alt="{g\sim N({\bf 0} , I)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%5Csim+N%28%7B%5Cbf+0%7D+%2C+I%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g\sim N({\bf 0} , I)}"/> and setting <img alt="{F(t) := \langle h(t), g \rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28t%29+%3A%3D+%5Clangle+h%28t%29%2C+g+%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(t) := \langle h(t), g \rangle}"/>, so that <img alt="{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h}"/> is also an isometric embedding of the above distance function into <img alt="{\ell_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_2}"/>.</p>
<p>
The arguments of the previous section apply to centered Gaussian processes without change, and so we have.</p>
<blockquote><p><b>Theorem 2</b> <em> Let <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> be an arbitrary set, and <img alt="{\{ F(t) \}_{t\in T}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ F(t) \}_{t\in T}}"/> be a centered Gaussian process. Let <img alt="{T_0 \subseteq T_1 \subseteq \cdots T_k \subseteq \cdots \subseteq T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_0+%5Csubseteq+T_1+%5Csubseteq+%5Ccdots+T_k+%5Csubseteq+%5Ccdots+%5Csubseteq+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_0 \subseteq T_1 \subseteq \cdots T_k \subseteq \cdots \subseteq T}"/> be a countable sequence of finite subsets of <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> such that <img alt="{|T_0| = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CT_0%7C+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|T_0| = 1}"/> and <img alt="{|T_k| \leq 2^{2^k}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7CT_k%7C+%5Cleq+2%5E%7B2%5Ek%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|T_k| \leq 2^{2^k}}"/>. Then </em></p><em>
<p align="center"><img alt="\displaystyle  \mathop{\mathbb E} \sup_{t\in T} F(t) \leq O(1) \cdot \sup_{t\in T} \sum_{k=1}^\infty 2^{k/2} d (t , \pi_k(t)) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%5Cin+T%7D+F%28t%29+%5Cleq+O%281%29+%5Ccdot+%5Csup_%7Bt%5Cin+T%7D+%5Csum_%7Bk%3D1%7D%5E%5Cinfty+2%5E%7Bk%2F2%7D+d+%28t+%2C+%5Cpi_k%28t%29%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E} \sup_{t\in T} F(t) \leq O(1) \cdot \sup_{t\in T} \sum_{k=1}^\infty 2^{k/2} d (t , \pi_k(t)) "/></p>
</em><p><em> where <img alt="{d(\cdot,\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%28%5Ccdot%2C%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d(\cdot,\cdot)}"/> is the distance function <img alt="{d(t_1 , t_2) = \sqrt{ \mathop{\mathbb E} (F(t_1) - F(t_2))^2 }}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%28t_1+%2C+t_2%29+%3D+%5Csqrt%7B+%5Cmathop%7B%5Cmathbb+E%7D+%28F%28t_1%29+-+F%28t_2%29%29%5E2+%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d(t_1 , t_2) = \sqrt{ \mathop{\mathbb E} (F(t_1) - F(t_2))^2 }}"/>and <img alt="{\pi_k(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi_k%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\pi_k(t)}"/> is the element of <img alt="{T_{k-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_%7Bk-1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_{k-1}}"/> closest to <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> according to <img alt="{d(\cdot,\cdot)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%28%5Ccdot%2C%5Ccdot%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d(\cdot,\cdot)}"/>. </em></p></blockquote>
<p>
</p><p><b>3. Sub-Gaussian Random Processes </b></p>
<p/><p>
This theory does not seem to apply to problems such as bounding the max cut of a <img alt="{G_{n,p}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG_%7Bn%2Cp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G_{n,p}}"/> unweighted graph, or bounding the largest eigenvalue of a random symmetric matrix with <img alt="{\pm 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\pm 1}"/> entries, because such problems have a finite sample space and so cannot be modeled as Gaussian processes.</p>
<p>
Fortunately, there is a notion of a <em>sub-Gaussian</em> process, which applies to such problems and which reduces their analysis to the analysis of a related Gaussian process. </p>
<p>
First, recall that a centered real-valued random variable <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> is <em>sub-Gaussian</em> if there is a centered Gaussian random variable whose tail dominates the tail of <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/>, that is, if we have two constants <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> and <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/> such that, for all <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>:</p>
<p/><p align="center"><img alt="\displaystyle  \Pr [ X \geq t ] \leq k \cdot e^{-t^2 /v } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPr+%5B+X+%5Cgeq+t+%5D+%5Cleq+k+%5Ccdot+e%5E%7B-t%5E2+%2Fv+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \Pr [ X \geq t ] \leq k \cdot e^{-t^2 /v } "/></p>
<p>
An equivalent condition is that there is a <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/> such that</p>
<p/><p align="center"><img alt="\displaystyle  \mathop{\mathbb E} e^{X^2 / v} = O(1) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+e%5E%7BX%5E2+%2F+v%7D+%3D+O%281%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E} e^{X^2 / v} = O(1) "/></p>
<p>
In that case, we can define a norm, called the <img alt="{\Psi_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CPsi_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Psi_2}"/> norm as</p>
<p/><p align="center"><img alt="\displaystyle  || X ||_{\Psi_2} := \inf \{ \sigma : \mathop{\mathbb E} e^{X^2 / \sigma^2} \leq 2 \} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+X+%7C%7C_%7B%5CPsi_2%7D+%3A%3D+%5Cinf+%5C%7B+%5Csigma+%3A+%5Cmathop%7B%5Cmathbb+E%7D+e%5E%7BX%5E2+%2F+%5Csigma%5E2%7D+%5Cleq+2+%5C%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  || X ||_{\Psi_2} := \inf \{ \sigma : \mathop{\mathbb E} e^{X^2 / \sigma^2} \leq 2 \} "/></p>
<p>
which is, roughly, the standard deviation of a centered Gaussian that dominates <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/>.</p>
<blockquote><p><b>Example 1</b> <em> All bounded random variables are sub-Gaussian. </em></p></blockquote>
<p>
</p><blockquote><p><b>Example 2</b> <em> If </em></p><em>
<p align="center"><img alt="\displaystyle  X = \sum_i X_i " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X+%3D+%5Csum_i+X_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  X = \sum_i X_i "/></p>
</em><p><em> where the <img alt="{X_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X_i}"/> are independent Rademacher random variables, that is, if each <img alt="{X_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X_i}"/> is equally likely to be +1 or -1, then <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> is sub-Gaussian with <img alt="{|| X||_{\Psi_2} = O(\sqrt n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C+X%7C%7C_%7B%5CPsi_2%7D+%3D+O%28%5Csqrt+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|| X||_{\Psi_2} = O(\sqrt n)}"/>, which is within a constant factor of its actual standard deviation. </em></p></blockquote>
<p>
</p><blockquote><p><b>Example 3</b> <em> If </em></p><em>
<p align="center"><img alt="\displaystyle  X = \sum_i X_i " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X+%3D+%5Csum_i+X_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  X = \sum_i X_i "/></p>
</em><p><em> where the <img alt="{X_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X_i}"/> are independent and each <img alt="{X_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X_i}"/> has probability <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> of being equal to <img alt="{1-p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1-p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1-p}"/> and probability <img alt="{1-p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1-p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1-p}"/> of being equal to <img alt="{-p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-p}"/> (that is, each <img alt="{X_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X_i}"/> is a centered Bernoulli random variable), then <img alt="{|| X||_{\Psi_2} = \Omega \left( \sqrt {\frac {n} {\log 1/p } } \right)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C+X%7C%7C_%7B%5CPsi_2%7D+%3D+%5COmega+%5Cleft%28+%5Csqrt+%7B%5Cfrac+%7Bn%7D+%7B%5Clog+1%2Fp+%7D+%7D+%5Cright%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|| X||_{\Psi_2} = \Omega \left( \sqrt {\frac {n} {\log 1/p } } \right)}"/>, which is much more than the standard deviation of <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X}"/> when <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> is small. </em></p></blockquote>
<p>
</p><blockquote><p><b>Example 4</b> <em> If </em></p><em>
<p align="center"><img alt="\displaystyle  X = \sum_i a_i X_i " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++X+%3D+%5Csum_i+a_i+X_i+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  X = \sum_i a_i X_i "/></p>
</em><p><em> where the <img alt="{X_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X_i}"/> are independent Rademacher random variables, and the <img alt="{a_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a_i}"/> are arbitrary real scalars, then <img alt="{|| X ||_{\Psi_2} = O(\sqrt {\sum_i a_i^2 })}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%7C+X+%7C%7C_%7B%5CPsi_2%7D+%3D+O%28%5Csqrt+%7B%5Csum_i+a_i%5E2+%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|| X ||_{\Psi_2} = O(\sqrt {\sum_i a_i^2 })}"/>, which is within a constant factor of the standard deviation that we would get by replacing each <img alt="{X_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X_i}"/> with a standard Gaussian. </em></p></blockquote>
<p/><p>
Let now <img alt="{\{ F(t) \}_{t\in T}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ F(t) \}_{t\in T}}"/> be a centered random process. We will say that a Gaussian process <img alt="{\{ \hat F(t) \}_{t\in T} }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+%5Chat+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ \hat F(t) \}_{t\in T} }"/> <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/>-dominates <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> if, for every <img alt="{t_1,t_2 \in T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt_1%2Ct_2+%5Cin+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t_1,t_2 \in T}"/> we have</p>
<p/><p align="center"><img alt="\displaystyle  || F(t_1) - F(t_2) ||_{\Psi_2} \leq K \cdot \sqrt{\mathop{\mathbb E} \left (\hat F(t_1) - \hat F(t_2) \right)^2 }" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+F%28t_1%29+-+F%28t_2%29+%7C%7C_%7B%5CPsi_2%7D+%5Cleq+K+%5Ccdot+%5Csqrt%7B%5Cmathop%7B%5Cmathbb+E%7D+%5Cleft+%28%5Chat+F%28t_1%29+-+%5Chat+F%28t_2%29+%5Cright%29%5E2+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  || F(t_1) - F(t_2) ||_{\Psi_2} \leq K \cdot \sqrt{\mathop{\mathbb E} \left (\hat F(t_1) - \hat F(t_2) \right)^2 }"/></p>
<p>
That is, every random variable of the form <img alt="{F(t_1) - F(t_2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28t_1%29+-+F%28t_2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(t_1) - F(t_2)}"/> is sub-Gaussian, and its tail is dominated by the tail of the Gaussian distribution <img alt="{\hat F(t_1) - \hat F(t_2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Chat+F%28t_1%29+-+%5Chat+F%28t_2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\hat F(t_1) - \hat F(t_2)}"/></p>
<blockquote><p><b>Theorem 3 (Talagrand’s comparison inequality)</b> <em> There is an absolute constant <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> such that if <img alt="{\{ F(t) \}_{t\in T}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ F(t) \}_{t\in T}}"/> is a centered random process that is <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/>-dominated by a centered Gaussian random process <img alt="{\{ \hat F(t) \}_{t\in T} }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+%5Chat+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ \hat F(t) \}_{t\in T} }"/>, then </em></p><em>
<p align="center"><img alt="\displaystyle  \mathop{\mathbb E} \sup_{t\in T} \ F(t) \leq CK \mathop{\mathbb E} \sup_{t\in T} \hat F(t) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%5Cin+T%7D+%5C+F%28t%29+%5Cleq+CK+%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%5Cin+T%7D+%5Chat+F%28t%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E} \sup_{t\in T} \ F(t) \leq CK \mathop{\mathbb E} \sup_{t\in T} \hat F(t) "/></p>
<p> Furthermore, for every <img alt="{\ell \geq 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell \geq 0}"/>, </p>
<p align="center"><img alt="\displaystyle  \Pr \left[ \sup_{t_1,t_2 \in T} F(t_1) - F(t_2) \geq CK \ \left( \ell \cdot diam(T) + \mathop{\mathbb E} \sup_{t\in T} \hat F(t) \right) \right] \leq 2 e^{-\ell^2} \ , " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPr+%5Cleft%5B+%5Csup_%7Bt_1%2Ct_2+%5Cin+T%7D+F%28t_1%29+-+F%28t_2%29+%5Cgeq+CK+%5C+%5Cleft%28+%5Cell+%5Ccdot+diam%28T%29+%2B+%5Cmathop%7B%5Cmathbb+E%7D+%5Csup_%7Bt%5Cin+T%7D+%5Chat+F%28t%29+%5Cright%29+%5Cright%5D+%5Cleq+2+e%5E%7B-%5Cell%5E2%7D+%5C+%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \Pr \left[ \sup_{t_1,t_2 \in T} F(t_1) - F(t_2) \geq CK \ \left( \ell \cdot diam(T) + \mathop{\mathbb E} \sup_{t\in T} \hat F(t) \right) \right] \leq 2 e^{-\ell^2} \ , "/></p>
</em><p><em> where <img alt="{diam(T)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bdiam%28T%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{diam(T)}"/> is the diameter of <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> with respect to the distance function <img alt="{d(s,t) := \sqrt{\mathop{\mathbb E} \left (\hat F(s) - \hat F(t) \right)^2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%28s%2Ct%29+%3A%3D+%5Csqrt%7B%5Cmathop%7B%5Cmathbb+E%7D+%5Cleft+%28%5Chat+F%28s%29+-+%5Chat+F%28t%29+%5Cright%29%5E2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d(s,t) := \sqrt{\mathop{\mathbb E} \left (\hat F(s) - \hat F(t) \right)^2}}"/>. </em></p></blockquote>
<p/><p>
The way to apply this theory is the following. </p>
<p>
Suppose that we want estimate, on average or with high probability, the optimum of an optimization problem with feasible set <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> over the randomness of the choice of a random instance. We model this problem like a centered random process <img alt="{\{ F(t) \}_{t\in T}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ F(t) \}_{t\in T}}"/> in which <img alt="{F(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(t)}"/> is the difference between the cost of solution <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> in a random instance minus the average cost of <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>.</p>
<p>
Then we think about a related random experiment, in which the random choices involved in constructing our instance are replaced by Gaussian choices (for example, instead of a <img alt="{G_{n,1/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG_%7Bn%2C1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G_{n,1/2}}"/> random graph we may think of a complete graph with Gaussian weights on the edges chosen with expectation 1/2 and constant variance) and we let <img alt="{\{ \hat F(t) \}_{t\in T}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+%5Chat+F%28t%29+%5C%7D_%7Bt%5Cin+T%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ \hat F(t) \}_{t\in T}}"/> be the analogous process in this Gaussian model.</p>
<p>
If we can argue that <img alt="{\hat F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Chat+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\hat F}"/> dominates <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/>, then it remains to estimate <img alt="{\mathop{\mathbb E} \sup \hat F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D+%5Csup+%5Chat+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathop{\mathbb E} \sup \hat F}"/>, which we can do either by the generic chaining theorem or by other methods.</p>
<p>
</p><p><b>4. An Example </b></p>
<p/><p>
We will now use this machinery to show that the largest eigenvalue of a random symmetric <img alt="{n\times n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Ctimes+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n\times n}"/> matrix with Rademacher entries is <img alt="{O(\sqrt n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\sqrt n)}"/>. This is certainly not the simplest way of proving such a result, but it will give a sense of how these techniques can be applied.</p>
<p>
We let <img alt="{T = \{ x\in {\mathbb R}^n : ||x ||=1 \}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT+%3D+%5C%7B+x%5Cin+%7B%5Cmathbb+R%7D%5En+%3A+%7C%7Cx+%7C%7C%3D1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T = \{ x\in {\mathbb R}^n : ||x ||=1 \}}"/> be the unit sphere. </p>
<p>
Our Gaussian process will be to pick standard Gaussians <img alt="{g_{i,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g_{i,j}}"/>, for each <img alt="{1 \leq i \leq j \leq n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%5Cleq+i+%5Cleq+j+%5Cleq+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 \leq i \leq j \leq n}"/>, define the matrix <img alt="{ \hat M_{i,j} = \hat M_{j,i} = g_{i,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+%5Chat+M_%7Bi%2Cj%7D+%3D+%5Chat+M_%7Bj%2Ci%7D+%3D+g_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{ \hat M_{i,j} = \hat M_{j,i} = g_{i,j}}"/> and let</p>
<p/><p align="center"><img alt="\displaystyle  \hat F(x) = x^T \hat M x " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Chat+F%28x%29+%3D+x%5ET+%5Chat+M+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \hat F(x) = x^T \hat M x "/></p>
<p> for every <img alt="{x\in T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x\in T}"/>.</p>
<p>
Our “sub-Gaussian” random process is to pick Rademacher random variables <img alt="{r_{i,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r_{i,j}}"/>, for each <img alt="{1 \leq i \leq j \leq n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%5Cleq+i+%5Cleq+j+%5Cleq+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 \leq i \leq j \leq n}"/>, define the matrix <img alt="{ M_{i,j} = M_{j,i} = r_{i,j}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+M_%7Bi%2Cj%7D+%3D+M_%7Bj%2Ci%7D+%3D+r_%7Bi%2Cj%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{ M_{i,j} = M_{j,i} = r_{i,j}}"/> and let</p>
<p/><p align="center"><img alt="\displaystyle  F(x) = x^T M x " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28x%29+%3D+x%5ET+M+x+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F(x) = x^T M x "/></p>
<p> for every <img alt="{x\in T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5Cin+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x\in T}"/>.</p>
<p>
We will argue that <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> is <img alt="{O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(1)}"/>-dominated by <img alt="{\hat F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Chat+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\hat F}"/> and that <img alt="{\mathop{\mathbb E} \sup \hat F = O(\sqrt n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D+%5Csup+%5Chat+F+%3D+O%28%5Csqrt+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathop{\mathbb E} \sup \hat F = O(\sqrt n)}"/>.</p>
<p>
For the first claim, we see that for every <img alt="{x,y \in T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy+%5Cin+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x,y \in T}"/>, we can write <img alt="{F(x)-F(y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%28x%29-F%28y%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F(x)-F(y)}"/> as</p>
<p/><p align="center"><img alt="\displaystyle  F(x) - F(y) = \sum_{i,j} M_{i,j} (x_ix_j - y_iy_j) = 2 \sum_{i&lt;j} r_{i,j} (x_ix_j - y_iy_j) + \sum_i r_{i,i} (x_i^2 - y_i^2) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%28x%29+-+F%28y%29+%3D+%5Csum_%7Bi%2Cj%7D+M_%7Bi%2Cj%7D+%28x_ix_j+-+y_iy_j%29+%3D+2+%5Csum_%7Bi%3Cj%7D+r_%7Bi%2Cj%7D+%28x_ix_j+-+y_iy_j%29+%2B+%5Csum_i+r_%7Bi%2Ci%7D+%28x_i%5E2+-+y_i%5E2%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F(x) - F(y) = \sum_{i,j} M_{i,j} (x_ix_j - y_iy_j) = 2 \sum_{i&lt;j} r_{i,j} (x_ix_j - y_iy_j) + \sum_i r_{i,i} (x_i^2 - y_i^2) "/></p>
<p>
So, as noted in one of our examples above, we can say that </p>
<p align="center"><img alt="\displaystyle  || F(x) - F(y) ||^2_{\Psi_2} \leq O \left( 4 \sum_{i&lt;j} (x_ix_j - y_iy_j) ^2 + \sum_i (x_i^2 - y_i^2)^2 \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+F%28x%29+-+F%28y%29+%7C%7C%5E2_%7B%5CPsi_2%7D+%5Cleq+O+%5Cleft%28+4+%5Csum_%7Bi%3Cj%7D+%28x_ix_j+-+y_iy_j%29+%5E2+%2B+%5Csum_i+%28x_i%5E2+-+y_i%5E2%29%5E2+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  || F(x) - F(y) ||^2_{\Psi_2} \leq O \left( 4 \sum_{i&lt;j} (x_ix_j - y_iy_j) ^2 + \sum_i (x_i^2 - y_i^2)^2 \right) "/></p>
<p> and we see that </p>
<p align="center"><img alt="\displaystyle  (d(x,y))^2 = \mathop{\mathbb E} ( \hat F(x) - \hat F(y) )^2 = 4 \sum_{i&lt;j} (x_ix_j - y_iy_j) ^2 + \sum_i (x_i^2 - y_i^2)^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28d%28x%2Cy%29%29%5E2+%3D+%5Cmathop%7B%5Cmathbb+E%7D+%28+%5Chat+F%28x%29+-+%5Chat+F%28y%29+%29%5E2+%3D+4+%5Csum_%7Bi%3Cj%7D+%28x_ix_j+-+y_iy_j%29+%5E2+%2B+%5Csum_i+%28x_i%5E2+-+y_i%5E2%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  (d(x,y))^2 = \mathop{\mathbb E} ( \hat F(x) - \hat F(y) )^2 = 4 \sum_{i&lt;j} (x_ix_j - y_iy_j) ^2 + \sum_i (x_i^2 - y_i^2)^2 "/></p>
<p> so that, indeed, <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> is <img alt="{O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(1)}"/>-dominated by <img alt="{\hat F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Chat+F%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\hat F}"/>.</p>
<p>
Now we need to apply generic chaining to <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/>. It is very helpful to note that the distance function defined on <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> by the Gaussian process is dominated by Euclidean distance between the vectors <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> and <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/>, because</p>
<p/><p align="center"><img alt="\displaystyle  (d(x,y))^2 \leq 2 || xx^T - yy^T ||^2_F \leq 2 \cdot (||x|| + ||y||)^2 \cdot ||x-y||^2 = 8 ||x-y||^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%28d%28x%2Cy%29%29%5E2+%5Cleq+2+%7C%7C+xx%5ET+-+yy%5ET+%7C%7C%5E2_F+%5Cleq+2+%5Ccdot+%28%7C%7Cx%7C%7C+%2B+%7C%7Cy%7C%7C%29%5E2+%5Ccdot+%7C%7Cx-y%7C%7C%5E2+%3D+8+%7C%7Cx-y%7C%7C%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  (d(x,y))^2 \leq 2 || xx^T - yy^T ||^2_F \leq 2 \cdot (||x|| + ||y||)^2 \cdot ||x-y||^2 = 8 ||x-y||^2 "/></p>
<p> where we used the inequality </p>
<p align="center"><img alt="\displaystyle  || xx^T - yy^T ||_F \leq ( ||x || + ||y||) \cdot ||x-y|| " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%7C+xx%5ET+-+yy%5ET+%7C%7C_F+%5Cleq+%28+%7C%7Cx+%7C%7C+%2B+%7C%7Cy%7C%7C%29+%5Ccdot+%7C%7Cx-y%7C%7C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  || xx^T - yy^T ||_F \leq ( ||x || + ||y||) \cdot ||x-y|| "/></p>
<p>
We can conclude that an <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>-net over the unit Euclidean sphere is also a <img alt="{\sqrt {8}\cdot \epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt+%7B8%7D%5Ccdot+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sqrt {8}\cdot \epsilon}"/>-net for the metric space <img alt="{(T,d)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28T%2Cd%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(T,d)}"/>. For the unit Euclidean sphere there is an <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>-net of size at most <img alt="{(3/\epsilon)^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%283%2F%5Cepsilon%29%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(3/\epsilon)^n}"/>. To apply generic chaining, let <img alt="{T_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_k}"/> be an arbitrary subset of <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> of cardinality <img alt="{2^{2^k}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7B2%5Ek%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{2^k}}"/> if <img alt="{2^k &lt; n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Ek+%3C+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^k &lt; n}"/>, and an <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>-net with <img alt="{\epsilon = 3\cdot 2^{-2^k / n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3D+3%5Ccdot+2%5E%7B-2%5Ek+%2F+n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon = 3\cdot 2^{-2^k / n}}"/> otherwise. Applying the generic chaining inequality,</p>
<p/><p align="center"><img alt="\displaystyle  \mathop{\mathbb E} \sup \ \hat F \leq O(1) \cdot \sum_k 2^{k/2} \cdot \sqrt{8} \cdot \min \left\{ 2, 3\cdot 2^{-2^k / n} \right\} = O(\sqrt n) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D+%5Csup+%5C+%5Chat+F+%5Cleq+O%281%29+%5Ccdot+%5Csum_k+2%5E%7Bk%2F2%7D+%5Ccdot+%5Csqrt%7B8%7D+%5Ccdot+%5Cmin+%5Cleft%5C%7B+2%2C+3%5Ccdot+2%5E%7B-2%5Ek+%2F+n%7D+%5Cright%5C%7D+%3D+O%28%5Csqrt+n%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E} \sup \ \hat F \leq O(1) \cdot \sum_k 2^{k/2} \cdot \sqrt{8} \cdot \min \left\{ 2, 3\cdot 2^{-2^k / n} \right\} = O(\sqrt n) "/></p>
<p/></div>
    </content>
    <updated>2020-05-03T17:19:38Z</updated>
    <published>2020-05-03T17:19:38Z</published>
    <category term="math"/>
    <category term="theory"/>
    <category term="generic chaining"/>
    <category term="Talagrand"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2020-05-09T06:20:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/067</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/067" rel="alternate" type="text/html"/>
    <title>TR20-067 |  Computational and proof complexity of partial string avoidability | 

	Dmitry Itsykson, 

	Alexander Okhotin, 

	Vsevolod Oparin</title>
    <summary>The partial string avoidability problem is stated as follows: given a finite set of strings with possible ``holes'' (wildcard symbols), determine whether there exists a two-sided infinite string containing no substrings from this set, assuming that a hole matches every symbol. The problem is known to be NP-hard and in PSPACE, and this paper establishes its PSPACE-completeness. Next, string avoidability over the binary alphabet is interpreted as a version of conjunctive normal form satisfiability problem (SAT), where each clause has infinitely many shifted variants. Non-satisfiability of these formulas can be proved using variants of classical propositional proof systems, augmented with derivation rules for shifting proof lines
(such as clauses, inequalities, polynomials, etc). First, it is proved that there is a particular formula that has a short refutation in Resolution with a shift rule, but requires classical proofs of exponential size At the same time, it is shown that exponential lower bounds for classical proof systems can be translated for their shifted versions. Finally, it is shown that superpolynomial lower bounds on the size of shifted proofs would separate NP from PSPACE; a connection to lower bounds on circuit complexity is also established.</summary>
    <updated>2020-05-03T11:07:00Z</updated>
    <published>2020-05-03T11:07:00Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-09T06:20:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/066</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/066" rel="alternate" type="text/html"/>
    <title>TR20-066 |  Quantum Implications of Huang&amp;#39;s Sensitivity Theorem | 

	Scott Aaronson, 

	Shalev Ben-David, 

	Robin Kothari, 

	Avishay Tal</title>
    <summary>Based on the recent breakthrough of Huang (2019), we show that for any total Boolean function $f$, the deterministic query complexity, $D(f)$, is at most quartic in the quantum query complexity, $Q(f)$: $D(f) = O(Q(f)^4)$. This matches the known separation (up to log factors) due to Ambainis, Balodis, Belovs, Lee, Santha, and Smotrovs (2017). We also use the result to resolve the quantum analogue of the Aanderaa-Karp-Rosenberg conjecture. We show that if $f$ is a nontrivial monotone graph property of an $n$-vertex graph specified by its adjacency matrix, then $Q(f) = \Omega(n)$, which is also optimal.</summary>
    <updated>2020-05-03T11:01:55Z</updated>
    <published>2020-05-03T11:01:55Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-05-09T06:20:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://adamsheffer.wordpress.com/?p=5494</id>
    <link href="https://adamsheffer.wordpress.com/2020/05/03/math-summer-programs/" rel="alternate" type="text/html"/>
    <title>Math Summer Programs</title>
    <summary>The virus is causing some math summer programs to cancel. Surprisingly, this led to something wonderful. Unusually strong undergrads are starting to run their own online summer math programs for high school students. 1. The MORPH program is run by the Harvard math club. It offers a variety of mathematical topics at different levels of […]</summary>
    <updated>2020-05-03T01:11:48Z</updated>
    <published>2020-05-03T01:11:48Z</published>
    <category term="Math events"/>
    <author>
      <name>Adam Sheffer</name>
    </author>
    <source>
      <id>https://adamsheffer.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://adamsheffer.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://adamsheffer.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://adamsheffer.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://adamsheffer.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Discrete geometry and other typos</subtitle>
      <title>Some Plane Truths</title>
      <updated>2020-05-09T06:21:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4780</id>
    <link href="https://www.scottaaronson.com/blog/?p=4780" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4780#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4780" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Vaccine challenge trials NOW!</title>
    <summary xml:lang="en-US">Update (May 5): Here’s a Quillette article making the case for human challenge trials. I think there’s an actual non-negligible chance that this cause will win—but every wasted day means thousands more dead. I’ve asked myself again and again over the last few months: why are human challenge trials for covid vaccines not an ethical […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong><span class="has-inline-color has-vivid-red-color">Update (May 5):</span></strong> <a href="https://quillette.com/2020/05/01/human-challenge-trials-a-coronavirus-taboo/">Here’s a Quillette article</a> making the case for human challenge trials.  I think there’s an actual non-negligible chance that this cause will win—but every wasted day means thousands more dead.</p>



<p>I’ve asked myself again and again over the last few months: why are human challenge trials for covid vaccines not an ethical no-brainer?  What am I missing that all the serious medical experts see?  And what are we waiting for: for 10 million more to die? 20 million?  So it made me feel a little less crazy that <a href="https://www.washingtonpost.com/opinions/2020/04/27/pandemic-ethics-case-experiments-human-volunteers/?arc404=true">the world’s most famous living ethicist agrees</a>.</p>



<p>I loved the way James Miller put it on my Facebook:</p>



<blockquote class="wp-block-quote"><p>This is the trolley problem where the fat man wants to jump knowing his chance of death is below 1% and our decision is whether to stop him.</p></blockquote>



<p>Like, suppose someone willingly sacrificed themselves so that doctors could use their body parts to save 10 million people.  We might say: we would’ve lacked the strength to do the same in their place.  We might say: we hope they weren’t pressured or coerced into it.  But after the deed is done, is there anything to call this person but a hero, or even a martyr?  Whatever we feel about the fireman who sacrifices his life in the course of saving 10 kids from a burning building, shouldn’t we feel it about this person a million times over?  And of course, I deliberately made this vastly more extreme than the actual situation faced by young, healthy volunteers in a covid challenge trial, who in all likelihood would recover and be fine.</p>



<p>Regarding the obvious question: so would <em>I</em> volunteer to take an unproved vaccine, followed by a deliberate covid injection?  Sure!  Unfortunately, I might no longer be a candidate: I’m now nearing middle age and pre-diabetic, I help watch two young kids, and I live with two immunocompromised parents.  But on the principle of walking the walk: if it were a vaccine candidate that I considered promising (and there are now several), and if it were practical to isolate me away from home for the requisite time, and if I could actually be of use, then absolutely, jab me.</p>



<p><strong>On a somewhat related note:</strong> Last night I watched the <a href="https://en.wikipedia.org/wiki/Ender%27s_Game_(film)"><em>Ender’s Game</em> movie</a> with my 7-year-old daughter Lily (neither of us had seen it; I’d read the book but only as a kid).  Not surprisingly, the movie was a <em>huge</em> hit with Lily; she’s already begging to see it again.  As for me, my first thought was: what a hackneyed sci-fi premise, that the entire human race is under attack from some alien species, and that all human children grow up in the shadow of that knowledge.  Nothing whatsoever like the real world of 2020!  My second thought was: what a quaint concept, that faced with a threat to humanity, the earth-authorities would immediately respond “quick, we need to find and train and cultivate <em>super-geniuses willing to break the rules</em>, and put them in command!”  Only in the movies, never in real life!  Except in, y’know, WWII, where that mindset was pretty crucial to the Allied victory?  But 75 years later, yes, it reads to us as science fiction.</p>



<p>To inject a tiny note of optimism, I’m hopeful that we <em>will</em> eventually see some fruits of genius commensurate with the threat, whether in the realm of treatments or vaccines or contact-tracing apps or PPE or something else that no one’s thought of yet.  Right now, though, the sad fact is this: as far as I know, the only indisputable work of genius to have arisen in response to the covid crisis has been the <a href="https://twitter.com/steak_umm?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor">Twitter account for steak-umms</a>.</p></div>
    </content>
    <updated>2020-05-02T01:30:45Z</updated>
    <published>2020-05-02T01:30:45Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Rage Against Doofosity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-05-09T05:57:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17007</id>
    <link href="https://rjlipton.wordpress.com/2020/05/01/mathematics-of-covid-19/" rel="alternate" type="text/html"/>
    <title>Mathematics of COVID-19</title>
    <summary>Its not just [ Sir Francis Galton by Charles Wellington Furse ] Francis Galton is a perfect example of a Victorian era scientist. Sir Galton, he was knighted in 1909, had many roles: a statistician, a sociologist, a psychologist, an anthropologist, a tropical explorer, a geographer, a meteorologist, a geneticist, and an inventor. He coined […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Its not just <img alt="{R_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R_0}"/></em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/05/01/mathematics-of-covid-19/220px-sir_francis_galton_by_charles_wellington_furse/" rel="attachment wp-att-17012"><img alt="" class="alignright size-full wp-image-17012" src="https://rjlipton.files.wordpress.com/2020/05/220px-sir_francis_galton_by_charles_wellington_furse.jpg?w=600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ Sir Francis Galton by Charles Wellington Furse ]</font></td>
</tr>
</tbody>
</table>
<p>
Francis Galton is a perfect example of a Victorian era scientist. Sir Galton, he was knighted in 1909, had many roles: a statistician, a sociologist, a psychologist, an anthropologist, a tropical explorer, a geographer, a meteorologist, a geneticist, and an inventor. He coined the phrase “nature versus nurture” and <a href="https://en.wikipedia.org/wiki/Francis_Galton">more</a>.</p>
<p>
Today we trade in jokes for some mathematics of the virus.</p>
<p>
We wish there was something clever we can say about the spread of the virus. But the statistics of the spread are complex. We wish there was something theory can contribute to the fight against the virus. But the front-line is clearly dominated by medicine and biology.</p>
<p>
However there are two areas that are relevant. The first is the math of how fast the virus spreads and the second is how valid are the claims about the virus. The latter is an area where theory could play a role in the future.</p>
<p>
In preparing this discussion we noted that Galton was indeed an inventor. He invented a device to demonstrate the central limit theorem. You probably have seen some <a href="http://www.karlsims.com/marbles/">version</a>. Sometimes called the bean machine, it gives a visual demonstration of the central limit theorem. Other times it is called the Galton board. Perhaps if Galton were alive today he would be on cable news explaining how the virus spreads. </p>
<p>
<a href="https://rjlipton.wordpress.com/2020/05/01/mathematics-of-covid-19/marble-run/" rel="attachment wp-att-17009"><img alt="" class="aligncenter size-medium wp-image-17009" height="300" src="https://rjlipton.files.wordpress.com/2020/05/marble-run.jpg?w=168&amp;h=300" width="168"/></a></p>
<p>
Galton also had views that are troubling. See <a href="https://www.statisticsviews.com/details/news/11158556/June-2019-issue-of-Significance-just-published.html">this</a> for example. He lived over one hundred years ago, but his views on eugenics are still upsetting. Should we not have featured him? What do you think?</p>
<p/><h2> Extinction </h2><p/>
<p/><p>
The issue is will the terrible virus stop infecting people? Will it become extinct? Or will it at least stop infecting more and more people. The part of math that studies such questions was invented by Galton in 1889 as a model to track family names. We wish we were talking today about family names and not a killer virus. The area he invented is now called <a href="https://en.wikipedia.org/wiki/Branching_process">branching process</a>. </p>
<blockquote><p><b> </b> <em> There was concern amongst the Victorians that aristocratic surnames were becoming extinct. Galton originally posed the question regarding the probability of such an event in an 1873 issue of The Educational Times, and the Reverend Henry Watson replied with a solution. Together, they then wrote an 1874 paper entitled <a href="https://www.jstor.org/stable/2841222?origin=crossref&amp;seq=1#metadata_info_tab_contents">On the probability of the extinction of families</a>. </em>
</p></blockquote>
<p/><p>
We are interested in branching processes and when they are likely to become extinct. We want the virus to stop infecting people, and become extinct. Or at least stop its explosive growth that is so terrible. See these <a href="https://theconversation.com/how-to-flatten-the-curve-of-coronavirus-a-mathematician-explains-133514">comments</a>, for more information. </p>
<p>
<a href="https://rjlipton.wordpress.com/2020/05/01/mathematics-of-covid-19/fig/" rel="attachment wp-att-17010"><img alt="" class="aligncenter size-medium wp-image-17010" height="225" src="https://rjlipton.files.wordpress.com/2020/05/fig.png?w=300&amp;h=225" width="300"/></a></p>
<p>
</p><p/><h2> On Average </h2><p/>
<p/><p>
The contagiousness of a disease is described by its “reproduction rate” or the average number of people infected by one infectious person in a population without immunity. You might also hear this number referred to as the <img alt="{R_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R_0}"/> value. When it is less than <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>, the disease does not become an pandemic. This process is called a <i>branching process</i>. In order to tell if a branching process will eventually become extinct we need more than <img alt="{R_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R_0}"/>. That is we need to understand more than the average number of descendants. Let’s see why.</p>
<p>
Consider a process <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> that creates <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> descendants with probability <img alt="{a_{k}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba_%7Bk%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a_{k}}"/> and a process <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> that creates <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> descendants with probability <img alt="{b_{k}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb_%7Bk%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b_{k}}"/>. The number of average descendants is for <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> is 	</p>
<p align="center"><img alt="\displaystyle  \mu_{A} = a_{1} + 2a_{2} + \dots " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmu_%7BA%7D+%3D+a_%7B1%7D+%2B+2a_%7B2%7D+%2B+%5Cdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mu_{A} = a_{1} + 2a_{2} + \dots "/></p>
<p>and for <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> is 	</p>
<p align="center"><img alt="\displaystyle  \mu_{B} = b_{1} + 2b_{2} + \dots " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmu_%7BB%7D+%3D+b_%7B1%7D+%2B+2b_%7B2%7D+%2B+%5Cdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mu_{B} = b_{1} + 2b_{2} + \dots "/></p>
<p>Is it always better to have the process with the smaller average? The answer is no.</p>
<p>
Consider the process <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> so that 	</p>
<p align="center"><img alt="\displaystyle  a_{0} = 0, a_{1} = 1, a_{2} = \epsilon, a_{3} = 0, \dots " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++a_%7B0%7D+%3D+0%2C+a_%7B1%7D+%3D+1%2C+a_%7B2%7D+%3D+%5Cepsilon%2C+a_%7B3%7D+%3D+0%2C+%5Cdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  a_{0} = 0, a_{1} = 1, a_{2} = \epsilon, a_{3} = 0, \dots "/></p>
<p>And consider the process <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> so that 	</p>
<p align="center"><img alt="\displaystyle  b_{0} = 1/n, b_{1} = 1/n, \dots, b_{n} = 1/n, b_{n+1} = 0, \dots " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++b_%7B0%7D+%3D+1%2Fn%2C+b_%7B1%7D+%3D+1%2Fn%2C+%5Cdots%2C+b_%7Bn%7D+%3D+1%2Fn%2C+b_%7Bn%2B1%7D+%3D+0%2C+%5Cdots+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  b_{0} = 1/n, b_{1} = 1/n, \dots, b_{n} = 1/n, b_{n+1} = 0, \dots "/></p>
<p>The first average <img alt="{\mu_{A} = 1 + 2\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmu_%7BA%7D+%3D+1+%2B+2%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mu_{A} = 1 + 2\epsilon}"/> and the second average is 	</p>
<p align="center"><img alt="\displaystyle  1/n + 2/n + \cdots + n/n = (n+1)/2. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++1%2Fn+%2B+2%2Fn+%2B+%5Ccdots+%2B+n%2Fn+%3D+%28n%2B1%29%2F2.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  1/n + 2/n + \cdots + n/n = (n+1)/2. "/></p>
<p>Clearly the second has a much larger value for <img alt="{n \ge 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Cge+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n \ge 2}"/>. But the first will never go extinct and the second can become extinct.</p>
<p>
</p><p/><h2> It’s In the Variance </h2><p/>
<p/><p>
The key difference is that the second process has higher <em>variance</em>. The importance of the variance—as opposed to the mean—is remembered in some ways but seems to be forgotten in others. It is the nub of one of the <a href="https://rjlipton.wordpress.com/2020/04/26/time-for-some-jokes/">jokes</a> we included in the previous post:</p>
<blockquote><p><b> </b> <em> There was a statistician who drowned crossing a river—that was only 3 feet deep on average. </em>
</p></blockquote>
<p/><p>
For an example closer to our point, suppose a third-party candidate <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> entering a race expects to take more votes away from candidate <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> than candidate <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> so as to double the margin that <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> expects to lose by. But suppose <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> alters the dynamics of the race so that the standard deviation is quadrupled. Then <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> generally has a better chance of winning under that scenario. If the distribution is normal and the original standard deviation equaled the expected margin, then <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/>‘s chances of winning improve from 16% to over 30%.</p>
<p>
In our case, “winning” means outcomes where the virus dies out, locally and ultimately globally. Such outcomes are needed for opening up. It is not enough to reduce the number of active cases to “<img alt="{O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(1)}"/>” because the branching started from such a state. Whatever active cases there are must be known and contained as well as <img alt="{O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(1)}"/>. This is the situation currently <a href="https://www.bbc.com/news/world-asia-52436658">claimed</a> in New Zealand.</p>
<p>
</p><p/><h2> Branching and Uncertainty </h2><p/>
<p/><p>
At the other end is the state where the virus is not contained but branching stops because of saturation. When the proportion of targeted descendants who have already had the virus and are (we hope) immune is greater than the branching factor, then the expectation takes over from the variance as a determinant of stoppage. This proportion is what is meant by “herd immunity” and is estimated to be 60–70% for this virus.</p>
<p>
What we feel is the central mystery is whether there are enough undetected cases to bring that point even possibly in range. Randomized testing in the New York area has found nearly a 25% rate of antibodies. Analogous <a href="https://www.nytimes.com/2020/04/21/health/coronavirus-antibodies-california.html">tests</a> in less-affected California and in other countries have found under 10% positive rates, however. Those results are subject to uncertainty in the representativeness of the samples.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
We had planned on saying something about checking if reported data about the virus is correct, or is it faked. With so much at stake it seems smart to insist on data being verified. More on that in the future.</p>
<p/></font></font></div>
    </content>
    <updated>2020-05-01T21:12:58Z</updated>
    <published>2020-05-01T21:12:58Z</published>
    <category term="Ideas"/>
    <category term="P=NP"/>
    <category term="People"/>
    <category term="Results"/>
    <category term="branching process"/>
    <category term="coronavirus"/>
    <category term="extinct"/>
    <category term="R0"/>
    <category term="virus"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2020-05-09T06:21:03Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-5535989522381894451</id>
    <link href="https://blog.computationalcomplexity.org/feeds/5535989522381894451/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/05/predicting-virus.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5535989522381894451" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5535989522381894451" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/05/predicting-virus.html" rel="alternate" type="text/html"/>
    <title>Predicting the Virus</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">As a complexity theorist I often find myself far more intrigued in what we cannot compute than what we can. <div><br/></div><div>In 2009 I <a href="https://blog.computationalcomplexity.org/2009/06/failure-of-social-networks.html">posted on some predictions of the spread of the H1N1 virus</a> which turned out to be off by two orders of magnitude. I wrote "I always worry that bad predictions from scientists make it harder to have the public trust us when we really need them to." Now we need them to.</div><div><br/></div><div>We find ourselves bombarded with predictions from a variety of experts and even larger variety of mathematicians, computer scientists, physicists, engineers, economists and others who try to make their own predictions with no earlier experience in epidemiology. Many of these models give different predictions and even the best have proven significantly different than reality. We keep coming back to the George Box quote "All models are wrong, but some are useful."</div><div><br/></div><div>So why do these models have so much trouble? The standard complaint of inaccurate and inconsistently collected data certainly holds. And if a prediction changes our behavior, we cannot fault the predictor for not continuing to be accurate.</div><div><br/></div><div>There's another issue. You often here of a single event having a dramatic effect in a region--a soccer game in Italy, a funeral in Georgia, a Bar Mitzvah in New York. These events ricocheted, people infected attended other events that infected others. This becomes a complex process that simple network models can never get right. Plenty of soccer games, funerals and Bar Mitzvahs didn't spread the virus. If a region has hadn't a large number of cases and deaths is it because they did the right thing or just got lucky. Probably something in between but that makes it hard to generalize and learn from experience. We do know that less events means less infection but beyond that is less clear.</div><div><br/></div><div>As countries and states decide how to open up and universities decide how to handle the fall semester, we need to rely on some sort of predictive models and the public's trust in them to move forward. We can't count on the accuracy of any model but which models are useful? We don't have much time to figure it out.</div></div>
    </content>
    <updated>2020-05-01T14:40:00Z</updated>
    <published>2020-05-01T14:40:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-05-08T21:30:51Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=3460</id>
    <link href="https://francisbach.com/gradient-flows/" rel="alternate" type="text/html"/>
    <title>Effortless optimization through gradient flows</title>
    <summary>Optimization algorithms often rely on simple intuitive principles, but their analysis quickly leads to a lot of algebra, where the original idea is not transparent. In last month post, Adrien Taylor explained how convergence proofs could be automated. This month, I will show how proof sketches can be obtained easily for algorithms based on gradient...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">Optimization algorithms often rely on simple intuitive principles, but their analysis quickly leads to a lot of algebra, where the original idea is not transparent. In last month <a href="https://francisbach.com/computer-aided-analyses/">post</a>, <a href="https://www.di.ens.fr/~ataylor/">Adrien Taylor</a> explained how convergence proofs could be automated. This month, I will show how proof sketches can be obtained easily for algorithms based on gradient descent. This will be done using vanishing step-sizes that lead to <em>gradient flows</em>.</p>



<h2>Gradient as local information</h2>



<p class="justify-text">The intuitive principle behind gradient descent is the quest for <em>local</em> descent. We thus need to characterize the local behavior of the function we aim to optimize. This is what gradients are for.</p>



<p class="justify-text">In this blog post, I will consider minimizing a function \(f\) over \(\mathbb{R}^d\). Assuming \(f\) is differentiable, a first order Taylor expansion of \(f\) around a point \(x\) leads to $$f(x+\delta) = f(x) + \nabla f(x) ^\top \delta + o(\| \delta\|),$$ for any norm \(\| \cdot \|\) on \(\mathbb{R}^d\), where \(\nabla f(x) \in \mathbb{R}^d\) is  the gradient of \(f\) at \(x\), composed of partial derivatives of \(f\). Therefore, around \(x\), \(f\) is approximately affine.</p>



<p class="justify-text">Since we have a local affine approximation around \(x\), we can look for the direction of steepest descent, that is, the unit norm vector \(u \in \mathbb{R}^d\) such that \(f\) decays the most along \(u\), that is such that $$  u^\top \nabla f(x)$$ is minimized. This steepest descent direction depends on the choice of norm (assuming that the gradient is not zero at \(x\)).</p>



<p class="justify-text">For the \(\ell_2\)-norm, then minimizing \(u^\top \nabla f(x)\) such that \(\|u\|_2 = 1\), leads to $$ \displaystyle u = \ – \frac{\nabla f(x)}{ \| \nabla f(x) \|_2},$$ that is the steepest descent is along the negative gradient (see an illustration below). In this blog post I will only focus on this steepest descent direction. </p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3542" height="149" src="https://francisbach.com/wp-content/uploads/2020/04/gradient_contours-1024x358.png" width="428"/>Function \(f\) represented through its contour lines for values 1, 2, 3, 4 and 5. Negative gradient \(– \nabla f(x)\) as the steepest descent direction at point \(x\), which is orthogonal to the contour lines.</figure></div>



<p class="justify-text">As as side note, for the \(\ell_1\)-norm, then minimizing \(u^\top \nabla f(x)\) such that \(\|u\|_1 = 1\), leads to $$u \in\  – \arg\max_{ v \in \{-e_1,\, e_1,\, -e_2,\, e_2,\dots,\, -e_d,\, e_d \}} v^\top \nabla f(x),$$ where \(e_i\) is the \(i\)-th canonical basis vector of \(\mathbb{R}^d\). Here the steepest descent is along a coordinate axis (along the positive or negative side), and this leads to various forms of <a href="https://en.wikipedia.org/wiki/Coordinate_descent">coordinate descent</a> (this will probably be a topic for another post). </p>



<p class="justify-text">Given that the negative gradient leads to the steepest descent direction (for the Euclidean norm), it is natural to use this as a direction for an iterative algorithm, an idea that dates back to <a href="https://en.wikipedia.org/wiki/Augustin-Louis_Cauchy">Cauchy</a> in 1847 [<a href="http://gallica.bnf.fr/ark:/12148/bpt6k90190w/f406">1</a>] (see the nice summary by Claude Le Maréchal [<a href="https://www.math.uni-bielefeld.de/documenta/vol-ismp/40_lemarechal-claude.pdf">2</a>]).</p>



<h2>From gradient descent to gradient flows</h2>



<p class="justify-text"><a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient descent</a> is the most classical iterative algorithm to minimize differentiable functions. It takes the form $$x_{n+1} = x_{n} \, – \gamma \nabla f(x_{n})$$ at iteration \(n\), where \(\gamma &gt; 0 \) is a step-size.  </p>



<p class="justify-text">Gradient descent comes in many flavors, steepest, stochastic, pre-conditioned, conjugate, proximal, projected, accelerated, etc. There are lots of papers and books [e.g., 3, 4, 5] analyzing it in various settings.</p>



<p class="justify-text">In this post, to simplify its analysis and setting the stage for later posts, I will present the gradient flow, which is essentially the limit of gradient descent when the step-size \(\gamma\) tends to zero.</p>



<p class="justify-text">More precisely, this is obtained by considering that our iterates \(x_n\) are sampled at each multiple of \(\gamma\), from a function \(X: \mathbb{R}_+ \to \mathbb{R}^d\), as $$x_n = X(n\gamma).$$ We can then use a piecewise affine interpolation to define a function defined on all points. We then have for \(t = n\gamma\), $$X(t + \gamma) = x_{n+1} =x_{n} \, – \gamma \nabla f(x_{n}) = X(t)\, – \gamma \nabla f(X(t)).$$ Dividing by \(\gamma\), we get $$ \frac{1}{\gamma} \big[ X(t + \gamma) \, – X(t) \big] = \, – \nabla f(X(t)).$$</p>



<p class="justify-text">When \(\gamma\) tends to zero (and with simple additional regularity assumptions), the left hand side tends to the derivative of \(X\) at \(t\), and thus the function \(X\) tends to the solution of the following <a href="https://en.wikipedia.org/wiki/Ordinary_differential_equation">ordinary differential equation</a> $$ \dot{X}(t) = \ – \nabla f (X(t)).$$ See an illustration below.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-3479" height="271" src="https://francisbach.com/wp-content/uploads/2020/04/logistic_2d_flow.gif" width="348"/>Gradient descent (with piece-wise affine interpolation between iterates) vs. gradient flow on the same time scale for a logistic regression problem.</figure></div>



<p class="justify-text">Studying the gradient flow in lieu of the gradient descent recursions comes with pros and cons.</p>



<p class="justify-text"><strong>Simplified analyses</strong>. The gradient flow has no step-size, so all the traditional annoying issues regarding the choice of step-size, with line-search, constant, decreasing or with a weird schedule are unnecessary. Moreover, the use of differential calculus makes proving properties really simple (see examples below). We can thus focus on the essence of the algorithm rather than on technicalities.</p>



<p class="justify-text"><strong>From (continuous) flow to actual (discrete) algorithms</strong>. A flow cannot be run on a computer at is is a continuous-time object. The traditional discretization is the <a href="https://en.wikipedia.org/wiki/Euler_method">Euler method</a>, that exactly replaces the flow by a piecewise-affine interpolation of the gradient descent iterates, where as shown above, we see \(x_n\) as \(X(n\gamma)\), where \(\gamma\) is the time increment between two samples. Four interesting observations:</p>



<ul class="justify-text"><li><em>No direct proof transfer</em> : While Euler discretization always provides an algorithm, the generic convergence proofs do not allow to transfer immediately continuous-time proofs to convergence results for the discrete analysis. A key difficulty is to set-up the step-size \(\gamma\). However, the analysis can often be mimicked, i.e., similar <a href="https://en.wikipedia.org/wiki/Lyapunov_function">Lyapunov functions</a> can be used (see examples below).</li><li><em>Proximal algorithms</em> : Faced with non-continuous gradient functions, the <em>forward</em> version of Euler discretization \(x_{n+1} = x_{n} – \gamma \nabla f(x_{n})\) can be replaced by the <em>backward</em> version $$x_{n+1} = x_{n} \, –  \gamma \nabla f(x_{n+1}),$$ which is only implicit as it can be solved by minimizing $$ f(x) + \frac{1}{2\gamma}\|x-x_{n}\|_2^2,$$ thus leading to the <a href="https://fr.wikipedia.org/wiki/Algorithme_proximal_(optimisation)">proximal point algorithm</a>. Forward-backward schemes can also be recovered when \(f\) is the sum of a smooth and a non-smooth term.</li><li><em>Stochastic gradient descent</em> : There are two ways to deal with stochastic gradient descent, leading to two very different continuous limits. Adding independent and identically distributed (for simplicity) zero-mean noise \(\varepsilon_n\) to the gradient leads to the recursion $$x_{n+1} = x_{n} – \gamma \big[ \nabla f(x_{n}) + \varepsilon_n\big] = x_{n}\, – \gamma \nabla f(x_{n}) \,- \gamma \varepsilon_n,$$ where the noise is multiplied by the step-size \(\gamma\). Surprisingly, taking the limit when \(\gamma\) tends to zero leads to the deterministic gradient flow equation. A more detailed argument is presented at the end of post, but the main hand-waving reason is that the noise contribution vanishes because it is multiplied by the step-size. Note that this limiting behavior is consistent with a convergence to a minimizer of \(f\).</li><li><em>Convergence to a Langevin diffusion</em> : When instead the noise is added with magnitude proportional to the square root \(\sqrt{2 \gamma}\) of the step-size (which is asymptotically larger than \(\gamma\)), when \(\gamma\) tends to zero, and if the covariance of the noise is identity, we converge to a <a href="https://en.wikipedia.org/wiki/Diffusion_process">diffusion process</a> which is the solution of a <a href="https://en.wikipedia.org/wiki/Stochastic_differential_equation">stochastic differential equation</a>: $$ dX(t) = \ – \nabla f(X(t)) + \sqrt{2} dB(t),$$ where \(B\) is a standard <a href="https://en.wikipedia.org/wiki/Brownian_motion">Brownian motion</a>. Moreover, as \(t\) tends to infinity, \(X(t)\) happens to tend in distribution to a random variable with density proportional to \(\exp( – f(x) )\). See more details at the end of the post and in [6]. The difference in behavior is illustrated below.</li></ul>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-3527" height="318" src="https://francisbach.com/wp-content/uploads/2020/04/logistic_2d_flow_SGD-3.gif" width="359"/> Comparison of flow and diffusion, for the same small \(\gamma\). The flow is deterministic and converges to a stationary point of \(f\) (here the global minimum), while the diffusion is stochastic and converges to a distribution (which is typically not a point mass)</figure></div>



<h2>Properties of gradient flows</h2>



<p class="justify-text">The gradient flow $$ \dot{X}(t) = \ – \nabla f (X(t)) $$ is well-defined for a wide variety of conditions on the function \(f\). The most classical ones are <a href="https://en.wikipedia.org/wiki/Picard%E2%80%93Lindel%C3%B6f_theorem">Lipschitz-continuity</a> or semi-convexity [<a href="https://link.springer.com/content/pdf/10.1007/s13373-017-0101-1.pdf">7</a>].</p>



<p class="justify-text">The most obvious property is that the function decreases along the flow; in other words, \(f(X(t))\) is decreasing, which is a simple consequence of $$ \frac{d}{dt} f(X(t)) =  \nabla f(X(t))^\top \frac{dX(t)}{dt} =\  – \| \nabla f (X(t) )\|_2^2 \leqslant 0.$$</p>



<p class="justify-text">If \(f\) is bounded from below, then \(f(X(t))\) will always converge (as a non-increasing function which is bounded from below, see <a href="https://en.wikipedia.org/wiki/Monotone_convergence_theorem">here</a>). However, in general, \(X(t)\) may not always converge without any further assumptions, e.g., it may oscillate forever. This is however rare and there are a variety of sufficient conditions for convergence of gradient flows, that date back to Lojasiewicz [8], and are based on “Lojasiewicz inequalities” that state that for \(y\) and \(x\) close enough, \(|f(x) – f(y)|^{1-\theta} \leqslant C \| \nabla f(x)\|\) for some \(C &gt; 0 \) and \(\theta \in (0,1)\). These are satisfied for “sub-analytical functions”, that include most functions one can imagine [<a href="https://www.sciencedirect.com/sdfe/reader/pii/S0022247X05006864/pdf">9</a>].</p>



<p class="justify-text">Once \(X(t)\) converges to some \(X(\infty) \in \mathbb{R}^d\), assuming \(\nabla f\) is continuous, we must have \(\nabla f(X(\infty))=0\), that is, \(X(\infty)\) is a stationary point of \(f\). Among all stationary points (that can be local minima, local maxima, or saddle-points), the one to which \(X(t)\) converges to depends on \(X(0)\).</p>



<p class="justify-text">Given any stationary point, one can look at the set of initializations that lead to it. Typically, only local minima are stable, that is, the attraction basins of other stationary points has typically zero Lebesgue measure (see, e.g., [<a href="http://www.jmlr.org/proceedings/papers/v49/lee16.pdf">10</a>]). See examples below. </p>



<p class="justify-text">We start with a simple function defined on the two-dimensional plane, with several local minima and saddle-points.</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3490" height="401" src="https://francisbach.com/wp-content/uploads/2020/04/plot_non_convex-1.png" width="511"/>Various gradient flows trajectories, starting from green points and ending in black points. Note the proximity of the three top starting points, all ending in different local minima. See the motion below.</figure></div>



<div class="wp-block-image justify-text"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-3492" height="500" src="https://francisbach.com/wp-content/uploads/2020/04/logistic_2d_flow_noncvx-1.gif" width="519"/>Various gradient flows trajectories, in motion! All flows share the same time scale. Some seem “slower” than others (because the gradient norm is small).</figure></div>



<p class="justify-text">Before moving on, I cannot resist presenting a “real” two-dimensional example that probably all skiers, hikers, and cyclists with some form of mathematical abilities have thought of, the topographic map. Here is an example below:</p>



<div class="wp-block-image justify-text"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-3751" height="458" src="https://francisbach.com/wp-content/uploads/2020/04/glandon_croix_de_fer-1-1024x1024.jpg" width="458"/>Extract from <a href="https://www.geoportail.gouv.fr/">IGN</a> topographic map, around <a href="https://en.wikipedia.org/wiki/Col_du_Glandon">Col du Glandon</a> and <a href="https://en.wikipedia.org/wiki/Col_de_la_Croix_de_Fer">Col de la Croix de Fer</a> (French Alps).</figure></div>



<p class="justify-text">Given the topographic map, how would gradient descent or gradient flow perform? Clearly, this corresponds to a non convex function, but it is quite well-behaved, as following water flows will typically lead to sea level. I chose two starting points famous to cyclists, <a href="https://en.wikipedia.org/wiki/Col_du_Glandon">Col du Glandon</a> and <a href="https://en.wikipedia.org/wiki/Col_de_la_Croix_de_Fer">Col de la Croix de Fer</a>, and ran gradient descent with a small step-size (to approximate the gradient flow), without noise (left) and with noise (right), on the topographic map (thanks to <a href="http://recherche.ign.fr/labos/matis/cv.php?nom=Landrieu">Loïc Landrieu</a> for the data extraction).</p>



<figure class="wp-block-image size-large"><img alt="" class="wp-image-3753" src="https://francisbach.com/wp-content/uploads/2020/04/flows_final_square_small-1024x394.png"/></figure>



<p class="justify-text">Without noise, the descent from la Croix de Fer ends up getting stuck quickly in a local minimum, while the one from Glandon goes down to the valley, but then is not able to follow the almost flat slope. When noise is added, the two flows go a bit lower, highlighting the benefits of noise to escape local minima.</p>



<h2>Gradient flows for optimization and machine learning</h2>



<p class="justify-text">There are (at least) two key questions in optimization and machine learning related to gradient flows: </p>



<ul class="justify-text"><li>When can we have global guarantees for convergence? That is, can we make sure that we choose an initialization point well enough to get the the global optimum <em>without knowing where the global optimum is</em>. A key difficulty is that the volume of the attraction basin of the global optimum can be made arbitrarily small, even for infinitely differentiable functions (imagine a function equal to zero everywhere except on a small ball where it is negative).</li><li>How fast can we get there? “there” can be a stationary point or a global optimum. This is an important question as mere convergence in the limit may be arbitrarily slow [<a href="https://papers.nips.cc/paper/6707-gradient-descent-can-take-exponential-time-to-escape-saddle-points.pdf">11</a>].</li></ul>



<p class="justify-text">An important class of function is <a href="https://en.wikipedia.org/wiki/Convex_function">convex functions</a>, where everything works out very well. We will study them below. Other functions will be studied in future posts.</p>



<h2>Convex functions</h2>



<p class="justify-text">We now assume that the function \(f\) is <a href="https://en.wikipedia.org/wiki/Convex_function">convex</a> and differentiable. Within machine learning, this corresponds to objective functions encountered for supervised learning which are based on empirical risk minimization with a prediction function which is linearly parameterized, such as <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a>.</p>



<p class="justify-text">There are various definitions of convexity, which are based on global properties (the function is always “below its chords”, or it is always “above its tangents”) or local properties (the Hessian is always positive semi-definite). The one which we need here is to be above its tangents, that is, for any \(x, y \in \mathbb{R}^d\), $$f(x) \geqslant f(y)  + \nabla f(y)^\top ( x \, – y).$$ Applying this to any stationary point \(y\) such that \(\nabla f(y)=0\) shows that for all \(x\), \(f(x) \geqslant f(y)\), that is, \(y\) is a global minimizer of \(f\). This is the classical benefit of convexity: no need to worry about local minima.</p>



<p class="justify-text">Another property we will need is the Lojasiewicz inequality, which is in particular satisfied when \(f\) is \(\mu\)-<a href="https://en.wikipedia.org/wiki/Convex_function#Strongly_convex_functions">strongly convex</a> (that is, \(f – \frac{1}{2} \| \cdot \|_2^2\) is convex): $$ f(x) \ – f(x_\ast) \leqslant \frac{1}{2 \mu} \| \nabla f (x)\|^2$$ for any minimizer \(x_\ast\) of \(f\) and any \(x\). This property allows to go from a bound on the gradient norm to a bound on function values.</p>



<p class="justify-text">We then obtain the convergence rate <em>in one line</em> as follows (see more details in [<a href="http://papers.nips.cc/paper/6711-integration-methods-and-optimization-algorithms.pdf">12</a>]): $$ \frac{d}{dt} \big[ f(X(t))\ – f(x_\ast) \big] =\  \nabla f(X(t))^\top \dot{X}(t) =  \ – \| \nabla f(X(t))\|_2^2 \leqslant \ – 2\mu  \big[ f(X(t)) \ – f(x_\ast) \big]$$ using the Lojasiewicz inequality above, leading to by simple integration of the derivative of \(\log \big[ f(X(t)) \ – f(x_\ast) \big]\): $$f(X(t)) \ – f(x_\ast) \leqslant \exp( – 2\mu t ) \big[ f(X(0))\  – f(x_\ast) \big], $$ that is, the convergence is exponential and the characteristic time is proportional to \(1/\mu\).</p>



<p class="justify-text">The gradient flow gives the main insight (exponential convergence); and applying the result above to \(t = \gamma n\), we seem to recover the traditional rate proportional to \(\exp( – \gamma \mu n)\); HOWEVER, this is only true asymptotically for \(\gamma\) tending to zero, and proving a result for gradient descent requires extra steps to deal with having a constant step-size. This requires typically \(\gamma \leqslant 1/L\), where \(L\) is the smoothness constant of \(f\), and the simplest proof happens to use the same structure (see [<a href="https://arxiv.org/pdf/1608.04636">13</a>] and references therein, as well as [<a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=zvmmf&amp;paperid=7813&amp;what=fullt&amp;option_lang=eng">14</a>]).</p>



<p class="justify-text">Without strong convexity, we have, using the tangent property at \(X(t)\) and \(x_\ast\): $$ \frac{d}{dt}\big[   \| X(t)\ – x_\ast \|^2 \big] = \ –   2 ( X(t) \ – x_\ast )^\top \nabla f(X(t)) \leqslant \ – 2 \big[ f(X(t)) \ – f(x_\ast) \big],$$  leading to, by integrating from \(0\) to \(t\), and using the monotonicity of \(f(X(t))\): $$  f(X(t)) \ – f(x_\ast) \leqslant \frac{1}{t} \int_0^t \big[ f(X(u)) \ – f(x_\ast) \big] du \leqslant \frac{1}{2t} \| X(0) \ – x_\ast \|^2 \ – \frac{1}{2t} \| X(t) \ – x_\ast \|^2.$$ We recover the usual rates in \(O(1/n)\), with \(t = \gamma n\), with the same caveat as above (the step-size needs to be bounded).</p>



<h2>Conclusion</h2>



<p class="justify-text">In this blog post, I covered the basic aspects of gradient flows, in particular their relationships with various forms of gradient descent, and their use in obtaining simple convergence justifications. Next months, I will cover extensions of the analyses above, in particular in terms of (1) acceleration for convex functions, where several flows and discretizations are interesting beyond the gradient flow and Euler method [12, 15], and (2) another class of functions which includes non-convex functions as encountered when learning with neural networks [16].</p>



<h2>References</h2>



<p class="justify-text">[1] Augustin Louis Cauchy. <a href="http://gallica.bnf.fr/ark:/12148/bpt6k90190w/f406">Méthode générale pour la résolution des systèmes d’équations simultanées</a>. Compte Rendu à l’Académie des Sciences, 25:536–538, 1847.<br/>[2] Claude Lemaréchal. <a href="https://www.math.uni-bielefeld.de/documenta/vol-ismp/40_lemarechal-claude.pdf">Cauchy and the Gradient Method</a>. <em>Documenta Mathematica</em>, <a href="https://www.math.uni-bielefeld.de/documenta/vol-ismp/vol-ismp.html">Extra Volume: Optimization Stories</a>, 251–254, 2012.<br/>[3] Yurii Nesterov. <em>Introductory lectures on convex optimization: A basic course</em> (Vol. 87). Springer Science &amp; Business Media, 2013.<br/>[4] Dimitri P. Bertsekas, <em>Nonlinear programming</em>. Athena Scientific, 1999.<br/>[5] Jorge Nocedal and Stephen Wright. <em>Numerical optimization</em>. Springer Science &amp; Business Media, 2006.<br/>[6] Arnak S. Dalalyan. <a href="https://rss.onlinelibrary.wiley.com/doi/epdf/10.1111/rssb.12183">Theoretical guarantees for approximate sampling from smooth and log‐concave densities</a>. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 79(3), 651-676, 2017.<br/>[7] Filippo Santambrogio. <a href="https://link.springer.com/content/pdf/10.1007/s13373-017-0101-1.pdf">{Euclidean, metric, and Wasserstein} gradient flows: an overview</a>. <em>Bulletin of Mathematical Sciences</em>, <em>7</em>(1), 87-154, 2017.<br/>[8] Stanislaw Lojasiewicz. Sur les trajectoires du gradient d’une fonction analytique. <em>Seminari di Geometria</em>, 1983:115–117, 1982.<br/>[9] Jérôme Bolte, Aris Daniilidis, and Adrian Lewis. <a href="https://www.sciencedirect.com/sdfe/reader/pii/S0022247X05006864/pdf">A nonsmooth Morse–Sard theorem for subanalytic functions</a>. <em>Journal of Mathematical Analysis and Applications</em>, 321(2):729–740, 2006.<br/>[10] Jason D. Lee, Max Simchowitz, Michael I. Jordan, Benjamin Recht. <a href="http://www.jmlr.org/proceedings/papers/v49/lee16.pdf">Gradient descent only converges to minimizers</a>. <em>Conference on learning theory</em>, 1246-1257, 2016.<br/>[11] Simon S. Du, Chi Jin, Jason D. Lee, Michael I. Jordan, Barnabas Poczos, Aarti Singh. <a href="https://papers.nips.cc/paper/6707-gradient-descent-can-take-exponential-time-to-escape-saddle-points.pdf">Gradient descent can take exponential time to escape saddle points</a>. <em>Advances in neural information processing systems</em>, 1067-1077, 2017.<br/>[12] Damien Scieur, Vincent Roulet, Francis Bach, Alexandre d’Aspremont,. <a href="http://papers.nips.cc/paper/6711-integration-methods-and-optimization-algorithms.pdf">Integration methods and optimization algorithms</a>. <em>Advances in Neural Information Processing Systems</em>, 1109-1118, 2017.<br/>[13] Hamed Karimi, Julie Nutini, Mark Schmidt. <a href="https://arxiv.org/pdf/1608.04636">Linear convergence of gradient and proximal-gradient methods under the Polyak-Lojasiewicz condition</a>. <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</em>, 795-811, 2016.<br/>[14] Boris T. Polyak. <a href="http://www.mathnet.ru/php/getFT.phtml?jrnid=zvmmf&amp;paperid=7813&amp;what=fullt&amp;option_lang=eng">Gradient methods for minimizing functionals</a>. <em>Zh. Vychisl. Mat. Mat. Fiz.</em>, 3(4):643–653, 1963. <br/>[15] Weijie Su, Stephen Boyd, Emmanuel J. Candès. <a href="http://www.jmlr.org/papers/volume17/15-084/15-084.pdf">A differential equation for modeling Nesterov’s accelerated gradient method: theory and insights</a>. <em>Journal of Machine Learning Research</em>, 17(1), 5312-5354, 2017.<br/>[16] Lénaïc Chizat, Francis Bach. <a href="http://papers.nips.cc/paper/7567-on-the-global-convergence-of-gradient-descent-for-over-parameterized-models-using-optimal-transport.pdf">On the global convergence of gradient descent for over-parameterized models using optimal transport</a>. <em>Advances in Neural Information Processing Systems</em>, 3036-3046, 2018.</p>



<h2>Limits of stochastic gradient descent for vanishing step-sizes</h2>



<p class="justify-text"><strong>Convergence to gradient flow. </strong>We consider fixed times \(t = n \gamma \) and \(s = m \gamma\), and we let \(\gamma\) tend to zero, with thus \(m\) and \(n\) tending to infinity. Starting from the recursion $$x_{n+1} = x_{n}\, – \gamma \nabla f(x_{n})\  – \gamma \varepsilon_n,$$ we get the following by applying it \(m\) times: $$X(t+s) \ – X(t) = x_{n+m}-x_n = \ – \gamma \sum_{k=0}^{m-1} \nabla f\Big(X\Big(t+\frac{sk}{m}\Big)\Big)\  – \gamma \sum_{k=0}^{m-1} \varepsilon_{k+n}.$$ The term \(\displaystyle \gamma \sum_{k=0}^{m-1} \nabla f\Big(X\Big(t+\frac{sk}{m}\Big)\Big)\) converges to \(\displaystyle \int_{t}^{t+s}\!\!\! \nabla f(X(u)) du\), while the term \(\gamma \sum_{k=0}^{m-1} \varepsilon_{k+n}\) has zero expectation and variance equal to \(\gamma^2 m = \gamma s \) times the variance of each \(\varepsilon_{k+n}\), and thus it tends to zero (since \(\gamma\) tends to zero). Thus, in the limit, $$X(t+s)\  – X(t) = \ – \int_{t}^{t+s} \!\!\! \nabla f(X(u)) du,$$ which is equivalent to the gradient flow equation.</p>



<p class="justify-text"><strong>Convergence to diffusion.</strong> We consider the recursion $$x_{n+1} = x_{n}\,  – \gamma \nabla f(x_{n}) + \sqrt{2\gamma} \varepsilon_n.$$ With the same argument as above, we now get $$X(t+s) \ – X(t) = x_{n+m}-x_n =\ – \gamma \sum_{k=0}^{m-1} \nabla f\Big(X\Big(t+\frac{sk}{m}\Big)\Big)\ – \sqrt{2\gamma} \sum_{k=0}^{m-1} \varepsilon_{k+n}.$$ Now the second term has zero mean but a variance proportional to \(2s\) (<em>which does not go to zero when \(\gamma\) goes to zero</em>). We can then use when \(m\) tends to infinity the <a href="https://en.wikipedia.org/wiki/Wiener_process#Wiener_process_as_a_limit_of_random_walk">limit of the sum of independent variables as a Wiener process</a>, to get $$X(t+s)\ – X(t) =\  – \int_{t}^{t+s} \!\!\! \nabla f(X(u)) du + \sqrt{2} \big[ B(t+s)-B(t) \big].$$ The <a href="https://en.wikipedia.org/wiki/It%C3%B4_diffusion#Invariant_measures">limiting distribution</a> of \(X(t)\) happens to be the so-called <a href="https://en.wikipedia.org/wiki/Gibbs_measure">Gibbs</a> distribution, with density \(\exp(-f(x))\) (the factor of \(\sqrt{2}\) was added to avoid an extra constant factor in the Gibbs distribution). More on this in a future post.</p></div>
    </content>
    <updated>2020-05-01T05:15:04Z</updated>
    <published>2020-05-01T05:15:04Z</published>
    <category term="Machine learning"/>
    <category term="Tools"/>
    <author>
      <name>Francis Bach</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2020-05-09T06:22:52Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=428</id>
    <link href="https://tcsplus.wordpress.com/2020/04/30/tcs-talk-wednesday-may-6-nathan-klein-university-of-washington/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, May 6 — Nathan Klein, University of Washington</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, May 6th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Nathan Klein from the University of Washington will speak about “An improved approximation algorithm for TSP in the half integral case” (abstract below). You can reserve a spot […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, May 6th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <strong>Nathan Klein</strong> from the University of Washington will speak about “<em>An improved approximation algorithm for TSP in the half integral case” (abstract below).</em></p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The link to the YouTube livestream will also be posted <a href="https://sites.google.com/site/plustcs/livetalk">on our<br/>
website</a> on the day of the talk, so people who did not sign up will still be able to watch the talk live.) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a>suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: A classic result from Christofides in the 70s tells us that a fast algorithm for the traveling salesperson problem (TSP) exists which returns a solution at most 3/2 times worse than the optimal. Since then, however, no better approximation algorithm has been found. In this talk, I will give an overview of research towards the goal of beating 3/2 and will present the first sub-3/2 approximation algorithm for the special case of “half integral” TSP instances. These instances have received significant attention in part due to a conjecture from Schalekamp, Williamson and van Zuylen that they attain the integrality gap of the subtour polytope. If this conjecture is true, our work shows that the integrality gap of the polytope is bounded away from 3/2, giving hope for an improved approximation for the general case. This presentation is of joint work with Anna Karlin and Shayan Oveis Gharan.</p></blockquote></div>
    </content>
    <updated>2020-05-01T03:10:05Z</updated>
    <published>2020-05-01T03:10:05Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2020-05-09T06:21:45Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/04/30/linkage</id>
    <link href="https://11011110.github.io/blog/2020/04/30/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Goodbye Insta (). I don’t have an Instagram account. I don’t want to join yet another closed system for capturing my data and sending it to a corporation. But there were a few people whose Instagram accounts I would check out semi-regularly. No longer. Now Instagram won’t show me any posts not-logged-in. If you’re going to fence yourself off from the Internet, then you’re fencing yourself off from me. If you think this is going to encourage me to make an account, the opposite is true.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p>Goodbye Insta (<a href="https://mathstodon.xyz/@11011110/104009624456399772"/>). I don’t have an Instagram account. I don’t want to join yet another closed system for capturing my data and sending it to a corporation. But there were a few people whose Instagram accounts I would check out semi-regularly. No longer. Now Instagram won’t show me any posts not-logged-in. If you’re going to fence yourself off from the Internet, then you’re fencing yourself off from me. If you think this is going to encourage me to make an account, the opposite is true.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2004.07630">Four pages are indeed necessary for planar graphs</a> (<a href="https://mathstodon.xyz/@11011110/104016197893535979"/>). At STOC 1986, Yannakakis proved that planar graphs have 4-page <a href="https://en.wikipedia.org/wiki/Book_embedding">book embeddings</a>, and announced an example requiring 4 pages, but never published the example. Finally now Bekos et al. have provided detailed constructions for planar graphs requiring 4 pages. Still lost in limbo: Unger’s claim from 1992 that testing 3-page embeddability with fixed vertex ordering is polynomial.</p>
  </li>
  <li>
    <p><a href="https://www.latimes.com/california/story/2020-04-16/uc-reeling-under-staggering-coronavirus-costs-the-worst-impacts-all-at-once">Universities are starting to see the costs of the lockdown</a> (<a href="https://mathstodon.xyz/@11011110/104023169399231809"/>), in lost revenue from students and medical centers and extra expenses from the transition to remote learning — the linked story is on the University of California, but other universities are likely in similar or worse shape. So far my campus has not announced any specific cuts but colleagues predict that a hiring freeze, at least, is likely to come.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2003.13777">Subgraph densities in a surface</a> (<a href="https://mathstodon.xyz/@11011110/104035284995793475"/>). In 1993 I published a paper “<a href="https://www.ics.uci.edu/~eppstein/pubs/Epp-JGT-93.pdf">Connectivity, graph minors, and subgraph multiplicity</a>” showing that a planar graph  can have at most linearly many copies in larger planar graphs if and only if  is 3-connected. I thought it was long-forgotten, but now Huynh, Joret and Wood have generalized it in two ways: other surfaces than the plane, and other exponents than one in the number of copies.</p>
  </li>
  <li>
    <p><a href="https://github.com/microsoft/STL/pull/724">Fix boyer_moore_searcher with the Rytter correction</a> (<a href="https://mathstodon.xyz/@11011110/104040417854587667"/>, <a href="https://news.ycombinator.com/item?id=22895932">via</a>). A 40-year-old bugfix to an even older linear-time string matching algorithm finally makes it to production code, with an admonishment that this should have been mentioned in more recent explanations of the algorithm such as <a href="https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string-search_algorithm">Wikipedia’s</a> (since added).</p>
  </li>
  <li>
    <p><a href="https://gilkalai.wordpress.com/2020/04/19/to-cheer-you-up-in-difficult-times-ii-mysterious-matching-news-by-gal-beniamini-naom-nisan-vijay-vazirani-and-thorben-trobst/">To cheer you up in difficult times II: Mysterious matching news</a> (<a href="https://mathstodon.xyz/@11011110/104046659532923035"/>). Gil Kalai blogs about two recent papers: one by Gal Beniamini and Noam Nisan on <a href="https://arxiv.org/abs/2001.07642">a polynomial that is one for bipartite graphs with perfect matchings and zero otherwise</a> and one by UCI colleagues Vijay Vazirani and Thorben Tröbst <a href="https://arxiv.org/abs/2003.08917">extending it to test whether a subgraph of a weighted complete bipartite graph contains a minimum-weight perfect matching</a>.</p>
  </li>
  <li>
    <p><a href="https://inkscape.org/release/inkscape-1.0rc1/">Inkscape 1.0 (release candidate) now runs natively on OS X</a> (<a href="https://mathstodon.xyz/@11011110/104052350948366314"/>, <a href="https://news.ycombinator.com/item?id=22855357">via</a>). I still haven’t tried it, and the discussion suggests it still needs some tuning. But it seems pretty popular in free-software circles, and it’s good to know that there are free vector drawing programs out there that can compete with the (expensive) one I use, Adobe Illustrator.</p>
  </li>
  <li>
    <p>Antoine Chambert-Loir <a href="https://mathstodon.xyz/@antoinechambertloir/104018917625130123">asks for an explanation of the Schläfli graph’s Hamiltonicity</a>, or more specifically how <a href="https://en.wikipedia.org/wiki/Schl%C3%A4fli_graph">the Wikipedia article’s</a> infobox illustration can be related to more standard constructions of the graph.</p>
  </li>
  <li>
    <p><a href="https://aperiodical.com/2020/04/the-big-lock-down-math-off-match-5/">The eggbox puzzle</a> (<a href="https://mathstodon.xyz/@11011110/104063021972727214"/>). <em>The Aperiodical</em> has been posting “Big Lock-Down Math-Off” posts, double-headers with a vote for which “made you say Aha! the loudest”. I chose this one with James Munro’s eggbox puzzle because the illustrations made me say Aha! This state space is a <a href="https://en.wikipedia.org/wiki/Median_graph">median graph</a>!</p>

    <p>To find the median of three -egg placements, put the th egg in the median of its three positions. In fact, it’s a distributive lattice where the meet of two placements is to move each egg to the rightmost of its two positions and the join is to move each egg to the leftmost of the two. Of course that doesn’t help much in solving the actual puzzle…</p>
  </li>
  <li>
    <p><a href="https://listserv.umd.edu/cgi-bin/wa?A2=ind2004&amp;L=CAST10&amp;P=3191">Another MDPI journal editorial board resigns</a> (<a href="https://mathstodon.xyz/@11011110/104068456940158118"/>; 
“another” because of the 2018 <em>Nutrients</em> mass resignation). The topic appears to be related to chemical engineering. The resigning editor-in-chief describes the reason: “MDPI has stated that it will not modify the current plan for rapid, quota-driven growth, while the Editorial Board will not compromise its overarching goals of publication quality and scholarly contribution.”</p>
  </li>
  <li>
    <p>I had been using Wunderlist for to-do lists of review/submit deadlines, a shared family grocery list, etc but <a href="https://11011110.github.io/blog/2020/03/15/stay-home-linkage.html">as I posted earlier</a>, it is being strangled by new owner Microsoft to push you to their other thing. So after comparing other cross-platform shareable to-do-list apps I chose <a href="https://todoist.com/">todoist</a> because of its similar workflow and <a href="https://todoist.com/import/wunderlist">Wunderlist import feature</a> (<a href="https://mathstodon.xyz/@11011110/104074471851344995"/>). Close second was Tom Hull’s suggestion, Trello. Not as close: Microsoft.</p>
  </li>
  <li>
    <p><a href="http://statnet.org/COVID-JustOneFriend/">Can’t I please just visit one friend?</a> (<a href="https://mathstodon.xyz/@11011110/104085917246234853"/>) Or, how graph drawing helps us understand the importance of maintaining strict isolation.</p>
  </li>
  <li>
    <p><a href="https://www.icann.org/news/blog/icann-board-withholds-consent-for-a-change-of-control-of-the-public-interest-registry-pir">ICANN blocks .org sale</a> (<a href="https://mathstodon.xyz/@11011110/104091255847507444"/>, <a href="https://news.ycombinator.com/item?id=23038637">via</a>). This is very good news, and follows onto their <a href="https://www.theregister.co.uk/2020/04/17/icann_california_org_sale_delay/">delay of the sale a couple of weeks ago after the intervention of the California attorney general</a>.</p>
  </li>
</ul></div>
    </content>
    <updated>2020-04-30T21:37:00Z</updated>
    <published>2020-04-30T21:37:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-05-04T05:19:55Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-27705661.post-2580044341116607459</id>
    <link href="http://processalgebra.blogspot.com/feeds/2580044341116607459/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=27705661&amp;postID=2580044341116607459" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/2580044341116607459" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/27705661/posts/default/2580044341116607459" rel="self" type="application/atom+xml"/>
    <link href="http://processalgebra.blogspot.com/2020/04/an-interview-with-rob-van-glabbeek.html" rel="alternate" type="text/html"/>
    <title>An interview with Rob van Glabbeek, CONCUR Test-of-Time Award recipient</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This post is devoted to the third interview with the <a href="https://concur2020.forsyte.at/test-of-time/index.html">colleagues</a> who were selected for the first edition of the CONCUR  Test-of-Time Award. (See <a href="https://processalgebra.blogspot.com/2020/04/an-interview-with-davide-sangiorgi.html">here</a> for the interview with <a href="http://www.cs.unibo.it/~sangio/">Davide Sangiorgi</a>, and <a href="https://processalgebra.blogspot.com/2020/04/an-interview-with-nancy-lynch-and.html">here</a> for the interview with <a href="https://people.csail.mit.edu/lynch/">Nancy Lynch</a> and <a href="http://profs.sci.univr.it/~segala/">Roberto Segala</a>.) I asked <a href="http://theory.stanford.edu/~rvg/">Rob van Glabbeek</a> (Data61, CSIRO, Sydney, Australia) a few questions via email and I am happy share his answers with readers of this blog below. Rob's words brought me back to the time when the CONCUR conference started and our field of concurrency theory was roughly a decade old. I hope that his interesting account and opinions will inspire young researchers in concurrency theory of all ages.<br/><br/>Luca: You receive one of the two CONCUR ToT Awards for the period  1990-1993 for your companion papers on "The Linear Time-Branching Time  Spectrum", published at CONCUR 1990 and 1993. Could you tell us briefly  what spurred you to  embark in the encyclopaedic study of process semantics you developed in  those two papers and what were your main sources of inspiration?<br/><br/>Rob: My PhD supervisors, Jan Willem Klop and Jan Bergstra, were examining<br/>bisimulation semantics, but also failures semantics, in collaboration<br/>with Ernst-Ruediger Olderog, and at some point, together with Jos Baeten,<br/>wrote a paper on ready-trace semantics to deal with a priority operator.<br/>I had a need to see these activities, and also those of Tony Hoare and<br/>Robin Milner, as revealing different aspects of the same greater whole,<br/>rather than as isolated research efforts. This led naturally to an<br/>abstraction of their work in which the crucial difference was the semantic<br/>equivalence employed. Ordering these semantics in a lattice was never<br/>optional for me, but necessary to see the larger picture.  It may be in<br/>my nature to relate different solutions to similar problems I encounter<br/>by placing them in lattices.<br/><span class="im"/> <br/>Luca: Did you imagine at the time that the papers on "The Linear Time-Branching Time Spectrum" would have so much impact? <br/><div><br/></div><div>Rob: I didn't think much about impact in those days. I wrote the papers<br/>because I felt the need to do so, and given that, could just as well<br/>publish them. Had I made an estimate, I would have guessed that my<br/>papers would have impact, as I imagined that things that I find<br/>necessary to know would interest some others too. But I would have<br/>underestimated the amount of impact, in part because I had grossly<br/>underestimated the size of the research community eventually working<br/>on related matters.<span class="im"><br/></span></div><div><br/></div><div>Luca: In your papers, you present characterizations of process semantics in  relational form, via testing scenarios, via modal logics and in terms of  (conditional) equational axiomatizations. Of all those  characterizations, which ones do you think have been most useful in  future work? To your mind, what are the most  interesting or unexpected uses in the  literature of the notions and techniques you developed in the "The  Linear Time-Branching Time Spectrum"? </div><div> </div><div>Rob: I think that all these characterizations together shed the light on<br/>process semantics that makes them understandable and useful.<br/>Relational characterizations, i.e., of (bi)simulations, are often the<br/>most useful to grasp what kind of relation one has, and this is essential<br/>for any use. But modal characterizations are intimately linked with<br/>model checking, which is one of the more successful verification tools.<br/>Equational characterizations also find practical use in verification,<br/>e.g., in normalization. Yet, they are most useful in illustrating<br/>the essential difference between process semantics, and thus form a guide<br/>in choosing the right one for an application. To me, the testing scenarios<br/>are the most useful in weighing how realistic certain semantic<br/>identifications are in certain application contexts.</div><div><span class="im"/> </div><div>Luca: How much of your later work has  built on your award-winning papers? What follow-up results of yours are  you most proud of and why?</div><div> </div><div>Rob: About one third, I think.</div><div><ul><li>My work on action refinement, together with Ursula Goltz, extends theclassification of process semantics in a direction orthogonal to the linear time - branching time spectrum. I am proud of this work mostly because, for a distinct class of applications, action refinement is great criterion to tell which semantics equivalences one ought to prefer over others. These process semantics have also been used in later work on distributability with Ursula and with Jens-Wolfhard Schicke Uffmann. Distributability tells us which distributed systems can be cast as sequential systems cooperating asynchronously, and which fundamentally cannot.</li><li>I wrote several papers on Structural Operational Semantics, many jointly with Wan Fokkink, aiming to obtain congruence results for semantic equivalences. These works carefully employed the modal characterizations from my spectrum papers. I consider this work important because congruence properties are essential in verification, as we need compositionality to avoid state explosions. </li><li>I used a version of failure simulation semantics from my second spectrum paper in work with Yuxin Deng, Matthew Hennessy, Carroll Morgan and Chenyi Zhang, characterizing may- and must testing equivalences for probabilistic nondeterministic processes. Probability and nondeterminism interact in many applications, and I think that may- and must testing are the right tools to single out the essence of their semantics.</li></ul></div><div><span class="im"><br/></span> </div><div>Luca: If I remember correctly, your master thesis was in pure mathematics. Why  did you decide to move to a PhD in computer science? What was it like  to work as a PhD student at CWI in the late 1980s? How did you start  your collaborations with Jos Baeten and fellow PhD student Frits  Vaandrager, amongst others? </div><div> </div><div>Rob: I was interested in mathematical logic already since high school,<br/>after winning a book on the foundations of logic set theory in a<br/>mathematics Olympiad.  I found the material in my logic course,<br/>thought at the University of Leiden by Jan Bergstra, fascinating, and<br/>when doing a related exam at CWI, where Jan had moved to at that<br/>time, and he asked me to do a PhD on related subjects, I say no reason<br/>not to. But he had to convince me first that my work would not involve<br/>any computer science, as my student advisor at The University of<br/>Leiden, Professor Claas, had told us that this was not a real<br/>science. Fortunately, Jan had no problem passing that hurdle.<br/><br/>I was the third member of the team of Jan Bergstra and Jan Willem<br/>Klop, after Jos Baeten had been recruited a few months earlier, but in<br/>a more senior position. Frits arrived a month later than me. As we<br/>were exploring similar subjects, and shared an office, it was natural<br/>to collaborate. As these collaborations worked out very well, they<br/>became quite strong.<br/><br/>CWI was a marvelous environment in the late 1980s. Our group gradually<br/>grew beyond a dozen members, but there remained a much greater<br/>cohesion then I have seen anywhere else. We had weekly process<br/>algebra meetings, where each of us shared our recent ideas with others<br/>in the group. At those meetings, if one member presented a proof, the<br/>entire audience followed it step by step, contributing meaningfully<br/>where needed, and not a single error went unnoticed.</div><div><span class="im"><br/></span> </div><div>Luca: Already as PhD student, you took up leadership roles within the  community. For instance, I remember attending a workshop on "Combining  Compositionality and Concurrency" <span>in March 1988,</span> which you  co-organized with Ursula Goltz and Ernst-Ruediger Olderog. Would you  recommend to a PhD student that he/she become involved in that kind of  activities early on in their career? Do you think that doing so had a  positive effect on your future career?  </div><div> </div><div>Rob: Generally I recommend this. Although I co-organized this workshop<br/>because the circumstances were right for it, and the need great; not<br/>because I felt it necessary to organize things.  My advice to PhD<br/>students would not be specifically to add a leadership component to<br/>their CV regardless of the circumstances, but instead do take such an<br/>opportunity when the time is right, and not be shy because of lack of<br/>experience.</div><div> </div><div>This workshop definitively had a positive effect on my future career.<br/>When finishing my PhD I inquired at Stanford and MIT for a possible<br/>post-doc position, and the good impressions I had left at this<br/>workshop were a factor in getting offers from both institutes.<br/>In fact, Vaughan Pratt, who was deeply impressed with work I had<br/>presented at this workshop, convinced me to apply straight away for an<br/>assistant professorship, and this led to my job at Stanford.   </div><div><span class="im"><br/></span> </div><div>Luca: I still have vivid memories of your talk at the first CONCUR conference  in 1990 in Amsterdam, where you introduced notions of black-box  experiments on processes using an actual box and beer-glass mats. You  have attended and contributed to many of the editions of the conference  since then. To your mind, how has the focus of CONCUR changed since its  first edition in 1990? </div><div> </div><div>Rob: I think the focus on foundations has shifted somewhat to more applied<br/>topics, and the scope of "concurrency theory" has broadened significantly.<br/>Still, many topics from the earlier CONCURs are still celebrated today.</div><div><span class="im"><br/></span> </div><div>Luca: The last forty years have seen a huge amount of work on process algebra  and process calculi. However, the emphasis on that field of research  within concurrency theory seems to have diminished over the last few  years, even in Europe. What advice would you give to a young researcher  interested in working  on process calculi today? </div><div> </div><div>Rob: Until 10 years ago, many people I met in industry voiced the opinion<br/>that formals methods in general, and process algebra in particular,<br/>has been tried in the previous century, and had been found to not to<br/>scale to tackle practical problems. But more recently this opinion is<br/>on the way out, in part because it became clear that the kind of<br/>problems solved by formal methods are much more pressing then originally<br/>believed, and in part because many of the applications that were<br/>deemed to hard in the past, are now being addressed quite adequately.<br/>For these reasons, I think work on process calculi is still a good bet<br/>for a PhD study, although a real-world application motivating the<br/>theory studied is perhaps harder needed than in the past, and toy<br/>examples just to illustrate the theory are not always enough.</div><div><span class="im"><br/></span> </div><div>Luca: What are the research topics that currently excite you the most?</div><div> </div><div>Rob: I am very excited about showing liveness properties, in Lamport's<br/>sense, telling that a modeled system eventually achieves a desirable outcome.<br/>And even more in creating process algebraic verification frameworks<br/>that are in principle able to do this. I have come to believe that<br/>traditional approaches popular in process algebra and temporal logic<br/>can only deal with liveness properties when making fairness<br/>assumptions that are too strong and lead to unwarranted conclusions.<br/>Together with Peter Hoefner I have been advocating a weaker concept of<br/>fairness, that we call justness, that is much more suitable as a basis<br/>for formulating liveness properties. Embedding this concept into the<br/>conceptual foundations of process algebra and concurrency theory, in<br/>such a way that it can be effectively used in verification, is for a me<br/>a challenging task, involving many exciting open questions.<br/><br/>A vaguely related subject that excites me since the end of last year,<br/>is the extension of standard process algebra with a time-out operator,<br/>while still abstaining from quantifying time. I believe that such an<br/>extension reaches exactly the right level of expressiveness necessary<br/>for many applications.<br/><br/>Both topics also relate with the impossibility of correctly expressing<br/>expressing mutual exclusion in standard process algebras, a discovery<br/>that called forth many philosophical questions, such as under which<br/>assumptions on our hardware is mutual exclusion, when following the<br/>formal definitions of Dijkstra and others, even theoretically implementable.</div><div> </div></div>
    </content>
    <updated>2020-04-30T21:16:00Z</updated>
    <published>2020-04-30T21:16:00Z</published>
    <author>
      <name>Luca Aceto</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/01092671728833265127</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-27705661</id>
      <author>
        <name>Luca Aceto</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/01092671728833265127</uri>
      </author>
      <link href="http://processalgebra.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://processalgebra.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/27705661/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Papers I find interesting---mostly, but not solely, in Process Algebra---, and some fun stuff in Mathematics and Computer Science at large and on general issues related to research, teaching and academic life.</subtitle>
      <title>Process Algebra Diary</title>
      <updated>2020-05-07T08:59:41Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-7481748881751465174</id>
    <link href="https://blog.computationalcomplexity.org/feeds/7481748881751465174/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/04/a-guest-blog-on-pandemics-affect-on.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7481748881751465174" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7481748881751465174" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/04/a-guest-blog-on-pandemics-affect-on.html" rel="alternate" type="text/html"/>
    <title>A Guest Blog on the Pandemic's affect on disability students</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div>I asked my Grad Ramsey Theory class to email me about whatever thoughts they have on the pandemic that they want to share with the world, with the intend of making some of them into a blog post. I thought there would be several short thoughts for one post. And I may still do that post. But I got a FANTASTIC long answer from one Emily Mae Kaplitz. Normally I would ask to shorten or edit a guest post, but I didn't do that here since that might make it less authentic.</div><div><br/></div><div>Here is Emily Kaplitz's email (with her enthusiastic permission)</div><div><br/></div><div>--------------------------------------------------------------------------------------</div><div><br/></div><div>Ok so this might be super ranty, (It definitely is.) but I think it is super important to bring up in a blog post written by an academic that will be probably read by other academics. </div><div><br/></div><div>The students that are being most affected by this pandemic with online learning are disability students. As a disability student, we carefully cultivate the way that we learn best based off of years of trial and error. This is harder than anything else, we have to face in our lifetime. Most of the time disability students are left on the back burner and that statement is so much more prevalent right now. My friends brother is autistic. He is struggling so much right now because he is at home. Disability students learn what environment works best for them and at home is usually not the best place. We have to split our lives into different boxes that each have different tools to help us get our brains to focus and work well when we need them too. Disability students will rely on everything being planned out, so that they can succeed. Teachers and professors cannot understand the stress and strain that having to work at home puts on the student. Every time I go to another school, it is a struggle to figure out what new thing I need to add into the mix and what old thing I need to throw away. It's exhausting, but when I go from one school to another I at least know that the basics are the same. I sit in a classroom, the professors lecturer, and then I do work at home that is assigned to me. Changing to online changes that dynamic so much. A professor cannot see when a student is visibly struggling with a topic because we'll all behind computers. A neurotypical person might ask, "well why don't you just ask a question? Why don't you just let the professor know that you don't understand". Let me answer that simply. If all your life you've been silenced because of something that you cannot control, is your first reaction to speak out or to stay silent. It is so hard for disability students to ask a question after we've been labeled the dumb kid. Every time we ask a question, we always have the thought of: is this going to make me sound stupid. We've worked so hard to eliminate that word from our vocabulary and from others who will throw that word back at us. Disability students are being left in the hands of their parents and teachers/professors who do not understand us and our needs even if they try to or want to. It is so hard for us to explain what our normal is because we don't live your normal and therefore don't know the difference. Many disability students have their confidence slashed the moment they enter a classroom and realize that they are not like the other kids. Even more so because they don't understand why they aren't. Disability students are one of the most hard-working individuals when we have a cheerleader to cheer us on because it's hard. It's harder than anything anyone has to do. Because no one listens to you when you are stupid and no one cares for you if you're not easy to care for unless they are given a specific reason to. Fighting a losing battle every day is awful. Now imagine all of your weapons that you have carefully crafted over the years have been taken away and you are left defenseless. While we have things like ADS that are supposed to help support us, it's not enough. Just like putting a Band-Aid on an open infected wound will not be enough. Now more than ever we need to learn from this as academics. We need to learn that helping disability students does not only help disability students. It helps all students because all students learn differently. All students if given the chance can excel at any field that we put them in. We just have to figure out the best way to get that student to shine. That is one of the reasons why I am a PhD student right now. I saw in the tutoring center at my undergrad how many students came to me with so much frustration about something they are doing in class. Both students with disabilities and without. These students are constantly apologizing because they don't understand something. In one session by just changing the way that we talk about a subject the student was able to get it in less time than the professor taught it. I've had students come to me after an exam and tell me that the only reason they got the grade that they did was because in their head was my voice coaching them on a subject. We are not teaching optimally. We are teaching the way that it has been done for years and years and years and that is not the best way to teach. It might be the best way to teach the strongest links but really the link that matters the most is the weakest link that will snap under pressure because you can't pull a tractor with a broken link. Disability students think differently. Imagine how many impossible problems we can solve when we have people that think differently. But that's just my two cents as a disability student who is struggling and sees other disability students struggling every day. And really just wants to help all students succeed.</div><div><br/></div><div>I blame any misspellings, grammar errors, and run on sentences on my speech to text and text to speech. This was a long email and if we were on tumblr, I would post a potato at the end. Since we aren't, I will leave this email with this. Thank you for taking the time to read this rant. Even if you don't include this in your blog post, I believe one person reading this has made the difference.</div><div><br/></div><div>Thanks!</div><div><br/></div><div>Emily Mae Kaplitz</div></div>
    </content>
    <updated>2020-04-29T13:44:00Z</updated>
    <published>2020-04-29T13:44:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-05-08T21:30:51Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/04/28/cartesian-triangle-centers</id>
    <link href="https://11011110.github.io/blog/2020/04/28/cartesian-triangle-centers.html" rel="alternate" type="text/html"/>
    <title>Cartesian triangle centers</title>
    <summary>The usual definition of a Euclidean triangle center is a point defined from a triangle in a way that is equivariant under similarity transformations, meaning that applying the transformation to a triangle and then constructing its center produces the same point as constructing the center first and then transforming it. These include familiar points defined from triangles like the orthocenter (where perpendiculars to the sides meet), incenter (the center of the inscribed circle), and circumcenter (the center of the circumscribed circle). What I have in mind in this post is a similarly-defined concept, but with a different group of transformations than similarity: the combination of any two separate linear transformations to the two coordinate axes of the Cartesian plane, or transformations that swap the (transformed) axes. Another way of describing these transformations is that they preserve axis-parallel lines and relative area. They also preserve other lines and parallelism of lines.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The usual definition of a Euclidean <a href="https://en.wikipedia.org/wiki/Triangle_center">triangle center</a> is a point defined from a triangle in a way that is <a href="https://en.wikipedia.org/wiki/Equivariant_map">equivariant</a> under <a href="https://en.wikipedia.org/wiki/Similarity_(geometry)">similarity transformations</a>, meaning that applying the transformation to a triangle and then constructing its center produces the same point as constructing the center first and then transforming it. These include familiar points defined from triangles like the <a href="https://en.wikipedia.org/wiki/Orthocenter">orthocenter</a> (where perpendiculars to the sides meet), <a href="https://en.wikipedia.org/wiki/Incenter">incenter</a> (the center of the inscribed circle), and <a href="https://en.wikipedia.org/wiki/Circumcenter">circumcenter</a> (the center of the circumscribed circle). What I have in mind in this post is a similarly-defined concept, but with a different group of transformations than similarity: the combination of any two separate linear transformations to the two coordinate axes of the <a href="https://en.wikipedia.org/wiki/Cartesian_plane">Cartesian plane</a>, or transformations that swap the (transformed) axes. Another way of describing these transformations is that they preserve axis-parallel lines and relative area. They also preserve other lines and parallelism of lines.</p>

<p>One of the standard Euclidean triangle centers turns out to be a Cartesian center as well: the centroid has as its coordinates the average of the three triangle vertex coordinates, and this coordinatewise calculation (independent of the ordering of the three vertices) shows it to be equivariant under Cartesian transformations. It is also the point where the three lines through each vertex and opposite side midpoint meet.</p>

<p style="text-align: center;"><img alt="Centroid of a triangle" src="https://11011110.github.io/blog/assets/2020/centroid.svg"/></p>

<p>Similarly, we can find a Cartesian triangle center from the coordinatewise median of the three vertices.</p>

<p style="text-align: center;"><img alt="Median of a triangle" src="https://11011110.github.io/blog/assets/2020/median.svg"/></p>

<p>The center of the axis-parallel bounding box is also a Cartesian triangle center, again computed coordinatewise as the average of the minimum and maximum coordinates.</p>

<p style="text-align: center;"><img alt="Bounding-box center of a triangle" src="https://11011110.github.io/blog/assets/2020/bbcenter.svg"/></p>

<p>But a Cartesian triangle center does not have to be determined coordinatewise in this way. As an example, denote the three triangle vertices by  and consider the point  with the property that the axis-parallel bounding boxes of the three line segments  all have equal area. For two points  and , the locus of points for which the two bounding boxes have equal area is a line through the other two corners of the bounding box of segment . (To see this, consider a linear transformation that takes this bounding box to a square, and apply symmetry.) The three lines defined in this way from the three pairs of vertices of a non-degenerate triangle always meet in a point, the center we are seeking. (This follows from the shared equality of the three rectangles, but it is also an instance of the dual of <a href="https://en.wikipedia.org/wiki/Pappus%27s_hexagon_theorem">Pappus’s theorem</a>.) And this point is equivariant under Cartesian transformations because these transformations preserve the ratios of areas of bounding rectangles.</p>

<p style="text-align: center;"><img alt="Point of equal bounding boxes of a triangle" src="https://11011110.github.io/blog/assets/2020/equalbb.svg"/></p>

<p>You can tell it’s not a coordinatewise center because, in the figure above, the triangle vertices have equally spaced -coordinates, from which it follows that all coordinatewise centers lie on the horizontal line with median -coordinate, but this center doesn’t.</p>

<p>In general, the geometry of the plane with Cartesian instead of Euclidean transformations does not seem to have been very thoroughly explored. These examples show, I think, that there are interesting things to find there.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/104079992540248342">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2020-04-28T21:24:00Z</updated>
    <published>2020-04-28T21:24:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-05-04T05:19:55Z</updated>
    </source>
  </entry>
</feed>
