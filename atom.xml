<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-11-13T18:21:28Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/161</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/161" rel="alternate" type="text/html"/>
    <title>TR19-161 |  Hardness of Learning DNFs using Halfspaces | 

	Suprovat Ghoshal, 

	Rishi Saket</title>
    <summary>The problem of learning $t$-term DNF formulas (for $t = O(1)$) has been studied extensively in the PAC model since its introduction by Valiant (STOC 1984). A $t$-term DNF can be efficiently learnt using a $t$-term DNF only if $t = 1$ i.e., when it is an AND, while even weakly learning a $2$-term DNF using a constant term DNF was shown to be NP-hard by Khot and Saket (FOCS 2008). On the other hand, Feldman et al. (FOCS 2009) showed the hardness of weakly learning a noisy AND using a halfspace -- the latter being a generalization of an AND, while Khot and Saket (STOC 2008) showed that an intersection of two halfspaces is hard to weakly learn using any function of constantly many halfspaces. The question of whether a $2$-term DNF is efficiently learnable using $2$ or constantly many halfspaces remained open.
	In this work we answer this question in the negative by showing the hardness of weakly learning a $2$-term DNF as well as a noisy AND using any function of a constant number of halfspaces. In particular we prove the following. 
	For any constants $\nu, \zeta &gt; 0$ and $\ell \in \mathbb{N}$, given a distribution over point-value pairs $\{0,1\}^n \times \{0,1\}$, it is NP-hard to decide whether,
YES Case. There is a $2$-term DNF that classifies all the points of the distribution, and an AND that classifies at least $1-\zeta$ fraction of the points correctly.
NO Case. Any boolean function depending on at most $\ell$ halfspaces classifies at most $1/2 + \nu$ fraction of the points of the distribution correctly.
	Our result generalizes and strengthens the previous best results mentioned above on the hardness of learning a $2$-term DNF, learning an intersection of two halfspaces, and learning a noisy AND.</summary>
    <updated>2019-11-13T17:07:59Z</updated>
    <published>2019-11-13T17:07:59Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-11-13T18:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/11/13/phd-fellow-in-algorithms-at-university-of-copenhagen-apply-by-january-5-2020/</id>
    <link href="https://cstheory-jobs.org/2019/11/13/phd-fellow-in-algorithms-at-university-of-copenhagen-apply-by-january-5-2020/" rel="alternate" type="text/html"/>
    <title>PhD fellow in Algorithms at University of Copenhagen (apply by January 5, 2020)</title>
    <summary>The group is prolific at the topmost conferences like SODA, STOC, and FOCS, e.g., with 7 papers accepted for SODA’20. Adding to the excitement, we have the center Basic Algorithms Research Copenhagen (BARC) which also involves the IT University of Copenhagen. The aim is to attract top talent from around the world to an ambitious, […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The group is prolific at the topmost conferences like SODA, STOC, and FOCS, e.g., with 7 papers accepted for SODA’20. Adding to the excitement, we have the center Basic Algorithms Research Copenhagen (BARC) which also involves the IT University of Copenhagen. The aim is to attract top talent from around the world to an ambitious, creative, collaborative, and fun environment.</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=150628&amp;DepartmentId=18970&amp;MediaId=4642">https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=150628&amp;DepartmentId=18970&amp;MediaId=4642</a><br/>
Email: pbl@science.ku.dk</p></div>
    </content>
    <updated>2019-11-13T12:43:11Z</updated>
    <published>2019-11-13T12:43:11Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-11-13T18:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/11/13/postdoctoral-fellowships-of-algorithms-at-university-of-copenhagen-apply-by-january-5-2020/</id>
    <link href="https://cstheory-jobs.org/2019/11/13/postdoctoral-fellowships-of-algorithms-at-university-of-copenhagen-apply-by-january-5-2020/" rel="alternate" type="text/html"/>
    <title>Postdoctoral Fellowship(s) of Algorithms at University of Copenhagen (apply by January 5, 2020)</title>
    <summary>The Algorithms and Complexity Section. The group is prolific at the topmost conferences like SODA, STOC, and FOCS, e.g., with 7 papers accepted for SODA’20. Adding to the excitement, we have the center Basic Algorithms Research Copenhagen (BARC). The aim is to attract top talent from around the world to an ambitious, creative, collaborative, and […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Algorithms and Complexity Section. The group is prolific at the topmost conferences like SODA, STOC, and FOCS, e.g., with 7 papers accepted for SODA’20. Adding to the excitement, we have the center Basic Algorithms Research Copenhagen (BARC). The aim is to attract top talent from around the world to an ambitious, creative, collaborative, and fun environment.</p>
<p>Website: <a href="https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=150628&amp;DepartmentId=18970&amp;MediaId=4642">https://candidate.hr-manager.net/ApplicationInit.aspx?cid=1307&amp;ProjectId=150628&amp;DepartmentId=18970&amp;MediaId=4642</a><br/>
Email: pbl@science.ku.dk</p></div>
    </content>
    <updated>2019-11-13T12:38:17Z</updated>
    <published>2019-11-13T12:38:17Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-11-13T18:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=18435</id>
    <link href="https://gilkalai.wordpress.com/2019/11/13/gils-collegial-quantum-supremacy-skepticism-faq/" rel="alternate" type="text/html"/>
    <title>Gil’s Collegial Quantum Supremacy Skepticism FAQ</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The first 15 samples of Google’s  53 qubit  flagship quantum supremacy experiment!   After the sensationally successful Scott’s Supreme Quantum Superiority FAQ and Boaz’s inferior classical inferiority FAQ let me add my contribution, explaining my current skeptical view. (I was actually … <a href="https://gilkalai.wordpress.com/2019/11/13/gils-collegial-quantum-supremacy-skepticism-faq/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://gilkalai.files.wordpress.com/2019/11/s53.png"><img alt="" class="size-medium wp-image-18585 aligncenter" height="152" src="https://gilkalai.files.wordpress.com/2019/11/s53.png?w=300&amp;h=152" width="300"/></a><span style="color: #ff0000;">The first 15 samples of Google’s  53 qubit  flagship quantum supremacy experiment!  </span></p>
<p>After the sensationally successful <a href="https://www.scottaaronson.com/blog/?p=4317">Scott’s Supreme Quantum Superiority FAQ</a> and <a href="https://windowsontheory.org/2019/10/24/boazs-inferior-classical-inferiority-faq/">Boaz’s inferior classical inferiority FAQ</a> let me add my contribution, explaining my current skeptical view. (I was actually asked many of the questions below.) I also recommend <a href="https://rjlipton.wordpress.com/2019/10/27/quantum-supremacy-at-last/">Lipton and Regan’s post on the Google paper.</a></p>
<p><span style="color: #0000ff;">While much of the post will be familiar let me mention right away a new critique of the Google supremacy claim: One of the central claims in the Google experiment – that the fidelity (quality) of a complex circuit is very close to the product of the fidelity of its basic components –  qubit and gates, seems very improbable and this may shed serious doubts on the validity of the experiment and on its conclusions.</span></p>
<p>Before we start, a few links: For the amazing news on the threshold of random discrete structures, see <a href="https://gilkalai.wordpress.com/2019/10/30/amazing-keith-frankston-jeff-kahn-bhargav-narayanan-jinyoung-park-thresholds-versus-fractional-expectation-thresholds/">this post</a>.  Here is my <a href="https://gilkalai.wordpress.com/2019/09/23/quantum-computers-amazing-progress-google-ibm-and-extraordinary-but-probably-false-supremacy-claims-google/">first post</a> on Google matter. Let me recommend the paper <a href="https://www.ams.org/journals/notices/201910/rnoti-p1618.pdf">From Operator Algebras to Complexity Theory and Back</a> by Thomas Vidick. It is about a problem by Boris Tsirelson (related to various deep mathematics) and about connections to quantum computation. And just fresh on the arXiv, <a href="https://arxiv.org/abs/1911.03748">Quantum speedups need structure</a> by Nathan Keller, Ohad Klein, resolving the Aaronson-Ambainis Conjecture. Congrats to Nathan and Ohad!</p>
<p><span style="color: #993300;">And now, lets start.</span></p>
<p><span style="color: #0000ff;">So what is quantum supremacy? And what other things do we need to know in order to understand the claims regarding quantum computers?</span></p>
<p><strong>Quantum supremacy</strong>  is the ability of quantum computers to demonstrate computations that classical computers cannot demonstrate. (Or that it is very very hard for classical computers  to demonstrate.)</p>
<p><strong>Quantum error correcting codes</strong> are certain quantum gadgets that a quantum computer needs to create that will be used as building blocks for larger quantum computers.</p>
<p><strong>A sampling task</strong> is  a task where the computer (quantum or classic) produces samples from a certain probability distribution <strong>D</strong>. Each sample is 0-1 vector of length n.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/11/john_martinis-300x256.jpg"><img alt="" class="alignnone size-full wp-image-18579" src="https://gilkalai.files.wordpress.com/2019/11/john_martinis-300x256.jpg?w=640"/></a></p>
<p><span style="color: #ff0000;"><strong>John Martinis</strong></span></p>
<h2>Google</h2>
<p><span style="color: #0000ff;">What did the Google team do?</span></p>
<p>The Google team produced a sample of a few millions 0-1 vectors of length 53 which is based on a certain “ideal” probability distribution <strong>D</strong>. They made two crucial claims regarding their sample</p>
<p>A) The statistical test for how close their sample is to the ideal distribution <strong>D</strong> will give a result above t=1/10,000</p>
<p>B) Producing a sample with similar statistical property will require 10,000 years on a supercomputer.</p>
<p>The probability distribution <strong>D</strong> depends on a quantum computation process (or by the technical jargon, a <strong>quantum circuit</strong>) denoted later by C.</p>
<p><span style="color: #0000ff;">What is the meaning of the statistical statement in part A)?</span></p>
<p>Google’s quantum computers (like any other current quantum computers) are very “noisy” so what the computer is producing are not samples from <strong>D</strong> but rather a noisy version which could roughly be described as follows: a fraction <em>t</em> of the samples are from <strong>D</strong> and a fraction <em>(1-t)</em> of the samples are from a uniform distribution. The statistical test allows to estimate the value of <em>t</em> which is referred to as the fidelity.</p>
<p><span style="color: #0000ff;">Could they directly verify claims A) and B) ?</span></p>
<p>No, it was only possible to give indirect evidence for both these claims.</p>
<p><span style="color: #0000ff;">What is the logic of Google’s quantum supremacy argument?</span></p>
<p>For claim A) regarding the success of the statistical test on the sample they have two arguments:</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/11/gsformula.png"><img alt="" class="alignnone size-medium wp-image-18590" height="44" src="https://gilkalai.files.wordpress.com/2019/11/gsformula.png?w=300&amp;h=44" width="300"/></a></p>
<ol>
<li>Analysis based on the fidelity of the components of the quantum computer – qubits and gates (see formula (77) above),</li>
<li>Experiments that support the analysis in the regime where they can be tested by a classical computer.</li>
</ol>
<p>According to the paper  the fidelity of entire circuits agrees perfectly with the prediction of the simple mathematical formula (77) with deviation under 10-20 percents. There are several reported experiments in the classically tractable regime including on simplified circuits (that are easier to simulate on classical computers) to support the assumption that the prediction given by formula (77) for the fidelity applies to the 53-qubit circuit in the supremacy regime.</p>
<p>For claim B) For the classical difficulty they rely on:</p>
<ol>
<li>Extrapolation from the running time of a specific algorithm that they use.</li>
<li>Computational complexity support for the assertion that the task they consider is asymptotically difficult.</li>
</ol>
<p><span style="color: #0000ff;">What are the weak points in this logic?</span></p>
<p>A main weakness (which is crucial in my mind) of the experiment is that the experimental support from the regime where the experiments can be tested by a classical computer is too sparse. Much more could have been done and should have been done.</p>
<p> </p>
<h2><a href="https://gilkalai.files.wordpress.com/2019/11/supremacy-figure.png"><img alt="" class="alignnone size-medium wp-image-18586" height="277" src="https://gilkalai.files.wordpress.com/2019/11/supremacy-figure.png?w=300&amp;h=277" width="300"/></a></h2>
<p><span style="color: #ff0000;">In my opinion, a major weakness of the Google experiment is that the support from experiments in the classically tractable regime (blue in the picture) is much too sparse and is unconvincing.</span></p>
<p>Another weakness is that the arguments for classical difficulty were mainly based on the performance of a specific algorithm.</p>
<p>Sources: The link to the <a href="https://www.nature.com/articles/s41586-019-1666-5">Google paper in </a><em><a href="https://www.nature.com/articles/s41586-019-1666-5">Nature.</a> </em>A <a href="https://youtu.be/FklMpRiTeTA">videotaped lecture</a> by John Martinis at Caltech.</p>
<h2>Assessment</h2>
<p><span style="color: #0000ff;">What is your assessment of the Google claims, Gil?</span></p>
<p>I think that the claims are incorrect. Specifically, I find the evidence for  the claim “the statistical test applied to the 53-qubit  sample will give a result above 1/10,000 too weak and I expect that this claim and other related claims in the paper will not stand after a closer scrutiny and further experiments in the classically tractable regime. I also doubt the perfect proximity between predictions based on the 1- and 2- qubit fidelity and the circuit fidelity.</p>
<p>The Google experiment represents a very large leap in several aspects of human ability to control noisy quantum systems and accepting their claims  requires very careful evaluation of the experiments and, of course, successful replications.</p>
<h2>The “most amazing thing” – is it real?</h2>
<p><span style="color: #0000ff;">Do you want to tell us more about Formula (77)?</span></p>
<p><a href="https://gilkalai.files.wordpress.com/2019/11/77.png"><img alt="" class="size-medium wp-image-18615 aligncenter" height="222" src="https://gilkalai.files.wordpress.com/2019/11/77.png?w=300&amp;h=222" width="300"/></a></p>
<p style="text-align: center;"><span style="color: #ff0000;">Formula (77)</span></p>
<p>Yes, thank you. Here again is Formula (77) and its explanation in the paper. The fact that the fidelity of entire circuits agrees with the prediction of the simple mathematical Formula (77) is “most amazing” <a href="https://youtu.be/FklMpRiTeTA?t=2455">according to John Martinis (videotaped lecture at Caltech).</a>  Indeed the deviation according to the paper is at most 10-20 percents.  This perfect agreement can be seen in various other parts of the paper. The authors’ interpretation of this finding is that it validates the digital error model and shows that there are no new mechanisms for errors.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/11/77-explained.png"><img alt="" class="alignnone size-large wp-image-18641" height="400" src="https://gilkalai.files.wordpress.com/2019/11/77-explained.png?w=640&amp;h=400" width="640"/></a></p>
<p><span style="color: #ff0000;">John explains the significance of Formula (77) at Caltech. Amazing big surprises are often false.</span></p>
<p><span style="color: #0000ff;">And what do you think about it, Gil</span></p>
<p>I completely agree that this is most amazing and, as a matter of fact, there are reasons to consider the  predictions based on Formula (77) as <span style="color: #ff0000;"><strong>too good to be true</strong></span> even if qubit and gates fidelity account for all the errors in the system. The issue is that Formula (77) itself is very sensitive to noise. The formula estimates the fidelity as the product of hundreds of contributions from individual qubits and gates. Fairly small errors in estimating the individual terms can have large accumulative effect, well beyond the 10%-20% margin.</p>
<p>Anyway, this matter deserves further study.</p>
<p><span style="color: #0000ff;">Why?</span></p>
<p>Because this is considered by the authors as a major discovery and while looking skeptically at the Google papers this appears to be an orthogonal “miracle” to the main supremacy claim.</p>
<h2>How to proceed</h2>
<p><span style="color: #0000ff;">Let’s go back to your overall assessment. What could change your mind, Gil?</span></p>
<p>Here goes:</p>
<h3>Verification</h3>
<p>A) An independent verification of the statistical tests outcomes for the experiments in the regime where the Google team classically computed the probabilities. This looks to me like a crucial step in a verification of such an important experiment.</p>
<p>A more difficult verification that I’d also regard as necessary at a later stage would be to independently check the probability distributions for the circuits given by the Google computation.</p>
<h3>Further analysis and crucial improvements</h3>
<p>B) Experiments with the quantum computers giving sufficiently many samples to understand the noisy probability distribution for circuits in the 10-25 qubit range.  See <a href="https://gilkalai.wordpress.com/2019/09/23/quantum-computers-amazing-progress-google-ibm-and-extraordinary-but-probably-false-supremacy-claims-google/">my first post</a>, <a href="https://www.scottaaronson.com/blog/?p=4372#comment-1822400" rel="nofollow ugc">this comment by Ryan O’Donnell</a>, and this <a href="https://www.scottaaronson.com/blog/?p=4317#comment-1819915">earlier one</a>.  We need to understand the noisy probability distribution produced by the Google quantum computer, the actual fidelity, and the quality of the statistical tests used by the Google team.</p>
<h3>Replications</h3>
<p>C) Experiments in the 10-30 qubit range on the IBM (and other) quantum computers.  It is quite possible that experimenting of this kind with random quantum circuits was already carried out.</p>
<p>D) Since improved classical algorithms were found by the IBM team (but analyzing the 53 qubits samples still seems practically beyond reach).   Google can produce samples for 41-49 qubits for which IBM (or others) can compute the probabilities quickly and test Google’s prediction for the fidelity.</p>
<h3>Follow-ups</h3>
<p>E) Success in demonstrating distance-3 and distance-5 surface codes and other quantum error-correcting codes.</p>
<p><span style="color: #0000ff;">So what precisely will convince you and what is the time-schedule that you expect for matters to be clarified?</span></p>
<p>A successful and convincing combination of<strong> three or more</strong> from A), B), C), D) or E) will go a long way to convince me. The verification part A) is important, and I don’t expect problems there, I expect that the Google claims will be verified and I consider it as very important that the data will be public and that various groups will verify the claims. This may take several months and certainly it should take less than a year.</p>
<p>At present, I expect parts B)-D) will not support Google’s supremacy claims. So outcomes of experiments in the next couple of years both by the Google group and by other groups will be crucial. One direction that I <em>do not</em> regard, at present, as useful for strengthening the quantum supremacy claims is increasing the number of qubits of the quantum computer.</p>
<p><span style="color: #0000ff;">What is required for the (easy) verification stage?</span></p>
<p>(1) Right now the raw samples of Google’s sampling experiments are public. There are altogether 300 files with samples.</p>
<p>(2) For every circuit that they experiment, the Google team also plans to upload  the 2^n probabilities that they obtained by the Feynman-Schrodinger algorithm.  This will  allow verifying their statistical tests, making subsequent analysis, and for other researchers to test other algorithms for computing the same probabilities.</p>
<p>(3) A convenient form of the data from (2) is a file that will give for every experiment the probabilities that the Google team computed for the samples. (For a large <em>n</em> those are much smaller files.)</p>
<p>(4) For each of the 300 experiments the estimated fidelity that formula (77) gave and the contribution of each qubit and gate to the RHS of (77).</p>
<p><span style="color: #0000ff;">Do you plan to take part yourself?</span></p>
<p>I plan to get involved myself with the “easy” verification and analysis of the “raw data” once it will become available. I do expect that the statistical tests will agree with the assertions in the Google paper, and at the same time, as I said,  I think it is important that this and other aspects of the experiments will be double checked and triple checked. This basic data already allows interesting analysis and indeed Google’s supplementary paper describes such analysis (that the Google people kindly pointed me to) on how the data fits the theory and on their statistical tests. See Figures S32-S36, table V and associated materials around pages 37-40.</p>
<h2>IBM</h2>
<p><span style="color: #0000ff;">What did the IBM rebuttal paper show?</span></p>
<p>Recall that the Google claim is based on two assertions:</p>
<p>A) The statistical test applied to the sample will give a result above 1/10,000</p>
<p>B) that producing a sample with similar statistical property will require 10,000 years on a supercomputer.</p>
<p>The IBM team described a different algorithm (on an even stronger current supercomputer) that would take only 2.5 days rather than 10,000 years.</p>
<p><span style="color: #0000ff;">Can the 2.5 days be further reduced?</span></p>
<p>As far as I can see the IBM claim is about a full computation of all the 2^53 probabilities. It is reasonable to think that producing a sample (or even a complete list of 2^53 probabilities) with fidelity t reduces the classical effort linearly with t. (This is the claim about the specific algorithm used by the Google team.) If this holds for the IBM algorithm then the 2.5 days will go down to less than a minute. (This will still be a notable “quantum speed up” in terms of the number of operations.) I don’t have an opinion as to whether  we should expect considerably better than IBM’s classical algorithms for computing the exact probabilities.</p>
<p><span style="color: #0000ff;">But lack of enthusiasm and skepticism of researchers from IBM about the Google paper appears to go beyond this particular point of the 2.5 computing days. Do you think that the objection by IBM people is motivated by fierce competition or envy?</span></p>
<p>No, I tend to think that there is a genuine interest by researchers who question the Google paper to understand the scientific matter, and carefully, critically and skeptically examine  Google’s claims. Maybe Google’s claims might seem to some other researchers who are working on quantum computers with superconducting qubits as remote from their own experimental experience, and this may give a strong reason for skepticism. It is also possible that in time people in IBM and elsewhere will change their mind and will become  more enthusiastic about the Google results.</p>
<p>IBM <a href="https://arxiv.org/abs/1910.09534">paper</a> and <a href="https://www.ibm.com/blogs/research/2019/10/on-quantum-supremacy/">blog post</a> responding to Google’s announcement.</p>
<h2>Noise</h2>
<p><span style="color: #0000ff;">Tell us a little more about noise</span></p>
<p>Here is a nice toy model (which I think is quite realistic) to understand what the noise is.  Suppose that you ran a circuit C on your quantum computer with n qubits and the ideal probability distribution is <img alt="D_C" class="latex" src="https://s0.wp.com/latex.php?latex=D_C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D_C"/>. The fidelity <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/> noisy version of <img alt="D_C" class="latex" src="https://s0.wp.com/latex.php?latex=D_C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D_C"/> will be <img alt="N_C(x)=t D_C(x) + (1-t)S_C" class="latex" src="https://s0.wp.com/latex.php?latex=N_C%28x%29%3Dt+D_C%28x%29+%2B+%281-t%29S_C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="N_C(x)=t D_C(x) + (1-t)S_C"/>. And here <img alt="S_C" class="latex" src="https://s0.wp.com/latex.php?latex=S_C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S_C"/> is the average (or weighted average) of values of <img alt="D_C(y)" class="latex" src="https://s0.wp.com/latex.php?latex=D_C%28y%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D_C(y)"/> where y is a vector in the neighborhood of x.</p>
<p>Here is a concrete version: We look at the expected value of <img alt="D_C(y)" class="latex" src="https://s0.wp.com/latex.php?latex=D_C%28y%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D_C(y)"/> where y is a new vector and <img alt="y_i=x_i" class="latex" src="https://s0.wp.com/latex.php?latex=y_i%3Dx_i&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y_i=x_i"/> with probability <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/> <img alt="y_i=(1-x_i)" class="latex" src="https://s0.wp.com/latex.php?latex=y_i%3D%281-x_i%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="y_i=(1-x_i)"/> with probability <img alt="(1-p)" class="latex" src="https://s0.wp.com/latex.php?latex=%281-p%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(1-p)"/> <span style="color: #008000;"><strong>independently</strong></span>. We choose <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p"/> so that <img alt="p^n=t" class="latex" src="https://s0.wp.com/latex.php?latex=p%5En%3Dt&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p^n=t"/>. There are cases where positive correlation of errors for 2-qubit gates lead to correlated errors. (This is especially relevant in the context of quantum error correction.) To add this effect to the toy noise model replace the  word <span style="color: #008000;">independently</span> by “<span style="color: #008000;">positively correlated</span>“.</p>
<h2>Your argument, Gil</h2>
<p><span style="color: #0000ff;">Why do you think that quantum supremacy is not possible at all?</span></p>
<p>The gist of my argument against the possibility of achieving quantum supremacy by Noisy intermediate scale quantum computers is quite simple: “<em>NISQ</em> <em>devices can’t outperform classical computers, for the simple reason that they <span style="color: #ff0000;">are</span> primitive classical computers.” </em></p>
<p>(Note the similarity to Scott Aaronson’s <a href="https://www.scottaaronson.com/blog/?p=4342">critique</a> on a <a href="https://www.nature.com/articles/s41586-019-1557-9">paper</a> published by <em>Nature</em> claiming implementation of a Shor-like algorithm on a classical device called “p-bits”.  Scott offered a very quick debunking: “ ‘<em>p-bit’ devices can’t scalably outperform classical computers, for the simple reason that they <span style="color: #ff0000;">are</span> classical computers.)</em></p>
<p><span style="color: #0000ff;">If Google’s claim are correct – does it falsify your argument?</span></p>
<p>Yes! I predict that probability distributions described (robustly) by a noisy quantum circuit represent a polynomial time algorithm in terms of the description of the circuit. And by a polynomial time algorithm I assume small degree and modest constants. The Google claim, if true, appears to falsify this prediction. (And for this you do not need quantum supremacy in the strongest form of the term.)</p>
<p><span style="color: #0000ff;">But is there no way that Google’s huge (or at least large) computational advantage coexists with what you say?</span></p>
<p>There is an issue that I myself am not completely sure about regarding the possibility of chaotic behavior of quantum computers. Here is the classical analog: If you have <em>n</em> bits of memory inside a (classical) computer of <em>m</em> bits and ask about the complexity of the evolution on the <em>n</em> bits which may be chaotic. Of course, we cannot expect that this chaotic computer can lead to a computation that requires thousands of years in a super computer. But can it lead to a robust computation which is superpolynomial  in <em>n</em> (but polynomial in <em>m</em>)?</p>
<p>I don’t know the general answer but, in any case, I don’t think that it changes the situation here. If the Google claims  stand, I would regard it as a very strong argument against my theory. (Even if the noisy distributions themselves are not robust.) In any case, the question if the samples in the experiments represent robust distributions or are chaotic could and should be tested. (I discussed it in <a href="https://gilkalai.wordpress.com/2019/10/03/noisy-quantum-circuits-how-do-we-know-that-we-have-robust-experimental-outcomes-at-all-and-do-we-care/">this post</a>.)</p>
<p><span style="color: #0000ff;">If Google’s claims do not stand, will it confirm or give a strong support to your position that quantum supremacy and quantum error correction are impossible?</span></p>
<p>Failure of the Google claim will mainly support the position that quantum supremacy and quantum error correction require substantial improvement of the quality of qubit and gates. It would give a noteworthy support to my position (and probably would draw some attention to it) but I would not regard it as a decisive support. Let me mention that various specific predictions that I made can be tested in the Google, IBM and other systems.</p>
<p><span style="color: #0000ff;">OK, so why <em>do you think</em> that the quality of qubits and gates <em>cannot</em> be improved?</span></p>
<p><span style="color: #ff0000;">Yes, this is the crucial point. One argument (that I already mentioned) for thinking that there is a barrier for the quality of gates and qubits is computational theoretic. Computationally speaking NISQ devices are primitive classical computing devices and this gives a strong reason to think that it will not be possible to reduce the error rate to the level allowing computational supremacy. But there is an additional argument: for a wide range of lower levels of noise, reducing the noise will have the effect of making the system more chaotic. So the first argument tells us that there is a small range of error-rates that we can hope to  achieve and the second argument tells us that for a large range  of lower error-rates all we gain is chaos!</span></p>
<p> </p>
<p>Links to my work: <a href="https://gilkalai.files.wordpress.com/2019/09/main-pr.pdf">Three puzzles on mathematics computations, and games,</a> Proc. ICM2018; <a href="https://arxiv.org/abs/1908.02499">The argument against quantum computers</a>, To appear in Itamar Pitowsky’s memorial volume; <a href="https://www.ams.org/journals/notices/201605/rnoti-p508.pdf">The quantum computer puzzle</a>, Notices AMS, May 2016</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/09/cern.pptx">Slides</a> from my 2019 CERN lecture. My ICM 2018 <a href="https://www.youtube.com/watch?v=oR-ufBz13Eg">videotaped lecture</a>.</p>
<h2>Correlated errors and quantum error correction</h2>
<p><span style="color: #0000ff;">People mainly refer to your conjectures about correlated errors.</span></p>
<p>Yes, this reflects my work between 2005-2013 (and was a central issue in <a href="http://rjlipton.wordpress.com/2012/01/30/perpetual-motion-of-the-21st-century/">my debate</a> <a href="http://rjlipton.wordpress.com/2012/10/03/quantum-supremacy-or-classical-control/">with Aram Harrow</a>) and I think it is an important part of the overall picture. But this issue is different than my argument against quantum computers which represents my work between 2014-2019.  I think that my earlier work on error correlation is a key (or a starting point) to the question: What do we learn from failure of quantum computers on general properties of quantum noise. Indeed there are various consequences; some of them are fairly intuitive; some of them are counter-intuitive, and some of them are both. The basic intuition is that once your computation really makes use of a large portion of the Hilbert space, so will the error!</p>
<p>The major challenge is to put this intuition into formal mathematical terms and to relate it to the mathematics and physics of quantum physics.</p>
<p>I made a similar idea in  a comment to Dave Bacon in 2006 when I wrote “I believe that you may be able to approximate a rank-one matrix up to a rank-one error. I do not believe that you will be able to approximate an arbitrary matrix up to a rank one matrix.” to which Dave replied “I will never look at rank one matrices the same <img alt="&#x1F609;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f609.png" style="height: 1em;"/>”. Dave Bacon is among the authors of the new Google paper.</p>
<p><span style="color: #0000ff;">What is the connection between the ability to achieve quantum supremacy and the ability to achieve quantum error-correction?</span></p>
<p>One of the main claims in my recent works is that quantum supremacy is an easier task compared to creating good quality error-correcting codes. For the attempted experiments by Google, we see  a clear demonstration that achieving good quality quantum error correction is harder than demonstrating quantum supremacy. Low fidelity circuits that Google claims to achieve are far from sufficient for quantum error-correction. The other claim in my argument is that quantum supremacy cannot be achieved without quantum error correction (and, in particular, not at all in the NISQ regime) and this claim is, of course, challenged by the Google claims.</p>
<p><span style="color: #0000ff;">You claim that without quantum error correction to start with we cannot reach quantum supremacy. But maybe John Martinis’ experimental methods have some seeds of quantum error correction inside them?</span></p>
<p>Maybe <img alt="&#x1F642;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;"/>  See this 2017 cartoon from <a href="https://gilkalai.wordpress.com/2017/10/16/if-quantum-computers-are-not-possible-why-are-classical-computers-possible/">this post</a>.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/11/sulam35c2.png"><img alt="" class="alignnone size-medium wp-image-18525" height="223" src="https://gilkalai.files.wordpress.com/2019/11/sulam35c2.png?w=300&amp;h=223" width="300"/></a></p>
<p>(Here is a <a href="https://youtu.be/h6p_ZeMqGIU">nice overview video from 2014 about my stance and earlier work</a>.)</p>
<h2>The Google experiment: concerns and attacks</h2>
<p><span style="color: #0000ff;">Beside the critique on experimental evidence that could be tested did you find some concrete issues with the Google experiment?</span></p>
<p>Perhaps even too many <img alt="&#x1F642;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;"/> . In the first post and comments I raised quite a few objections. Some of them are relevant and some of them turned out to be irrelevant or incorrect. Anyway, here, taken from my first post, are some of my concerns and attempted attacks on the Google experiment:</p>
<ol>
<li>Not enough experiments with full histograms; not enough experiments in the regime where they can be directly tested</li>
<li>Classical supremacy argument is overstated and is based on the performance of a specific algorithm</li>
<li>Error correlation may falsify the Google noise model</li>
<li>Low degree Fourier coefficients may fool the statistical test</li>
<li>(Motivated by <a href="https://www.scottaaronson.com/blog/?p=4317#comment-1819915">a comment</a> by Ryan.) It is easier to optimize toward the new statistical test “linear cross-ratio entropy” compared to the old logarithmic one.</li>
<li>“System calibration” may reflect an optimization towards the specific required circuit.</li>
<li>The interpolation argument is unjustified (because of the calibration issue).</li>
</ol>
<p><span style="color: #0000ff;">We talked about items 1 and 2 what about 3-5. In particular, are correlated errors relevant to the Google experiment?</span></p>
<p>No! (As far as I can see.) Correlated errors mean that in the smoothing the flipped coordinates are positively correlated.  But for the random circuit and the (Porter Thomas) distribution <img alt="D_C" class="latex" src="https://s0.wp.com/latex.php?latex=D_C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D_C"/>  this makes no difference!</p>
<p>As for item 4., it turns out (and this was essentially known by the <a href="https://arxiv.org/abs/1810.03176">work of Gao and Duan</a>) that in the case of random circuits (unlike the case of Boson Sampling) there is no low degree coefficients to fool the statistical test.</p>
<p>As for item 5., the answer is “nice observation, but so what?” (Let me note that the supplementary paper of the Google team compares and analyzes the linear and logarithmic statistical measures.)</p>
<p><span style="color: #0000ff;">What about the calibration? You got a little overworked about it, no?</span></p>
<p>In almost every scientific experiment there could be concerns that there will be some sort of biased data selection toward the hoped-for result.</p>
<p>Based on the description of the calibration method I got the impression that part of the calibration/verification process (“system calibration”) was carried out towards the experimental outcome for a specific circuit, and that this does not improve the fidelity as the authors thought but rather mistakenly tweaked the experimental outcomes toward a specific probability distribution. This type of calibration would be a major (yet innocent) flaw in the experiment. However, this possibility was excluded by a clear assertion of the researchers regarding the nature of the calibration process, and also by a more careful reading of the paper itself by Peter Shor and Greg Kuperberg. I certainly was, for a short while, way overconfident about this theory.</p>
<p>One nice (and totally familiar) observation is that a blind experiment can largely eliminate the concern of biased data selection.</p>
<h2>So how do you feel, Gil?</h2>
<p><span style="color: #0000ff;">When did you hear about the Google claim?</span></p>
<p>There were certainly some reasons to think that Google’s quantum supremacy was coming for example a quanta magazine article by Kevin Hartnett entitled <a href="https://www.quantamagazine.org/quantum-supremacy-is-coming-heres-what-you-should-know-20190718/">Quantum Supremacy Is Coming: Here’s What You Should Know</a> and another article about Neven’s double exponential law. Also Scott Aaronson gave some hints about it.</p>
<p>On September 18, I met Thomas Vidick in a very nice conference of the Israeli and US academies on the future of computer science (it was mentioned in <a href="https://gilkalai.wordpress.com/2019/09/04/computer-science-and-its-impact-on-our-future/">this post</a>, links to all videos will be added here, Naftali Tishby’s lecture is especially recommended.) Thomas told me about the new expected Google paper.</p>
<p>When I introduced Thomas to <a href="https://gilkalai.wordpress.com/2008/09/03/the-prisonner-dilemma-sympathy-and-yaaris-challenge/">Menachem Yaari</a> (who was the president of the Israeli Academy), describing the former as a young superstar in quantum computation, Menachem’s reaction was: “but you do not believe in quantum computers.” I replied that I believe it is a fascinating intellectual area, and that perhaps I am even wrong about them being infeasible. Thomas said: “our area needs more people like Gil.” (!)</p>
<p><span style="color: #0000ff;">What about Scott?</span></p>
<p>Scott and I have been on friendly terms for many years and share a lot of interests and values. We are deeply divided regarding quantum computers and, naturally, I think that I am right and that Scott is wrong. In the context of the Google paper Scott’s references to me and my stance were <a href="https://www.scottaaronson.com/blog/?p=4312#comment-1819487">peculiar</a> and even a <a href="https://www.scottaaronson.com/blog/?p=4312#comment-1819821">little hostile</a> which was especially strange since at that time I did not have access to the paper and Scott was the referee of the paper.</p>
<p><span style="color: #0000ff;">Gil, how do you vision a situation where you are proven wrong?</span></p>
<p>If my theory of quantum computation being derailed by noise inherent in quantum gates is proven wrong, then physicists will say that I am a mathematician and mathematicians will say that I am a combinatorialist.</p>
<p><span style="color: #0000ff;">and how do you vision  a situation where you are proven right?</span><span id="more-18435"/></p>
<p> </p>
<p>If my theory of quantum computation being derailed by noise inherent in quantum gates is  proven successful, then physicists will say that I am a mathematician and mathematicians will say that I am a combinatorialist. <img alt="&#x1F642;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;"/></p>
<p><span style="color: #0000ff;">And what would you say if your theory  prevails?</span></p>
<p>Where I <em>have seen further</em> than others, it is because I stood on Peter Shor’s shoulders and looked at the opposite direction. <img alt="&#x1F642;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f642.png" style="height: 1em;"/></p>
<h2>Topological</h2>
<p><span style="color: #0000ff;">One last thing, Gil. Nick Read just</span> <a href="https://www.scottaaronson.com/blog/?p=4400#comment-1823614">commented</a> <span style="color: #0000ff;">that experimental evidence is gradually pointing towards you being false on the matter of topological quantum qubits.</span></p>
<p>Nick is a great guy and topological quantum computing is a great topic. The general situation is quite simple and it applies to topological quantum computing like any other form of quantum computing. The way I see it,  gradual experimental progress will hit a barrier and non-gradual experimental progress will be falsified.</p>
<p>(See this 2014 <a href="https://youtu.be/L5gSZsezhoQ">videotaped lecture of mine on topological quantum computing,</a> and also Section 3.5 of <a href="https://arxiv.org/abs/1908.02499">The argument against quantum computers.</a>)</p>
<p> </p>
<h2>Appendix: The probability distribution and the statistical tests</h2>
<p><span style="color: #0000ff;">Let’s have an Appendix question. Can you try to briefly describe the probability distribution and the statistical test used in the Google paper?</span></p>
<p>Let me try. We start with a probability distribution described by the density function <img alt="z e^{-z}" class="latex" src="https://s0.wp.com/latex.php?latex=z+e%5E%7B-z%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z e^{-z}"/> supported on <img alt="\mathbb R_+" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R_%2B&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb R_+"/>. Now we consider our set <em>X</em> of <em>0-1</em> vectors of length <em>n</em>.</p>
<p>We draw a random probability distribution <img alt="p(x)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p(x)"/> on <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="X"/>. <img alt="p(x)=q(x)/2^n" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29%3Dq%28x%29%2F2%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p(x)=q(x)/2^n"/> and the value of <img alt="q(x)" class="latex" src="https://s0.wp.com/latex.php?latex=q%28x%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q(x)"/> is drawn at random from the probability distribution <img alt="z e^{-z}" class="latex" src="https://s0.wp.com/latex.php?latex=z+e%5E%7B-z%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="z e^{-z}"/>.  (A very slight normalization may still be needed.) A probability distribution of this kind on <img alt="\{0,1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B0%2C1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{0,1\}^n"/> is called a Porter-Thomas distribution.</p>
<p>A random quantum circuit <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C"/> leads to a (deterministic) probability distribution <img alt="D_C" class="latex" src="https://s0.wp.com/latex.php?latex=D_C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D_C"/> of this kind. A classical computer can compute the probabilities based on the description of the quantum circuits but this becomes increasingly hard with <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/>. A quantum computer can easily sample  <img alt="x \in \{0,1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5C%7B0%2C1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x \in \{0,1\}^n"/> according to <img alt="D_C" class="latex" src="https://s0.wp.com/latex.php?latex=D_C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D_C"/>.</p>
<p>We are given <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="t"/> samples <img alt="x_1,\dots, x_t" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cdots%2C+x_t&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x_1,\dots, x_t"/> that our noisy quantum computer drew.</p>
<p>Our research hypothesis is that the samples are drawn from <img alt="\rho D_C +(1-\rho)U" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho+D_C+%2B%281-%5Crho%29U&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho D_C +(1-\rho)U"/> where <img alt="U" class="latex" src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="U"/> is a uniform distribution. <img alt="\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho"/> is called the fidelity. The null hypothesis is that the samples were drawn uniformly at random. (There is also a finer description of the noisy distribution with a Gaussian low order term depending on <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="C"/>. (This can be seen already from the noise toy model above but I will not discuss it here.)</p>
<p>The main test used in the Google paper is an estimator for <img alt="\rho" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\rho"/>:</p>
<p><img alt="\frac {2^n}{t} \sum_{i=1}^t D_C(x_i) -1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac+%7B2%5En%7D%7Bt%7D+%5Csum_%7Bi%3D1%7D%5Et+D_C%28x_i%29+-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac {2^n}{t} \sum_{i=1}^t D_C(x_i) -1"/>.</p>
<p>They also considered a logarithmic version.</p>
<p>The samples in the experiment (at least when <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> is large) are two sparse to identify the probability distribution on <img alt="\{0,1\}^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B0%2C1%5C%7D%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{0,1\}^n"/>. (This was one of my concerns that was also endorsed by Ryan.) But once you compute the probability distribution <img alt="D_C" class="latex" src="https://s0.wp.com/latex.php?latex=D_C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="D_C"/> you can study statistically the set of probabilities that you obtained for your sample. The Google paper offers some interesting statistical studies and in particular a statistical comparison between the set of values <img alt="\{D_C(x_i):i=1,\dots,t\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7BD_C%28x_i%29%3Ai%3D1%2C%5Cdots%2Ct%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{D_C(x_i):i=1,\dots,t\}"/>.</p></div>
    </content>
    <updated>2019-11-13T07:17:23Z</updated>
    <published>2019-11-13T07:17:23Z</published>
    <category term="Combinatorics"/>
    <category term="Physics"/>
    <category term="Quantum"/>
    <category term="John Martinis"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-11-13T18:20:30Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-8890204.post-4110486083097630045</id>
    <link href="http://mybiasedcoin.blogspot.com/feeds/4110486083097630045/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=8890204&amp;postID=4110486083097630045" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/4110486083097630045" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/8890204/posts/default/4110486083097630045" rel="self" type="application/atom+xml"/>
    <link href="http://mybiasedcoin.blogspot.com/2019/11/allston-students-should-speak-up.html" rel="alternate" type="text/html"/>
    <title>Allston :  Students Should Speak Up</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">I have had (as is pretty usual for me) some number of lunches with students this semester, and many of them asked me about the Computer Science move to Allston.  A lot of times, these questions are concerns:  what food will we have there, how frequently will buses run, what sort of space will there be for students to hang out/study etc., what all will be over there?<br/><br/>These are good questions.  I don't know the answers, I think a lot is still being worked out or decided.  I realize that's not a great response. <br/><br/>I've started encouraging students to start asking these questions, more directly, to the powers that be.  Specifically, I've suggested to the small number of undergrads I've been lunching with that they should start sending e-mails to the powers that be, asking whatever questions they have, and expressing their concerns.  If you have questions, or wishes, regarding the food, safety, transportation, etc. at the new Allston building, you should ask or let your desires be known.  And, I'll be frank here, you should not (just) let the faculty know -- we're at best an indirect channel to the powers, and they may listen to all of you more than they listen to us.  Contact the powers directly.<br/><br/>(Graduate students too.)<br/><br/>It might be more impactful if students organize to make their questions and/or wishes known.  Or not.  That's up to you really.  But if you want Allston to be successful from where you stand, now is a pretty good time to get involved, before we all start over there next fall.  For Allston to be the academic home you want, you may have to speak up. <br/><br/>I realize undergrads might not know who are the powers that be that they should contact.  So I'll provide a starting list:<br/><br/>Frank Doyle, Dean of the School of Engineering and Applied Sciences<br/>Claudine Gay, Dean of the Faculty of Arts and Sciences<br/>Alan Garber, Provost<br/>Rakesh Khurana, Dean of Harvard College<br/>Amanda Claybaugh, Dean of Undergraduate Education<br/><br/>And finally, yes, of course it's a <a href="https://en.wikipedia.org/wiki/Powers_That_Be_(Angel)">Buffy (well, Angel) reference</a>. <br/><br/></div>
    </content>
    <updated>2019-11-13T06:23:00Z</updated>
    <published>2019-11-13T06:23:00Z</published>
    <author>
      <name>Michael Mitzenmacher</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06738274256402616703</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-8890204</id>
      <category term="conferences"/>
      <category term="research"/>
      <category term="society"/>
      <category term="algorithms"/>
      <category term="administration"/>
      <category term="teaching"/>
      <category term="Harvard"/>
      <category term="papers"/>
      <category term="graduate students"/>
      <category term="funding"/>
      <category term="talks"/>
      <category term="blogs"/>
      <category term="codes"/>
      <category term="jobs"/>
      <category term="reviews"/>
      <category term="personal"/>
      <category term="travel"/>
      <category term="undergraduate students"/>
      <category term="books"/>
      <category term="open problems"/>
      <category term="PCs"/>
      <category term="consulting"/>
      <category term="randomness"/>
      <category term="CCC"/>
      <category term="blog book project"/>
      <category term="research labs"/>
      <category term="ISIT"/>
      <category term="tenure"/>
      <category term="comments"/>
      <category term="recommendations"/>
      <category term="outreach"/>
      <category term="students"/>
      <author>
        <name>Michael Mitzenmacher</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06738274256402616703</uri>
      </author>
      <link href="http://mybiasedcoin.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://mybiasedcoin.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/8890204/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">My take on computer science -- <br/> 
algorithms, networking, information theory -- <br/> 
and related items.</div>
      </subtitle>
      <title>My Biased Coin</title>
      <updated>2019-11-13T06:23:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.05060</id>
    <link href="http://arxiv.org/abs/1911.05060" rel="alternate" type="text/html"/>
    <title>Fully-Dynamic Space-Efficient Dictionaries and Filters with Constant Number of Memory Accesses</title>
    <feedworld_mtime>1573603200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Ioana O. Bercea, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Even:Guy.html">Guy Even</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.05060">PDF</a><br/><b>Abstract: </b>A fully-dynamic dictionary is a data structure for maintaining sets that
supports insertions, deletions and membership queries. A filter approximates
membership queries with a one-sided error. We present two designs:
</p>
<p>1. The first space-efficient fully-dynamic dictionary that maintains both
sets and random multisets and supports queries, insertions, and deletions with
a constant number of memory accesses in the worst case with high probability.
The comparable dictionary of Arbitman, Naor, and Segev [FOCS 2010] works only
for sets.
</p>
<p>2. By a reduction from our dictionary for random multisets, we obtain a
space-efficient fully-dynamic filter that supports queries, insertions, and
deletions with a constant number of memory accesses in the worst case with high
probability (as long as the false positive probability is $2^{-O(w)}$, where
$w$ denotes the word length). This is the first in-memory space-efficient
fully-dynamic filter design that provably achieves these properties.
</p>
<p>We also present an application of the techniques used to design our
dictionary to the static Retrieval Problem.
</p></div>
    </summary>
    <updated>2019-11-13T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.05032</id>
    <link href="http://arxiv.org/abs/1911.05032" rel="alternate" type="text/html"/>
    <title>FPT Algorithms for Diverse Collections of Hitting Sets</title>
    <feedworld_mtime>1573603200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Baste:Julien.html">Julien Baste</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jaffke:Lars.html">Lars Jaffke</a>, Tomáš Masařík, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Philip:Geevarghese.html">Geevarghese Philip</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rote:G=uuml=nter.html">Günter Rote</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.05032">PDF</a><br/><b>Abstract: </b>In this work, we study the $d$-Hitting Set and Feedback Vertex Set problems
through the paradigm of finding diverse collections of $r$ solutions of size at
most $k$ each, which has recently been introduced to the field of parameterized
complexity [Baste et al., 2019]. This paradigm is aimed at addressing the loss
of important side information which typically occurs during the abstraction
process which models real-world problems as computational problems. We use two
measures for the diversity of such a collection: the sum of all pairwise
Hamming distances, and the minimum pairwise Hamming distance. We show that both
problems are FPT in $k + r$ for both diversity measures. A key ingredient in
our algorithms is a (problem independent) network flow formulation that, given
a set of `base' solutions, computes a maximally diverse collection of
solutions. We believe that this could be of independent interest.
</p></div>
    </summary>
    <updated>2019-11-13T02:24:09Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.04871</id>
    <link href="http://arxiv.org/abs/1911.04871" rel="alternate" type="text/html"/>
    <title>On the Computational Complexity of Multi-Agent Pathfinding on Directed Graphs</title>
    <feedworld_mtime>1573603200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nebel:Bernhard.html">Bernhard Nebel</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.04871">PDF</a><br/><b>Abstract: </b>The determination of the computational complexity of multi-agent pathfinding
on directed graphs has been an open problem for many years. For undirected
graphs, solvability can be decided in polynomial time, as has been shown
already in the eighties. Further, recently it has been shown that a special
case on directed graphs is solvable in polynomial time. In this paper, we show
that the problem is NP-hard in the general case. In addition, some upper bounds
are proven.
</p></div>
    </summary>
    <updated>2019-11-13T02:20:53Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.04734</id>
    <link href="http://arxiv.org/abs/1911.04734" rel="alternate" type="text/html"/>
    <title>Sumcheck-based delegation of quantum computing to rational server</title>
    <feedworld_mtime>1573603200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Takeuchi:Yuki.html">Yuki Takeuchi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Morimae:Tomoyuki.html">Tomoyuki Morimae</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tani:Seiichiro.html">Seiichiro Tani</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.04734">PDF</a><br/><b>Abstract: </b>Delegated quantum computing enables a client with a weak computational power
to delegate quantum computing to a remote quantum server in such a way that the
integrity of the server is efficiently verified by the client. Recently, a new
model of delegated quantum computing has been proposed, namely, rational
delegated quantum computing. In this model, after the client interacts with the
server, the client pays a reward to the server. The rational server sends
messages that maximize the expected value of the reward. It is known that the
classical client can delegate universal quantum computing to the rational
quantum server in one round. In this paper, we propose novel one-round rational
delegated quantum computing protocols by generalizing the classical rational
sumcheck protocol. The construction of the previous rational protocols depends
on gate sets, while our sumcheck technique can be easily realized with any
local gate set. Furthermore, as with the previous protocols, our reward
function satisfies natural requirements. We also discuss the reward gap. Simply
speaking, the reward gap is a minimum loss on the expected value of the
server's reward incurred by the server's behavior that makes the client accept
an incorrect answer. Although our sumcheck-based protocols have only
exponentially small reward gaps as with the previous protocols, we show that a
constant reward gap can be achieved if two non-communicating but entangled
rational servers are allowed. We also discuss that a single rational server is
sufficient under the (widely-believed) assumption that the learning-with-errors
problem is hard for polynomial-time quantum computing. Apart from these
results, we show, under a certain condition, the equivalence between $rational$
and $ordinary$ delegated quantum computing protocols. Based on this
equivalence, we give a reward-gap amplification method.
</p></div>
    </summary>
    <updated>2019-11-13T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.04686</id>
    <link href="http://arxiv.org/abs/1911.04686" rel="alternate" type="text/html"/>
    <title>Online Stochastic Matching with Edge Arrivals</title>
    <feedworld_mtime>1573603200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gravin:Nick.html">Nick Gravin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tang:Zhihao_Gavin.html">Zhihao Gavin Tang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Kangning.html">Kangning Wang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.04686">PDF</a><br/><b>Abstract: </b>Online bipartite matching with edge arrivals is an important extension of the
classic vertex-arrival model of Karp, Vazirani and Vazirani. Finding an
algorithm better than the straightforward greedy strategy remained a major open
question for a long time until a recent negative result by Gamlath et al.
(forthcoming FOCS 2019), who showed that no online algorithm has a competitive
ratio better than $0.5$ in the worst case.
</p>
<p>In this work, we consider the bipartite matching problem with edge arrivals
in the stochastic framework. We find an online algorithm that on average is
$0.506$-competitive. We give a supplementary upper bound of $\frac{2}{3}$ and
also obtain along the way an interesting auxiliary result that if the initial
instance has a $2$-regular stochastic subgraph, then a natural prune &amp; greedy
algorithm matches at least $0.532$-fraction of all vertices.
</p></div>
    </summary>
    <updated>2019-11-13T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.04681</id>
    <link href="http://arxiv.org/abs/1911.04681" rel="alternate" type="text/html"/>
    <title>On Robustness to Adversarial Examples and Polynomial Optimization</title>
    <feedworld_mtime>1573603200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Awasthi:Pranjal.html">Pranjal Awasthi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dutta:Abhratanu.html">Abhratanu Dutta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vijayaraghavan:Aravindan.html">Aravindan Vijayaraghavan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.04681">PDF</a><br/><b>Abstract: </b>We study the design of computationally efficient algorithms with provable
guarantees, that are robust to adversarial (test time) perturbations. While
there has been an proliferation of recent work on this topic due to its
connections to test time robustness of deep networks, there is limited
theoretical understanding of several basic questions like (i) when and how can
one design provably robust learning algorithms? (ii) what is the price of
achieving robustness to adversarial examples in a computationally efficient
manner?
</p>
<p>The main contribution of this work is to exhibit a strong connection between
achieving robustness to adversarial examples, and a rich class of polynomial
optimization problems, thereby making progress on the above questions. In
particular, we leverage this connection to (a) design computationally efficient
robust algorithms with provable guarantees for a large class of hypothesis,
namely linear classifiers and degree-2 polynomial threshold functions (PTFs),
(b) give a precise characterization of the price of achieving robustness in a
computationally efficient manner for these classes, (c) design efficient
algorithms to certify robustness and generate adversarial attacks in a
principled manner for 2-layer neural networks. We empirically demonstrate the
effectiveness of these attacks on real data.
</p></div>
    </summary>
    <updated>2019-11-13T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.04676</id>
    <link href="http://arxiv.org/abs/1911.04676" rel="alternate" type="text/html"/>
    <title>Prediction of Bottleneck Points for Manipulation Planning in Cluttered Environment using a 3D Convolutional Neural Network</title>
    <feedworld_mtime>1573603200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Indraneel Patil, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rout:B=_K=.html">B. K. Rout</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kalaichelvi:V=.html">V. Kalaichelvi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.04676">PDF</a><br/><b>Abstract: </b>Latest research in industrial robotics is aimed at making human robot
collaboration possible seamlessly. For this purpose, industrial robots are
expected to work on the fly in unstructured and cluttered environments and
hence the subject of perception driven motion planning plays a vital role.
Sampling based motion planners are proven to be the most effective for such
high dimensional planning problems with real time constraints. Unluckily random
stochastic samplers suffer from the phenomenon of 'narrow passages' or
bottleneck regions which need targeted sampling to improve their convergence
rate. Also identifying these bottleneck regions in a diverse set of planning
problems is a challenge. In this paper an attempt has been made to address
these two problems by designing an intelligent 'bottleneck guided' heuristic
for a Rapidly Exploring Random Tree Star (RRT*) planner which is based on
relevant context extracted from the planning scenario using a 3D Convolutional
Neural Network and it is also proven that the proposed technique generalises to
unseen problem instances. This paper benchmarks the technique (bottleneck
guided RRT*) against a 10% Goal biased RRT star planner, shows significant
improvement in planning time and memory requirement and uses ABB 1410
industrial manipulator as a platform for implantation and validation of the
results.
</p></div>
    </summary>
    <updated>2019-11-13T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.04629</id>
    <link href="http://arxiv.org/abs/1911.04629" rel="alternate" type="text/html"/>
    <title>Fast Stochastic Peer Selection in Proof-of-Stake Protocols</title>
    <feedworld_mtime>1573603200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nguyen:Quan.html">Quan Nguyen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cronje:Andre.html">Andre Cronje</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kong:Michael.html">Michael Kong</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.04629">PDF</a><br/><b>Abstract: </b>The problem of peer selection, which randomly selects a peer from a set, is
commonplace in Proof-of-Stake (PoS) protocols. In PoS, peers are chosen
randomly with probability proportional to the amount of stake that they
possess. This paper presents an approach that relates PoS peer selection to
Roulette-wheel selection, which is frequently used in genetic and evolutionary
algorithms or complex network modelling. In particular, we introduce the use of
stochastic acceptance algorithm [6] for fast peer selection. The roulette-wheel
selection algorithm [6] achieves O(1) complexity based on stochastic
acceptance, whereas searching based algorithms may take O(N ) or O(logN )
complexity in a network of N peers.
</p></div>
    </summary>
    <updated>2019-11-13T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/11/12/postdoc-at-university-of-bergen-apply-by-january-15-2020/</id>
    <link href="https://cstheory-jobs.org/2019/11/12/postdoc-at-university-of-bergen-apply-by-january-15-2020/" rel="alternate" type="text/html"/>
    <title>postdoc at University of Bergen (apply by January 15, 2020)</title>
    <summary>At the Department of Informatics of the University of Bergen (UiB), Norway, there is a vacancy of up to two positions as postdoctoral researcher in algorithmic foundations of data science, associated with the newly founded Center for Data Science (CEDAS). Each position is for a fixed period of 2 years. Website: https://www.jobbnorge.no/en/available-jobs/job/177613/researcher-in-informatics-algorithmic-foundations-of-data-science Email: fedor.fomin@uib.no</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>At the Department of Informatics of the University of Bergen (UiB), Norway, there is a vacancy of up to two positions as postdoctoral researcher in algorithmic foundations of data science, associated with the newly founded Center for Data Science (CEDAS). Each position is for a fixed period of 2 years.</p>
<p>Website: <a href="https://www.jobbnorge.no/en/available-jobs/job/177613/researcher-in-informatics-algorithmic-foundations-of-data-science">https://www.jobbnorge.no/en/available-jobs/job/177613/researcher-in-informatics-algorithmic-foundations-of-data-science</a><br/>
Email: fedor.fomin@uib.no</p></div>
    </content>
    <updated>2019-11-12T15:32:57Z</updated>
    <published>2019-11-12T15:32:57Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-11-13T18:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4409</id>
    <link href="https://www.scottaaronson.com/blog/?p=4409" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4409#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4409" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Annual recruitment post</title>
    <summary xml:lang="en-US">Just like I did last year, and the year before, I’m putting up a post to let y’all know about opportunities in our growing Quantum Information Center at UT Austin. I’m proud to report that we’re building something pretty good here. This fall Shyam Shankar joined our Electrical and Computer Engineering (ECE) faculty to do […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Just like I did <a href="https://www.scottaaronson.com/blog/?p=3964">last year</a>, and <a href="https://www.scottaaronson.com/blog/?p=3508">the year before</a>, I’m putting up a post to let y’all know about opportunities in our growing <a href="https://www.cs.utexas.edu/~qic/">Quantum Information Center</a> at UT Austin.</p>



<p>I’m proud to report that we’re building something pretty good here.  This fall <a href="http://sites.utexas.edu/shyamshankar/">Shyam Shankar</a> joined our Electrical and Computer Engineering (ECE) faculty to do experimental superconducting qubits, while (as I <a href="https://www.scottaaronson.com/blog/?p=4233">blogged</a> in the summer) the quantum complexity theorist <a href="http://www.mit.edu/~jswright/">John Wright</a> will join me on the CS faculty in Fall 2020.  Meanwhile, <a href="https://sites.google.com/utexas.edu/potter/home">Drew Potter</a>, an expert on topological qubits, rejoined our physics faculty after a brief leave.  Our weekly quantum information group meeting now regularly attracts around 30 participants—from the turnout, you wouldn’t know it’s not MIT or Caltech or Waterloo.  My own group now has five postdocs and six PhD students—as well as some amazing undergrads striving to meet the bar set by <a href="https://www.scottaaronson.com/blog/?p=3880">Ewin Tang</a>.  Course offerings in quantum information currently include Brian La Cour’s <a href="https://cns.utexas.edu/component/cobalt/item/3138-quantum-computing?Itemid=1971">Freshman Research Initiative</a>, my own undergrad <a href="https://www.scottaaronson.com/blog/?p=3943">Intro to Quantum Information Science</a> honors class, and graduate classes on quantum complexity theory, experimental realizations of QC, and topological matter (with more to come).  We’ll also be starting an undergraduate Quantum Information Science concentration next fall.</p>



<p>So without further ado:</p>



<p>(1) If you’re interested in pursuing a PhD focused on quantum computing and information (and/or classical theoretical computer science) at UT Austin: please apply!  If you want to work with me or John Wright on quantum algorithms and complexity, <a href="https://www.cs.utexas.edu/graduate/prospective-students/apply">apply to CS</a> (I can also supervise physics students in rare cases).  Also apply to CS, of course, if you want to work with our other CS theory faculty: David Zuckerman, Dana Moshkovitz, Adam Klivans, Anna Gal, Eric Price, Brent Waters, Vijaya Ramachandran, or Greg Plaxton.  If you want to work with Drew Potter on nonabelian anyons or suchlike, or with <a href="https://web2.ph.utexas.edu/~macdgrp/">Allan MacDonald</a>, <a href="http://order.ph.utexas.edu/people/Reichl.htm">Linda Reichl</a>, <a href="https://sites.cns.utexas.edu/liopticsut/home">Elaine Li</a>, or others on many-body quantum theory, <a href="https://ph.utexas.edu/prospective-graduate-students/admissions">apply to physics</a>.  If you want to work with Shyam Shankar on superconducting qubits, <a href="http://www.ece.utexas.edu/graduate/admissions">apply to ECE</a>.  Note that the deadline for CS and physics is <strong>December 1</strong>, while the deadline for ECE is <strong>December 15</strong>.</p>



<p>You don’t need to ask me whether I’m on the lookout for great students: I always am!  If you say on your application that you want to work with me, I’ll be sure to see it.  Emailing individual faculty members is not how it works and won’t help.  Admissions are extremely competitive, so I strongly encourage you to apply broadly to maximize your options.</p>



<p>(2) If you’re interested in a postdoc in my group, I’ll have approximately two openings starting in Fall 2020.  To apply, just send me an email by <strong>January 1, 2020</strong> with the following info:<br/>– Your CV<br/>– 2 or 3 of your best papers (links or PDF attachments)<br/>– The names of two recommenders (who should email me their letters separately)</p>



<p>(3) If you’re on the faculty job market in quantum computing and information—well, please give me a heads-up if you’re potentially interested in Austin!  Our CS, physics, and ECE departments are all open to considering additional candidates in quantum information, both junior and senior.  I can’t take credit for this—it surely has to do with developments beyond my control, both at UT and beyond—but I’m happy to relay that, in the three years since I arrived in Texas, the appetite for strengthening UT’s presence in quantum information has undergone jaw-dropping growth at every level of the university.</p>



<p>Also, Austin-Bergstrom International Airport now has direct flights to London, Frankfurt, and (soon) Amsterdam and Paris.</p>



<p>Hook ’em Hadamards!</p></div>
    </content>
    <updated>2019-11-12T07:02:25Z</updated>
    <published>2019-11-12T07:02:25Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2019-11-12T21:05:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.04415</id>
    <link href="http://arxiv.org/abs/1911.04415" rel="alternate" type="text/html"/>
    <title>Revisiting the Approximate Carath\'eodory Problem via the Frank-Wolfe Algorithm</title>
    <feedworld_mtime>1573516800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Combettes:Cyrille_W=.html">Cyrille W. Combettes</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pokutta:Sebastian.html">Sebastian Pokutta</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.04415">PDF</a><br/><b>Abstract: </b>The approximate Carath\'eodory theorem states that given a polytope
$\mathcal{P}$, each point in $\mathcal{P}$ can be approximated within
$\epsilon$-accuracy in $\ell_p$-norm as the convex combination of
$\mathcal{O}(pD_p^2/\epsilon^2)$ vertices, where $p\geq2$ and $D_p$ is the
diameter of $\mathcal{P}$ in $\ell_p$-norm. A solution satisfying these
properties can be built using probabilistic arguments [Barman, 2015] or by
applying mirror descent to the dual problem [Mirrokni et al., 2017]. We revisit
the approximate Carath\'eodory problem by solving the primal problem via the
Frank-Wolfe algorithm, providing a simplified analysis and leading to an
efficient practical method. Sublinear to linear sparsity bounds are derived
naturally using existing convergence results of the Frank-Wolfe algorithm in
different scenarios.
</p></div>
    </summary>
    <updated>2019-11-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-12T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.04382</id>
    <link href="http://arxiv.org/abs/1911.04382" rel="alternate" type="text/html"/>
    <title>GRASS: Spectral Sparsification Leveraging Scalable Spectral Perturbation Analysis</title>
    <feedworld_mtime>1573516800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feng:Zhuo.html">Zhuo Feng</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.04382">PDF</a><br/><b>Abstract: </b>Spectral graph sparsification aims to find ultra-sparse subgraphs whose
Laplacian matrix can well approximate the original Laplacian eigenvalues and
eigenvectors. In recent years, spectral sparsification techniques have been
extensively studied for accelerating various numerical and graph-related
applications. Prior nearly-linear-time spectral sparsification methods first
extract low-stretch spanning tree from the original graph to form the backbone
of the sparsifier, and then recover small portions of spectrally-critical
off-tree edges to the spanning tree to significantly improve the approximation
quality. However, it is not clear how many off-tree edges should be recovered
for achieving a desired spectral similarity level within the sparsifier.
Motivated by recent graph signal processing techniques, this paper proposes a
similarity-aware spectral graph sparsification framework that leverages
efficient spectral off-tree edge embedding and filtering schemes to construct
spectral sparsifiers with guaranteed spectral similarity (relative condition
number) level. An iterative graph densification scheme is also introduced to
facilitate efficient and effective filtering of off-tree edges for highly
ill-conditioned problems. The proposed method has been validated using various
kinds of graphs obtained from public domain sparse matrix collections relevant
to VLSI CAD, finite element analysis, as well as social and data networks
frequently studied in many machine learning and data mining applications. For
instance, a sparse SDD matrix with 40 million unknowns and 180 million nonzeros
can be solved (1E-3 accuracy level) within two minutes using a single CPU core
and about 6GB memory.
</p></div>
    </summary>
    <updated>2019-11-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-12T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.04372</id>
    <link href="http://arxiv.org/abs/1911.04372" rel="alternate" type="text/html"/>
    <title>Information carefull worstcase DecreaseKey heaps with simple nonMeld variant</title>
    <feedworld_mtime>1573516800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Majerech:Vladan.html">Vladan Majerech</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.04372">PDF</a><br/><b>Abstract: </b>We analyze priority queues including DecreaseKey method in its interface. The
paper is inspired by Strict Fibonacci Heaps [2], where G. S. Brodal, G.
Lagogiannis, and R. E. Tarjan implemented the heap with DecreaseKey and Meld
interface in assymptotically optimal worst case times (based on key
comparisons). At the end of the paper there are mentioned possible variants of
other structural properties an violations than they have used in the analysis.
In the main variant a lot of information is wasted during violation reduction
steps. Our goal is to concentrate on other variants and to invent natural
strategy not losing that much in the information value. In other words we try
to choose among them one which corresponds to superexpensive comparision
principle as much as possible. The principle was described in [5] of myself,
but after publication I have found these ideas in [4] of H. Kaplan, R. E.
Tarjan, and U. Zwick.
</p></div>
    </summary>
    <updated>2019-11-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-12T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.04249</id>
    <link href="http://arxiv.org/abs/1911.04249" rel="alternate" type="text/html"/>
    <title>A polynomial kernel for $3$-leaf power deletion</title>
    <feedworld_mtime>1573516800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Jungho Ahn, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Eiben:Eduard.html">Eduard Eiben</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kwon:O=joung.html">O-joung Kwon</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Oum:Sang=il.html">Sang-il Oum</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.04249">PDF</a><br/><b>Abstract: </b>A graph $G$ is an $\ell$-leaf power of a tree $T$ if $V(G)$ is equal to the
set of leaves of $T$, and distinct vertices $v$ and $w$ of $G$ are adjacent if
and only if the distance between $v$ and $w$ in $T$ is at most $\ell$. Given a
graph $G$, the $3$-leaf Power Deletion problem asks whether there is a set
$S\subseteq V(G)$ of size at most $k$ such that $G\setminus S$ is a $3$-leaf
power of some tree $T$. We provide a polynomial kernel for this problem. More
specifically, we present a polynomial-time algorithm for an input instance
$(G,k)$ to output an equivalent instance $(G',k')$ such that $k'\le k$ and $G'$
has at most $O(k^{14}\log^{12}k)$ vertices.
</p></div>
    </summary>
    <updated>2019-11-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-12T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.04202</id>
    <link href="http://arxiv.org/abs/1911.04202" rel="alternate" type="text/html"/>
    <title>Dv2v: A Dynamic Variable-to-Variable Compressor</title>
    <feedworld_mtime>1573516800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brisaboa:Nieves_R=.html">Nieves R. Brisaboa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fari=ntilde=a:Antonio.html">Antonio Fariña</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/G=oacute=mez=Brand=oacute=n:Adri=aacute=n.html">Adrián Gómez-Brandón</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Navarro:Gonzalo.html">Gonzalo Navarro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rodeiro:Tirso_V=.html">Tirso V. Rodeiro</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.04202">PDF</a><br/><b>Abstract: </b>We present Dv2v, a new dynamic (one-pass) variable-to-variable compressor.
Variable-to-variable compression aims at using a modeler that gathers
variable-length input symbols and a variable-length statistical coder that
assigns shorter codewords to the more frequent symbols. In Dv2v, we process the
input text word-wise to gather variable-length symbols that can be either
terminals (new words) or non-terminals, subsequences of words seen before in
the input text. Those input symbols are set in a vocabulary that is kept sorted
by frequency. Therefore, those symbols can be easily encoded with dense codes.
Our Dv2v permits real-time transmission of data, i.e. compression/transmission
can begin as soon as data become available. Our experiments show that Dv2v is
able to overcome the compression ratios of the v2vDC, the state-of-the-art
semi-static variable-to-variable compressor, and to almost reach p7zip values.
It also draws a competitive performance at both compression and decompression.
</p></div>
    </summary>
    <updated>2019-11-12T23:32:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-12T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.04198</id>
    <link href="http://arxiv.org/abs/1911.04198" rel="alternate" type="text/html"/>
    <title>GraCT: A Grammar-based Compressed Index for Trajectory Data</title>
    <feedworld_mtime>1573516800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brisaboa:Nieves_R=.html">Nieves R. Brisaboa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/G=oacute=mez=Brand=oacute=n:Adri=aacute=n.html">Adrián Gómez-Brandón</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Navarro:Gonzalo.html">Gonzalo Navarro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Param=aacute=:Jos=eacute=_R=.html">José R. Paramá</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.04198">PDF</a><br/><b>Abstract: </b>We introduce a compressed data structure for the storage of free trajectories
of moving objects (such as ships and planes) that efficiently supports various
spatio-temporal queries. Our structure, dubbed GraCT, stores the absolute
positions of all the objects at regular time intervals (snapshots) using a
$k^2$-tree, which is a space- and time-efficient version of a region quadtree.
Positions between snapshots are represented as logs of relative movements and
compressed using Re-Pair, a grammar-based compressor. The nonterminals of this
grammar are enhanced with MBR information to enable fast queries.
</p>
<p>The GraCT structure of a dataset occupies less than the raw data compressed
with a powerful traditional compressor such as p7zip. Further, instead of
requiring full decompression to access the data like a traditional compressor,
GraCT supports direct access to object trajectories or to their position at
specific time instants, as well as spatial range and nearest-neighbor queries
on time instants and/or time intervals.
</p>
<p>Compared to traditional methods for storing and indexing spatio-temporal
data, GraCT requires two orders of magnitude less space, and is competitive in
query times. In particular, thanks to its compressed representation, the GraCT
structure may reside in main memory in situations where any classical
uncompressed index must resort to disk, thereby being one or two orders of
magnitude faster.
</p></div>
    </summary>
    <updated>2019-11-12T23:44:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-12T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.04122</id>
    <link href="http://arxiv.org/abs/1911.04122" rel="alternate" type="text/html"/>
    <title>Classification on the Computational Complexity of Spin Models</title>
    <feedworld_mtime>1573516800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Shi=Xin.html">Shi-Xin Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.04122">PDF</a><br/><b>Abstract: </b>In this note, we provide a unifying framework to investigate the
computational complexity of classical spin models and give the full
classification on spin models in terms of system dimensions, randomness,
external magnetic fields and types of spin coupling. We further discuss about
the implications of NP-complete Hamiltonian models in physics and the
fundamental limitations of all numerical methods imposed by such models. We
conclude by a brief discussion on the picture when quantum computation and
quantum complexity theory are included.
</p></div>
    </summary>
    <updated>2019-11-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-11-12T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.04112</id>
    <link href="http://arxiv.org/abs/1911.04112" rel="alternate" type="text/html"/>
    <title>Dependency Stochastic Boolean Satisfiability: A Logical Formalism for NEXPTIME Decision Problems with Uncertainty</title>
    <feedworld_mtime>1573516800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lee:Nian=Ze.html">Nian-Ze Lee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jiang:Jie=Hong_R=.html">Jie-Hong R. Jiang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.04112">PDF</a><br/><b>Abstract: </b>Stochastic Boolean Satisfiability (SSAT) is a logical formalism to model
decision problems with uncertainty, such as Partially Observable Markov
Decision Process (POMDP). SSAT, however, is limited by its descriptive power
within the PSPACE complexity class. More complex problems, such as the
NEXPTIME-complete Decentralized POMDP (Dec-POMDP), cannot be succinctly encoded
with SSAT. To provide a logical formalism of such problems, we generalize
Dependency Quantified Boolean Formula (DQBF), a representative problem in the
NEXPTIME-complete class, to its stochastic variant, named Dependency SSAT
(DSSAT), and show that DSSAT is also NEXPTIME-complete. To demonstrate the
descriptive power of DSSAT, we further establish a polynomial-time reduction
from Dec-POMDP to DSSAT. Our results may encourage DSSAT solver development to
enable potential broad applications.
</p></div>
    </summary>
    <updated>2019-11-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-11-12T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.04063</id>
    <link href="http://arxiv.org/abs/1911.04063" rel="alternate" type="text/html"/>
    <title>A*SLAM: A Dual Fisheye Stereo Edge SLAM</title>
    <feedworld_mtime>1573516800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Guoxuan.html">Guoxuan Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.04063">PDF</a><br/><b>Abstract: </b>This paper proposes an A*SLAM system that features combining two sets of
fisheye stereo cameras and taking the image edge as the SLAM features. The dual
fisheye stereo camera sets cover the full environmental view of the SLAM
system. From each fisheye stereo image pair, a panorama depth image can be
directly extracted for initializing the SLAM feature. The edge feature is an
illumination invariant feature. The paper presents a method of the edge-based
simultaneous localization and mapping process using both the normal and
inverted images interchangeably.
</p></div>
    </summary>
    <updated>2019-11-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-11-12T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.04026</id>
    <link href="http://arxiv.org/abs/1911.04026" rel="alternate" type="text/html"/>
    <title>A generic imperative language for polynomial time</title>
    <feedworld_mtime>1573516800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leivant:Daniel.html">Daniel Leivant</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.04026">PDF</a><br/><b>Abstract: </b>We propose a generic imperative programming language STR that captures PTime
computations, on both infinite inductive structures and families of finite
structures. The approach, set up in [29] for primitive-recursive complexity,
construes finite partial-functions as a universal canonical form of data, and
uses structure components for loop variants. STR is obtained by the further
refinement that assigns ranks to finite partial-functions, which regulate the
interaction of loops, yielding programs that run in polynomial time. STR
captures algorithms that have eluded ramified recurrence, and is promising as
an artifact of Implicit Complexity which is malleable to static analysis
implementations.
</p></div>
    </summary>
    <updated>2019-11-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-11-12T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.04014</id>
    <link href="http://arxiv.org/abs/1911.04014" rel="alternate" type="text/html"/>
    <title>Interaction is necessary for distributed learning with privacy or communication constraints</title>
    <feedworld_mtime>1573516800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dagan:Yuval.html">Yuval Dagan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feldman:Vitaly.html">Vitaly Feldman</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.04014">PDF</a><br/><b>Abstract: </b>Local differential privacy (LDP) is a model where users send privatized data
to an untrusted central server whose goal it to solve some data analysis task.
In the non-interactive version of this model the protocol consists of a single
round in which a server sends requests to all users then receives their
responses. This version is deployed in industry due to its practical advantages
and has attracted significant research interest. Our main result is an
exponential lower bound on the number of samples necessary to solve the
standard task of learning a large-margin linear separator in the
non-interactive LDP model. Via a standard reduction this lower bound implies an
exponential lower bound for stochastic convex optimization and specifically,
for learning linear models with a convex, Lipschitz and smooth loss. These
results answer the questions posed in \citep{SmithTU17,DanielyF18}. Our lower
bound relies on a new technique for constructing pairs of distributions with
nearly matching moments but whose supports can be nearly separated by a large
margin hyperplane. These lower bounds also hold in the model where
communication from each user is limited and follow from a lower bound on
learning using non-adaptive \emph{statistical queries}.
</p></div>
    </summary>
    <updated>2019-11-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-12T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.03990</id>
    <link href="http://arxiv.org/abs/1911.03990" rel="alternate" type="text/html"/>
    <title>Implementing geometric complexity theory: On the separation of orbit closures via symmetries</title>
    <feedworld_mtime>1573516800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Ikenmeyer:Christian.html">Christian Ikenmeyer</a>, Umangathan Kandasamy <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.03990">PDF</a><br/><b>Abstract: </b>Understanding the difference between group orbits and their closures is a key
difficulty in geometric complexity theory (GCT): While the GCT program is set
up to separate certain orbit closures, many beautiful mathematical properties
are only known for the group orbits, in particular close relations with
symmetry groups and invariant spaces, while the orbit closures seem much more
difficult to understand. However, in order to prove lower bounds in algebraic
complexity theory, considering group orbits is not enough.
</p>
<p>In this paper we tighten the relationship between the orbit of the power sum
polynomial and its closure, so that we can separate this orbit closure from the
orbit closure of the product of variables by just considering the symmetry
groups of both polynomials and their representation theoretic decomposition
coefficients. In a natural way our construction yields a multiplicity
obstruction that is neither an occurrence obstruction, nor a so-called
vanishing ideal occurrence obstruction. All multiplicity obstructions so far
have been of one of these two types.
</p>
<p>Our paper is the first implementation of the ambitious approach that was
originally suggested in the first papers on geometric complexity theory by
Mulmuley and Sohoni (SIAM J Comput 2001, 2008): Before our paper, all existence
proofs of obstructions only took into account the symmetry group of one of the
two polynomials (or tensors) that were to be separated. In our paper the
multiplicity obstruction is obtained by comparing the representation theoretic
decomposition coefficients of both symmetry groups.
</p>
<p>Our proof uses a semi-explicit description of the coordinate ring of the
orbit closure of the power sum polynomial in terms of Young tableaux, which
enables its comparison to the coordinate ring of the orbit.
</p></div>
    </summary>
    <updated>2019-11-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-11-12T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.03989</id>
    <link href="http://arxiv.org/abs/1911.03989" rel="alternate" type="text/html"/>
    <title>On the Equivalence of SDP Feasibility and a Convex Hull Relaxation for System of Quadratic Equations</title>
    <feedworld_mtime>1573516800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kalantari:Bahman.html">Bahman Kalantari</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.03989">PDF</a><br/><b>Abstract: </b>We show {\it semidefinite programming} (SDP) feasibility problem is
equivalent to solving a {\it convex hull relaxation} (CHR) for system of
quadratic equations. On the one hand this offers a simple description of SDP.
On the other hand, this equivalence makes it possible to describe a version of
the {\it Triangle Algorithm} for SDP feasibility based on solving CHR.
Specifically, the Triangle Algorithm either computes an approximation to the
least-norm feasible solution of SDP, or using its {\it distance duality},
provides a separation when no solution within a prescribed norm exists. The
worst-case complexity of each iteration is computing largest eigenvalue of a
symmetric matrix arising in that iteration. Alternate complexity bounds on the
total number of iterations are derived. Further applications includes solving
an SDP optimization problem. The Triangle Algorithm thus provides an
alternative to the existing interior-point algorithms for SDP. Finally, gaining
from these results and insights, we discuss potential extension to solving
general system of polynomial equations.
</p></div>
    </summary>
    <updated>2019-11-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-11-12T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.03858</id>
    <link href="http://arxiv.org/abs/1911.03858" rel="alternate" type="text/html"/>
    <title>Ar{\i}kan meets Shannon: Polar codes with near-optimal convergence to channel capacity</title>
    <feedworld_mtime>1573516800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guruswami:Venkatesan.html">Venkatesan Guruswami</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Riazanov:Andrii.html">Andrii Riazanov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Ye:Min.html">Min Ye</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.03858">PDF</a><br/><b>Abstract: </b>Let $W$ be a binary-input memoryless symmetric (BMS) channel with Shannon
capacity $I(W)$ and fix any $\alpha &gt; 0$. We construct, for any sufficiently
small $\delta &gt; 0$, binary linear codes of block length
$O(1/\delta^{2+\alpha})$ and rate $I(W)-\delta$ that enable reliable
communication on $W$ with quasi-linear time encoding and decoding. Shannon's
noisy coding theorem established the existence of such codes (without efficient
constructions or decoding) with block length $O(1/\delta^2)$. This quadratic
dependence on the gap $\delta$ to capacity is known to be best possible. Our
result thus yields a constructive version of Shannon's theorem with
near-optimal convergence to capacity as a function of the block length. This
resolves a central theoretical challenge associated with the attainment of
Shannon capacity. Previously such a result was only known for the erasure
channel.
</p>
<p>Our codes are a variant of Ar{\i}kan's polar codes based on multiple
carefully constructed local kernels, one for each intermediate channel that
arises in the decoding. A crucial ingredient in the analysis is a strong
converse of the noisy coding theorem when communicating using random linear
codes on arbitrary BMS channels. Our converse theorem shows extreme
unpredictability of even a single message bit for random coding at rates
slightly above capacity.
</p></div>
    </summary>
    <updated>2019-11-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-12T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.03757</id>
    <link href="http://arxiv.org/abs/1911.03757" rel="alternate" type="text/html"/>
    <title>Universal Communication, Universal Graphs, and Graph Labeling</title>
    <feedworld_mtime>1573516800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Harms:Nathaniel.html">Nathaniel Harms</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.03757">PDF</a><br/><b>Abstract: </b>We introduce a communication model called universal SMP, in which Alice and
Bob receive a function $f$ belonging to a family $\mathcal{F}$, and inputs $x$
and $y$. Alice and Bob use shared randomness to send a message to a third party
who cannot see $f, x, y$, or the shared randomness, and must decide $f(x,y)$.
Our main application of universal SMP is to relate communication complexity to
graph labeling, where the goal is to give a short label to each vertex in a
graph, so that adjacency or other functions of two vertices $x$ and $y$ can be
determined from the labels $\ell(x),\ell(y)$. We give a universal SMP protocol
using $O(k^2)$ bits of communication for deciding whether two vertices have
distance at most $k$ on distributive lattices (generalizing the $k$-Hamming
Distance problem in communication complexity), and explain how this implies an
$O(k^2\log n)$ labeling scheme for determining $\mathrm{dist}(x,y) \leq k$ on
distributive lattices with size $n$; in contrast, we show that a universal SMP
protocol for determining $\mathrm{dist}(x,y) \leq 2$ in modular lattices (a
superset of distributive lattices) has super-constant $\Omega(n^{1/4})$
communication cost. On the other hand, we demonstrate that many graph families
known to have efficient adjacency labeling schemes, such as trees,
low-arboricity graphs, and planar graphs, admit constant-cost communication
protocols for adjacency. Trees also have an $O(k)$ protocol for deciding
$\mathrm{dist}(x,y) \leq k$ and planar graphs have an $O(1)$ protocol for
$\mathrm{dist}(x,y) \leq 2$, which implies a new $O(\log n)$ labeling scheme
for the same problem on planar graphs.
</p></div>
    </summary>
    <updated>2019-11-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-12T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.03748</id>
    <link href="http://arxiv.org/abs/1911.03748" rel="alternate" type="text/html"/>
    <title>Quantum speedups need structure</title>
    <feedworld_mtime>1573516800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Keller:Nathan.html">Nathan Keller</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Klein:Ohad.html">Ohad Klein</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.03748">PDF</a><br/><b>Abstract: </b>We prove the following conjecture, raised by Aaronson and Ambainis in 2008:
Let $f:\{-1,1\}^n \rightarrow [-1,1]$ be a multilinear polynomial of degree
$d$. Then there exists a variable $x_i$ whose influence on $f$ is at least
$\mathrm{poly}(\mathrm{Var}(f)/d)$.
</p>
<p>As was shown by Aaronson and Ambainis, this result implies the following
well-known conjecture on the power of quantum computing, dating back to 1999:
Let $Q$ be a quantum algorithm that makes $T$ queries to a Boolean input and
let $\epsilon,\delta &gt; 0$. Then there exists a deterministic classical
algorithm that makes $\mathrm{poly}(T,1/\epsilon,1/\delta)$ queries to the
input and that approximates $Q$'s acceptance probability to within an additive
error $\epsilon$ on a $1-\delta$ fraction of inputs. In other words, any
quantum algorithm can be simulated on most inputs by a classical algorithm
which is only polynomially slower, in terms of query complexity.
</p></div>
    </summary>
    <updated>2019-11-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-11-12T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.03683</id>
    <link href="http://arxiv.org/abs/1911.03683" rel="alternate" type="text/html"/>
    <title>A Polynomial Kernel for Paw-Free Editing</title>
    <feedworld_mtime>1573516800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Eiben:Eduard.html">Eduard Eiben</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lochet:William.html">William Lochet</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saurabh:Saket.html">Saket Saurabh</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.03683">PDF</a><br/><b>Abstract: </b>For a fixed graph $H$, the $H$-free-editing problem asks whether we can
modify a given graph $G$ by adding or deleting at most $k$ edges such that the
resulting graph does not contain $H$ as an induced subgraph. The problem is
known to be NP-complete for all fixed $H$ with at least $3$ vertices and it
admits a $2^{O(k)}n^{O(1)}$ algorithm. Cai and Cai showed that the
$H$-free-editing problem does not admit a polynomial kernel whenever $H$ or its
complement is a path or a cycle with at least $4$ edges or a $3$-connected
graph with at least $1$ edge missing. Their results suggest that if $H$ is not
independent set or a clique, then $H$-free-editing admits polynomial kernels
only for few small graphs $H$, unless $\textsf{coNP} \in \textsf{NP/poly}$.
Therefore, resolving the kernelization of $H$-free-editing for small graphs $H$
plays a crucial role in obtaining a complete dichotomy for this problem. In
this paper, we positively answer the question of compressibility for one of the
last two unresolved graphs $H$ on $4$ vertices. Namely, we give the first
polynomial kernel for paw-free editing with $O(k^{6})$vertices.
</p></div>
    </summary>
    <updated>2019-11-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-12T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.03620</id>
    <link href="http://arxiv.org/abs/1911.03620" rel="alternate" type="text/html"/>
    <title>Adaptivity in Adaptive Submodularity</title>
    <feedworld_mtime>1573516800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Esfandiari:Hossein.html">Hossein Esfandiari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Karbasi:Amin.html">Amin Karbasi</a>, Vahab Mirrokni <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.03620">PDF</a><br/><b>Abstract: </b>Adaptive sequential decision making is one of the central challenges in
machine learning and artificial intelligence. In such problems, the goal is to
design an interactive policy that plans for an action to take, from a finite
set of $n$ actions, given some partial observations. It has been shown that in
many applications such as active learning, robotics, sequential experimental
design, and active detection, the utility function satisfies adaptive
submodularity, a notion that generalizes the notion of diminishing returns to
policies. In this paper, we revisit the power of adaptivity in maximizing an
adaptive monotone submodular function. We propose an efficient batch policy
that with $O(\log n \times\log k)$ adaptive rounds of observations can achieve
an almost tight $(1-1/e-\epsilon)$ approximation guarantee with respect to an
optimal policy that carries out $k$ actions in a fully sequential setting. To
complement our results, we also show that it is impossible to achieve a
constant factor approximation with $o(\log n)$ adaptive rounds. We also extend
our result to the case of adaptive stochastic minimum cost coverage where the
goal is to reach a desired utility $Q$ with the cheapest policy. We first prove
the conjecture by Golovin and Krause that the greedy policy achieves the
asymptotically tight logarithmic approximation guarantee without resorting to
stronger notions of adaptivity. We then propose a batch policy that provides
the same guarantee in polylogarithmic adaptive rounds through a similar
information-parallelism scheme. Our results shrink the adaptivity gap in
adaptive submodular maximization by an exponential factor.
</p></div>
    </summary>
    <updated>2019-11-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-12T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.03605</id>
    <link href="http://arxiv.org/abs/1911.03605" rel="alternate" type="text/html"/>
    <title>How bad is worst-case data if you know where it comes from?</title>
    <feedworld_mtime>1573516800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Justin Y. Chen, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Valiant:Gregory.html">Gregory Valiant</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Valiant:Paul.html">Paul Valiant</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.03605">PDF</a><br/><b>Abstract: </b>We introduce a framework for studying how distributional assumptions on the
process by which data is partitioned into a training and test set can be
leveraged to provide accurate estimation or learning algorithms, even for
worst-case datasets. We consider a setting of $n$ datapoints, $x_1,\ldots,x_n$,
together with a specified distribution, $P$, over partitions of these
datapoints into a training set, test set, and irrelevant set. An algorithm
takes as input a description of $P$ (or sample access), the indices of the test
and training sets, and the datapoints in the training set, and returns a model
or estimate that will be evaluated on the datapoints in the test set. We
evaluate an algorithm in terms of its worst-case expected performance: the
expected performance over potential test/training sets, for worst-case
datapoints, $x_1,\ldots,x_n.$ This framework is a departure from more typical
distributional assumptions on the datapoints (e.g. that data is drawn
independently, or according to an exchangeable process), and can model a number
of natural data collection processes, including processes with dependencies
such as "snowball sampling" and "chain sampling", and settings where test and
training sets satisfy chronological constraints (e.g. the test instances were
observed after the training instances).
</p>
<p>Within this framework, we consider the setting where datapoints are bounded
real numbers, and the goal is to estimate the mean of the test set. We give an
efficient algorithm that returns a weighted combination of the training
set---whose weights depend on the distribution, $P$, and on the training and
test set indices---and show that the worst-case expected error achieved by this
algorithm is at most a multiplicative $\pi/2$ factor worse than the optimal of
such algorithms. The algorithm, and its proof, leverage a surprising connection
to the Grothendieck problem.
</p></div>
    </summary>
    <updated>2019-11-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-12T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.03542</id>
    <link href="http://arxiv.org/abs/1911.03542" rel="alternate" type="text/html"/>
    <title>Space Efficient Construction of Lyndon Arrays in Linear Time</title>
    <feedworld_mtime>1573516800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bille:Philip.html">Philip Bille</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Ellert:Jonas.html">Jonas Ellert</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fischer:Johannes.html">Johannes Fischer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/G=oslash=rtz:Inge_Li.html">Inge Li Gørtz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kurpicz:Florian.html">Florian Kurpicz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Munro:Ian.html">Ian Munro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rotenberg:Eva.html">Eva Rotenberg</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.03542">PDF</a><br/><b>Abstract: </b>We present the first linear time algorithm to construct the $2n$-bit version
of the Lyndon array using only $o(n)$ bits of working space. A simpler variant
of this algorithm computes the plain ($n\lg n$-bit) version of the Lyndon array
using only $\mathcal{O}(1)$ words of additional working space. All previous
algorithms are either not linear, or use at least $n\lg n$ bits of additional
working space. Also in practice, our new algorithms outperform the previous
best ones by an order of magnitude, both in terms of time and space.
</p></div>
    </summary>
    <updated>2019-11-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-12T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1911.03456</id>
    <link href="http://arxiv.org/abs/1911.03456" rel="alternate" type="text/html"/>
    <title>Parallel Data Distribution Management on Shared-Memory Multiprocessors</title>
    <feedworld_mtime>1573516800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Marzolla:Moreno.html">Moreno Marzolla</a>, Gabriele D'Angelo <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1911.03456">PDF</a><br/><b>Abstract: </b>The problem of identifying intersections between two sets of d-dimensional
axis-parallel rectangles appears frequently in the context of agent-based
simulation studies. For this reason, the High Level Architecture (HLA)
specification -- a standard framework for interoperability among simulators --
includes a Data Distribution Management (DDM) service whose responsibility is
to report all intersections between a set of subscription and update regions.
The algorithms at the core of the DDM service are CPU-intensive, and could
greatly benefit from the large computing power of modern multi-core processors.
In this paper we propose two parallel solutions to the DDM problem that can
operate effectively on shared-memory multiprocessors. The first solution is
based on a data structure (the Interval Tree) that allows concurrent
computation of intersections between subscription and update regions. The
second solution is based on a novel parallel extension of the Sort Based
Matching algorithm, whose sequential version is considered among the most
efficient solutions to the DDM problem. Extensive experimental evaluation of
the proposed algorithms confirm their effectiveness on taking advantage of
multiple execution units in a shared-memory architecture.
</p></div>
    </summary>
    <updated>2019-11-12T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-11-12T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/160</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/160" rel="alternate" type="text/html"/>
    <title>TR19-160 |  Tractable Unordered 3-CNF Games | 

	Md Lutfar Rahman, 

	Thomas Watson</title>
    <summary>The classic TQBF problem can be viewed as a game in which two players alternate turns assigning truth values to a CNF formula's variables in a prescribed order, and the winner is determined by whether the CNF gets satisfied. The complexity of deciding which player has a winning strategy in this game is well-understood: it is NL-complete for 2-CNFs and PSPACE-complete for 3-CNFs.

We continue the study of the unordered variant of this game, in which each turn consists of picking any remaining variable and assigning it a truth value. The complexity of deciding who can win on a given CNF is less well-understood; prior work by the authors showed it is in L for 2-CNFs and PSPACE-complete for 5-CNFs. We conjecture it may be efficiently solvable on 3-CNFs, and we make progress in this direction by proving the problem is in P, indeed in L, for 3-CNFs with a certain restriction, namely that each width-3 clause has at least one variable that appears in no other clause. Another (incomparable) restriction of this problem was previously shown to be tractable by Kutz.</summary>
    <updated>2019-11-11T22:26:59Z</updated>
    <published>2019-11-11T22:26:59Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-11-13T18:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/159</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/159" rel="alternate" type="text/html"/>
    <title>TR19-159 |  SETH-hardness of Coding Problems | 

	Noah Stephens-Davidowitz, 

	Vinod Vaikuntanathan</title>
    <summary>We show that assuming the strong exponential-time hypothesis (SETH), there are no non-trivial algorithms for the nearest codeword problem (NCP), the minimum distance problem (MDP), or the nearest codeword problem with preprocessing (NCPP) on linear codes over any finite field. More precisely, we show that there are no NCP, MDP, or NCPP algorithms running in time $q^{(1-\epsilon)n}$ for any constant $\epsilon&gt;0$ for codes with $q^n$ codewords. (In the case of NCPP, we assume non-uniform SETH.)

We also show that there are no sub-exponential-time algorithms for $\gamma$-approximate versions of these problems for some constant $\gamma &gt; 1$, under different versions of the exponential-time hypothesis.</summary>
    <updated>2019-11-11T19:23:19Z</updated>
    <published>2019-11-11T19:23:19Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-11-13T18:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/11/11/faculty-any-rank-at-penn-state-apply-by-january-1-2020/</id>
    <link href="https://cstheory-jobs.org/2019/11/11/faculty-any-rank-at-penn-state-apply-by-january-1-2020/" rel="alternate" type="text/html"/>
    <title>FACULTY (ANY RANK)  at PENN STATE (apply by January 1, 2020)</title>
    <summary>Applications are invited for multiple tenure-track positions at all levels across all areas of theoretical computer science. Our department is looking to grow rapidly in several areas, and theory is one of them. Website: https://academicjobsonline.org/ajo/jobs/14482 Email: ablanca@cse.psu.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Applications are invited for multiple tenure-track positions at all levels across all areas of theoretical computer science. Our department is looking to grow rapidly in several areas, and theory is one of them.</p>
<p>Website: <a href="https://academicjobsonline.org/ajo/jobs/14482">https://academicjobsonline.org/ajo/jobs/14482</a><br/>
Email: ablanca@cse.psu.edu</p></div>
    </content>
    <updated>2019-11-11T18:50:24Z</updated>
    <published>2019-11-11T18:50:24Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-11-13T18:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/158</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/158" rel="alternate" type="text/html"/>
    <title>TR19-158 |  Sorting Can Exponentially Speed Up Pure Dynamic Programming | 

	Stasys Jukna, 

	Hannes Seiwert</title>
    <summary>Many discrete minimization problems, including various versions of the shortest path problem, can be efficiently solved by dynamic programming (DP) algorithms that are ``pure'' in that they only perform basic operations, as $\min$, $\max$, $+$, but no conditional branchings via if-then-else in their recursion equations. It is known that any pure $(\min,+)$ DP algorithm solving the minimum weight spanning tree problem on undirected $n$-vertex graphs must perform at least $2^{\Omega(\sqrt{n})}$ operations. We show  that this problem \emph{can} be solved by a pure  $(\min,\max,+)$ DP algorithm performing only $O(n^3)$ operations. The algorithm is essentially a $(\min,\max)$ algorithm: addition operations are only used to output the final values. The presence of both $\min$ and $\max$ operations means that now DP algorithms can sort: this explains the title of the paper.</summary>
    <updated>2019-11-11T18:19:07Z</updated>
    <published>2019-11-11T18:19:07Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-11-13T18:20:25Z</updated>
    </source>
  </entry>
</feed>
