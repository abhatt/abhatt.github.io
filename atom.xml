<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-07-10T16:21:42Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-US">
    <id>https://blogs.princeton.edu/imabandit/?p=1382</id>
    <link href="https://blogs.princeton.edu/imabandit/2019/07/10/guest-post-by-julien-mairal-a-kernel-point-of-view-on-convolutional-neural-networks-part-i/" rel="alternate" type="text/html"/>
    <title>Guest post by Julien Mairal: A Kernel Point of View on Convolutional Neural Networks, part I</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>    I (n.b., Julien Mairal) have been interested in drawing links between neural networks and kernel methods for some time, and I am grateful to Sebastien for giving me the opportunity to say a few words about it on … <a href="https://blogs.princeton.edu/imabandit/2019/07/10/guest-post-by-julien-mairal-a-kernel-point-of-view-on-convolutional-neural-networks-part-i/">Continue reading <span class="meta-nav">→</span></a></p></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> </p>
<p><a class="liimagelink" href="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/uploads/sites/122/2019/07/kernel_fig.jpg?ssl=1"><img alt="" class="alignnone wp-image-1393" height="368" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/uploads/sites/122/2019/07/kernel_fig.jpg?resize=646%2C368&amp;ssl=1" width="646"/></a></p>
<p> </p>
<p>I (<em>n.b., <a class="liinternal" href="https://lear.inrialpes.fr/people/mairal/">Julien Mairal</a></em>) have been interested in drawing links between neural networks and kernel methods for some time, and I am grateful to Sebastien for giving me the opportunity to say a few words about it on his blog. My initial motivation was not to provide another “why deep learning works” theory, but simply to encode into kernel methods a few successful principles from convolutional neural networks (CNNs), such as the ability to model the local stationarity of natural images at multiple scales—we may call that modeling receptive fields—along with feature compositions and invariant representations. There was also something challenging in trying to reconcile end-to-end deep neural networks and non-parametric methods based on kernels that typically decouple data representation from the learning task.</p>
<p>The main goal of this blog post is then to discuss the construction of a particular multilayer kernel for images that encodes the previous principles, derive some invariance and stability properties for CNNs, and also present a simple mechanism to perform feature learning in reproducing kernel Hilbert spaces. In other words, we should not see any intrinsic contradiction between kernels and representation learning.</p>
<p><strong>Preliminaries on kernel methods</strong></p>
<p>Given data living in a set <img alt="\mathcal{X}" class="ql-img-inline-formula " height="12" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e44d6dd2d58e906a7f3ec11d7f3cac9c_l3.png?resize=15%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="15"/>, a positive definite kernel <img alt="K: \mathcal{X} \times \mathcal{X} \to \mathbb{R}" class="ql-img-inline-formula " height="13" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-f0d19a1401658006e20eb7aff7c20689_l3.png?resize=124%2C13&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="124"/> implicitly defines a Hilbert space <img alt="\mathcal{H}" class="ql-img-inline-formula " height="12" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d8c7ae0e5e08bd1b3f5ef053720bf142_l3.png?resize=15%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="15"/> of functions from <img alt="\mathcal{X}" class="ql-img-inline-formula " height="12" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e44d6dd2d58e906a7f3ec11d7f3cac9c_l3.png?resize=15%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="15"/> to <img alt="\mathbb{R}" class="ql-img-inline-formula " height="12" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b2c3c459eddec9847f841b19a2274a3d_l3.png?resize=13%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="13"/>, called reproducing kernel Hilbert space (RKHS), along with a mapping function <img alt="\varphi: \mathcal{X} \to \mathcal{H}" class="ql-img-inline-formula " height="16" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-075eb9a40ac7f19fc1d24932d430cf57_l3.png?resize=84%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="84"/>.</p>
<p>A predictive model <img alt="f" class="ql-img-inline-formula " height="16" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c7d97b919a3b73617cf2fbb375fff3b1_l3.png?resize=10%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="10"/> in <img alt="\mathcal{H}" class="ql-img-inline-formula " height="12" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d8c7ae0e5e08bd1b3f5ef053720bf142_l3.png?resize=15%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="15"/> associates to every point <img alt="x" class="ql-img-inline-formula " height="8" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-1b9fbfb207b6d17d74b33c6d8342a1a4_l3.png?resize=10%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="10"/> a label in <img alt="\mathbb{R}" class="ql-img-inline-formula " height="12" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b2c3c459eddec9847f841b19a2274a3d_l3.png?resize=13%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="13"/>, and admits a simple form <img alt="f(x) =\langle f, \varphi(x) \rangle_{\mathcal{H}}" class="ql-img-inline-formula " height="19" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-af2242f529038b9f66bdd803a7fcf32d_l3.png?resize=138%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="138"/>. Then, Cauchy-Schwarz inequality gives us a first basic stability property</p>
<p class="ql-center-displayed-equation" style="line-height: 21px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[ \forall x, x'\in \mathcal{X},~~~~~ |f(x)-f(x')| \leq \|f\|_{\mathcal{H}} \| \varphi(x) - \varphi(x')\|_\mathcal{H}. \]" class="ql-img-displayed-equation " height="21" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ba1f97e9889116f67e3caf7d27f6dca2_l3.png?resize=418%2C21&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="418"/></p>
<p>This relation exhibits a discrepancy between neural networks and kernel methods. Whereas neural networks optimize the data representation for a specific task, the term on the right involves the product of two quantities where data representation and learning are decoupled:</p>
<p><img alt="\|\varphi(x)-\varphi(x')\|_\mathcal{H}" class="ql-img-inline-formula " height="19" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9b7eefc0051c0b86a82ee0265f44a085_l3.png?resize=125%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="125"/> is a distance between two data representations <img alt="\varphi(x),\varphi(x')" class="ql-img-inline-formula " height="18" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-41c6c60616e1acea2bdd02deee51011e_l3.png?resize=83%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="83"/>, which are independent of the learning process, and <img alt="\|f\|_\mathcal{H}" class="ql-img-inline-formula " height="19" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c755a8a9349d0895075e9494d1b11fc1_l3.png?resize=38%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="38"/> is a norm on the model <img alt="f" class="ql-img-inline-formula " height="16" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c7d97b919a3b73617cf2fbb375fff3b1_l3.png?resize=10%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="10"/> (typically optimized over data) that acts as a measure of complexity.</p>
<p>Thinking about neural networks in terms of kernel methods then requires defining the underlying representation <img alt="\varphi(x)" class="ql-img-inline-formula " height="18" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-eb419c2adecf84ed9a2d9693bc58d101_l3.png?resize=35%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="35"/>, which can only depend on the network architecture, and the model <img alt="f" class="ql-img-inline-formula " height="16" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c7d97b919a3b73617cf2fbb375fff3b1_l3.png?resize=10%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="10"/>, which will be parametrized by (learned) network’s weights.</p>
<p><strong>Building a convolutional kernel for convolutional neural networks</strong></p>
<p>Following <a class="lipdf" href="http://jmlr.org/papers/volume20/18-190/18-190.pdf">Alberto Bietti’s paper</a>, we now consider the direct construction of a multilayer convolutional kernel for images. Given a two-dimensional image <img alt="x_0" class="ql-img-inline-formula " height="11" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-55b536a6647748d6c0c6b58015805c68_l3.png?resize=17%2C11&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="17"/>, the main idea is to build a sequence of “feature maps” <img alt="x_1,x_2,\ldots" class="ql-img-inline-formula " height="12" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-4e504020251e8444e8047821206317fa_l3.png?resize=71%2C12&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="71"/> that are two-dimensional spatial maps carrying information about image neighborhoods (a.k.a receptive fields) at every location. As we proceed in this sequence, the goal is to model larger neighborhoods with more “invariance”.</p>
<p>Formally, an input image <img alt="x_0" class="ql-img-inline-formula " height="11" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-55b536a6647748d6c0c6b58015805c68_l3.png?resize=17%2C11&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="17"/> is represented as a square-integrable function in <img alt="L^2(\Omega,\mathcal{H}_0)" class="ql-img-inline-formula " height="19" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b1cdcac953d52ed35e77925a243c3df7_l3.png?resize=76%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="76"/>, where <img alt="\Omega" class="ql-img-inline-formula " height="12" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ec0c546b6596f336d8e1d41bb064b951_l3.png?resize=12%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="12"/> is a set of pixel coordinates, and <img alt="\mathcal{H}_0" class="ql-img-inline-formula " height="15" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c58a47e1230e20fa0f090bbe6e111ba7_l3.png?resize=22%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="22"/> is a Hilbert space. <img alt="\Omega" class="ql-img-inline-formula " height="12" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ec0c546b6596f336d8e1d41bb064b951_l3.png?resize=12%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="12"/> may be a discrete grid or a continuous domain such as <img alt="\mathbb{R}^2" class="ql-img-inline-formula " height="15" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d5abe0f29e8cc710ae26f4f0af5a0859_l3.png?resize=20%2C15&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="20"/>, and <img alt="\mathcal{H}_0" class="ql-img-inline-formula " height="15" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c58a47e1230e20fa0f090bbe6e111ba7_l3.png?resize=22%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="22"/> may simply be <img alt="\mathbb{R}^3" class="ql-img-inline-formula " height="15" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-97886402213f48c46e631e5331a34035_l3.png?resize=20%2C15&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="20"/> for RGB images. Then, a feature map <img alt="x_k" class="ql-img-inline-formula " height="11" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3ad23c5c360c3f33031a5d000d37416f_l3.png?resize=17%2C11&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="17"/> in <img alt="L^2(\Omega,\mathcal{H}_k)" class="ql-img-inline-formula " height="19" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-66fdb69a62e8ec8647eac89f54998a71_l3.png?resize=77%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="77"/> is obtained from a previous layer <img alt="x_{k-1}" class="ql-img-inline-formula " height="12" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-246f2a4e2f0c791d5589f43eca6383b8_l3.png?resize=35%2C12&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="35"/> as follows:</p>
<ul>
<li><em> modeling larger neighborhoods than in the previous layer:</em> we map neighborhoods (patches) from <img alt="x_{k-1}" class="ql-img-inline-formula " height="12" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-246f2a4e2f0c791d5589f43eca6383b8_l3.png?resize=35%2C12&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="35"/> to a new Hilbert space <img alt="\mathcal{H}_k" class="ql-img-inline-formula " height="15" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-bc26f0de4084a72b9e625a080bd5d674_l3.png?resize=22%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="22"/>. Concretely, we define a homogeneous dot-product kernel between patches <img alt="z, z'" class="ql-img-inline-formula " height="18" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ce80943b7f55934d998e09542933b73e_l3.png?resize=30%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="30"/> from <img alt="x_{k-1}" class="ql-img-inline-formula " height="12" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-246f2a4e2f0c791d5589f43eca6383b8_l3.png?resize=35%2C12&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="35"/>:
<p class="ql-center-displayed-equation" style="line-height: 43px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[ K_k(z,z') = \|z\| \|z'\| \kappa_k \left( \left\langle \frac{z}{\|z\|}, \frac{z'}{\|z'\|} \right\rangle \right), \]" class="ql-img-displayed-equation " height="43" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e98e6c584e7aa34a129d04fa46a6981c_l3.png?resize=304%2C43&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="304"/></p>
<p> where <img alt="\langle . , . \rangle" class="ql-img-inline-formula " height="19" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b62527a227d32e3e2f43b8b9b2b31ad5_l3.png?resize=29%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="29"/> is an inner-product derived from <img alt="\mathcal{H}_{k-1}" class="ql-img-inline-formula " height="16" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ec7eee8a3bac08b4c319cfce53408682_l3.png?resize=39%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="39"/>, and <img alt="\kappa_k" class="ql-img-inline-formula " height="11" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-684fcf23472c51919624049fb4e0129a_l3.png?resize=17%2C11&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="17"/> is a non-linear function that ensures positive definiteness, <em>e.g.</em>, <img alt="\kappa_k(\langle u,u'\rangle ) = e^{\alpha (\langle u,u'\rangle -1)} = e^{-\frac{\alpha}{2}\|u-u'\|^2}" class="ql-img-inline-formula " height="23" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-46c64b76ccc9f508d30fec2fb80e244d_l3.png?resize=289%2C23&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="289"/> for vectors <img alt="u, u'" class="ql-img-inline-formula " height="18" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-0b88ce07daf9a52ba8a46659cff355fd_l3.png?resize=32%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="32"/> with unit norm, see <a class="lipdf" href="http://jmlr.org/papers/volume20/18-190/18-190.pdf">this paper</a>. By doing so, we implicitly define a kernel mapping <img alt="\varphi_k" class="ql-img-inline-formula " height="12" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-243ed60e88d807834cd7cb1e1fbe0658_l3.png?resize=19%2C12&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="19"/> that maps patches from <img alt="x_{k-1}" class="ql-img-inline-formula " height="12" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-246f2a4e2f0c791d5589f43eca6383b8_l3.png?resize=35%2C12&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="35"/> to a new Hilbert space <img alt="\mathcal{H}_k" class="ql-img-inline-formula " height="15" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-bc26f0de4084a72b9e625a080bd5d674_l3.png?resize=22%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="22"/>. This mechanism is illustrated in the picture at the beginning of the post, and produces a spatial map that carries these patch representations.</p></li>
<li><em>increasing invariance:</em> to gain invariance to small deformations, we smooth~<img alt="x_{k-1}" class="ql-img-inline-formula " height="12" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-246f2a4e2f0c791d5589f43eca6383b8_l3.png?resize=35%2C12&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="35"/> with a linear filter, as shown in the picture at the beginning of the post, which may be interpreted as anti-aliasing (in terms of signal processing) or linear pooling (in terms of neural networks).</li>
</ul>
<p>Formally, the previous construction amounts to applying operators <img alt="P_k" class="ql-img-inline-formula " height="15" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-4726bbf70431cf284be54bbc6a04ad60_l3.png?resize=18%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="18"/> (patch extraction), <img alt="M_k" class="ql-img-inline-formula " height="15" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-78f07026bc8c5150a11bf9e00756b7a7_l3.png?resize=24%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="24"/> (kernel mapping), and <img alt="A_k" class="ql-img-inline-formula " height="15" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-866de181a59a21d2ca2306a9adbd9bc1_l3.png?resize=20%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="20"/> (smoothing/pooling operator) to <img alt="x_{k-1}" class="ql-img-inline-formula " height="12" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-246f2a4e2f0c791d5589f43eca6383b8_l3.png?resize=35%2C12&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="35"/> such that the <img alt="n" class="ql-img-inline-formula " height="8" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-a63eb5ff0272d3119fa684be6e7acce8_l3.png?resize=11%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/>-th layer representation can be written as</p>
<p class="ql-center-displayed-equation" style="line-height: 21px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[ \Phi_n(x_0)= x_n= A_n M_n P_n \ldots A_1 M_1 P_1 x_0~~~\text{in}~~~~L^2(\Omega,\mathcal{H}_n). \]" class="ql-img-displayed-equation " height="21" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e8c2d99cc679426d1af08e6d15510211_l3.png?resize=437%2C21&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="437"/></p>
<p>We may finally define a kernel for images as <img alt="\mathcal{K}_n(x_0,x_0')=\langle \Phi_n(x_0), \Phi_n(x_0') \rangle" class="ql-img-inline-formula " height="19" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-16be084d5dd2ed3d7a18cdcf70c33fe2_l3.png?resize=231%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="231"/>, whose RKHS contains the functions <img alt="f_w(x_0) = \langle w , \Phi_n(x_0) \rangle" class="ql-img-inline-formula " height="19" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-95aa9a4388dd5f5da6292875abe6596a_l3.png?resize=162%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="162"/> for <img alt="w" class="ql-img-inline-formula " height="8" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-78d46af3f19bae0d88ac0cabd450a296_l3.png?resize=13%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="13"/> in <img alt="L^2(\Omega,\mathcal{H}_n)" class="ql-img-inline-formula " height="19" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-502d385c60e5ecdb1a0f26ee770d30b1_l3.png?resize=77%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="77"/>. Note now that we have introduced a concept of image representation <img alt="\Phi_n" class="ql-img-inline-formula " height="16" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-273242b8e92b3a9f4dc13c62b2785bd3_l3.png?resize=21%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="21"/>, which only depends on some network architecture (amounts of pooling, patch size), and predictive model <img alt="f_w" class="ql-img-inline-formula " height="16" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-fb636251e88ba51d909c76c1110eed5e_l3.png?resize=19%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="19"/> parametrized by <img alt="w" class="ql-img-inline-formula " height="8" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-78d46af3f19bae0d88ac0cabd450a296_l3.png?resize=13%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="13"/>.</p>
<p>From such a construction, we will now derive stability results for classical convolutional neural networks (CNNs) and then derive non-standard CNNs based on kernel approximations that we call convolutional kernel networks (CKNs).</p>
<p> </p>
<p>Next week, we will see how to perform feature (end-to-end) learning with the previous kernel representation, and also discuss other classical links between neural networks and kernel methods.</p>
<p> </p>
<p> </p></div>
    </content>
    <updated>2019-07-10T15:20:34Z</updated>
    <published>2019-07-10T15:20:34Z</published>
    <category term="Machine learning"/>
    <author>
      <name>Sebastien Bubeck</name>
    </author>
    <source>
      <id>https://blogs.princeton.edu/imabandit</id>
      <link href="https://blogs.princeton.edu/imabandit/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://blogs.princeton.edu/imabandit" rel="alternate" type="text/html"/>
      <subtitle>Random topics in optimization, probability, and statistics. By Sébastien Bubeck</subtitle>
      <title>I’m a bandit</title>
      <updated>2019-07-10T15:21:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.04302</id>
    <link href="http://arxiv.org/abs/1907.04302" rel="alternate" type="text/html"/>
    <title>Interactive Verifiable Polynomial Evaluation</title>
    <feedworld_mtime>1562716800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sahraei:Saeid.html">Saeid Sahraei</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maddah=Ali:Mohammad_Ali.html">Mohammad Ali Maddah-Ali</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Avestimehr:Salman.html">Salman Avestimehr</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.04302">PDF</a><br/><b>Abstract: </b>Cloud computing platforms have created the possibility for computationally
limited users to delegate demanding tasks to strong but untrusted servers.
Verifiable computing algorithms help build trust in such interactions by
enabling the server to provide a proof of correctness of his results which the
user can check very efficiently. In this paper, we present a doubly-efficient
interactive algorithm for verifiable polynomial evaluation. Unlike the
mainstream literature on verifiable computing, the soundness of our algorithm
is information-theoretic and cannot be broken by a computationally unbounded
server. By relying on basic properties of error correcting codes, our algorithm
enforces a dishonest server to provide false results to problems which become
progressively easier to verify. After roughly $\log d$ rounds, the user can
verify the response of the server against a look-up table that has been
pre-computed during an initialization phase. For a polynomial of degree $d$, we
achieve a user complexity of $O(d^{\epsilon})$, a server complexity of
$O(d^{1+\epsilon})$, a round complexity of $O(\log d)$ and an initialization
complexity of $O(d^{1+\epsilon})$.
</p></div>
    </summary>
    <updated>2019-07-10T01:20:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-07-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.04295</id>
    <link href="http://arxiv.org/abs/1907.04295" rel="alternate" type="text/html"/>
    <title>Better Sample -- Random Subset Sum in $2^{0.255n}$ and its Impact on Decoding Random Linear Codes</title>
    <feedworld_mtime>1562716800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Esser:Andre.html">Andre Esser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/May:Alexander.html">Alexander May</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.04295">PDF</a><br/><b>Abstract: </b>We propose a new heuristic algorithm for solving random subset sum instances
$a_1, \ldots, a_n, t \in \mathbb{Z}_{2^n}$, which play a crucial role in
cryptographic constructions. Our algorithm is search tree-based and solves the
instances in a divide-and-conquer method using the representation method. From
a high level perspective, our algorithm is similar to the algorithm of
Howgrave-Graham-Joux (HGJ) and Becker-Coron-Joux (BCJ), but instead of
enumerating the initial lists we sample candidate solutions. So whereas HGJ and
BCJ are based on combinatorics, our analysis is stochastic. Our sampling
technique introduces variance that increases the amount of representations and
gives our algorithm more optimization flexibility. This results in the
remarkable and natural property that we improve with increasing search tree
depth.
</p>
<p>Whereas BCJ achieves the currently best known (heuristic) run time
$2^{0.291n}$ for random subset sum, we improve (heuristically) down to
$2^{0.255n}$ using a search tree of depth at least $13$.
</p>
<p>We also apply our subset algorithm to the decoding of random binary linear
codes, where we improve the best known run time of the Becker-Joux-May-Meurer
algorithm from $2^{0.048n}$ in the half distance decoding setting down to
$2^{0.042n}$.
</p></div>
    </summary>
    <updated>2019-07-10T01:38:09Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.04284</id>
    <link href="http://arxiv.org/abs/1907.04284" rel="alternate" type="text/html"/>
    <title>No-dimensional Tverberg Theorems and Algorithms</title>
    <feedworld_mtime>1562716800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Choudhary:Aruni.html">Aruni Choudhary</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mulzer:Wolfgang.html">Wolfgang Mulzer</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.04284">PDF</a><br/><b>Abstract: </b>Tverberg's theorem is a classic result in discrete geometry. It states that
for any integer $k \ge 2$ and any finite $d$-dimensional point set $P \subseteq
\mathbb{R}^d$ of at least $(d + 1)(k - 1) + 1$ points, we can partition $P$
into $k$ subsets whose convex hulls have a non-empty intersection. The
computational problem of finding such a partition lies in the complexity class
$\mathrm{PPAD} \cap \mathrm{PLS}$, but no hardness results are known.
Tverberg's theorem also has a colorful variant: the points in $P$ have colors,
and under certain conditions, $P$ can be partitioned into colorful sets, i.e.,
sets in which each color appears exactly once such that the convex hulls of the
sets intersect.
</p>
<p>Recently, Adiprasito, Barany, and Mustafa [SODA 2019] proved a no-dimensional
version of Tverberg's theorem, in which the convex hulls of the sets in the
partition may intersect in an approximate fashion, relaxing the requirement on
the cardinality of $P$. The argument is constructive, but it does not result in
a polynomial-time algorithm.
</p>
<p>We present an alternative proof for a no-dimensional Tverberg theorem that
leads to an efficient algorithm to find the partition. More specifically, we
show an deterministic algorithm that finds for any set $P \subseteq
\mathbb{R}^d$ of $n$ points and any $k \in \{2, \dots, n\}$ in $O(nd \log k )$
time a partition of $P$ into $k$ subsets such that there is a ball of radius
$O(\frac{k}{\sqrt{n}}\textrm{diam}(P))$ intersecting the convex hull of each
subset. A similar result holds also for the colorful version.
</p>
<p>To obtain our result, we generalize Sarkaria's tensor product constructions
[Israel Journal Math., 1992] that reduces the Tverberg problem to the Colorful
Caratheodory problem. By carefully choosing the vectors used in the tensor
products, we implement the reduction in an efficient manner.
</p></div>
    </summary>
    <updated>2019-07-10T01:45:49Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-07-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.04279</id>
    <link href="http://arxiv.org/abs/1907.04279" rel="alternate" type="text/html"/>
    <title>Multiple Knapsack-Constrained Monotone DR-Submodular Maximization on Distributive Lattice --- Continuous Greedy Algorithm on Median Complex ---</title>
    <feedworld_mtime>1562716800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maehara:Takanori.html">Takanori Maehara</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nakashima:So.html">So Nakashima</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yamaguchi:Yutaro.html">Yutaro Yamaguchi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.04279">PDF</a><br/><b>Abstract: </b>We consider a problem of maximizing a monotone DR-submodular function under
multiple order-consistent knapsack constraints on a distributive lattice. Since
a distributive lattice is used to represent a dependency constraint, the
problem can represent a dependency constrained version of a submodular
maximization problem on a set. We propose a $1 - 1/e$ approximation algorithm
for this problem. To achieve this result, we generalize the continuous greedy
algorithm to distributive lattices: We choose a median complex as a continuous
relaxation of a distributive lattice and define the multilinear extension on
it. We show that the median complex admits special curves, named uniform linear
motions, such that the multilinear extension of a DR-submodular function is
concave along a positive uniform linear motion, which is a key property of the
continuous greedy algorithm.
</p></div>
    </summary>
    <updated>2019-07-10T01:38:17Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.04274</id>
    <link href="http://arxiv.org/abs/1907.04274" rel="alternate" type="text/html"/>
    <title>Reconstruction under outliers for Fourier-sparse functions</title>
    <feedworld_mtime>1562716800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Xue.html">Xue Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/De:Anindya.html">Anindya De</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.04274">PDF</a><br/><b>Abstract: </b>We consider the problem of learning an unknown $f$ with a sparse Fourier
spectrum in the presence of outlier noise. In particular, the algorithm has
access to a noisy oracle for (an unknown) $f$ such that (i) the Fourier
spectrum of $f$ is $k$-sparse; (ii) at any query point $x$, the oracle returns
$y$ such that with probability $1-\rho$, $|y-f(x)| \le \epsilon$. However, with
probability $\rho$, the error $y-f(x)$ can be arbitrarily large.
</p>
<p>We study Fourier sparse functions over both the discrete cube $\{0,1\}^n$ and
the torus $[0,1)$ and for both these domains, we design efficient algorithms
which can tolerate any $\rho&lt;1/2$ fraction of outliers. We note that the
analogous problem for low-degree polynomials has recently been studied in
several works~[AK03, GZ16, KKP17] and similar algorithmic guarantees are known
in that setting.
</p>
<p>While our main results pertain to the case where the location of the
outliers, i.e., $x$ such that $|y-f(x)|&gt;\epsilon$ is randomly distributed, we
also study the case where the outliers are adversarially located. In
particular, we show that over the torus, assuming that the Fourier transform
satisfies a certain \emph{granularity} condition, there is a sample efficient
algorithm to tolerate $\rho =\Omega(1)$ fraction of outliers and further, that
this is not possible without such a granularity condition. Finally, while not
the principal thrust, our techniques also allow us non-trivially improve on
learning low-degree functions $f$ on the hypercube in the presence of
adversarial outlier noise.
</p>
<p>Our techniques combine a diverse array of tools from compressive sensing,
sparse Fourier transform, chaining arguments and complex analysis.
</p></div>
    </summary>
    <updated>2019-07-10T01:38:40Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.04217</id>
    <link href="http://arxiv.org/abs/1907.04217" rel="alternate" type="text/html"/>
    <title>Streaming 1.9 Billion Hypersparse Network Updates per Second with D4M</title>
    <feedworld_mtime>1562716800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kepner:Jeremy.html">Jeremy Kepner</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gadepally:Vijay.html">Vijay Gadepally</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Milechin:Lauren.html">Lauren Milechin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Samsi:Siddharth.html">Siddharth Samsi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Arcand:William.html">William Arcand</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bestor:David.html">David Bestor</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bergeron:William.html">William Bergeron</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Byun:Chansup.html">Chansup Byun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hubbell:Matthew.html">Matthew Hubbell</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Houle:Michael.html">Michael Houle</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jones:Michael.html">Michael Jones</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Klein:Anne.html">Anne Klein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Michaleas:Peter.html">Peter Michaleas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mullen:Julie.html">Julie Mullen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Prout:Andrew.html">Andrew Prout</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rosa:Antonio.html">Antonio Rosa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yee:Charles.html">Charles Yee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reuther:Albert.html">Albert Reuther</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.04217">PDF</a><br/><b>Abstract: </b>The Dynamic Distributed Dimensional Data Model (D4M) library implements
associative arrays in a variety of languages (Python, Julia, and Matlab/Octave)
and provides a lightweight in-memory database implementation of hypersparse
arrays that are ideal for analyzing many types of network data. D4M relies on
associative arrays which combine properties of spreadsheets, databases,
matrices, graphs, and networks, while providing rigorous mathematical
guarantees, such as linearity. Streaming updates of D4M associative arrays put
enormous pressure on the memory hierarchy. This work describes the design and
performance optimization of an implementation of hierarchical associative
arrays that reduces memory pressure and dramatically increases the update rate
into an associative array. The parameters of hierarchical associative arrays
rely on controlling the number of entries in each level in the hierarchy before
an update is cascaded. The parameters are easily tunable to achieve optimal
performance for a variety of applications. Hierarchical arrays achieve over
40,000 updates per second in a single instance. Scaling to 34,000 instances of
hierarchical D4M associative arrays on 1,100 server nodes on the MIT SuperCloud
achieved a sustained update rate of 1,900,000,000 updates per second. This
capability allows the MIT SuperCloud to analyze extremely large streaming
network data sets.
</p></div>
    </summary>
    <updated>2019-07-10T01:32:54Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.04211</id>
    <link href="http://arxiv.org/abs/1907.04211" rel="alternate" type="text/html"/>
    <title>Universal One-Dimensional Cellular Automata Derived for Turing Machines and its Dynamical Behaviour</title>
    <feedworld_mtime>1562716800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Sergio J. Martinez, Ivan M. Mendoza, Genaro J. Martinez, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Ninagawa:Shigeru.html">Shigeru Ninagawa</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.04211">PDF</a><br/><b>Abstract: </b>Universality in cellular automata theory is a central problem studied and
developed from their origins by John von Neumann. In this paper, we present an
algorithm where any Turing machine can be converted to one-dimensional cellular
automaton with a 2-linear time and display its spatial dynamics. Three
particular Turing machines are converted in three universal one-dimensional
cellular automata, they are: binary sum, rule 110 and a universal reversible
Turing machine.
</p></div>
    </summary>
    <updated>2019-07-10T01:37:57Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.04165</id>
    <link href="http://arxiv.org/abs/1907.04165" rel="alternate" type="text/html"/>
    <title>Global Cardinality Constraints Make Approximating Some Max-2-CSPs Harder</title>
    <feedworld_mtime>1562716800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Austrin:Per.html">Per Austrin</a>, Aleksa Stankovic <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.04165">PDF</a><br/><b>Abstract: </b>Assuming the Unique Games Conjecture, we show that existing approximation
algorithms for some Boolean Max-2-CSPs with cardinality constraints are
optimal. In particular, we prove that Max-Cut with cardinality constraints is
UG-hard to approximate within \approx 0.858, and that Max-2-Sat with
cardinality constraints is UG-hard to approximate within \approx 0.929. In both
cases, the previous best hardness results were the same as the hardness of the
corresponding unconstrained Max-2-CSP (\approx 0.878 for Max-Cut, and \approx
0.940 for Max-2-Sat). The hardness for Max-2-Sat applies to monotone Max-2-Sat
instances, meaning that we also obtain tight inapproximability for the
Max-k-Vertex-Cover problem.
</p></div>
    </summary>
    <updated>2019-07-10T01:23:02Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-07-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.04154</id>
    <link href="http://arxiv.org/abs/1907.04154" rel="alternate" type="text/html"/>
    <title>EU H2020 Gauss project. Geo-Fencing Software System</title>
    <feedworld_mtime>1562716800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Hao.html">Hao Xu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.04154">PDF</a><br/><b>Abstract: </b>The Geofencing system is the key to operate the Unmanned Aerial Vehicle (UAV)
within the safe and appropriate zone to avoid public concerns and other privacy
issues. The system is designed to keep the UAV away from geofenced obstacles
using the onboard GNSS and IMU location. The Geofencing system is part of the
H2020 GAUSS project and facilities other subsystems, for instance, to support
the command and control link, which is the security measure to secure the UAV
from hijacking and signal spoofing. The regulatory authorities expressed the
concern of having UAVs flying in the no-fly zone and causing troubles from
offending private privacy to hazards at airport airspace. Hence the geofence
system shall provide guidance message, which enables the UAV to evacuate from
no-fly-zone, based on real-time updated location. This thesis aims to first
illustrate the generation of geofence and then apply the geofence system on UAV
operation. This application enables UAV to fly in the designated area without
human intervention. The project is built with JAVA using GIS-enabled Database
Management System and Open Soured Map data powered by OpenStreetMap and OS map.
This method has been tested by simulations which had results of high accuracy.
</p></div>
    </summary>
    <updated>2019-07-10T01:46:33Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-07-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.04132</id>
    <link href="http://arxiv.org/abs/1907.04132" rel="alternate" type="text/html"/>
    <title>Linear MIM-Width of Trees</title>
    <feedworld_mtime>1562716800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Svein Høgemo, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Telle:Jan_Arne.html">Jan Arne Telle</a>, Erlend Raa Vågset <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.04132">PDF</a><br/><b>Abstract: </b>We provide an $O(n \log n)$ algorithm computing the linear maximum induced
matching width of a tree and an optimal layout.
</p></div>
    </summary>
    <updated>2019-07-10T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.04117</id>
    <link href="http://arxiv.org/abs/1907.04117" rel="alternate" type="text/html"/>
    <title>A spectral bound on hypergraph discrepancy</title>
    <feedworld_mtime>1562716800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Potukuchi:Aditya.html">Aditya Potukuchi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.04117">PDF</a><br/><b>Abstract: </b>Let $\mathcal{H}$ be a $t$-regular hypergraph on $n$ vertices and $m$ edges.
Let $M$ be the $m \times n$ incidence matrix of $\mathcal{H}$ and let us denote
$\lambda =\max_{v \in \mathbf{1}^{\perp}} \frac{1}{\sqrt{t}\|v\|}\|Mv\|$. We
show that the discrepancy of $\mathcal{H}$ is $O(\sqrt{\lambda t})$. As a
corollary, this gives us that for every $t$, the discrepancy of a random
$t$-regular hypergraph with $n$ vertices and $m \geq n$ edges is almost surely
$O(\sqrt{t})$ as $n$ grows. The proof also gives a polynomial time algorithm
that takes a hypergraph as input and outputs a coloring with the above
guarantee.
</p></div>
    </summary>
    <updated>2019-07-10T01:26:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.04088</id>
    <link href="http://arxiv.org/abs/1907.04088" rel="alternate" type="text/html"/>
    <title>$r$-Gather Clustering and $r$-Gathering on Spider: FPT Algorithms and Hardness</title>
    <feedworld_mtime>1562716800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumabe:Soh.html">Soh Kumabe</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maehara:Takanori.html">Takanori Maehara</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.04088">PDF</a><br/><b>Abstract: </b>We consider min-max $r$-gather clustering problem and min-max $r$-gathering
problem. In the min-max $r$-gather clustering problem, we are given a set of
users and divide them into clusters with size at least $r$; the goal is to
minimize the maximum diameter of clusters. In the min-max $r$-gathering
problem, we are additionally given a set of facilities and assign each cluster
to a facility; the goal is to minimize the maximum distance between the users
and the assigned facility. In this study, we consider the case that the users
and facilities are located on a ``spider'' and propose the first
fixed-parameter tractable (FPT) algorithms for both problems, which are
parametrized by only the number of legs. Furthermore, we prove that these
problems are NP-hard when the number of legs is arbitrarily large.
</p></div>
    </summary>
    <updated>2019-07-10T01:26:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.04087</id>
    <link href="http://arxiv.org/abs/1907.04087" rel="alternate" type="text/html"/>
    <title>PTAS and Exact Algorithms for $r$-Gathering Problems on Tree</title>
    <feedworld_mtime>1562716800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumabe:Soh.html">Soh Kumabe</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maehara:Takanori.html">Takanori Maehara</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.04087">PDF</a><br/><b>Abstract: </b>r-gathering problem is a variant of facility location problems. In this
problem, we are given a set of users and a set of facilities on same metric
space. We open some of the facilities and assign each user to an open facility,
so that at least r users are assigned to every open facility. We aim to
minimize the maximum distance between user and assigned facility. In general,
this problem is NP-hard and admit an approximation algorithm with factor 3. It
is known that the problem does not admit any approximation algorithm within a
factor less than 3. In our another paper, we proved that this problem is
NP-hard even on spider, which is a special case of tree metric. In this paper,
we concentrate on the problems on a tree. First, we give a PTAS for r-gathering
problem on a tree. Furthermore, we give PTAS for some variants of the problems
on a tree, and also give exact polynomial-time algorithms for another variants
of r-gathering problem on a tree.
</p></div>
    </summary>
    <updated>2019-07-10T01:23:58Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.04083</id>
    <link href="http://arxiv.org/abs/1907.04083" rel="alternate" type="text/html"/>
    <title>Stochastic Monotone Submodular Maximization with Queries</title>
    <feedworld_mtime>1562716800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maehara:Takanori.html">Takanori Maehara</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yamaguchi:Yutaro.html">Yutaro Yamaguchi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.04083">PDF</a><br/><b>Abstract: </b>We study a stochastic variant of monotone submodular maximization problem as
follows. We are given a monotone submodular function as an objective function
and a feasible domain defined on a finite set, and our goal is to find a
feasible solution that maximizes the objective function. A special part of the
problem is that each element in the finite set has a random hidden state,
active or inactive, only the active elements contribute to the objective value,
and we can conduct a query to an element to reveal its hidden state. The goal
is to obtain a feasible solution having a large objective value by conducting a
small number of queries. This is the first attempt to consider nonlinear
objective functions in such a stochastic model.
</p>
<p>We prove that the problem admits a good query strategy if the feasible domain
has a uniform exchange property. This result generalizes Blum et al.'s result
on the unweighted matching problem and Behnezhad and Reyhani's result on the
weighted matching problem in both objective function and feasible domain.
</p></div>
    </summary>
    <updated>2019-07-10T01:24:20Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.04065</id>
    <link href="http://arxiv.org/abs/1907.04065" rel="alternate" type="text/html"/>
    <title>Trustworthy Graph Algorithms</title>
    <feedworld_mtime>1562716800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Abdulaziz:Mohammad.html">Mohammad Abdulaziz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mehlhorn:Kurt.html">Kurt Mehlhorn</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nipkow:Tobias.html">Tobias Nipkow</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.04065">PDF</a><br/><b>Abstract: </b>The goal of the LEDA project was to build an easy-to-use and extendable
library of correct and efficient data structures, graph algorithms and
geometric algorithms. We report on the use of formal program verification to
achieve an even higher level of trustworthiness. Specifically, we report on an
ongoing and largely finished verification of the blossom-shrinking algorithm
for maximum cardinality matching.
</p></div>
    </summary>
    <updated>2019-07-10T01:39:26Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.03963</id>
    <link href="http://arxiv.org/abs/1907.03963" rel="alternate" type="text/html"/>
    <title>Vertex-weighted Online Stochastic Matching with Patience Constraints</title>
    <feedworld_mtime>1562716800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brubach:Brian.html">Brian Brubach</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grammel:Nathaniel.html">Nathaniel Grammel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Srinivasan:Aravind.html">Aravind Srinivasan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.03963">PDF</a><br/><b>Abstract: </b>Online Bipartite Matching is a classic problem introduced by Karp, Vazirani,
and Vazirani (Proc. ACM STOC, 1990) and motivated by applications such as
e-commerce, online advertising, and ride-sharing. We wish to match a set of
online vertices (e.g., webpage views, users, customers, or riders) which arrive
sequentially one-by-one to a set of offline vertices (e.g., ads, items, or
drivers). Each vertex can be matched at most once and the goal is to maximize
the matching size.
</p>
<p>Here, we consider the more general problem allowing vertex weights (on the
offline vertices, representing differing item prices, for example), stochastic
rewards (attempted matches may be unsuccessful, providing no reward), and
patience constraints (each online vertex has a patience, representing the
maximum number of unsuccessful match attempts allowed for it). In doing so, we
discuss and clarify the definition of competitive ratio in the stochastic
setting and compare competing notions. Specifically, we advocate for the use of
a competitive ratio that compares the performance of an online algorithm to
that of an offline algorithm in the same stochastic setting, whereas some prior
work---see, e.g., Mehta and Panigrahi (FOCS, 2012)---uses a linear programming
(LP) formulation to compare the performance of an online algorithm to the
optimal solution of a closely related non-stochastic offline problem. We define
the stochasticity gap for evaluating the usefulness of an LP formulation (which
exchanges probabilities for fractional coefficients) of a given stochastic
problem and bound this gap for a commonly used LP. We further present and
analyze a new algorithm for our problem, achieving a competitive ratio of 0.5,
the first constant competitive ratio for this and several related problems.
</p></div>
    </summary>
    <updated>2019-07-10T01:24:37Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.03931</id>
    <link href="http://arxiv.org/abs/1907.03931" rel="alternate" type="text/html"/>
    <title>Near-optimal Repair of Reed-Solomon Codes with Low Sub-packetization</title>
    <feedworld_mtime>1562716800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Guruswami:Venkatesan.html">Venkatesan Guruswami</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jiang:Haotian.html">Haotian Jiang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.03931">PDF</a><br/><b>Abstract: </b>Minimum storage regenerating (MSR) codes are MDS codes which allow for
recovery of any single erased symbol with optimal repair bandwidth, based on
the smallest possible fraction of the contents downloaded from each of the
other symbols. Recently, certain Reed-Solomon codes were constructed which are
MSR. However, the sub-packetization of these codes is exponentially large,
growing like $n^{\Omega(n)}$ in the constant-rate regime. In this work, we
study the relaxed notion of $\epsilon$-MSR codes, which incur a factor of
$(1+\epsilon)$ higher than the optimal repair bandwidth, in the context of
Reed-Solomon codes. We give constructions of constant-rate $\epsilon$-MSR
Reed-Solomon codes with polynomial sub-packetization of $n^{O(1/\epsilon)}$ and
thereby giving an explicit tradeoff between the repair bandwidth and
sub-packetization.
</p></div>
    </summary>
    <updated>2019-07-10T01:26:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.03863</id>
    <link href="http://arxiv.org/abs/1907.03863" rel="alternate" type="text/html"/>
    <title>The Densest k Subgraph Problem in b-Outerplanar Graphs</title>
    <feedworld_mtime>1562716800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Sean Gonzales, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Migler:Theresa.html">Theresa Migler</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.03863">PDF</a><br/><b>Abstract: </b>We give an exact $O(nk^2)$ algorithm for finding the densest k subgraph in
outerplanar graphs. We extend this to an exact $O(nk^2 8^b)$ algorithm for
finding the densest k subgraph in b-outerplanar graphs. Finally, we hypothesize
that Baker's PTAS technique will not work for the densest k subgraph problem in
planar graphs.
</p></div>
    </summary>
    <updated>2019-07-10T01:24:14Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.03850</id>
    <link href="http://arxiv.org/abs/1907.03850" rel="alternate" type="text/html"/>
    <title>Counting and Finding Homomorphisms is Universal for Parameterized Complexity Theory</title>
    <feedworld_mtime>1562716800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roth:Marc.html">Marc Roth</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wellnitz:Philip.html">Philip Wellnitz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.03850">PDF</a><br/><b>Abstract: </b>Counting homomorphisms from a graph $H$ into another graph $G$ is a
fundamental problem of (parameterized) counting complexity theory. In this
work, we study the case where \emph{both} graphs $H$ and $G$ stem from given
classes of graphs: $H\in \mathcal{H}$ and $G\in \mathcal{G}$. By this, we
combine the structurally restricted version of this problem, with the
language-restricted version.
</p>
<p>Our main result is a construction based on Kneser graphs that associates
every problem $\tt P$ in $\#\mathsf{W[1]}$ with two classes of graphs
$\mathcal{H}$ and $\mathcal{G}$ such that the problem $\tt P$ is
\emph{equivalent} to the problem $\#{\tt HOM}(\mathcal{H}\to \mathcal{G})$ of
counting homomorphisms from a graph in $\mathcal{H}$ to a graph in
$\mathcal{G}$. In view of Ladner's seminal work on the existence of
$\mathsf{NP}$-intermediate problems [J.ACM'75] and its adaptations to the
parameterized setting, a classification of the class $\#\mathsf{W[1]}$ in
fixed-parameter tractable and $\#\mathsf{W[1]}$-complete cases is unlikely.
Hence, obtaining a complete classification for the problem $\#{\tt
HOM}(\mathcal{H}\to \mathcal{G})$ seems unlikely. Further, our proofs easily
adapt to $\mathsf{W[1]}$.
</p>
<p>In search of complexity dichotomies, we hence turn to special graph classes.
Those classes include line graphs, claw-free graphs, perfect graphs, and
combinations thereof, and $F$-colorable graphs for fixed graphs $F$: If the
class $\mathcal{G}$ is one of those classes and the class $\mathcal{H}$ is
closed under taking minors, then we establish explicit criteria for the class
$\mathcal{H}$ that partition the family of problems $\#{\tt
HOM}(\mathcal{H}\to\mathcal{G})$ into polynomial-time solvable and
$\#\mathsf{W[1]}$-hard cases. In particular, we can drop the condition of
$\mathcal{H}$ being minor-closed for $F$-colorable graphs.
</p></div>
    </summary>
    <updated>2019-07-10T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.03816</id>
    <link href="http://arxiv.org/abs/1907.03816" rel="alternate" type="text/html"/>
    <title>The Power of Comparisons for Actively Learning Linear Classifiers</title>
    <feedworld_mtime>1562716800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hopkins:Max.html">Max Hopkins</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kane:Daniel_M=.html">Daniel M. Kane</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lovett:Shachar.html">Shachar Lovett</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.03816">PDF</a><br/><b>Abstract: </b>In the world of big data, large but costly to label datasets dominate many
fields. Active learning, an unsupervised alternative to the standard
PAC-learning model, was introduced to explore whether adaptive labeling could
learn concepts with exponentially fewer labeled samples. While previous results
show that active learning performs no better than its supervised alternative
for important concept classes such as linear separators, we show that by adding
weak distributional assumptions and allowing comparison queries, active
learning requires exponentially fewer samples. Further, we show that these
results hold as well for a stronger model of learning called Reliable and
Probably Useful (RPU) learning. In this model, our learner is not allowed to
make mistakes, but may instead answer "I don't know." While previous negative
results showed this model to have intractably large sample complexity for label
queries, we show that comparison queries make RPU-learning at worst
logarithmically more expensive in the passive case, and quadratically more
expensive in the active case.
</p></div>
    </summary>
    <updated>2019-07-10T01:48:24Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-07-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.03797</id>
    <link href="http://arxiv.org/abs/1907.03797" rel="alternate" type="text/html"/>
    <title>Faster Deterministic Distributed Coloring Through Recursive List Coloring</title>
    <feedworld_mtime>1562716800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuhn:Fabian.html">Fabian Kuhn</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.03797">PDF</a><br/><b>Abstract: </b>We provide novel deterministic distributed vertex coloring algorithms. As our
main result, we give a deterministic distributed algorithm to compute a
$(\Delta+1)$-coloring of an $n$-node graph with maximum degree $\Delta$ in
$2^{O(\sqrt{\log\Delta})}\cdot\log n$ rounds. This improves on the best
previously known time complexity for a large range of values of $\Delta$. For
graphs with arboricity $a$, we obtain a deterministic distributed algorithm to
compute a $(2+o(1))a$-coloring in time $2^{O(\sqrt{\log a})}\cdot\log^2 n$.
Further, for graphs with bounded neighborhood independence, we show that a
$(\Delta+1)$-coloring can be computed more efficiently in time
$2^{O(\sqrt{\log\Delta})} + O(\log^* n)$. This in particular implies that also
a $(2\Delta-1)$-edge coloring can be computed deterministically in
$2^{O(\sqrt{\log\Delta})} + O(\log^* n)$ rounds, which improves the best known
time bound for small values of $\Delta$. All results even hold for the list
coloring variants of the problems. As a consequence, we also obtain an improved
deterministic $2^{O(\sqrt{\log\Delta})}\cdot\log^3 n$-round algorithm for
$\Delta$-coloring non-complete graphs with maximum degree $\Delta\geq 3$. Most
of our algorithms only require messages of $O(\log n)$ bits (including the
$(\Delta+1)$-vertex coloring algorithms).
</p>
<p>Our main technical contribution is a recursive deterministic distributed list
coloring algorithm to solve list coloring problems with lists of size
$\Delta^{1+o(1)}$. Given some list coloring problem and an orientation of the
edges, we show how to recursively divide the global color space into smaller
subspaces, assign one of the subspaces to each node of the graph, and compute a
new edge orientation such that for each node, the list size to out-degree ratio
degrades at most by a constant factor on each recursion level.
</p></div>
    </summary>
    <updated>2019-07-10T01:31:20Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-10T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/092</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/092" rel="alternate" type="text/html"/>
    <title>TR19-092 |  Revisiting Alphabet Reduction in Dinur&amp;#39;s PCP | 

	Venkatesan Guruswami, 

	Jakub Opršal, 

	Sai Sandeep</title>
    <summary>Dinur's celebrated proof of the PCP theorem alternates two main steps in several iterations: gap amplification to increase the soundness gap by a large constant factor (at the expense of much larger alphabet size), and a composition step that brings back the alphabet size to an absolute constant (at the expense of a fixed constant factor loss in the soundness gap). We note that the gap amplification can produce a Label Cover CSP. This allows us to reduce the alphabet size via a direct long-code based reduction from Label Cover to a Boolean CSP. Our composition step thus bypasses the concept of Assignment Testers from Dinur's proof, and we believe it is more intuitive --- it is just a gadget reduction. The analysis also uses only elementary facts (Parseval's identity) about Fourier Transforms over the hypercube.</summary>
    <updated>2019-07-09T16:04:11Z</updated>
    <published>2019-07-09T16:04:11Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-07-10T16:20:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=17523</id>
    <link href="https://gilkalai.wordpress.com/2019/07/09/imre-barany-limit-shape/" rel="alternate" type="text/html"/>
    <title>Imre Bárány: Limit shape</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Limit shapes are fascinating objects in the interface between probability and geometry and between the discrete and the continuous. This post is kindly contributed by Imre Bárány. What is a limit shape? There are finitely many convex lattice polygons contained … <a href="https://gilkalai.wordpress.com/2019/07/09/imre-barany-limit-shape/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>Limit shapes are fascinating objects in the interface between probability and geometry and between the discrete and the continuous. This post is kindly contributed by Imre Bárány.</em></p>
<h3><a href="https://gilkalai.files.wordpress.com/2019/07/imre_barany_2011.jpg"><img alt="" class="alignnone size-full wp-image-17560" src="https://gilkalai.files.wordpress.com/2019/07/imre_barany_2011.jpg?w=640"/></a></h3>
<h2>What is a limit shape?</h2>
<p>There are finitely many convex lattice polygons contained in the <img alt="{[0,n]^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5B0%2Cn%5D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{[0,n]^2}"/> square. Their number turns out to be</p>
<p><img alt="\displaystyle \exp\{3\sqrt[3]{\zeta(3)/\zeta(2)}n^{2/3}(1+o(1))\}. \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cexp%5C%7B3%5Csqrt%5B3%5D%7B%5Czeta%283%29%2F%5Czeta%282%29%7Dn%5E%7B2%2F3%7D%281%2Bo%281%29%29%5C%7D.+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \exp\{3\sqrt[3]{\zeta(3)/\zeta(2)}n^{2/3}(1+o(1))\}. \ \ \ \ \ (1)"/></p>
<p>This is a large number. How does a typical element of this large set look? Is there a<br/>
limit shape of these convex lattice polygons?</p>
<p>To answer this question it is convenient to consider the lattice <img alt="{{\mathbb{Z}}_n=\frac 1n {\mathbb{Z}}^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BZ%7D%7D_n%3D%5Cfrac+1n+%7B%5Cmathbb%7BZ%7D%7D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\mathbb{Z}}_n=\frac 1n {\mathbb{Z}}^2}"/> and define <img alt="{\mathcal{F}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BF%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{F}^n}"/> as the family of all convex <img alt="{{\mathbb{Z}}_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BZ%7D%7D_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\mathbb{Z}}_n}"/>-lattice polygons lying in the unit square <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q}"/>. The polygons in <img alt="{\mathcal{F}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BF%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{F}^n}"/> have a limit shape (as <img alt="{n\rightarrow \infty}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n\rightarrow \infty}"/>) if there is a convex set <img alt="{K\subset Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%5Csubset+Q%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K\subset Q}"/> such that the overwhelming majority of the polygons in <img alt="{\mathcal{F}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BF%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{F}^n}"/> are very close to <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/>. In other words, for every <img alt="{\epsilon&gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon&gt;0}"/> the number of polygons in <img alt="{\mathcal{F}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BF%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{F}^n}"/> that are farther than <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/> from <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> (in Hausdorff distance, say) is a minute part of <img alt="{\mathcal{F}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BF%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{F}^n}"/>, that is, <img alt="{o(|\mathcal{F}^n|)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bo%28%7C%5Cmathcal%7BF%7D%5En%7C%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{o(|\mathcal{F}^n|)}"/> as <img alt="{n \rightarrow \infty}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n \rightarrow \infty}"/>. To put it differently, the average of the characteristic functions <img alt="{\chi_P(.)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cchi_P%28.%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\chi_P(.)}"/> of <img alt="{P\in \mathcal{F}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%5Cin+%5Cmathcal%7BF%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P\in \mathcal{F}^n}"/> tends to a zero-one function:</p>
<p><img alt="\lim {\rm Ave}_{P\in \mathcal{F}^n}\chi_P(x)=\begin{cases} 1&amp; \mbox{ if } x \in K,\\ 0&amp; \mbox{ if } x \notin K. \end{cases} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Clim+%7B%5Crm+Ave%7D_%7BP%5Cin+%5Cmathcal%7BF%7D%5En%7D%5Cchi_P%28x%29%3D%5Cbegin%7Bcases%7D+1%26+%5Cmbox%7B+if+%7D+x+%5Cin+K%2C%5C%5C+0%26+%5Cmbox%7B+if+%7D+x+%5Cnotin+K.+%5Cend%7Bcases%7D+&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\lim {\rm Ave}_{P\in \mathcal{F}^n}\chi_P(x)=\begin{cases} 1&amp; \mbox{ if } x \in K,\\ 0&amp; \mbox{ if } x \notin K. \end{cases} "/></p>
<h2>The limit shape theorem</h2>
<h3/>
<p>The limit shape theorem says that such a <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K"/> exists, its boundary consists of four parabola arcs each touching consecutive sides of <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="Q"/> at their midpoints, see the figure.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/07/limitshape.png"><img alt="" class="alignnone size-full wp-image-17536" src="https://gilkalai.files.wordpress.com/2019/07/limitshape.png?w=640"/></a></p>
<h3>Generating functions and saddle point methods</h3>
<p>The proof is based on the fact that a convex (lattice or non-lattice) polygon with vertices <img alt="v_1,\ldots,v_n" class="latex" src="https://s0.wp.com/latex.php?latex=v_1%2C%5Cldots%2Cv_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v_1,\ldots,v_n"/> (in this order on its boundary) is uniquely determined by the edge-vectors <img alt="v_2-v_1,\ldots,v_n-v_{n-1}, v_1-v_n" class="latex" src="https://s0.wp.com/latex.php?latex=v_2-v_1%2C%5Cldots%2Cv_n-v_%7Bn-1%7D%2C+v_1-v_n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v_2-v_1,\ldots,v_n-v_{n-1}, v_1-v_n"/>. Using this one can write down the generating function of the number of convex lattice paths from <img alt="(0,0)" class="latex" src="https://s0.wp.com/latex.php?latex=%280%2C0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(0,0)"/> to <img alt="(a,b)\in \mathbb{Z}^2" class="latex" src="https://s0.wp.com/latex.php?latex=%28a%2Cb%29%5Cin+%5Cmathbb%7BZ%7D%5E2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(a,b)\in \mathbb{Z}^2"/> lying in the triangle whose vertices are <img alt="(0,0),(a,0)" class="latex" src="https://s0.wp.com/latex.php?latex=%280%2C0%29%2C%28a%2C0%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(0,0),(a,0)"/> and <img alt="(a,b)" class="latex" src="https://s0.wp.com/latex.php?latex=%28a%2Cb%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(a,b)"/>. And this number can be estimated by saddle point methods (from complex variables). This is also how formula (1) for <img alt="|\mathcal{F}^n|" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Cmathcal%7BF%7D%5En%7C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="|\mathcal{F}^n|"/> can be established.</p>
<h3>A beautiful geometric result</h3>
<p>On the geometry part one needs a beautiful (and almost elementary) result saying that, in a triangle <img alt="T" class="latex" src="https://s0.wp.com/latex.php?latex=T&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T"/> with vertices <img alt="1,2,3" class="latex" src="https://s0.wp.com/latex.php?latex=1%2C2%2C3&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="1,2,3"/> and with subtriangles <img alt="T_1" class="latex" src="https://s0.wp.com/latex.php?latex=T_1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T_1"/> and <img alt="T_2" class="latex" src="https://s0.wp.com/latex.php?latex=T_2&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="T_2"/> (see the figure)</p>
<p><img alt="\displaystyle \sqrt[3]{\textrm{Area} \; T}\ge \sqrt[3]{\textrm{Area} \; T_1}+\sqrt[3]{\textrm{Area} \; T_2}, \ \ \ \ \ (2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csqrt%5B3%5D%7B%5Ctextrm%7BArea%7D+%5C%3B+T%7D%5Cge+%5Csqrt%5B3%5D%7B%5Ctextrm%7BArea%7D+%5C%3B+T_1%7D%2B%5Csqrt%5B3%5D%7B%5Ctextrm%7BArea%7D+%5C%3B+T_2%7D%2C+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \sqrt[3]{\textrm{Area} \; T}\ge \sqrt[3]{\textrm{Area} \; T_1}+\sqrt[3]{\textrm{Area} \; T_2}, \ \ \ \ \ (2)"/></p>
<p><a href="https://gilkalai.files.wordpress.com/2019/07/triangle.png"><img alt="" class="alignnone size-full wp-image-17537" src="https://gilkalai.files.wordpress.com/2019/07/triangle.png?w=640"/></a></p>
<p>with equality iff the line segment 46 is touches the special parabola arc at point 5. The special parabola arc is the one that touches sides 12 and 13 of <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> at points 2 and 3. I was very proud of inequality (2) but it turned out that it had been known for long (cf Blaschke: Vorlesungen Über Differentialgeometrie II, (1923) page 38).</p>
<p>In the proof one needs a slightly stronger version of (2). Assuming point 4 (resp. 6) divides segment 12 (and 13) in ratio <img alt="{1-a:a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1-a%3Aa%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1-a:a}"/>, (and <img alt="{b:1-b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%3A1-b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b:1-b}"/>), the stronger inequality says that</p>
<p><img alt="\displaystyle \sqrt[3]{\textrm{Area} \; T}\left(1-\frac13 (a-b)^2\right)\ge \sqrt[3]{\textrm{Area} \; T_1}+\sqrt[3]{\textrm{Area} \; T_2}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Csqrt%5B3%5D%7B%5Ctextrm%7BArea%7D+%5C%3B+T%7D%5Cleft%281-%5Cfrac13+%28a-b%29%5E2%5Cright%29%5Cge+%5Csqrt%5B3%5D%7B%5Ctextrm%7BArea%7D+%5C%3B+T_1%7D%2B%5Csqrt%5B3%5D%7B%5Ctextrm%7BArea%7D+%5C%3B+T_2%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \sqrt[3]{\textrm{Area} \; T}\left(1-\frac13 (a-b)^2\right)\ge \sqrt[3]{\textrm{Area} \; T_1}+\sqrt[3]{\textrm{Area} \; T_2}, "/></p>
<p>which was probably not known to Blaschke.</p>
<p>It is important to point out that <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> is the unique convex subset of <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q}"/> whose affine perimeter is the largest among all convex subsets of <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q}"/>. I’ll return to the affine perimeter later.</p>
<p>Yakov Sinai came up with a different, elegant, and more powerful proof using canonical ensembles from statistical physics. His method was developed further by Vershik and Zeitouni, by Bureaux and Enriquez, by Bogachev and Zarbaliev.</p>
<h2>Limit shapes for polygons in convex bodies</h2>
<p>More generally, one can consider a convex body (compact convex set with non-empty interior) <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> in the plane and the family <img alt="{\mathcal{F}^n(C)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BF%7D%5En%28C%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{F}^n(C)}"/> of all convex <img alt="{{\mathbb{Z}}_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BZ%7D%7D_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\mathbb{Z}}_n}"/>-lattice polygons contained in <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>, and ask whether a similar limit shape exists in this case. The answer is yes. The limit shape, <img alt="{C_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_0}"/>, is the unique convex subset of <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> whose affine perimeter is maximal among all convex subsets of <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. The affine perimeter is upper semicontinuous, implying the existence of convex subset of <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> with maximal affine perimeter. The proof of its uniqueness requires extra effort. In the case when <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> is the unit square <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q}"/>, the limit shape <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> is equal to <img alt="{Q_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_0}"/>. Note that for every convex body <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> with <img alt="{Q_0\subset C \subset Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ_0%5Csubset+C+%5Csubset+Q%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q_0\subset C \subset Q}"/>, <img alt="{C_0=Q_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_0%3DQ_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_0=Q_0}"/>.</p>
<h2>Random points vs. lattice points</h2>
<p>What happens if, instead of the <img alt="{{\mathbb{Z}}_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BZ%7D%7D_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\mathbb{Z}}_n}"/>-lattice points in <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>, we take a random sample <img alt="{X_n=\{x_1,\ldots,x_n\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_n%3D%5C%7Bx_1%2C%5Cldots%2Cx_n%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X_n=\{x_1,\ldots,x_n\}}"/> of points from <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>, chosen independently and uniformly? Let <img alt="{\mathcal{G}(X_n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BG%7D%28X_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{G}(X_n)}"/> be the set of all polygons whose vertices belong to <img alt="{X_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X_n}"/>. This is again a finite set and one can show that<br/>
<img alt="\displaystyle \lim_{n\rightarrow \infty} n^{-1/3}\log \mathop{\mathbb E}( |\mathcal{G}(X_n)|)=3\cdot2^{-2/3}\frac{A^*(C)}{\sqrt[3]{\textrm{Area} \; C}}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Clim_%7Bn%5Crightarrow+%5Cinfty%7D+n%5E%7B-1%2F3%7D%5Clog+%5Cmathop%7B%5Cmathbb+E%7D%28+%7C%5Cmathcal%7BG%7D%28X_n%29%7C%29%3D3%5Ccdot2%5E%7B-2%2F3%7D%5Cfrac%7BA%5E%2A%28C%29%7D%7B%5Csqrt%5B3%5D%7B%5Ctextrm%7BArea%7D+%5C%3B+C%7D%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \lim_{n\rightarrow \infty} n^{-1/3}\log \mathop{\mathbb E}( |\mathcal{G}(X_n)|)=3\cdot2^{-2/3}\frac{A^*(C)}{\sqrt[3]{\textrm{Area} \; C}}, "/></p>
<p>where <img alt="{A^*(C)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%5E%2A%28C%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A^*(C)}"/> is equal to the affine perimeter, <img alt="{AP(C_0)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAP%28C_0%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{AP(C_0)}"/>, of <img alt="{C_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_0}"/>. This confirms the philosophy (or my intuition) that random points and lattice points in convex bodies behave similarly. Note that <img alt="{\mathop{\mathbb E}|\mathcal{G}(X_n)|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathop%7B%5Cmathbb+E%7D%7C%5Cmathcal%7BG%7D%28X_n%29%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathop{\mathbb E}|\mathcal{G}(X_n)|}"/> is of order <img alt="{\exp\{c_1n^{1/3}\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cexp%5C%7Bc_1n%5E%7B1%2F3%7D%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\exp\{c_1n^{1/3}\}}"/> while <img alt="{|\mathcal{F}^n(C)|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%5Cmathcal%7BF%7D%5En%28C%29%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|\mathcal{F}^n(C)|}"/> is of order <img alt="{\exp\{c_2n^{2/3}\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cexp%5C%7Bc_2n%5E%7B2%2F3%7D%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\exp\{c_2n^{2/3}\}}"/> which is fine as number of <img alt="{{\mathbb{Z}}_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BZ%7D%7D_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\mathbb{Z}}_n}"/>-lattice points in <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> is approximately <img alt="{n^2 \textrm{Area} \; C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E2+%5Ctextrm%7BArea%7D+%5C%3B+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^2 \textrm{Area} \; C}"/>.</p>
<p>Even more interestingly, the limit shape of the polygons in <img alt="{\mathcal{G}(X_n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BG%7D%28X_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{G}(X_n)}"/> is <img alt="{C_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_0}"/>, in the sense that, in expectation, the overwhelming majority of polygons in <img alt="{\mathcal{G}(X_n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BG%7D%28X_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{G}(X_n)}"/> is very close to <img alt="{C_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_0}"/>. More precisely, for every <img alt="{\epsilon&gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon&gt;0}"/><br/>
<img alt="\displaystyle \lim_{n \rightarrow \infty} \frac{\mathop{\mathbb E}|\{P\in \mathcal{G}(X_n):\delta(P,C_0)&gt;\epsilon\}|}{\mathop{\mathbb E}(|\mathcal{G}(X_n)|)}=0, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Clim_%7Bn+%5Crightarrow+%5Cinfty%7D+%5Cfrac%7B%5Cmathop%7B%5Cmathbb+E%7D%7C%5C%7BP%5Cin+%5Cmathcal%7BG%7D%28X_n%29%3A%5Cdelta%28P%2CC_0%29%3E%5Cepsilon%5C%7D%7C%7D%7B%5Cmathop%7B%5Cmathbb+E%7D%28%7C%5Cmathcal%7BG%7D%28X_n%29%7C%29%7D%3D0%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \lim_{n \rightarrow \infty} \frac{\mathop{\mathbb E}|\{P\in \mathcal{G}(X_n):\delta(P,C_0)&gt;\epsilon\}|}{\mathop{\mathbb E}(|\mathcal{G}(X_n)|)}=0, "/></p>
<p>where <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta}"/> stands for the Hausdorf distance.</p>
<p>The proof of the lattice case does not work here. The edge vectors determine the convex polygon, still, but the edge vectors can’t be used, there is no generating function, etc. Instead the proof is based on the following two theorems. For the first let <img alt="{\Delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CDelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Delta}"/> be a triangle with two specified vertices <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a}"/> and <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/> say, and let <img alt="{X_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X_k}"/> be a random independent sample of <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> uniform points from <img alt="{\Delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CDelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Delta}"/>. Then <img alt="{X_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X_k}"/> is called a convex chain (from <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a}"/> to <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/>) if the convex hull of <img alt="{\{a,b\}\bigcup X_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7Ba%2Cb%5C%7D%5Cbigcup+X_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{a,b\}\bigcup X_k}"/> is a convex polygon with exactly <img alt="{k+2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%2B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k+2}"/> vertices. Then<br/>
<img alt="\displaystyle \Pr[X_k \mbox{ is a convex chain}]=\frac {2^k}{k!(k+1)!}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CPr%5BX_k+%5Cmbox%7B+is+a+convex+chain%7D%5D%3D%5Cfrac+%7B2%5Ek%7D%7Bk%21%28k%2B1%29%21%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \Pr[X_k \mbox{ is a convex chain}]=\frac {2^k}{k!(k+1)!}, "/></p>
<p>a surprisingly precise result (due to Pavel Valtr).</p>
<p>For the second theorem let <img alt="{p(n,C)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%28n%2CC%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p(n,C)}"/> denote the probability that the random sample <img alt="{X_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X_n}"/> (independent and uniform again) lands in convex position, that is, their convex hull is a convex <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-gon. For <img alt="{n=4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%3D4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n=4}"/> this is Sylvester’s famous four point problem from 1864 (although he did not specify the underlying convex body <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>). Then<br/>
<img alt="\displaystyle \lim_{n\rightarrow \infty} n^2\sqrt[n]{p(n,C)}=\frac {e^2}4\frac{A^*(C)}{\sqrt[3]{\textrm{Area} \; C}}, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Clim_%7Bn%5Crightarrow+%5Cinfty%7D+n%5E2%5Csqrt%5Bn%5D%7Bp%28n%2CC%29%7D%3D%5Cfrac+%7Be%5E2%7D4%5Cfrac%7BA%5E%2A%28C%29%7D%7B%5Csqrt%5B3%5D%7B%5Ctextrm%7BArea%7D+%5C%3B+C%7D%7D%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \lim_{n\rightarrow \infty} n^2\sqrt[n]{p(n,C)}=\frac {e^2}4\frac{A^*(C)}{\sqrt[3]{\textrm{Area} \; C}}, "/></p>
<p>with the same <img alt="{A^*(C)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%5E%2A%28C%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A^*(C)}"/> as before. The proof of this theorem uses the previous result of Valtr about convex chains in triangles and the properties of <img alt="{C_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_0}"/>, the largest affine area convex subset of <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. The set <img alt="{C_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_0}"/> appears again: it is the limit shape of <img alt="{\textrm{conv}X_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctextrm%7Bconv%7DX_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\textrm{conv}X_n}"/> under the condition that <img alt="{X_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X_n}"/> landed in convex position.</p>
<p>The map <img alt="{C \rightarrow C_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC+%5Crightarrow+C_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C \rightarrow C_0}"/> is affinely equivariant and has interesting properties. <img alt="{C_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_0}"/> turns out to be the limit shape in some further cases as well. For instance, the maximal number of vertices of the polygons in <img alt="{\mathcal{F}^n(C)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BF%7D%5En%28C%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{F}^n(C)}"/> equals</p>
<p><img alt="\displaystyle \frac {3n^{2/3}}{(2\pi)^{2/3}}A^*(C)(1+o(1)), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cfrac+%7B3n%5E%7B2%2F3%7D%7D%7B%282%5Cpi%29%5E%7B2%2F3%7D%7DA%5E%2A%28C%29%281%2Bo%281%29%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \frac {3n^{2/3}}{(2\pi)^{2/3}}A^*(C)(1+o(1)), "/></p>
<p>as <img alt="{n \rightarrow \infty}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n \rightarrow \infty}"/>. This is of course the same as the maximal number of points in <img alt="{{\mathbb{Z}}_n\cap C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BZ%7D%7D_n%5Ccap+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\mathbb{Z}}_n\cap C}"/> that are in convex position. Although the convex <img alt="{{\mathbb{Z}}_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb%7BZ%7D%7D_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\mathbb{Z}}_n}"/>-lattice polygon <img alt="{\mathcal{F}^n(C)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BF%7D%5En%28C%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{F}^n(C)}"/> with maximal number of vertices is not necessary unique, they have a limit shape which is again <img alt="{C_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_0}"/>. The same happens in <img alt="{\mathcal{G}_n(C)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BG%7D_n%28C%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{G}_n(C)}"/> as well. In this case, however, the expectation of the maximal number of vertices is equal to constant times <img alt="{A^*(C)n^{1/3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%5E%2A%28C%29n%5E%7B1%2F3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A^*(C)n^{1/3}}"/> but the value of this (positive) constant is not known. The reason is the following. In the triangle <img alt="{\Delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CDelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Delta}"/> with specified vertices <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a}"/> and <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/>, and random sample <img alt="{X_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X_k}"/> we define <img alt="{L_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_k}"/> as the maximal number of points from <img alt="{X_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{X_k}"/> that form a convex chain in <img alt="{\Delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CDelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Delta}"/> from <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a}"/> to <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/>. The random variable <img alt="{L_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_k}"/> is concentrated around its expectation, which is equal to some non-negative constant times <img alt="{k^{1/3}(1+o(1))}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%5E%7B1%2F3%7D%281%2Bo%281%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k^{1/3}(1+o(1))}"/> as <img alt="{k \rightarrow \infty}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k \rightarrow \infty}"/> but this constant is not known. Experiments suggest that it is equal to 3 but there is no proof in sight. Not surprisingly, the limit shape of these maximal convex chains is again the special parabola arc in <img alt="{\Delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CDelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Delta}"/>. This question about the random variable <img alt="{L_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_k}"/> is similar to the longest increasing subsequence problem but much less is known about it.</p>
<h2>Open Problem: high dimensions</h2>
<p>What remains of the limit shape phenomenon in higher dimensions? Well, hardly anything has been proved. In the simplest case, let <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q}"/> denote the unit cube in 3-space, and let <img alt="{\mathcal{F}^n(Q)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BF%7D%5En%28Q%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{F}^n(Q)}"/> denote the set of all convex <img alt="{\frac 1n {\mathbb{Z}}^3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+1n+%7B%5Cmathbb%7BZ%7D%7D%5E3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac 1n {\mathbb{Z}}^3}"/>-lattice polygons contained in <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q}"/>. It is known that <img alt="{\log |\mathcal{F}^n(Q)|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+%7C%5Cmathcal%7BF%7D%5En%28Q%29%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log |\mathcal{F}^n(Q)|}"/> is between <img alt="{c_1n^{1/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_1n%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_1n^{1/2}}"/> and <img alt="{c_2 n^{1/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_2+n%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_2 n^{1/2}}"/> with <img alt="{0&lt;c_1&lt;c_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%3Cc_1%3Cc_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0&lt;c_1&lt;c_2}"/>, but nothing more precise. Probably there is a limit shape here as well, and it might be the convex subset of <img alt="{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BQ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Q}"/> that has the largest affine surface area. The existence of such a set follows the same way as above but its uniqueness is not known.</p>
<h2>The affine perimeter</h2>
<p><a href="https://gilkalai.files.wordpress.com/2019/07/affper.png"><img alt="" class="alignnone size-full wp-image-17542" src="https://gilkalai.files.wordpress.com/2019/07/affper.png?w=640"/></a></p>
<p> </p>
<p>Finally a few words about the affine perimeter. Given a convex curve <img alt="{\Gamma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Gamma}"/> in the plane, choose points <img alt="{x_0,\ldots, x_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_0%2C%5Cldots%2C+x_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_0,\ldots, x_n}"/> on it, take the tangent lines at these points and form the triangles <img alt="{T_1,\ldots,T_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_1%2C%5Cldots%2CT_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_1,\ldots,T_n}"/> as in the figure. By definition, the affine perimeter <img alt="{AP(\Gamma)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAP%28%5CGamma%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{AP(\Gamma)}"/> is the infimum of the sum <img alt="{2\sum_1^n(\textrm{Area} \; T_i)^{1/3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Csum_1%5En%28%5Ctextrm%7BArea%7D+%5C%3B+T_i%29%5E%7B1%2F3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2\sum_1^n(\textrm{Area} \; T_i)^{1/3}}"/> as the subdivision <img alt="{x_0,\ldots,x_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_0%2C%5Cldots%2Cx_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_0,\ldots,x_n}"/> gets finer and finer. The affine perimeter of the unit circle is <img alt="{2\pi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2\pi}"/> which explains the constant <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/> in front of <img alt="{\sum_1^n(\textrm{Area} \; T_i)^{1/3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_1%5En%28%5Ctextrm%7BArea%7D+%5C%3B+T_i%29%5E%7B1%2F3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_1^n(\textrm{Area} \; T_i)^{1/3}}"/>. The exponent <img alt="{1/3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2F3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/3}"/> is the right choice here: for larger exponent the sum is zero, and for smaller it is infinity (for the circle for instance). Inequality (2) shows that infimum in the definition can be replaced by limit. The affine perimeter of a convex polygon is zero.</p>
<p>For a twice differentiable curve <img alt="{AP(\Gamma) = \int_{\Gamma}\kappa^{1/3}ds=\int_{\Gamma}r^{-1/3}ds}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAP%28%5CGamma%29+%3D+%5Cint_%7B%5CGamma%7D%5Ckappa%5E%7B1%2F3%7Dds%3D%5Cint_%7B%5CGamma%7Dr%5E%7B-1%2F3%7Dds%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{AP(\Gamma) = \int_{\Gamma}\kappa^{1/3}ds=\int_{\Gamma}r^{-1/3}ds}"/> where <img alt="{\kappa}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ckappa%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\kappa}"/> is the curvature and <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> the radius of curvature and <img alt="{ds}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bds%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{ds}"/> means integration with arc length. The affine perimeter is an affine invariant or rather equivariant meaning that <img alt="{AP(S(\Gamma))=\sqrt[3]{|\det S|} AP(\Gamma)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAP%28S%28%5CGamma%29%29%3D%5Csqrt%5B3%5D%7B%7C%5Cdet+S%7C%7D+AP%28%5CGamma%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{AP(S(\Gamma))=\sqrt[3]{|\det S|} AP(\Gamma)}"/> for a non-degenerate affine transformation <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>. Quite often the affine perimeter (and the affine surface area) appears in connection with affine equivariant properties of the convex set <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. One example is best approximation by inscribed polygons <img alt="{P_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P_n}"/> on <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> vertices. When approximation is measured by <img alt="{\textrm{Area}\;(C\backslash P_n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctextrm%7BArea%7D%5C%3B%28C%5Cbackslash+P_n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\textrm{Area}\;(C\backslash P_n)}"/> then the best approximating polygon <img alt="{P_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P_n}"/> satisfies the estimate<br/>
<img alt="\displaystyle \textrm{Area} \; (C\backslash P_n)= \frac 1{4\sqrt 3} \frac {AP(C)^3}{n^2}(1+o(1)). \ \ \ \ \ (3)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Ctextrm%7BArea%7D+%5C%3B+%28C%5Cbackslash+P_n%29%3D+%5Cfrac+1%7B4%5Csqrt+3%7D+%5Cfrac+%7BAP%28C%29%5E3%7D%7Bn%5E2%7D%281%2Bo%281%29%29.+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \textrm{Area} \; (C\backslash P_n)= \frac 1{4\sqrt 3} \frac {AP(C)^3}{n^2}(1+o(1)). \ \ \ \ \ (3)"/></p>
<p>The set <img alt="{C_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_0}"/>, the convex subset of <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> with maximal affine perimeter has interesting properties. For instance its boundary contains no line segment, and if some piece of its boundary lies in the interior of <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>, then this piece is a parabola arc. It has positive curvature everywhere. It is of course affinely equivariant meaning that <img alt="{S(C_0)= S(C)_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%28C_0%29%3D+S%28C%29_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S(C_0)= S(C)_0}"/>. According to (3) <img alt="{C_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_0}"/> has the worst approximation properties among all convex subsets of <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{C}"/>.  This might explain why it comes up as the limit shape so often. Actually, the high dimensional analogue of (3) suggests that the limit shape in higher dimensions is again connected to the maximal affine surface area subset of the underlying convex body.</p>
<p> </p>
<p>More reading: Imre Bárány, <a href="https://www.ams.org/journals/bull/2008-45-03/S0273-0979-08-01210-X/">Random points and lattice points in convex bodies</a>, Bull AMS (2008)</p></div>
    </content>
    <updated>2019-07-09T08:35:29Z</updated>
    <published>2019-07-09T08:35:29Z</published>
    <category term="Combinatorics"/>
    <category term="Convexity"/>
    <category term="Geometry"/>
    <category term="Guest blogger"/>
    <category term="Probability"/>
    <category term="Imre Barany"/>
    <category term="limit shape"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-07-10T16:20:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.03620</id>
    <link href="http://arxiv.org/abs/1907.03620" rel="alternate" type="text/html"/>
    <title>Contraction Clustering (RASTER): A Very Fast Big Data Algorithm for Sequential and Parallel Density-Based Clustering in Linear Time, Constant Memory, and a Single Pass</title>
    <feedworld_mtime>1562630400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Ulm:Gregor.html">Gregor Ulm</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Smith:Simon.html">Simon Smith</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nilsson:Adrian.html">Adrian Nilsson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gustavsson:Emil.html">Emil Gustavsson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jirstrand:Mats.html">Mats Jirstrand</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.03620">PDF</a><br/><b>Abstract: </b>Clustering is an essential data mining tool for analyzing and grouping
similar objects. In big data applications, however, many clustering algorithms
are infeasible due to their high memory requirements and/or unfavorable runtime
complexity. In contrast, Contraction Clustering (RASTER) is a single-pass
algorithm for identifying density-based clusters with linear time complexity.
Due to its favorable runtime and the fact that its memory requirements are
constant, this algorithm is highly suitable for big data applications where the
amount of data to be processed is huge. It consists of two steps: (1) a
contraction step which projects objects onto tiles and (2) an agglomeration
step which groups tiles into clusters. This algorithm is extremely fast in both
sequential and parallel execution. In single-threaded execution on a
contemporary workstation, an implementation in Rust processes a batch of 500
million points with 1 million clusters in less than 50 seconds. The speedup due
to parallelization is significant, amounting to a factor of around 4 on an
8-core machine.
</p></div>
    </summary>
    <updated>2019-07-09T23:29:57Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.03535</id>
    <link href="http://arxiv.org/abs/1907.03535" rel="alternate" type="text/html"/>
    <title>More Hierarchy in Route Planning Using Edge Hierarchies</title>
    <feedworld_mtime>1562630400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hespe:Demian.html">Demian Hespe</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sanders:Peter.html">Peter Sanders</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.03535">PDF</a><br/><b>Abstract: </b>A highly successful approach to route planning in networks (particularly road
networks) is to identify a hierarchy in the network that allows faster queries
after some preprocessing that basically inserts additional "shortcut"-edges
into a graph. In the past there has been a succession of techniques that infer
a more and more fine grained hierarchy enabling increasingly more efficient
queries. This appeared to culminate in contraction hierarchies that assign one
hierarchy level to each vertex. In this paper we show how to identify an even
more fine grained hierarchy that assigns one level to each edge of the network.
Our findings indicate that this can lead to considerably smaller search spaces
in terms of visited edges. Currently, this does not result in improved query
times so that it remains an open question whether these edge hierarchies can
lead to overall improved performance. However, we believe that the technique as
such is a noteworthy enrichment of the portfolio of available techniques that
might prove useful in the future.
</p></div>
    </summary>
    <updated>2019-07-09T23:32:30Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.03533</id>
    <link href="http://arxiv.org/abs/1907.03533" rel="alternate" type="text/html"/>
    <title>A Formal Axiomatization of Computation</title>
    <feedworld_mtime>1562630400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ramezanian:Rasoul.html">Rasoul Ramezanian</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.03533">PDF</a><br/><b>Abstract: </b>We introduce a set of axioms for the notion of computation, and show that P=
NP is not derivable from this set of axioms.
</p></div>
    </summary>
    <updated>2019-07-09T23:20:35Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-07-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.03526</id>
    <link href="http://arxiv.org/abs/1907.03526" rel="alternate" type="text/html"/>
    <title>Inapproximability Results for Scheduling with Interval and Resource Restrictions</title>
    <feedworld_mtime>1562630400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maack:Marten.html">Marten Maack</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jansen:Klaus.html">Klaus Jansen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.03526">PDF</a><br/><b>Abstract: </b>In the restricted assignment problem, the input consists of a set of machines
and a set of jobs each with a processing time and a subset of eligible
machines. The goal is to find an assignment of the jobs to the machines
minimizing the makespan, that is, the maximum summed up processing time any
machine receives. Herein, jobs should only be assigned to those machines on
which they are eligible. It is well-known that there is no polynomial time
approximation algorithm with an approximation guarantee of less than 1.5 for
the restricted assignment problem unless P=NP. In this work, we show hardness
results for variants of the restricted assignment problem with particular types
of restrictions.
</p>
<p>In the case of interval restrictions the machines can be totally ordered such
that jobs are eligible on consecutive machines. We resolve the open question of
whether the problem admits a polynomial time approximation scheme (PTAS) in the
negative (unless P=NP). There are several special cases of this problem known
to admit a PTAS.
</p>
<p>Furthermore, we consider a variant with resource restriction where each
machine has capacities and each job demands for a fixed number of resources. A
job is eligible on a machine if its demand is at most the capacity of the
machine for each resource. For one resource, this problem is known to admit a
PTAS, for two, the case of interval restrictions is contained, and in general,
the problem is closely related to unrelated scheduling with a low rank
processing time matrix. We show that there is no polynomial time approximation
algorithm with a rate smaller than 48/47 or 1.5 for scheduling with resource
restrictions with 2 or 4 resources, respectively, unless P=NP. All our results
can be extended to the so called Santa Claus variants of the problems where the
goal is to maximize the minimal processing time any machine receives.
</p></div>
    </summary>
    <updated>2019-07-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.03437</id>
    <link href="http://arxiv.org/abs/1907.03437" rel="alternate" type="text/html"/>
    <title>Fair Byzantine Agreements for Blockchains</title>
    <feedworld_mtime>1562630400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chao:Tzu=Wei.html">Tzu-Wei Chao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chung:Hao.html">Hao Chung</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuo:Po=Chun.html">Po-Chun Kuo</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.03437">PDF</a><br/><b>Abstract: </b>Byzantine general problem is the core problem of the consensus algorithm, and
many protocols are proposed recently to improve the decentralization level, the
performance and the security of the blockchain. There are two challenging
issues when the blockchain is operating in practice. First, the outcomes of the
consensus algorithm are usually related to the incentive model, so whether each
participant's value has an equal probability of being chosen becomes essential.
However, the issues of fairness are not captured in the traditional security
definition of Byzantine agreement. Second, the blockchain should be resistant
to network failures, such as cloud services shut down or malicious attack,
while remains the high performance most of the time.
</p>
<p>This paper has two main contributions. First, we propose a novel notion
called fair validity for Byzantine agreement. Intuitively, fair validity
lower-bounds the expected numbers that honest nodes' values being decided if
the protocol is executed many times. However, we also show that any Byzantine
agreement could not achieve fair validity in an asynchronous network, so we
focus on synchronous protocols. This leads to our second contribution: we
propose a fair, responsive and partition-resilient Byzantine agreement protocol
tolerating up to 1/3 corruptions. Fairness means that our protocol achieves
fair validity. Responsiveness means that the termination time only depends on
the actual network delay instead of depending on any pre-determined time bound.
Partition-resilience means that the safety still holds even if the network is
partitioned, and the termination will hold if the partition is resolved.
</p></div>
    </summary>
    <updated>2019-07-09T23:32:57Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.03235</id>
    <link href="http://arxiv.org/abs/1907.03235" rel="alternate" type="text/html"/>
    <title>Bidirectional Text Compression in External Memory</title>
    <feedworld_mtime>1562630400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dinklage:Patrick.html">Patrick Dinklage</a>, Jonas Ellert, Jonas Ellert, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fischer:Johannes.html">Johannes Fischer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/K=ouml=ppl:Dominik.html">Dominik Köppl</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Penschuck:Manuel.html">Manuel Penschuck</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.03235">PDF</a><br/><b>Abstract: </b>Bidirectional compression algorithms work by substituting repeated substrings
by references that, unlike in the famous LZ77-scheme, can point to either
direction. We present such an algorithm that is particularly suited for an
external memory implementation. We evaluate it experimentally on large data
sets of size up to 128 GiB (using only 16 GiB of RAM) and show that it is
significantly faster than all known LZ77 compressors, while producing a roughly
similar number of factors. We also introduce an external memory decompressor
for texts compressed with any uni- or bidirectional compression scheme.
</p></div>
    </summary>
    <updated>2019-07-09T23:29:33Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.03205</id>
    <link href="http://arxiv.org/abs/1907.03205" rel="alternate" type="text/html"/>
    <title>Oracle Separations Between Quantum and Non-interactive Zero-Knowledge Classes</title>
    <feedworld_mtime>1562630400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Benjamin Morrison, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Groce:Adam.html">Adam Groce</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.03205">PDF</a><br/><b>Abstract: </b>We study the relationship between problems solvable by quantum algorithms in
polynomial time and those for which zero-knowledge proofs exist. In prior work,
Aaronson [arxiv:<a href="http://export.arxiv.org/abs/quant-ph/0111102">quant-ph/0111102</a>] showed an oracle separation between BQP and
SZK, i.e. an oracle $A$ such that $\mathrm{SZK}^A \not\subseteq
\mathrm{BQP}^A$. In this paper we give a simple extension of Aaronson's result
to non-interactive zero-knowledge proofs with perfect security. This class,
NIPZK, is the most restrictive zero-knowledge class. We show that even for this
class we can construct an $A$ with $\mathrm{NIPZK}^A \not\subseteq
\mathrm{BQP}^A$.
</p></div>
    </summary>
    <updated>2019-07-09T23:23:29Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-07-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.03201</id>
    <link href="http://arxiv.org/abs/1907.03201" rel="alternate" type="text/html"/>
    <title>A Randomized Algorithm for Edge-Colouring Graphs in $O(m\sqrt{n})$ Time</title>
    <feedworld_mtime>1562630400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sinnamon:Corwin.html">Corwin Sinnamon</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.03201">PDF</a><br/><b>Abstract: </b>We present a simple randomized algorithm to edge-colour arbitrary simple
graphs based on the classic decomposition strategy of Gabow et al. The
algorithm uses $d+1$ colours and runs in $O(m \sqrt n)$ time with high
probability.
</p></div>
    </summary>
    <updated>2019-07-09T23:32:53Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.03197</id>
    <link href="http://arxiv.org/abs/1907.03197" rel="alternate" type="text/html"/>
    <title>Composable Core-sets for Determinant Maximization: A Simple Near-Optimal Algorithm</title>
    <feedworld_mtime>1562630400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Indyk:Piotr.html">Piotr Indyk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mahabadi:Sepideh.html">Sepideh Mahabadi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gharan:Shayan_Oveis.html">Shayan Oveis Gharan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rezaei:Alireza.html">Alireza Rezaei</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.03197">PDF</a><br/><b>Abstract: </b>``Composable core-sets'' are an efficient framework for solving optimization
problems in massive data models. In this work, we consider efficient
construction of composable core-sets for the determinant maximization problem.
This can also be cast as the MAP inference task for determinantal point
processes, that have recently gained a lot of interest for modeling diversity
and fairness. The problem was recently studied in [IMOR'18], where they
designed composable core-sets with the optimal approximation bound of $\tilde
O(k)^k$. On the other hand, the more practical Greedy algorithm has been
previously used in similar contexts. In this work, first we provide a
theoretical approximation guarantee of $O(C^{k^2})$ for the Greedy algorithm in
the context of composable core-sets; Further, we propose to use a Local Search
based algorithm that while being still practical, achieves a nearly optimal
approximation bound of $O(k)^{2k}$; Finally, we implement all three algorithms
and show the effectiveness of our proposed algorithm on standard data sets.
</p></div>
    </summary>
    <updated>2019-07-09T23:23:40Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.03190</id>
    <link href="http://arxiv.org/abs/1907.03190" rel="alternate" type="text/html"/>
    <title>Testing Mixtures of Discrete Distributions</title>
    <feedworld_mtime>1562630400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aliakbarpour:Maryam.html">Maryam Aliakbarpour</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Ravi.html">Ravi Kumar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rubinfeld:Ronitt.html">Ronitt Rubinfeld</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.03190">PDF</a><br/><b>Abstract: </b>There has been significant study on the sample complexity of testing
properties of distributions over large domains. For many properties, it is
known that the sample complexity can be substantially smaller than the domain
size. For example, over a domain of size $n$, distinguishing the uniform
distribution from distributions that are far from uniform in $\ell_1$-distance
uses only $O(\sqrt{n})$ samples.
</p>
<p>However, the picture is very different in the presence of arbitrary noise,
even when the amount of noise is quite small. In this case, one must
distinguish if samples are coming from a distribution that is $\epsilon$-close
to uniform from the case where the distribution is $(1-\epsilon)$-far from
uniform. The latter task requires nearly linear in $n$ samples [Valiant 2008,
Valian and Valiant 2011].
</p>
<p>In this work, we present a noise model that on one hand is more tractable for
the testing problem, and on the other hand represents a rich class of noise
families. In our model, the noisy distribution is a mixture of the original
distribution and noise, where the latter is known to the tester either
explicitly or via sample access; the form of the noise is also known a priori.
Focusing on the identity and closeness testing problems leads to the following
mixture testing question: Given samples of distributions $p, q_1,q_2$, can we
test if $p$ is a mixture of $q_1$ and $q_2$? We consider this general question
in various scenarios that differ in terms of how the tester can access the
distributions, and show that indeed this problem is more tractable. Our results
show that the sample complexity of our testers are exactly the same as for the
classical non-mixture case.
</p></div>
    </summary>
    <updated>2019-07-09T23:25:41Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.03182</id>
    <link href="http://arxiv.org/abs/1907.03182" rel="alternate" type="text/html"/>
    <title>Towards Testing Monotonicity of Distributions Over General Posets</title>
    <feedworld_mtime>1562630400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aliakbarpour:Maryam.html">Maryam Aliakbarpour</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gouleakis:Themis.html">Themis Gouleakis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peebles:John.html">John Peebles</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rubinfeld:Ronitt.html">Ronitt Rubinfeld</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yodpinyanee:Anak.html">Anak Yodpinyanee</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.03182">PDF</a><br/><b>Abstract: </b>In this work, we consider the sample complexity required for testing the
monotonicity of distributions over partial orders. A distribution $p$ over a
poset is monotone if, for any pair of domain elements $x$ and $y$ such that $x
\preceq y$, $p(x) \leq p(y)$. To understand the sample complexity of this
problem, we introduce a new property called bigness over a finite domain, where
the distribution is $T$-big if the minimum probability for any domain element
is at least $T$. We establish a lower bound of $\Omega(n/\log n)$ for testing
bigness of distributions on domains of size $n$. We then build on these lower
bounds to give $\Omega(n/\log{n})$ lower bounds for testing monotonicity over a
matching poset of size $n$ and significantly improved lower bounds over the
hypercube poset. We give sublinear sample complexity bounds for testing bigness
and for testing monotonicity over the matching poset.
</p>
<p>We then give a number of tools for analyzing upper bounds on the sample
complexity of
</p>
<p>the monotonicity testing problem.
</p></div>
    </summary>
    <updated>2019-07-09T23:30:30Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.03129</id>
    <link href="http://arxiv.org/abs/1907.03129" rel="alternate" type="text/html"/>
    <title>Constant-Factor Approximation Algorithms for Parity-Constrained Facility Location Problems</title>
    <feedworld_mtime>1562630400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kim:Kangsan.html">Kangsan Kim</a>, Yongho Shin, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/An:Hyung=Chan.html">Hyung-Chan An</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.03129">PDF</a><br/><b>Abstract: </b>Facility location is a prominent optimization problem that has inspired a
large quantity of both theoretical and practical studies in combinatorial
optimization. Although the problem has been investigated under various settings
reflecting typical structures within the optimization problems of practical
interest, little is known on how the problem behaves in conjunction with parity
constraints. This shortfall of understanding was rather disturbing when we
consider the central role of parity in the field of combinatorics. In this
paper, we present the first constant-factor approximation algorithm for the
facility location problem with parity constraints. We are given as the input a
metric on a set of facilities and clients, the opening cost of each facility,
and the parity requirement--odd, even, or unconstrained--of every facility in
this problem. The objective is to open a subset of facilities and assign every
client to an open facility so as to minimize the sum of the total opening costs
and the assignment distances, but subject to the condition that the number of
clients assigned to each open facility must have the same parity as its
requirement.
</p>
<p>Although the unconstrained facility location problem as a relaxation for this
parity-constrained generalization has unbounded gap, we demonstrate that it
yields a structured solution whose parity violation can be corrected at small
cost. This correction is prescribed by a T-join on an auxiliary graph
constructed by the algorithm. This graph does not satisfy the triangle
inequality, but we show that a carefully chosen set of shortcutting operations
leads to a cheap and sparse T-join. Finally, we bound the correction cost by
exhibiting a combinatorial multi-step construction of an upper bound. We also
present the first constant-factor approximation algorithm for the
parity-constrained k-center problem, the bottleneck optimization variant.
</p></div>
    </summary>
    <updated>2019-07-09T23:32:41Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.03037</id>
    <link href="http://arxiv.org/abs/1907.03037" rel="alternate" type="text/html"/>
    <title>Near-Optimal Fully Dynamic Densest Subgraph</title>
    <feedworld_mtime>1562630400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sawlani:Saurabh.html">Saurabh Sawlani</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Junxing.html">Junxing Wang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.03037">PDF</a><br/><b>Abstract: </b>We give the first fully dynamic algorithm which maintains a
$(1+\epsilon)$-approximate densest subgraph in worst-case time
$\text{poly}(\log n, \epsilon^{-1})$ per update with high probability. Dense
subgraph discovery is an important primitive for many real-world applications
such as community detection, link spam detection, distance query indexing, and
computational biology. Our result improves upon the previous best approximation
factor of $(4+\epsilon)$ for fully dynamic densest subgraph obtained by
[Bhattacharya-Henzinger-Nanongkai-Tsourakakis, STOC`15]. Our algorithm combines
the uniform sparsification technique used in
[Mitzenmacher-Pachocki-Peng-Tsourakakis-Xu, KDD`15] and
[McGregor-Tench-Vorotnikova-Vu, MFCS`15] along with an augmenting path-like
dual adjustment technique to maintain an approximate solution efficiently.
</p></div>
    </summary>
    <updated>2019-07-09T23:30:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8041836663315088806</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8041836663315088806/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/07/fortran-is-underated.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8041836663315088806" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8041836663315088806" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/07/fortran-is-underated.html" rel="alternate" type="text/html"/>
    <title>Fortran is underated!</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">(Joint Post with David Marcus who was a classmate of mine at SUNY Stony Brook [now called Stony Brook University]. I was class of 1980, he was class of 1979. We were both math majors.)<br/>
<br/>
David has been reading <a href="https://www.amazon.com/Problems-Point-Exploring-Computer-Science/dp/9813279974">Problems with a POINT</a> (I'm glad someone is reading it) and emailed me a comment on the following passage which was essentially <a href="https://blog.computationalcomplexity.org/2012/02/dusting-off-my-bookshelf-i-find-book-on.html">this post</a>. I paraphrase what I wrote:<br/>
<br/>
PASSAGE IN BOOK:<br/>
I dusted off my book shelves and found a book on Fortran. On the back it said:<br/>
<br/>
FORTRAN is one of the oldest high-level languages and remains the premier language for writing code for science and engineering applications. (NOTE- The back of the book uses Fortran but the spell checker I am using insists on FORTRAN. As a fan of capital letters, I don't mind going along.)<br/>
<br/>
When was the book written?<br/>
<br/>
The answer was surprising in that it was 2012 (the Chapter title was <i>Trick Question or Stupid Question</i>. This was a Trick Question.) I would have thought that FORTRAN was no longer the premier language by then. I also need to dust my bookshelves more often.<br/>
END OF PASSAGE IN BOOK<br/>
<br/>
David Marcus emailed me the following:<br/>
<br/>
DAVID'S EMAIL<br/>
Page 201. Fortran. One clue is that it said "Fortran" rather than"FORTRAN". Fortran 90 changed the name from all upper case. Whether it is the "premier language" depends on what you mean by "premier". It is probably the best language for scientific computing. I used it pretty much exclusively (by choice) in my previous job that I left in 2006. The handling of arrays is better than any other language I've used. Maybe there are some better languages that I'm not familiar with, but the huge number of high-quality scientific libraries available for Fortran makes it hard to beat. On the other hand, I never wrote a GUI app with it (Delphi is best for that).<br/>
END OF DAVID'S EMAIL<br/>
<br/>
In later emails we agreed that Fortran is not used that much (there are lists of most-used languages and neither Fortran nor FORTRAN is ever in the top 10).  But what intrigued me was the following contrast:<br/>
<br/>
1) David says that its the BEST language for Scientific Computing.  I will assume he is right.<br/>
<br/>
2) I doubt much NEW code is being written in it.  I will assume I am right.<br/>
<br/>
So---what's up with that? Some options<br/>
<br/>
OPTION 1) People SHOULD use Fortran but DON'T. If so, why is that?  Fortran is not taught in schools. People are used to what they already know.  Perhaps people who do pick up new things easily and want to use new things would rather use NEW things rather than NEW-TO-THEM-BUT-NOT-TO-THEIR-GRANDMOTHER things. Could be a coolness factor.  Do the advantages of Fortran outweight the disadvantages?  Is what they are using good enough?<br/>
<br/>
OPTION 2) The amount of Scientific computing software being written is small since we already have these great Fortran packages. So it may be a victim of its own success.<br/>
<br/>
CAVEAT: When I emailed David a first draft of the post he pointed out the following which has to do with the lists of most-used programming languages:<br/>
<br/>
DAVIDS EMAIL:<br/>
The problem with the lists you were looking at is that most people in the world are not scientists, so most software being written is not for scientists. Scientists and technical people are writing lots of new code.  If you look at a list of scientific languages, you will see Fortran, e.g., <a href="https://en.wikipedia.org/wiki/Scientific_programming_language">here</a> and <a href="https://en.wikipedia.org/wiki/Fortran#Science_and_engineering">here</a>.<br/>
<br/>
<br/>
There are several Fortran compilers available. One of the best was bought by Intel some time back and they still sell it. I doubt they would do that if no one was using it. Actually, I think Intel had a compiler, but bought the Compaq compiler (which used to be the Digital Equipment compiler) and merged the Compaq team with their team. Something like that. I was using the Compaq compiler around that time.<br/>
END OF DAVID's EMAIL<br/>
<br/>
One quote from the second pointer I find intriguing.  (Second use of the word <i>intriguing</i>. It was my word-of-the-day on my word-calendar).<br/>
<br/>
<i>... facilities for inter-operation with C were added to Fortran 2003 and enhanced by ISO/ICE technical specification 29113, which will be incorporated into Fortran 2018. </i><br/>
<br/>
I (Bill) don't know what some of that means; however, it does mean that Fortran is still active.<br/>
<br/>
<br/>
One fear: with its not being taught that much, will knowledge of it die out.  We be like Star Trek aliens:<br/>
<br/>
<i>The old ones built these machines, but then died and we can't fix them!<br/>
<br/>
<br/>
<br/>
</i></div>
    </content>
    <updated>2019-07-08T03:55:00Z</updated>
    <published>2019-07-08T03:55:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-07-10T15:46:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/091</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/091" rel="alternate" type="text/html"/>
    <title>TR19-091 |  A Sublinear-space and Polynomial-time Separator Algorithm for Planar Graphs | 

	Ryo Ashida, 

	Tatsuya Imai, 

	Kotaro Nakagawa, 

	A.  Pavan, 

	Vinodchandran Variyam, 

	Osamu Watanabe</title>
    <summary>In [12] (CCC 2013), the authors presented an algorithm for the reachability problem over directed planar graphs that runs in polynomial-time and uses $O(n^{1/2+\epsilon})$ space. A critical ingredient  of their algorithm is a polynomial-time, $\tldO(\sqrt{n})$-space algorithm to compute a separator of a planar graph. The conference version provided a sketch of the algorithm and many nontrivial details were left unexplained. In this work, we provide a detailed construction of their algorithm.</summary>
    <updated>2019-07-07T23:37:15Z</updated>
    <published>2019-07-07T23:37:15Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-07-10T16:20:27Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4233</id>
    <link href="https://www.scottaaronson.com/blog/?p=4233" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4233#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4233" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">John Wright joins UT Austin</title>
    <summary xml:lang="en-US">I’m delighted to announce that quantum computing theorist John Wright will be joining the computer science faculty at UT Austin in Fall 2020, after he finishes a one-year postdoc at Caltech. John made an appearance on this blog a few months ago, when I wrote about the new breakthrough by him and Anand Natarajan: namely, […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><div class="wp-block-image"><figure class="aligncenter"><img alt="" class="wp-image-4244" src="https://www.scottaaronson.com/blog/wp-content/uploads/2019/07/image-1.png"/></figure></div>



<p>I’m delighted to announce that quantum computing theorist <a href="http://www.mit.edu/~jswright/">John Wright</a> will be joining the computer science faculty at UT Austin in Fall 2020, after he finishes a one-year postdoc at Caltech. </p>



<p>John made an appearance on this blog a few months ago, when I <a href="https://www.scottaaronson.com/blog/?p=4172">wrote about</a> the <a href="https://arxiv.org/abs/1904.05870">new breakthrough</a> by him and <a href="http://www.its.caltech.edu/~anataraj/">Anand Natarajan</a>: namely, that MIP* (multi-prover interactive proofs with entangled provers) contains NEEXP (nondeterministic double-exponential time).  Previously, MIP* had only been known to contain NEXP (nondeterministic <em>single</em> exponential time).  So, this is an exponential expansion in the power of entangled provers over what was previously known and believed, and the first proof that entanglement actually <em>increases</em> the power of multi-prover protocols, rather than decreasing it (as it could’ve done a priori).  Even more strikingly, there seems to be no natural stopping point: MIP* might soon swallow up arbitrary towers of exponentials or even the halting problem (!).  For more, see for example <a href="https://www.quantamagazine.org/computer-scientists-expand-the-frontier-of-verifiable-knowledge-20190523/">this <em>Quanta</em> article</a>, or <a href="https://mycqstate.wordpress.com/2019/04/14/randomness-and-interaction-entanglement-ups-the-game/">this post by Thomas Vidick</a>, or <a href="http://www.henryyuen.net/post/alice-and-bob-visit/">this short story [sic] by Henry Yuen</a>.</p>



<p>John grew up in Texas, so he’s no stranger to BBQ brisket or scorching weather.  He did his undergrad in computer science at UT Austin—my colleagues remember him as a star—and then completed his PhD with Ryan O’Donnell at Carnegie Mellon, followed by a postdoc at MIT.  Besides the work on MIP*, John is also well-known for his <a href="https://arxiv.org/abs/1508.01907">2015 work with O’Donnell</a> pinning down the sample complexity of quantum state tomography.  Their important result, a version of which was independently obtained by <a href="https://arxiv.org/abs/1508.01797">Haah et al.</a>, says that if you want to learn an unknown d-dimensional quantum mixed state ρ to a reasonable precision, then ~d<sup>2</sup> copies of ρ are both necessary and sufficient.  This solved a problem that had personally interested me, and already plays a role in, e.g., my work on <a href="https://arxiv.org/abs/1711.01053">shadow tomography</a> and <a href="https://www.scottaaronson.com/papers/dpgentle.pdf">gentle measurements</a>.</p>



<p>Our little <a href="https://www.cs.utexas.edu/~qic/">quantum information center</a> at UT Austin is growing rapidly.  <a href="http://sites.utexas.edu/shyamshankar/">Shyam Shankar</a>, a superconducting qubits guy who previously worked in Michel Devoret’s group at Yale, will also be joining UT’s Electrical and Computer Engineering department this fall.  I’ll have two new postdocs—<a href="https://twitter.com/a_rocchetto?lang=en">Andrea Rocchetto</a> and <a href="http://www.cs.huji.ac.il/~yosiat/">Yosi Atia</a>—as well as new PhD students.  We’ll continue recruiting this coming year, with potential opportunities for students, postdocs, faculty, and research scientists across the CS, physics, and ECE departments as well as the Texas Advanced Computing Center (TACC).  I hope you’ll consider applying to join us.</p>



<p>With no evaluative judgment attached, I can honestly say that this is an unprecedented time for quantum computing as a field.  Where once faculty applicants struggled to make a case for quantum computing (physics departments: “but isn’t this really CS?” / CS departments: “isn’t it really physics?” / everyone: “couldn’t this whole QC thing, like, all blow over in a year?”), today departments are vying with each other and with industry players and startups to recruit talented people.  In such an environment, we’re fortunate to be doing as well as we are.  We hope to continue to expand.</p>



<p>Meanwhile, this was an <a href="https://www.nytimes.com/2019/01/24/technology/computer-science-courses-college.html">unprecedented year for CS hiring at UT Austin</a> more generally.  John Wright is one of at least four new faculty (probably more) who will be joining us.  It’s a good time to be in CS.</p>



<p>A huge welcome to John, and hook ’em Hadamards!</p>



<p>(And for US readers: have a great 4<sup>th</sup>!  Though how could any fireworks match the proof of the Sensitivity Conjecture?)</p></div>
    </content>
    <updated>2019-07-04T01:08:54Z</updated>
    <published>2019-07-04T01:08:54Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2019-07-09T14:27:23Z</updated>
    </source>
  </entry>
</feed>
