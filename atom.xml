<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-07-26T07:22:09Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.11209</id>
    <link href="http://arxiv.org/abs/1907.11209" rel="alternate" type="text/html"/>
    <title>Integrality Gap of the Vertex Cover Linear Programming Relaxation</title>
    <feedworld_mtime>1564099200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singh:Mohit.html">Mohit Singh</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.11209">PDF</a><br/><b>Abstract: </b>We give a characterization result for the integrality gap of the natural
linear programming relaxation for the vertex cover problem. We show that
integrality gap of the standard linear programming relaxation for any graph G
equals $\left(2-\frac{2}{\chi^f(G)}\right)$ where $\chi^f(G)$ denotes the
fractional chromatic number of G.
</p></div>
    </summary>
    <updated>2019-07-26T01:40:16Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.11206</id>
    <link href="http://arxiv.org/abs/1907.11206" rel="alternate" type="text/html"/>
    <title>The Strong 3SUM-INDEXING Conjecture is False</title>
    <feedworld_mtime>1564099200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kopelowitz:Tsvi.html">Tsvi Kopelowitz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Porat:Ely.html">Ely Porat</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.11206">PDF</a><br/><b>Abstract: </b>In the 3SUM-Indexing problem the goal is to preprocess two lists of elements
from $U$, $A=(a_1,a_2,\ldots,a_n)$ and $B=(b_1,b_2,...,b_n)$, such that given
an element $c\in U$ one can quickly determine whether there exists a pair
$(a,b)\in A \times B$ where $a+b=c$. Goldstein et al.~[WADS'2017] conjectured
that there is no algorithm for 3SUM-Indexing which uses $n^{2-\Omega(1)}$ space
and $n^{1-\Omega(1)}$ query time.
</p>
<p>We show that the conjecture is false by reducing the 3SUM-Indexing problem to
the problem of inverting functions, and then applying an algorithm of Fiat and
Naor [SICOMP'1999] for inverting functions.
</p></div>
    </summary>
    <updated>2019-07-26T01:44:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.11078</id>
    <link href="http://arxiv.org/abs/1907.11078" rel="alternate" type="text/html"/>
    <title>Approximating APSP without Scaling: Equivalence of Approximate Min-Plus and Exact Min-Max</title>
    <feedworld_mtime>1564099200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bringmann:Karl.html">Karl Bringmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/K=uuml=nnemann:Marvin.html">Marvin Künnemann</a>, Karol Węgrzycki <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.11078">PDF</a><br/><b>Abstract: </b>Zwick's $(1+\varepsilon)$-approximation algorithm for the All Pairs Shortest
Path (APSP) problem runs in time $\widetilde{O}(\frac{n^\omega}{\varepsilon}
\log{W})$, where $\omega \le 2.373$ is the exponent of matrix multiplication
and $W$ denotes the largest weight. This can be used to approximate several
graph characteristics including the diameter, radius, median, minimum-weight
triangle, and minimum-weight cycle in the same time bound.
</p>
<p>Since Zwick's algorithm uses the scaling technique, it has a factor $\log W$
in the running time. In this paper, we study whether APSP and related problems
admit approximation schemes avoiding the scaling technique. That is, the number
of arithmetic operations should be independent of $W$; this is called strongly
polynomial. Our main results are as follows.
</p>
<p>- We design approximation schemes in strongly polynomial time
$O(\frac{n^\omega}{\varepsilon} \text{polylog}(\frac{n}{\varepsilon}))$ for
APSP on undirected graphs as well as for the graph characteristics diameter,
radius, median, minimum-weight triangle, and minimum-weight cycle on directed
or undirected graphs.
</p>
<p>- For APSP on directed graphs we design an approximation scheme in strongly
polynomial time $O(n^{\frac{\omega + 3}{2}} \varepsilon^{-1}
\text{polylog}(\frac{n}{\varepsilon}))$. This is significantly faster than the
best exact algorithm.
</p>
<p>- We explain why our approximation scheme for APSP on directed graphs has a
worse exponent than $\omega$: Any improvement over our exponent $\frac{\omega +
3}{2}$ would improve the best known algorithm for Min-Max Product In fact, we
prove that approximating directed APSP and exactly computing the Min-Max
Product are equivalent.
</p></div>
    </summary>
    <updated>2019-07-26T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.11015</id>
    <link href="http://arxiv.org/abs/1907.11015" rel="alternate" type="text/html"/>
    <title>A new approach (extra vertex) and generalization of Shoelace Algorithm usage in convex polygon (Point-in-Polygon)</title>
    <feedworld_mtime>1564099200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Ochilbek Rakhmanov <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.11015">PDF</a><br/><b>Abstract: </b>In this paper we aim to bring new approach into usage of Shoelace Algorithm
for area calculation in convex polygons on Cartesian coordinate system, with
concentration on point in polygon concept. Generalization of usage of the
concept will be proposed for line segment and polygons. Testing of new method
will be done using Python language. Results of tests show that the new approach
is more effective than the current one.
</p></div>
    </summary>
    <updated>2019-07-26T01:50:42Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-07-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.11010</id>
    <link href="http://arxiv.org/abs/1907.11010" rel="alternate" type="text/html"/>
    <title>Deciding Fast Termination for Probabilistic VASS with Nondeterminism</title>
    <feedworld_mtime>1564099200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Tomáš Brázdil, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chatterjee:Krishnendu.html">Krishnendu Chatterjee</a>, Antonín Kučera, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Novotn=yacute=:Petr.html">Petr Novotný</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Velan:Dominik.html">Dominik Velan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.11010">PDF</a><br/><b>Abstract: </b>A probabilistic vector addition system with states (pVASS) is a finite state
Markov process augmented with non-negative integer counters that can be
incremented or decremented during each state transition, blocking any behaviour
that would cause a counter to decrease below zero. The pVASS can be used as
abstractions of probabilistic programs with many decidable properties. The use
of pVASS as abstractions requires the presence of nondeterminism in the model.
In this paper, we develop techniques for checking fast termination of pVASS
with nondeterminism.
</p>
<p>That is, for every initial configuration of size n, we consider the worst
expected number of transitions needed to reach a configuration with some
counter negative (the expected termination time). We show that the problem
whether the asymptotic expected termination time is linear is decidable in
polynomial time for a certain natural class of pVASS with nondeterminism.
Furthermore, we show the following dichotomy: if the asymptotic expected
termination time is not linear, then it is at least quadratic, i.e., in
$\Omega(n^2)$.
</p></div>
    </summary>
    <updated>2019-07-26T01:22:10Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-07-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10984</id>
    <link href="http://arxiv.org/abs/1907.10984" rel="alternate" type="text/html"/>
    <title>Enumerating Range Modes</title>
    <feedworld_mtime>1564099200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sumigawa:Kentaro.html">Kentaro Sumigawa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chakraborty:Sankardeep.html">Sankardeep Chakraborty</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sadakane:Kunihiko.html">Kunihiko Sadakane</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Satti:Srinivasa_Rao.html">Srinivasa Rao Satti</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10984">PDF</a><br/><b>Abstract: </b>We consider the range mode problem where given a sequence and a query range
in it, we want to find items with maximum frequency in the range. We give time-
and space- efficient algorithms for this problem. Our algorithms are efficient
for small maximum frequency cases. We also consider a natural generalization of
the problem: the range mode enumeration problem, for which there has been no
known efficient algorithms. Our algorithms have query time complexities which
is linear to the output size plus small terms.
</p></div>
    </summary>
    <updated>2019-07-26T01:47:36Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10937</id>
    <link href="http://arxiv.org/abs/1907.10937" rel="alternate" type="text/html"/>
    <title>Polylogarithmic-Time Deterministic Network Decomposition and Distributed Derandomization</title>
    <feedworld_mtime>1564099200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Václav Rozhoň, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghaffari:Mohsen.html">Mohsen Ghaffari</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10937">PDF</a><br/><b>Abstract: </b>We present a simple polylogarithmic-time deterministic distributed algorithm
for network decomposition. This improves on a celebrated $2^{O(\sqrt{\log
n})}$-time algorithm of Panconesi and Srinivasan [STOC'93] and settles one of
the long-standing and central questions in distributed graph algorithms. It
also leads to the first polylogarithmic-time deterministic distributed
algorithms for numerous other graph problems, hence resolving several open
problems, including Linial's well-known question about the deterministic
complexity of maximal independent set [FOCS'87].
</p>
<p>Put together with the results of Ghaffari, Kuhn, and Maus [STOC'17] and
Ghaffari, Harris, and Kuhn [FOCS'18], we get a general distributed
derandomization result that implies $\mathsf{P}$-$\mathsf{RLOCAL}$ =
$\mathsf{P}$-$\mathsf{LOCAL}$. That is, for any distributed problem whose
solution can be checked in polylogarithmic-time, any polylogarithmic-time
randomized algorithm can be derandomized to a polylogarithmic-time
deterministic algorithm.
</p>
<p>By known connections, our result leads also to substantially faster
randomized algorithms for a number of fundamental problems including
$(\Delta+1)$-coloring, MIS, and Lov\'{a}sz Local Lemma.
</p></div>
    </summary>
    <updated>2019-07-26T01:39:13Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10930</id>
    <link href="http://arxiv.org/abs/1907.10930" rel="alternate" type="text/html"/>
    <title>GAMA: A Novel Algorithm for Non-Convex Integer Programs</title>
    <feedworld_mtime>1564099200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alghassi:Hedayat.html">Hedayat Alghassi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dridi:Raouf.html">Raouf Dridi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tayur:Sridhar.html">Sridhar Tayur</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10930">PDF</a><br/><b>Abstract: </b>Inspired by the decomposition in the hybrid quantum-classical optimization
algorithm we introduced in <a href="http://export.arxiv.org/abs/1902.04215">arXiv:1902.04215</a>, we propose here a new (fully
classical) approach to solving certain non-convex integer programs using Graver
bases. This method is well suited when (a) the constraint matrix $A$ has a
special structure so that its Graver basis can be computed systematically, (b)
several feasible solutions can also be constructed easily and (c) the objective
function can be viewed as many convex functions quilted together. Classes of
problems that satisfy these conditions include Cardinality Boolean Quadratic
Problems (CBQP), Quadratic Semi-Assignment Problems (QSAP) and Quadratic
Assignment Problems (QAP). Our Graver Augmented Multi-seed Algorithm (GAMA)
utilizes augmentation along Graver basis elements (the improvement direction is
obtained by comparing objective function values) from these multiple initial
feasible solutions. We compare our approach with a best-in-class commercially
available solver (Gurobi). Sensitivity analysis indicates that the rate at
which GAMA slows down as the problem size increases is much lower than that of
Gurobi. We find that for several instances of practical relevance, GAMA not
only vastly outperforms in terms of time to find the optimal solution (by two
or three orders of magnitude), but also finds optimal solutions within minutes
when the commercial solver is not able to do so in 4 or 10 hours (depending on
the problem class) in several cases.
</p></div>
    </summary>
    <updated>2019-07-26T01:38:58Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10895</id>
    <link href="http://arxiv.org/abs/1907.10895" rel="alternate" type="text/html"/>
    <title>Fast Deterministic Constructions of Linear-Size Spanners and Skeletons</title>
    <feedworld_mtime>1564099200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Elkin:Michael.html">Michael Elkin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Matar:Shaked.html">Shaked Matar</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10895">PDF</a><br/><b>Abstract: </b>In the distributed setting, the only existing constructions of \textit{sparse
skeletons}, (i.e., subgraphs with $O(n)$ edges) either use randomization or
large messages, or require $\Omega(D)$ time, where $D$ is the hop-diameter of
the input graph $G$. We devise the first deterministic distributed algorithm in
the CONGEST model (i.e., uses small messages) for constructing linear-size
skeletons in time $2^{O(\sqrt{{\log n}\cdot{\log{\log n}}})}$. We can also
compute a linear-size spanner with stretch $polylog(n)$ in low deterministic
polynomial time, i.e., $O(n^\rho)$ for an arbitrarily small constant $\rho &gt;0$,
in the CONGEST model. Yet another algorithm that we devise runs in $O({\log
n})^{\kappa-1}$ time, for a parameter $\kappa=1,2,\dots,$ and constructs an
$O({\log n})^{\kappa-1}$ spanner with $O(n^{1+1/\kappa})$ edges. All our
distributed algorithms are lightweight from the computational perspective,
i.e., none of them employs any heavy computations.
</p></div>
    </summary>
    <updated>2019-07-26T01:35:15Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10874</id>
    <link href="http://arxiv.org/abs/1907.10874" rel="alternate" type="text/html"/>
    <title>How to Store a Random Walk</title>
    <feedworld_mtime>1564099200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Viola:Emanuele.html">Emanuele Viola</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Weinstein:Omri.html">Omri Weinstein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yu:Huacheng.html">Huacheng Yu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10874">PDF</a><br/><b>Abstract: </b>Motivated by storage applications, we study the following data structure
problem: An encoder wishes to store a collection of jointly-distributed files
$\overline{X}:=(X_1,X_2,\ldots, X_n) \sim \mu$ which are \emph{correlated}
($H_\mu(\overline{X}) \ll \sum_i H_\mu(X_i)$), using as little (expected)
memory as possible, such that each individual file $X_i$ can be recovered
quickly with few (ideally constant) memory accesses.
</p>
<p>In the case of independent random files, a dramatic result by \Pat (FOCS'08)
and subsequently by Dodis, \Pat and Thorup (STOC'10) shows that it is possible
to store $\overline{X}$ using just a \emph{constant} number of extra bits
beyond the information-theoretic minimum space, while at the same time decoding
each $X_i$ in constant time. However, in the (realistic) case where the files
are correlated, much weaker results are known, requiring at least
$\Omega(n/poly\lg n)$ extra bits for constant decoding time, even for "simple"
joint distributions $\mu$.
</p>
<p>We focus on the natural case of compressing\emph{Markov chains}, i.e.,
storing a length-$n$ random walk on any (possibly directed) graph $G$. Denoting
by $\kappa(G,n)$ the number of length-$n$ walks on $G$, we show that there is a
succinct data structure storing a random walk using $\lg_2 \kappa(G,n) + O(\lg
n)$ bits of space, such that any vertex along the walk can be decoded in $O(1)$
time on a word-RAM. For the harder task of matching the \emph{point-wise}
optimal space of the walk, i.e., the empirical entropy $\sum_{i=1}^{n-1} \lg
(deg(v_i))$, we present a data structure with $O(1)$ extra bits at the price of
$O(\lg n)$ decoding time, and show that any improvement on this would lead to
an improved solution on the long-standing Dictionary problem. All of our data
structures support the \emph{online} version of the problem with constant
update and query time.
</p></div>
    </summary>
    <updated>2019-07-26T01:36:43Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10779</id>
    <link href="http://arxiv.org/abs/1907.10779" rel="alternate" type="text/html"/>
    <title>Improved Girth Approximation and Roundtrip Spanners</title>
    <feedworld_mtime>1564099200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chechik:Shiri.html">Shiri Chechik</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Yang_P=.html">Yang P. Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rotem:Omer.html">Omer Rotem</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sidford:Aaron.html">Aaron Sidford</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10779">PDF</a><br/><b>Abstract: </b>In this paper we provide improved algorithms for approximating the girth and
producing roundtrip spanners of $n$-node $m$-edge directed graphs with
non-negative edge lengths. First, for any integer $k \ge 1$, we provide a
deterministic $\tilde{O}(m^{1+ 1/k})$ time algorithm which computes a
$O(k\log\log n)$ multiplicative approximation of the girth and a $O(k\log\log
n)$ multiplicative roundtrip spanner with $\tilde{O}(n^{1+1/k})$ edges. Second,
we provide a randomized $\tilde{O}(m\sqrt{n})$ time algorithm that with high
probability computes a 3-multiplicative approximation to the girth. Third, we
show how to combine these algorithms to obtain for any integer $k \ge 1$ a
randomized algorithm which in $\tilde{O}(m^{1+ 1/k})$ time computes a $O(k\log
k)$ multiplicative approximation of the girth and $O(k \log k)$ multiplicative
roundtrip spanner with high probability.
</p>
<p>The previous fastest algorithms for these problems either ran in All-Pairs
Shortest Paths (APSP) time, i.e. $\tilde{O}(mn)$, or were due Pachocki et al
(SODA 2018) which provided a randomized algorithm that for any integer $k \ge
1$ in time $\tilde{O}(m^{1+1/k})$ computed with high probability a $O(k\log n)$
multiplicative approximation of the girth and a $O(k\log n)$ multiplicative
roundtrip spanners with $\tilde{O}(n^{1+1/k})$ edges. Our first algorithm
removes the need for randomness and improves the approximation factor in
Pachocki et al (SODA 2018), our second is constitutes the first sub-APSP-time
algorithm for approximating the girth to constant accuracy with high
probability, and our third is the first time versus quality trade-offs for
obtaining constant approximations.
</p></div>
    </summary>
    <updated>2019-07-26T01:41:09Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10707</id>
    <link href="http://arxiv.org/abs/1907.10707" rel="alternate" type="text/html"/>
    <title>Real-time Deformation of Soft Tissue Internal Structure with Surface Profile Variations using Particle System</title>
    <feedworld_mtime>1564099200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Haoyin.html">Haoyin Zhou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gombos:Eva.html">Eva Gombos</a>, Mehra Golshan, Jayender Jagadeesan <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10707">PDF</a><br/><b>Abstract: </b>Intraoperative observation of tissue internal structure is often difficult.
Hence, real-time soft tissue deformation is essential for the localization of
tumor and other internal structures. We propose a method to simulate the
internal structural deformations in a soft tissue with surface profile
variations. The deformation simulation utilizes virtual physical particles that
receive interaction forces from the surface and other particles and adjust
their positions accordingly. The proposed method involves two stages. In the
initialization stage, the three-dimensional internal structure of the surface
mesh is uniformly sampled using the particle expansion and attracting-repelling
force models whilst simultaneously building the internal particle connections.
In the simulation stage, under surface profile variations, we simulate the
internal structural deformation based on a deformation force model that uses
the internal particle connections. The main advantage of this method is that it
greatly reduces the computational burden as it only involves simplified
calculations and also does not require generating three-dimensional meshes.
Preliminary experimental results show that the proposed method can handle up to
10,000 particles in 0.3s.
</p></div>
    </summary>
    <updated>2019-07-26T01:50:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-07-26T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-7683541918979097623</id>
    <link href="https://blog.computationalcomplexity.org/feeds/7683541918979097623/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/07/the-advisoradvisee-relationship.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7683541918979097623" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7683541918979097623" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/07/the-advisoradvisee-relationship.html" rel="alternate" type="text/html"/>
    <title>The Advisor/Advisee Relationship</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">I've always felt a strong advisor/advisee relationship is the single most important factor in a successful PhD career. At its best, the advisor works closely with the student to successful research agenda and help mentor them through their graduate career and beyond. The advisor/advisee relationship can feel like a parent/child relationship that lasts an entire career. Nothing gives me more pleasure as an academic than to see the success of my current and former students.<br/>
<br/>
Take your time when picking an advisor. Don't choose an advisor based solely on research area or because they are "famous". Pick the advisor that will best guide you to a successful academic career.<br/>
<br/>
At its worst, a bad advisor/advisee relationship will destroy your graduate career, making you feel miserable, perhaps dropping out of graduate school or worse, particularly if a student doesn't feel like they are being treated fairly.<br/>
<br/>
Two incidents prompted this post. On TCS-Stack Exchange, a student has <a href="https://cstheory.stackexchange.com/questions/42704/single-author-papers-against-my-advisors-will/42711">authorship issues</a> with their advisor. Unfortunately these kinds of incidents happen more often than one suspects. If you can't work it out with the advisor, go talk to someone about it, another faculty, the graduate or department chair, a grad student ombudsperson if your institution has one. We care about our students, and will work hard to resolve problems.<br/>
<br/>
In a much more <a href="https://medium.com/@huixiangvoice/the-hidden-story-behind-the-suicide-phd-candidate-huixiang-chen-236cd39f79d3">tragic event</a>, a student felt it easier to take his own life than feeling that he had to cover up potential academic misconduct. Again, if you ever find yourself in such a situation please reach out. Giving up is never the answer.</div>
    </content>
    <updated>2019-07-25T20:39:00Z</updated>
    <published>2019-07-25T20:39:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-07-26T05:44:48Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16124</id>
    <link href="https://rjlipton.wordpress.com/2019/07/25/discrepancy-games-and-sensitivity/" rel="alternate" type="text/html"/>
    <title>Discrepancy Games and Sensitivity</title>
    <summary>Can we connect the talks that closed this month’s Random Structures and Algorithms conference? Cropped from NYU homepage Joel Spencer gave the closing talk of last week’s Random Structures and Algorithms conference at ETH Zurich. Today we discuss his talk and the one that preceded it, which was by Hao Huang on his proof this […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Can we connect the talks that closed this month’s Random Structures and Algorithms conference?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2019/07/joelspencerhomepageinrussia.jpg"><img alt="" class="alignright wp-image-16126" height="200" src="https://rjlipton.files.wordpress.com/2019/07/joelspencerhomepageinrussia.jpg?w=137&amp;h=200" width="137"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from NYU <a href="https://cs.nyu.edu/spencer/">homepage</a></font></td>
</tr>
</tbody>
</table>
<p>
Joel Spencer gave the closing talk of last week’s Random Structures and Algorithms <a href="https://math.ethz.ch/fim/conferences/19th-int-conf-random-structures-algorithms/schedule.html">conference</a> at ETH Zurich.</p>
<p>
Today we discuss his talk and the one that preceded it, which was by Hao Huang on his proof this month of the Boolean sensitivity conjecture.</p>
<p>
Spencer’s <a href="https://ethz.ch/content/dam/ethz/special-interest/math/mathematical-research/fim-dam/Conferences/2019/19th Int Conf on Random Structures and Algorithms/Abstracts/Spencer_Joel.pdf">talk</a> was titled “Four Discrepancies” and based on a joint <a href="https://arxiv.org/pdf/1903.06898.pdf">paper</a> with Nikhil Bhansal. The main new result in the talk was a case where a bound of <img alt="{O(\sqrt{n\log n})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt%7Bn%5Clog+n%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\sqrt{n\log n})}"/> arising from reasoning about normal distribution can, surprisingly, be improved to a sharp <img alt="{O(\sqrt{n})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Csqrt%7Bn%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\sqrt{n})}"/>. </p>
<p>
We will talk about this first, but then progress to the talk that preceded Spencer’s. It was by Hao Huang, who was extended an invitation right after his announced <a href="http://www.mathcs.emory.edu/~hhuan30/papers/sensitivity_1.pdf">proof</a> of the Boolean Sensitivity Conjecture earlier this month. Our ulterior purpose is to ask whether any concrete connections can be found besides both talks addressing problems on the <img alt="{\{-1,+1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B-1%2C%2B1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{-1,+1\}^n}"/> hypercube.</p>
<p>
</p><p/><h2> Discrepancy Games </h2><p/>
<p/><p>
First suppose you get a stream of single bits <img alt="{v_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v_t}"/>, each <img alt="{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-1}"/> or <img alt="{+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{+1}"/>. For each bit, you can say “Keep it” or “Flip it.” In the latter case, your bit <img alt="{w_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w_t}"/> is <img alt="{-v_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-v_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-v_t}"/>. Your goal is to keep the sum <img alt="{\sum_{k=1}^t w_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bk%3D1%7D%5Et+w_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_{k=1}^t w_k}"/> within a bounded range. OK, this is easy: you can make the sum always be <img alt="{+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{+1}"/> when <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> is odd and <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> when <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> is even by flipping when needed.</p>
<p>
Now suppose the bits come in pairs <img alt="{(v_{t,1},v_{t,2})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28v_%7Bt%2C1%7D%2Cv_%7Bt%2C2%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(v_{t,1},v_{t,2})}"/> and your goal is to keep the sum in each coordinate bounded. Again there is a simple strategy: There are four kinds of vectors. For each kind, every time you see it for the second time, flip it. That keeps the contribution of each kind within <img alt="{-1 \ldots +1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-1+%5Cldots+%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-1 \ldots +1}"/> in each coordinate. Thus simple reasoning says each coordinate stays within <img alt="{-4 \ldots +4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-4+%5Cldots+%2B4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-4 \ldots +4}"/>. </p>
<p>
The trouble with extending this idea to vectors of length <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> is that the number of kinds is <img alt="{2^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^n}"/> and our simple-minded bounds, while constant in terms of the number <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> of vectors given, are exponential in <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>. Well, we can certainly do better. Going back to the <img alt="{n = 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n = 2}"/> case, we can maintain a policy of never letting both coordinates become <img alt="{+2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%2B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{+2}"/> or both become <img alt="{-2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-2}"/>. This keeps both sums within <img alt="{-2 \ldots +2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-2+%5Cldots+%2B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-2 \ldots +2}"/> regardless of the sequence of the vectors. But for larger vector lengths <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>, how large can the <em>discrepancy</em> among the <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> coordinate sums—relative to zero or to each other—become?</p>
<p>
A final help is if we know the number <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> of vectors in any sequence we might be given is bounded. In fact, Spencer’s paper with Bhansal first considers the case <img alt="{T = n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT+%3D+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T = n}"/>. There are two main questions about randomness:</p>
<ol>
<li>
Does it matter whether the sequence of vectors given is random or worst-case? <p/>
</li><li>
Can you do better than choosing “keep” or “flip” randomly?
</li></ol>
<p>
Long back, Spencer <a href="https://cs.nyu.edu/spencer/sixsigma.pdf">proved</a> that if you can see the whole sequence of <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> vectors in advance, then you can always keep the sums within <img alt="{-5.32 \sqrt{n} \ldots +5.32\sqrt{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-5.32+%5Csqrt%7Bn%7D+%5Cldots+%2B5.32%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-5.32 \sqrt{n} \ldots +5.32\sqrt{n}}"/>, whatever the sequence. If not—if you must decide “keep” or “flip” for each vector before seeing the next—then Spencer had also <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=HTxo3kP7_5gC&amp;oi=fnd&amp;pg=PP2&amp;dq=Ten+Lectures+on+the+Probabilistic+Method+Spencer&amp;ots=hGDS5tiCgB&amp;sig=qP53Ty3AQgVJF2J4wzcmT948mmY#v=onepage&amp;q=Ten Lectures on the Probabilistic Method Spencer&amp;f=false">proved</a>:</p>
<blockquote><p><b>Theorem 1</b> <em> If an adversary can choose the next vector based on your current sums, then the best bound <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{B}"/> such that you can keep all your sums within absolute value <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{B}"/> grows as <img alt="{\Theta(\sqrt{n\log n})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CTheta%28%5Csqrt%7Bn%5Clog+n%7D%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\Theta(\sqrt{n\log n})}"/>. Moreover, up to the constant in the “<img alt="{\Theta,}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CTheta%2C%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\Theta,}"/>” with high probability you cannot do any better than choosing the sign for each vector randomly. </em>
</p></blockquote>
<p/><p>
See also his famous <a href="https://www.wiley.com/en-us/The+Probabilistic+Method/+4th+Edition-p-9781119061953">book</a>, <em>The Probabilistic Method</em>, with Noga Alon. The new theorem is:</p>
<blockquote><p><b>Theorem 2</b> <em> If the adversary presents vectors uniformly at random, then you <b>can</b> achieve <img alt="{B = O(\sqrt{n})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB+%3D+O%28%5Csqrt%7Bn%7D%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{B = O(\sqrt{n})}"/> with high probability—and not by choosing the signs randomly. </em>
</p></blockquote>
<p/><p>
The algorithm defines a kind of <em>potential function</em> and has you play “keep” or “flip” according to which has the lesser growth in potential. The analysis is quite complicated. As usual we refer to the <a href="https://arxiv.org/pdf/1903.06898.pdf">paper</a> for details. Instead we ask: are there insights to mine here for further advances on the Boolean sensitivity problem?</p>
<p>
</p><p/><h2> Boolean Sensitivity </h2><p/>
<p/><p>
We didn’t talk about Boolean sensitivity in our previous <a href="https://rjlipton.wordpress.com/2019/07/12/tools-and-sensitivity/">post</a> on it. Our purpose now is to convey how many concepts are related, hence how they might connect to discrepancy on the hypercube. </p>
<p>
To get the idea of sensitivity, first consider the parity function <img alt="{f_{\oplus}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_%7B%5Coplus%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_{\oplus}}"/>. If you change one bit of any argument <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> you change the value of <img alt="{f_{\oplus}(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_%7B%5Coplus%7D%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_{\oplus}(x)}"/>. Define <img alt="{x^i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5Ei%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^i}"/> to mean <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> with bit <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> flipped. Then for any <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> and <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>, <img alt="{f_{\oplus}(x^i) \neq f_{\oplus}(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_%7B%5Coplus%7D%28x%5Ei%29+%5Cneq+f_%7B%5Coplus%7D%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_{\oplus}(x^i) \neq f_{\oplus}(x)}"/>. This means parity is extremely sensitive.</p>
<p>
The OR function <img alt="{f_{\vee}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_%7B%5Cvee%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_{\vee}}"/> is intuitively less sensitive, but it too has an argument <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> such that <img alt="{f_{\vee}(x^i) \neq f_{\vee}(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_%7B%5Cvee%7D%28x%5Ei%29+%5Cneq+f_%7B%5Cvee%7D%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_{\vee}(x^i) \neq f_{\vee}(x)}"/> for all <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>, namely <img alt="{x = 0^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%3D+0%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x = 0^n}"/>. For any Boolean function <img alt="{f: \{0,1\}^n \rightarrow \{0,1\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%3A+%5C%7B0%2C1%5C%7D%5En+%5Crightarrow+%5C%7B0%2C1%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f: \{0,1\}^n \rightarrow \{0,1\}}"/> define its <em>sensitivity</em> (at <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>) by </p>
<p align="center"><img alt="\displaystyle  s(f) = s_n(f) = \max_{x \in \{0,1\}^n} ||\{i: f(x^i) \neq f(x)\}||. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++s%28f%29+%3D+s_n%28f%29+%3D+%5Cmax_%7Bx+%5Cin+%5C%7B0%2C1%5C%7D%5En%7D+%7C%7C%5C%7Bi%3A+f%28x%5Ei%29+%5Cneq+f%28x%29%5C%7D%7C%7C.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  s(f) = s_n(f) = \max_{x \in \{0,1\}^n} ||\{i: f(x^i) \neq f(x)\}||. "/></p>
<p>The OR-of-AND function <img alt="{f_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_2}"/> is less sensitive. Say it is an OR of <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> blocks, each of <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> variables, and the blocks use disjoint variables so <img alt="{n = km}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+km%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n = km}"/>. If <img alt="{f_2(x) = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_2%28x%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_2(x) = 1}"/>, then some block is all <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>, so flipping any other bit makes no difference, and the most sensitive we can get is <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>. If <img alt="{f_2(x) = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_2%28x%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_2(x) = 0}"/> then the most sensitive case is when all blocks have exactly one 0. So <img alt="{s(f_2) = \max\{k,m\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%28f_2%29+%3D+%5Cmax%5C%7Bk%2Cm%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s(f_2) = \max\{k,m\}}"/>. When <img alt="{k = m = \sqrt{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%3D+m+%3D+%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k = m = \sqrt{n}}"/>, <img alt="{s_n(f) = \sqrt{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_n%28f%29+%3D+%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_n(f) = \sqrt{n}}"/>.</p>
<p>
If we make each block of <img alt="{f_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_2}"/> return <em>true</em> if exactly one bit is <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>, then we again get sensitivity <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> on the all-<img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> assignment. Now, however, consider the related function <img alt="{f'_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%27_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f'_2}"/> (with <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> even, <img alt="{k = 2\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%3D+2%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k = 2\ell}"/>) that is still an OR over blocks, but each block is true when some consecutive pair <img alt="{x_{2\ell - 1},x_{2\ell}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_%7B2%5Cell+-+1%7D%2Cx_%7B2%5Cell%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_{2\ell - 1},x_{2\ell}}"/> are both <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> with all other pairs being both <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>. Then we can’t do the same trick with the all-<img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> assignment changing just one bit. So when <img alt="{n = 4\ell^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+4%5Cell%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n = 4\ell^2}"/> and <img alt="{m = k = 2\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm+%3D+k+%3D+2%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m = k = 2\ell}"/> we again get sensitivity (no more than) <img alt="{\sqrt{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sqrt{n}}"/>. </p>
<p>
We can, however, consider <img alt="{f'_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%27_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f'_2}"/> to be more sensitive if we can flip more than one bit at a time. Partition <img alt="{[n]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Bn%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{[n]}"/> into <em>blocks</em> <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> of two consecutive bit-places each, and given any <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>, define <img alt="{x^B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5EB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^B}"/> to be the result of flipping the bits in <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/>. We get <img alt="{n/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n/2}"/> blocks, and the all-<img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/> assignment becomes a <em>true</em> case of <img alt="{f'_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%27_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f'_2}"/> if any block is flipped. Generally define the <em>block sensitivity</em> by considering any partitions <img alt="{\mathcal{B} = \{B_j\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BB%7D+%3D+%5C%7BB_j%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{B} = \{B_j\}}"/> of <img alt="{[n]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Bn%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{[n]}"/> into disjoint subsets and writing </p>
<p align="center"><img alt="\displaystyle  bs(f) = \max_{x,\mathcal{B}} ||\{j: f(x^{B_j}) \neq f(x)\}||. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++bs%28f%29+%3D+%5Cmax_%7Bx%2C%5Cmathcal%7BB%7D%7D+%7C%7C%5C%7Bj%3A+f%28x%5E%7BB_j%7D%29+%5Cneq+f%28x%29%5C%7D%7C%7C.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  bs(f) = \max_{x,\mathcal{B}} ||\{j: f(x^{B_j}) \neq f(x)\}||. "/></p>
<p>Note that not every member of the partition has to flip the function—we can discard the ones that don’t flip and count only the disjoint subsets that do flip the value. So back to our example, we have </p>
<p align="center"><img alt="\displaystyle  bs(f'_2) = \frac{n}{2} = \frac{1}{2}s(f'_2)^2. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++bs%28f%27_2%29+%3D+%5Cfrac%7Bn%7D%7B2%7D+%3D+%5Cfrac%7B1%7D%7B2%7Ds%28f%27_2%29%5E2.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  bs(f'_2) = \frac{n}{2} = \frac{1}{2}s(f'_2)^2. "/></p>
<p>Andris Ambainis and Xiaoming Sun <a href="https://arxiv.org/abs/1108.3494">improved</a> the constant from <img alt="{\frac{1}{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{1}{2}}"/> asymptotically to <img alt="{\frac{2}{3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B2%7D%7B3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{2}{3}}"/>, but their relation is still quadratic.</p>
<p>
</p><p/><h2> Some Boolean Complexity Connections </h2><p/>
<p/><p>
This <a href="https://link.springer.com/article/10.1007/BF01200762">example</a> of quadratic discrepancy is still the best known lower bound on <img alt="{bs(f)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bbs%28f%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{bs(f)}"/> in terms of <img alt="{s(f)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%28f%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s(f)}"/>. But no one had proved anything better than an exponential upper bound until Huang’s result, from which it follows that:</p>
<blockquote><p><b>Theorem 3</b> <em><a name="Huang"/> For all Boolean functions <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f}"/>, <img alt="{bs(f) \leq 2s(f)^4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bbs%28f%29+%5Cleq+2s%28f%29%5E4%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{bs(f) \leq 2s(f)^4}"/>. </em>
</p></blockquote>
<p/><p>
This bound is concrete, not just asymptotic. It still leaves a gap between quadratic and quartic. It is, however, the combination of two quadratic upper bounds. One was shown by Nisan and Szegedy in their <a href="https://www.researchgate.net/publication/2508255_On_the_Degree_of_Boolean_Functions_as_Real_Polynomials">paper</a>: </p>
<p align="center"><img alt="\displaystyle  bs(f) \leq 2\deg(f)^2, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++bs%28f%29+%5Cleq+2%5Cdeg%28f%29%5E2%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  bs(f) \leq 2\deg(f)^2, "/></p>
<p>where <img alt="{\deg(f)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdeg%28f%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\deg(f)}"/> means the degree of the unique multi-linear real polynomial that agrees with <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f}"/> on the cube <img alt="{\{-1,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B-1%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{-1,1\}^n}"/> with <img alt="{-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{-1}"/> for <em>true</em>. The other is the conjecture </p>
<p align="center"><img alt="\displaystyle  \deg(f) \leq s(f)^2 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cdeg%28f%29+%5Cleq+s%28f%29%5E2+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \deg(f) \leq s(f)^2 "/></p>
<p>in a 1992 <a href="https://www.sciencedirect.com/science/article/pii/0097316592900608">paper</a> by Craig Gotsman and Nati Linial. Huang proved this by exploiting a connection to graphs that was also shown by Gotsman and Linial. Consider any red-or-white coloring of the <img alt="{2^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^n}"/> nodes of the hypercube, let <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> be the graph induced by the red nodes, and define <img alt="{g(x) = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%28x%29+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g(x) = 1}"/> if node <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> is red, <img alt="{g(x) = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%28x%29+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g(x) = 0}"/> otherwise. Now if the maximum degree <img alt="{d(G)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d(G)}"/> of a node in <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> is small then every red node has many white neighbors, so the Boolean function <img alt="{g}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g}"/> is very sensitive. However, going to each neighbor in the hypercube flips the parity. Hence the function </p>
<p align="center"><img alt="\displaystyle  g'(x) = g(x) \oplus f_{\oplus}(x) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++g%27%28x%29+%3D+g%28x%29+%5Coplus+f_%7B%5Coplus%7D%28x%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  g'(x) = g(x) \oplus f_{\oplus}(x) "/></p>
<p>is <b>not</b> very sensitive. Moreover, it has the same sensitivity as the function <img alt="{h'(x) = h(x) \oplus f_{\oplus}(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%27%28x%29+%3D+h%28x%29+%5Coplus+f_%7B%5Coplus%7D%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h'(x) = h(x) \oplus f_{\oplus}(x)}"/> where <img alt="{h(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{h(x)}"/> is true on the white nodes. This nice duality between <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> and the graph <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/> induced by the white nodes enables us to fix “<img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>” to mean whichever of the two has more nodes in the following theorem statement: </p>
<blockquote><p><b>Theorem 4</b> <em> Provided <img alt="{m &gt; 2^{n-1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm+%3E+2%5E%7Bn-1%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{m &gt; 2^{n-1}}"/>, every graph <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{G}"/> induced by <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{m}"/> nodes of the <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/>-cube has <img alt="{d(G) \geq \sqrt{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd%28G%29+%5Cgeq+%5Csqrt%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{d(G) \geq \sqrt{n}}"/> if and only if every Boolean function <img alt="{f}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{f}"/> has <img alt="{\deg(f) \leq s(f)^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdeg%28f%29+%5Cleq+s%28f%29%5E2%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\deg(f) \leq s(f)^2}"/>. </em>
</p></blockquote>
<p/><p>
The proof uses some Fourier analysis with <img alt="{f = g'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf+%3D+g%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f = g'}"/> as above. It, too, takes only one page of a really short paper. </p>
<p>
The main open question now is whether the 4th-power upper bound in Huang’s theorem <a href="https://rjlipton.wordpress.com/feed/#Huang">3</a> can be improved to quadratic. It is possible that a deeper application of Fourier analysis may show that cases of quadratic separation from <em>block-sensitivity</em> to <em>degree</em> and <em>degree</em> to <em>sensitivity</em> cannot “amplify” any more than quadratic. This is where there might be some commonality with discrepancy. </p>
<p>
There is a much wider suite of Boolean complexity measures besides <img alt="{s(f)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%28f%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s(f)}"/>, <img alt="{bs(f)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bbs%28f%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{bs(f)}"/>, and <img alt="{\deg(f)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdeg%28f%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\deg(f)}"/> discussed here. For example, consider how many bits of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> you need to fix in order to preserve the value <img alt="{f(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x)}"/>. That is, define <img alt="{C(f)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%28f%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C(f)}"/> to be the maximum over <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> of the minimum size of a set <img alt="{I \subseteq [n]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BI+%5Csubseteq+%5Bn%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{I \subseteq [n]}"/> such that whenever <img alt="{x'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x'}"/> agrees with <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> on <img alt="{I}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BI%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{I}"/>, <img alt="{f(x') = f(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf%28x%27%29+%3D+f%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f(x') = f(x)}"/>. Clearly <img alt="{I}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BI%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{I}"/> needs to include at least one bit from each <img alt="{B_j.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB_j.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B_j.}"/> This proves <img alt="{C(f) \geq bs(f)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%28f%29+%5Cgeq+bs%28f%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C(f) \geq bs(f)}"/>. There are many other relations that might be improved. We end by considering ways of broadening Huang’s techniques.</p>
<p>
</p><p/><h2> Weak Adjacency Matrices </h2><p/>
<p/><p>
We—that is Ken and I—have been thinking about how to build on Huang’s proof. We take a somewhat more-general approach compared to the paper and Ken’s <a href="https://rjlipton.wordpress.com/2019/07/12/tools-and-sensitivity/">post</a>.</p>
<blockquote><p><b>Definition 5</b> <em> Let <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{G}"/> be a graph on <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/> vertices. The <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/> by <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/> matrix <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A}"/> is a <b>weak adjacency</b> matrix of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{G}"/> provided </em></p><em>
<ol>
<li>
Every entry <img alt="{A_{x,y}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7Bx%2Cy%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A_{x,y}}"/> is bounded by <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{1}"/> in absolute value; <p/>
</li><li>
For all <img alt="{x,y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%2Cy%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{x,y}"/> if <img alt="{x \rightarrow y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%5Crightarrow+y%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{x \rightarrow y}"/> is not an edge in <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{G}"/>, then <img alt="{A_{x,y}=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7Bx%2Cy%7D%3D0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A_{x,y}=0}"/>.
</li></ol>
</em><p><em/>
</p></blockquote>
<p/><p>
As usual the maximum degree of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/> is denoted by <img alt="{\deg(G)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdeg%28G%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\deg(G)}"/>. The following is almost the same proof as the classic result on adjacency matrices. </p>
<blockquote><p><b>Lemma 6 (H-Lemma)</b> <em> Suppose that <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A}"/> is a weak adjacency matrix of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{G}"/>. Then if <img alt="{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\lambda}"/> is an eigenvalue of <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A}"/>, 	</em></p><em>
<p align="center"><img alt="\displaystyle  | \lambda | \le \deg(G). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C+%5Clambda+%7C+%5Cle+%5Cdeg%28G%29.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="\displaystyle  | \lambda | \le \deg(G). "/></p>
</em><p><em/>
</p></blockquote>
<p/><p>
<em>Proof:</em>  By the definition of eigenvalue, there is some non-zero vector <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{v}"/> so that <img alt="{Av = \lambda v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAv+%3D+%5Clambda+v%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Av = \lambda v}"/>. Let <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> be an index so that <img alt="{|v_{k}|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Cv_%7Bk%7D%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|v_{k}|}"/> maximum. Then 	</p>
<p align="center"><img alt="\displaystyle  |\lambda v_{k}| =|(A v)_{k}|= \left|\sum_{y=1}^{n} A_{k,y}v_{y} \right| \leq \sum_{y=1}^{n} |A_{k,y}| \cdot |v_{k}|. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%5Clambda+v_%7Bk%7D%7C+%3D%7C%28A+v%29_%7Bk%7D%7C%3D+%5Cleft%7C%5Csum_%7By%3D1%7D%5E%7Bn%7D+A_%7Bk%2Cy%7Dv_%7By%7D+%5Cright%7C+%5Cleq+%5Csum_%7By%3D1%7D%5E%7Bn%7D+%7CA_%7Bk%2Cy%7D%7C+%5Ccdot+%7Cv_%7Bk%7D%7C.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  |\lambda v_{k}| =|(A v)_{k}|= \left|\sum_{y=1}^{n} A_{k,y}v_{y} \right| \leq \sum_{y=1}^{n} |A_{k,y}| \cdot |v_{k}|. "/></p>
<p>But then the last sum is upper bounded by 	</p>
<p align="center"><img alt="\displaystyle  D|v_{k}|, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++D%7Cv_%7Bk%7D%7C%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  D|v_{k}|, "/></p>
<p>where <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> is the number of <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> so that <img alt="{A_{k,y}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7Bk%2Cy%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_{k,y}}"/> is not zero. This uses property (1) of the definition of a weak adjacency matrix. Property (2) implies that <img alt="{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D}"/> is at most the degree of vertex <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> is <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>. Thus 	</p>
<p align="center"><img alt="\displaystyle  |\lambda v_{k}| \leq D |v_{k}| \leq \deg(G) |v_{k}|. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C%5Clambda+v_%7Bk%7D%7C+%5Cleq+D+%7Cv_%7Bk%7D%7C+%5Cleq+%5Cdeg%28G%29+%7Cv_%7Bk%7D%7C.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  |\lambda v_{k}| \leq D |v_{k}| \leq \deg(G) |v_{k}|. "/></p>
<p>Dividing by <img alt="{|v_{k}|&gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Cv_%7Bk%7D%7C%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|v_{k}|&gt;0}"/> yields the lemma. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
Let <img alt="{G-k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG-k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G-k}"/> be the graph that results after we delete the vertex <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> from <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G}"/>. Let <img alt="{A-k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA-k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A-k}"/> be the matrix that results after we delete the column and row <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> from <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/>. </p>
<blockquote><p><b>Lemma 7</b> <em> Suppose that <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A}"/> is a weak adjacency matrix of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{G}"/>. Then for any vertex <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{k}"/>, <img alt="{A-k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA-k%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A-k}"/> is a weak adjacency matrix of <img alt="{G-k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG-k%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{G-k}"/>. </em>
</p></blockquote>
<p/><p>
<em>Proof:</em>  The bound on the entries of <img alt="{A-k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA-k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A-k}"/> is immediate. Whenever <img alt="{G-k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG-k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{G-k}"/> has no edge from <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> to <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/>, then <img alt="{(A-k)_{x,y}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28A-k%29_%7Bx%2Cy%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(A-k)_{x,y}}"/> must be zero. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<blockquote><p><b>Lemma 8</b> <em> Suppose that <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{G}"/> a <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/>-graph has a real symmetric weak adjacency matrix <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{A}"/> with <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{m}"/> of the top eigenvalues greater than <img alt="{B \ge 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB+%5Cge+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{B \ge 0}"/>. Then every induced subgraph <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{H}"/> of <img alt="{G}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BG%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{G}"/> with at least <img alt="{n-m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn-m%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n-m}"/> vertices has degree at least <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{B}"/>. </em>
</p></blockquote>
<p>
</p><blockquote><p><b>Definition 9</b> <em> The <b>hypercube</b> <img alt="{C_{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{C_{n}}"/> is the graph on <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/> bit vectors so that two vertices are adjacent if they differ in one bit position. </em>
</p></blockquote>
<p>
</p><blockquote><p><b>Theorem 10</b> <em> The hypercube <img alt="{C_{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{C_{n}}"/> has a real symmetric weak adjacency matrix with all its eigenvalues <img alt="{\pm \sqrt{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+%5Csqrt%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\pm \sqrt{n}}"/>. </em>
</p></blockquote>
<p/><p>
<em>Proof:</em>  Let <img alt="{A_{1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_{1}}"/> be 	</p>
<p align="center"><img alt="\displaystyle  \begin{bmatrix}   0 &amp; 1 \\   1 &amp; 0  \end{bmatrix}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Bbmatrix%7D+%09%090+%26+1+%5C%5C+%09%091+%26+0+%09%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{bmatrix}   0 &amp; 1 \\   1 &amp; 0  \end{bmatrix}. "/></p>
<p>and <img alt="{A_{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_{n}}"/> be 	</p>
<p align="center"><img alt="\displaystyle  \begin{bmatrix}   A_{n-1} &amp; I \\   I &amp; -A_{n-1}  \end{bmatrix}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Bbmatrix%7D+%09%09A_%7Bn-1%7D+%26+I+%5C%5C+%09%09I+%26+-A_%7Bn-1%7D+%09%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{bmatrix}   A_{n-1} &amp; I \\   I &amp; -A_{n-1}  \end{bmatrix}. "/></p>
<p>
Induction shows that <img alt="{A_{n}^{2} = nI}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7Bn%7D%5E%7B2%7D+%3D+nI%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_{n}^{2} = nI}"/>. It follows that 	</p>
<p align="center"><img alt="\displaystyle  A_{n}x = \lambda x, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++A_%7Bn%7Dx+%3D+%5Clambda+x%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  A_{n}x = \lambda x, "/></p>
<p>implies that 	</p>
<p align="center"><img alt="\displaystyle  nx = \lambda Ax. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++nx+%3D+%5Clambda+Ax.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  nx = \lambda Ax. "/></p>
<p>Thus 	</p>
<p align="center"><img alt="\displaystyle  nx = \lambda^{2}x. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++nx+%3D+%5Clambda%5E%7B2%7Dx.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  nx = \lambda^{2}x. "/></p>
<p>So the eigenvalues are <img alt="{\pm \sqrt{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpm+%5Csqrt%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\pm \sqrt{n}}"/> and by trace same number of each.</p>
<p>
The upper-left and lower-right blocks of <img alt="{A_{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_{n}}"/> correspond to the two <img alt="{n-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n-1}"/> dimensional subcubes of <img alt="{C_{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_{n}}"/>, and the two identity blocks correspond to the perfect matching connecting these two subcubes. So <img alt="{A_{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A_{n}}"/> is a weak adjacency matrix for <img alt="{C_{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_%7Bn%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_{n}}"/>. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<blockquote><p><b>Corollary 11</b> <em> Every induced subgraph <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{H}"/> of <img alt="{C_{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{C_{n}}"/> with at least <img alt="{2^{n-1}+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bn-1%7D%2B1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{2^{n-1}+1}"/> vertices has degree at least <img alt="{\sqrt{n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7Bn%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\sqrt{n}}"/>. </em>
</p></blockquote>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Can the last two talks at the conference be further connected?</p>
<p/></font></font></div>
    </content>
    <updated>2019-07-25T20:35:29Z</updated>
    <published>2019-07-25T20:35:29Z</published>
    <category term="All Posts"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Open Problems"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="algoritrhms"/>
    <category term="Boolean functions"/>
    <category term="Boolean sensitivity conjecture"/>
    <category term="discrepancy"/>
    <category term="Hao Huang"/>
    <category term="hypercube"/>
    <category term="Joel Spencer"/>
    <category term="Nihkil Bhansal"/>
    <category term="randomized algorithms"/>
    <category term="randomness"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-07-26T07:20:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=17669</id>
    <link href="https://gilkalai.wordpress.com/2019/07/25/tyi-39-can-a-coalition-of-children-guarantees-being-in-the-same-class/" rel="alternate" type="text/html"/>
    <title>TYI 39 : Can a coalition of children guarantees all being in the same class?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">There is a class of children that have just finished elementary school. Now they all move from elementary school to high school and classes are reshuffled. Each child lists three friends, and the assignment of children into classes ensures that … <a href="https://gilkalai.wordpress.com/2019/07/25/tyi-39-can-a-coalition-of-children-guarantees-being-in-the-same-class/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://gilkalai.files.wordpress.com/2019/07/class_school.jpg"><img alt="" class="alignnone size-medium wp-image-17676" height="213" src="https://gilkalai.files.wordpress.com/2019/07/class_school.jpg?w=300&amp;h=213" width="300"/></a></p>
<p>There is a class of children that have just finished elementary school. Now they all move from elementary school to high school and classes are reshuffled. Each child lists three friends, and the assignment of children into classes ensures that each child will have at least one of these three friends in his class.</p>
<p>One of the children heard from five of his schoolmates that they found that they can make their selections in a way that will ensure that all five will be assigned to the same class!</p>
<h3><span style="color: #993366;">Test your intuition: Is there a strategy for five of the children that will ensure that all five will be assigned to the same class?</span></h3>
<p>Can a larger group of children coordinate their choices to ensure that they will all necessarily be assigned to the same class?</p>
<a name="pd_a_10371327"/><div class="CSS_Poll PDS_Poll" id="PDI_container10371327" style="display: inline-block;"/><div id="PD_superContainer"/><noscript>&lt;a href="https://polldaddy.com/p/10371327" target="_blank"&gt;Take Our Poll&lt;/a&gt;</noscript>
<p><span id="more-17669"/></p>
<p><strong>Bonus question:</strong> In case that every child lists only two friends and one of them is guaranteed to be in the same class. Is there a strategy of five children that will ensure them to be in the same class?</p></div>
    </content>
    <updated>2019-07-25T12:59:05Z</updated>
    <published>2019-07-25T12:59:05Z</published>
    <category term="Combinatorics"/>
    <category term="Economics"/>
    <category term="Mathematics to the rescue"/>
    <category term="Test your intuition"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-07-26T07:20:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10531</id>
    <link href="http://arxiv.org/abs/1907.10531" rel="alternate" type="text/html"/>
    <title>Sampling and Optimization on Convex Sets in Riemannian Manifolds of Non-Negative Curvature</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goyal:Navin.html">Navin Goyal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shetty:Abhishek.html">Abhishek Shetty</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10531">PDF</a><br/><b>Abstract: </b>The Euclidean space notion of convex sets (and functions) generalizes to
Riemannian manifolds in a natural sense and is called geodesic convexity.
Extensively studied computational problems such as convex optimization and
sampling in convex sets also have meaningful counterparts in the manifold
setting. Geodesically convex optimization is a well-studied problem with
ongoing research and considerable recent interest in machine learning and
theoretical computer science. In this paper, we study sampling and convex
optimization problems over manifolds of non-negative curvature proving
polynomial running time in the dimension and other relevant parameters. Our
algorithms assume a warm start. We first present a random walk based sampling
algorithm and then combine it with simulated annealing for solving convex
optimization problems. To our knowledge, these are the first algorithms in the
general setting of positively curved manifolds with provable polynomial
guarantees under reasonable assumptions, and the first study of the connection
between sampling and optimization in this setting.
</p></div>
    </summary>
    <updated>2019-07-25T23:28:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10499</id>
    <link href="http://arxiv.org/abs/1907.10499" rel="alternate" type="text/html"/>
    <title>P-SLOCAL-Completeness of Maximum Independent Set Approximation</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maus:Yannic.html">Yannic Maus</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10499">PDF</a><br/><b>Abstract: </b>We prove that the maximum independent set approximation problem with
polylogarithmic approximation factor is P-SLOCAL-complete. Thus an efficient
algorithm for the maximum independent set approximation in the LOCAL model
efficient algorithms for many problems in the LOCAL model including the
computation of (polylog n, polylog n) network decompositions.
</p></div>
    </summary>
    <updated>2019-07-25T23:32:08Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10468</id>
    <link href="http://arxiv.org/abs/1907.10468" rel="alternate" type="text/html"/>
    <title>The Complexity of Computational Problems about Nash Equilibria in Symmetric Win-Lose Games</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bil=ograve=:Vittorio.html">Vittorio Bilò</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mavronicolas:Marios.html">Marios Mavronicolas</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10468">PDF</a><br/><b>Abstract: </b>We revisit the complexity of deciding, given a {\it bimatrix game,} whether
it has a {\it Nash equilibrium} with certain natural properties; such decision
problems were early known to be ${\mathcal{NP}}$-hard~\cite{GZ89}. We show that
${\mathcal{NP}}$-hardness still holds under two significant restrictions in
simultaneity: the game is {\it win-lose} (that is, all {\it utilities} are $0$
or $1$) and {\it symmetric}. To address the former restriction, we design
win-lose {\it gadgets} and a win-lose reduction; to accomodate the latter
restriction, we employ and analyze the classical {\it
${\mathsf{GHR}}$-symmetrization}~\cite{GHR63} in the win-lose setting. Thus,
{\it symmetric win-lose bimatrix games} are as complex as general bimatrix
games with respect to such decision problems. As a byproduct of our techniques,
we derive hardness results for search, counting and parity problems about Nash
equilibria in symmetric win-lose bimatrix games.
</p></div>
    </summary>
    <updated>2019-07-25T23:21:03Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10444</id>
    <link href="http://arxiv.org/abs/1907.10444" rel="alternate" type="text/html"/>
    <title>Constant Delay Traversal of Grammar-Compressed Graphs with Bounded Rank</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maneth:Sebastian.html">Sebastian Maneth</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peternek:Fabian.html">Fabian Peternek</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10444">PDF</a><br/><b>Abstract: </b>We present a pointer-based data structure for constant time traversal of the
edges of an edge-labeled (alphabet $\Sigma$) directed hypergraph (a graph where
edges can be incident to more than two vertices, and the incident vertices are
ordered) given as hyperedge-replacement grammar $G$. It is assumed that the
grammar has a fixed rank $\kappa$ (maximal number of vertices connected to a
nonterminal hyperedge) and that each vertex of the represented graph is
incident to at most one $\sigma$-edge per direction ($\sigma \in \Sigma$).
Precomputing the data structure needs $O(|G||\Sigma|\kappa r h)$ space and
$O(|G||\Sigma|\kappa rh^2)$ time, where $h$ is the height of the derivation
tree of $G$ and $r$ is the maximal rank of a terminal edge occurring in the
grammar.
</p></div>
    </summary>
    <updated>2019-07-25T23:33:24Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10398</id>
    <link href="http://arxiv.org/abs/1907.10398" rel="alternate" type="text/html"/>
    <title>Medians in median graphs in linear time</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Laurine Bénéteau, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chalopin:J=eacute=r=eacute=mie.html">Jérémie Chalopin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chepoi:Victor.html">Victor Chepoi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vax=egrave=s:Yann.html">Yann Vaxès</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10398">PDF</a><br/><b>Abstract: </b>The median of a graph $G$ is the set of all vertices $x$ of $G$ minimizing
the sum of distances from $x$ to all other vertices of $G$. It is known that
computing the median of dense graphs in subcubic time refutes the APSP
conjecture and computing the median of sparse graphs in subquadratic time
refutes the HS conjecture. In this paper, we present a linear time algorithm
for computing medians of median graphs, improving over the existing quadratic
time algorithm. Median graphs constitute the principal class of graphs
investigated in metric graph theory, due to their bijections with other
discrete and geometric structures (CAT(0) cube complexes, domains of event
structures, and solution sets of 2-SAT formulas). Our algorithm is based on the
known majority rule characterization of medians in a median graph $G$ and on a
fast computation of parallelism classes of edges ($\Theta$-classes) of $G$. The
main technical contribution of the paper is a linear time algorithm for
computing the $\Theta$-classes of a median graph $G$ using Lexicographic
Breadth First Search (LexBFS). Namely, we show that any LexBFS ordering of the
vertices of a median graph $G$ has the following \emph{fellow traveler
property}: the fathers of any two adjacent vertices of $G$ are also adjacent.
Using the fast computation of the $\Theta$-classes of a median graph $G$, we
also compute the Wiener index (total distance) of $G$ in linear time.
</p></div>
    </summary>
    <updated>2019-07-25T23:28:51Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10376</id>
    <link href="http://arxiv.org/abs/1907.10376" rel="alternate" type="text/html"/>
    <title>Reducing Path TSP to TSP</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Traub:Vera.html">Vera Traub</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vygen:Jens.html">Jens Vygen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zenklusen:Rico.html">Rico Zenklusen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10376">PDF</a><br/><b>Abstract: </b>We present a black-box reduction from the path version of the Traveling
Salesman Problem (Path TSP) to the classical tour version (TSP). More
precisely, we show that given an $\alpha$-approximation algorithm for TSP,
then, for any $\epsilon &gt;0$, there is an $(\alpha+\epsilon)$-approximation
algorithm for the more general Path TSP. This reduction implies that the
approximability of Path TSP is the same as for TSP, up to an arbitrarily small
error. This avoids future discrepancies between the best known approximation
factors achievable for these two problems, as they have existed until very
recently.
</p>
<p>A well-studied special case of TSP, Graph TSP, asks for tours in unit-weight
graphs. Our reduction shows that any $\alpha$-approximation algorithm for Graph
TSP implies an $(\alpha+\epsilon)$-approximation algorithm for its path
version. By applying our reduction to the $1.4$-approximation algorithm for
Graph TSP by Seb\H{o} and Vygen, we obtain a polynomial-time
$(1.4+\epsilon)$-approximation algorithm for Graph Path TSP, improving on a
recent $1.497$-approximation algorithm of Traub and Vygen.
</p>
<p>We obtain our results through a variety of new techniques, including a novel
way to set up a recursive dynamic program to guess significant parts of an
optimal solution. At the core of our dynamic program we deal with instances of
a new generalization of (Path) TSP which combines parity constraints with
certain connectivity requirements. This problem, which we call $\Phi$-TSP, has
a constant-factor approximation algorithm and can be reduced to TSP in certain
cases when the dynamic program would not make sufficient progress.
</p></div>
    </summary>
    <updated>2019-07-25T23:32:16Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10367</id>
    <link href="http://arxiv.org/abs/1907.10367" rel="alternate" type="text/html"/>
    <title>DispVoxNets: Non-Rigid Point Set Alignment with Supervised Learning Proxies</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shimada:Soshi.html">Soshi Shimada</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Golyanik:Vladislav.html">Vladislav Golyanik</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tretschk:Edgar.html">Edgar Tretschk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stricker:Didier.html">Didier Stricker</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Theobalt:Christian.html">Christian Theobalt</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10367">PDF</a><br/><b>Abstract: </b>We introduce a supervised-learning framework for non-rigid point set
alignment of a new kind - Displacements on Voxels Networks (DispVoxNets) -
which abstracts away from the point set representation and regresses 3D
displacement fields on regularly sampled proxy 3D voxel grids. Thanks to
recently released collections of deformable objects with known intra-state
correspondences, DispVoxNets learn a deformation model and further priors
(e.g., weak point topology preservation) for different object categories such
as cloths, human bodies and faces. DispVoxNets cope with large deformations,
noise and clustered outliers more robustly than the state-of-the-art. At test
time, our approach runs orders of magnitude faster than previous techniques.
All properties of DispVoxNets are ascertained numerically and qualitatively in
extensive experiments and comparisons to several previous methods.
</p></div>
    </summary>
    <updated>2019-07-25T23:34:45Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10363</id>
    <link href="http://arxiv.org/abs/1907.10363" rel="alternate" type="text/html"/>
    <title>Classification of linear codes using canonical augmentation</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bouyukliev:Iliya.html">Iliya Bouyukliev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bouyuklieva:Stefka.html">Stefka Bouyuklieva</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10363">PDF</a><br/><b>Abstract: </b>We propose an algorithm for classification of linear codes over different
finite fields based on canonical augmentation. We apply this algorithm to
obtain classification results over fields with 2, 3 and 4 elements.
</p></div>
    </summary>
    <updated>2019-07-25T23:31:11Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10333</id>
    <link href="http://arxiv.org/abs/1907.10333" rel="alternate" type="text/html"/>
    <title>Anti-unification in Constraint Logic Programming</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Gonzague Yernaux, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vanhoof:Wim.html">Wim Vanhoof</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10333">PDF</a><br/><b>Abstract: </b>Anti-unification refers to the process of generalizing two (or more) goals
into a single, more general, goal that captures some of the structure that is
common to all initial goals. In general one is typically interested in
computing what is often called a most specific generalization, that is a
generalization that captures a maximal amount of shared structure. In this work
we address the problem of anti-unification in CLP, where goals can be seen as
unordered sets of atoms and/or constraints. We show that while the concept of a
most specific generalization can easily be defined in this context, computing
it becomes an NP-complete problem. We subsequently introduce a generalization
algorithm that computes a well-defined abstraction whose computation can be
bound to a polynomial execution time. Initial experiments show that even a
naive implementation of our algorithm produces acceptable generalizations in an
efficient way. Under consideration for acceptance in TPLP.
</p></div>
    </summary>
    <updated>2019-07-25T23:23:18Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10250</id>
    <link href="http://arxiv.org/abs/1907.10250" rel="alternate" type="text/html"/>
    <title>Learning Embedding of 3D models with Quadric Loss</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Agarwal:Nitin.html">Nitin Agarwal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yoon:Sung=eui.html">Sung-eui Yoon</a>, M Gopi <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10250">PDF</a><br/><b>Abstract: </b>Sharp features such as edges and corners play an important role in the
perception of 3D models. In order to capture them better, we propose quadric
loss, a point-surface loss function, which minimizes the quadric error between
the reconstructed points and the input surface. Computation of Quadric loss is
easy, efficient since the quadric matrices can be computed apriori, and is
fully differentiable, making quadric loss suitable for training point and mesh
based architectures. Through extensive experiments we show the merits and
demerits of quadric loss. When combined with Chamfer loss, quadric loss
achieves better reconstruction results as compared to any one of them or other
point-surface loss functions.
</p></div>
    </summary>
    <updated>2019-07-25T23:43:46Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10230</id>
    <link href="http://arxiv.org/abs/1907.10230" rel="alternate" type="text/html"/>
    <title>An FPT algorithm for orthogonal buttons and scissors</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tsur:Dekel.html">Dekel Tsur</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10230">PDF</a><br/><b>Abstract: </b>We study the puzzle game Buttons and Scissors in which the goal is to remove
all buttons from an $n\times m$ grid by a series of horizontal and vertical
cuts. We show that the corresponding parameterized problem has an algorithm
with time complexity $2^{O(k^2 \log k)} (n+m)^{O(1)}$, where $k$ is an upper
bound on the number of cuts.
</p></div>
    </summary>
    <updated>2019-07-25T23:31:31Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1907.10121</id>
    <link href="http://arxiv.org/abs/1907.10121" rel="alternate" type="text/html"/>
    <title>SciPy 1.0--Fundamental Algorithms for Scientific Computing in Python</title>
    <feedworld_mtime>1564012800</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Pauli Virtanen, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gommers:Ralf.html">Ralf Gommers</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Oliphant:Travis_E=.html">Travis E. Oliphant</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haberland:Matt.html">Matt Haberland</a>, Tyler Reddy, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cournapeau:David.html">David Cournapeau</a>, Evgeni Burovski, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peterson:Pearu.html">Pearu Peterson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Weckesser:Warren.html">Warren Weckesser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bright:Jonathan.html">Jonathan Bright</a>, Stéfan J. van der Walt, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brett:Matthew.html">Matthew Brett</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wilson:Joshua.html">Joshua Wilson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Millman:K=_Jarrod.html">K. Jarrod Millman</a>, Nikolay Mayorov, Andrew R. J. Nelson, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jones:Eric.html">Eric Jones</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kern:Robert.html">Robert Kern</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Larson:Eric.html">Eric Larson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Carey:CJ.html">CJ Carey</a>, İlhan Polat, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Feng:Yu.html">Yu Feng</a>, Eric W. Moore, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/VanderPlas:Jake.html">Jake VanderPlas</a>, Denis Laxalde, Josef Perktold, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cimrman:Robert.html">Robert Cimrman</a>, Ian Henriksen, E. A. Quintero, Charles R Harris, Anne M. Archibald, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ribeiro:Ant=ocirc=nio_H=.html">Antônio H. Ribeiro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pedregosa:Fabian.html">Fabian Pedregosa</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mulbregt:Paul_van.html">Paul van Mulbregt</a>, SciPy 1.0 Contributors <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1907.10121">PDF</a><br/><b>Abstract: </b>SciPy is an open source scientific computing library for the Python
programming language. SciPy 1.0 was released in late 2017, about 16 years after
the original version 0.1 release. SciPy has become a de facto standard for
leveraging scientific algorithms in the Python programming language, with more
than 600 unique code contributors, thousands of dependent packages, over
100,000 dependent repositories, and millions of downloads per year. This
includes usage of SciPy in almost half of all machine learning projects on
GitHub, and usage by high profile projects including LIGO gravitational wave
analysis and creation of the first-ever image of a black hole (M87). The
library includes functionality spanning clustering, Fourier transforms,
integration, interpolation, file I/O, linear algebra, image processing,
orthogonal distance regression, minimization algorithms, signal processing,
sparse matrix handling, computational geometry, and statistics. In this work,
we provide an overview of the capabilities and development practices of the
SciPy library and highlight some recent technical developments.
</p></div>
    </summary>
    <updated>2019-07-25T23:25:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-07-25T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/097</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/097" rel="alternate" type="text/html"/>
    <title>TR19-097 |  Reversible Pebble Games and the Relation Between Tree-Like and General Resolution Space | 

	Florian Wörz, 

	Jacobo Toran</title>
    <summary>We show a new connection between the space measure in tree-like resolution and the reversible pebble game in graphs. Using this connection we provide several formula classes for which there is a logarithmic factor separation between the space complexity measure in tree-like and general resolution. We show that these separations are almost optimal by proving upper bounds for tree-like resolution space in terms of general resolution clause and variable space. In particular we show that for any formula F, its tree-like resolution is upper bounded by space(?)log time(?) where ? is any general resolution refutation of F. This holds considering as space(?) the clause space of the refutation as well as considering its variable space. For the concrete case of Tseitin formulas we are able to improve this bound to the optimal bound space(?)log(n), where n is the number of vertices of the corresponding graph.</summary>
    <updated>2019-07-24T12:09:27Z</updated>
    <published>2019-07-24T12:09:27Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-07-26T07:20:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/096</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/096" rel="alternate" type="text/html"/>
    <title>TR19-096 |  On the $\text{AC}^0[\oplus]$ complexity of Andreev&amp;#39;s Problem | 

	Aditya Potukuchi</title>
    <summary>Andreev's Problem asks the following: Given an integer $d$ and a subset of $S \subseteq \mathbb{F}_q \times \mathbb{F}_q$, is there a polynomial $y = p(x)$ of degree at most $d$ such that  for every $a \in \mathbb{F}_q$, $(a,p(a)) \in S$? We show an $\text{AC}^0[\oplus]$ lower bound for this problem. 

This problem appears to be similar to the list recovery problem for degree $d$-Reed-Solomon codes over $\mathbb{F}_q$ which asks the following: Given subsets $A_1,\ldots,A_q$ of $\mathbb{F}_q$, output all (if any) Reed-Solomon codewords contained in $A_1\times \cdots \times A_q$. For our purpose, we study this problem when $A_1, \ldots, A_q$ are random subsets of a given size, which may be of independent interest.</summary>
    <updated>2019-07-23T17:29:25Z</updated>
    <published>2019-07-23T17:29:25Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-07-26T07:20:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=17642</id>
    <link href="https://gilkalai.wordpress.com/2019/07/23/matan-harel-frank-mousset-and-wojciech-samotij-and-the-the-infamous-upper-tail-problem/" rel="alternate" type="text/html"/>
    <title>Matan Harel, Frank Mousset, and Wojciech Samotij and the “the infamous upper tail” problem</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Let me report today on a major breakthrough in random graph theory and probabilistic combinatorics. Congratulations to Matan, Frank, and Vojtek! Artist: Heidi Buck. “Catch a Dragon by the Tail 2” ( source ) Upper tails via high moments and entropic … <a href="https://gilkalai.wordpress.com/2019/07/23/matan-harel-frank-mousset-and-wojciech-samotij-and-the-the-infamous-upper-tail-problem/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Let me report today on a major breakthrough in random graph theory and probabilistic combinatorics. Congratulations to Matan, Frank, and Vojtek!</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/07/dragon_tail_2_web.png"><img alt="" class="alignnone size-full wp-image-17644" height="496" src="https://gilkalai.files.wordpress.com/2019/07/dragon_tail_2_web.png?w=640&amp;h=496" width="640"/></a></p>
<p>Artist: Heidi Buck.<strong><span style="color: #ff0000;"> “Catch a Dragon by the Tail 2”</span></strong> ( <a href="https://www.hbdragon.com/main/content/catch-dragon-tail-2">source</a> )</p>
<p class="title mathjax"><a href="https://arxiv.org/abs/1904.08212">Upper tails via high moments and entropic stability</a> by Matan Harel, Frank Mousset, and Wojciech Samotij</p>
<p><strong>Abstract:</strong></p>
<p>Suppose that <em>X</em> is a bounded-degree polynomial with nonnegative coefficients on the <em>p</em>-biased discrete hypercube. Our main result gives sharp estimates on the logarithmic upper tail probability of X whenever an associated extremal problem satisfies a certain entropic stability property. We apply this result to solve two long-standing open problems in probabilistic combinatorics: the upper tail problem for the number of arithmetic progressions of a fixed length in the <em>p</em>-random subset of the integers and the upper tail problem for the number of cliques of a fixed size in the random graph <img alt="G_{n,p}" class="latex" src="https://s0.wp.com/latex.php?latex=G_%7Bn%2Cp%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G_{n,p}"/>. We also make significant progress on the upper tail problem for the number of copies of a fixed regular graph <em>H</em> in <img alt="G_{n,p}." class="latex" src="https://s0.wp.com/latex.php?latex=G_%7Bn%2Cp%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G_{n,p}."/> To accommodate readers who are interested in learning the basic method, we include a short, self-contained solution to the upper tail problem for the number of triangles in <img alt="G_{n,p}" class="latex" src="https://s0.wp.com/latex.php?latex=G_%7Bn%2Cp%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="G_{n,p}"/> for all <em>p=p(n)</em> satisfying <img alt="\frac {\log n}{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac+%7B%5Clog+n%7D%7Bn%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\frac {\log n}{n}"/> <img alt="\ll" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cll&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\ll"/> <img alt="p \ll 1" class="latex" src="https://s0.wp.com/latex.php?latex=p+%5Cll+1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p \ll 1"/>.</p>
<p>The introduction gives very nicely the rich history of the problem.  Here is a 2002 paper by Svante Janson and Andrzej Ruciński on <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1002/rsa.10031?casa_token=7FoHUVq0w5IAAAAA:AqmK_4tpjDzXErWnGh4qnE2kn4NTupyoW27eSXW7Uwr5l0JgpBP5SG7PaVnJ6Lvr4JhCaSIH2HPv-QTsUg">the infamous upper tail</a>.  (And a lot have happened since then on this problem and on non linear large deviation theory). Following, there is a lovely section with a short solution for the case of triangles.</p>
<p>Forthcoming reference [48] talks about lower tails! Stay tuned!</p></div>
    </content>
    <updated>2019-07-23T06:51:02Z</updated>
    <published>2019-07-23T06:51:02Z</published>
    <category term="Combinatorics"/>
    <category term="Probability"/>
    <category term="Frank Mousset"/>
    <category term="Matan Harel"/>
    <category term="Wojciech Samotij"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-07-26T07:20:42Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-7828284719883166611</id>
    <link href="https://blog.computationalcomplexity.org/feeds/7828284719883166611/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/07/answer-to-both-infinite-hats-problems.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7828284719883166611" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7828284719883166611" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/07/answer-to-both-infinite-hats-problems.html" rel="alternate" type="text/html"/>
    <title>Answer to both Infinite Hats Problems from the last post</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><br/>
(This is a joint post with David Marcus. You'll see why later.)<br/>
<br/>
In a prior  I posed two infinite hat problems. Today I post the solutions. Actually this is a copy of my last post with the solutions added, so it is self contained.<br/>
<br/>
A Hat Problem that you have probably seen:<br/>
<br/>
1) There are an infinite number of people, numbered 1,2,3,...  There are 2 colors of hats. They can all see everyone's hat but their own. <br/>
<br/>
2) The adversary is going to put hats on all the people. They will guess their own hat color<i> at the same time</i>. <br/>
<br/>
3) The people can discuss strategy ahead of time, but must use a deterministic strategy and the adversary knows the strategy.<br/>
<br/>
4) The people want to minimize how many they get wrong. <br/>
<br/>
5) The adversary puts on hats to maximize how many they get wrong.<br/>
<br/>
I ask two questions (the answers are in a document I point to) and one meta-question:<br/>
<br/>
Q1: Is there a solution where they get all but a finite number of the guesses right? (If you have read my prior post on hat puzzles, <a href="https://blog.computationalcomplexity.org/2017/07/two-hat-problems-you-may-or-may-not.html">here</a> then you can do this one.) <br/>
<br/>
Q2: Is there a solution where they get all but at most (say) 18 wrong.<br/>
<br/>
<br/>
Answers to Q1 and Q2 are <a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/infinitehats.pdf">here</a>.<br/>
<br/>
How did I get into this problem? I was looking at hat problems a while back. Then  I began discussing Q1 and Q2 by email  (Does the term <i>discussing</i> have as a default that it is by email?) with David Marcus who had just read the chapter of <a href="https://www.amazon.com/Problems-Point-Exploring-Computer-Science/dp/9813279974">Problems with a Point</a> on hat puzzles. After a few emails back and fourth, he began looking on the web for answers. He found one. There is a website of hat puzzles! It was MY website papers on  Hat Puzzles! It is  <a href="https://www.cs.umd.edu/users/gasarch/TOPICS/hats/hats.html">here</a>. And on it was a relevant paper <a href="https://www.cs.umd.edu/users/gasarch/TOPICS/hats/infinite-hats-and-ac.pdf">here</a>. We did not find any other source of the problem or its solution. <br/>
<br/>
Q3: How well known is problem Q2 and the solution?  I've seen Q1 around but the only source on Q2 that I know of is that paper, and now this blog post. So, please leave a comment telling me if you have seen Q2 and/or the solution someplace else, and if so where.<br/>
<br/>
The responses to my last post indicated that YES the problem was out there, but the proof that you could not get all-but-18 was not well known. <br/>
<br/>
I THINK that all of the proofs that you can't do all-but-18 in the comment of the last post were essentially the same as the solution I pointed to in this blog. I would be interested if there is an alternative proof. <br/></div>
    </content>
    <updated>2019-07-22T03:00:00Z</updated>
    <published>2019-07-22T03:00:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-07-26T05:44:48Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/095</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/095" rel="alternate" type="text/html"/>
    <title>TR19-095 |  Unambiguous Catalytic Computation | 

	Chetan Gupta, 

	Rahul Jain, 

	Vimal Raj Sharma, 

	Raghunath Tewari</title>
    <summary>The catalytic Turing machine is a model of computation defined by Buhrman, Cleve,
Kouck, Loff, and Speelman (STOC 2014). Compared to the classical space-bounded Turing
machine, this model has an extra space which is filled with arbitrary content in addition
to the clean space. In such a model we study if this additional filled space can be used to
increase the power of computation or not, with the condition that the initial content of this
extra filled space must be restored at the end of the computation.
In this paper, we define the notion of unambiguous catalytic Turing machine and prove
that under a standard derandomization assumption, the class of problems solved by an
unambiguous catalytic Turing machine is same as the class of problems solved by a general
nondeterministic catalytic Turing machine in the logspace setting.</summary>
    <updated>2019-07-21T11:21:02Z</updated>
    <published>2019-07-21T11:21:02Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-07-26T07:20:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=17574</id>
    <link href="https://gilkalai.wordpress.com/2019/07/20/isabella-novik-and-hailun-zheng-neighborly-centrally-symmetric-spheres-exist-in-all-dimensions/" rel="alternate" type="text/html"/>
    <title>Isabella Novik and Hailun Zheng: Neighborly centrally symmetric spheres exist in all dimensions!</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">A twit long summary: The cyclic polytope is wonderful and whenever we construct an analogous object we are happy. Examples: Neighborly cubic polytopes; The amplituhedron; and as of last week, the Novik-Zheng new construction of neighborly centrally symmetric spheres! At … <a href="https://gilkalai.wordpress.com/2019/07/20/isabella-novik-and-hailun-zheng-neighborly-centrally-symmetric-spheres-exist-in-all-dimensions/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><span style="color: #0000ff;">A twit long summary: The cyclic polytope is wonderful and whenever we construct an analogous object we are happy. Examples: Neighborly cubic polytopes; The amplituhedron; and as of last week, the Novik-Zheng new construction of neighborly centrally symmetric spheres!</span></p>
<h2>At last: Neighborly CS spheres!</h2>
<p>The news: Isabella Novik and Hailun Zheng’ paper  <a href="https://arxiv.org/abs/1907.06115">Highly neighborly centrally symmetric spheres</a>, resolves an old standing problem in this field.</p>
<p>Here is the abstract:</p>
<blockquote><p>In 1995, Jockusch constructed an infinite family of centrally symmetric 3-dimensional simplicial spheres that are cs-<em>2</em>-neighborly. Here we generalize his construction and show that for all d ≥ 4 and n ≥ d, there exists a centrally symmetric (d − 1)-dimensional simplicial sphere with <em>2n</em> vertices that is cs-[d/2]-neighborly. This result combined with work of Adin and Stanley completely resolves the upper bound problem for centrally symmetric simplicial spheres.</p></blockquote>
<p>Congratulations to Isabella and Hailun!</p>
<h2>Some background to the Novik and Zheng breakthrough</h2>
<p><strong>Centrally symmetric bodies:</strong> A centrally symmetric (cs) polytope convex body in <img alt="\mathbb R^d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb R^d"/> satisfies <img alt="x \in P" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="x \in P"/> implies <img alt="-x \in P" class="latex" src="https://s0.wp.com/latex.php?latex=-x+%5Cin+P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="-x \in P"/>. Centrally symmetric bodies are the unit balls of normed spaces.</p>
<p><strong>Centrally symmetric simplicial spheres:</strong> A triangulation <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K"/> of a  <img alt="(d-1)" class="latex" src="https://s0.wp.com/latex.php?latex=%28d-1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(d-1)"/>-dimensional sphere with a set <img alt="V" class="latex" src="https://s0.wp.com/latex.php?latex=V&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="V"/> of vertices is centrally symmetric if there is an involution <img alt="\phi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi"/> on <img alt="V" class="latex" src="https://s0.wp.com/latex.php?latex=V&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="V"/> that <img alt="\phi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi"/> maps a face of <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K"/> to a face of <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K"/> and for every vertex <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/>, <img alt="\phi(v) \ne v" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi%28v%29+%5Cne+v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi(v) \ne v"/> and <img alt="\{v , \phi (v)\}" class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7Bv+%2C+%5Cphi+%28v%29%5C%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\{v , \phi (v)\}"/> is not an edge of <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K"/>.  The boundary complex of a cs <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>-polytopes is a cs triangulation of <img alt="S^{d-1}" class="latex" src="https://s0.wp.com/latex.php?latex=S%5E%7Bd-1%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="S^{d-1}"/>.</p>
<p><strong>Neighborliness.</strong> A simplicial complex <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K"/>  is <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/>-neighborly of every set of <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/> vertices of <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/> form a face.  (The definition was first considered  for simplicial <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>-polytopes <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="P"/>.) The cyclic <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>-polytope with <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="n"/> vertices is <img alt="[d/2]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bd%2F2%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="[d/2]"/>-neighborly. (The only (<img alt="[d/2]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bd%2F2%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="[d/2]"/>+1)-neighborly simplicial <img alt="(d-1)" class="latex" src="https://s0.wp.com/latex.php?latex=%28d-1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(d-1)"/>-sphere is the simplex. There are many other <img alt="[d/2]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bd%2F2%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="[d/2]"/>-neighborly simplicial <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="d"/>-polytopes and <img alt="(d-1)" class="latex" src="https://s0.wp.com/latex.php?latex=%28d-1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(d-1)"/>-spheres.</p>
<p><strong>cs-Neighborliness. </strong>Let <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K"/> be a simplicial complex with an involution <img alt="\phi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi"/> on its vertices which acts on <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K"/> (maps faces to faces) and has the property that <img alt="\phi(v)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi%28v%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi(v)"/> is not adjacent to <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/> (and <img alt="\phi (v) \ne v" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi+%28v%29+%5Cne+v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi (v) \ne v"/>). We will call <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="v"/> and <img alt="\phi (v)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi+%28v%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\phi (v)"/> <strong>antipodal</strong>. <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K"/> is cs-<img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/>-neighborly if every set of <img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/> vertices that contains no pair of antipodal vertices is a face of <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="K"/>. The only cs-<img alt="m" class="latex" src="https://s0.wp.com/latex.php?latex=m&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="m"/>-neighborly simplicial sphere is the boundary complex of the cross polytope.</p>
<p><strong>The existence of cs-<img alt="[d/2]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bd%2F2%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="[d/2]"/>-neighborly spheres. </strong>It was an important open question to understand if cs <img alt="[d/2]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bd%2F2%5D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="[d/2]"/>-neighborly simplicial spheres exist. (The only cs-<img alt="([d/2]+1)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Bd%2F2%5D%2B1%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="([d/2]+1)"/>-neighborly spheres is the boundary complex of the cross polytope.)  The first example (whoch is not a cross polytope) was given by Grunbaum in 1969 in his paper “The importance of being straight(?).” In 1995  Jockusch constructed an infinite family cs-2-neighborly centrally symmetric 3-dimensional simplicial spheres. This problem has now been solved by Novik and Zheng.</p>
<p><strong>Neighborly centrally symmetric polytopes.  </strong>In the 1960s Grünbaum noted the big difference between neighborly centrally symmetric spheres and centrally symmetric polytopes. He proved (This is Theorem 4.1 in his book “Convex polytopes”) that no cs-2-neighborly 4 polytope with 12 vertices exists.  This is an example of the important themes of “streightening” or “linearizing” combimatorial objects and  of extending theorems from the “streight” or “linear” case to more general combinatorial settings.)</p>
<p>This result by Grünbaum was extended in various directions. Let me mention two major results in the field:</p>
<p><strong>Theorem</strong> McMullen and Shephard (1968): A cs <em>d</em>-dimensional polytope with <em>2(d + 2)</em> vertices cannot be more than cs-<em>⌊(d + 1)/3⌋</em>-neighborly.</p>
<p><strong>Theorem</strong> <a href="https://arxiv.org/abs/math/0507280">Linial and Novik (2006):</a> A cs-2-neighborly <em>d</em>-dimensional polytope has at most  <img alt="2^d" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Ed&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^d"/>;</p>
<p>Novik (2017) <a href="https://arxiv.org/abs/1712.09489">constructed</a>  cs-2-neighborly d polytopes with <img alt="2^{d-1}+1" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7Bd-1%7D%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^{d-1}+1"/> vertices. She used a 2017 <a href="https://arxiv.org/abs/1709.03411">breakthrough construction</a> by Gerencsér–Harang of an acute set of size <img alt="2^{d-1}+1" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7Bd-1%7D%2B1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="2^{d-1}+1"/> in <img alt="\mathbb R^d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\mathbb R^d"/>. (A set <em>S</em> is <em>acute</em> if every three points from <em>S </em>determine an acute triangle.)</p>
<p><strong>Face numbers of centrally-symmetric polytopes and spheres. </strong>As the abstract asserts the new construction is related to questions about face numbers of centrally symmetric polytopes, spheres and other cellular objects. In fact, this was the next item in our planned posts on algebraic combinatorics of cellular objects. (The first and only post so far<a href="https://gilkalai.wordpress.com/2018/06/20/beyond-the-g-conjecture-algebraic-combinatorics-of-cellular-spaces-i/"> is here</a>.) Here is a recent survey by Isabella Novik  <a href="https://arxiv.org/abs/1711.09310">A tale on centrally symmetric  polytopes and spheres</a>.</p>
<p><strong>Upper bound theorems.</strong> Neighborly polytopes and spheres are the equality cases of the upper bound theorem (proved by McMullen for polytopes and by Stanley for spheres). A version of the upper bound inequality for centrally symmetric spheres was proved by Adin and Stanley and the new construction shows that the Adin-Stanley inequality is tight. For more on the upper bound theorem and neighborliness see Section 2 of my 2000 survey  <a href="http://www.ma.huji.ac.il/~kalai/VIS.pdf">Combinatorics with geometric flavor. </a> See also the post <a href="https://gilkalai.wordpress.com/2009/04/04/how-the-g-conjecture-came-about/">How the g-conjecture came about</a> and  and the post <a href="https://gilkalai.wordpress.com/2013/09/10/how-the-proof-of-the-upper-bound-theorem-for-spheres-was-found/" rel="bookmark">Richard Stanley: How the Proof of the Upper Bound Theorem (for spheres) was Found.</a></p>
<p> </p>
<p> </p></div>
    </content>
    <updated>2019-07-20T18:33:03Z</updated>
    <published>2019-07-20T18:33:03Z</published>
    <category term="Combinatorics"/>
    <category term="Convexity"/>
    <category term="Hailun Zheng"/>
    <category term="Isabella Novik"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-07-26T07:20:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4267</id>
    <link href="https://www.scottaaronson.com/blog/?p=4267" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4267#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4267" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Fake it till you make it (to the moon)</title>
    <summary xml:lang="en-US">While I wait to board a flight at my favorite location on earth—Philadelphia International Airport—I figured I might as well blog something to mark the 50th anniversary of Apollo 11. (Thanks also to Joshua Zelinsky for a Facebook post that inspired this.) I wasn’t alive for Apollo, but I’ve been alive for 3/4 of the […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>While I wait to board a flight at my favorite location on earth—Philadelphia International Airport—I figured I might as well blog something to mark the 50<sup>th</sup> anniversary of Apollo 11.  (Thanks also to Joshua Zelinsky for a Facebook post that inspired this.)</p>



<p>I wasn’t alive for Apollo, but I’ve been alive for 3/4 of the time <em>after</em> it, even though it now seems like ancient history—specifically, like a Roman cathedral being gawked at by a medieval peasant, like an achievement by some vanished, more cohesive civilization that we can’t even replicate today, let alone surpass.</p>



<p>Which brings me to a depressing mystery: why do so many people now deny that humans walked on the moon at all?  Like, why <em>that</em> specifically?  While they’re at it, why don’t they also deny that WWII happened, or that the Beatles existed?</p>



<p>Surprisingly, skepticism of the reality of Apollo seems to have gone all the way back to the landings themselves.  One of my favorite stories growing up was of my mom, as a teenager, working as a waitress at an Israeli restaurant in Philadelphia, on the night of Apollo 11 landing.  My mom asked for a few minutes off to listen to news of the landing on the radio.  The owners wouldn’t grant it—explaining that it was all Hollywood anyway, just some actors in spacesuits on a sound stage, and obviously my mom wasn’t so naïve as to think anyone was <em>actually</em> walking to the moon?</p>



<p>Alas, as we get further and further from the event, with no serious prospect of ever replicating it past the stage of announcing an optimistic timetable (nor, to be honest, any scientific <em>reason</em> to replicate it), as the people involved die off, and as our civilization becomes ever more awash in social-media-fueled paranoid conspiracies, I fear that moon-landing denalism will become more common.</p>



<p>Because here’s the thing: Apollo could happen, but <em>only</em> because of a wildly improbable, once-in-history confluence of social and geopolitical factors.  It was economically insane, taking 100,000 people and 4% of the US federal budget for some photo-ops, a flag-planting, some data and returned moon rocks that had genuine scientific value but could’ve been provided much more cheaply by robots.  It was dismantled immediately afterwards like a used movie set, rather than leading to any greater successes. Indeed, manned spaceflight severely <em>regressed</em> afterwards, surely mocking the expectations of every last science fiction fan and techno-utopian who was alive at that time.</p>



<p>One could summarize the situation by saying that, in certain respects, the Apollo program really <strong>was</strong> “faked.”  It’s just that the way they “faked” it, involved actually landing people on the moon!</p></div>
    </content>
    <updated>2019-07-19T21:40:43Z</updated>
    <published>2019-07-19T21:40:43Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Nerd Interest"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Procrastination"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2019-07-19T23:00:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://blogs.princeton.edu/imabandit/?p=1397</id>
    <link href="https://blogs.princeton.edu/imabandit/2019/07/17/guest-post-by-julien-mairal-a-kernel-point-of-view-on-convolutional-neural-networks-part-ii/" rel="alternate" type="text/html"/>
    <title>Guest post by Julien Mairal: A Kernel Point of View on Convolutional Neural Networks, part II</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This is a continuation of Julien Mairal‘s guest post on CNNs, see part I here. Stability to deformations of convolutional neural networks In their ICML paper Zhang et al. introduce a functional space for CNNs with one layer, by noticing … <a href="https://blogs.princeton.edu/imabandit/2019/07/17/guest-post-by-julien-mairal-a-kernel-point-of-view-on-convolutional-neural-networks-part-ii/">Continue reading <span class="meta-nav">→</span></a></p></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a class="liimagelink" href="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/uploads/sites/122/2019/07/Mairal2.jpg?ssl=1"><img alt="" class="alignnone wp-image-1399" height="324" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/uploads/sites/122/2019/07/Mairal2.jpg?resize=639%2C324&amp;ssl=1" width="639"/></a></p>
<p>This is a continuation of <a class="liinternal" href="https://lear.inrialpes.fr/people/mairal/">Julien Mairal</a>‘s guest post on CNNs, see <a class="liinternal" href="https://blogs.princeton.edu/imabandit/2019/07/10/guest-post-by-julien-mairal-a-kernel-point-of-view-on-convolutional-neural-networks-part-i/">part I here.</a></p>
<p><strong>Stability to deformations of convolutional neural networks</strong></p>
<p>In their <a class="lipdf" href="http://proceedings.mlr.press/v70/zhang17f/zhang17f.pdf">ICML paper</a> Zhang et al. introduce a functional space for CNNs with one layer, by noticing that for some dot-product kernels, smoothed variants of rectified linear unit activation functions (ReLU) live in the corresponding RKHS, see also <a class="lipdf" href="http://proceedings.mlr.press/v48/zhangd16.pdf">this paper</a> and <a class="lipdf" href="https://www.cs.cornell.edu/~sridharan/sicomp.pdf">that one</a>. By following a similar reasoning with multiple layers, it is then possible to show that the functional space described in <a class="liinternal" href="https://blogs.princeton.edu/imabandit/2019/07/10/guest-post-by-julien-mairal-a-kernel-point-of-view-on-convolutional-neural-networks-part-i/">part I</a> <img alt="\{ f_w: x \mapsto \langle w , \Phi_n(x_0) \rangle; w \in L^2(\Omega,\mathcal{H}_n) \}" class="ql-img-inline-formula " height="20" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3e321e5f0406c9879f25b6b1d69a5fc3_l3.png?resize=298%2C20&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="298"/> contains CNNs with such smoothed ReLU, and that the norm <img alt="\|f_w\|" class="ql-img-inline-formula " height="19" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-62e1a48032624994ba16c4e26421676e_l3.png?resize=34%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="34"/> of such networks can be controlled by the spectral norms of filter matrices. This is consistent with previous measures of complexity for CNNs, see <a class="lipdf" href="https://papers.nips.cc/paper/7204-spectrally-normalized-margin-bounds-for-neural-networks.pdf">this paper</a> by Bartlett et al.</p>
<p>A perhaps more interesting finding is that the abstract representation <img alt="\Phi_n(x)" class="ql-img-inline-formula " height="18" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3ca9a5351b772e88bebe85e7e4a13632_l3.png?resize=45%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="45"/>, which only depends on the network architecture, may provide near-translation invariance and stability to small image deformations while preserving information—that is, <img alt="x" class="ql-img-inline-formula " height="8" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-1b9fbfb207b6d17d74b33c6d8342a1a4_l3.png?resize=10%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="10"/> can be recovered from <img alt="\Phi_n(x)" class="ql-img-inline-formula " height="18" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3ca9a5351b772e88bebe85e7e4a13632_l3.png?resize=45%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="45"/>. The original characterization we use was introduced by Mallat in <a class="lipdf" href="https://www.di.ens.fr/~mallat/papiers/ScatCPAM.pdf">his paper</a> on the scattering transform—a multilayer architecture akin to CNNs based on wavelets, and was extended to <img alt="\Phi_n" class="ql-img-inline-formula " height="16" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-273242b8e92b3a9f4dc13c62b2785bd3_l3.png?resize=21%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="21"/> by Alberto Bietti, who should be credited for all the hard work here.</p>
<p>Our goal is to understand under which conditions it is possible to obtain a representation that (i) is near-translation invariant, (ii) is stable to deformations, (iii) preserves signal information. Given a <img alt="C^1" class="ql-img-inline-formula " height="15" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-2e9ea203bbd77c5cd8bee967e2729d8b_l3.png?resize=20%2C15&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="20"/>-diffeomorphism <img alt="\tau: \mathbb{R}^2 \to \mathbb{R}^2" class="ql-img-inline-formula " height="16" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c6f8f1dde2ee4682653c2a6b37d8a42d_l3.png?resize=93%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="93"/> and denoting by <img alt="L_\tau x(u) = x(u-\tau(u))" class="ql-img-inline-formula " height="18" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-835d9f864f712213ee317332b3f3675a_l3.png?resize=167%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="167"/> its action operator (for an image defined on the continuous domain <img alt="\mathbb{R}^2" class="ql-img-inline-formula " height="15" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d5abe0f29e8cc710ae26f4f0af5a0859_l3.png?resize=20%2C15&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="20"/>), the main stability bound we obtain is the following one, see Theorem 7 in <a class="lipdf" href="https://www.di.ens.fr/~mallat/papiers/ScatCPAM.pdf">Mallat’s paper</a> if <img alt="\|\nabla \tau\|_\infty \leq 1/2" class="ql-img-inline-formula " height="19" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-758b3cac273166048ed1879acf427860_l3.png?resize=104%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="104"/>, for all <img alt="x" class="ql-img-inline-formula " height="8" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-1b9fbfb207b6d17d74b33c6d8342a1a4_l3.png?resize=10%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="10"/>,</p>
<p class="ql-center-displayed-equation" style="line-height: 43px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[ \| \Phi_n(L_\tau x) - \Phi_n(x)\| \leq \left ( C_1 (1+n) \|\nabla \tau\|_\infty + \frac{C_2}{\sigma_n} \|\tau\|_\infty \right) \|x\|, \]" class="ql-img-displayed-equation " height="43" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-51041d0067a72066938e31b1f00529fa_l3.png?resize=455%2C43&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="455"/></p>
<p>where <img alt="C_1, C_2" class="ql-img-inline-formula " height="16" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-08d1f29fa9c0981e916619b6c6bc7eee_l3.png?resize=48%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="48"/> are universal constants, <img alt="\sigma_n" class="ql-img-inline-formula " height="11" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c0142846a2999e170f7beec7be1523f2_l3.png?resize=18%2C11&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="18"/> is the scale parameter of the pooling operator <img alt="A_n" class="ql-img-inline-formula " height="15" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-e1872c7d7a65e0dc92f8a4a04608b88a_l3.png?resize=21%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="21"/> corresponding to the “amount of pooling” performed up to the last layer, <img alt="\|\tau\|_\infty" class="ql-img-inline-formula " height="19" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c1a5effb150d36de3c7074eaa980c357_l3.png?resize=39%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="39"/> is the maximum pixel displacement and <img alt="\|\nabla \tau\|_\infty" class="ql-img-inline-formula " height="19" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-ab4c5d3fe8fd25af25beb4f58a55c938_l3.png?resize=53%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="53"/> represents the maximum amount of deformation, see <a class="lipdf" href="https://www.di.ens.fr/~mallat/papiers/ScatCPAM.pdf">the paper</a> for the precise definitions of all these quantities. Note that when <img alt="C_2/\sigma_n \to 0" class="ql-img-inline-formula " height="18" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b732bf857c5f04c7d10dda247f1a5022_l3.png?resize=85%2C18&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="85"/>, the representation <img alt="\Phi_n" class="ql-img-inline-formula " height="16" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-273242b8e92b3a9f4dc13c62b2785bd3_l3.png?resize=21%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="21"/> becomes translation invariant: indeed, consider the particular case of <img alt="\tau" class="ql-img-inline-formula " height="8" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3af6c51247895b176bb502f0ee0857ee_l3.png?resize=10%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="10"/> being a translation, then <img alt="\nabla \tau=0" class="ql-img-inline-formula " height="14" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-aa1278a7149925a4f299de0dbb85cec0_l3.png?resize=57%2C14&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="57"/> and <img alt="\|\Phi_n(L_\tau x) - \Phi_n(x)\| \to 0" class="ql-img-inline-formula " height="19" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-cd1d650abd9970e357384c0653960577_l3.png?resize=186%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="186"/>.</p>
<p>The stability bound and a few additional results tell us a few things about the network architecture: (a) small patches lead to more stable representations (the dependency is hidden in <img alt="C_1" class="ql-img-inline-formula " height="16" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-782c65cbd411fb8862688afc92bc1eea_l3.png?resize=19%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="19"/>); (b) signal preservation for discrete signals requires small subsampling factors (and thus small pooling) between layers. In such a setting, the scale parameter <img alt="\sigma_n" class="ql-img-inline-formula " height="11" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c0142846a2999e170f7beec7be1523f2_l3.png?resize=18%2C11&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="18"/> still grows exponentially with <img alt="n" class="ql-img-inline-formula " height="8" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-a63eb5ff0272d3119fa684be6e7acce8_l3.png?resize=11%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="11"/> and near translation invariance may be achieved with several layers.</p>
<p>Interestingly, we may now come back to the Cauchy-Schwarz inequality from part 1, and note that if <img alt="\Phi_n" class="ql-img-inline-formula " height="16" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-273242b8e92b3a9f4dc13c62b2785bd3_l3.png?resize=21%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="21"/> is stable, the RKHS norm <img alt="\|f\|" class="ql-img-inline-formula " height="19" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-afe70184469e7e3a14405a7193eedf29_l3.png?resize=24%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="24"/> is then a natural quantity that provides stability to deformations to the prediction function <img alt="f" class="ql-img-inline-formula " height="16" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-c7d97b919a3b73617cf2fbb375fff3b1_l3.png?resize=10%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="10"/>, in addition to measuring model complexity in a traditional sense.</p>
<p><strong>Feature learning in RKHSs and convolutional kernel networks</strong></p>
<p>The previous paragraph is devoted to the characterization of convolutional architectures such as CNNs but the previous kernel construction can in fact be used to derive more traditional kernel methods. After all, why should one spend efforts defining a kernel between images if not to use it?</p>
<p>This can be achieved by considering finite-dimensional approximations of the previous feature maps. In order to shorten the presentation, we simply describe the main idea based on the Nystrom approximation and refer to <a class="lipdf" href="http://papers.nips.cc/paper/6184-end-to-end-kernel-learning-with-supervised-convolutional-kernel-networks.pdf">the paper</a> for more details. Approximating the infinite-dimensional feature maps <img alt="x_k" class="ql-img-inline-formula " height="11" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3ad23c5c360c3f33031a5d000d37416f_l3.png?resize=17%2C11&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="17"/> (see the figure at the top of <a class="liinternal" href="https://blogs.princeton.edu/imabandit/2019/07/10/guest-post-by-julien-mairal-a-kernel-point-of-view-on-convolutional-neural-networks-part-i/">part I</a>) can be done by projecting each point in <img alt="\mathcal{H}_k" class="ql-img-inline-formula " height="15" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-bc26f0de4084a72b9e625a080bd5d674_l3.png?resize=22%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="22"/> onto a <img alt="p_k" class="ql-img-inline-formula " height="12" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5c72bc331dc0008f57d454e7071dc39e_l3.png?resize=17%2C12&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="17"/>-dimensional subspace <img alt="\mathcal{F}_k" class="ql-img-inline-formula " height="16" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b08b176c0bf0adbd9cbe41b31147e1f7_l3.png?resize=20%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="20"/> leading to a finite-dimensional feature map <img alt="\tilde{x}_k" class="ql-img-inline-formula " height="15" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5f55b75318f3b8da67917ee0b0e190ce_l3.png?resize=17%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="17"/> akin to CNNs, see the figure at the top of the post.</p>
<p>By parametrizing <img alt="\mathcal{F}_k=\text{span}(\varphi_k(z_1),\varphi_k(z_2),\ldots,\varphi_k(z_{p_k}))" class="ql-img-inline-formula " height="20" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5dd8802df8efddb9acc5056af47339d7_l3.png?resize=297%2C20&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="297"/> with <img alt="p_k" class="ql-img-inline-formula " height="12" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5c72bc331dc0008f57d454e7071dc39e_l3.png?resize=17%2C12&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="17"/> anchor points <img alt="Z=[z_1,\ldots,z_{p_k}]" class="ql-img-inline-formula " height="19" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-4614e1cdba47dc6a6db7957fb1d82632_l3.png?resize=123%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="123"/>, and using a dot-product kernel, a patch <img alt="z" class="ql-img-inline-formula " height="8" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-d9d772a59543419785ce66946592259a_l3.png?resize=9%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="9"/> from <img alt="\tilde{x}_{k-1}" class="ql-img-inline-formula " height="16" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-6419748397a324cd2a2ebc3f119b7f80_l3.png?resize=35%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="35"/> is encoded through the mapping function</p>
<p class="ql-center-displayed-equation" style="line-height: 43px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[ \psi_k(z) = \|z\| \kappa_k( Z^\top Z)^{-1/2} \kappa_k\left( Z^\top \frac{z}{\|z\|} \right), \]" class="ql-img-displayed-equation " height="43" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-cc43da382f024d96cb50e3dc3f051d6f_l3.png?resize=306%2C43&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="306"/></p>
<p>where <img alt="\kappa_k" class="ql-img-inline-formula " height="11" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-684fcf23472c51919624049fb4e0129a_l3.png?resize=17%2C11&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="17"/> is applied pointwise. Then, computing <img alt="\tilde{x}_k" class="ql-img-inline-formula " height="15" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-5f55b75318f3b8da67917ee0b0e190ce_l3.png?resize=17%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="17"/> from <img alt="\tilde{x}_{k-1}" class="ql-img-inline-formula " height="16" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-6419748397a324cd2a2ebc3f119b7f80_l3.png?resize=35%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="35"/> admits a CNN interpretation, where only the normalization and the matrix multiplication by <img alt="\kappa_k( Z^\top Z)^{-1/2}" class="ql-img-inline-formula " height="21" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-a74cffbbd51922298a13f864fbedaa98_l3.png?resize=103%2C21&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="103"/> are not standard operations. It remains now to choose the anchor points:</p>
<ul>
<li><strong>kernel approximation:</strong> a first approach consists of using a variant of the Nystrom method, see <a class="lipdf" href="https://papers.nips.cc/paper/1866-using-the-nystrom-method-to-speed-up-kernel-machines.pdf">this paper</a> and <a class="lipdf" href="http://home.cse.ust.hk/~twinsen/nystrom.pdf">that one</a>. When plugging the corresponding image representation in a linear classifier, the resulting approach behaves as a classical kernel machine. Empirically, we observe that the higher the number of anchor points, the better the kernel approximation, and the higher the accuracy. For instance, a two-layer network with a <img alt="300k" class="ql-img-inline-formula " height="13" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-81a4466abb5fecba81f8a3aa055a1a14_l3.png?resize=36%2C13&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="36"/>-dimensional representations achieves about <img alt="86\%" class="ql-img-inline-formula " height="15" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-0eea28372ada596bc618b4b94fee69ec_l3.png?resize=32%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="32"/> accuracy on CIFAR-10 without data augmentation (see <a class="liinternal" href="https://gitlab.inria.fr/mairal/ckn-cudnn-matlab">here</a>).</li>
<li><strong>back-propagation, feature selection</strong>: learning the anchor points <img alt="Z" class="ql-img-inline-formula " height="12" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-cc9f8fff9fd24060bc054e78f01d5bfb_l3.png?resize=12%2C12&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="12"/> can also be done as in a traditional CNN, by optimizing them end-to-end. This allows using deeper lower-dimensional architectures and empirically seems to perform better when enough data is available, e.g., <img alt="92\%" class="ql-img-inline-formula " height="15" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-659ab3cccda2422f955af880d20646cf_l3.png?resize=32%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="32"/> accuracy on CIFAR-10 with simple data augmentation. There, the subspaces <img alt="\mathcal{F}_k" class="ql-img-inline-formula " height="16" src="https://i1.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b08b176c0bf0adbd9cbe41b31147e1f7_l3.png?resize=20%2C16&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="20"/> are not learned anymore to provide the best kernel approximation, but the model seems to perform a sort of feature selection in each layer’s RKHS <img alt="\mathcal{H}_k" class="ql-img-inline-formula " height="15" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-bc26f0de4084a72b9e625a080bd5d674_l3.png?resize=22%2C15&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="22"/>, which is not well understood yet (This feature selection interpretation is due to my collaborator Laurent Jacob).</li>
</ul>
<p>Note that the first CKN model published <a class="lipdf" href="https://papers.nips.cc/paper/5348-convolutional-kernel-networks.pdf">here</a> was based on a different approximation principle, which was not compatible with end-to-end training. We found this to be less scalable and effective.</p>
<p><strong>Other links between neural networks and kernel methods</strong></p>
<p>Finally, other links between kernels and infinitely-wide neural networks with random weights are classical, but they were not the topic of this blog post (they should be the topic of another one!). In a nutshell, for a large collection of weights distributions and nonlinear functions <img alt="s: \mathbb{R} \to \mathbb{R}" class="ql-img-inline-formula " height="13" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-b4680e3f9e8274687d2d04f0a262ed00_l3.png?resize=76%2C13&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="76"/>, the following quantity admits an analytical form</p>
<p class="ql-center-displayed-equation" style="line-height: 22px;"><span class="ql-right-eqno">   </span><span class="ql-left-eqno">   </span><img alt="\[ K(x,x') = \E_{w}[ s(w^\top x) s(w^\top x')], \]" class="ql-img-displayed-equation " height="22" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-9144f2d0b847adb69db90629ed805148_l3.png?resize=229%2C22&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="229"/></p>
<p>where the terms <img alt="s(w^\top x)" class="ql-img-inline-formula " height="19" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-da82e444bbc3a5e594b7edbf0b1ba3a0_l3.png?resize=56%2C19&amp;ssl=1" title="Rendered by QuickLaTeX.com" width="56"/> may be seen as an infinitely-wide single-layer neural network. The first time such a relation appears is likely to be in <a class="liexternal" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&amp;rep=rep1&amp;type=pdf">the PhD thesis</a> of Radford Neal with a Gaussian process interpretation, and it was revisited later by <a class="lipdf" href="http://proceedings.mlr.press/v2/leroux07a/leroux07a.pdf">Le Roux and Bengio</a> and by <a class="lipdf" href="http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf">Cho and Saul</a> with multilayer models.</p>
<p>In particular, when <img alt="s" class="ql-img-inline-formula " height="8" src="https://i2.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-3bcfb3f0b6b04be3b598743cd774dd78_l3.png?resize=8%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="8"/> is the rectified linear unit and <img alt="w" class="ql-img-inline-formula " height="8" src="https://i0.wp.com/blogs.princeton.edu/imabandit/wp-content/ql-cache/quicklatex.com-78d46af3f19bae0d88ac0cabd450a296_l3.png?resize=13%2C8&amp;ssl=1" style="vertical-align: 0px;" title="Rendered by QuickLaTeX.com" width="13"/> follows a Gaussian distribution, it is known that we recover the arc-cosine kernel. We may also note that <a class="lipdf" href="http://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf">random Fourier features</a> also yield a similar interpretation.</p>
<p>Other important links have also been drawn recently between kernel regression and strongly over-parametrized neural networks, see <a class="lipdf" href="http://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf">this paper</a> and <a class="lipdf" href="https://arxiv.org/pdf/1812.07956.pdf">that one</a>, which is another exciting story.</p></div>
    </content>
    <updated>2019-07-17T16:02:03Z</updated>
    <published>2019-07-17T16:02:03Z</published>
    <category term="Machine learning"/>
    <author>
      <name>Sebastien Bubeck</name>
    </author>
    <source>
      <id>https://blogs.princeton.edu/imabandit</id>
      <link href="https://blogs.princeton.edu/imabandit/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://blogs.princeton.edu/imabandit" rel="alternate" type="text/html"/>
      <subtitle>Random topics in optimization, probability, and statistics. By Sébastien Bubeck</subtitle>
      <title>I’m a bandit</title>
      <updated>2019-07-25T23:47:28Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://minimizingregret.wordpress.com/?p=207</id>
    <link href="https://minimizingregret.wordpress.com/2019/07/17/boosting-for-dynamical-systems/" rel="alternate" type="text/html"/>
    <title>Boosting for Dynamical Systems</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">by Nataly Brukhim, Naman Agarwal and Elad Hazan, based on this paper  In a famous 1906 competition at a local fair in Plymouth, England, participants were asked to guess the weight of an ox. Out of a crowd of hundreds, no one came close to the ox’s actual weight, but the average of all guesses was … <a class="more-link" href="https://minimizingregret.wordpress.com/2019/07/17/boosting-for-dynamical-systems/">Continue reading <span class="screen-reader-text">Boosting for Dynamical Systems</span> <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>by Nataly Brukhim, Naman Agarwal and Elad Hazan, based on <a href="https://arxiv.org/abs/1906.08720">this paper</a> </em></p>
<p>In a famous 1906 competition at a local fair in Plymouth, England, participants were asked to guess the weight of an ox. Out of a crowd of hundreds, no one came close to the ox’s actual weight, but the average of all guesses was almost correct. How is it that combining the opinions of laymen can somehow arrive at highly reasoned decisions, despite the weak judgment of individual members? This concept of harnessing wisdom from weak rules of thumb to form a highly accurate prediction rule, is the basis of ensemble methods and <b>boosting</b>. Boosting is a theoretically sound methodology that has transformed machine learning across a variety of applications; in classification and regression tasks, online learning, and many more.</p>
<p>In the case of online learning, examples for training a predictor are not available in advance, but are revealed one at a time. Online boosting combines a set of online prediction rules, or <i>weak learners. </i>At every time step, each weak learner outputs a prediction, suffers some loss and is then updated accordingly. The performance of an online learner is measured using the <i>regret</i> criterion, which compares the accumulated loss over time with that of the best fixed decision in hindsight. A <i>Boosting</i> algorithm can choose which examples are fed to each of the weak learners, as well as the losses they incur. Intuitively, the online booster can encourage some weak learners to become really good in predicting certain common cases, while allowing others to focus on edge cases that are harder to predict. Overall, the <a href="http://proceedings.mlr.press/v37/beygelzimer15.pdf">online</a> <a href="https://arxiv.org/abs/1506.04820">boosting</a> framework can achieve low regret guarantees based on the learners’ individual regret values.</p>
<p>However, online learning can become more challenging when our actions have consequences on the environment. This can be illustrated with the following experiment: imagine learning to balance a long pole on your hand. When you move your hand slightly, the pole tilts. You then move your hand in the opposite direction, and it bounces back and tilts to the other side. One jerk the wrong way might have you struggling for a good few seconds to rebalance. In other words, a <u>sequence of decisions</u> you made earlier determines whether or not the pole is balanced at any given time, rather than the single decision you make at that point.<img alt="" class=" aligncenter" height="129" src="https://minimizingregret.files.wordpress.com/2019/07/image.jpeg?w=136&amp;h=129" title="" width="136"/></p>
<p>More generally, consider cases when our environment has a <b>state, </b>and is in some sense “remembering” our past choices. A stateful framework, able to model a wide range of such phenomena, is a <i>dynamical system</i>. A dynamical system can be thought of as a function that determines, given the current state, what the state of the system will be in the next time step. Think of the physical dynamics that determines our pole’s position based on sequential hand movements. Other intuitive examples are the fluctuations of stock prices in the stock market, or the local weather temperatures; these can all be modeled with dynamical systems.</p>
<p>So how can boosting help us make better predictions for a dynamical system? In <a href="https://arxiv.org/abs/1906.08720">recent work</a> we propose an algorithm, which we refer to as DynaBoost, that achieves this goal. In the paper we provide theoretical regret bounds, as well as an empirical evaluation in a variety of applications, such as online control and time-series prediction.</p>
<p><b>Learning for Online Control</b></p>
<p>Control theory is a field of applied mathematics that deals with the control of various physical processes and engineering systems. The objective is to design an action rule, or <i>controller</i>, for a dynamical system such that steady state values are achieved quickly, and the system maintains stability.</p>
<p>Consider a simple Linear Dynamical System (LDS):</p>
<p style="text-align: center;"><img alt="x_{t+1} = A x_t + B u_t + w_t" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7Bt%2B1%7D+%3D+A+x_t+%2B+B+u_t+%2B+w_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="x_{t+1} = A x_t + B u_t + w_t"/></p>
<p>where <img alt="x_t,u_t" class="latex" src="https://s0.wp.com/latex.php?latex=x_t%2Cu_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="x_t,u_t"/> are the state and control values at time t, respectively. Assume a known transition dynamics specified by the matrices A and B, and an arbitrary disturbance to the system given by <img alt="w_t" class="latex" src="https://s0.wp.com/latex.php?latex=w_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="w_t"/>. The goal of the controller is to minimize a convex cost function <img alt="c_t(x_t,u_t)" class="latex" src="https://s0.wp.com/latex.php?latex=c_t%28x_t%2Cu_t%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="c_t(x_t,u_t)"/>.</p>
<p>A provably optimal controller for the Gaussian noise case (where <img alt="w_t" class="latex" src="https://s0.wp.com/latex.php?latex=w_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="w_t"/> are normally distributed) and when the cost functions are quadratic, is the Linear Quadratic Regulator (LQR). LQR computes a pre-fixed matrix K such that <img alt="u_t^{LQR} = K x_t" class="latex" src="https://s0.wp.com/latex.php?latex=u_t%5E%7BLQR%7D+%3D+K+x_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="u_t^{LQR} = K x_t"/>. In other words, LQR computes a linear controller – which linearly maps the state into a control at every time step.</p>
<p>A <a href="http://proceedings.mlr.press/v97/agarwal19c/agarwal19c.pdf">recent advancement</a> in online control considers <i>arbitrary</i> disturbances, as opposed to normally distributed noise. In this more general setting, there is no closed form for the optimal controller. Instead, it is proposed to use a weighted sum of previously observed noises, i.e., <img alt="u_t^{WL} = K x_t + \sum_{i=1}^H M_i w_{t-i} " class="latex" src="https://s0.wp.com/latex.php?latex=u_t%5E%7BWL%7D+%3D+K+x_t+%2B+%5Csum_%7Bi%3D1%7D%5EH+M_i+w_%7Bt-i%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="u_t^{WL} = K x_t + \sum_{i=1}^H M_i w_{t-i} "/> , where <img alt="M_1,...,M_H" class="latex" src="https://s0.wp.com/latex.php?latex=M_1%2C...%2CM_H&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="M_1,...,M_H"/> are learned parameters, updated in an online fashion. This method is shown to attain vanishing average regret compared to the best fixed linear controller in hindsight, and is applicable for general convex cost functions as opposed to only quadratics.</p>
<p>Crucially, the state-dependent term <img alt="Kx_t" class="latex" src="https://s0.wp.com/latex.php?latex=Kx_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="Kx_t"/> is not learned. Since the learned parameters of the above controller therefore considers only a fixed number of recent disturbances, we can apply existing <a href="http://ocobook.cs.princeton.edu/">online convex optimization</a> techniques developed for <a href="https://papers.nips.cc/paper/6025-online-learning-for-adversaries-with-memory-price-of-past-mistakes">learning with loss functions that have bounded memory</a>.</p>
<p><strong>Boosting for Online Control</strong></p>
<p>Using the insights described above to remove state in online control, we can now use techniques from online boosting. DynaBoost maintains multiple copies of the base-controller above, with each copy corresponding to one stage in boosting. At every time step, the control <img alt="u_t" class="latex" src="https://s0.wp.com/latex.php?latex=u_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="u_t"/> is obtained from a convex combination of the base-controllers’ outputs <img alt="u_t^{WL(1)},...,u_t^{WL(N)}" class="latex" src="https://s0.wp.com/latex.php?latex=u_t%5E%7BWL%281%29%7D%2C...%2Cu_t%5E%7BWL%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="u_t^{WL(1)},...,u_t^{WL(N)}"/>. To update each base-controller’s parameters, DynaBoost feeds each controller with a <i>residual</i> <i>proxy </i>cost function, and seeks to obtain a minimizing point in the direction of the residual loss function’s gradient. Stability ensures that minimizing regret over the proxy costs (which have finite memory) suffices to minimize overall regret. See <a href="https://arxiv.org/abs/1906.08720">our paper</a> for the detailed description of the algorithm and its regret guarantees.</p>
<p><strong>Sanity check experiment</strong></p>
<p>We first conducted experiments for the standard LQR setting with i.i.d. Gaussian noise and known dynamics. We applied our boosting method to the non-optimal controller with learned parameters (Control-WL), and we observe that boosting improves its loss and achieves near-optimal performance (here the optimal controller is given by the fixed LQR solution). We have tested an LDS of different dimensions <img alt="d = 1,10,100" class="latex" src="https://s0.wp.com/latex.php?latex=d+%3D+1%2C10%2C100&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="d = 1,10,100"/>, and averaged results over multiple runs.</p>
<p><img alt="" height="182" src="https://minimizingregret.files.wordpress.com/2019/07/null.png?w=624&amp;h=182" title="" width="624"/></p>
<p><strong>Correlated noise experiment</strong></p>
<p>When the disturbances are not independently drawn, the LQR controller is not guaranteed to perform optimally. We experimented with two LDS settings with correlated disturbances in which (a) the disturbances <img alt="w_t" class="latex" src="https://s0.wp.com/latex.php?latex=w_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="w_t"/> are generated from a Gaussian random-walk, and (b) where they are generated by a sine function applied to the time index. In these cases, boosted controllers perform better compared to the “weak” learned controller, and can also outperform the fixed LQR solution. We have also tested a Recurrent Neural Network, and observed that boosting is effective for RNNs as well.</p>
<p><img alt="" height="266" src="https://minimizingregret.files.wordpress.com/2019/07/null-1.png?w=624&amp;h=266" title="" width="624"/></p>
<p><strong>Inverted Pendulum experiment</strong></p>
<p>A more challenging experiment with a non-linear dynamical system is the Inverted Pendulum experiment. This is very similar to the pole balancing example we discussed above, and is a standard benchmark for control methods. The goal is to balance the inverted pendulum by applying torque that will stabilize it in a vertically upright position, in the presence of noise. In our experiments, we used correlated disturbances from a Gaussian random-walk. We follow the dynamics implemented in <a href="https://gym.openai.com/">OpenAI Gym</a>, and test the performance of different controllers: LQR, a learned controller, and boosting. The video below visualizes this experiment:</p>
<div class="jetpack-video-wrapper"/>
<p>When averaging the loss value over multiple experiment runs, we get the following plot:</p>
<p style="text-align: center;"><img alt="" height="276" src="https://minimizingregret.files.wordpress.com/2019/07/null-2.png?w=369&amp;h=276" title="" width="369"/></p>
<p>It can be seen that the learned controller performs much better than the LQR in the presence of correlated noise, and that boosting can improve its stability and achieve lower average loss.</p>
<p><b>Boosting for Time-Series Prediction</b></p>
<p>Similarly to the control setting, in time-series prediction tasks it is sufficient to use fixed horizons, and online boosting can be efficiently applied here as well. In time-series prediction, the data is often assumed to be generated from an autoregressive moving average (ARMA) model:</p>
<p style="text-align: center;"><img alt="x_{t} = \sum_{i=1}^k \alpha_i x_{t-i} + \sum_{j=1}^q \beta_j w_j + w_t" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7Bt%7D+%3D+%5Csum_%7Bi%3D1%7D%5Ek+%5Calpha_i+x_%7Bt-i%7D+%2B+%5Csum_%7Bj%3D1%7D%5Eq+%5Cbeta_j+w_j+%2B+w_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="x_{t} = \sum_{i=1}^k \alpha_i x_{t-i} + \sum_{j=1}^q \beta_j w_j + w_t"/></p>
<p>In words, each data point <img alt="x_t" class="latex" src="https://s0.wp.com/latex.php?latex=x_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="x_t"/> is given by a weighted sum of previous points, previous noises and a new noise term <img alt="w_t" class="latex" src="https://s0.wp.com/latex.php?latex=w_t&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="w_t"/>, where <img alt="\alpha,\beta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%2C%5Cbeta&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\alpha,\beta"/> are the coefficients vectors.</p>
<p>To test our boosting method, We experimented with 4 simulated settings: 1) normally distributed noises, 2) coefficients of the dynamical system slowly change over time, 3) a single, abrupt, change of the coefficients, and, 4) correlated noise: Gaussian random walk.</p>
<p>The weak learners tested here are the ARMA-ONS (online newton step) and ARMA-OGD (online gradient descent) algorithms for time-series prediction (See <a href="http://proceedings.mlr.press/v30/Anava13.pdf">this</a> paper for more details). We applied our boosting method, as well as a fast version of it, which applies to quadratic loss functions (we used squared difference in this case).</p>
<p><i><b>1) Gaussian Noise </b></i> <i><b>2) Changing Coefficients</b></i></p>
<p><img alt="" height="217" src="https://minimizingregret.files.wordpress.com/2019/07/null-3.png?w=289&amp;h=217" title="" width="289"/><img alt="" height="217" src="https://minimizingregret.files.wordpress.com/2019/07/null-4.png?w=289&amp;h=217" title="" width="289"/><img alt="" height="218" src="https://minimizingregret.files.wordpress.com/2019/07/null-5.png?w=292&amp;h=218" title="" width="292"/><img alt="" height="217" src="https://minimizingregret.files.wordpress.com/2019/07/null-6.png?w=290&amp;h=217" title="" width="290"/></p>
<p><i><b>3) Abrupt Change </b></i> <i><b>4) Correlated Noise</b></i></p>
<p>We can see in the plots above that all weak learners’ loss values (red) can be improved by online boosting methods (blue). A similar observation arises when experimenting with real-world data; we experimented with the Air Quality dataset from the <a href="https://archive.ics.uci.edu/ml/index.php">UCI Machine Learning repository</a>, that contains hourly averaged measurements of air quality properties from an Italian city throughout one year, as measured by chemical sensors. We apply similar weak learners to this task, as well as our boosting algorithms. Here we again obtain better averaged losses for boosted methods (blue) compared to the baselines (red).</p>
<p style="text-align: center;"><img alt="" height="373" src="https://minimizingregret.files.wordpress.com/2019/07/null-7.png?w=498&amp;h=373" title="" width="498"/></p></div>
    </content>
    <updated>2019-07-17T14:24:42Z</updated>
    <published>2019-07-17T14:24:42Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Elad Hazan</name>
    </author>
    <source>
      <id>https://minimizingregret.wordpress.com</id>
      <logo>https://minimizingregret.files.wordpress.com/2017/08/cropped-pu1.png?w=32</logo>
      <link href="https://minimizingregret.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://minimizingregret.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://minimizingregret.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://minimizingregret.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Google Princeton AI and Hazan Lab @ Princeton University</subtitle>
      <title>Minimizing Regret</title>
      <updated>2019-07-26T07:21:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=17564</id>
    <link href="https://gilkalai.wordpress.com/2019/07/17/dan-romik-on-the-riemann-zeta-function/" rel="alternate" type="text/html"/>
    <title>Dan Romik on the Riemann zeta function</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This post about the Rieman zeta function, among the most important and mysterious mathematical objects is kindly written by Dan Romik. It is related to his paper Orthogonal polynomial expansions for the Riemann xi function,  that we mentioned in this … <a href="https://gilkalai.wordpress.com/2019/07/17/dan-romik-on-the-riemann-zeta-function/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>This post about the Rieman zeta function, among the most important and mysterious mathematical objects is kindly written by Dan Romik. It is related to his paper <a href="https://arxiv.org/abs/1902.06330">Orthogonal polynomial expansions for the Riemann xi function</a>,  that we mentioned in <a href="https://gilkalai.wordpress.com/2019/02/28/dan-romik-studies-the-riemanns-zeta-function-and-other-zeta-news/">this post</a>.</em></p>
<h2>Dan Romik on the Riemann zeta function</h2>
<p>Recently when I was thinking about the Riemann zeta function, I had the double thrill of discovering some new results about it, and then later finding out that my new ideas were closely related to some very classical ideas due to two icons of twentieth-century mathematics, George Pólya and Pál Turán. When you are trying to stand on the shoulders of giants, it’s nice to see other giants right there beside you trying to do the same!</p>
<p>It all goes back to one of the most famous problems in mathematics, the Riemann Hypothesis (RH). Both Pólya and Turán were rather enamored with this problem and published about it extensively; Pólya was said to have been preoccupied with the problem to the very end of his life.(1) And they both recognized that an important first step in trying to prove something about the zeros of the zeta function is having a good representation<br/>
for the Riemann zeta function. After all, there are many different formulas that can be used to define or compute the zeta function. If you don’t choose the right one, you probably won’t get very far with your analysis.</p>
<p>Pólya in one of his famous attacks on the problem considered the representation of the zeta function (or more precisely of the Riemann xi function, which is a symmetrized and better-behaved version of the zeta function; see below) as a Fourier transform—a standard representation due (essentially) to Riemann. I’ll have more to say about that later.</p>
<p>Turán also looked at the Riemann xi function, and instead of working with one of the standard “named” representations such as the Fourier transform or Taylor series, looked around a bit more intentionally for a representation of the function that seemed particularly suited to answering the specific question of whether the zeros all lie on a line. In a 1950 address to the Hungarian Academy of Sciences, he put forward his ideas about what he thought was the correct representation to look at: the infinite series expansion of the xi function in the Hermite polynomials. About eighty years after Turán’s discovery, my own investigations led me to discover [5] that the Hermite polynomials are not the only polynomials in which it’s interesting to expand the Riemann xi function. It turns out that there are at least two other families of polynomials for which the respective expansions are no less (and, in some ways, more) well-behaved. My motto for these polynomial families, which are known to experts in special functions but have until now been somewhat esoteric (though I hope that is about to change), is that they are “the coolest polynomials that you never heard of.”</p>
<p>Let’s look at some of the technical details so that I can explain why these new expansions are interesting, and how they relate to Turán’s work and ultimately back to Pólya’s ideas and one of the particular threads that grew out of them. First, define the Riemann xi function as</p>
<p><img alt="\displaystyle \xi(s) = \frac12 s(s-1) \pi^{-s/2} \Gamma\left(\frac{s}{2}\right) \zeta(s) \qquad (s\in\mathbb{C}), " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cxi%28s%29+%3D+%5Cfrac12+s%28s-1%29+%5Cpi%5E%7B-s%2F2%7D+%5CGamma%5Cleft%28%5Cfrac%7Bs%7D%7B2%7D%5Cright%29+%5Czeta%28s%29+%5Cqquad+%28s%5Cin%5Cmathbb%7BC%7D%29%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \xi(s) = \frac12 s(s-1) \pi^{-s/2} \Gamma\left(\frac{s}{2}\right) \zeta(s) \qquad (s\in\mathbb{C}), "/></p>
<p>where <img alt="{\Gamma(z)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CGamma%28z%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Gamma(z)}"/> is the Euler gamma function and <img alt="{\zeta(s)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Czeta%28s%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\zeta(s)}"/> is the Riemann zeta function. It’s also common to denote<br/>
<img alt="\displaystyle \Xi(t) = \xi\left(\frac12+it\right) \qquad (t\in\mathbb{C}). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CXi%28t%29+%3D+%5Cxi%5Cleft%28%5Cfrac12%2Bit%5Cright%29+%5Cqquad+%28t%5Cin%5Cmathbb%7BC%7D%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle \Xi(t) = \xi\left(\frac12+it\right) \qquad (t\in\mathbb{C}). "/></p>
<p>This is Riemann’s “capital xi” function, which is still usually referred to as Riemann’s xi function. (This seems reasonable: the two functions are the same up to a trivial linear change of variables.) The main point of these definitions is that <img alt="{\Xi(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CXi%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Xi(t)}"/> is an entire function of the complex variable <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>, and that RH can now be reformulated as the statement that the zeros of <img alt="{\Xi(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CXi%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Xi(t)}"/> all lie on the real line. Moreover, the famous functional equation satisfied by the Riemann zeta function maps to the statement that <img alt="{\Xi(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CXi%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Xi(t)}"/> is an even function.<br/>
Now consider the following four ways of representing the xi function:</p>
<p><img alt="\displaystyle \Xi(t) = \sum_{n=0}^\infty (-1)^n a_{2n} t^{2n},~~~~~(1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CXi%28t%29+%3D+%5Csum_%7Bn%3D0%7D%5E%5Cinfty+%28-1%29%5En+a_%7B2n%7D+t%5E%7B2n%7D%2C%7E%7E%7E%7E%7E%281%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle \Xi(t) = \sum_{n=0}^\infty (-1)^n a_{2n} t^{2n},~~~~~(1)"/></p>
<p><img alt="\displaystyle \Xi(t) = \sum_{n=0}^\infty (-1)^n b_{2n} H_{2n}(t),~~~~~(2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CXi%28t%29+%3D+%5Csum_%7Bn%3D0%7D%5E%5Cinfty+%28-1%29%5En+b_%7B2n%7D+H_%7B2n%7D%28t%29%2C%7E%7E%7E%7E%7E%282%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle \Xi(t) = \sum_{n=0}^\infty (-1)^n b_{2n} H_{2n}(t),~~~~~(2)"/></p>
<p><img alt="\displaystyle\Xi(t) = \sum_{n=0}^\infty (-1)^n c_{2n} f_{2n}\left(\frac{t}{2}\right),~~~~~(3)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle%5CXi%28t%29+%3D+%5Csum_%7Bn%3D0%7D%5E%5Cinfty+%28-1%29%5En+c_%7B2n%7D+f_%7B2n%7D%5Cleft%28%5Cfrac%7Bt%7D%7B2%7D%5Cright%29%2C%7E%7E%7E%7E%7E%283%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle\Xi(t) = \sum_{n=0}^\infty (-1)^n c_{2n} f_{2n}\left(\frac{t}{2}\right),~~~~~(3)"/></p>
<p><img alt="\displaystyle \Xi(t) = \sum_{n=0}^\infty (-1)^n d_{2n} g_{2n}\left(\frac{t}{2}\right).~~~~~(4)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5CXi%28t%29+%3D+%5Csum_%7Bn%3D0%7D%5E%5Cinfty+%28-1%29%5En+d_%7B2n%7D+g_%7B2n%7D%5Cleft%28%5Cfrac%7Bt%7D%7B2%7D%5Cright%29.%7E%7E%7E%7E%7E%284%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle \Xi(t) = \sum_{n=0}^\infty (-1)^n d_{2n} g_{2n}\left(\frac{t}{2}\right).~~~~~(4)"/></p>
<p>Here, the first representation (1) is simply the Taylor expansion of <img alt="{\Xi(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CXi%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Xi(t)}"/>, which contains only even terms since <img alt="{\Xi(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CXi%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Xi(t)}"/> is an even function. The numbers <img alt="{a_{2n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba_%7B2n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a_{2n}}"/> are (up to the <img alt="{(-1)^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28-1%29%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(-1)^n}"/> sign factor) the Taylor coefficients. Some attempts have been made to understand them, and one interesting and fairly trivial observation (again going back to facts already known to Riemann) is that they are all positive. Some additional and less trivial things can be said—see for example Section 6.1 of my paper [5], and the recent paper by Griffin, Ono, Rolen and Zagier [2]. But at the end of the day, no one has yet succeeded in using the Taylor expansion to prove anything new about the location of the zeros.</p>
<p>The second representation (2) is the infinite series expansion of <img alt="{\Xi(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CXi%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Xi(t)}"/> in the classical sequence of Hermite polynomials, defined by the well-known formula</p>
<p><img alt="\displaystyle H_n(t) = (-1)^n e^{t^2} \frac{d^n}{dt^n} \left( e^{-t^2} \right). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+H_n%28t%29+%3D+%28-1%29%5En+e%5E%7Bt%5E2%7D+%5Cfrac%7Bd%5En%7D%7Bdt%5En%7D+%5Cleft%28+e%5E%7B-t%5E2%7D+%5Cright%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle H_n(t) = (-1)^n e^{t^2} \frac{d^n}{dt^n} \left( e^{-t^2} \right). "/></p>
<p>This is the representation whose use was advocated by Turán. His reasoning was that expanding a function of a complex variable (for example, in the simplest case, a polynomial) in monomials <img alt="{t^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t^n}"/> doesn’t provide useful information to easily decide if the function has only real zeros, because the monomials have, roughly speaking, radial symmetry: their level curves are concentric circles. The Hermite polynomials on the other hand, at least heuristically, have level curves that are closer to being straight lines parallel to the real axis, Turán argued; thus, they are more suited to the geometry of the problem we are trying to solve.</p>
<p>Turán’s case for supporting the Hermite polynomials as the right basis to use is quite detailed—you can read about it in his papers [6,7,8] (and no, he was not able to actually prove anything about the zeros of <img alt="{\Xi(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CXi%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Xi(t)}"/>; this is a common theme in most of the attacks on RH to date…). I’ll simply mention that again one interesting and fairly easy observation is that the coefficients <img alt="{b_{2n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb_%7B2n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b_{2n}}"/> in the expansion (2)—adjusted through the introduction of the sign factor <img alt="{(-1)^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28-1%29%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(-1)^n}"/>—end up being positive numbers. Their asymptotic behavior can also be analyzed: I prove a result about this in my paper (though it’s not particularly pretty).</p>
<p>Now comes the part that to me seems the most exciting, involving the expansions (3) and (4). These are the expansions in the more exotic families of polynomials</p>
<p><img alt="\displaystyle f_n(x)=(-i)^n \sum_{k=0}^n 2^k\binom{n+\frac12}{n-k}\binom{-\frac34+ix}{k}," class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+f_n%28x%29%3D%28-i%29%5En+%5Csum_%7Bk%3D0%7D%5En+2%5Ek%5Cbinom%7Bn%2B%5Cfrac12%7D%7Bn-k%7D%5Cbinom%7B-%5Cfrac34%2Bix%7D%7Bk%7D%2C&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle f_n(x)=(-i)^n \sum_{k=0}^n 2^k\binom{n+\frac12}{n-k}\binom{-\frac34+ix}{k},"/></p>
<p><img alt="\displaystyle g_n(x)= (-i)^n \sum_{k=0}^n \frac{(n+k+1)!}{(n-k)!(3/2)_k^2} \binom{-\frac34+ix}{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+g_n%28x%29%3D+%28-i%29%5En+%5Csum_%7Bk%3D0%7D%5En+%5Cfrac%7B%28n%2Bk%2B1%29%21%7D%7B%28n-k%29%21%283%2F2%29_k%5E2%7D+%5Cbinom%7B-%5Cfrac34%2Bix%7D%7Bk%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="\displaystyle g_n(x)= (-i)^n \sum_{k=0}^n \frac{(n+k+1)!}{(n-k)!(3/2)_k^2} \binom{-\frac34+ix}{k}"/></p>
<p>(where <img alt="{(3/2)_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%283%2F2%29_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(3/2)_n}"/> is a <a href="https://en.wikipedia.org/wiki/Falling_and_rising_factorials">Pochhammer symbol</a>), mildly rescaled by replacing <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>  with <img alt="{t/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t/2}"/>. In the terminology of the theory of orthogonal polynomials, the family <img alt="{f_n(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_n%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_n(x)}"/> is a special case of a two-parameter family <img alt="{P_n^{(\lambda)}(x;\phi)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP_n%5E%7B%28%5Clambda%29%7D%28x%3B%5Cphi%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P_n^{(\lambda)}(x;\phi)}"/> known as the Meixner-Pollaczek polynomials, with the parameters taking the particular values <img alt="{\phi=\frac{\pi}{2}, \lambda=\frac{3}{4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cphi%3D%5Cfrac%7B%5Cpi%7D%7B2%7D%2C+%5Clambda%3D%5Cfrac%7B3%7D%7B4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\phi=\frac{\pi}{2}, \lambda=\frac{3}{4}}"/>. Similarly, the family <img alt="{g_n(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg_n%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g_n(x)}"/> is a special case of the four-parameter family <img alt="{p_n(x;a,b,c,d)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_n%28x%3Ba%2Cb%2Cc%2Cd%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_n(x;a,b,c,d)}"/> known as the continuous Hahn polynomials, with the parameters taking the particular values <img alt="{a=b=c=d=\frac{3}{4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%3Db%3Dc%3Dd%3D%5Cfrac%7B3%7D%7B4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a=b=c=d=\frac{3}{4}}"/>. Their main characterizing property is that they are orthogonal sequences of polynomials for two specific weight functions on <img alt="{\mathbb{R}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb%7BR%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbb{R}}"/>: the <img alt="{f_n(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_n%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_n(x)}"/> are orthogonal with respect to the weight function <img alt="{w_1(x)=\left|\Gamma\left(\frac34+ix\right)\right|^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_1%28x%29%3D%5Cleft%7C%5CGamma%5Cleft%28%5Cfrac34%2Bix%5Cright%29%5Cright%7C%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w_1(x)=\left|\Gamma\left(\frac34+ix\right)\right|^2}"/>, and the <img alt="{g_n(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg_n%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g_n(x)}"/> are orthogonal with respect to <img alt="{w_2(x)=\left|\Gamma\left(\frac34+ix\right)\right|^4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_2%28x%29%3D%5Cleft%7C%5CGamma%5Cleft%28%5Cfrac34%2Bix%5Cright%29%5Cright%7C%5E4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w_2(x)=\left|\Gamma\left(\frac34+ix\right)\right|^4}"/>. Again, fairly esoteric. But interesting!</p>
<p>There are several things that make the expansions (3)–(4) well-behaved. First, the coefficients <img alt="{c_{2n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_%7B2n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_{2n}}"/>, <img alt="{d_{2n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd_%7B2n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d_{2n}}"/> are again positive. This actually seems pretty relevant for questions like RH: for example, if we consider “toy” versions of (1)–(3) in which the coefficient sequences <img alt="{a_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a_n}"/>, <img alt="{b_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b_n}"/> and <img alt="{c_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_n}"/> are replaced by the sequence <img alt="{\alpha^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha^n}"/> for fixed <img alt="{0&lt;\alpha&lt;1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%3C%5Calpha%3C1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0&lt;\alpha&lt;1}"/>, all three expansions sum up to rescaled cosines, which are entire functions that of course have only real zeros. (Without the <img alt="{(-1)^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28-1%29%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(-1)^n}"/> factor, we would get a hyperbolic cosine, which has imaginary zeros.)</p>
<p>Second, one can derive asymptotics for <img alt="{c_{2n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_%7B2n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_{2n}}"/> and <img alt="{d_{2n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd_%7B2n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d_{2n}}"/>, and they are quite a bit nicer than the asymptotic formulas for the Taylor and Hermite expansion coefficients. In my paper, I proved that <img alt="\displaystyle c_{2n} \sim A \sqrt{n} e^{-B \sqrt{n}}, \qquad d_{2n} \sim C n^{4/3} e^{-D n^{2/3}} \qquad \textrm{as }n\rightarrow\infty, " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+c_%7B2n%7D+%5Csim+A+%5Csqrt%7Bn%7D+e%5E%7B-B+%5Csqrt%7Bn%7D%7D%2C+%5Cqquad+d_%7B2n%7D+%5Csim+C+n%5E%7B4%2F3%7D+e%5E%7B-D+n%5E%7B2%2F3%7D%7D+%5Cqquad+%5Ctextrm%7Bas+%7Dn%5Crightarrow%5Cinfty%2C+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle c_{2n} \sim A \sqrt{n} e^{-B \sqrt{n}}, \qquad d_{2n} \sim C n^{4/3} e^{-D n^{2/3}} \qquad \textrm{as }n\rightarrow\infty, "/> where <img alt="{A,B,C,D}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%2CB%2CC%2CD%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A,B,C,D}"/> are the constants <img alt="\displaystyle A = 16\sqrt{2}\pi^{3/2}, \qquad B = 4\sqrt{\pi}, \qquad C = \frac{128 \times 2^{1/3} \pi^{2/3} e^{-2\pi /3}}{\sqrt{3}}, \qquad D = 3 (4\pi)^{1/3}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+A+%3D+16%5Csqrt%7B2%7D%5Cpi%5E%7B3%2F2%7D%2C+%5Cqquad+B+%3D+4%5Csqrt%7B%5Cpi%7D%2C+%5Cqquad+C+%3D+%5Cfrac%7B128+%5Ctimes+2%5E%7B1%2F3%7D+%5Cpi%5E%7B2%2F3%7D+e%5E%7B-2%5Cpi+%2F3%7D%7D%7B%5Csqrt%7B3%7D%7D%2C+%5Cqquad+D+%3D+3+%284%5Cpi%29%5E%7B1%2F3%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle A = 16\sqrt{2}\pi^{3/2}, \qquad B = 4\sqrt{\pi}, \qquad C = \frac{128 \times 2^{1/3} \pi^{2/3} e^{-2\pi /3}}{\sqrt{3}}, \qquad D = 3 (4\pi)^{1/3}. "/></p>
<p>Third, the expansions have some conceptual meaning: (3) turns out to be equivalent to the expansion of the elementary function <img alt="{\frac{d^2}{du^2} (u \coth(\pi u))}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7Bd%5E2%7D%7Bdu%5E2%7D+%28u+%5Ccoth%28%5Cpi+u%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{d^2}{du^2} (u \coth(\pi u))}"/>, <img alt="{u&gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u&gt;0}"/>, in an orthogonal basis of functions related to the Laguerre polynomials <img alt="{L_n^{1/2}(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_n%5E%7B1%2F2%7D%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_n^{1/2}(x)}"/>. And analogously, (4) arises out of the expansion of a certain auxiliary function <img alt="{\tilde{\nu}(u)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctilde%7B%5Cnu%7D%28u%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\tilde{\nu}(u)}"/> (I won’t define it here) in yet another classical family of orthogonal polynomials, the Chebyshev polynomials of the second kind.</p>
<p>Fourth (and fifth, sixth, …): the expansions are just… nice, in the sense that they arise in a way that seems natural when one asks certain questions, that they have excellent convergence properties, and that the coefficients <img alt="{c_{2n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_%7B2n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{c_{2n}}"/> and <img alt="{d_{2n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bd_%7B2n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{d_{2n}}"/> have several elegant formulas, each revealing something interesting about them. Read the paper to understand more.</p>
<p>I said I will get back to Pólya’s work on RH. This post is already quite long so I will say only a little bit about this. One of Pólya’s major discoveries was that there are operations on entire functions that (under certain mild assumptions) preserve the property of the function having only real zeros. Specifically this is the case for the operation of multiplying the Fourier transform of the function by the factor <img alt="{e^{\lambda u^2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%5E%7B%5Clambda+u%5E2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e^{\lambda u^2}}"/> for <img alt="{\lambda&gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda&gt;0}"/>  (where <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{u}"/> is the frequency variable). This opens the way to defining a family of deformations <img alt="{\Xi_\lambda(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CXi_%5Clambda%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Xi_\lambda(t)}"/> of the Riemann xi function arising out of this operation, and trying to generalize RH by asking for which values of <img alt="{\lambda}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clambda%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\lambda}"/> it is the case that <img alt="{\Xi_\lambda(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CXi_%5Clambda%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Xi_\lambda(t)}"/> has only real zeros. Since Pólya’s work, and important later extensions of it by De Bruijn and Newman, this has become a very active topic of research, nowadays referred to under the name of the De Bruijn-Newman constant.<br/>
See the recent survey of Newman and Wu [3], a 2018 paper by Rodgers and Tao [4] proving a major conjecture of Newman, and the recent paper [9] by the <a href="https://terrytao.wordpress.com/2018/12/28/polymath-15-eleventh-thread-writing-up-the-results-and-exploring-negative-t/">Polymath15 project</a> (mentioned by Gil in his <a href="https://gilkalai.wordpress.com/2019/02/28/dan-romik-studies-the-riemanns-zeta-function-and-other-zeta-news/">earlier post</a>), for the latest on this subject.</p>
<p>The connection I found between this topic and the idea of expanding the Riemann xi function in families of orthogonal polynomials is the following: expansions such as (2)–(4) suggest yet another natural way of “deforming” the Riemann xi function by adding a parameter <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha}"/>: simply multiply the <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>th term in the expansion by <img alt="{\alpha^{2n}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%5E%7B2n%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\alpha^{2n}}"/> (the linear operator that does this is called the Poisson kernel, and generalizes the standard Poisson kernel from complex analysis and the theory of harmonic functions). It turns out—and is actually easy to prove, and really isn’t terribly surprising in the grand scheme of things—that in the case of the Hermite expansion (2), this family of deformations is the same, up to some trivial reparametrization, as the family of deformations <img alt="{\Xi_\lambda(t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CXi_%5Clambda%28t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Xi_\lambda(t)}"/> that was studied in connection with the work of Pólya, De Bruijn, Newman and their successors. A nice connection between two threads of research that were not previously recognized as being related to each other, I think. Furthermore, this suggests that the Poisson kernel and associated deformations may yet have an important role to play in the context of the new expansions in the orthogonal polynomial families <img alt="{f_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_n}"/> and <img alt="{g_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bg_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{g_n}"/>, where we get genuinely new families of deformations of the Riemann xi function. I explore this idea in my paper and it leads to some interesting things.</p>
<p>So let’s summarize. The key questions you are no doubt wondering about are: where does any of this lead? And do these new ideas say anything really useful or especially relevant for the Riemann hypothesis? The answer is that I don’t know (and I’m wondering about the same things). That being said, these orthogonal polynomial expansions seem quite interesting in their own right. The Riemann zeta function is a mysterious object, and there are <a href="https://en.wikipedia.org/wiki/Lindel%C3%B6f_hypothesis">other things</a> we wish to understand about it beside where its zeros are, so it’s always good to have additional points of view from which to approach it. Moreover, even on the question of the zeros there are reasons to be cautiously optimistic that this approach may have something useful to offer; see Chapter 7 of my paper for a brief discussion of why that is the case.</p>
<h2/>
<h2>References</h2>
<p>[1] D. Albers and G. L. Alexanderson, editors. Mathematical People: Profiles and Interviews. A K Peters, 2008.</p>
<p>[2] M. Griffin, K. Ono, L. Rolen and D. Zagier. Jensen polynomials for the Riemann zeta function and other sequences. Preprint (2019), <a href="https://arxiv.org/abs/1902.07321">arXiv:1902.07321</a>.</p>
<p>[3] C. M. Newman. Constants of de Bruijn-Newman type in analytic number theory and statistical physics. To appear in Bull. Amer. Math. Soc.</p>
<p>[4] B. Rodgers and T. Tao. The De Bruijn-Newman constant is nonnegative. Preprint (2018), <a href="https://arxiv.org/abs/1801.05914">arXiv:1801.05914</a>.</p>
<p>[5] D. Romik. <a href="https://arxiv.org/abs/1902.06330">Orthogonal polynomial expansions for the Riemann xi function</a>. Preprint (2019), <a href="https://arxiv.org/abs/1902.06330">arXiv:1902.06330</a>.</p>
<p>[6] P. Turán. Sur l’algèbre fonctionelle. Pages 279–290 in: Comptes Rendus du Premier Congrès des Mathématiciens Hongrois, 27 Août–2 Septembre 1950. Akadémiai Kiadó, 1952. Reprinted in Collected Papers of Paul Turán, Ed. P. Erdos, Vol. 1, pp. 677–688. Akadémiai Kiadó, 1990. An English translation of the paper by Dan Romik <a href="http://math.ucdavis.edu/~romik/data/uploads/misc/turan1952-english.pdf">On functional algebra</a>.</p>
<p>[7] P. Turán. Hermite-expansion and strips for zeros of polynomials. Arch. Math. 5 (1954), 148–152. Reprinted in Collected Papers of Paul Turán, Ed. P. Erdos, Vol. 1, pp. 738–742. Akadémiai Kiadó, 1990.</p>
<p>[8]  P. Turán. To the analytical theory of algebraic equations. Bulgar. Akad. Nauk. Otd. Mat. Fiz. Nauk. Izv. Mat. Inst. 3 (1959), 123–137. Reprinted in Collected Papers of Paul Turán, Ed. P. Erdos, Vol. 2, pp. 1080–1090. Akadémiai Kiadó, 1990.</p>
<p>[9] D.H.J. Polymath. Effective approximation of heat flow evolution of the Riemann ξ function, and a new upper bound for the de Bruijn-Newman constant. Preprint (2019), <a href="https://arxiv.org/abs/1904.12438">arXiv:1904.12438</a>.</p>
<h2>Notes:</h2>
<p>(1) Alexanderson writes in [1, p. 259]: “A week or so before he died, Pólya asked me to look on his desk at home for some papers on which he said he had written down some interesting ideas he had for proving RH. Of course I could find no such notes, but until the day he died he was thinking about that famous problem.”</p>
<p> </p>
<p>(2) Turán’s Hungarian Academy of Sciences talk was published in a rather obscure French-language paper [6] that seems to have been largely forgotten. It’s an interesting read nonetheless, and to make it more accessible to anyone who may be interested, I recently translated it to English.</p>
<p> </p>
<p>(3) Turán mentions in [8] that he discovered the results on the Hermite expansion in 1938–39, but they were not published until much later. Clearly this was not a convenient time in history for publishing such discoveries; Turán, a Hungarian Jew, spent much of World War II interned in labor camps in Hungary.</p></div>
    </content>
    <updated>2019-07-17T06:22:33Z</updated>
    <published>2019-07-17T06:22:33Z</published>
    <category term="Combinatorics"/>
    <category term="Guest blogger"/>
    <category term="Number theory"/>
    <category term="Dan Romik"/>
    <category term="George Polya"/>
    <category term="Paul Turan"/>
    <category term="Riemann Hypothesis"/>
    <category term="Riemann zeta function"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-07-26T07:20:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16114</id>
    <link href="https://rjlipton.wordpress.com/2019/07/16/summer-reading-in-theory/" rel="alternate" type="text/html"/>
    <title>Summer Reading in Theory</title>
    <summary>Some formative books in mathematics and computing theory LSE source: “Calculus on Clay?” Norman Biggs is the author of the wonderful book Algebraic Graph Theory. Both Ken and I read it long ago, and both of us have it out now because of its relevance to Hao Huang’s beautiful short proof of the Boolean Sensitivity […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Some formative books in mathematics and computing theory</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2019/07/biggs-norman-150x150.jpg"><img alt="" class="alignright size-full wp-image-16115" src="https://rjlipton.files.wordpress.com/2019/07/biggs-norman-150x150.jpg?w=600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">LSE <a href="https://blogs.lse.ac.uk/maths/2016/04/15/norman-biggs-calculus-on-clay/">source</a>: <i>“Calculus on Clay?”</i></font></td>
</tr>
</tbody>
</table>
<p>
Norman Biggs is the author of the wonderful <a href="https://www.cambridge.org/core/books/algebraic-graph-theory/6C70471342F19680068C35EF174075DC">book</a> <em>Algebraic Graph Theory</em>. Both Ken and I read it long ago, and both of us have it out now because of its relevance to Hao Huang’s beautiful short <a href="http://www.mathcs.emory.edu/~hhuan30/papers/sensitivity_1.pdf">proof</a> of the Boolean Sensitivity Conjecture. </p>
<p>
Today we wish to ask, <i>What are your top five favorite books on mathematics and theory for summer reading?</i><br/>
<span id="more-16114"/></p>
<p>
There’s an <a href="https://en.wikipedia.org/wiki/Aporia">aporia</a> in that question. A working definition of aporia is: “a self-contradiction that isn’t.” The point is that books for summer reading should be new, so how would you already know which are your favorites? Well, we are thinking of books that are so rich you can always find new things in them—and that also played formative roles earlier in our careers.</p>
<p>
Ken knew Biggs during his first year at Oxford when Biggs was visiting there from London. He took part in a weekly sitting-room seminar organized by Peter Neumann. Biggs’s book was a central reference for Ken’s undergraduate senior thesis at Princeton, and both he and Ken presented material based on it. </p>
<p>
</p><p/><h2> Best Five Books—Dick </h2><p/>
<p/><p>
Here are my votes for all-time best books in mathematics and in computer science theory.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <a href="https://www.cambridge.org/core/books/algebraic-graph-theory/6C70471342F19680068C35EF174075DC"><i>Algebraic Graph Theory</i></a>, by Norman Biggs. A wonderful book. First appeared in 1974.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <a href="https://www.amazon.com/Introduction-Probability-Theory-Applications-Vol/dp/0471257087/ref=sr_1_1?keywords=William+Feller&amp;qid=1563190401&amp;s=books&amp;sr=1-1"><i> An Introduction to Probability Theory and Its Applications, Vol. 1</i></a>, by William Feller. This is the book I used to learn probability theory.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <a href="https://www.amazon.com/gp/product/0199219869/ref=as_li_tl?ie=UTF8&amp;tag=mathblog05-20&amp;camp=1789&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=0199219869&amp;linkId=a71963a143733e948f50588526d624c0"><i> An Introduction to the Theory of Numbers</i></a>, by Godfrey Hardy and Edward Wright. Now updated by Andrew Wiles, Roger Heath-Brown, and Joseph Silverman. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <a href="https://www.amazon.com/Elements-Number-Theory-Dover-Mathematics/dp/0486781658/ref=sr_1_1?keywords=Vinogradov&amp;qid=1563190340&amp;s=books&amp;sr=1-1"><i>Elements of Number Theory</i></a>, by Ivan Vinogradov. Another small book that is loaded with ideas. </p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <a href="https://www.abebooks.com/Paul-Erds-Art-Counting-Erdos-Joel/13380002114/bd"><i>The Art of Counting</i></a>, by Paul Erdős and Joel Spencer. This book changed my life. Today the book is of course <a href="https://www.amazon.com/dp/0470170204/?tag=stackoverfl08-20"><i>The Probabilistic Method</i></a>, by Noga Alon and Joel Spencer. </p>
<p>
</p><p/><h2> Best Five Books—Ken </h2><p/>
<p/><p>
Ken reaches back to his teen years but it’s still the same span of years as my list. Here he tells it:</p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> All books by Martin Gardner—in particular, the books of collections of his “Mathematical Games” columns in <em>Scientific American</em>. Here is an <a href="https://blogs.scientificamerican.com/guest-blog/the-top-10-martin-gardner-scientific-american-articles/">overview</a>.</p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> <a href="https://www.lybrary.com/scarne-on-dice-p-655.html"><i>Scarne on Dice</i></a> and <a href="https://www.lybrary.com/scarne-on-cards-p-759.html"><i> Scarne on Cards</i></a>. Originally it was neither of these books—nor John Scarne’s <em>Complete Guide to Gambling</em>—but a different book on in which both Scarne and Gardner figured prominently. Alas I, Ken, cannot trace it. That’s what I used to learn probability theory.</p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> <a href="https://www.amazon.com/Spectra-Graphs-Application-Applied-Mathematics/dp/0121951502"><i>Spectra of Graphs</i></a>, by Dragoš Cvetković, Michael Doob, and Horst Sachs. I could put Biggs’s book here, but this is the one that got me on to the whole subject just before my senior year at Princeton. It was fresh out in 1980—I recall the tactile sensation of the dark green spanking new cover in the Fine Hall Library’s copy. A great book with pictures and algebra. </p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> <a href="https://www.amazon.com/Ideals-Varieties-Algorithms-Computational-Undergraduate/dp/0387356509"><i> Ideals, Varieties, and Algorithms</i></a>, by David Cox, John Little, and Donal O’Shea. Fast forward to 1997. Having realized that techniques from algebraic geometry could surmount the “Natural Proofs” <a href="https://rjlipton.wordpress.com/2009/03/25/whos-afraid-of-natural-proofs/">barrier</a> (see also <a href="https://en.wikipedia.org/wiki/Geometric_complexity_theory">GCT</a>), I went whole-hog after it. See “Manic Monomials” in this <a href="https://rjlipton.wordpress.com/2012/07/04/july-fourth-sale-of-ideas/">post</a> for one thing that tripped it up. The book remains incredibly stimulating. It has a <a href="https://www.amazon.com/Using-Algebraic-Geometry-Graduate-Mathematics/dp/0387984879/">sequel</a>, <em>Using Algebraic Geometry</em>.</p>
<p>
<img alt="{\bullet}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet}"/> <a href="https://en.wikipedia.org/wiki/Quantum_Computation_and_Quantum_Information"><i>Quantum Computation and Quantum Information</i></a> by Michael Nielsen and Isaac Chuang. As with Hardy and Wright, it has its own Wikipedia page. Dick and I can say this is nominating a competitor, but Chaung &amp; Nielsen is really in a class by itself for the sheer richness and writing style. One odd mark of its influence: In 2006 when I reacted to the sensational and frightening accusations of cheating at the world championship <a href="https://en.wikipedia.org/wiki/World_Chess_Championship_2006">match</a>, my first thought was to apply distributional distance measures of the kind used in its later chapters. Among such measures is (quantum) <a href="https://en.wikipedia.org/wiki/Fidelity_of_quantum_states">fidelity</a>, and although I focused more on Jensen-Shannon divergence before deciding on simpler stuff, my chess research <a href="https://cse.buffalo.edu/~regan/chess/fidelity/">website</a> retains “fidelity” in its name as part of a multi-way reference to <a href="https://en.wikipedia.org/wiki/FIDE">FIDE</a>, faith, and playing in good faith.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
What books most influenced you? What are your votes for the best books that might influence others?	 </p></font></font></div>
    </content>
    <updated>2019-07-17T04:26:38Z</updated>
    <published>2019-07-17T04:26:38Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="Oldies"/>
    <category term="Teaching"/>
    <category term="algebraic"/>
    <category term="books"/>
    <category term="Hao Huang"/>
    <category term="Norman Biggs"/>
    <category term="probabilistic"/>
    <category term="spectral graph theory"/>
    <category term="summer reading"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-07-26T07:20:49Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6069637759837834972</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6069637759837834972/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/07/guest-post-by-samir-khuller-on.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6069637759837834972" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6069637759837834972" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/07/guest-post-by-samir-khuller-on.html" rel="alternate" type="text/html"/>
    <title>Guest post by Samir Khuller on attending The TCS Women 2019 meeting</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><br/>
(I will post the solution to the problem in the last blog later in the week---probably Thursday. Meanwhile, enjoy these thoughts from Samir Khuller on the TCS Women 2019 meeting.)<br/>
<br/>
Guest Post by Samir Khuller:<br/>
<br/>
Am I even allowed here?” was the first thought that crossed my mind when I entered the room. It was packed with women (over 95%), however a few minutes later, several men had trickled in. I was at the TCS Women spotlight workshop on the day before STOC. Kudos to Barna Saha, Sofya Raskhodnikova, and Virginia Vassilevska Williams for putting this grand (and long needed) event together, which serves as a role model and showcases some of the recent work by rising stars. In addition to the Sun afternoon workshop, the event was followed by both an all women panel and a poster session (which I sadly did not attend).<br/>
<br/>
<br/>
The rising stars talks were given by Naama Ben-David (CMU), Andrea Lincoln (MIT), Debarati Das (Charles University) and Oxana Poburinnaya (Boston U). After a short break the inspirational talk was by Ronitt Rubinfeld from MIT.  Ronitt’s talk was on the topic of Program Checking, but she made it inspirational by putting us in her shoes as a young graduate student, three decades back, trying to make a dent in research by working on something that her advisor Manuel Blum, and his senior graduate student Sampath Kannan had been working on, and I must say she made a pretty big dent in the process! She also related those ideas to other pieces of work done since in a really elegant manner and how these pieces of work lead to work on property testing.<br/>
<br/>
<br/>
I am delighted to say that NSF supported the workshop along with companies such as Amazon, Akamai, Google and Microsoft. SIGACT plans to be a major sponsor next year.<br/>
<br/>
<br/>
The Full program for the workshop is at the following URL<a href="https://sigact.org/tcswomen/tcs-women-2019/">here.</a><br/>
<br/></div>
    </content>
    <updated>2019-07-16T23:38:00Z</updated>
    <published>2019-07-16T23:38:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-07-26T05:44:48Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/094</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/094" rel="alternate" type="text/html"/>
    <title>TR19-094 |  Rainbow coloring hardness via low sensitivity polymorphisms | 

	Venkatesan Guruswami, 

	Sai Sandeep</title>
    <summary>A $k$-uniform hypergraph is said to be $r$-rainbow colorable if there is an $r$-coloring of its vertices such that every hyperedge intersects all $r$ color classes. Given as input such a hypergraph, finding a $r$-rainbow coloring of it is NP-hard for all $k \ge 3$ and $r \ge 2$. Therefore, one settles for finding a rainbow coloring with fewer colors (which is an easier task).  When $r=k$ (the maximum possible value), i.e., the hypergraph is $k$-partite, one can efficiently $2$-rainbow color the hypergraph, i.e., $2$-color its vertices so that there are no monochromatic edges. In this work we consider the next smaller value of $r=k-1$, and prove that in this case it is NP-hard to rainbow color the hypergraph with $q :=  \lceil \frac{k-2}{2} \rceil$ colors. In particular, for $k \le 6$, it is NP-hard to $2$-color $(k-1)$-rainbow colorable $k$-uniform hypergraphs.

Our proof follows the algebraic approach to promise constraint satisfaction problems. It proceeds by characterizing the polymorphisms associated with the approximate rainbow coloring problem, which are rainbow colorings of some product hypergraphs on vertex set $[r]^n$. We prove that any such polymorphism $f: [r]^n \to [q]$ must be $C$-fixing, i.e., there is a small subset $S$ of $C$ coordinates and a setting $a \in [q]^S$ such that fixing $x_{|S} = a$ determines the value of $f(x)$. The key step in our proof is bounding the sensitivity of certain rainbow colorings, thereby arguing that they must be juntas. Armed with the $C$-fixing characterization, our NP-hardness is obtained via a reduction from smooth Label Cover.</summary>
    <updated>2019-07-16T01:19:56Z</updated>
    <published>2019-07-16T01:19:56Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-07-26T07:20:36Z</updated>
    </source>
  </entry>
</feed>
