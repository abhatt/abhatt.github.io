<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-04-17T20:21:52Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07798</id>
    <link href="http://arxiv.org/abs/2004.07798" rel="alternate" type="text/html"/>
    <title>The Dimensions of Hyperspaces</title>
    <feedworld_mtime>1587081600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lutz:Jack_H=.html">Jack H. Lutz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lutz:Neil.html">Neil Lutz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mayordomo:Elvira.html">Elvira Mayordomo</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07798">PDF</a><br/><b>Abstract: </b>We use the theory of computing to prove general hyperspace dimension theorems
for three important fractal dimensions. Let $X$ be a separable metric space,
and let $\mathcal{K}(X)$ be the \emph{hyperspace} of $X$, i.e., the set of all
nonempty compact subsets of $X$, endowed with the Hausdorff metric. For the
lower and upper Minkowski (i.e., box-counting) dimensions, we give precise
formulas for the dimension of $\mathcal{K}(E)$, where $E$ is any subset of $X$.
For packing dimension, we give a tight bound on the dimension of
$\mathcal{K}(E)$ in terms of the dimension of $E$, where $E$ is any analytic
(i.e., $\mathbf{\Sigma}^1_1$) subset of $X$. These results also hold for a
large class of gauge families (Hausdorff families of gauge functions).
</p>
<p>The logical structures of our proofs are of particular interest. We first
extend two algorithmic fractal dimensions--computability-theoretic versions of
classical Hausdorff and packing dimensions that assign dimensions $\dim(x)$ and
$\textrm{Dim}(x)$ to individual points $x \in X$--to arbitrary separable metric
spaces and to arbitrary gauge families. We then extend the point-to-set
principle of J. Lutz and N. Lutz (2018) to arbitrary separable metric spaces
and to a large class of gauge families. Finally, we use this principle to prove
our hyperspace packing dimension theorem. This is one of a handful of
cases--all very recent and all using the point-to-set principle--in which the
theory of computing has been used to prove new theorems in classical geometric
measure theory, theorems whose statements do not involve computation or logic.
</p></div>
    </summary>
    <updated>2020-04-17T01:20:20Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-04-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07718</id>
    <link href="http://arxiv.org/abs/2004.07718" rel="alternate" type="text/html"/>
    <title>Coresets for Clustering in Excluded-minor Graphs and Beyond</title>
    <feedworld_mtime>1587081600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Braverman:Vladimir.html">Vladimir Braverman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jiang:Shaofeng_H==C=.html">Shaofeng H.-C. Jiang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krauthgamer:Robert.html">Robert Krauthgamer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Xuan.html">Xuan Wu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07718">PDF</a><br/><b>Abstract: </b>Coresets are modern data-reduction tools that are widely used in data
analysis to improve efficiency in terms of running time, space and
communication complexity. Our main result is a fast algorithm to construct a
small coreset for k-Median in (the shortest-path metric of) an excluded-minor
graph. Specifically, we give the first coreset of size that depends only on
$k$, $\epsilon$ and the excluded-minor size, and our running time is
quasi-linear (in the size of the input graph).
</p>
<p>The main innovation in our new algorithm is that is iterative; it first
reduces the $n$ input points to roughly $O(\log n)$ reweighted points, then to
$O(\log\log n)$, and so forth until the size is independent of $n$. Each step
in this iterative size reduction is based on the importance sampling framework
of Feldman and Langberg (STOC 2011), with a crucial adaptation that reduces the
number of \emph{distinct points}, by employing a terminal embedding (where low
distortion is guaranteed only for the distance from every terminal to all other
points). Our terminal embedding is technically involved and relies on
shortest-path separators, a standard tool in planar and excluded-minor graphs.
</p>
<p>Furthermore, our new algorithm is applicable also in Euclidean metrics, by
simply using a recent terminal embedding result of Narayanan and Nelson, (STOC
2019), which extends the Johnson-Lindenstrauss Lemma. We thus obtain an
efficient coreset construction in high-dimensional Euclidean spaces, thereby
matching and simplifying state-of-the-art results (Sohler and Woodruff, FOCS
2018; Huang and Vishnoi, STOC 2020).
</p>
<p>In addition, we also employ terminal embedding with additive distortion to
obtain small coresets in graphs with bounded highway dimension, and use
applications of our coresets to obtain improved approximation schemes, e.g., an
improved PTAS for planar k-Median via a new centroid set.
</p></div>
    </summary>
    <updated>2020-04-17T01:24:15Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07671</id>
    <link href="http://arxiv.org/abs/2004.07671" rel="alternate" type="text/html"/>
    <title>Isomorphism Testing for Graphs Excluding Small Minors</title>
    <feedworld_mtime>1587081600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grohe:Martin.html">Martin Grohe</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Neuen:Daniel.html">Daniel Neuen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wiebking:Daniel.html">Daniel Wiebking</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07671">PDF</a><br/><b>Abstract: </b>We prove that there is a graph isomorphism test running in time
$n^{\operatorname{polylog}(h)}$ on $n$-vertex graphs excluding some $h$-vertex
graph as a minor. Previously known bounds were $n^{\operatorname{poly}(h)}$
(Ponomarenko, 1988) and $n^{\operatorname{polylog}(n)}$ (Babai, STOC 2016). For
the algorithm we combine recent advances in the group-theoretic graph
isomorphism machinery with new graph-theoretic arguments.
</p></div>
    </summary>
    <updated>2020-04-17T01:21:30Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07659</id>
    <link href="http://arxiv.org/abs/2004.07659" rel="alternate" type="text/html"/>
    <title>Algorithmic Foundations for the Diffraction Limit</title>
    <feedworld_mtime>1587081600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Sitan.html">Sitan Chen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moitra:Ankur.html">Ankur Moitra</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07659">PDF</a><br/><b>Abstract: </b>For more than a century and a half it has been widely-believed (but was never
rigorously shown) that the physics of diffraction imposes certain fundamental
limits on the resolution of an optical system. However our understanding of
what exactly can and cannot be resolved has never risen above heuristic
arguments which, even worse, appear contradictory. In this work we remedy this
gap by studying the diffraction limit as a statistical inverse problem and,
based on connections to provable algorithms for learning mixture models, we
rigorously prove upper and lower bounds on how many photons we need (and how
precisely we need to record their locations) to resolve closely-spaced point
sources.
</p>
<p>We give the first provable algorithms that work regardless of the separation,
but only for a constant number of point sources. Surprisingly, we show that
when the number of point sources becomes large, there is a phase transition
where the sample complexity goes from polynomial to exponential, and we pin
down its location to within a universal constant that is independent of the
number of point sources. Thus it is rigorously possible both to break the
diffraction limit and yet to prove strong impossibility results depending on
the setup. This is the first non-asymptotic statistical foundation for
resolution in a model arising from first principles in physics, and helps
clarify many omnipresent debates in the optics literature.
</p></div>
    </summary>
    <updated>2020-04-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07650</id>
    <link href="http://arxiv.org/abs/2004.07650" rel="alternate" type="text/html"/>
    <title>Fully Dynamic $c$-Edge Connectivity in Subpolynomial Time</title>
    <feedworld_mtime>1587081600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jin:Wenyu.html">Wenyu Jin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Xiaorui.html">Xiaorui Sun</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07650">PDF</a><br/><b>Abstract: </b>We present a deterministic fully dynamic algorithm for $c$-edge connectivity
problem with $n^{o(1)}$ worst case update and query time for any positive
integer $c = (\log n)^{o(1)}$ for a graph with $n$ vertices. Previously, only
polylogarithmic, $O(\sqrt{n})$, and $O(n^{2/3})$ worst case update time
algorithms were known for fully dynamic $1$, $2$ and $3$-edge connectivity
problems respectively.
</p>
<p>Our techniques include a multi-level $c$-edge connectivity sparsifier, an
online-batch update algorithm for the sparsifier, and a general approach to
turn an online-batch dynamic algorithm with small amortized update time into a
fully dynamic algorithm with worst case update time.
</p></div>
    </summary>
    <updated>2020-04-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07630</id>
    <link href="http://arxiv.org/abs/2004.07630" rel="alternate" type="text/html"/>
    <title>Four Pages Are Indeed Necessary for Planar Graphs</title>
    <feedworld_mtime>1587081600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bekos:Michael_A=.html">Michael A. Bekos</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kaufmann:Michael.html">Michael Kaufmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Klute:Fabian.html">Fabian Klute</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pupyrev:Sergey.html">Sergey Pupyrev</a>, Chrysanthi Raftopoulou, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Ueckerdt:Torsten.html">Torsten Ueckerdt</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07630">PDF</a><br/><b>Abstract: </b>An embedding of a graph in a book consists of a linear order of its vertices
along the spine of the book and of an assignment of its edges to the pages of
the book, so that no two edges on the same page cross. The book thickness of a
graph is the minimum number of pages over all its book embeddings. Accordingly,
the book thickness of a class of graphs is the maximum book thickness over all
its members. In this paper, we address a long-standing open problem regarding
the exact book thickness of the class of planar graphs, which previously was
known to be either three or four. We settle this problem by demonstrating
planar graphs that require four pages in any of their book embeddings, thus
establishing that the book thickness of the class of planar graphs is four.
</p></div>
    </summary>
    <updated>2020-04-17T01:26:01Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07574</id>
    <link href="http://arxiv.org/abs/2004.07574" rel="alternate" type="text/html"/>
    <title>A polynomial time algorithm for solving the closest vector problem in zonotopal lattices</title>
    <feedworld_mtime>1587081600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/McCormick:S=_Thomas.html">S. Thomas McCormick</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Peis:Britta.html">Britta Peis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Scheidweiler:Robert.html">Robert Scheidweiler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vallentin:Frank.html">Frank Vallentin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07574">PDF</a><br/><b>Abstract: </b>In this note we give a polynomial time algorithm for solving the closest
vector problem in the class of zonotopal lattices. Zonotopal lattices are
characterized by the fact that their Voronoi cell is a zonotope, i.e. a
projection of a regular cube. Examples of zonotopal lattices include lattices
of Voronoi's first kind and tensor products of root lattices of type A. The
combinatorial structure of zonotopal lattices can be described by regular
matroids/totally unimodular matrices. We observe that a linear algebra version
of the minimum mean cycling canceling method can be applied for efficiently
solving the closest vector problem in zonotopal lattices.
</p></div>
    </summary>
    <updated>2020-04-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07572</id>
    <link href="http://arxiv.org/abs/2004.07572" rel="alternate" type="text/html"/>
    <title>Centralized and Parallel Multi-Source Shortest Paths via Hopsets and Fast Matrix Multiplication</title>
    <feedworld_mtime>1587081600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Elkin:Michael.html">Michael Elkin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Neiman:Ofer.html">Ofer Neiman</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07572">PDF</a><br/><b>Abstract: </b>Consider an undirected weighted graph $G = (V,E,w)$. We study the problem of
computing $(1+\epsilon)$-approximate shortest paths for $S \times V$, for a
subset $S \subseteq V$ of $|S| = n^r$ sources, for some $0 &lt; r \le 1$. We
devise a significantly improved algorithm for this problem in the entire range
of parameter $r$, in both the classical centralized and in the parallel (PRAM)
models of computation. Specifically, our centralized algorithm for this problem
requires time $\tilde{O}(|E| \cdot n^{o(1)} + n^{\omega(r)})$, where
$n^{\omega(r)}$ is the time required to multiply an $n^r \times n$ matrix by an
$n \times n$ one. Our PRAM algorithm has polylogarithmic time $(\log
n)^{O(1/\rho)}$, and its work complexity is $\tilde{O}(|E| \cdot n^\rho +
n^{\omega(r)})$, for any arbitrarily small constant $\rho &gt;0$.
</p>
<p>In particular, for $r \le 0.313\ldots$, our centralized algorithm computes $S
\times V$ $(1+\epsilon)$-approximate shortest paths in $n^{2 + o(1)}$ time. Our
PRAM polylogarithmic-time algorithm has work complexity $O(|E| \cdot n^\rho +
n^{2+o(1)})$, for any arbitrarily small constant $\rho &gt;0$. Previously existing
solutions either require centralized time/parallel work of $O(|E| \cdot |S|)$
or provide much weaker approximation guarantees.
</p>
<p>We also devise efficient algorithms for computing $(1+\epsilon)$-approximate
shortest paths from each vertex to its $k$ nearest neighbors in {\em directed}
graphs. Here too the running time is only $O(n^{2+o(1)})$ even for polynomially
large $k\le n^{0.168}$.
</p>
<p>Our algorithm combines fast matrix multiplication with hopsets. Related ideas
were formerly used in the context of the Congested Clique model by
Censor-Hillel et al. \cite{CDKL19}. That model, however, suppresses heavy local
computations. We show that these computations can be replaced by fast matrix
multiplication.
</p></div>
    </summary>
    <updated>2020-04-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07566</id>
    <link href="http://arxiv.org/abs/2004.07566" rel="alternate" type="text/html"/>
    <title>Approximating Independent Set and Dominating Set on VPG graphs</title>
    <feedworld_mtime>1587081600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Galby:Esther.html">Esther Galby</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Munaro:Andrea.html">Andrea Munaro</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07566">PDF</a><br/><b>Abstract: </b>We consider Independent Set and Dominating Set restricted to VPG graphs (or,
equivalently, string graphs). We show that they both remain $\mathsf{NP}$-hard
on $B_0$-VPG graphs admitting a representation such that each grid-edge belongs
to at most one path and each horizontal path has length at most two. On the
other hand, combining the well-known Baker's shifting technique with bounded
mim-width arguments, we provide simple PTASes on VPG graphs admitting a
representation such that each grid-edge belongs to at most $t$ paths and the
length of the horizontal part of each path is at most $c$, for some $c \geq 1$.
</p></div>
    </summary>
    <updated>2020-04-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-04-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07558</id>
    <link href="http://arxiv.org/abs/2004.07558" rel="alternate" type="text/html"/>
    <title>Framework for $\exists \mathbb{R}$-Completeness of Two-Dimensional Packing Problems</title>
    <feedworld_mtime>1587081600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Abrahamsen:Mikkel.html">Mikkel Abrahamsen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Miltzow:Tillmann.html">Tillmann Miltzow</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Seiferth:Nadja.html">Nadja Seiferth</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07558">PDF</a><br/><b>Abstract: </b>We show that many natural two-dimensional packing problems are
algorithmically equivalent to finding real roots of multivariate polynomials. A
two-dimensional packing problem is defined by the type of pieces, containers,
and motions that are allowed. The aim is to decide if a given set of pieces can
be placed inside a given container. The pieces must be placed so that they are
pairwise interior-disjoint, and only motions of the allowed type can be used to
move them there. We establish a framework which enables us to show that for
many combinations of allowed pieces, containers, and motions, the resulting
problem is $\exists\mathbb R$-complete. This means that the problem is
equivalent (under polynomial time reductions) to deciding whether a given
system of polynomial equations and inequalities with integer coefficients has a
real solution. We consider packing problems where only translations are allowed
as the motions, and problems where arbitrary rigid motions are allowed, i.e.,
both translations and rotations. When rotations are allowed, we show that the
following combinations of allowed pieces and containers are $\exists\mathbb
R$-complete:
</p>
<p>$\bullet$ simple polygons, each of which has at most 8 corners, into a
square,
</p>
<p>$\bullet$ convex objects bounded by line segments and hyperbolic curves into
a square,
</p>
<p>$\bullet$ convex polygons into a container bounded by line segments and
hyperbolic curves.
</p>
<p>Restricted to translations, we show that the following combinations are
$\exists\mathbb R$-complete:
</p>
<p>$\bullet$ objects bounded by segments and hyperbolic curves into a square,
</p>
<p>$\bullet$ convex polygons into a container bounded by segments and hyperbolic
curves.
</p></div>
    </summary>
    <updated>2020-04-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-04-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07492</id>
    <link href="http://arxiv.org/abs/2004.07492" rel="alternate" type="text/html"/>
    <title>Steiner Trees for Hereditary Graph Classes: a Treewidth Perspective</title>
    <feedworld_mtime>1587081600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Hans Bodlaender, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brettell:Nick.html">Nick Brettell</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Johnson:Matthew.html">Matthew Johnson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Paesani:Giacomo.html">Giacomo Paesani</a>, Daniel Paulusma, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Leeuwen:Erik_Jan_van.html">Erik Jan van Leeuwen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07492">PDF</a><br/><b>Abstract: </b>We consider the classical problems (Edge) Steiner Tree and Vertex Steiner
Tree after restricting the input to some class of graphs characterized by a
small set of forbidden induced subgraphs. We show a dichotomy for the former
problem restricted to $(H_1,H_2)$-free graphs and a dichotomy for the latter
problem restricted to $H$-free graphs. We find that there exists an infinite
family of graphs $H$ such that Vertex Steiner Tree is polynomial-time solvable
for $H$-free graphs, whereas there exist only two graphs $H$ for which this
holds for Edge Steiner Tree. We also find that Edge Steiner Tree is
polynomial-time solvable for $(H_1,H_2)$-free graphs if and only if the
treewidth of the class of $(H_1,H_2)$-free graphs is bounded (subject to P
$\neq$ NP). To obtain the latter result, we determine all pairs $(H_1,H_2)$ for
which the class of $(H_1,H_2)$-free graphs has bounded treewidth.
</p></div>
    </summary>
    <updated>2020-04-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07470</id>
    <link href="http://arxiv.org/abs/2004.07470" rel="alternate" type="text/html"/>
    <title>Faster Dynamic Matrix Inverse for Faster LPs</title>
    <feedworld_mtime>1587081600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jiang:Shunhua.html">Shunhua Jiang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Song:Zhao.html">Zhao Song</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Weinstein:Omri.html">Omri Weinstein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Hengjie.html">Hengjie Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07470">PDF</a><br/><b>Abstract: </b>Motivated by recent Linear Programming solvers, we design dynamic data
structures for maintaining the inverse of an $n\times n$ real matrix under
$\textit{low-rank}$ updates, with polynomially faster amortized running time.
Our data structure is based on a recursive application of the Woodbury-Morrison
identity for implementing $\textit{cascading}$ low-rank updates, combined with
recent sketching technology. Our techniques and amortized analysis of
multi-level partial updates, may be of broader interest to dynamic matrix
problems.
</p>
<p>This data structure leads to the fastest known LP solver for general (dense)
linear programs, improving the running time of the recent algorithms of (Cohen
et al.'19, Lee et al.'19, Brand'20) from $O^*(n^{2+ \max\{\frac{1}{6},
\omega-2, \frac{1-\alpha}{2}\}})$ to $O^*(n^{2+\max\{\frac{1}{18}, \omega-2,
\frac{1-\alpha}{2}\}})$, where $\omega$ and $\alpha$ are the fast matrix
multiplication exponent and its dual. Hence, under the common belief that
$\omega \approx 2$ and $\alpha \approx 1$, our LP solver runs in
$O^*(n^{2.055})$ time instead of $O^*(n^{2.16})$.
</p></div>
    </summary>
    <updated>2020-04-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07447</id>
    <link href="http://arxiv.org/abs/2004.07447" rel="alternate" type="text/html"/>
    <title>Resolving the Optimal Metric Distortion Conjecture</title>
    <feedworld_mtime>1587081600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gkatzelis:Vasilis.html">Vasilis Gkatzelis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Halpern:Daniel.html">Daniel Halpern</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shah:Nisarg.html">Nisarg Shah</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07447">PDF</a><br/><b>Abstract: </b>We study the following metric distortion problem: there are two finite sets
of points, V and C, that lie in the same metric space, and our goal is to
choose a point in C whose total distance from the points in V is as small as
possible. However, rather than having access to the underlying distance metric,
we only know, for each point in V , a ranking of its distances to the points in
C. We propose algorithms that choose a point in C using only these rankings as
input and we provide bounds on their distortion (worst-case approximation
ratio). A prominent motivation for this problem comes from voting theory, where
V represents a set of voters, C represents a set of candidates, and the
rankings correspond to ordinal preferences of the voters. A major conjecture in
this framework is that the optimal deterministic algorithm has distortion 3. We
resolve this conjecture by providing a polynomial-time algorithm that achieves
distortion 3, matching a known lower bound. We do so by proving a novel lemma
about matching rankings of candidates to candidates, which we refer to as the
ranking-matching lemma. This lemma induces a family of novel algorithms, which
may be of independent interest, and we show that a special algorithm in this
family achieves distortion 3. We also provide more refined, parameterized,
bounds using the notion of {\alpha}-decisiveness, which quantifies the extent
to which a voter may prefer her top choice relative to all others. Finally, we
introduce a new randomized algorithm with improved distortion compared to known
results, and also provide improved lower bounds on the distortion of all
deterministic and randomized algorithms.
</p></div>
    </summary>
    <updated>2020-04-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07444</id>
    <link href="http://arxiv.org/abs/2004.07444" rel="alternate" type="text/html"/>
    <title>Fast exact computation of the $k$ most abundant isotope peaks with layer-ordered heaps</title>
    <feedworld_mtime>1587081600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kreitzberg:Patrick.html">Patrick Kreitzberg</a>, Jake Pennington, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lucke:Kyle.html">Kyle Lucke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Serang:Oliver.html">Oliver Serang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07444">PDF</a><br/><b>Abstract: </b>The theoretical computation of isotopic distribution of compounds is crucial
in many important applications of mass spectrometry, especially as machine
precision grows. A considerable amount of good tools have been created in the
last decade for doing so. In this paper we present a novel algorithm for
calculating the top $k$ peaks of a given compound. The algorithm takes
advantage of layer-ordered heaps used in an optimal method of selection on
$X+Y$ and is able to efficiently calculate the top $k$ peaks on very large
molecules. Among its peers, this algorithm shows a significant speedup on
molecules whose elements have many isotopes. The algorithm obtains a speedup of
more than 31x when compared to $\textsc{IsoSpec}$ on \ch{Au2Ca10Ga10Pd76} when
computing 47409787 peaks, which covers 0.999 of the total abundance.
</p></div>
    </summary>
    <updated>2020-04-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07403</id>
    <link href="http://arxiv.org/abs/2004.07403" rel="alternate" type="text/html"/>
    <title>On the computability of continuous maximum entropy distributions with applications</title>
    <feedworld_mtime>1587081600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Jonathan Leake, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vishnoi:Nisheeth_K=.html">Nisheeth K. Vishnoi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07403">PDF</a><br/><b>Abstract: </b>We initiate a study of the following problem: Given a continuous domain
$\Omega$ along with its convex hull $\mathcal{K}$, a point $A \in \mathcal{K}$
and a prior measure $\mu$ on $\Omega$, find the probability density over
$\Omega$ whose marginal is $A$ and that minimizes the KL-divergence to $\mu$.
This framework gives rise to several extremal distributions that arise in
mathematics, quantum mechanics, statistics, and theoretical computer science.
Our technical contributions include a polynomial bound on the norm of the
optimizer of the dual problem that holds in a very general setting and relies
on a "balance" property of the measure $\mu$ on $\Omega$, and exact algorithms
for evaluating the dual and its gradient for several interesting settings of
$\Omega$ and $\mu$. Together, along with the ellipsoid method, these results
imply polynomial-time algorithms to compute such KL-divergence minimizing
distributions in several cases. Applications of our results include: 1) an
optimization characterization of the Goemans-Williamson measure that is used to
round a positive semidefinite matrix to a vector, 2) the computability of the
entropic barrier for polytopes studied by Bubeck and Eldan, and 3) a
polynomial-time algorithm to compute the barycentric quantum entropy of a
density matrix that was proposed as an alternative to von Neumann entropy in
the 1970s: this corresponds to the case when $\Omega$ is the set of rank one
projections matrices and $\mu$ corresponds to the Haar measure on the unit
sphere. Our techniques generalize to the setting of Hermitian rank $k$
projections using the Harish-Chandra-Itzykson-Zuber formula, and are applicable
even beyond, to adjoint orbits of compact Lie groups.
</p></div>
    </summary>
    <updated>2020-04-17T01:20:40Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07354</id>
    <link href="http://arxiv.org/abs/2004.07354" rel="alternate" type="text/html"/>
    <title>Efficient Algorithms for Battleship</title>
    <feedworld_mtime>1587081600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Crombez:Lo=iuml=c.html">Loïc Crombez</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fonseca:Guilherme_D=_da.html">Guilherme D. da Fonseca</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gerard:Yan.html">Yan Gerard</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07354">PDF</a><br/><b>Abstract: </b>We consider an algorithmic problem inspired by the Battleship game. In the
variant of the problem that we investigate, there is a unique ship of shape $S
\subset Z^2$ which has been translated in the lattice $Z^2$. We assume that a
player has already hit the ship with a first shot and the goal is to sink the
ship using as few shots as possible, that is, by minimizing the number of
missed shots. While the player knows the shape $S$, which position of $S$ has
been hit is not known.
</p>
<p>Given a shape $S$ of $n$ lattice points, the minimum number of misses that
can be achieved in the worst case by any algorithm is called the Battleship
complexity of the shape $S$ and denoted $c(S)$. We prove three bounds on
$c(S)$, each considering a different class of shapes. First, we have $c(S) \leq
n-1$ for arbitrary shapes and the bound is tight for parallelogram-free shapes.
Second, we provide an algorithm that shows that $c(S) = O(\log n)$ if $S$ is an
HV-convex polyomino. Third, we provide an algorithm that shows that $c(S) =
O(\log \log n)$ if $S$ is a digital convex set. This last result is obtained
through a novel discrete version of the Blaschke-Lebesgue inequality relating
the area and the width of any convex body.
</p></div>
    </summary>
    <updated>2020-04-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-04-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07346</id>
    <link href="http://arxiv.org/abs/2004.07346" rel="alternate" type="text/html"/>
    <title>Online Multiserver Convex Chasing and Optimization</title>
    <feedworld_mtime>1587081600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bubeck:S=eacute=bastien.html">Sébastien Bubeck</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rabani:Yuval.html">Yuval Rabani</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sellke:Mark.html">Mark Sellke</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07346">PDF</a><br/><b>Abstract: </b>We introduce the problem of $k$-chasing of convex functions, a simultaneous
generalization of both the famous k-server problem in $R^d$, and of the problem
of chasing convex bodies and functions. Aside from fundamental interest in this
general form, it has natural applications to online $k$-clustering problems
with objectives such as $k$-median or $k$-means. We show that this problem
exhibits a rich landscape of behavior. In general, if both $k &gt; 1$ and $d &gt; 1$
there does not exist any online algorithm with bounded competitiveness. By
contrast, we exhibit a class of nicely behaved functions (which include in
particular the above-mentioned clustering problems), for which we show that
competitive online algorithms exist, and moreover with dimension-free
competitive ratio. We also introduce a parallel question of top-$k$ action
regret minimization in the realm of online convex optimization. There, too, a
much rougher landscape emerges for $k &gt; 1$. While it is possible to achieve
vanishing regret, unlike the top-one action case the rate of vanishing does not
speed up for strongly convex functions. Moreover, vanishing regret necessitates
both intractable computations and randomness. Finally we leave open whether
almost dimension-free regret is achievable for $k &gt; 1$ and general convex
losses. As evidence that it might be possible, we prove dimension-free regret
for linear losses via an information-theoretic argument.
</p></div>
    </summary>
    <updated>2020-04-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07319</id>
    <link href="http://arxiv.org/abs/2004.07319" rel="alternate" type="text/html"/>
    <title>The Impact of Heterogeneity and Geometry on the Proof Complexity of Random Satisfiability</title>
    <feedworld_mtime>1587081600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bl=auml=sius:Thomas.html">Thomas Bläsius</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Friedrich:Tobias.html">Tobias Friedrich</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/G=ouml=bel:Andreas.html">Andreas Göbel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Levy:Jordi.html">Jordi Levy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rothenberger:Ralf.html">Ralf Rothenberger</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07319">PDF</a><br/><b>Abstract: </b>Satisfiability is considered the canonical NP-complete problem and is used as
a starting point for hardness reductions in theory, while in practice heuristic
SAT solving algorithms can solve large-scale industrial SAT instances very
efficiently. This disparity between theory and practice is believed to be a
result of inherent properties of industrial SAT instances that make them
tractable. Two characteristic properties seem to be prevalent in the majority
of real-world SAT instances, heterogeneous degree distribution and locality. To
understand the impact of these two properties on SAT, we study the proof
complexity of random k-SAT models that allow to control heterogeneity and
locality. Our findings show that heterogeneity alone does not make SAT easy as
heterogeneous random k-SAT instances have superpolynomial resolution size. This
implies intractability of these instances for modern SAT-solvers. On the other
hand, modeling locality with an underlying geometry leads to small
unsatisfiable subformulas, which can be found within polynomial time.
</p>
<p>A key ingredient for the result on geometric random k-SAT can be found in the
complexity of higher-order Voronoi diagrams. As an additional technical
contribution, we show a linear upper bound on the number of non-empty Voronoi
regions, that holds for points with random positions in a very general setting.
In particular, it covers arbitrary p-norms, higher dimensions, and weights
affecting the area of influence of each point multiplicatively. This is in
stark contrast to quadratic lower bounds for the worst case.
</p></div>
    </summary>
    <updated>2020-04-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-04-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07286</id>
    <link href="http://arxiv.org/abs/2004.07286" rel="alternate" type="text/html"/>
    <title>Locality Sensitive Hashing for Set-Queries, Motivated by Group Recommendations</title>
    <feedworld_mtime>1587081600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kaplan:Haim.html">Haim Kaplan</a>, Jay Tenenbaum <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07286">PDF</a><br/><b>Abstract: </b>Locality Sensitive Hashing (LSH) is an effective method to index a set of
points such that we can efficiently find the nearest neighbors of a query
point. We extend this method to our novel Set-query LSH (SLSH), such that it
can find the nearest neighbors of a set of points, given as a query.
</p>
<p>Let $ s(x,y) $ be the similarity between two points $ x $ and $ y $. We
define a similarity between a set $ Q$ and a point $ x $ by aggregating the
similarities $ s(p,x) $ for all $ p\in Q $. For example, we can take $ s(p,x) $
to be the angular similarity between $ p $ and $ x $ (i.e., $1-{\angle
(x,p)}/{\pi}$), and aggregate by arithmetic or geometric averaging, or taking
the lowest similarity.
</p>
<p>We develop locality sensitive hash families and data structures for a large
set of such arithmetic and geometric averaging similarities, and analyze their
collision probabilities. We also establish an analogous framework and hash
families for distance functions. Specifically, we give a structure for the
euclidean distance aggregated by either averaging or taking the maximum.
</p>
<p>We leverage SLSH to solve a geometric extension of the approximate near
neighbors problem. In this version, we consider a metric for which the unit
ball is an ellipsoid and its orientation is specified with the query.
</p>
<p>An important application that motivates our work is group recommendation
systems. Such a system embeds movies and users in the same feature space, and
the task of recommending a movie for a group to watch together, translates to a
set-query $ Q $ using an appropriate similarity.
</p></div>
    </summary>
    <updated>2020-04-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-04-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.06830</id>
    <link href="http://arxiv.org/abs/2004.06830" rel="alternate" type="text/html"/>
    <title>Differentially Private Assouad, Fano, and Le Cam</title>
    <feedworld_mtime>1587081600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Acharya:Jayadev.html">Jayadev Acharya</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Ziteng.html">Ziteng Sun</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Huanyu.html">Huanyu Zhang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.06830">PDF</a><br/><b>Abstract: </b>Le Cam's method, Fano's inequality, and Assouad's lemma are three widely used
techniques to prove lower bounds for statistical estimation tasks. We propose
their analogues under central differential privacy. Our results are simple,
easy to apply and we use them to establish sample complexity bounds in several
estimation tasks.
</p>
<p>We establish the optimal sample complexity of discrete distribution
estimation under total variation distance and $\ell_2$ distance. We also
provide lower bounds for several other distribution classes, including product
distributions and Gaussian mixtures that are tight up to logarithmic factors.
The technical component of our paper relates coupling between distributions to
the sample complexity of estimation under differential privacy.
</p></div>
    </summary>
    <updated>2020-04-17T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.09020</id>
    <link href="http://arxiv.org/abs/1910.09020" rel="alternate" type="text/html"/>
    <title>SneakySnake: A Fast and Accurate Universal Genome Pre-Alignment Filter for CPUs, GPUs, and FPGAs</title>
    <feedworld_mtime>1587081600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alser:Mohammed.html">Mohammed Alser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shahroodi:Taha.html">Taha Shahroodi</a>, Juan Gomez-Luna, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alkan:Can.html">Can Alkan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mutlu:Onur.html">Onur Mutlu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.09020">PDF</a><br/><b>Abstract: </b>Motivation: We introduce SneakySnake, a highly parallel and highly accurate
pre-alignment filter that remarkably reduces the need for the computationally
costly sequence alignment step. The key idea of SneakySnake is to reduce the
approximate string matching (ASM) problem to the single net routing (SNR)
problem in VLSI chip layout. In the SNR problem, we are interested in only
finding the optimal path that connects two terminals with the least routing
cost on a special grid layout that contains obstacles. The SneakySnake
algorithm quickly solves the SNR problem and uses the found optimal path to
decide whether performing sequence alignment is necessary. Reducing the ASM
problem into SNR also makes SneakySnake efficient to implement for all modern
high-performance computing architectures (CPUs, GPUs, and FPGAs).
</p>
<p>Results: SneakySnake significantly improves the accuracy of pre-alignment
filtering by up to four orders of magnitude compared to the state-of-the-art
pre-alignment filters, Shouji, GateKeeper, and SHD. SneakySnake accelerates
Edlib (state-of-the-art implementation of Myers's bit-vector algorithm) and
Parasail (sequence aligner with configurable scoring function), by up to 37.6x
and 43.9x (&gt;12x on average), respectively, without requiring hardware
acceleration, and by up to 413x and 689x (&gt;400x on average), respectively,
using hardware acceleration. SneakySnake also accelerates the sequence
alignment of minimap2, a state-of-the-art read mapper, by 2.51x to 6.83x
without requiring hardware acceleration. As SneakySnake does not replace
sequence alignment, users can still configure the aligner of their choice for
different scoring functions, surpassing most existing efforts that aim to
accelerate sequence alignment.
</p>
<p>Availability: https://github.com/CMU-SAFARI/SneakySnake
</p></div>
    </summary>
    <updated>2020-04-17T01:47:26Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1809.07858</id>
    <link href="http://arxiv.org/abs/1809.07858" rel="alternate" type="text/html"/>
    <title>Shouji: A Fast and Efficient Pre-Alignment Filter for Sequence Alignment</title>
    <feedworld_mtime>1587081600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alser:Mohammed.html">Mohammed Alser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hassan:Hasan.html">Hasan Hassan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Akash.html">Akash Kumar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mutlu:Onur.html">Onur Mutlu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alkan:Can.html">Can Alkan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1809.07858">PDF</a><br/><b>Abstract: </b>Motivation: The ability to generate massive amounts of sequencing data
continues to overwhelm the processing capability of existing algorithms and
compute infrastructures. In this work, we explore the use of hardware/software
co-design and hardware acceleration to significantly reduce the execution time
of short sequence alignment, a crucial step in analyzing sequenced genomes. We
introduce Shouji, a highly-parallel and accurate pre-alignment filter that
remarkably reduces the need for computationally-costly dynamic programming
algorithms. The first key idea of our proposed pre-alignment filter is to
provide high filtering accuracy by correctly detecting all common subsequences
shared between two given sequences. The second key idea is to design a hardware
accelerator that adopts modern FPGA (Field-Programmable Gate Array)
architectures to further boost the performance of our algorithm.
</p>
<p>Results: Shouji significantly improves the accuracy of pre-alignment
filtering by up to two orders of magnitude compared to the state-of-the-art
pre-alignment filters, GateKeeper and SHD. Our FPGA-based accelerator is up to
three orders of magnitude faster than the equivalent CPU implementation of
Shouji. Using a single FPGA chip, we benchmark the benefits of integrating
Shouji with five state-of-the-art sequence aligners, designed for different
computing platforms. The addition of Shouji as a pre-alignment step reduces the
execution time of the five state-of-the-art sequence aligners by up to 18.8x.
Shouji can be adapted for any bioinformatics pipeline that performs sequence
alignment for verification. Unlike most existing methods that aim to accelerate
sequence alignment, Shouji does not sacrifice any of the aligner capabilities,
as it does not modify or replace the alignment step.
</p>
<p>Availability: https://github.com/CMU-SAFARI/Shouji
</p></div>
    </summary>
    <updated>2020-04-17T01:29:36Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1604.01789</id>
    <link href="http://arxiv.org/abs/1604.01789" rel="alternate" type="text/html"/>
    <title>GateKeeper: A New Hardware Architecture for Accelerating Pre-Alignment in DNA Short Read Mapping</title>
    <feedworld_mtime>1587081600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alser:Mohammed.html">Mohammed Alser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hassan:Hasan.html">Hasan Hassan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xin:Hongyi.html">Hongyi Xin</a>, Oğuz Ergin, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mutlu:Onur.html">Onur Mutlu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Alkan:Can.html">Can Alkan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1604.01789">PDF</a><br/><b>Abstract: </b>Motivation: High throughput DNA sequencing (HTS) technologies generate an
excessive number of small DNA segments -called short reads- that cause
significant computational burden. To analyze the entire genome, each of the
billions of short reads must be mapped to a reference genome based on the
similarity between a read and "candidate" locations in that reference genome.
The similarity measurement, called alignment, formulated as an approximate
string matching problem, is the computational bottleneck because: (1) it is
implemented using quadratic-time dynamic programming algorithms, and (2) the
majority of candidate locations in the reference genome do not align with a
given read due to high dissimilarity. Calculating the alignment of such
incorrect candidate locations consumes an overwhelming majority of a modern
read mapper's execution time. Therefore, it is crucial to develop a fast and
effective filter that can detect incorrect candidate locations and eliminate
them before using computationally costly alignment operations. Results: We
propose GateKeeper, a new hardware accelerator that functions as a
pre-alignment step that quickly filters out most incorrect candidate locations.
GateKeeper is the first design to accelerate pre-alignment using
Field-Programmable Gate Arrays (FPGAs), which can perform pre-alignment much
faster than software. GateKeeper can be integrated with any mapper that
performs sequence alignment for verification. When implemented on a single FPGA
chip, GateKeeper maintains high accuracy (on average &gt;96%) while providing up
to 105-fold and 215-fold speedup over the state-of-the-art software
pre-alignment techniques, Adjacency Filter and Shifted Hamming Distance (SHD),
respectively. Availability: GateKeeper is available at:
https://github.com/BilkentCompGen/GateKeeper.
</p></div>
    </summary>
    <updated>2020-04-17T01:37:39Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-17T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/049</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/049" rel="alternate" type="text/html"/>
    <title>TR20-049 |  Automating Cutting Planes is NP-Hard | 

	Mika Göös, 

	Sajin Koroth, 

	Ian Mertz, 

	Toniann Pitassi</title>
    <summary>We show that Cutting Planes (CP) proofs are hard to find: Given an unsatisfiable formula $F$,

(1) it is NP-hard to find a CP refutation of $F$ in time polynomial in the length of the shortest such refutation; and

(2) unless Gap-Hitting-Set admits a nontrivial algorithm, one cannot find a tree-like CP refutation of $F$ in time polynomial in the length of the shortest such refutation.

The first result extends the recent breakthrough of Atserias and Muller (FOCS 2019) that established an analogous result for Resolution. Our proofs rely on two new lifting theorems: (1) Dag-like lifting for gadgets with many output bits. (2) Tree-like lifting that simulates an $r$-round protocol with gadgets of query complexity $O(\log r)$ independent of input length.</summary>
    <updated>2020-04-16T22:12:13Z</updated>
    <published>2020-04-16T22:12:13Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-04-17T20:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/048</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/048" rel="alternate" type="text/html"/>
    <title>TR20-048 |  Improved lifting theorems via robust sunflowers | 

	Jiapeng Zhang, 

	Shachar Lovett, 

	Raghu Meka</title>
    <summary>Lifting theorems are a generic way to lift lower bounds in query complexity to lower bounds in communication complexity, with applications in diverse areas, such as combinatorial optimization, proof complexity, game theory. Lifting theorems rely on a gadget, where smaller gadgets give stronger lower bounds. However, existing proof techniques are known to require somewhat large gadgets.

We focus on one of the most widely used gadgets, the index gadget. For this gadget, existing lifting techniques are known to require at least a quadratic gadget size. We develop a new approach to prove lifting theorems for the indexing gadget, based on a novel connection to the recently developed robust sunflower lemmas. We show that this allows to reduce the gadget size to linear. We conjecture that it can be further improved to poly logarithmic, similar to the known bounds for the corresponding robust sunflower lemmas.</summary>
    <updated>2020-04-16T17:49:17Z</updated>
    <published>2020-04-16T17:49:17Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-04-17T20:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/047</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/047" rel="alternate" type="text/html"/>
    <title>TR20-047 |  Explicit Uniquely Decodable Codes for Space Bounded Channels That Achieve List-Decoding Capacity | 

	Jad Silbak, 

	Ronen Shaltiel</title>
    <summary>We consider codes for space bounded channels. This is a model for communication under noise that was introduced by Guruswami and Smith (J. ACM 2016) and lies between the Shannon (random) and Hamming (adversarial) models. In this model, a channel is a space bounded procedure that reads the codeword in one pass, and modifies at most a $p$ fraction of the bits of the codeword.

Explicit uniquely decodable codes for space bounded channels: Our main result is that for every $0 \le p \le \frac{1}{4}$, there exists a constant $\delta\ge 0$ and a \emph{uniquely decodable} code that is \emph{explicit} (meaning that encoding and decoding are in poly-time) and has rate $1-H(p)$ for channels with space $n^{\delta}$.

This improves upon previous explicit codes by Guruswami and Smith, and Kopparty, Shaltiel and Silbak (FOCS 2019). Specifically, we obtain the same space and rate as earlier works, even though prior work gave only list-decodable codes (rather than uniquely decodable codes).

Complete characterization of the capacity of space bounded channels: Together with a result by Guruswami and Smith showing the impossibility of unique decoding for $p \ge \frac{1}{4}$, our techniques also give a complete characterization of the capacity $R(p)$ of space $n^{1-o(1)}$ channels, specifically: $R(p)=1-H(p)$ for $0 \le p \le 1/4$ and $R(p)=0$ for $p \ge 1/4$.

In particular, $R(\cdot)$ is not continuous at $p=1/4$. This capacity is strictly larger than the capacity of Hamming channels for every $0 \le p \le \frac{1}{4}$, and matches the capacity of list decoding, and binary symmetric channels in this range.


Our results are incomparable to recent work on casual channels. Casual channels are stronger channels in which the channel reads the codeword in one pass, but there is no space restriction. The best known codes for casual channels, due to Chen, Jaggi and Langberg (STOC 2015), are shown to exist by the probabilistic method, and no explicit codes are known. Furthermore, our results imply that for $p\ge p_0 \approx 0.0804$, there is a separation between the capacities of space bounded channels and casual channels, and the capacity of the former is strictly larger than that of the latter.


Our approach builds on previous explicit list decodable codes for space bounded channels. We introduce and study a notion of ``\emph{evasivenss}'' of codes, which is concerned with whether a decoding algorithm rejects a word that is obtained when a channel induces few errors to a \emph{uniformly chosen} string. We use evasiveness (as well as several additional new ideas related to coding theory and pseudorandomness) to identify the ``correct'' message in the list. Loosely speaking, this is achieved by arguing that on ``incorrect messages'' the decoding algorithm cannot distinguish the codeword from a uniform string.</summary>
    <updated>2020-04-16T10:28:32Z</updated>
    <published>2020-04-16T10:28:32Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-04-17T20:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07220</id>
    <link href="http://arxiv.org/abs/2004.07220" rel="alternate" type="text/html"/>
    <title>Log-Concave Polynomials IV: Exchange Properties, Tight Mixing Times, and Faster Sampling of Spanning Trees</title>
    <feedworld_mtime>1586995200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Anari:Nima.html">Nima Anari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Liu:Kuikui.html">Kuikui Liu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gharan:Shayan_Oveis.html">Shayan Oveis Gharan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vinzant:Cynthia.html">Cynthia Vinzant</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07220">PDF</a><br/><b>Abstract: </b>We prove tight mixing time bounds for natural random walks on bases of
matroids, determinantal distributions, and more generally distributions
associated with log-concave polynomials. For a matroid of rank $k$ on a ground
set of $n$ elements, or more generally distributions associated with
log-concave polynomials of homogeneous degree $k$ on $n$ variables, we show
that the down-up random walk, started from an arbitrary point in the support,
mixes in time $O(k\log k)$. Our bound has no dependence on $n$ or the starting
point, unlike the previous analyses [ALOV19, CGM19], and is tight up to
constant factors. The main new ingredient is a property we call approximate
exchange, a generalization of well-studied exchange properties for matroids and
valuated matroids, which may be of independent interest.
</p>
<p>Additionally, we show how to leverage down-up random walks to approximately
sample spanning trees in a graph with $n$ edges in time $O(n\log^2 n)$,
improving on the almost-linear time algorithm by Schild [Sch18]. Our analysis
works on weighted graphs too, and is the first to achieve nearly-linear running
time.
</p></div>
    </summary>
    <updated>2020-04-16T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07217</id>
    <link href="http://arxiv.org/abs/2004.07217" rel="alternate" type="text/html"/>
    <title>A Small Improvement to the Upper Bound on the Integrality Ratio for the $s-t$ Path TSP</title>
    <feedworld_mtime>1586995200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhong:Xianghui.html">Xianghui Zhong</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07217">PDF</a><br/><b>Abstract: </b>In this paper we investigate the integrality ratio of the standard LP
relaxation for the metric $s-t$ path TSP. We make a near-optimal choice for an
auxiliary function used in the analysis of Traub and Vygen which leads to an
improved upper bound for the integrality ratio of 1.5273.
</p></div>
    </summary>
    <updated>2020-04-16T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07214</id>
    <link href="http://arxiv.org/abs/2004.07214" rel="alternate" type="text/html"/>
    <title>Enumerating minimal dominating sets in the (in)comparability graphs of bounded dimension posets</title>
    <feedworld_mtime>1586995200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bonamy:Marthe.html">Marthe Bonamy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Defrain:Oscar.html">Oscar Defrain</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Micek:Piotr.html">Piotr Micek</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nourine:Lhouari.html">Lhouari Nourine</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07214">PDF</a><br/><b>Abstract: </b>Enumerating minimal transversals in a hypergraph is a notoriously hard
problem. It can be reduced to enumerating minimal dominating sets in a graph,
in fact even to enumerating minimal dominating sets in an incomparability
graph. We provide an output-polynomial time algorithm for incomparability
graphs whose underlying posets have bounded dimension. Through a different
proof technique, we also provide an output-polynomial algorithm for their
complements, i.e., for comparability graphs of bounded dimension posets.
</p>
<p>Our algorithm for incomparability graphs is based on flashlight search and
relies on the geometrical representation of incomparability graphs with bounded
dimension, as given by Golumbic et al. in 1983. It runs with polynomial delay
and only needs polynomial space. Our algorithm for comparability graphs is
based on the flipping method introduced by Golovach et al. in 2015. It performs
in incremental-polynomial time and requires exponential space.
</p>
<p>In addition, we show how to improve the flipping method so that it requires
only polynomial space. Since the flipping method is a key tool for the best
known algorithms enumerating minimal dominating sets in a number of graph
classes, this yields direct improvements on the state of the art.
</p></div>
    </summary>
    <updated>2020-04-16T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07151</id>
    <link href="http://arxiv.org/abs/2004.07151" rel="alternate" type="text/html"/>
    <title>An algorithmic framework for colouring locally sparse graphs</title>
    <feedworld_mtime>1586995200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Davies:Ewan.html">Ewan Davies</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kang:Ross_J=.html">Ross J. Kang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pirot:Fran=ccedil=ois.html">François Pirot</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sereni:Jean=S=eacute=bastien.html">Jean-Sébastien Sereni</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07151">PDF</a><br/><b>Abstract: </b>We develop an algorithmic framework for graph colouring that reduces the
problem to verifying a local probabilistic property of the independent sets.
</p>
<p>With this we give, for any fixed $k\ge 3$ and $\varepsilon&gt;0$, a randomised
polynomial-time algorithm for colouring graphs of maximum degree $\Delta$ in
which each vertex is contained in at most $t$ copies of a cycle of length $k$,
where $1/2\le t\le \Delta^\frac{2\varepsilon}{1+2\varepsilon}/(\log\Delta)^2$,
with $\lfloor(1+\varepsilon)\Delta/\log(\Delta/\sqrt t)\rfloor$ colours.
</p>
<p>This generalises and improves upon several notable results including those of
Kim (1995) and Alon, Krivelevich and Sudakov (1999), and more recent ones of
Molloy (2019) and Achlioptas, Iliopoulos and Sinclair (2019). This bound on the
chromatic number is tight up to an asymptotic factor $2$ and it coincides with
a famous algorithmic barrier to colouring random graphs.
</p></div>
    </summary>
    <updated>2020-04-16T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07118</id>
    <link href="http://arxiv.org/abs/2004.07118" rel="alternate" type="text/html"/>
    <title>Complete Edge-Colored Permutation Graphs</title>
    <feedworld_mtime>1586995200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hartmann:Tom.html">Tom Hartmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bannach:Max.html">Max Bannach</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Middendorf:Martin.html">Martin Middendorf</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stadler:Peter_F=.html">Peter F. Stadler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wieseke:Nicolas.html">Nicolas Wieseke</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hellmuth:Marc.html">Marc Hellmuth</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07118">PDF</a><br/><b>Abstract: </b>We introduce the concept of complete edge-colored permutation graphs as
complete graphs that are the edge-disjoint union of "classical" permutation
graphs. We show that a graph $G=(V,E)$ is a complete edge-colored permutation
graph if and only if each monochromatic subgraph of $G$ is a "classical"
permutation graph and $G$ does not contain a triangle with~$3$ different
colors. Using the modular decomposition as a framework we demonstrate that
complete edge-colored permutation graphs are characterized in terms of their
strong prime modules, which induce also complete edge-colored permutation
graphs. This leads to an $\mathcal{O}(|V|^2)$-time recognition algorithm. We
show, moreover, that complete edge-colored permutation graphs form a superclass
of so-called symbolic ultrametrics and that the coloring of such graphs is
always a Gallai coloring.
</p></div>
    </summary>
    <updated>2020-04-16T23:20:37Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.07058</id>
    <link href="http://arxiv.org/abs/2004.07058" rel="alternate" type="text/html"/>
    <title>Computing Tropical Prevarieties with Satisfiability Modulo Theory (SMT) Solvers</title>
    <feedworld_mtime>1586995200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/L=uuml=ders:Christoph.html">Christoph Lüders</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.07058">PDF</a><br/><b>Abstract: </b>I am presenting a novel way to use SMT (Satisfiability Modulo Theory) to
compute the tropical prevariety (resp. equilibrium) of a polynomial system. The
new method is benchmarked against a naive approach that uses purely polyhedral
methods. It turns out that the SMT approach is faster than the polyhedral
approach for models that would otherwise take more than one minute to compute,
in many cases by a factor of 25 or more. Furthermore, the new approach offers a
way to compute at least parts of the solution if the polyhedral approach is
infeasible.
</p></div>
    </summary>
    <updated>2020-04-16T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-04-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.06898</id>
    <link href="http://arxiv.org/abs/2004.06898" rel="alternate" type="text/html"/>
    <title>Learning sums of powers of low-degree polynomials in the non-degenerate case</title>
    <feedworld_mtime>1586995200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garg:Ankit.html">Ankit Garg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kayal:Neeraj.html">Neeraj Kayal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saha:Chandan.html">Chandan Saha</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.06898">PDF</a><br/><b>Abstract: </b>We develop algorithms for writing a polynomial as sums of powers of low
degree polynomials. Consider an $n$-variate degree-$d$ polynomial $f$ which can
be written as $$f = c_1Q_1^{m} + \ldots + c_s Q_s^{m},$$ where each $c_i\in
\mathbb{F}^{\times}$, $Q_i$ is a homogeneous polynomial of degree $t$, and $t m
= d$. In this paper, we give a $\text{poly}((ns)^t)$-time learning algorithm
for finding the $Q_i$'s given (black-box access to) $f$, if the $Q_i's$ satisfy
certain non-degeneracy conditions and $n$ is larger than $d^2$. The set of
degenerate $Q_i$'s (i.e., inputs for which the algorithm does not work) form a
non-trivial variety and hence if the $Q_i$'s are chosen according to any
reasonable (full-dimensional) distribution, then they are non-degenerate with
high probability (if $s$ is not too large).
</p>
<p>Our algorithm is based on a scheme for obtaining a learning algorithm for an
arithmetic circuit model from a lower bound for the same model, provided
certain non-degeneracy conditions hold. The scheme reduces the learning problem
to the problem of decomposing two vector spaces under the action of a set of
linear operators, where the spaces and the operators are derived from the input
circuit and the complexity measure used in a typical lower bound proof. The
non-degeneracy conditions are certain restrictions on how the spaces decompose.
</p></div>
    </summary>
    <updated>2020-04-16T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.06879</id>
    <link href="http://arxiv.org/abs/2004.06879" rel="alternate" type="text/html"/>
    <title>On the Complexity of the Plantinga-Vegter Algorithm</title>
    <feedworld_mtime>1586995200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cucker:Felipe.html">Felipe Cucker</a>, Alperen A. Ergür, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tonelli=Cueto:Josu=eacute=.html">Josué Tonelli-Cueto</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.06879">PDF</a><br/><b>Abstract: </b>We introduce a general toolbox for precision control and complexity analysis
of subdivision based algorithms in computational geometry. We showcase the
toolbox on a well known example from this family; the adaptive subdivision
algorithm due to Plantinga and Vegter. The only existing complexity estimate on
this rather fast algorithm was an exponential worst-case upper bound for its
interval arithmetic version. We go beyond the worst-case by considering
smoothed analysis, and prove polynomial time complexity estimates for both
interval arithmetic and finite precision versions of the Plantinga-Vegter
algorithm. The employed toolbox is a blend of robust probabilistic techniques
coming from geometric functional analysis with condition numbers and the
continuous amortization paradigm introduced by Burr, Krahmer and Yap. We hope
this combination of tools from different disciplines would prove useful for
understanding complexity aspects of the broad family of subdivision based
algorithms in computational geometry.
</p></div>
    </summary>
    <updated>2020-04-16T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-04-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.06828</id>
    <link href="http://arxiv.org/abs/2004.06828" rel="alternate" type="text/html"/>
    <title>Population Recovery from the Deletion Channel: Nearly Matching Trace Reconstruction Bounds</title>
    <feedworld_mtime>1586995200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Narayanan:Shyam.html">Shyam Narayanan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.06828">PDF</a><br/><b>Abstract: </b>The population recovery problem asks one to recover an unknown distribution
over $n$-bit strings given query access to independent noisy samples of strings
drawn from the distribution. Recently, Ban et. al. [BCF+19] studied the problem
where the unknown distribution over $n$-bit strings is known to be
$\ell$-sparse for some fixed $\ell$, and the noise is induced through the
deletion channel. The deletion channel is a noise model where each bit of the
string is independently deleted with some fixed probability, and the retained
bits are concatenated. We note that if $\ell = 1,$ i.e., we are trying to learn
a single string, learning the distribution is equivalent to the famous trace
reconstruction problem. The best known algorithms for trace reconstruction
require $\exp\left(O(n^{1/3})\right)$ samples.
</p>
<p>For population recovery under the deletion channel, Ban et. al. provided an
algorithm that could learn $\ell$-sparse distributions over strings using
$\exp\left(n^{1/2} \cdot (\log n)^{O(\ell)}\right)$ samples. In this work, we
provide an algorithm that learns the distribution using only
$\exp\big(\tilde{O}(n^{1/3}) \cdot \ell^2\big)$ samples, by developing a
higher-moment analog of the algorithms of [DOS17, NP17]. We also give the first
algorithm with a runtime subexponential in $n$, which solves population
recovery in $\exp\big(\tilde{O}(n^{1/3}) \cdot \ell^3\big)$ samples and time.
</p>
<p>Notably, our dependence on $n$ nearly matches the known upper bound when
$\ell = 1$, and we reduce the dependence on $\ell$ from doubly to nearly singly
exponential. Therefore, we are able to learn the mixture even for much larger
values of $\ell$. For instance, Ban et. al.'s algorithm can only learn a
mixture of $O(\log n/\log \log n)$ strings with a subexponential number of
queries, whereas we are able to learn a mixture of up to $n^{o(1)}$ strings in
subexponential queries and time.
</p></div>
    </summary>
    <updated>2020-04-16T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-04-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2004.06776</id>
    <link href="http://arxiv.org/abs/2004.06776" rel="alternate" type="text/html"/>
    <title>The Circumbilliard: Any Triangle can be a 3-Periodic</title>
    <feedworld_mtime>1586995200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Reznik:Dan.html">Dan Reznik</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garcia:Ronaldo.html">Ronaldo Garcia</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2004.06776">PDF</a><br/><b>Abstract: </b>A Circumconic passes through a triangle's vertices. We define the
Circumbilliard, a circumellipse to a generic triangle for which the latter is a
3-periodic. We study its properties and associated loci.
</p></div>
    </summary>
    <updated>2020-04-16T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-04-16T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/04/15/linkage</id>
    <link href="https://11011110.github.io/blog/2020/04/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Mathematics as a team sport (). What a week-long research workshop at Oberwolfach (or Dagstuhl, or many similar retreats) can be like. The workshop in the link is on low-dimensional topology, but the story would be the same for many other subjects. In late March, instead of attending a Bellairs workshop, we all collaborated remotely. I think we got a fair amount of research accomplished, but I didn’t have the same sense of all being brought together to do that one thing.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://www.quantamagazine.org/mathematics-as-a-team-sport-20200331/">Mathematics as a team sport</a> (<a href="https://mathstodon.xyz/@11011110/103927542292984705"/>). What a week-long research workshop at Oberwolfach (or Dagstuhl, or many similar retreats) can be like. The workshop in the link is on low-dimensional topology, but the story would be the same for many other subjects. In late March, instead of attending a Bellairs workshop, we all collaborated remotely. I think we got a fair amount of research accomplished, but I didn’t have the same sense of all being brought together to do that one thing.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2003.11832">Semidefinite programming bounds for the average kissing number</a> (<a href="https://mathstodon.xyz/@11011110/103932970304148800"/>). Spheres kiss by touching with no overlap. The kissing number is how many unit spheres can touch a central one, and lattice kissing number is how many can touch in a lattice packing; both are 12 in 3d. Average kissing number is for finitely many non-unit spheres. It is ≥ lattice kissing number and ≤ 2x kissing number. One of my papers has a slightly better lower bound in 3d, and now we have better upper bounds in many dimensions.</p>
  </li>
  <li>
    <p><a href="https://mathsedideas.blogspot.com/p/resources.html#RAMs">320 Random Acts of Maths</a> (<a href="https://mathstodon.xyz/@antoinechambertloir/103920121638187355"/>). Pocket-sized problems, teasers, curios, provocations, inspirations, etc.</p>
  </li>
  <li>
    <p><a href="https://www.nature.com/articles/d41586-020-00998-2">Mochizuki will publish his purported proof of the abc conjecture in the journal of which he is editor in chief</a> (<a href="https://mathstodon.xyz/@11011110/103941722283312831"/>, <a href="https://retractionwatch.com/2020/04/04/weekend-reads-covid-19-and-peer-review-blaming-a-spell-checker-for-plagiarism-the-fastest-retracting-country/">via</a>). “The latest announcement seems unlikely to move many researchers over to Mochizuki’s camp.”</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Ring_lemma">The ring lemma</a> (<a href="https://mathstodon.xyz/@11011110/103950535538050348"/>). New Wikipedia article on the ratio between sizes of adjacent circles in a circle packing.</p>
  </li>
  <li>
    <p><a href="https://sites.google.com/site/calculatinghistory/home/computing-linkages">Computing linkages</a> (<a href="https://mathstodon.xyz/@esoterica/103950724802083262"/>). Article by Andries de Man on computing devices built with hinged rods instead of electronics or gears.</p>
  </li>
  <li>
    <p><a href="https://bl.ocks.org/robinhouston/6096950">Doyle spiral explorer</a> (<a href="https://mathstodon.xyz/@11011110/103961199355813355"/>). If you slur “Doyle spiral” enough it kind of sounds like “Dora”. You can also <a href="https://observablehq.com/@mbostock/double-doyle-spiral">Möbius transform these things and get double spirals</a>.</p>
  </li>
  <li>
    <p>This photo (<a href="https://mathstodon.xyz/@11011110/103965972080045328"/>) is actually from a year ago but I neglected to upload it then and only rediscovered it recently while attempting to explain to an older relative, over the phone, how to attach images to text messages. It’s a shallow Showa bowl from Japan, bought when my wife and I visited Tokyo three years ago. We usually hold fruit in it; at the time I posted, it held three bananas and three lemons.</p>

    <p style="text-align: center;"><a href="https://www.ics.uci.edu/~eppstein/pix/radialbowl/index.html"><img alt="Showa bowl from above" src="https://www.ics.uci.edu/~eppstein/pix/radialbowl/RadialBowl-m.jpg" style="border-style: solid; border-color: black;"/></a></p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/C._Doris_Hellman">C. Doris Hellman, historian of astronomy</a> (<a href="https://mathstodon.xyz/@11011110/103970811680488005"/>), now a Good Article on Wikipedia. Through Hellman’s work on the Great Comet of 1577, historians came to see how Tycho Brahe’s observations of the comet moving unobstructed through translunar space <a href="https://en.wikipedia.org/wiki/Copernican_Revolution">became key evidence for heliocentrism and against the geocentric model of crystal spheres holding up the planets</a>. She also translated the definitive biography of Johannes Kepler from German into English.</p>
  </li>
  <li>
    <p>Found in Friedman’s <em>A History of Folding in Mathematics</em>, p. 71, a quote from Francesco Maurolico from <span style="white-space: nowrap;">1537 (<a href="https://mathstodon.xyz/@11011110/103978188972978011"/>):</span> “Item manifestum est in unoquoque regularium solidorum, numerum basium coniunctum cum numero cacuminum conflare numerum, qui binario excedit numerum laterum”. Except for the fact that he considers only Platonic solids, this is Euler’s formula  for convex polyhedra (in the equivalent form ), long before Euler (1752) and Descartes (1630).</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Convex_hull">Convex hull</a> (<a href="https://mathstodon.xyz/@11011110/103981026707788374"/>), now a Good Article on Wikipedia.</p>
  </li>
  <li>
    <p>Appreciations of John Conway, following his death from the coronavirus (<a href="https://mathstodon.xyz/@11011110/103987909082094935"/>):</p>

    <ul>
      <li>
        <p><a href="https://cameroncounts.wordpress.com/2020/04/12/john-conway/">Peter Cameron</a></p>
      </li>
      <li>
        <p><a href="https://terrytao.wordpress.com/2020/04/12/john-conway/">Terry Tao</a></p>
      </li>
      <li>
        <p><a href="https://www.scottaaronson.com/blog/?p=4732">Scott Aaronson</a></p>
      </li>
      <li>
        <p><a href="https://www.solipsys.co.uk/new/RememberingConway.html?td12mn">Colin Wright</a></p>
      </li>
      <li>
        <p><a href="https://rjlipton.wordpress.com/2020/04/14/john-horton-conway-1937-2020/">Richard Lipton and Ken Regan</a></p>
      </li>
      <li>
        <p><a href="https://xkcd.com/2293/">xkcd</a></p>
      </li>
      <li>
        <p><em><a href="http://www.theguardian.com/science/2015/jul/23/john-horton-conway-the-most-charismatic-mathematician-in-the-world">The Guardian</a></em></p>
      </li>
    </ul>
  </li>
  <li>
    <p><a href="http://www.dam.brown.edu/people/mumford/blog/2015/WakeUp.html">Wake up!</a> (<a href="https://mathstodon.xyz/@11011110/103998469608885562"/>, <a href="https://en.wikipedia.org/wiki/Wikipedia:Articles_for_deletion/A_K_Peters">via</a>). A five-year-old blog post by David Mumford on the problems of corporate takeover of academic publishing, and the ensuing destruction of traditional author-editor relations.</p>
  </li>
  <li>
    <p><a href="http://utf8everywhere.org/">utf8 everywhere</a> (<a href="https://mathstodon.xyz/@11011110/104001143871939014"/>, <a href="https://news.ycombinator.com/item?id=22867503">via</a>). Mostly a manifesto about how we should be using utf8 instead of utf16 for unicode text files. But I think the sentiment that we should be treating utf8 as the default instead of ascii (which still is the actual default in many situations) is also valid.</p>
  </li>
  <li>
    <p>In <a href="https://11011110.github.io/blog/2020/03/29/backyard-sunlight.html">the previous batch of photos</a> the stripy shadows were artificial (caused by the slats of a trellis shading our patio) but this time they’re natural: they come from the fronds of one of the palm trees in our garden, shading the trunk of the tree (<a href="https://mathstodon.xyz/@11011110/104005420231873385"/>).</p>

    <p style="text-align: center;"><a href="https://www.ics.uci.edu/~eppstein/pix/palmshadow/index.html"><img alt="Palm tree casts shadows on itself" src="https://www.ics.uci.edu/~eppstein/pix/palmshadow/PalmShadow-m.jpg" style="border-style: solid; border-color: black;"/></a></p>
  </li>
</ul></div>
    </content>
    <updated>2020-04-15T21:54:00Z</updated>
    <published>2020-04-15T21:54:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2020-04-16T19:17:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/046</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/046" rel="alternate" type="text/html"/>
    <title>TR20-046 |  A Robust Version of Heged\H{u}s&amp;#39;s Lemma, with Applications | 

	Srikanth Srinivasan</title>
    <summary>Heged\H{u}s's lemma is the following combinatorial statement regarding polynomials over finite fields. Over a field $\mathbb{F}$ of characteristic $p &gt; 0$ and for $q$ a  power of $p$, the lemma says that any multilinear polynomial $P\in \mathbb{F}[x_1,\ldots,x_n]$ of degree less than $q$ that vanishes at all points in $\{0,1\}^n$ of Hamming weight $k\in [q,n-q]$ must also vanish at all points in $\{0,1\}^n$ of weight $k + q$. This lemma was used by Heged\H{u}s (2009) to give a solution to \emph{Galvin's problem}, an extremal problem about set systems; by Alon, Kumar and Volk (2018) to improve the best-known multilinear circuit lower bounds; and by Hrube\v{s}, Ramamoorthy, Rao and Yehudayoff (2019) to prove optimal lower bounds against depth-$2$ threshold circuits for computing some symmetric functions. 
		
		In this paper, we formulate a robust version of Heged\H{u}s's lemma. Informally, this version says that if a polynomial of degree $o(q)$ vanishes at most points of weight $k$, then it vanishes at many points of weight $k+q$. We prove this lemma and give the following three different applications.
		
		1. Degree lower bounds for the coin problem: The \emph{$\delta$-Coin Problem} is the problem of distinguishing between a coin that is heads with probability $((1/2) + \delta)$ and a coin that is heads with probability $1/2$. We show that over a field of positive (fixed) characteristic, any polynomial that solves the $\delta$-coin problem with error $\varepsilon$ must have degree $\Omega(\frac{1}{\delta}\log(1/\varepsilon)),$ which is tight up to constant factors.
		
		2. Probabilistic degree lower bounds: The \emph{Probabilistic degree} of a Boolean function is the minimum $d$ such that there is a random polynomial of degree $d$ that agrees with the function at each point with high probability. We give tight lower bounds on the probabilistic degree of \emph{every} symmetric Boolean function over positive (fixed) characteristic. As far as we know, this was not known even for some very simple functions such as unweighted Exact Threshold functions, and constant error.
		
		3. A robust version of the combinatorial result of Heged\H{u}s (2009) mentioned above.</summary>
    <updated>2020-04-15T16:25:07Z</updated>
    <published>2020-04-15T16:25:07Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-04-17T20:20:25Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-104815394733465706</id>
    <link href="https://blog.computationalcomplexity.org/feeds/104815394733465706/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/04/theoretical-computer-science-for-future.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/104815394733465706" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/104815394733465706" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/04/theoretical-computer-science-for-future.html" rel="alternate" type="text/html"/>
    <title>Theoretical Computer Science for the Future</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><i>Guest post by the TCS4F initiative
(Antoine Amarilli, Thomas Colcombet, Hugo Férée, Thomas Schwentick) </i><div><br/></div><div><a href="https://tcs4f.org/">TCS4F</a> is an initiative by theoretical computer scientists who are
concerned about that other major crisis of our time: climate change. We
anticipate that the climate crisis will be a major challenge of the
decades to come, that it will require major changes at all levels of
society to mitigate the harm that it will cause, and that researchers in
theoretical computer science, like all other actors, must be part of the
solution and not part of the problem.</div>
<div><br/></div><div>Our initiative is to propose a <a href="https://tcs4f.org/">manifesto</a> to commit
to a reduction of greenhouse gas emissions: following <a href="https://www.ipcc.ch/2018/10/08/summary-for-policymakers-of-ipcc-special-report-on-global-warming-of-1-5c-approved-by-governments/">IPCC goals</a>, the
objective is to reduce by at least 50% before 2030 relative to pre-2020
levels. The manifesto is more than a simple expression of concern,
because it is a pledge with concrete objectives. However, it does not
prescribe specific measures, as we believe this discussion is not
settled yet and the right steps to take can vary depending on everyone's
practices. </div><div><br/>
The manifesto can be signed by individual researchers (like you, dear
reader!), by research groups, and by organizers of conferences and
workshops. Currently, over 50 researchers have signed it. The goal of
TCS4F is also to start organizing a community of concerned researchers,
across theoretical computer science, to think about the issue of climate
change and how to adjust what we do, in particular our travel habits. </div><div><br/>
We need your help to make this initiative a success and help theoretical
CS lead the way towards a sustainable, carbon-neutral future:</div><div><ul style="text-align: left;"><li>If you agree with our concerns and are ready to commit to reducing
 your carbon footprint, consider <a href="https://tcs4f.org/">signing the manifesto</a>. Signing is open to all researchers in
 theoretical CS in the broadest possible sense.</li><li>Advertise your support of the manifesto (e.g., by putting one of our
 badges on your webpage). Talk in your research teams and departments
 about the manifesto, and see if you can gather support for signing the
 manifesto collectively as a research group.</li><li>If you are involved in conferences and workshops, start a discussion
 about the carbon footprint of the event, and whether the event could
 commit to the manifesto's goal. Indeed, now that conferences across
 the globe are moving online because of the COVID-19 pandemic, it is a
 good time to discuss how conferences could evolve towards more
 sustainable models.</li><li>Spread the word about the issue of climate change and the TCS4F
 initiative, and encourage discussion of this important challenge in
 our communities. </li></ul></div><div>
As theoretical researchers, we are not used to discussing uncomfortable
non-scientific questions like the effects of our activities on the
world. However, we believe that the magnitude of the climate crisis
obliges us to act now as a community. We are confident that great
changes can be achieved if we do not limit our creativity to our
specific research areas and also use it to re-think our way to do
research.
<br/></div></div>
    </content>
    <updated>2020-04-15T13:42:00Z</updated>
    <published>2020-04-15T13:42:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-04-17T20:20:08Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/045</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/045" rel="alternate" type="text/html"/>
    <title>TR20-045 |  Learning sums of powers of low-degree polynomials in the non-degenerate case | 

	Ankit Garg, 

	Neeraj Kayal, 

	Chandan Saha</title>
    <summary>We develop algorithms for writing a polynomial as sums of powers of low degree polynomials. Consider an $n$-variate degree-$d$ polynomial $f$ which can be written as
$$f = c_1Q_1^{m} + \ldots + c_s Q_s^{m},$$
where each $c_i\in \mathbb{F}^{\times}$, $Q_i$ is a homogeneous polynomial of degree $t$, and $t m = d$. In this paper, we give a $\text{poly}((ns)^t)$-time learning algorithm for finding the $Q_i$'s given (black-box access to) $f$, if the $Q_i's$ satisfy certain non-degeneracy conditions and $n$ is larger than $d^2$. The set of degenerate $Q_i$'s (i.e., inputs for which the algorithm does not work) form a non-trivial variety and hence if the $Q_i$'s are chosen according to any reasonable (full-dimensional) distribution, then they are non-degenerate with high probability (if $s$ is not too large). This problem generalizes symmetric tensor decomposition, which corresponds to the $t = 1$ case and is widely studied, having many applications in machine learning. Our algorithm (for $t=2$) allows us to solve the moment problem for mixtures of zero-mean Gaussians in the non-degenerate case.

Our algorithm is based on a scheme for obtaining a learning algorithm for an arithmetic circuit model from a lower bound for the same model, provided certain non-degeneracy conditions hold. The scheme reduces the learning problem to the problem of decomposing two vector spaces under the action of a set of linear operators, where the spaces and the operators are derived from the input circuit and the complexity measure used in a typical lower bound proof. The non-degeneracy conditions are certain restrictions on how the spaces decompose. Such a scheme is present in a rudimentary form in an earlier work of Kayal and Saha. Here, we make it more general and detailed, and potentially applicable to learning other circuit models.

An exponential lower bound for the representation above (also known as homogeneous $\Sigma \wedge \Sigma \Pi^{[t]}$ circuits) is known using the shifted partials measure. However, the number of linear operators in shifted partials is exponential and also the non-degeneracy condition emerging out of this measure is unlikely to be satisfied by a random $\Sigma \wedge \Sigma \Pi^{[t]}$ circuit when the number of variables is large with respect to the degree. We bypass this hurdle by proving a lower bound (which is nearly as strong as the previous bound) using a novel variant of the partial derivatives measure, namely affine projections of partials (APP). The non-degeneracy conditions appearing from this new measure are satisfied by a random $\Sigma \wedge \Sigma \Pi^{[t]}$ circuit. The APP measure could be of independent interest for proving other lower bounds.</summary>
    <updated>2020-04-15T07:50:58Z</updated>
    <published>2020-04-15T07:50:58Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-04-17T20:20:25Z</updated>
    </source>
  </entry>
</feed>
