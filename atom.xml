<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2020-07-22T01:53:42Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10924</id>
    <link href="http://arxiv.org/abs/2007.10924" rel="alternate" type="text/html"/>
    <title>Partial Boolean functions with exact quantum 1-query complexity</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Guoliang.html">Guoliang Xu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/q/Qiu:Daowen.html">Daowen Qiu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10924">PDF</a><br/><b>Abstract: </b>We provide two sufficient and necessary conditions to characterize any
$n$-bit partial Boolean function with exact quantum 1-query complexity. Using
the first characterization, we present all $n$-bit partial Boolean functions
that depend on $n$ bits and have exact quantum 1-query complexity. Due to the
second characterization, we construct a function $F$ that maps any $n$-bit
partial Boolean function to some integer, and if an $n$-bit partial Boolean
function $f$ depends on $k$ bits and has exact quantum 1-query complexity, then
$F(f)$ is non-positive. In addition, we show that the number of all $n$-bit
partial Boolean functions that depend on $k$ bits and have exact quantum
1-query complexity is not bigger than $n^{2}2^{2^{n-1}(1+2^{2-k})+2n^{2}}$ for
all $n\geq 3$ and $k\geq 2$.
</p></div>
    </summary>
    <updated>2020-07-22T01:20:45Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10898</id>
    <link href="http://arxiv.org/abs/2007.10898" rel="alternate" type="text/html"/>
    <title>Static and Streaming Data Structures for Fr\'echet Distance Queries</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Filtser:Arnold.html">Arnold Filtser</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Filtser:Omrit.html">Omrit Filtser</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10898">PDF</a><br/><b>Abstract: </b>Given a curve $P$ with points in $\mathbb{R}^d$ in a streaming fashion, and
parameters $\varepsilon&gt;0$ and $k$, we construct a distance oracle that uses
$O(\frac{1}{\varepsilon})^{kd}\log\varepsilon^{-1}$ space, and given a query
curve $Q$ with $k$ points in $\mathbb{R}^d$, returns in $\tilde{O}(kd)$ time a
$1+\varepsilon$ approximation of the discrete Fr\'echet distance between $Q$
and $P$.
</p>
<p>In addition, we construct simplifications in the streaming model, oracle for
distance queries to a sub-curve (in the static setting), and introduce the
zoom-in problem. Our algorithms work in any dimension $d$, and therefore we
generalize some useful tools and algorithms for curves under the discrete
Fr\'echet distance to work efficiently in high dimensions.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10863</id>
    <link href="http://arxiv.org/abs/2007.10863" rel="alternate" type="text/html"/>
    <title>Outer approximations of core points for integer programming</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bremner:David.html">David Bremner</a>, Naghmeh Shahverdi <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10863">PDF</a><br/><b>Abstract: </b>For several decades the dominant techniques for integer linear programming
have been branching and cutting planes. Recently, several authors have
developed core point methods for solving symmetric integer linear programs
(ILPs). An integer point is called a core point if its orbit polytope is
lattice-free. It has been shown that for symmetric ILPs, optimizing over the
set of core points gives the same answer as considering the entire space.
Existing core point techniques rely on the number of core points (or
equivalence classes) being finite, which requires special symmetry groups. In
this paper we develop some new methods for solving symmetric ILPs (based on
outer approximations of core points) that do not depend on finiteness but are
more efficient if the group has large disjoint cycles in its set of generators.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10857</id>
    <link href="http://arxiv.org/abs/2007.10857" rel="alternate" type="text/html"/>
    <title>Smoothed Complexity of 2-player Nash Equilibria</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Boodaghians:Shant.html">Shant Boodaghians</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Brakensiek:Joshua.html">Joshua Brakensiek</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hopkins:Samuel_B=.html">Samuel B. Hopkins</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rubinstein:Aviad.html">Aviad Rubinstein</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10857">PDF</a><br/><b>Abstract: </b>We prove that computing a Nash equilibrium of a two-player ($n \times n$)
game with payoffs in $[-1,1]$ is PPAD-hard (under randomized reductions) even
in the smoothed analysis setting, smoothing with noise of constant magnitude.
This gives a strong negative answer to conjectures of Spielman and Teng [ST06]
and Cheng, Deng, and Teng [CDT09].
</p>
<p>In contrast to prior work proving PPAD-hardness after smoothing by noise of
magnitude $1/\operatorname{poly}(n)$ [CDT09], our smoothed complexity result is
not proved via hardness of approximation for Nash equilibria. This is by
necessity, since Nash equilibria can be approximated to constant error in
quasi-polynomial time [LMM03]. Our results therefore separate smoothed
complexity and hardness of approximation for Nash equilibria in two-player
games.
</p>
<p>The key ingredient in our reduction is the use of a random zero-sum game as a
gadget to produce two-player games which remain hard even after smoothing. Our
analysis crucially shows that all Nash equilibria of random zero-sum games are
far from pure (with high probability), and that this remains true even after
smoothing.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10824</id>
    <link href="http://arxiv.org/abs/2007.10824" rel="alternate" type="text/html"/>
    <title>Parameter estimation for Gibbs distributions</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Harris:David_G=.html">David G. Harris</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kolmogorov:Vladimir.html">Vladimir Kolmogorov</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10824">PDF</a><br/><b>Abstract: </b>We consider \emph{Gibbs distributions}, which are families of probability
distributions over a discrete space $\Omega$ with probability mass function of
the form $\mu^\Omega_\beta(x) \propto e^{\beta H(x)}$ for $\beta$ in an
interval $[\beta_{\min}, \beta_{\max}]$ and $H(x) \in \{0 \} \cup [1, n]$. The
\emph{partition function} is the normalization factor
$Z(\beta)=\sum_{x\in\Omega}e^{\beta H(x)}$.
</p>
<p>Two important parameters of these distributions are the partition ratio $q =
\log \tfrac{Z(\beta_{\max})}{Z(\beta_{\min})}$ and the counts $c_x =
|H^{-1}(x)|$ for each value $x$. These are correlated with system parameters in
a number of physical applications and sampling algorithms. Our first main
result is to estimate the values $c_x$ using roughly $\tilde O(
\frac{q}{\varepsilon^2})$ samples for general Gibbs distributions and $\tilde
O( \frac{n^2}{\varepsilon^2} )$ samples for integer-valued distributions
(ignoring some second-order terms and parameters), and we show this is optimal
up to logarithmic factors. We illustrate with improved algorithms for counting
connected subgraphs and perfect matchings in a graph.
</p>
<p>As a key subroutine, we estimate the partition function $Z$ using $\tilde
O(\frac{q}{\varepsilon^2})$ samples for general Gibbs distributions and $\tilde
O(\frac{n^2}{\varepsilon^2})$ samples for integer-valued distributions. We
construct a data structure capable of estimating $Z(\beta)$ for \emph{all}
values $\beta$, without further samples. This improves over a prior algorithm
of Kolmogorov (2018) which computes the single point estimate $Z(\beta_{\max})$
using $\tilde O(\frac{q}{\varepsilon^2})$ samples. We show matching lower
bounds, demonstrating that this complexity is optimal as a function of $n$ and
$q$ up to logarithmic terms.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10673</id>
    <link href="http://arxiv.org/abs/2007.10673" rel="alternate" type="text/html"/>
    <title>Quantum and Classical Hybrid Generations for Classical Correlations</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Xiaodie Lin, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wei:Zhaohui.html">Zhaohui Wei</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yao:Penghui.html">Penghui Yao</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10673">PDF</a><br/><b>Abstract: </b>We consider two-stage hybrid protocols that combine quantum resource and
classical resource to generate classical correlations shared by two separated
players. Our motivation is twofold. First, in the near future the scale of
quantum information processing is quite limited, and when quantum resource
available is not sufficient for certain tasks, a possible way to strengthen the
capability of quantum schemes is introducing extra classical resource. We
analyze the mathematical structures of these hybrid protocols, and characterize
the relation between the amount of quantum resource and classical resource
needed. Second, a fundamental open problem in communication complexity theory
is to describe the advantages of sharing prior quantum entanglement over
sharing prior randomness, which is still widely open. It turns out that our
quantum and classical hybrid protocols provide new insight into this important
problem.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10658</id>
    <link href="http://arxiv.org/abs/2007.10658" rel="alternate" type="text/html"/>
    <title>A family of non-periodic tilings of the plane by right golden triangles</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Nikolay Vereshchagin <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10658">PDF</a><br/><b>Abstract: </b>We consider tilings of the plane by two prototiles which are right triangles.
They are called the small and the large tiles. The small tile is similar to the
large tile with some similarity coefficient $\psi$. The large tile can be cut
into two pieces so that one piece is a small tile and the other one is similar
to the small tile with the same similarity coefficient $\psi$. Using this cut
we define in a standard way the substitution scheme, in which the large tile is
replaced by a large and a small tile and the small tile is replaced by a large
tile. To every substitution of this kind, there corresponds a family of the
so-called substitution tilings of the plane in the sense of [C.
Goodman-Strauss, Matching Rules and Substitution Tilings, Annals of Mathematics
147 (1998) 181-223]. All tilings in this family are non-periodic. It was shown
in the paper [N. Vereshchagin. Aperiodic Tilings by Right Triangles. In: Proc.
of DCFS 2014, LNCS vol. 8614 (2014) 29--41] that this family of substitution
tilings is not an SFT. This means that looking at a given tiling trough a
bounded window, we cannot determine whether that tiling belongs to the family
or not, however large the size of the window is.
</p>
<p>In the present paper, we prove that this family of substitution tilings is
sofic. This means that we can color the prototiles ina finite number of colors
and define some local rules for colored prototiles so that the following holds.
For any tiling from the family, we can color its tiles so that the resulting
tiling (by colored tiles) satisfies local rules. And conversely, for any tiling
of the plane satisfying the local rules, by removing colors we obtain a tiling
from the family. Besides, the considered substitution can be generalized to
colored tiles so that the family of substitution tilings for the resulting
substitution coincides with the family of tilings satisfying our local rules.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10632</id>
    <link href="http://arxiv.org/abs/2007.10632" rel="alternate" type="text/html"/>
    <title>Rational homotopy type and computability</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Manin:Fedor.html">Fedor Manin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10632">PDF</a><br/><b>Abstract: </b>Given a simplicial pair $(X,A)$, a simplicial complex $Y$, and a map $f:A \to
Y$, does $f$ have an extension to $X$? We show that for a fixed $Y$, this
question is algorithmically decidable for all $X$, $A$, and $f$ if and only if
$Y$ has the rational homotopy type of an H-space. As a corollary, many
questions related to bundle structures over a finite complex are decidable.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10622</id>
    <link href="http://arxiv.org/abs/2007.10622" rel="alternate" type="text/html"/>
    <title>Online Discrepancy Minimization for Stochastic Arrivals</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bansal:Nikhil.html">Nikhil Bansal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jiang:Haotian.html">Haotian Jiang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meka:Raghu.html">Raghu Meka</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singla:Sahil.html">Sahil Singla</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sinha:Makrand.html">Makrand Sinha</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10622">PDF</a><br/><b>Abstract: </b>In the stochastic online vector balancing problem, vectors
$v_1,v_2,\ldots,v_T$ chosen independently from an arbitrary distribution in
$\mathbb{R}^n$ arrive one-by-one and must be immediately given a $\pm$ sign.
The goal is to keep the norm of the discrepancy vector, i.e., the signed
prefix-sum, as small as possible for a given target norm.
</p>
<p>We consider some of the most well-known problems in discrepancy theory in the
above online stochastic setting, and give algorithms that match the known
offline bounds up to $\mathsf{polylog}(nT)$ factors. This substantially
generalizes and improves upon the previous results of Bansal, Jiang, Singla,
and Sinha (STOC' 20). In particular, for the Koml\'{o}s problem where
$\|v_t\|_2\leq 1$ for each $t$, our algorithm achieves $\tilde{O}(1)$
discrepancy with high probability, improving upon the previous
$\tilde{O}(n^{3/2})$ bound. For Tusn\'{a}dy's problem of minimizing the
discrepancy of axis-aligned boxes, we obtain an $O(\log^{d+4} T)$ bound for
arbitrary distribution over points. Previous techniques only worked for product
distributions and gave a weaker $O(\log^{2d+1} T)$ bound. We also consider the
Banaszczyk setting, where given a symmetric convex body $K$ with Gaussian
measure at least $1/2$, our algorithm achieves $\tilde{O}(1)$ discrepancy with
respect to the norm given by $K$ for input distributions with sub-exponential
tails.
</p>
<p>Our key idea is to introduce a potential that also enforces constraints on
how the discrepancy vector evolves, allowing us to maintain certain
anti-concentration properties. For the Banaszczyk setting, we further enhance
this potential by combining it with ideas from generic chaining. Finally, we
also extend these results to the setting of online multi-color discrepancy.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10545</id>
    <link href="http://arxiv.org/abs/2007.10545" rel="alternate" type="text/html"/>
    <title>Online Carpooling using Expander Decompositions</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gupta:Anupam.html">Anupam Gupta</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Krishnaswamy:Ravishankar.html">Ravishankar Krishnaswamy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kumar:Amit.html">Amit Kumar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singla:Sahil.html">Sahil Singla</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10545">PDF</a><br/><b>Abstract: </b>We consider the online carpooling problem: given $n$ vertices, a sequence of
edges arrive over time. When an edge $e_t = (u_t, v_t)$ arrives at time step
$t$, the algorithm must orient the edge either as $v_t \rightarrow u_t$ or $u_t
\rightarrow v_t$, with the objective of minimizing the maximum discrepancy of
any vertex, i.e., the absolute difference between its in-degree and out-degree.
Edges correspond to pairs of persons wanting to ride together, and orienting
denotes designating the driver. The discrepancy objective then corresponds to
every person driving close to their fair share of rides they participate in.
</p>
<p>In this paper, we design efficient algorithms which can maintain
polylog$(n,T)$ maximum discrepancy (w.h.p) over any sequence of $T$ arrivals,
when the arriving edges are sampled independently and uniformly from any given
graph $G$. This provides the first polylogarithmic bounds for the online
(stochastic) carpooling problem. Prior to this work, the best known bounds were
$O(\sqrt{n \log n})$-discrepancy for any adversarial sequence of arrivals, or
$O(\log\!\log n)$-discrepancy bounds for the stochastic arrivals when $G$ is
the complete graph.
</p>
<p>The technical crux of our paper is in showing that the simple greedy
algorithm, which has provably good discrepancy bounds when the arriving edges
are drawn uniformly at random from the complete graph, also has polylog
discrepancy when $G$ is an expander graph. We then combine this with known
expander-decomposition results to design our overall algorithm.
</p></div>
    </summary>
    <updated>2020-07-22T01:32:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10470</id>
    <link href="http://arxiv.org/abs/2007.10470" rel="alternate" type="text/html"/>
    <title>tight approximations for modular and submodular optimization with $d$-resource multiple knapsack constraints</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fairstein:Yaron.html">Yaron Fairstein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kulik:Ariel.html">Ariel Kulik</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shachnai:Hadas.html">Hadas Shachnai</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10470">PDF</a><br/><b>Abstract: </b>A multiple knapsack constraint over a set of items is defined by a set of
bins of arbitrary capacities, and a weight for each of the items. An assignment
for the constraint is an allocation of subsets of items to the bins, such that
the total weight of items assigned to each bin is bounded by the bin capacity.
We study modular (linear) and submodular maximization problems subject to a
constant number of (i.e., $d$-resource) multiple knapsack constraints, in which
a solution is a subset of items, along with an assignment of the selected items
for each of the $d$ multiple knapsack constraints.
</p>
<p>Our results include a {\em polynomial time approximation scheme} for modular
maximization with a constant number of multiple knapsack constraints and a
matroid constraint, thus generalizing the best known results for the classic
{\em multiple knapsack problem} as well as {\em d-dimensional knapsack}, for
any $d \geq 2$. We further obtain a tight $(1-e^{-1}-\epsilon)$-approximation
for monotone submodular optimization subject to a constant number of multiple
knapsack constraints and a matroid constraint, and a
$(0.385-\epsilon)$-approximation for non-monotone submodular optimization
subject to a constant number of multiple knapsack constraints. At the heart of
our algorithms lies a novel representation of a multiple knapsack constraint as
a polytope. We consider this key tool as a main technical contribution of this
paper.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10430</id>
    <link href="http://arxiv.org/abs/2007.10430" rel="alternate" type="text/html"/>
    <title>A Survey of Algorithms for Geodesic Paths and Distances</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Crane:Keenan.html">Keenan Crane</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Livesu:Marco.html">Marco Livesu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Puppo:Enrico.html">Enrico Puppo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/q/Qin:Yipeng.html">Yipeng Qin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10430">PDF</a><br/><b>Abstract: </b>Numerical computation of shortest paths or geodesics on curved domains, as
well as the associated geodesic distance, arises in a broad range of
applications across digital geometry processing, scientific computing, computer
graphics, and computer vision. Relative to Euclidean distance computation,
these tasks are complicated by the influence of curvature on the behavior of
shortest paths, as well as the fact that the representation of the domain may
itself be approximate. In spite of the difficulty of this problem, recent
literature has developed a wide variety of sophisticated methods that enable
rapid queries of geodesic information, even on relatively large models. This
survey reviews the major categories of approaches to the computation of
geodesic paths and distances, highlighting common themes and opportunities for
future improvement.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.10331</id>
    <link href="http://arxiv.org/abs/2007.10331" rel="alternate" type="text/html"/>
    <title>Evolution toward a Nash equilibrium</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Avramopoulos:Ioannis.html">Ioannis Avramopoulos</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.10331">PDF</a><br/><b>Abstract: </b>In this paper, we study the dynamic behavior of Hedge, a well-known algorithm
in theoretical machine learning and algorithmic game theory. The empirical
average (arithmetic mean) of the iterates Hedge generates is known to converge
to a minimax equilibrium in zero-sum games. We generalize that result to show
convergence of the empirical average to Nash equilibrium in symmetric bimatrix
games (that is bimatrix games where the payoff matrix of each player is the
transpose of that of the other) in the sense that every limit point of the
sequence of averages is an $\epsilon$-approximate symmetric equilibrium
strategy for any desirable $\epsilon$. Our analysis gives rise to a symmetric
equilibrium fully polynomial-time approximation scheme, implying P = PPAD.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.09793</id>
    <link href="http://arxiv.org/abs/2007.09793" rel="alternate" type="text/html"/>
    <title>$2$-blocks in strongly biconnected directed graphs</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jaberi:Raed.html">Raed Jaberi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09793">PDF</a><br/><b>Abstract: </b>A directed graph $G=(V,E)$ is called strongly biconnected if $G$ is strongly
connected and the underlying graph of $G$ is biconnected. A strongly
biconnected component of a strongly connected graph $G=(V,E)$ is a maximal
vertex subset $L\subseteq V$ such that the induced subgraph on $L$ is strongly
biconnected. Let $G=(V,E)$ be a strongly biconnected directed graph. A
$2$-edge-biconnected block in $G$ is a maximal vertex subset $U\subseteq V$
such that for any two distict vertices $v,w \in U$ and for each edge $b\in E$,
the vertices $v,w$ are in the same strongly biconnected components of
$G\setminus\left\lbrace b\right\rbrace $. A $2$-strong-biconnected block in $G$
is a maximal vertex subset $U\subseteq V$ of size at least $2$ such that for
every pair of distinct vertices $v,w\in U$ and for every vertex $z\in
V\setminus\left\lbrace v,w \right\rbrace $, the vertices $v$ and $w$ are in the
same strongly biconnected component of $G\setminus \left\lbrace v,w
\right\rbrace $. In this paper we study $2$-edge-biconnected blocks and
$2$-strong biconnected blocks.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.09773</id>
    <link href="http://arxiv.org/abs/2007.09773" rel="alternate" type="text/html"/>
    <title>Shortest Secure Path in a Voronoi Diagram</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Har=Peled:Sariel.html">Sariel Har-Peled</a>, Rajgopal Varadharajan <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09773">PDF</a><br/><b>Abstract: </b>We investigate the problem of computing the shortest secure path in a Voronoi
diagram. Here, a path is secure if it is a sequence of touching Voronoi cells,
where each Voronoi cell in the path has a uniform cost of being secured.
Importantly, we allow inserting new sites, which in some cases leads to
significantly shorter paths. We present an $O(n \log n)$ time algorithm for
solving this problem in the plane, which uses a dynamic additive weighted
Voronoi diagram to compute this path. The algorithm is an interesting
combination of the continuous and discrete Dijkstra algorithms. We also
implemented the algorithm using CGAL.
</p></div>
    </summary>
    <updated>2020-07-22T00:27:51Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.09768</id>
    <link href="http://arxiv.org/abs/2007.09768" rel="alternate" type="text/html"/>
    <title>FPT Algorithms for Finding Dense Subgraphs in $c$-Closed Graphs</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Husic:Edin.html">Edin Husic</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Roughgarden:Tim.html">Tim Roughgarden</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09768">PDF</a><br/><b>Abstract: </b>Dense subgraph detection is a fundamental problem in network analysis for
which few worst-case guarantees are known, motivating its study through the
lens of fixed-parameter tractability. But for what parameter? Recent work has
proposed parameterizing graphs by their degree of triadic closure, with a
$c$-closed graph defined as one in which every vertex pair with at least $c$
common neighbors are themselves connected by an edge. The special case of
enumerating all maximal cliques (and hence computing a maximum clique) of a
$c$-closed graph is known to be fixed-parameter tractable with respect to $c$
(Fox et al., SICOMP 2020).
</p>
<p>In network analysis, sufficiently dense subgraphs are typically as notable
and meaningful as cliques. We investigate the fixed-parameter tractability
(with respect to $c$) of optimization and enumeration in $c$-closed graphs, for
several notions of dense subgraphs. We focus on graph families that are the
complements of the most well-studied notions of sparse graphs, including graphs
with bounded degree, bounded treewidth, or bounded degeneracy, and provide
fixed-parameter tractable enumeration and optimization algorithms for these
families. To go beyond the special case of maximal cliques, we use a new
combinatorial bound (generalizing the Moon-Moser theorem); new techniques for
exploiting the $c$-closed condition; and more sophisticated enumeration
algorithms.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.09640</id>
    <link href="http://arxiv.org/abs/2007.09640" rel="alternate" type="text/html"/>
    <title>Exploitation of Multiple Replenishing Resources with Uncertainty</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Korman:Amos.html">Amos Korman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Emek:Yuval.html">Yuval Emek</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Collet:Simon.html">Simon Collet</a>, Aya Goldshtein, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yovel:Yossi.html">Yossi Yovel</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09640">PDF</a><br/><b>Abstract: </b>We consider an optimization problem in which a (single) bat aims to exploit
the nectar in a set of $n$ cacti with the objective of maximizing the expected
total amount of nectar it drinks. Each cactus $i \in [n]$ is characterized by a
parameter $r_{i} &gt; 0$ that determines the rate in which nectar accumulates in
$i$. In every round, the bat can visit one cactus and drink all the nectar
accumulated there since its previous visit. Furthermore, competition with other
bats, that may also visit some cacti and drink their nectar, is modeled by
means of a stochastic process in which cactus $i$ is emptied in each round
(independently) with probability $0 &lt; s_i &lt; 1$. Our attention is restricted to
purely-stochastic strategies that are characterized by a probability vector
$(p_1, \ldots, p_n)$ determining the probability $p_i$ that the bat visits
cactus $i$ in each round. We prove that for every $\epsilon &gt; 0$, there exists
a purely-stochastic strategy that approximates the optimal purely-stochastic
strategy to within a multiplicative factor of $1 + \epsilon$, while exploiting
only a small core of cacti. Specifically, we show that it suffices to include
at most $\frac{2 (1 - \sigma)}{\epsilon \cdot \sigma}$ cacti in the core, where
$\sigma = \min_{i \in [n]} s_{i}$. We also show that this upper bound on core
size is asymptotically optimal as a core of a significantly smaller size cannot
provide a $(1 + \epsilon)$-approximation of the optimal purely-stochastic
strategy. This means that when the competition is more intense (i.e., $\sigma$
is larger), a strategy based on exploiting smaller cores will be favorable.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.09634</id>
    <link href="http://arxiv.org/abs/2007.09634" rel="alternate" type="text/html"/>
    <title>GRMR: Generalized Regret-Minimizing Representatives</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Yanhao.html">Yanhao Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mathioudakis:Michael.html">Michael Mathioudakis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Yuchen.html">Yuchen Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tan:Kian=Lee.html">Kian-Lee Tan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09634">PDF</a><br/><b>Abstract: </b>Extracting a small subset of representative tuples from a large database is
an important task in multi-criteria decision making. The regret-minimizing set
(RMS) problem is recently proposed for representative discovery from databases.
Specifically, for a set of tuples (points) in $d$ dimensions, an RMS problem
finds the smallest subset such that, for any possible ranking function, the
relative difference in scores between the top-ranked point in the subset and
the top-ranked point in the entire database is within a parameter $\varepsilon
\in (0,1)$. Although RMS and its variations have been extensively investigated
in the literature, existing approaches only consider the class of nonnegative
(monotonic) linear functions for ranking, which have limitations in modeling
user preferences and decision-making processes.
</p>
<p>To address this issue, we define the generalized regret-minimizing
representative (GRMR) problem that extends RMS by taking into account all
linear functions including non-monotonic ones with negative weights. For
two-dimensional databases, we propose an optimal algorithm for GRMR via a
transformation into the shortest cycle problem in a directed graph. Since GRMR
is proven to be NP-hard even in three dimensions, we further develop a
polynomial-time heuristic algorithm for GRMR on databases in arbitrary
dimensions. Finally, we conduct extensive experiments on real and synthetic
datasets to confirm the efficiency, effectiveness, and scalability of our
proposed algorithms.
</p></div>
    </summary>
    <updated>2020-07-21T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.09599</id>
    <link href="http://arxiv.org/abs/2007.09599" rel="alternate" type="text/html"/>
    <title>Reconstructing weighted voting schemes from partial information about their power indices</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bennett:Huck.html">Huck Bennett</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/De:Anindya.html">Anindya De</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Servedio:Rocco_A=.html">Rocco A. Servedio</a>, Emmanouil V. Vlatakis-Gkaragkounis <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09599">PDF</a><br/><b>Abstract: </b>A number of recent works [Goldberg 2006; O'Donnell and Servedio 2011; De,
Diakonikolas, and Servedio 2017; De, Diakonikolas, Feldman, and Servedio 2014]
have considered the problem of approximately reconstructing an unknown weighted
voting scheme given information about various sorts of ``power indices'' that
characterize the level of control that individual voters have over the final
outcome. In the language of theoretical computer science, this is the problem
of approximating an unknown linear threshold function (LTF) over $\{-1, 1\}^n$
given some numerical measure (such as the function's $n$ ``Chow parameters,''
a.k.a. its degree-1 Fourier coefficients, or the vector of its $n$ Shapley
indices) of how much each of the $n$ individual input variables affects the
outcome of the function.
</p>
<p>In this paper we consider the problem of reconstructing an LTF given only
partial information about its Chow parameters or Shapley indices; i.e. we are
given only the Chow parameters or the Shapley indices corresponding to a subset
$S \subseteq [n]$ of the $n$ input variables. A natural goal in this partial
information setting is to find an LTF whose Chow parameters or Shapley indices
corresponding to indices in $S$ accurately match the given Chow parameters or
Shapley indices of the unknown LTF. We refer to this as the Partial Inverse
Power Index Problem.
</p></div>
    </summary>
    <updated>2020-07-21T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.09556</id>
    <link href="http://arxiv.org/abs/2007.09556" rel="alternate" type="text/html"/>
    <title>A $2^{n/2}$-Time Algorithm for $\sqrt{n}$-SVP and $\sqrt{n}$-Hermite SVP, and an Improved Time-Approximation Tradeoff for (H)SVP</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aggarwal:Divesh.html">Divesh Aggarwal</a>, Zeyong Li, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stephens=Davidowitz:Noah.html">Noah Stephens-Davidowitz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09556">PDF</a><br/><b>Abstract: </b>We show a $2^{n/2+o(n)}$-time algorithm that finds a (non-zero) vector in a
lattice $\mathcal{L} \subset \mathbb{R}^n$ with norm at most
$\tilde{O}(\sqrt{n})\cdot \min\{\lambda_1(\mathcal{L}),
\det(\mathcal{L})^{1/n}\}$, where $\lambda_1(\mathcal{L})$ is the length of a
shortest non-zero lattice vector and $\det(\mathcal{L})$ is the lattice
determinant. Minkowski showed that $\lambda_1(\mathcal{L}) \leq \sqrt{n}
\det(\mathcal{L})^{1/n}$ and that there exist lattices with
$\lambda_1(\mathcal{L}) \geq \Omega(\sqrt{n}) \cdot \det(\mathcal{L})^{1/n}$,
so that our algorithm finds vectors that are as short as possible relative to
the determinant (up to a polylogarithmic factor).
</p>
<p>The main technical contribution behind this result is new analysis of (a
simpler variant of) an algorithm from <a href="http://export.arxiv.org/abs/1412.7994">arXiv:1412.7994</a>, which was only
previously known to solve less useful problems. To achieve this, we rely
crucially on the ``reverse Minkowski theorem'' (conjectured by Dadush
<a href="http://export.arxiv.org/abs/1606.06913">arXiv:1606.06913</a> and proven by <a href="http://export.arxiv.org/abs/1611.05979">arXiv:1611.05979</a>), which can be thought of as a
partial converse to the fact that $\lambda_1(\mathcal{L}) \leq \sqrt{n}
\det(\mathcal{L})^{1/n}$.
</p>
<p>Previously, the fastest known algorithm for finding such a vector was the
$2^{.802n + o(n)}$-time algorithm due to [Liu, Wang, Xu, and Zheng, 2011],
which actually found a non-zero lattice vector with length $O(1) \cdot
\lambda_1(\mathcal{L})$. Though we do not show how to find lattice vectors with
this length in time $2^{n/2+o(n)}$, we do show that our algorithm suffices for
the most important application of such algorithms: basis reduction. In
particular, we show a modified version of Gama and Nguyen's slide-reduction
algorithm [Gama and Nguyen, STOC 2008], which can be combined with the
algorithm above to improve the time-length tradeoff for shortest-vector
algorithms in nearly all regimes, including the regimes relevant to
cryptography.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.09345</id>
    <link href="http://arxiv.org/abs/2007.09345" rel="alternate" type="text/html"/>
    <title>Combinatorial and computational investigations of Neighbor-Joining bias</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Davidson:Ruth.html">Ruth Davidson</a>, Abraham Martin del Campo <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09345">PDF</a><br/><b>Abstract: </b>The Neighbor-Joining algorithm is a popular distance-based phylogenetic
method that computes a tree metric from a dissimilarity map arising from
biological data. Realizing dissimilarity maps as points in Euclidean space, the
algorithm partitions the input space into polyhedral regions indexed by the
combinatorial type of the trees returned. A full combinatorial description of
these regions has not been found yet; different sequences of Neighbor-Joining
agglomeration events can produce the same combinatorial tree, therefore
associating multiple geometric regions to the same algorithmic output. We
resolve this confusion by defining agglomeration orders on trees, leading to a
bijection between distinct regions of the output space and weighted Motzkin
paths. As a result, we give a formula for the number of polyhedral regions
depending only on the number of taxa. We conclude with a computational
comparison between these polyhedral regions, to unveil biases introduced in any
implementation of the algorithm.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.09333</id>
    <link href="http://arxiv.org/abs/2007.09333" rel="alternate" type="text/html"/>
    <title>Additive Approximation Schemes for Load Balancing Problems</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Moritz Buchem, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rohwedder:Lars.html">Lars Rohwedder</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vredeveld:Tjark.html">Tjark Vredeveld</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wiese:Andreas.html">Andreas Wiese</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09333">PDF</a><br/><b>Abstract: </b>In this paper we introduce the concept of additive approximation schemes and
apply it to load balancing problems. Additive approximation schemes aim to find
a solution with an absolute error in the objective of at most $\epsilon h$ for
some suitable parameter $h$. In the case that the parameter $h$ provides a
lower bound an additive approximation scheme implies a standard multiplicative
approximation scheme and can be much stronger when $h \ll$ OPT. On the other
hand, when no PTAS exists (or is unlikely to exist), additive approximation
schemes can provide a different notion for approximation.
</p>
<p>We consider the problem of assigning jobs to identical machines with lower
and upper bounds for the loads of the machines. This setting generalizes
problems like makespan minimization, the Santa Claus problem (on identical
machines), and the envy-minimizing Santa Claus problem. For the last problem,
in which the objective is to minimize the difference between the maximum and
minimum load, the optimal objective value may be zero and hence it is NP-hard
to obtain any multiplicative approximation guarantee. For this class of
problems we present additive approximation schemes for $h = p_{\max}$, the
maximum processing time of the jobs.
</p>
<p>Our technical contribution is two-fold. First, we introduce a new relaxation
based on integrally assigning slots to machines and fractionally assigning jobs
to the slots (the slot-MILP). We identify structural properties of
(near-)optimal solutions of the slot-MILP, which allow us to solve it
efficiently, assuming that there are $O(1)$ different lower and upper bounds on
the machine loads (which is the relevant setting for the three problems
mentioned above). The second technical contribution is a local-search based
algorithm which rounds a solution to the slot-MILP introducing an additive
error on the target load intervals of at most $\epsilon\cdot p_{\max}$.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.09282</id>
    <link href="http://arxiv.org/abs/2007.09282" rel="alternate" type="text/html"/>
    <title>An APX for the Maximum-Profit Routing Problem with Variable Supply</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Armaselu:Bogdan.html">Bogdan Armaselu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09282">PDF</a><br/><b>Abstract: </b>In this paper, we study the Maximum-Profit Routing Problem with Variable
Supply (MPRP-VS). This is a more general version of the Maximum-Profit Public
Transportation Route Planning Problem, or simply Maximum-Profit Routing Problem
(MPRP), introduced in \cite{Armaselu-PETRA}. In this new version, the quantity
$q_i(t)$ supplied at site $i$ is linearly increasing in time $t$, as opposed to
\cite{Armaselu-PETRA}, where the quantity is constant in time. Our main result
is a $5.5 \log{T} (1 + \epsilon) (1 + \frac{1}{1 + \sqrt{m}})^2$ approximation
algorithm, where $T$ is the latest time window and $m$ is the number of
vehicles used. In addition, we improve upon the MPRP algorithm in
\cite{Armaselu-PETRA} under certain conditions.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.09261</id>
    <link href="http://arxiv.org/abs/2007.09261" rel="alternate" type="text/html"/>
    <title>Frequency Estimation in Data Streams: Learning the Optimal Hashing Scheme</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bertsimas:Dimitris.html">Dimitris Bertsimas</a>, Vassilis Digalakis Jr <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09261">PDF</a><br/><b>Abstract: </b>We present a novel approach for the problem of frequency estimation in data
streams that is based on optimization and machine learning. Contrary to
state-of-the-art streaming frequency estimation algorithms, which heavily rely
on random hashing to maintain the frequency distribution of the data steam
using limited storage, the proposed approach exploits an observed stream prefix
to near-optimally hash elements and compress the target frequency distribution.
We develop an exact mixed-integer linear optimization formulation, as well as
an efficient block coordinate descent algorithm, that enable us to compute
near-optimal hashing schemes for elements seen in the observed stream prefix;
then, we use machine learning to hash unseen elements. We empirically evaluate
the proposed approach on real-world search query data and show that it
outperforms existing approaches by one to two orders of magnitude in terms of
its average (per element) estimation error and by 45-90% in terms of its
expected magnitude of estimation error.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.09202</id>
    <link href="http://arxiv.org/abs/2007.09202" rel="alternate" type="text/html"/>
    <title>Query Complexity of Global Minimum Cut</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bishnu:Arijit.html">Arijit Bishnu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghosh:Arijit.html">Arijit Ghosh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mishra:Gopinath.html">Gopinath Mishra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Paraashar:Manaswi.html">Manaswi Paraashar</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09202">PDF</a><br/><b>Abstract: </b>In this work, we resolve the query complexity of global minimum cut problem
for a graph by designing a randomized algorithm for approximating the size of
minimum cut in a graph, where the graph can be accessed through local queries
like {\sc Degree}, {\sc Neighbor}, and {\sc Adjacency} queries.
</p>
<p>Given $\epsilon \in (0,1)$, the algorithm with high probability outputs an
estimate $\hat{t}$ satisfying the following $(1-\epsilon) t \leq \hat{t} \leq
(1+\epsilon) t$, where $m$ is the number of edges in the graph and $t$ is the
size of minimum cut in the graph. The expected number of local queries used by
our algorithm is $\min\left\{m+n,\frac{m}{t}\right\}\mbox{poly}\left(\log
n,\frac{1}{\epsilon}\right)$ where $n$ is the number of vertices in the graph.
Eden and Rosenbaum showed that $\Omega(m/t)$ many local queries are required
for approximating the size of minimum cut in graphs. These two results together
resolve the query complexity of the problem of estimating the size of minimum
cut in graphs using local queries.
</p>
<p>Building on the lower bound of Eden and Rosenbaum, we show that, for all $t
\in \mathbb{N}$, $\Omega(m)$ local queries are required to decide if the size
of the minimum cut in the graph is $t$ or $t-2$. Also, we show that, for any $t
\in \mathbb{N}$, $\Omega(m)$ local queries are required to find all the minimum
cut edges even if it is promised that the input graph has a minimum cut of size
$t$. Both of our lower bound results are randomized, and hold even if we can
make {\sc Random Edge} query apart from local queries.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.09192</id>
    <link href="http://arxiv.org/abs/2007.09192" rel="alternate" type="text/html"/>
    <title>The Edit Distance to $k$-Subsequence Universality</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fleischmann:Pamela.html">Pamela Fleischmann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kosche:Maria.html">Maria Kosche</a>, Tore Ko, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Manea:Florin.html">Florin Manea</a>, Stefan Siemer Kiel University, Computer Science Department, Germany, Gttingen University, Computer Science Department, Germany) <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09192">PDF</a><br/><b>Abstract: </b>A word $u$ is a subsequence of another word $w$ if $u$ can be obtained from
$w$ by deleting some of its letters. The word $w$ with alph$(w)=\Sigma$ is
called $k$-subsequence universal if the set of subsequences of length $k$ of
$w$ contains all possible words of length $k$ over $\Sigma$. We propose a
series of efficient algorithms computing the minimal number of edit operations
(insertion, deletion, substitution) one needs to apply to a given word in order
to reach the set of $k$-subsequence universal words.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.09172</id>
    <link href="http://arxiv.org/abs/2007.09172" rel="alternate" type="text/html"/>
    <title>Improved Approximations for Min Sum Vertex Cover and Generalized Min Sum Set Cover</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bansal:Nikhil.html">Nikhil Bansal</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Batra:Jatin.html">Jatin Batra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Farhadi:Majid.html">Majid Farhadi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tetali:Prasad.html">Prasad Tetali</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09172">PDF</a><br/><b>Abstract: </b>We study the generalized min sum set cover (GMSSC) problem, wherein given a
collection of hyperedges $E$ with arbitrary covering requirements $k_e$, the
goal is to find an ordering of the vertices to minimize the total cover time of
the hyperedges; a hyperedge $e$ is considered covered by the first time when
$k_e$ many of its vertices appear in the ordering. We give a $4.642$
approximation algorithm for GMSSC, coming close to the best possible bound of
$4$, already for the classical special case (with all $k_e=1$) of min sum set
cover (MSSC) studied by Feige, Lov\'{a}sz and Tetali, and improving upon the
previous best known bound of $12.4$ due to Im, Sviridenko and van der Zwaan.
Our algorithm is based on transforming the LP solution by a suitable kernel and
applying randomized rounding. This also gives an LP-based $4$ approximation for
MSSC. As part of the analysis of our algorithm, we also derive an inequality on
the lower tail of a sum of independent Bernoulli random variables, which might
be of independent interest and broader utility.
</p>
<p>Another well-known special case is the min sum vertex cover (MSVC) problem,
in which the input hypergraph is a graph and $k_e = 1$, for every edge. We give
a $16/9$ approximation for MSVC, and show a matching integrality gap for the
natural LP relaxation. This improves upon the previous best $1.999946$
approximation of Barenholz, Feige and Peleg. (The claimed $1.79$ approximation
result of Iwata, Tetali and Tripathi for the MSVC turned out have an
unfortunate, seemingly unfixable, mistake in it.) Finally, we revisit MSSC and
consider the $\ell_p$ norm of cover-time of the hyperedges. Using a dual
fitting argument, we show that the natural greedy algorithm achieves tight, up
to NP-hardness, approximation guarantees of $(p+1)^{1+1/p}$, for all $p\ge 1$.
For $p=1$, this gives yet another proof of the $4$ approximation for MSSC.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.08972</id>
    <link href="http://arxiv.org/abs/2007.08972" rel="alternate" type="text/html"/>
    <title>On convex holes in $d$-dimensional point sets</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bukh:Boris.html">Boris Bukh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chao:Ting=Wei.html">Ting-Wei Chao</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Holzman:Ron.html">Ron Holzman</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.08972">PDF</a><br/><b>Abstract: </b>Given a finite set $A \subseteq \mathbb{R}^d$, points
$a_1,a_2,\dotsc,a_{\ell} \in A$ form an $\ell$-hole in $A$ if they are the
vertices of a convex polytope which contains no points of $A$ in its interior.
We construct arbitrarily large point sets in general position in $\mathbb{R}^d$
having no holes of size $2^{7d}$ or more. This improves the previously known
upper bound of order $d^{d+o(d)}$ due to Valtr. Our construction uses a certain
type of equidistributed point sets, originating from numerical analysis, known
as $(t,m,s)$-nets or $(t,s)$-sequences.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2020-07-22T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.01811</id>
    <link href="http://arxiv.org/abs/2007.01811" rel="alternate" type="text/html"/>
    <title>JAMPI: efficient matrix multiplication in Spark using Barrier Execution Mode</title>
    <feedworld_mtime>1595376000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Foldi:Tamas.html">Tamas Foldi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Csefalvay:Chris_von.html">Chris von Csefalvay</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Perez:Nicolas_A=.html">Nicolas A. Perez</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.01811">PDF</a><br/><b>Abstract: </b>The new barrier mode in Apache Spark allows embedding distributed deep
learning training as a Spark stage to simplify the distributed training
workflow. In Spark, a task in a stage does not depend on any other tasks in the
same stage, and hence it can be scheduled independently. However, several
algorithms require more sophisticated inter-task communications, similar to the
MPI paradigm. By combining distributed message passing (using asynchronous
network IO), OpenJDK's new auto-vectorization and Spark's barrier execution
mode, we can add non-map/reduce based algorithms, such as Cannon's distributed
matrix multiplication to Spark. We document an efficient distributed matrix
multiplication using Cannon's algorithm, which improves significantly on the
performance of the existing MLlib implementation. Used within a barrier task,
the algorithm described herein results in an up to 24 percent performance
increase on a 10,000x10,000 square matrix with a significantly lower memory
footprint. Applications of efficient matrix multiplication include, among
others, accelerating the training and implementation of deep convolutional
neural network based workloads, and thus such efficient algorithms can play a
ground-breaking role in faster, more efficient execution of even the most
complicated machine learning tasks.
</p></div>
    </summary>
    <updated>2020-07-22T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2020-07-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4912</id>
    <link href="https://www.scottaaronson.com/blog/?p=4912" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4912#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4912" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Is this blog obsolete?</title>
    <summary xml:lang="en-US">Longtime friend-of-the-blog Michael Nielsen, formerly a quantum computing theorist and now an independent thinker about science and AI and learning, posted the following exchange with the astounding GPT-3 text generator / chatbot: Q: Is it possible to use quantum computers to solve NP-complete problems in polynomial time?A: It is not possible to use quantum computers []</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Longtime friend-of-the-blog <a href="http://michaelnielsen.org/">Michael Nielsen</a>, formerly a quantum computing theorist and now an independent thinker about science and AI and learning, posted the following <a href="https://twitter.com/michael_nielsen/status/1284988349418098688">exchange</a> with the astounding <a href="https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/">GPT-3</a> text generator / chatbot:</p>



<blockquote class="wp-block-quote"><p>Q: Is it possible to use quantum computers to solve NP-complete problems in polynomial time?<br/>A: It is not possible to use quantum computers to solve NP-complete problems in polynomial time.</p><p>Q: Why not?<br/>A: Because NP-complete problems are not in BQP.</p><p>Q: Youve just restated your earlier assertion!  What Id like is to understand the underlying reason why not!<br/>A: Im sorry, but I dont have time to explain the underlying reason why not.</p><p>Q: No offence, but this seems really bogus.  I dont believe you understand the reason why!<br/>A: Im sorry, but I dont have time to explain the underlying reason why not.</p></blockquote>



<p>(And then there are a few more rounds of the AI not having time to explain the underlying reason why not.)</p>



<p>This is, like, 80% of the way to replacing <em>Shtetl-Optimized</em>!</p>



<p>For much more discussion of GPT-3 and its implications, and samples of its output, see for example the <a href="https://www.reddit.com/r/slatestarcodex/">SSC subreddit</a>.  At the moment, as far as I can tell, the closest a person off the street can easily come to experimenting with GPT-3 themselves is using a website called <a href="https://play.aidungeon.io/">AI Dungeon</a>.</p>



<p>And yes, as many have already remarked, this is clearly the <a href="https://en.wikipedia.org/wiki/Altair_8800">MITS Altair</a> of text-generating AI, an amusing toy thats also the start of something that will change the world.</p></div>
    </content>
    <updated>2020-07-21T00:16:18Z</updated>
    <published>2020-07-21T00:16:18Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2020-07-21T00:16:18Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.09461</id>
    <link href="http://arxiv.org/abs/2007.09461" rel="alternate" type="text/html"/>
    <title>Controllability of reaction systems</title>
    <feedworld_mtime>1595289600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Ivanov:Sergiu.html">Sergiu Ivanov</a>, Ion Petre IBISC, Universit vry, Universit Paris-Saclay, France, Department of Mathematics and Statistics, University of Turku, Finland, National Institute for Research and Development in Biological Sciences, Romania) <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09461">PDF</a><br/><b>Abstract: </b>Controlling a dynamical system is the ability of changing its configuration
arbitrarily through a suitable choice of inputs. It is a very well studied
concept in control theory, with wide ranging applications in medicine, biology,
social sciences, engineering. We introduce in this article the concept of
controllability of reaction systems as the ability of transitioning between any
two states through a suitable choice of context sequences. We show that the
problem is PSPACE-hard. We also introduce a model of oncogenic signalling based
on reaction systems and use it to illustrate the intricacies of the
controllability of reaction systems.
</p></div>
    </summary>
    <updated>2020-07-21T23:23:10Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-07-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.09318</id>
    <link href="http://arxiv.org/abs/2007.09318" rel="alternate" type="text/html"/>
    <title>Monochromatic Triangles, Triangle Listing and APSP</title>
    <feedworld_mtime>1595289600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Williams:Virginia_Vassilevska.html">Virginia Vassilevska Williams</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Yinzhan.html">Yinzhan Xu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09318">PDF</a><br/><b>Abstract: </b>One of the main hypotheses in fine-grained complexity is that All-Pairs
Shortest Paths (APSP) for $n$-node graphs requires $n^{3-o(1)}$ time. Another
famous hypothesis is that the $3$SUM problem for $n$ integers requires
$n^{2-o(1)}$ time. Although there are no direct reductions between $3$SUM and
APSP, it is known that they are related: there is a problem,
$(\min,+)$-convolution that reduces in a fine-grained way to both, and a
problem Exact Triangle that both fine-grained reduce to.
</p>
<p>In this paper we find more relationships between these two problems and other
basic problems. P\u{a}tra\c{s}cu had shown that under the $3$SUM hypothesis the
All-Edges Sparse Triangle problem in $m$-edge graphs requires $m^{4/3-o(1)}$
time. The latter problem asks to determine for every edge $e$, whether $e$ is
in a triangle. It is equivalent to the problem of listing $m$ triangles in an
$m$-edge graph where $m=\tilde{O}(n^{1.5})$, and can be solved in $O(m^{1.41})$
time [Alon et al.'97] with the current matrix multiplication bounds, and in
$\tilde{O}(m^{4/3})$ time if $\omega=2$.
</p>
<p>We show that one can reduce Exact Triangle to All-Edges Sparse Triangle,
showing that All-Edges Sparse Triangle (and hence Triangle Listing) requires
$m^{4/3-o(1)}$ time also assuming the APSP hypothesis. This allows us to
provide APSP-hardness for many dynamic problems that were previously known to
be hard under the $3$SUM hypothesis.
</p>
<p>We also consider the previously studied All-Edges Monochromatic Triangle
problem. Via work of [Lincoln et al.'20], our result on All-Edges Sparse
Triangle implies that if the All-Edges Monochromatic Triangle problem has an
$O(n^{2.5-\epsilon})$ time algorithm for $\epsilon&gt;0$, then both the APSP and
$3$SUM hypotheses are false. We also connect the problem to other
``intermediate'' problems, whose runtimes are between $O(n^\omega)$ and
$O(n^3)$, such as the Max-Min product problem.
</p></div>
    </summary>
    <updated>2020-07-21T23:21:48Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-07-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2007.09281</id>
    <link href="http://arxiv.org/abs/2007.09281" rel="alternate" type="text/html"/>
    <title>Efficient Iterative Solutions to Complex-Valued Nonlinear Least-Squares Problems with Mixed Linear and Antilinear Operators</title>
    <feedworld_mtime>1595289600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kim:Tae_Hyung.html">Tae Hyung Kim</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Haldar:Justin_P=.html">Justin P. Haldar</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2007.09281">PDF</a><br/><b>Abstract: </b>We consider a setting in which it is desired to find an optimal complex
vector $\mathbf{x}\in\mathbb{C}^N$ that satisfies $\mathcal{A}(\mathbf{x})
\approx \mathbf{b}$ in a least-squares sense, where $\mathbf{b} \in
\mathbb{C}^M$ is a data vector (possibly noise-corrupted), and
$\mathcal{A}(\cdot): \mathbb{C}^N \rightarrow \mathbb{C}^M$ is a measurement
operator. If $\mathcal{A}(\cdot)$ were linear, this reduces to the classical
linear least-squares problem, which has a well-known analytic solution as well
as powerful iterative solution algorithms. However, instead of linear
least-squares, this work considers the more complicated scenario where
$\mathcal{A}(\cdot)$ is nonlinear, but can be represented as the summation
and/or composition of some operators that are linear and some operators that
are antilinear. Some common nonlinear operations that have this structure
include complex conjugation or taking the real-part or imaginary-part of a
complex vector. Previous literature has shown that this kind of mixed
linear/antilinear least-squares problem can be mapped into a linear
least-squares problem by considering $\mathbf{x}$ as a vector in
$\mathbb{R}^{2N}$ instead of $\mathbb{C}^N$. While this approach is valid, the
replacement of the original complex-valued optimization problem with a
real-valued optimization problem can be complicated to implement, and can also
be associated with increased computational complexity. In this work, we
describe theory and computational methods that enable mixed linear/antilinear
least-squares problems to be solved iteratively using standard linear
least-squares tools, while retaining all of the complex-valued structure of the
original inverse problem. An illustration is provided to demonstrate that this
approach can simplify the implementation and reduce the computational
complexity of iterative solution algorithms.
</p></div>
    </summary>
    <updated>2020-07-21T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2020-07-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://differentialprivacy.org/stoc2020/</id>
    <link href="https://differentialprivacy.org/stoc2020/" rel="alternate" type="text/html"/>
    <title>Conference Digest - STOC 2020</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="http://acm-stoc.org/stoc2020/">STOC 2020</a> was recently held online, as one of the first major theory conferences during the COVID-19 era.
It featured four papers on differential privacy, which we list and link below.
Each one is accompanied by a video from the conference, as well as a longer video if available.
Please let us know if we missed any papers on differential privacy, either in the comments below or by email.</p>

<ul>
  <li>
    <p><a href="https://arxiv.org/abs/1911.08339">The Power of Factorization Mechanisms in Local and Central Differential Privacy</a> (<a href="https://www.youtube.com/watch?v=hSenRTxhZhM">video</a>)<br/>
<a href="https://dblp.uni-trier.de/pers/hd/e/Edmonds:Alexander">Alexander Edmonds</a>, <a href="http://www.cs.toronto.edu/~anikolov/">Aleksandar Nikolov</a>, <a href="https://www.ccs.neu.edu/home/jullman/">Jonathan Ullman</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2005.04763">Private Stochastic Convex Optimization: Optimal Rates in Linear Time</a> (<a href="https://www.youtube.com/watch?v=Tlc-z-MFAmM">video</a>)<br/>
<a href="http://vtaly.net/">Vitaly Feldman</a>, <a href="https://tomerkoren.github.io/">Tomer Koren</a>, <a href="http://kunaltalwar.org/">Kunal Talwar</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1911.04014">Interaction is necessary for distributed learning with privacy or communication constraints</a> (<a href="https://www.youtube.com/watch?v=AWgzaFOU_HM">video</a>)<br/>
<a href="https://yuvaldagan.wordpress.com/">Yuval Dagan</a>, <a href="http://vtaly.net/">Vitaly Feldman</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1906.05271">Does Learning Require Memorization? A Short Tale about a Long Tail</a> (<a href="https://www.youtube.com/watch?v=sV59uoWJRnk">video</a>, <a href="https://www.youtube.com/watch?v=Fp7cgHRl8Yc">longer video</a>)<br/>
<a href="http://vtaly.net/">Vitaly Feldman</a></p>
  </li>
</ul></div>
    </summary>
    <updated>2020-07-20T14:00:00Z</updated>
    <published>2020-07-20T14:00:00Z</published>
    <author>
      <name>Gautam Kamath</name>
    </author>
    <source>
      <id>https://differentialprivacy.org</id>
      <link href="https://differentialprivacy.org" rel="alternate" type="text/html"/>
      <link href="https://differentialprivacy.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Website for the differential privacy research community</subtitle>
      <title>Differential Privacy</title>
      <updated>2020-07-22T01:53:42Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://gradientscience.org/transfer-learning/</id>
    <link href="https://gradientscience.org/transfer-learning/" rel="alternate" type="text/html"/>
    <title>Transfer Learning with Adversarially Robust Models</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a class="bbutton" href="https://arxiv.org/abs/2007.08489" style="float: left; width: 45%;">
<i class="fas fa-file-pdf"/>
  Paper
</a>
<a class="bbutton" href="https://github.com/Microsoft/robust-models-transfer" style="float: left; width: 45%;">
<i class="fab fa-github"/>
 Models and Code
</a>
<br/></p>

<p><i>In our <a href="https://arxiv.org/abs/2007.08489">latest paper</a>, in collaboration with <a href="https://www.microsoft.com/en-us/research/">Microsoft Research</a>, we explore adversarial
robustness as an avenue for training computer vision models with more transferrable
features. We find that robust models outperform their standard counterparts on
a variety of transfer learning tasks.</i></p>

<h2 id="what-is-transfer-learning">What is transfer learning?</h2>

<p>Transfer learning is a paradigm where one leverages information
from a source task to better solve another target task. Particularly when there is little training data or compute available for solving the target
task, transfer learning provides a simple and efficient way to obtain performant
machine learning models.</p>

<p>Transfer learning has already proven its utility in many ML contexts. In natural language processing, for example, one can leverage language models pre-trained on large
text corpora to beat state-of-the-art performance on
tasks like query answering, entity recognition or part-of-speech classification.</p>

<p>In our work we focus on computer vision; in this context, a standardand
remarkably successfultransfer learning pipeline is ImageNet pre-training.
This pipeline starts with a deep neural network trained on the <a href="http://image-net.org">ImageNet-1K</a>
dataset, and then refines this pre-trained model for a target task. The target task can range
from classification of smaller datasets (e.g., <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR</a>) to more complex
tasks like object detection (e.g., <a href="http://host.robots.ox.ac.uk/pascal/VOC/">VOC</a>).</p>

<p>Although there are many ways in which one can refine a pre-trained model, we
will restrict our attention to the two most popular methods:</p>

<ul>
  <li><strong>Fixed-feature</strong>: In fixed-feature transfer learning, we replace the final
(linear) layer of the neural network with a new layer that has the correct
number of outputs for the target task. Then, keeping the rest of the layers
<em>fixed</em>, we train the newly replaced layer on the target task.</li>
  <li><strong>Full-network</strong>: In full-network transfer learning, we also replace the last
layer but do not freeze any layers afterwards. Instead, we use the pre-trained
network
as a sort of initialization, and continue training <em>all</em> the layers on the
target task.</li>
</ul>

<p>When at least a moderate amount of data is available, full-network transfer
learning typically outperforms the fixed-feature strategy.</p>

<h2 id="how-can-we-improve-transfer-learning">How can we improve transfer learning?</h2>

<p>Although we dont have a comprehensive understanding of what makes transfer
learning algorithms tick, there has been a long line of work focused on identifying 
factors that improve (or worsen) performance (examples include
<a href="https://arxiv.org/abs/1406.5774">[1]</a>,
<a href="https://arxiv.org/abs/1608.08614">[2]</a>,
<a href="https://arxiv.org/abs/1805.08974">[3]</a>,
<a href="https://arxiv.org/abs/1804.08328">[4]</a>,
<a href="https://arxiv.org/abs/1411.1792">[5]</a>).</p>

<p>By design, the pre-trained ImageNet model itself plays a major role here:
indeed, a recent study by <a href="https://arxiv.org/abs/1805.08974">Kornblith, Shlens, and Le</a> finds that
pre-trained models which achieve a higher ImageNet accuracy also perform better when
transferred to downstream classification tasks, with a tight linear
correspondence between ImageNet accuracy and the accuracy on the target task:</p>

<p><img alt="A scatter plot of ImageNet accuracy versus downstream     transfer accuracy showing the linear relation." class="bigimg" src="https://gradientscience.org/assets/robust-transfer-learning/ksl.png"/></p>
<div class="footnote">
Reproduced from <a href="https://arxiv.org/abs/1805.08974">[KSL19]</a>. 
Each dot is a pre-trained model whose $x$ coordinate is given by its 
ImageNet accuracy and $y$ coordinate is given by its downstream 
accuracy on the target task (after the corresponding refinement on that task).
</div>

<p>But is improving ImageNet accuracy of the pre-trained model the <em>only</em> way to improve transfer learning performance?</p>

<p>After all, we want to obtain models that have learned broadly applicable features from the
source dataset. ImageNet accuracy likely correlates with the quality of
features that a model has learned, but may not fully describe the downstream
utility of these features.
Ultimately, the nature of learned features stems from the <em>priors</em> placed on
them during training. For example, there have been studies of the (sometimes
implicit) priors imposed by architectural components (e.g., <a href="https://dmitryulyanov.github.io/deep_image_prior">convolutional layers</a>),
<a href="https://www.tandfonline.com/doi/abs/10.1198/10618600152418584">data</a>
<a href="https://arxiv.org/abs/1911.09071">augmentation</a>, 
<a href="https://arxiv.org/abs/1811.00401">loss functions</a> and even
<a href="https://stats385.github.io/assets/lectures/Stanford_Donoho_class_Nov_19.pdf">gradient descent</a> on neural network training.</p>

<p>In <a href="https://arxiv.org/abs/2007.08489">our paper</a>, we study another prior: <em>adversarial robustness</em>.
Adversarial robustnessa rather frequent subject on this blogrefers to
models invariance to small (often imperceptible) perturbations of natural
inputs, called <a href="https://gradientscience.org/intro_adversarial">adversarial examples</a>.</p>

<p>Standard neural networks (i.e., trained with the goal of maximizing
accuracy) are extremely vulnerable to such adversarial examples. For example,
with just a tiny perturbation to the pig image below, a pre-trained ImageNet
classifier will predict it as an airliner with 99% confidence:</p>

<p><img alt="An adversarial example: a pig on the left which is imperceptibly perturbed to be classified as an airliner on the right." src="https://gradientscience.org/images/piggie.png"/></p>
<div class="footnote">
A "pigs-can-fly" adversarial example: The "pig" image on the left is correctly classified by a standard ML model, but its imperceptibly perturbed counterpart on the right is classified as an "airliner" with 99% confidence.
</div>

<p>Adversarial robustness is thus typically induced at training time by replacing
the standard loss minimization objective with a <em>robust optimization</em> objective
(see our <a href="https://gradientscience.org/robust_opt_pt1">post on robust optimization</a> for more background):</p>



<p>The above objective trains models to be robust to image perturbations that are
small in (pixel-wise) $\ell_2$ (Euclidean) normIn reality, an $\ell_2$ ball doesn't perfectly capture the
set of imperceptible perturbations we want models to be robust tobut robustness with respect to this fairly rudimentary notion of perturbations turns out to be already non-trivial and very helpful.. 
The parameter $\varepsilon$ is a hyperparameter
governing the intended degree of invariance of the resulting models to the
corresponding perturbations. Setting 
$\varepsilon = 0$ corresponds to standard training, and increasing $\varepsilon$
asks the model to be robust to increasingly large perturbations.
In short, the objective asks the model to minimize risk on not only the 
training datapoints but also the entire radius-$\varepsilon$
neighbourhood around them.</p>

<p><em>[A quick plug: Our <a href="https://github.com/MadryLab/robustness"><code class="language-plaintext highlighter-rouge">robustness</code> Python library</a>, used for the code release of this paper, enables one to easily train and manipulate both standard and adversarially robust models.]</em></p>

<p>Although adversarial robustness has been initially studied solely through the lens of machine learning security, a line
of recent work (including some thats been <a href="https://gradientscience.org/adv">previously</a> 
<a href="https://gradientscience.org/robust_apps">covered</a> on this blog) has begun to study
adversarially robust models in their own right, framing adversarial robustness
as a prior that forces models to learn features that are locally stable.
These works have found that on the one hand, adversarially robust models tend
to attain lower accuracy than their standardly-trained
counterparts.</p>

<p>On the other hand, recent work suggests that the feature
representations of robust models carry several advantages over those of
standard models, such as <a href="https://arxiv.org/abs/1805.12152">better-behaved</a>
<a href="https://arxiv.org/abs/1905.09797">gradients</a>, <a href="https://arxiv.org/abs/1910.08640">representation
invertibility</a>, and more <a href="https://arxiv.org/abs/2005.10190">specialized
features</a>.
Weve actually discussed some of these observations in earlier posts on this
blogsee, e.g., our posts about 
<a href="https://gradientscience.org/robust_reps">representation learning</a> and 
<a href="https://gradientscience.org/robust_apps">image synthesis</a>.</p>

<p>These desirable properties
might suggest that robust neural networks are learning better feature
representations than standard networks, which could improve transfer
performance.</p>

<h3 id="adversarial-robustness-and-transfer-learning">Adversarial robustness and transfer learning</h3>

<p>So in summary, we have standard models with high accuracy on the source task but
little (or no) robustness; and we have adversarially robust models, which are
worse in terms of ImageNet accuracy, but have the nice
representational properties identified and discussed by prior works. Which
models are better for transfer learning?</p>

<p>To answer this question, we trained and examined a large collection
of standard and robust ImageNet models, while grid searching over a wide range of
hyperparameters and architectures to find the best model of each type. (All
models are available for download via our <a href="https://github.com/microsoft/robust-models-transfer">code/model
release</a> and more
details on our training procedure can be found there and in <a href="https://arxiv.org/abs/2007.08489">our
paper</a>). We then performed transfer
learning (using both fixed-feature and full-network refinement) from each
trained model to 12 downstream classification tasks.</p>

<p>It turns out that 
adversarially robust source models fairly consistently outperform their standard counterparts in
terms of downstream accuracy. In the table below, we compare the accuracies of
the best standard model (searching over hyperparameters and
architecture) and the best robust model (searching over the
previous factors as well as robustness level $\varepsilon$):</p>

<p><img alt="Table showing that robust models     perform better than their standard counterparts." class="bigimg" src="https://gradientscience.org/assets/robust-transfer-learning/results-table.svg" style="width: 100%;"/></p>
<div class="footnote">
    The main result: Adversarially robust models outperform their standard counterparts when transferred to downstream classification tasks.
</div>

<p>This difference in performance tends to be particularly striking in the context of fixed-feature transfer learning. The following graph shows, for each architecture and
downstream classification task, the best standard model compared to the best
robust model in that setting. As we can see, adversarially robust models
improve on the performance of their standard counterparts, and the gap tends to
<em>increase</em> as networks increase in width:</p>

<p><img alt="A bar chart showing that robust models improve on     standard ones even without taking the maximum over architectures." class="bigimg" src="https://gradientscience.org/assets/robust-transfer-learning/LogisticRegression.svg"/></p>
<div class="footnote">
    Adversarially robust models tend to improve over standard networks for
    individual architectures too. (An analogous graph for full-network
    transfer learning is given in Figure 3 of <a href="https://arxiv.org/abs/2007.08489">our paper</a>.)
</div>

<p>Adversarial robustness improved downstream transfer
performance even when the target task was not a classification one. For example, the
following table compares standard and robust pre-training for use in downstream
object detection and instance segmentation:</p>

<p><img class="bigimg" src="https://gradientscience.org/../assets/robust-transfer-learning/obj-det-results.svg" style="width: 80%;"/></p>
<div class="footnote">
</div>

<h3 id="robustness-versus-accuracy">Robustness versus accuracy</h3>

<p>So it seems like robust models, despite being less accurate on the source task, are actually
better for transfer learning purposes. Indeed, the linear relation between
 ImageNet accuracy and transfer performance observed in prior work (see our discussion above) doesnt seem
 to hold when the robustness parameter is varied. Compare the graphs below to the ones at the very start of this post:</p>

<p><img class="bigimg" src="https://gradientscience.org/assets/robust-transfer-learning/wide_resnet50_4_LogisticRegression.svg"/></p>
<div class="footnote">
    Source-task (ImageNet) versus target (fixed-feature) accuracy for models with the same
    architecture while varying the robustness levels. Each dot is a
    WideResNet-50x4 model with $x$ coordinate given by source-task accuracy and
    $y$ coordinate given by fixed-feature transfer learning accuracy.
    Contrast the trends here with the "fixed-feature" trend in the first
    figure of this postthe linear trend depicted there largely disappears as less
    accurate but more robust models perform better in terms of transfer.
</div>

<p>How do we reconcile our observations with these trends observed by prior work?</p>

<p>We hypothesize that robustness and accuracy have <em>disentangled</em> effects on
transfer performance. That is, for a fixed level of robustness, higher
accuracy on the source task helps transfer, and for a fixed level of
accuracy, increased robustness helps transfer. Indeed, as shown below, for a
fixed level of robustness, the accuracy-transfer relation tends to hold
strongly:</p>

<p><img class="bigimg" src="https://gradientscience.org/assets/robust-transfer-learning/fixed-robustness.svg"/></p>
<div class="footnote">
Even though robust models appear to break the linear
accuracy-transfer trend, this trend is actually preserved for a fixed value of
robustness. Each dot in the graph is a different architecture, trained for the same level of robustness ($\varepsilon = 3.0$). The $x$ coordinate is source task (ImageNet) accuracy, and the $y$ coordinate is the downstream accuracy on each target dataset.
</div>

<p>In addition to reconciling our results with those of prior work, these findings suggest that ongoing work on developing more accurate robust models
may have the added benefit of further improving transfer learning performance.</p>

<h3 id="other-empirical-mysteries-and-future-work">Other empirical mysteries and future work</h3>

<p>This post discussed how adversarially robust models might constitute a promising
avenue for improving transfer learning, and already often outperform standard
models in terms of downstream accuracy. In <a href="https://arxiv.org/abs/2007.08489">our paper</a>, 
we study this phenomenon more closely: for example, we examine the effects of
model width, and we compare adversarial robustness to other notions of
robustness. We also uncover a few somewhat mysterious properties: for example,
resizing images seems to have a non-trivial effect on the relationship between
robustness and downstream accuracy.</p>

<p>Finally, while our work provides evidence that adversarially
robust computer vision models transfer better, understanding precisely <em>why</em> this is the case remains open. More broadly, the results we
observe indicate that we still do not yet fully understand (even empirically)
the ingredients that make transfer learning successful. We hope that our work
prompts an inquiry into the underpinnings of modern transfer learning.</p></div>
    </summary>
    <updated>2020-07-20T00:00:00Z</updated>
    <published>2020-07-20T00:00:00Z</published>
    <source>
      <id>https://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="https://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="https://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2020-07-22T01:50:18Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-2418581440113974615</id>
    <link href="https://blog.computationalcomplexity.org/feeds/2418581440113974615/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/07/erdos-turan-for-k3-is-true.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/2418581440113974615" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/2418581440113974615" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/07/erdos-turan-for-k3-is-true.html" rel="alternate" type="text/html"/>
    <title>Erdos-Turan for k=3 is True!</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">(All of the math in this post is summarized (without proofs) in a writeup by Erik Metz and myself which you can find<a href="https://www.cs.umd.edu/users/gasarch/BLOGPAPERS/3apblog.pdf">here</a>. It is a pdf file so you can click on links in it to get to the papers it refers to. There have been posts on this topic by<a href="https://gilkalai.wordpress.com/2020/07/08/to-cheer-you-up-in-difficult-times-7-bloom-and-sisask-just-broke-the-logarithm-barrier-for-roths-theorem/">Gil Kalai</a>and<a href="https://lucatrevisan.wordpress.com/2020/07/08/silver-linings/">Luca Trevisan</a>. If you know of others then let me know so I can add them to this post.)<br/>
<br/>
<br/>
<br/>
This is a sequel to <a href="https://blog.computationalcomplexity.org/2010/12/breakthrough-result-on-density-and-3.html">A BREAKTHROUGH result on density and 3-AP's</a>and<a href="https://blog.computationalcomplexity.org/2017/06/big-news-on-w3r.html">Big news on W(3,r)!</a><br/>
<br/>
For this post N is large, and all inequalites have a big-O or a big-Omega.<br/>
<br/>
For this post [N] is {1,...,N}<br/>
<br/>
Let<br/>
<br/>
r(N) be the least w such that if A is a subset of [N] and |A| &gt; w, then A has a 3-AP.<br/>
<br/>
There has been a long sequence of results getting smaller and smaller upper bounds on r(N).<br/>
<br/>
The motivation for getting these results is that if r(N) is &lt; N/(log N)^{1+\delta} with delta&gt;0 then the following holds:<br/>
<br/>
If sum_{x\in A} 1/x diverges then A has a 3-AP.<br/>
<br/>
This is the k=3 case of one of the Erdos-Turan Conjectures.<br/>
<br/>
Bloom and Sisack HAVE gotten N/(log N)^{1+delta} so they HAVE gotten ET k=3. Wow!<br/>
<br/>
1) I am NOT surprised that its true.<br/>
<br/>
2) I am SHOCKED and DELIGHTED that it was proven. Shocked because the results leading up to it (see the write up referenced at the beginning of this post) seemed Zeno-like, approaching the result needed got but not getting there. Delighted because... uh, as the kids say, just cause.<br/>
<br/>
I've heard that k=4 really is much harder (see my comments and Gil's response on his blog post, pointed to at the beginning of this post) and it is true that there has been far less progress on that case (the write up I pointed to at the beginning of this post says what is known). Hence I will again be <i>shocked </i>if it is proven. So, unlike The Who (see<a href="https://www.youtube.com/watch?v=UDfAdHBtK_Q">here</a>) I CAN be fooled again. That's okay--- I will be <i>delighted</i>. (ADDED LATER- there are more comments no Gil's website, from Thomas Bloom and Ben Green about what is likely to happen in the next 10 years.)<br/>
<br/>
Erdos offered a prize of $3000 for a proof that A has, for all k, a k-AP. The prize is now $5000. After Erdos passed away Ronald Graham became the Erdos-Bank and paid out the money when people solved a problem Erdos put a bounty on. What happens now? (If I have the facts wrong and/or if you know the answer, please leave a polite and enlightening comment.)<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/></div>
    </content>
    <updated>2020-07-19T19:12:00Z</updated>
    <published>2020-07-19T19:12:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2020-07-21T11:25:13Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/109</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/109" rel="alternate" type="text/html"/>
    <title>TR20-109 |  On Testing Hamiltonicity in the Bounded Degree Graph Model | 

	Oded Goldreich</title>
    <summary>We show that testing Hamiltonicity in the bounded-degree graph model requires a linear number of queries. This refers to both the path and the cycle versions of the problem, and similar results hold also for the directed analogues.
In addition, we present an alternative proof for the known fact that testing Independent Set Size (in this model) requires a linear number of queries.</summary>
    <updated>2020-07-19T15:45:33Z</updated>
    <published>2020-07-19T15:45:33Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-07-22T01:48:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/108</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/108" rel="alternate" type="text/html"/>
    <title>TR20-108 |  Query Complexity of Global Minimum Cut | 

	Arijit Bishnu, 

	Arijit Ghosh, 

	Gopinath Mishra, 

	Manaswi Paraashar</title>
    <summary>In this work, we resolve the query complexity of global minimum cut problem for a graph by designing a randomized algorithm for approximating the size of minimum cut in a graph, where the graph can be accessed through local queries like \textsc{Degree}, \textsc{Neighbor}, and \textsc{Adjacency} queries.

Given $\epsilon \in (0,1)$, the algorithm with high probability outputs an estimate $\hat{t}$ satisfying the following $(1-\epsilon) t \leq \hat{t} \leq (1+\epsilon) t$, where $m$ is the number of edges in the graph and $t$ is the size of minimum cut in the graph. The expected number of local queries used by our algorithm is $\min\left\{m+n,\frac{m}{t}\right\}\mbox{poly}\left(\log n,\frac{1}{\epsilon}\right)$ where $n$ is the number of vertices in the graph. Eden and Rosenbaum showed that $\Omega(m/t)$ many local queries are required for approximating the size of minimum cut in graphs. These two results together resolve the query complexity of the problem of estimating the size of minimum cut in graphs using local queries.

Building on the lower bound of Eden and Rosenbaum, we show that, for all $t \in \mathbb{N}$, $\Omega(m)$ local queries are required to decide if the size of the minimum cut in the graph is $t$ or $t-2$. Also, we show that, for any $t \in \mathbb{N}$, $\Omega(m)$ local queries are required to find all the minimum cut edges even if it is promised that the input graph has a minimum cut of size $t$. Both of our lower bound results are randomized, and hold even if we can make \textsc{Random Edge} query apart from local queries.</summary>
    <updated>2020-07-19T13:02:59Z</updated>
    <published>2020-07-19T13:02:59Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-07-22T01:48:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/107</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/107" rel="alternate" type="text/html"/>
    <title>TR20-107 |  Testing linear inequalities of subgraph statistics | 

	Lior Gishboliner, 

	Asaf Shapira, 

	Henrique Stagni</title>
    <summary>Property testers are fast randomized algorithms whose task is to distinguish between inputs satisfying some predetermined property ${\cal P}$ and those that are far from satisfying it. Since these algorithms operate by inspecting a small randomly selected portion of the input, the most natural property one would like to be able to test is whether the input does not contain certain forbidden small substructures. In the setting of graphs, such a result was obtained by Alon et al., who proved that for any finite family of graphs ${\cal F}$, the property of being induced ${\cal F}$-free (i.e. not containing an induced copy of any $F \in {\cal F}$) is testable.

It is natural to ask if one can go one step further and prove that more elaborate properties involving induced subgraphs are also testable. One such generalization of the result of Alon et al. was formulated by Goldreich and Shinkar who conjectured that for any finite family of graphs ${\cal F}$, and any linear inequality involving the densities of the graphs $F \in {\cal F}$ in the input graph,
the property of satisfying this inequality can be tested in a certain restricted model of graph property testing. Our main result in this paper disproves this conjecture in the following strong form: some properties of this type are not testable even in the classical (i.e. unrestricted) model of graph property testing.

The proof deviates significantly from prior non-testability results in this area. The main idea is to use a linear inequality relating induced subgraph densities in order to encode the property of being a quasirandom graph.</summary>
    <updated>2020-07-19T13:01:55Z</updated>
    <published>2020-07-19T13:01:55Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2020-07-22T01:48:01Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17307</id>
    <link href="https://rjlipton.wordpress.com/2020/07/18/mathematical-search/" rel="alternate" type="text/html"/>
    <title>Mathematical Search</title>
    <summary>A flying start from nearby Rochester Anurag Agarwal and Richard Zanibbi are tenured faculty in Mathematics and Computer Science, respectively, at RIT. They partner with Clyde Lee Giles of Penn State and Douglas Oard of U.Md. on the MathSeer project. If the name reminds you of CiteSeer, no surprise: Giles co-originated that and still directs []</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>A flying start from nearby Rochester</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wordpress.com/2020/07/18/mathematical-search/agarwalzanibbi/" rel="attachment wp-att-17309"><img alt="" class="alignright size-thumbnail wp-image-17309" height="142" src="https://rjlipton.files.wordpress.com/2020/07/agarwalzanibbi.png?w=150&amp;h=142" width="150"/></a></p>
<p>
Anurag Agarwal and Richard Zanibbi are tenured faculty in Mathematics and Computer Science, respectively, at RIT. They partner with Clyde Lee Giles of Penn State and Douglas Oard of U.Md. on the <a href="https://www.cs.rit.edu/~dprl/mathseer/">MathSeer</a> project. If the name reminds you of <a href="http://citeseerx.ist.psu.edu/">CiteSeer<img alt="{{\,}^x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5C%2C%7D%5Ex%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\,}^x}"/></a>, no surprise: Giles co-originated that and still directs it.</p>
<p>
Today we note last months release of a major piece of MathSeer called <a href="https://mathdeck.cs.rit.edu/">MathDeck</a> and show how to have fun with it.</p>
<p>
Agarwal is a PhD graduate from Buffalo. He did his thesis in the Mathematics Department under Thomas Cusick on cryptography, but often visited Computer Science. He took part in seminars on lattice-based cryptography led by Jin-Yi Cai when he was in Buffalo and in one of mine on related topics. I also knew him socially as a housemate of Pavan Aduri, whose joint work weve mentioned <a href="https://rjlipton.wordpress.com/2012/07/14/it-dont-come-easy/">here</a>. </p>
<p>
A long time ago, Dick wrote a <a href="https://rjlipton.wordpress.com/2010/12/20/some-mathematical-gifts/">post</a> on <a href="http://detexify.kirelabs.org/classify.html">Detexify</a>, which does optical character recognition (OCR) for mathematical symbols and finds corresponding (La)TeX commands. <em>MathDeck</em> does OCR as well, but what it is really trying to recognize is the <em>formula</em> you are trying to write. If it is a famous formulaor one you have already saved in your deckit will find and complete it for you. It also takes input from LaTeX. <em>MathDeck</em> was created by Gavin Nishizawa, Jennifer Liu, Yancarlos Diaz, Abishai Dmello, and Wei Zhong along with Zanibbi. They are credited on a brief <a href="https://www.cs.rit.edu/~rlaz/files/ECIR2020_MathDeck_Demo.pdf">paper</a> and <a href="https://www.cs.rit.edu/~dprl/mathseer/demos.html">video</a>.</p>
<p>
</p><p/><h2> Trying MathDeck </h2><p/>
<p/><p>
On first visit, the site shows a very brief tutorial which can be dismissed via a sometimes-invisible X in the upper-right corner. The first formula I thought to write was Leonhard Eulers mystic equation <img alt="{e^{i\pi} + 1 = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%5E%7Bi%5Cpi%7D+%2B+1+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e^{i\pi} + 1 = 0}"/>. Its OCR sprang into action as I drew and converted my attempt to draw a curly <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> and then <img alt="{\pi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\pi}"/> as <img alt="{7n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B7n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{7n}"/> with an extra crossbar.  (The website graphics are much sharper than our screenshots here.)</p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2020/07/18/mathematical-search/eulerformulamathdeck/" rel="attachment wp-att-17310"><img alt="" class="aligncenter wp-image-17310" height="400" src="https://rjlipton.files.wordpress.com/2020/07/eulerformulamathdeck.jpg?w=540&amp;h=400" width="540"/></a></p>
<p/><p><br/>
Nevertheless, its default deck of WikiCards recognizes the attempt and includes Eulers Identity as an option. Selecting the card changes the display to an immaculately typeset version. I then decided to change it to the form <img alt="{e^{i\pi} = -1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%5E%7Bi%5Cpi%7D+%3D+-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e^{i\pi} = -1}"/>. <em>MathDeck</em> does not have a pixel eraser like Microsoft Paint does but allows you to delete a region after selecting it:</p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2020/07/18/mathematical-search/eulerformulatypeset/" rel="attachment wp-att-17312"><img alt="" class="aligncenter wp-image-17312" height="190" src="https://rjlipton.files.wordpress.com/2020/07/eulerformulatypeset.jpg?w=360&amp;h=190" width="360"/></a></p>
<p/><p><br/>
Selecting trash did not close up the space to the equals sign. I drew a minus bar at far right, but my attempts to follow with <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> kept being interpreted as <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> or something worse, and the alternate form of Eulers identity did not come up below. Finally, I restored the LaTeX-input box on the right and edited the source to read, <font size="+1"><tt>e^{i\pi} = -1</tt></font>:</p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2020/07/18/mathematical-search/eulerformulatypeset2/" rel="attachment wp-att-17314"><img alt="" class="aligncenter wp-image-17314" height="175" src="https://rjlipton.files.wordpress.com/2020/07/eulerformulatypeset2.jpg?w=540&amp;h=175" width="540"/></a></p>
<p/><p><br/>
The artifact of my hand-drawn minus sign was still at far right. I could not select and trash it even after refreshing the page. What fixed the issue was drawing something else over the squiggle and having the OCR interpret the tandem into something else, which I could then select and delete. </p>
<p>
</p><p/><h2> Superposing Forms and Associations </h2><p/>
<p/><p>
I next wondered how the system would react to my trying to write Schrdingers equation. One challenge is that it has many forms. I chose the form given uppermost in Wikipedias <a href="https://en.wikipedia.org/wiki/Schrodinger_equation#Equation">article</a>: </p>
<p align="center"><img alt="\displaystyle  i\hbar \frac{d |\Psi(t)\rangle}{d t} = \hat{H}|\Psi(t)\rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++i%5Chbar+%5Cfrac%7Bd+%7C%5CPsi%28t%29%5Crangle%7D%7Bd+t%7D+%3D+%5Chat%7BH%7D%7C%5CPsi%28t%29%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  i\hbar \frac{d |\Psi(t)\rangle}{d t} = \hat{H}|\Psi(t)\rangle "/></p>
<p>To arrive at the challenge gradually, I first omitted the quantum <em>ket</em> notation, the hat on <img alt="{H}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H}"/>, and the dependence on <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>. I wrote partial derivatives and lowercase <img alt="{\psi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cpsi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\psi}"/>. Thus what I first tried to handwrite was: </p>
<p align="center"><img alt="\displaystyle  i\hbar \frac{\partial \psi}{\partial t} = H\psi " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++i%5Chbar+%5Cfrac%7B%5Cpartial+%5Cpsi%7D%7B%5Cpartial+t%7D+%3D+H%5Cpsi+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  i\hbar \frac{\partial \psi}{\partial t} = H\psi "/></p>
<p>The system jumped on my handwriting right away and I wont report the results except to say it looked like Dada art with math symbols. One can, however, do a drawing in Paint or similar app and upload it. So I drew</p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2020/07/18/mathematical-search/schrodingerdrawing/" rel="attachment wp-att-17315"><img alt="" class="aligncenter size-full wp-image-17315" src="https://rjlipton.files.wordpress.com/2020/07/schrodingerdrawing.png?w=600"/></a></p>
<p/><p><br/>
and obtained</p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2020/07/18/mathematical-search/schrodingerrendered/" rel="attachment wp-att-17316"><img alt="" class="aligncenter wp-image-17316" height="260" src="https://rjlipton.files.wordpress.com/2020/07/schrodingerrendered.jpg?w=360&amp;h=260" width="360"/></a></p>
<p/><p><br/>
The ten cards returned (one is below the snip) have some wild but inspired associations. Handwriting Wikipedias form as given did not produce better results. However, I realized I could snip what appears on Wikipedia and upload that:</p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2020/07/18/mathematical-search/schrodingersnip/" rel="attachment wp-att-17317"><img alt="" class="aligncenter size-full wp-image-17317" src="https://rjlipton.files.wordpress.com/2020/07/schrodingersnip.jpg?w=600"/></a></p>
<p/><p><br/>
The result was a ghostly evocation of the equation, with LaTeX to boot:</p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2020/07/18/mathematical-search/schrodingerdada/" rel="attachment wp-att-17318"><img alt="" class="aligncenter wp-image-17318" height="78" src="https://rjlipton.files.wordpress.com/2020/07/schrodingerdada.jpg?w=540&amp;h=78" width="540"/></a></p>
<p/><p><br/>
The LaTeX output reminded me that I could enter Schrdingers equation in LaTeX and remove all doubt. I did the short form first. Would the system recognize it?</p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2020/07/18/mathematical-search/schrodinger1/" rel="attachment wp-att-17319"><img alt="" class="aligncenter wp-image-17319" height="328" src="https://rjlipton.files.wordpress.com/2020/07/schrodinger1.jpg?w=540&amp;h=328" width="540"/></a></p>
<p/><p><br/>
The name <em>Schrdinger</em> popped up in the eighth card at bottom center, but not the form I had typed. It has an integral and no equals sign. Of greater note, the center card called up another giant of quantum mechanics and began with exactly the left-hand side I had typed. To its left came Wolfgang Pauli with another occurrence of <img alt="{i\hbar}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%5Chbar%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i\hbar}"/>. None of the output had the time variable <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>, however, so I put it in:</p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2020/07/18/mathematical-search/schrodinger2/" rel="attachment wp-att-17320"><img alt="" class="aligncenter wp-image-17320" height="349" src="https://rjlipton.files.wordpress.com/2020/07/schrodinger2.jpg?w=540&amp;h=349" width="540"/></a></p>
<p/><p><br/>
Bingothe card labeled simply <em>Schrdinger Equation</em> appears at upper center. Unlike Wikipedias version, it includes <b>r</b> standing for other coordinates andhenceproperly uses partial derivatives. Otherwise it is exactly what my search intended to summon.</p>
<p>
I felt the real reward came in the other cards. I did not know that Hubbles law had such a simple statement. I knew quantum mechanics takes glory in symmetries, of course, but did not know what equation would bear the definitive moniker, Symmetry in Quantum Mechanics. I remember as a child the fun of unstructured time in libraries where the physical card catalog was sorted by theme and one could browse adjacent ideas, as also on the shelves. </p>
<p>
Now I put in the ket notation, the hat to make <img alt="{\hat{H}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Chat%7BH%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\hat{H}}"/>, and the uppercase <img alt="{\Psi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CPsi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Psi}"/>. The latter two changes evidently moved the <em>Schrdinger Equation</em> card to the top of the deck, with <em>Pauli Equation</em> moving up behind it, but other cards completely changed:</p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2020/07/18/mathematical-search/schrodinger3/" rel="attachment wp-att-17321"><img alt="" class="aligncenter wp-image-17321" height="349" src="https://rjlipton.files.wordpress.com/2020/07/schrodinger3.jpg?w=540&amp;h=349" width="540"/></a></p>
<p/><p><br/>
I had not known the term <em>Einselection</em>. Finally, I decided to wipe the canvas and simply enter <img alt="{H\Psi = E\Psi}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH%5CPsi+%3D+E%5CPsi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{H\Psi = E\Psi}"/> as the minimalist form of Schrdingers equation. Ill leave you to see what the system comes up with on your own fresh canvas.</p>
<p>
</p><p/><h2> Search and Research </h2><p/>
<p/><p>
The goal is to augment search that includes equations as well as text. For instance, right now Id like to find sources that use a formula like </p>
<p align="center"><img alt="\displaystyle  \sum_{r=0}^{\infty}\frac{r^2}{e^{ar}} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Br%3D0%7D%5E%7B%5Cinfty%7D%5Cfrac%7Br%5E2%7D%7Be%5E%7Bar%7D%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{r=0}^{\infty}\frac{r^2}{e^{ar}} "/></p>
<p>in the context of statistical tests for distinguishing between distributions. Ive had success with Google on smaller pieces of TeX but this chunk yields nothing sensible. Changing the variable <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> to <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> changes some of the results but comes no closer; nor does changing to <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> or editing the sum to begin with <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/> not <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>. The search should somehow recognize that <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a}"/> is a constant but is also the main parameter.</p>
<p>
The fact that Ive typed a particular LaTeX form might be an impediment. I could have written <img alt="{r^2 e^{-ar}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%5E2+e%5E%7B-ar%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r^2 e^{-ar}}"/> in the body of the sum without using a fraction. The <em>MathDeck</em> documentation focuses on enabling mathematical search for non-LaTeX users, but independence from syntax for all users is a commensurate goal. The idea is to make formulas chunks in their own right, chunks governed by semantics more than syntax, and promote saving and recombining them. For instance, I could save the body and replace the sum by an integral. </p>
<p>
The visual unit for this in <em>MathDeck</em> is the blue oval enclosing a formula. They can be created and edited at the top, imported to make a new <em>card</em>, and combined onto the canvas to build up larger formulas. The paper calls them chips but for me they evoke hieroglyphic <a href="https://en.wikipedia.org/wiki/Cartouche">cartouches</a> enclosing royal names. Cards can be marked as favorites and the collection added to. </p>
<p>
Here is an example where I made a cartouche and card out of the abstract form of the main equation in my chess model, as I expounded in a <a href="https://rjlipton.wordpress.com/2018/10/18/london-calling/">post</a> to mark the 2018 world chess championship match in London.</p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2020/07/18/mathematical-search/chessequation/" rel="attachment wp-att-17324"><img alt="" class="aligncenter wp-image-17324" height="368" src="https://rjlipton.files.wordpress.com/2020/07/chessequation.jpg?w=600&amp;h=368" width="600"/></a></p>
<p/><p><br/>
Once again the system pitches in with interesting associations. Some were expected but others are surprises. Logit and logarithmic loss are naturally associated but I had not heard of Perplexity, and what is Benfords Law doing here? (Dick and I have been trying to find natural <em>exceptions</em> to Benfords Law in Covid-19 statisticsweve not had time yet to tell whether well succeed.)</p>
<p>
The searches at top are still vanilla Google search, and the search below the canvas is only within a deck or decks. We look forward to when a truly smart integration of mathematics into major search engines will be engendered by this project.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
How do you see <em>MathDeck</em> and the larger <em>MathSeer</em> project growing in the near future? We hope <em>MathDeck</em> stokes some immediate enjoyment and curiosity.</p>
<p/></font></font></div>
    </content>
    <updated>2020-07-18T16:11:54Z</updated>
    <published>2020-07-18T16:11:54Z</published>
    <category term="All Posts"/>
    <category term="News"/>
    <category term="Teaching"/>
    <category term="Anurag Agarwal"/>
    <category term="CiteSeer"/>
    <category term="Clyde Giles"/>
    <category term="Douglas Oard"/>
    <category term="MathDeck"/>
    <category term="mathematical writing"/>
    <category term="MathSeer"/>
    <category term="Richard Zanibbi"/>
    <category term="web search"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gdels Lost Letter and P=NP</title>
      <updated>2020-07-22T01:49:24Z</updated>
    </source>
  </entry>
</feed>
