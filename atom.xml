<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-10-31T14:21:50Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/146</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/146" rel="alternate" type="text/html"/>
    <title>TR19-146 |  Dynamic Kernels for Hitting Sets and Set Packing | 

	Max Max Bannach, 

	Zacharias Heinrich, 

	Rüdiger Reischuk, 

	Till Tantau</title>
    <summary>Computing kernels for the hitting set problem (the problem of
  finding a size-$k$ set that intersects each hyperedge of a
  hypergraph) is a well-studied computational problem. For hypergraphs
  with $m$ hyperedges, each of size at most~$d$, the best algorithms
  can compute kernels of size $O(k^d)$ in time $O(2^d m)$. In this
  paper we generalize the task to the dynamic setting where
  hyperedges may be continuously added and deleted and we always have
  to keep track of a hitting set kernel (including moments when no
  size-$k$ hitting set exists).  We present a deterministic solution,
  based on a novel data structure, that needs worst-case time
  $O^*(3^d)$ for updating the kernel upon hyperedge inserts and
  time~$O^*(5^d)$ for updates upon deletions -- thus nearly matching
  the time $O^*(2^d)$ needed by the best static algorithm per
  hyperedge. As a novel technical feature, our approach does not use
  the standard replace-sunflowers-by-their-cores methodology, but
  introduces a generalized concept that is actually easier to compute
  and that allows us to achieve a kernel size of $\sum_{i=0}^d k^i$
  rather than the typical size $d!\cdot k^d$ resulting from the Sunflower
  Lemma. We also show that our approach extends to the dual problem of
  finding packings in hypergraphs (the problem of finding $k$ pairwise
  disjoint hyperedges), albeit with a slightly larger kernel size of
  $\sum_{i=0}^d d^i(k-1)^i$.</summary>
    <updated>2019-10-31T13:46:55Z</updated>
    <published>2019-10-31T13:46:55Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-10-31T14:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/145</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/145" rel="alternate" type="text/html"/>
    <title>TR19-145 |  XOR Lemmas for Resilient Functions Against Polynomials | 

	Eshan Chattopadhyay, 

	Pooya Hatami, 

	Kaave Hosseini, 

	Shachar Lovett, 

	David Zuckerman</title>
    <summary>A major challenge in complexity theory is to explicitly construct functions that have small correlation with low-degree polynomials over $F_2$. We introduce a  new technique to prove such correlation bounds with $F_2$ polynomials.  Using this technique, we bound the correlation of an XOR of Majorities with constant degree polynomials. In fact, we prove a more general XOR lemma that extends to arbitrary resilient functions.   We conjecture that the technique generalizes to higher degree polynomials as well.

A key ingredient in our new approach is  a structural result  about the Fourier spectrum of low degree polynomials over  $F_2$. We show that for any n-variate polynomial $p$ over $F_2$ of degree at most $d$, there is a small set $S \subset [n]$ of variables, such that almost all of the  Fourier mass of $p$ lies on Fourier coefficients that intersect with $S$. In fact our result is more general, and finds such a set $S$ for any low-dimensional subspace of polynomials. This generality is crucial in deriving the new XOR lemmas.</summary>
    <updated>2019-10-31T06:42:35Z</updated>
    <published>2019-10-31T06:42:35Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-10-31T14:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=375</id>
    <link href="https://tcsplus.wordpress.com/2019/10/31/tcs-talk-wednesday-november-6-ryan-odonnell-cmu/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, November 6 — Ryan O’Donnell, CMU</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, November 6th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). Ryan O’Donnell from CMU will speak about “Explicit near-Ramanujan graphs of every degree” (abstract below). Please make sure you reserve a spot for your group to join us […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, November 6th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 18:00 UTC). <strong>Ryan O’Donnell</strong> from CMU will speak about “<em>Explicit near-Ramanujan graphs of every degree</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: For every constant <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=fff&amp;fg=444444&amp;s=0" title="d"/> and <img alt="\varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=fff&amp;fg=444444&amp;s=0" title="\varepsilon"/>, we give a deterministic <img alt="\text{poly}(n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctext%7Bpoly%7D%28n%29&amp;bg=fff&amp;fg=444444&amp;s=0" title="\text{poly}(n)"/>-time algorithm that outputs a <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=fff&amp;fg=444444&amp;s=0" title="d"/>-regular graph on <img alt="\approx n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Capprox+n&amp;bg=fff&amp;fg=444444&amp;s=0" title="\approx n"/> vertices that is <img alt="\varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvarepsilon&amp;bg=fff&amp;fg=444444&amp;s=0" title="\varepsilon"/>-near-Ramanujan; i.e., its eigenvalues are bounded in magnitude by <img alt="2\sqrt(d-1) + \varepsilon" class="latex" src="https://s0.wp.com/latex.php?latex=2%5Csqrt%28d-1%29+%2B+%5Cvarepsilon&amp;bg=fff&amp;fg=444444&amp;s=0" title="2\sqrt(d-1) + \varepsilon"/> (excluding the single trivial eigenvalue of <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=fff&amp;fg=444444&amp;s=0" title="d"/>).</p>
<p>Joint work with Sidhanth Mohanty (Berkeley) and Pedro Paredes (CMU).</p></blockquote></div>
    </content>
    <updated>2019-10-31T04:48:39Z</updated>
    <published>2019-10-31T04:48:39Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2019-10-31T14:21:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13984</id>
    <link href="http://arxiv.org/abs/1910.13984" rel="alternate" type="text/html"/>
    <title>Learning-Based Low-Rank Approximations</title>
    <feedworld_mtime>1572480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Indyk:Piotr.html">Piotr Indyk</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vakilian:Ali.html">Ali Vakilian</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yuan:Yang.html">Yang Yuan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13984">PDF</a><br/><b>Abstract: </b>We introduce a "learning-based" algorithm for the low-rank decomposition
problem: given an $n \times d$ matrix $A$, and a parameter $k$, compute a
rank-$k$ matrix $A'$ that minimizes the approximation loss $\|A-A'\|_F$. The
algorithm uses a training set of input matrices in order to optimize its
performance. Specifically, some of the most efficient approximate algorithms
for computing low-rank approximations proceed by computing a projection $SA$,
where $S$ is a sparse random $m \times n$ "sketching matrix", and then
performing the singular value decomposition of $SA$. We show how to replace the
random matrix $S$ with a "learned" matrix of the same sparsity to reduce the
error.
</p>
<p>Our experiments show that, for multiple types of data sets, a learned sketch
matrix can substantially reduce the approximation loss compared to a random
matrix $S$, sometimes by one order of magnitude. We also study mixed matrices
where only some of the rows are trained and the remaining ones are random, and
show that matrices still offer improved performance while retaining worst-case
guarantees.
</p>
<p>Finally, to understand the theoretical aspects of our approach, we study the
special case of $m=1$. In particular, we give an approximation algorithm for
minimizing the empirical loss, with approximation factor depending on the
stable rank of matrices in the training set. We also show generalization bounds
for the sketch matrix learning problem.
</p></div>
    </summary>
    <updated>2019-10-31T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13971</id>
    <link href="http://arxiv.org/abs/1910.13971" rel="alternate" type="text/html"/>
    <title>Superset Technique for Approximate Recovery in One-Bit Compressed Sensing</title>
    <feedworld_mtime>1572480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Flodin:Larkin.html">Larkin Flodin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gandikota:Venkata.html">Venkata Gandikota</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mazumdar:Arya.html">Arya Mazumdar</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13971">PDF</a><br/><b>Abstract: </b>One-bit compressed sensing (1bCS) is a method of signal acquisition under
extreme measurement quantization that gives important insights on the limits of
signal compression and analog-to-digital conversion. The setting is also
equivalent to the problem of learning a sparse hyperplane-classifier. In this
paper, we propose a generic approach for signal recovery in nonadaptive 1bCS
that leads to improved sample complexity for approximate recovery for a variety
of signal models, including nonnegative signals and binary signals. We
construct 1bCS matrices that are universal - i.e. work for all signals under a
model - and at the same time recover very general random sparse signals with
high probability. In our approach, we divide the set of samples (measurements)
into two parts, and use the first part to recover the superset of the support
of a sparse vector. The second set of measurements is then used to approximate
the signal within the superset. While support recovery in 1bCS is well-studied,
recovery of superset of the support requires fewer samples, which then leads to
an overall reduction in sample complexity for approximate recovery.
</p></div>
    </summary>
    <updated>2019-10-31T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13900</id>
    <link href="http://arxiv.org/abs/1910.13900" rel="alternate" type="text/html"/>
    <title>On a Decentralized $(\Delta{+}1)$-Graph Coloring Algorithm</title>
    <feedworld_mtime>1572480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chakrabarty:Deeparnab.html">Deeparnab Chakrabarty</a>, Paul de Supinski <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13900">PDF</a><br/><b>Abstract: </b>We consider a decentralized graph coloring model where each vertex only knows
its own color and whether some neighbor has the same color as it. The
networking community has studied this model extensively due to its applications
to channel selection, rate adaptation, etc. Here, we analyze variants of a
simple algorithm of Bhartia et al. [Proc., ACM MOBIHOC, 2016]. In particular,
we introduce a variant which requires only $O(n\log\Delta)$ expected
recolorings that generalizes the coupon collector problem. Finally, we show
that the $O(n\Delta)$ bound Bhartia et al. achieve for their algorithm still
holds and is tight in adversarial scenarios.
</p></div>
    </summary>
    <updated>2019-10-31T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13874</id>
    <link href="http://arxiv.org/abs/1910.13874" rel="alternate" type="text/html"/>
    <title>Group Centrality Maximization for Large-scale Graphs</title>
    <feedworld_mtime>1572480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Angriman:Eugenio.html">Eugenio Angriman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Grinten:Alexander_van_der.html">Alexander van der Grinten</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bojchevski:Aleksandar.html">Aleksandar Bojchevski</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Z=uuml=gner:Daniel.html">Daniel Zügner</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/G=uuml=nnemann:Stephan.html">Stephan Günnemann</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Meyerhenke:Henning.html">Henning Meyerhenke</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13874">PDF</a><br/><b>Abstract: </b>The study of vertex centrality measures is a key aspect of network analysis.
Naturally, such centrality measures have been generalized to groups of
vertices; for popular measures it was shown that the problem of finding the
most central group is $\mathcal{NP}$-hard. As a result, approximation
algorithms to maximize group centralities were introduced recently. Despite a
nearly-linear running time, approximation algorithms for group betweenness and
(to a lesser extent) group closeness are rather slow on large networks due to
high constant overheads.
</p>
<p>That is why we introduce GED-Walk centrality, a new submodular group
centrality measure inspired by Katz centrality. In contrast to closeness and
betweenness, it considers walks of any length rather than shortest paths, with
shorter walks having a higher contribution. We define algorithms that (i)
efficiently approximate the GED-Walk score of a given group and (ii)
efficiently approximate the (proved to be $\mathcal{NP}$-hard) problem of
finding a group with highest GED-Walk score.
</p>
<p>Experiments on several real-world datasets show that scores obtained by
GED-Walk improve performance on common graph mining tasks such as collective
classification and graph-level classification. An evaluation of empirical
running times demonstrates that maximizing GED-Walk (in approximation) is two
orders of magnitude faster compared to group betweenness approximation and for
group sizes $\leq 100$ one to two orders faster than group closeness
approximation. For graphs with tens of millions of edges, approximate GED-Walk
maximization typically needs less than one minute. Furthermore, our experiments
suggest that the maximization algorithms scale linearly with the size of the
input graph and the size of the group.
</p></div>
    </summary>
    <updated>2019-10-31T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13830</id>
    <link href="http://arxiv.org/abs/1910.13830" rel="alternate" type="text/html"/>
    <title>Extreme Classification in Log Memory using Count-Min Sketch: A Case Study of Amazon Search with 50M Products</title>
    <feedworld_mtime>1572480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Medini:Tharun.html">Tharun Medini</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Qixuan.html">Qixuan Huang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Yiqiu.html">Yiqiu Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mohan:Vijai.html">Vijai Mohan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shrivastava:Anshumali.html">Anshumali Shrivastava</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13830">PDF</a><br/><b>Abstract: </b>In the last decade, it has been shown that many hard AI tasks, especially in
NLP, can be naturally modeled as extreme classification problems leading to
improved precision. However, such models are prohibitively expensive to train
due to the memory blow-up in the last layer. For example, a reasonable softmax
layer for the dataset of interest in this paper can easily reach well beyond
100 billion parameters (&gt;400 GB memory). To alleviate this problem, we present
Merged-Average Classifiers via Hashing (MACH), a generic K-classification
algorithm where memory provably scales at O(logK) without any strong assumption
on the classes. MACH is subtly a count-min sketch structure in disguise, which
uses universal hashing to reduce classification with a large number of classes
to few embarrassingly parallel and independent classification tasks with a
small (constant) number of classes. MACH naturally provides a technique for
zero communication model parallelism. We experiment with 6 datasets; some
multiclass and some multilabel, and show consistent improvement over respective
state-of-the-art baselines. In particular, we train an end-to-end deep
classifier on a private product search dataset sampled from Amazon Search
Engine with 70 million queries and 49.46 million products. MACH outperforms, by
a significant margin,the state-of-the-art extreme classification models
deployed on commercial search engines: Parabel and dense embedding models. Our
largest model has 6.4 billion parameters and trains in less than 35 hours on a
single p3.16x machine. Our training times are 7-10x faster, and our memory
footprints are 2-4x smaller than the best baselines. This training time is also
significantly lower than the one reported by Google's mixture of experts (MoE)
language model on a comparable model size and hardware.
</p></div>
    </summary>
    <updated>2019-10-31T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13765</id>
    <link href="http://arxiv.org/abs/1910.13765" rel="alternate" type="text/html"/>
    <title>Solving Parity Games Using An Automata-Based Algorithm</title>
    <feedworld_mtime>1572480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stasio:Antonio_Di.html">Antonio Di Stasio</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Murano:Aniello.html">Aniello Murano</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Perelli:Giuseppe.html">Giuseppe Perelli</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vardi:Moshe_Y=.html">Moshe Y. Vardi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13765">PDF</a><br/><b>Abstract: </b>Parity games are abstract infinite-round games that take an important role in
formal verification. In the basic setting, these games are two-player,
turn-based, and played under perfect information on directed graphs, whose
nodes are labeled with priorities. The winner of a play is determined according
to the parities (even or odd) of the minimal priority occurring infinitely
often in that play. The problem of finding a winning strategy in parity games
is known to be in UPTime $\cap$ CoUPTime and deciding whether a polynomial time
solution exists is a long-standing open question. In the last two decades, a
variety of algorithms have been proposed. Many of them have been also
implemented in a platform named PGSolver. This has enabled an empirical
evaluation of these algorithms and a better understanding of their relative
merits. In this paper, we further contribute to this subject by implementing,
for the first time, an algorithm based on alternating automata. More precisely,
we consider an algorithm introduced by Kupferman and Vardi that solves a parity
game by solving the emptiness problem of a corresponding alternating parity
automaton. Our empirical evaluation demonstrates that this algorithm
outperforms other algorithms when the game has a a small number of priorities
relative to the size of the game. In many concrete applications, we do indeed
end up with parity games where the number of priorities is relatively small.
This makes the new algorithm quite useful in practice.
</p></div>
    </summary>
    <updated>2019-10-31T01:23:41Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13618</id>
    <link href="http://arxiv.org/abs/1910.13618" rel="alternate" type="text/html"/>
    <title>Optimal Analysis of Subset-Selection Based L_p Low Rank Approximation</title>
    <feedworld_mtime>1572480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dan:Chen.html">Chen Dan</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Hong.html">Hong Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhang:Hongyang.html">Hongyang Zhang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Yuchen.html">Yuchen Zhou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ravikumar:Pradeep.html">Pradeep Ravikumar</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13618">PDF</a><br/><b>Abstract: </b>We study the low rank approximation problem of any given matrix $A$ over
$\mathbb{R}^{n\times m}$ and $\mathbb{C}^{n\times m}$ in entry-wise $\ell_p$
loss, that is, finding a rank-$k$ matrix $X$ such that $\|A-X\|_p$ is
minimized. Unlike the traditional $\ell_2$ setting, this particular variant is
NP-Hard. We show that the algorithm of column subset selection, which was an
algorithmic foundation of many existing algorithms, enjoys approximation ratio
$(k+1)^{1/p}$ for $1\le p\le 2$ and $(k+1)^{1-1/p}$ for $p\ge 2$. This improves
upon the previous $O(k+1)$ bound for $p\ge 1$
\cite{chierichetti2017algorithms}. We complement our analysis with lower
bounds; these bounds match our upper bounds up to constant $1$ when $p\geq 2$.
At the core of our techniques is an application of \emph{Riesz-Thorin
interpolation theorem} from harmonic analysis, which might be of independent
interest to other algorithmic designs and analysis more broadly.
</p>
<p>As a consequence of our analysis, we provide better approximation guarantees
for several other algorithms with various time complexity. For example, to make
the algorithm of column subset selection computationally efficient, we analyze
a polynomial time bi-criteria algorithm which selects $O(k\log m)$ columns. We
show that this algorithm has an approximation ratio of $O((k+1)^{1/p})$ for
$1\le p\le 2$ and $O((k+1)^{1-1/p})$ for $p\ge 2$. This improves over the
best-known bound with an $O(k+1)$ approximation ratio. Our bi-criteria
algorithm also implies an exact-rank method in polynomial time with a slightly
larger approximation ratio.
</p></div>
    </summary>
    <updated>2019-10-31T01:35:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13615</id>
    <link href="http://arxiv.org/abs/1910.13615" rel="alternate" type="text/html"/>
    <title>Asymptotic Divergences and Strong Dichotomy</title>
    <feedworld_mtime>1572480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Xiang.html">Xiang Huang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lutz:Jack_H=.html">Jack H. Lutz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mayordomo:Elvira.html">Elvira Mayordomo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stull:Donald_M=.html">Donald M. Stull</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13615">PDF</a><br/><b>Abstract: </b>The Schnorr-Stimm dichotomy theorem concerns finite-state gamblers that bet
on infinite sequences of symbols taken from a finite alphabet $\Sigma$.
</p>
<p>In this paper we use the Kullback-Leibler divergence to formulate the
$\textit{lower asymptotic divergence}$ $\text{div}(S||\alpha)$ of a probability
measure $\alpha$ on $\Sigma$ from a sequence $S$ over $\Sigma$ and the
$\textit{upper asymptotic divergence}$ $\text{Div}(S||\alpha)$ of $\alpha$ from
$S$ in such a way that a sequence $S$ is $\alpha$-normal (meaning that every
string $w$ has asymptotic frequency $\alpha(w)$ in $S$) if and only if
$\text{Div}(S||\alpha)=0$. We also use the Kullback-Leibler divergence to
quantify the $\textit{total risk }$ $\text{Risk}_G(w)$ that a finite-state
gambler $G$ takes when betting along a prefix $w$ of $S$.
</p>
<p>Our main theorem is a $\textit{strong dichotomy theorem}$ that uses the above
notions to $\textit{quantify}$ the exponential rates of winning and losing on
the two sides of the Schnorr-Stimm dichotomy theorem (with the latter routinely
extended from normality to $\alpha$-normality). Modulo asymptotic caveats in
the paper, our strong dichotomy theorem says that the following two things hold
for prefixes $w$ of $S$.
</p>
<p>(1) The infinitely-often exponential rate of winning is
$2^{\text{Div}(S||\alpha)|w|}$.
</p>
<p>(2) The exponential rate of loss is $2^{-\text{Risk}_G(w)}$.
</p>
<p>We also use (1) to show that $1-\text{Div}(S||\alpha)/c$, where $c= \log(1/
\min_{a\in\Sigma}\alpha(a))$, is an upper bound on the finite-state
$\alpha$-dimension of $S$ and prove the dual fact that
$1-\text{div}(S||\alpha)/c$ is an upper bound on the finite-state strong
$\alpha$-dimension of $S$.
</p></div>
    </summary>
    <updated>2019-10-31T01:21:21Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-10-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13543</id>
    <link href="http://arxiv.org/abs/1910.13543" rel="alternate" type="text/html"/>
    <title>An Adaptive Step Toward the Multiphase Conjecture</title>
    <feedworld_mtime>1572480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Ko:Young_Kun.html">Young Kun Ko</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Weinstein:Omri.html">Omri Weinstein</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13543">PDF</a><br/><b>Abstract: </b>In 2010, P\v{a}tra\c{s}cu proposed the following three-phase dynamic problem,
as a candidate for proving polynomial lower bounds on the operational time of
dynamic data structures:
</p>
<p>I: Preprocess a collection of sets $\vec{S} = S_1, \ldots , S_k \subseteq
[n]$, where $k=\operatorname{poly}(n)$.
</p>
<p>II: A set $T\subseteq [n]$ is revealed, and the data structure updates its
memory.
</p>
<p>III: An index $i \in [k]$ is revealed, and the data structure must determine
if $S_i\cap T=^? \emptyset$.
</p>
<p>P\v{a}tra\c{s}cu conjectured that any data structure for the Multiphase
problem must make $n^\epsilon$ cell-probes in either Phase II or III, and
showed that this would imply similar unconditional lower bounds on many
important dynamic data structure problems. Alas, there has been almost no
progress on this conjecture in the past decade since its introduction. We show
an $\tilde{\Omega}(\sqrt{n})$ cell-probe lower bound on the Multiphase problem
for data structures with general (adaptive) updates, and queries with unbounded
but "layered" adaptivity. This result captures all known set-intersection data
structures and significantly strengthens previous Multiphase lower bounds,
which only captured non-adaptive data structures.
</p>
<p>Our main technical result is a communication lower bound on a 4-party variant
of P\v{a}tra\c{s}cu's Number-On-Forehead Multiphase game, using information
complexity techniques. We also show that a lower bound on P\v{a}tra\c{s}cu's
original NOF game would imply a polynomial ($n^{1+\epsilon}$) lower bound on
the number of wires of any constant-depth circuit with arbitrary gates
computing a random $\tilde{O}(n)\times n$ linear operator $x \mapsto Ax$, a
long-standing open problem in circuit complexity. This suggests that the NOF
conjecture is much stronger than its data structure counterpart.
</p></div>
    </summary>
    <updated>2019-10-31T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13533</id>
    <link href="http://arxiv.org/abs/1910.13533" rel="alternate" type="text/html"/>
    <title>Achieving Optimal Backlog in the Vanilla Multi-Processor Cup Game</title>
    <feedworld_mtime>1572480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuszmaul:William.html">William Kuszmaul</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13533">PDF</a><br/><b>Abstract: </b>In each step of the $p$-processor cup game on $n$ cups, a filler distributes
up to $p$ units of water among the cups, subject only to the constraint that no
cup receives more than $1$ unit of water; an emptier then removes up to $1$
unit of water from each of $p$ cups. Designing strategies for the emptier that
minimize backlog (i.e., the height of the fullest cup) is important for
applications in processor scheduling, buffer management in networks, quality of
service guarantees, and deamortization. We prove that the greedy algorithm
(i.e., the empty-from-fullest-cups algorithm) achieves backlog $O(\log n)$ for
any $p \ge 1$. This resolves a long-standing open problem for $p &gt; 1$, and is
asymptotically optimal as long as $n \ge 2p$. If the filler is an oblivious
adversary, then we prove that there is a randomized emptying algorithm that
achieve backlog $O(\log p + \log \log n)$ with probability $1 -
2^{-\operatorname{polylog}(n)}$ for $2^{\operatorname{polylog}(n)}$ steps. This
is known to be asymptotically optimal when $n$ is sufficiently large relative
to $p$. The analysis of the randomized algorithm can also be reinterpreted as a
smoothed analysis of the deterministic greedy algorithm. Previously, the only
known bound on backlog for $p &gt; 1$, and the only known randomized guarantees
for any $p$ (including when $p = 1$), required the use of resource
augmentation, meaning that the filler can only distribute at most $p(1 -
\epsilon)$ units of water in each step, and that the emptier is then permitted
to remove $1 + \delta$ units of water from each of $p$ cups, for some
$\epsilon, \delta &gt; 0$.
</p></div>
    </summary>
    <updated>2019-10-31T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13530</id>
    <link href="http://arxiv.org/abs/1910.13530" rel="alternate" type="text/html"/>
    <title>Quantum Weighted Model Counting</title>
    <feedworld_mtime>1572480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Riguzzi:Fabrizio.html">Fabrizio Riguzzi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13530">PDF</a><br/><b>Abstract: </b>In Weighted Model Counting (WMC) we assign weights to Boolean literals and we
want to compute the sum of the weights of the models of a Boolean function
where the weight of a model is the product of the weights of its literals. WMC
was shown to be particularly effective for performing inference in graphical
models, with a complexity of $O(n2^w)$ where $n$ is the number of variables and
$w$ is the treewidth. In this paper, we propose a quantum algorithm for
performing WMC, Quantum WMC (QWMC), that modifies the quantum model counting
algorithm to take into account the weights. In turn, the model counting
algorithm uses the algorithms of quantum search, phase estimation and Fourier
transform. In the black box model of computation, where we can only query an
oracle for evaluating the Boolean function given an assignment, QWMC solves the
problem approximately with a complexity of $\Theta(2^{\frac{n}{2}})$ oracle
calls while classically the best complexity is $\Theta(2^n)$, thus achieving a
quadratic speedup.
</p></div>
    </summary>
    <updated>2019-10-31T01:20:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-10-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13479</id>
    <link href="http://arxiv.org/abs/1910.13479" rel="alternate" type="text/html"/>
    <title>Practical Repetition-Aware Grammar Compression</title>
    <feedworld_mtime>1572480000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Furuya:Isamu.html">Isamu Furuya</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13479">PDF</a><br/><b>Abstract: </b>The goal of grammar compression is to construct a small sized context free
grammar which uniquely generates the input text data. Among grammar compression
methods, RePair is known for its good practical compression performance.
MR-RePair was recently proposed as an improvement to RePair for constructing
small-sized context free grammar for repetitive text data. However, a compact
encoding scheme has not been discussed for MR-RePair. We propose a practical
encoding method for MR-RePair and show its effectiveness through comparative
experiments. Moreover, we extend MR-RePair to run-length context free grammar
and design a novel variant for it called RL-MR-RePair. We experimentally
demonstrate that a compression scheme consisting of RL-MR-RePair and the
proposed encoding method show good performance on real repetitive datasets.
</p></div>
    </summary>
    <updated>2019-10-31T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-31T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-25562705.post-4967614199435754464</id>
    <link href="http://aaronsadventures.blogspot.com/feeds/4967614199435754464/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=25562705&amp;postID=4967614199435754464" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/25562705/posts/default/4967614199435754464" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/25562705/posts/default/4967614199435754464" rel="self" type="application/atom+xml"/>
    <link href="http://aaronsadventures.blogspot.com/2019/10/forc-new-conference-you-should-know.html" rel="alternate" type="text/html"/>
    <title>FORC: A new conference you should know about.</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Here is the CFP: <a href="https://responsiblecomputing.org/forc-2020-call-for-paper/">https://responsiblecomputing.org/forc-2020-call-for-paper/</a><br/><br/><h1 class="headline">FORC 2020: CALL FOR PAPERS</h1><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; padding: 12px 0px; text-align: center;"><b>Symposium on Foundations of Responsible Computing</b></div><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; padding: 12px 0px;">The Symposium on Foundations of Responsible Computing (FORC) is a forum for mathematical research in computation and society writ large.  The Symposium aims to catalyze the formation of a community supportive of the application of theoretical computer science, statistics, economics and other relevant analytical fields to problems of pressing and anticipated societal concern. </div><hr style="background-color: #dddddd; border: 0px; color: #dddddd; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; height: 1px; margin: 6px 0px 8px; padding: 0px; width: 1024px;"/><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; padding: 12px 0px;"><b>Important Dates</b></div><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; padding: 12px 0px;">February 11: Submission Deadline<br/>March 23: Notification to Authors<br/>April 1: Camera Ready Deadline<br/>June 1-3: The conference</div><hr style="background-color: #dddddd; border: 0px; color: #dddddd; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; height: 1px; margin: 6px 0px 8px; padding: 0px; width: 1024px;"/><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; padding: 12px 0px;">Any mathematical work on computation and society is welcomed, including topics that are not yet well-established and topics that will arise in the future. This includes the investigation of definitions, algorithms and lower bounds, trade-offs, and economic incentives in a variety of areas. A small sample of topics follow: formal approaches to privacy, including differential privacy; fairness and discrimination in machine learning; bias in the formation of, and diffusion in, social networks; electoral processes and allocation of elected representatives (including redistricting). </div><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; padding: 12px 0px;">The inaugural FORC will be held on June 1-3 at the Harvard Center for Mathematical Sciences and Applications (CMSA), and will have its proceedings published by LIPIcs. The program committee will review submissions to ensure a high quality program based on novel, rigorous and significant scientific contributions. Authors of accepted papers will have the option of publishing a 10-page version of their paper in the proceedings, or publishing only a 1-page extended abstract, to facilitate the publication of their work in another venue. 1-page abstracts will appear on the website, but not in the proceedings. The symposium itself will feature a mixture of talks by authors of accepted papers and invited talks.</div><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; padding: 12px 0px;">Authors should upload a PDF of the paper through Easychair: <a href="https://easychair.org/conferences/?conf=forc2020" style="color: #99cc33;">https://easychair.org/conferences/?conf=forc2020</a>. The font size should be at least 11 point and the paper should be formatted in a single column. Beyond these, there are no formatting or length requirements, but reviewers will only be asked to read the first 10 pages of the paper. It is the authors’ responsibility that the main results of the paper and their significance be clearly stated within the first 10 pages. Submissions should include proofs of all central claims, and the committee will put a premium on writing that conveys clearly and in the simplest possible way what the paper is accomplishing.  Authors are free to post their paper on arXiv, etc. Future details will appear on the conference website: <a href="https://responsiblecomputing.org/" style="color: #99cc33;">https://responsiblecomputing.org/</a>.</div><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; padding: 12px 0px;"><b>Steering Committee</b></div><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; padding: 12px 0px;">Avrim Blum<br/>Cynthia Dwork<br/>Sampath Kannan<br/>Jon Kleinberg<br/>Shafi Goldwasser<br/>Kobbi Nissim<br/>Toni Pitassi<br/>Omer Reingold<br/>Guy Rothblum<br/>Salvatore Ruggieri<br/>Salil Vadhan<br/>Adrian Weller</div><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; padding: 12px 0px;"><b>Program Committee</b></div><div style="color: #666666; font-family: Roboto, Arial, Helvetica, sans-serif; font-size: 16px; padding: 12px 0px;">Yiling Chen, Harvard<br/>Rachel Cummings, Georgia Tech<br/>Anupam Datta, Carnegie Mellon University<br/>Moritz Hardt, UC Berkeley<br/>Nicole Immorlica, Microsoft Research<br/>Michael Kearns, University of Pennsylvania<br/>Katrina Ligett, Hebrew University<br/>Audra McMillan, Boston University and Northeastern<br/>Aaron Roth, University of Pennsylvania (<span style="font-weight: 700;">Chair</span>)<br/>Guy Rothblum, Weizmann Institute<br/>Adam Smith, Boston University<br/>Steven Wu, University of Minnesota<br/>Jonathan Ullman, Northeastern<br/>Jenn Wortman Vaughan, Microsoft Research<br/>Suresh Venkatasubramanian, University of Utah<br/>Nisheeth Vishnoi, Yale<br/>James Zou, Stanford</div></div>
    </content>
    <updated>2019-10-30T20:33:00Z</updated>
    <published>2019-10-30T20:33:00Z</published>
    <author>
      <name>Aaron</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/09952936358739421126</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-25562705</id>
      <category term="game theory"/>
      <category term="news"/>
      <author>
        <name>Aaron</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/09952936358739421126</uri>
      </author>
      <link href="http://aaronsadventures.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/25562705/posts/default" rel="self" type="application/atom+xml"/>
      <link href="http://aaronsadventures.blogspot.com/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/25562705/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <title>Adventures in Computation</title>
      <updated>2019-10-30T20:35:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1520</id>
    <link href="https://theorydish.blog/2019/10/30/toc-for-society/" rel="alternate" type="text/html"/>
    <title>TOC for Society</title>
    <summary>I am delighted that the Symposium on Foundations of Responsible Computing (FORC) is on its way. This is a new forum that will host mathematical research in computation and society, under an inclusive umbrella. A major purpose is to give a home and to nurture the growing community within the Theory of Computing and neighboring fields whose research is focused on the societal impact of computing. This vision is shared by many in this area, and I am very proud of the remarkable list of steering committee members that have volunteered to create and promote the new conference. The call for papers for FORC 2020 is out. The PC chair, Aaron Roth, has done a remarkable job forming a strong program committee, and we are off to a great start. Please consider sending your research papers and looking forward to seeing many of you at FORC 2020 in the beginning of June at Harvard.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I am delighted that <a href="https://responsiblecomputing.org/">the Symposium on Foundations of Responsible Computing (FORC)</a> is on its way. This is a new forum that will host mathematical research in computation and society, under an inclusive umbrella. A major purpose is to give a home and to nurture the growing community within the Theory of Computing and neighboring fields whose research is focused on the societal impact of computing. This vision is shared by many in this area, and I am very proud of the remarkable list of steering committee members that have volunteered to create and promote the new conference.</p>
<p>The <a href="https://responsiblecomputing.org/forc-2020-call-for-paper/">call for papers for FORC 2020</a> is out. The PC chair, <a href="https://www.cis.upenn.edu/~aaroth/">Aaron Roth</a>, has done a remarkable job forming a strong program committee, and we are off to a great start. Please consider sending your research papers and looking forward to seeing many of you at FORC 2020 in the beginning of June at Harvard.</p></div>
    </content>
    <updated>2019-10-30T15:54:11Z</updated>
    <published>2019-10-30T15:54:11Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Omer Reingold</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2019-10-31T14:21:28Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4400</id>
    <link href="https://www.scottaaronson.com/blog/?p=4400" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4400#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4400" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">My New York Times op-ed on quantum supremacy</title>
    <summary xml:lang="en-US">Here it is. I’d like to offer special thanks to the editor in charge, Eleanor Barkhorn, who commissioned this piece and then went way, way beyond the call of duty to get it right—including relaxing the usual length limit to let me squeeze in amplitudes and interference, and working late into the night to fix […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://www.nytimes.com/2019/10/30/opinion/google-quantum-computer-sycamore.html">Here it is</a>.</p>



<p>I’d like to offer special thanks to the editor in charge, <a href="https://twitter.com/eleanorbarkhorn?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor">Eleanor Barkhorn</a>, who commissioned this piece and then went way, <strong>way</strong> beyond the call of duty to get it right—including relaxing the usual length limit to let me squeeze in amplitudes and interference, and working late into the night to fix last-minute problems.  Obviously I take sole responsibility for whatever errors remain.</p>



<p>Of course a lot of material still ended up on the cutting room floor, including a little riff about Andrew Yang’s tweet that because of quantum supremacy, now “no code is uncrackable,” as well as Ivanka Trump’s tweet giving credit for Google’s experiment (one that Google was working toward since 2015) partly to her father’s administration.</p>



<p>While I’m posting: those of a more technical bent might want to check out my <a href="https://arxiv.org/abs/1910.12085">new short preprint</a> with UT undergraduate Sam Gunn, where we directly study the complexity-theoretic hardness of spoofing Google’s linear cross-entropy benchmark using a classical computer.  Enjoy!</p></div>
    </content>
    <updated>2019-10-30T11:23:30Z</updated>
    <published>2019-10-30T11:23:30Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Bell's Theorem? But a Flesh Wound!"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2019-10-30T12:29:13Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=18400</id>
    <link href="https://gilkalai.wordpress.com/2019/10/30/amazing-keith-frankston-jeff-kahn-bhargav-narayanan-jinyoung-park-thresholds-versus-fractional-expectation-thresholds/" rel="alternate" type="text/html"/>
    <title>Amazing! Keith Frankston, Jeff Kahn, Bhargav Narayanan, Jinyoung Park: Thresholds versus fractional expectation-thresholds</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This post describes a totally unexpected breakthrough about expectation and thresholds. The result  by Frankston, Kahn, Narayanan, and Park has many startling applications and it builds on the recent breakthrough work of Alweiss, Lovett, Wu and Zhang on the sunflower … <a href="https://gilkalai.wordpress.com/2019/10/30/amazing-keith-frankston-jeff-kahn-bhargav-narayanan-jinyoung-park-thresholds-versus-fractional-expectation-thresholds/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This post describes a <a href="https://arxiv.org/abs/1910.13433">totally unexpected breakthrough</a> about expectation and thresholds. The result  by Frankston, Kahn, Narayanan, and Park has many startling applications and it builds on the <a href="https://gilkalai.wordpress.com/2019/08/23/amazing-ryan-alweiss-shachar-lovett-kewen-wu-jiapeng-zhang-made-dramatic-progress-on-the-sunflower-conjecture/">recent breakthrough work of Alweiss, Lovett, Wu and Zhang on the sunflower conjecture</a>. Warm congratulations to Keith, Jeff, Bhargav, and Jinyoung!</p>
<p><span style="color: #0000ff;">Let me start with an update on the matter of applying the intermediate value theorem for football (or soccer as referred to in the US). You can read about it <a href="https://gilkalai.wordpress.com/2009/04/20/the-intermediate-value-theorem-applied-to-football/">in this 2009 post</a>. As you may recall, Shmuel Weinberger’s <a href="https://gilkalai.wordpress.com/2009/04/20/the-intermediate-value-theorem-applied-to-football/#comment-1010">raised the concern</a> of instability of fixed points. (A partial solution of Gowers apply the original idea for three foreheads.) Sylvia Serfaty mentioned to me a possible one-player implementation based on <a href="https://en.wikipedia.org/wiki/Inverted_pendulum">inverted pendulum</a> control (See<a href="https://www.youtube.com/watch?v=5oGYCxkgnHQ"> this video</a>, and <a href="https://youtu.be/D3bblng-Kcc?t=1912">this one</a>, and <a href="https://www.youtube.com/watch?v=gnn21smGVrQ">this one</a>, and <a href="https://youtu.be/OCXrXUhJCTI?t=5118">this one</a>, and for the fascinating mathematics  <a href="https://youtu.be/swFwHWMTA4k">this lecture</a> by Jean-Michel Coron. <strong>Please, don’t try it at home.</strong>) <span style="color: #ff0000;">Implement this method in football is a notable remaining challenge <span style="color: #0000ff;">(this you can try at home)</span></span>. On another matter, for readers interested in the Google’s quantum supremacy news, here is a link of my <a href="https://gilkalai.wordpress.com/2019/09/23/quantum-computers-amazing-progress-google-ibm-and-extraordinary-but-probably-false-supremacy-claims-google/">main post on the matter</a>.</span></p>
<h2>Thresholds versus fractional expectation-thresholds</h2>
<p class="title mathjax">This morning the following paper appeared on the arXive: <a href="https://arxiv.org/abs/1910.13433">Thresholds versus fractional expectation-thresholds</a> by Keith Frankston, Jeff Kahn, Bhargav Narayanan, and Jinyoung Park.</p>
<p><strong>Abstract:</strong> Proving a conjecture of Talagrand, a fractional version of the ‘expectation-threshold’ conjecture of Kalai and the second author, we show for any increasing family <strong>F</strong> on a finite set <strong>X</strong> that <img alt="p_c(F)=O(q_f(F) \log \ell ({\bf F}))" class="latex" src="https://s0.wp.com/latex.php?latex=p_c%28F%29%3DO%28q_f%28F%29+%5Clog+%5Cell+%28%7B%5Cbf+F%7D%29%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_c(F)=O(q_f(F) \log \ell ({\bf F}))"/>, where <img alt="p_c({\bf F})" class="latex" src="https://s0.wp.com/latex.php?latex=p_c%28%7B%5Cbf+F%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="p_c({\bf F})"/> and <img alt="q_f({\bf F})" class="latex" src="https://s0.wp.com/latex.php?latex=q_f%28%7B%5Cbf+F%7D%29&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="q_f({\bf F})"/> are the threshold and ‘fractional expectation-threshold’ of <strong>F</strong>, and ℓ(<strong>F</strong>) is the largest size of a minimal member of <strong>F</strong>. This easily implies various heretofore difficult results in probabilistic combinatorics, e.g. thresholds for perfect hypergraph matchings (Johansson-Kahn-Vu) and bounded-degree spanning trees (Montgomery). We also resolve (and vastly extend) one version of the ‘random multi-dimensional assignment’ problem of Frieze and Sorkin. Our approach builds on recent breakthrough work of Alweiss, Lovett, Wu and Zhang on the Erdős-Rado ‘sunflower’ conjecture.</p>
<h2>The expectation-threshold conjecture</h2>
<p><a href="https://gilkalai.files.wordpress.com/2019/10/kahn-kalai.png"><img alt="" class="alignnone size-full wp-image-18406" height="476" src="https://gilkalai.files.wordpress.com/2019/10/kahn-kalai.png?w=640&amp;h=476" width="640"/></a></p>
<p>The 2006 <a href="https://arxiv.org/abs/math/0603218">expectation threshold conjecture</a> gives a justification for a naive way to estimate the threshold probability of a random graph property. Suppose that you are asked about the critical probability for a random graph in G(n,p) for having a perfect matching (or a Hamiltonian cycle). You compute the expected number of perfect matchings and realize that when p is C/n this expected number equals 1/2. (For Hamiltonian cycles it will be C’/n.) Of course, if the expectation is one half the probability for a perfect matching can be very low, indeed in this case, an isolated vertex is quite likely but when there is no isolated vertices the expected number of perfect matchings is rather large. Our 2006 conjecture boldly asserts that the gap between the value given by such a naive computation and the true threshold value is at most logarithmic in the number of vertices. Jeff and I tried hard to find a counterexample but instead we managed to find more general and stronger forms of the conjecture that we could not disprove.</p>
<h2>Conjectures by Talagrand</h2>
<p>The expectation threshold conjecture had some connections with a 1995 paper of Michel Talagrand entitled <a href="https://link.springer.com/chapter/10.1007%2F978-3-0348-9090-8_25">Are all sets of positive measure essentially convex?</a> In a 2010 STOC paper <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.6973&amp;rep=rep1&amp;type=pdf">Are Many Small Sets Explicitly Small?</a> Michel formulated a weaker fractional version of the expectation threshold conjecture which is sufficient for the various applications of the original conjecture. This conjecture (as well as a stronger form also posed by Talagrand) is now verified in the new paper!</p>
<h2>Connection to isoperimetry</h2>
<p>In our 2006 paper we tried to relate the expectation threshold conjecture to various questions of independent interest related to stability theorems for discrete isoperimetric inequalities. This direction did not play a role in the new paper. Let me note that the isoperimetric problems served as partial motivation for the recent breakthrough results by Peter Keevash, Noam Lifshitz, Eoin Long, and Dor Minzer that are reported in this <a href="https://gilkalai.wordpress.com/2018/10/30/exciting-beginning-of-the-year-activities-and-seminars/">October 2018 post</a>. See their paper <a href="https://arxiv.org/abs/1906.05568">Hypercontractivity for global functions and sharp thresholds</a>.</p>
<h2>A sample from the applications</h2>
<ol>
<li>The threshold value for perfect matching – this was proved already by Erdos and Renyi (1960)  and it follow from the new results. The same goes for the threshold for connectivity.</li>
<li>The threshold value for Hamiltonian circuits – posed as a problem by  Erdos and Renyi it was solved by Korshunov (1976) and by Posa (1976).</li>
<li>The threshold for perfect matching in 3-uniform hypergraphs – was posed by Schmidt and Shamir (1983) and was settled by  Johansson, Kahn, and Vu. (It was one of the motivation for my 2006 paper with Jeff.)</li>
<li>The threshold for bounded degree spanning trees that was open for a long time and was settled by Montgomery (2019).</li>
</ol>
<p>Let me mention that in various cases the gap between the (fractional) expectation threshold and threshold is a smaller power of log <em>n, </em>or is a constant, or has different behavior. Understanding this through a general theory is still unknown.</p>
<h2><strong>Connection to the sunflower breakthrough</strong></h2>
<p>What did play a major role in the new development was the recent <a href="https://arxiv.org/abs/1908.08483">breakthrough work</a> of Alweiss, Lovett, Wu and Zhang on the Erdős-Rado ‘sunflower’ conjecture. (See <a href="https://gilkalai.wordpress.com/2019/08/23/amazing-ryan-alweiss-shachar-lovett-kewen-wu-jiapeng-zhang-made-dramatic-progress-on-the-sunflower-conjecture/">this post</a>.)  I expected that the method of the sunflower paper will have major applications but this application took me by a surprise.</p>
<p> </p></div>
    </content>
    <updated>2019-10-30T08:53:54Z</updated>
    <published>2019-10-30T08:53:54Z</published>
    <category term="Combinatorics"/>
    <category term="Probability"/>
    <category term="Bhargav Narayanan"/>
    <category term="Jeff Kahn"/>
    <category term="Jinyoung Park"/>
    <category term="Keith Frankston"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-10-31T14:20:44Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13386</id>
    <link href="http://arxiv.org/abs/1910.13386" rel="alternate" type="text/html"/>
    <title>NC Algorithms for Popular Matchings in One-Sided Preference Systems and Related Problems</title>
    <feedworld_mtime>1572393600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hu:Changyong.html">Changyong Hu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garg:Vijay_K=.html">Vijay K. Garg</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13386">PDF</a><br/><b>Abstract: </b>The popular matching problem is of matching a set of applicants to a set of
posts, where each applicant has a preference list, ranking a non-empty subset
of posts in the order of preference, possibly with ties. A matching M is
popular if there is no other matching M' such that more applicants prefer M' to
M. We give the first NC algorithm to solve the popular matching problem without
ties. We also give an NC algorithm that solves the maximum-cardinality popular
matching problem. No NC or RNC algorithms were known for the matching problem
in preference systems prior to this work. Moreover, we give an NC algorithm for
a weaker version of the stable matching problem, that is, the problem of
finding the "next" stable matching given a stable matching.
</p></div>
    </summary>
    <updated>2019-10-30T23:21:08Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13367</id>
    <link href="http://arxiv.org/abs/1910.13367" rel="alternate" type="text/html"/>
    <title>Derivation and Analysis of Fast Bilinear Algorithms for Convolution</title>
    <feedworld_mtime>1572393600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Caleb Ju, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Solomonik:Edgar.html">Edgar Solomonik</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13367">PDF</a><br/><b>Abstract: </b>The prevalence of convolution in applications within signal processing, deep
neural networks, and numerical solvers has motivated the development of
numerous fast convolution algorithms. In many of these problems, convolution is
performed on terabytes or petabytes of data, so even constant factors of
improvement can significantly reduce the computation time. We leverage the
formalism of bilinear algorithms to describe and analyze all of the most
popular approaches. This unified lens permits us to study the relationship
between different variants of convolution as well as to derive error bounds and
analyze the cost of the various algorithms. We provide new derivations, which
predominantly leverage matrix and tensor algebra, to describe the Winograd
family of convolution algorithms as well as reductions between 1D and
multidimensional convolution. We provide cost and error bounds as well as
experimental numerical studies. Our experiments for two of these algorithms,
the overlap-add approach and Winograd convolution algorithm with polynomials of
degree greater than one, show that fast convolution algorithms can rival the
accuracy of the fast Fourier transform (FFT) without using complex arithmetic.
These algorithms can be used for convolution problems with multidimensional
inputs or for filters larger than size of four, extending the state-of-the-art
in Winograd-based convolution algorithms.
</p></div>
    </summary>
    <updated>2019-10-30T23:20:28Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13352</id>
    <link href="http://arxiv.org/abs/1910.13352" rel="alternate" type="text/html"/>
    <title>Equipartitions with Wedges and Cones</title>
    <feedworld_mtime>1572393600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schnider:Patrick.html">Patrick Schnider</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13352">PDF</a><br/><b>Abstract: </b>A famous result about mass partitions is the so called \emph{Ham-Sandwich
theorem}. It states that any $d$ mass distributions in $\mathbb{R}^d$ can be
simultaneously bisected by a single hyperplane. In this work, we study two
related questions.
</p>
<p>The first one is, whether we can bisect more than $d$ masses, if we allow for
bisections with more general objects such as cones, wedges or double wedges. We
answer this question in the affirmative by showing that with all of these
objects, we can simultaneously bisect $d+1$ masses. For double wedges, we prove
a stronger statement, namely that $d$ families of $d+1$ masses each can each by
simultaneously bisected by some double wedge such that all double wedges have
one hyperplane in common.
</p>
<p>The second question is, how many masses we can simultaneously equipartition
with a $k$-fan, that is, $k$ half-hyperplanes in $\mathbb{R}^d$, emanating from
a common $(d-2)$-dimensional apex. This question was already studied in the
plane, our contribution is to extend the planar results to higher dimensions.
</p>
<p>All of our results are proved using topological methods. We use some
well-established techniques, but also some newer methods. In particular, we
introduce a Borsuk-Ulam theorem for flag manifolds, which we believe to be of
independent interest.
</p></div>
    </summary>
    <updated>2019-10-30T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-10-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13297</id>
    <link href="http://arxiv.org/abs/1910.13297" rel="alternate" type="text/html"/>
    <title>Flexible Graph Connectivity: Approximating Network Design Problems Between 1- and 2-connectivity</title>
    <feedworld_mtime>1572393600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Adjiashvili:David.html">David Adjiashvili</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hommelsheim:Felix.html">Felix Hommelsheim</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/M=uuml=hlenthaler:Moritz.html">Moritz Mühlenthaler</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13297">PDF</a><br/><b>Abstract: </b>Graph connectivity and network design problems are among the most fundamental
problems in combinatorial optimization. The minimum spanning tree problem, the
two edge-connected spanning subgraph problem (2-ECSS) and the tree augmentation
problem (TAP) are all examples of fundamental well-studied network design tasks
that postulate different initial states of the network and different
assumptions on the reliability of network components. In this paper we motivate
and study \emph{Flexible Graph Connectivity} (FGC), a problem that mixes
together both the modeling power and the complexities of all aforementioned
problems and more. In a nutshell, FGC asks to design a connected network, while
allowing to specify different reliability levels for individual edges. While
this non-uniform nature of the problem makes it appealing from the modeling
perspective, it also renders most existing algorithmic tools for dealing with
network design problems unfit for approximating FGC.
</p>
<p>In this paper we develop a general algorithmic approach for approximating FGC
that yields approximation algorithms with ratios that are very close to the
best known bounds for many special cases, such as 2-ECSS and TAP. Our algorithm
and analysis combine various techniques including a weight-scaling algorithm, a
charging argument that uses a variant of exchange bijections between spanning
trees and a factor revealing non-linear optimization problem.
</p></div>
    </summary>
    <updated>2019-10-30T23:20:53Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13292</id>
    <link href="http://arxiv.org/abs/1910.13292" rel="alternate" type="text/html"/>
    <title>Real-time Bidding campaigns optimization using attribute selection</title>
    <feedworld_mtime>1572393600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Miralles:Luis.html">Luis Miralles</a>, M. Atif Qureshi, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Namee:Brian_Mac.html">Brian Mac Namee</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13292">PDF</a><br/><b>Abstract: </b>Real-Time Bidding is nowadays one of the most promising systems in the online
advertising ecosystem. In the presented study, the performance of RTB campaigns
is improved by optimising the parameters of the users' profiles and the
publishers' websites. Most studies about optimising RTB campaigns are focused
on the bidding strategy. In contrast, the objective of our research consists of
optimising RTB campaigns by finding out configurations that maximise both the
number of impressions and their average profitability. The experiments
demonstrate that, when the number of required visits by advertisers is low, it
is easy to find configurations with high average profitability, but as the
required number of visits increases, the average profitability tends to go
down. Additionally, configuration optimisation has been combined with other
interesting strategies to increase, even more, the campaigns' profitability.
Along with parameter configuration the study considers the following
complementary strategies to increase profitability: i) selecting multiple
configurations with a small number of visits instead of a unique configuration
with a large number, ii) discarding visits according to the thresholds of cost
and profitability, iii) analysing a reduced space of the dataset and
extrapolating the solution, and iv) increasing the search space by including
solutions below the required number of visits. The developed campaign
optimisation methodology could be offered by RTB platforms to advertisers to
make their campaigns more profitable.
</p></div>
    </summary>
    <updated>2019-10-30T23:22:45Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13123</id>
    <link href="http://arxiv.org/abs/1910.13123" rel="alternate" type="text/html"/>
    <title>Reconstruction of time-consistent species trees</title>
    <feedworld_mtime>1572393600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lafond:Manuel.html">Manuel Lafond</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hellmuth:Marc.html">Marc Hellmuth</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13123">PDF</a><br/><b>Abstract: </b>The history of gene families -- which are equivalent to event-labeled gene
trees -- can to some extent be reconstructed from empirically estimated
evolutionary event-relations containing pairs of orthologous, paralogous or
xenologous genes. The question then arises as whether inferred event-labeled
gene trees are "biologically feasible" which is the case if one can find a
species tree with which the gene tree can be reconciled in a time-consistent
way.
</p>
<p>In this contribution, we consider event-labeled gene trees that contain
speciation, duplication as well as horizontal gene transfer and we assume that
the species tree is unknown. We provide a cubic-time algorithm to decide
whether a "time-consistent" binary species for a given event-labeled gene tree
exists and, in the affirmative case, to construct the species tree within the
same time-complexity.
</p></div>
    </summary>
    <updated>2019-10-30T23:22:24Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1910.13011</id>
    <link href="http://arxiv.org/abs/1910.13011" rel="alternate" type="text/html"/>
    <title>A Survey on Subgraph Counting: Concepts, Algorithms and Applications to Network Motifs and Graphlets</title>
    <feedworld_mtime>1572393600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ribeiro:Pedro.html">Pedro Ribeiro</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Paredes:Pedro.html">Pedro Paredes</a>, Miguel E. P. Silva, David Aparicio, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Silva:Fernando.html">Fernando Silva</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1910.13011">PDF</a><br/><b>Abstract: </b>Computing subgraph frequencies is a fundamental task that lies at the core of
several network analysis methodologies, such as network motifs and
graphlet-based metrics, which have been widely used to categorize and compare
networks from multiple domains. Counting subgraphs is however computationally
very expensive and there has been a large body of work on efficient algorithms
and strategies to make subgraph counting feasible for larger subgraphs and
networks.
</p>
<p>This survey aims precisely to provide a comprehensive overview of the
existing methods for subgraph counting. Our main contribution is a general and
structured review of existing algorithms, classifying them on a set of key
characteristics, highlighting their main similarities and differences. We
identify and describe the main conceptual approaches, giving insight on their
advantages and limitations, and provide pointers to existing implementations.
We initially focus on exact sequential algorithms, but we also do a thorough
survey on approximate methodologies (with a trade-off between accuracy and
execution time) and parallel strategies (that need to deal with an unbalanced
search space).
</p></div>
    </summary>
    <updated>2019-10-30T23:22:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-10-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1903.02185</id>
    <link href="http://arxiv.org/abs/1903.02185" rel="alternate" type="text/html"/>
    <title>Stable Noncrossing Matchings</title>
    <feedworld_mtime>1572393600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Ruangwises:Suthee.html">Suthee Ruangwises</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/i/Itoh:Toshiya.html">Toshiya Itoh</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1903.02185">PDF</a><br/><b>Abstract: </b>Given a set of $n$ men represented by $n$ points lying on a line, and $n$
women represented by $n$ points lying on another parallel line, with each
person having a list that ranks some people of opposite gender as his/her
acceptable partners in strict order of preference. In this problem, we want to
match people of opposite genders to satisfy people's preferences as well as
making the edges not crossing one another geometrically. A noncrossing blocking
pair w.r.t. a matching $M$ is a pair $(m,w)$ of a man and a woman such that
they are not matched with each other but prefer each other to their own
partners in $M$, and the segment $(m,w)$ does not cross any edge in $M$. A
weakly stable noncrossing matching (WSNM) is a noncrossing matching that does
not admit any noncrossing blocking pair. In this paper, we prove the existence
of a WSNM in any instance by developing an $O(n^2)$ algorithm to find one in a
given instance.
</p></div>
    </summary>
    <updated>2019-10-30T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-10-30T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/144</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/144" rel="alternate" type="text/html"/>
    <title>TR19-144 |  An Adaptive Step Toward the Multiphase Conjecture | 

	Omri Weinstein, 

	Young Ko</title>
    <summary>In 2010, Patrascu proposed a dynamic set-disjointness problem, known as the Multiphase problem, as a candidate for proving $polynomial$ lower bounds on the operational time of dynamic data structures. Patrascu conjectured that any data structure for the Multiphase problem must make $n^\epsilon$ cell-probes in either the update or query phase, and showed that this would imply similar $unconditional$ lower bounds on many important dynamic data structure problems. Alas, there has been almost no progress on this conjecture in the past decade since its introduction. 

We show an $\tilde{\Omega}(\sqrt{n})$ cell-probe lower bound on the Multiphase problem for data structures with general (adaptive) updates, and queries with unbounded but "layered" adaptivity. This result captures all known set-intersection data structures  and significantly strengthens previous Multiphase lower bounds, which only captured non-adaptive data structures.
Our main technical result is a communication lower bound on a 4-party variant of Patrascu's Number-On-Forehead  Multiphase game, using information complexity techniques.  We also show that a lower bound on Patrascu's original NOF game would imply a polynomial ($n^{1+\epsilon}$) lower bound on the number of wires of any constant-depth circuit with $arbitrary$ gates computing a random $\tilde{O}(n)\times n$ $linear$ operator $x \mapsto Ax$, a long-standing open problem in circuit complexity. This suggests that the NOF conjecture is much stronger than its data structure counterpart.</summary>
    <updated>2019-10-29T22:20:01Z</updated>
    <published>2019-10-29T22:20:01Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-10-31T14:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/10/29/multiple-postdoctoral-fellowships-in-quantum-information-at-cu-boulder-center-for-theory-of-quantum-matter-apply-by-december-1-2019/</id>
    <link href="https://cstheory-jobs.org/2019/10/29/multiple-postdoctoral-fellowships-in-quantum-information-at-cu-boulder-center-for-theory-of-quantum-matter-apply-by-december-1-2019/" rel="alternate" type="text/html"/>
    <title>Multiple postdoctoral fellowships in quantum information at CU Boulder Center for Theory of Quantum Matter (apply by December 1, 2019)</title>
    <summary>Successful candidates will interact with CTQM faculty (O. DeWolfe, V. Gurarie, M. Hermele, M. Holland, E. Knill, A. Lucas, R. Nandkishore, E. Neil, L. Radzihovsky, A. M. Rey, P. Romatschke, G. Smith) and can also work with faculty throughout the Boulder Physics Department &amp; JILA. Applications esp. encouraged from candidates whose interests and/or expertise span […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Successful candidates will interact with CTQM faculty (O. DeWolfe, V. Gurarie, M. Hermele, M. Holland, E. Knill, A. Lucas, R. Nandkishore, E. Neil, L. Radzihovsky, A. M. Rey, P. Romatschke, G. Smith) and can also work with faculty throughout the Boulder Physics Department &amp; JILA. Applications esp. encouraged from candidates whose interests and/or expertise span traditionally distinct subfields.</p>
<p>Website: <a href="https://jobs.colorado.edu/jobs/JobDetail/?jobId=21755">https://jobs.colorado.edu/jobs/JobDetail/?jobId=21755</a><br/>
Email: Graeme.Smith@colorado.edu</p></div>
    </content>
    <updated>2019-10-29T19:30:15Z</updated>
    <published>2019-10-29T19:30:15Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-10-31T14:21:14Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://adamsheffer.wordpress.com/?p=5464</id>
    <link href="https://adamsheffer.wordpress.com/2019/10/29/incidences-open-problems-part-2/" rel="alternate" type="text/html"/>
    <title>Incidences: Open Problems (part 2)</title>
    <summary>We now continue our journey of seeing how we still don’t know much about geometric incidences. So far, we looked at two main problems concerning incidences with curves in the plane (see the first post of the series). It might make sense to move to study incidences in higher dimensions. Instead, we are now regressing […]</summary>
    <updated>2019-10-29T18:59:15Z</updated>
    <published>2019-10-29T18:59:15Z</published>
    <category term="Incidences"/>
    <author>
      <name>Adam Sheffer</name>
    </author>
    <source>
      <id>https://adamsheffer.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://adamsheffer.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://adamsheffer.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://adamsheffer.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://adamsheffer.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Discrete geometry and other typos</subtitle>
      <title>Some Plane Truths</title>
      <updated>2019-10-31T14:21:50Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3222567344389850699</id>
    <link href="https://blog.computationalcomplexity.org/feeds/3222567344389850699/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/09/random-non-partisan-thoughts-on-prez.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3222567344389850699" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3222567344389850699" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/09/random-non-partisan-thoughts-on-prez.html" rel="alternate" type="text/html"/>
    <title>Random non-partisan thoughts on the Prez Election</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><br/>
This post is non-partisan, but in the interest of full disclosure I disclose that I will almost surely be voting for the Democratic Nominee. And I say <i>almost surely</i> because very weird things could happen.I can imagine a republican saying, in 2015 <i>I will almost surely be voting for the Republican Nominee</i> and then later deciding to not vote for Trump. <br/>
<br/>
<br/>
<i>My Past Predictions</i>: Early on in 2007 I predicted it would be Obama vs McCain and that Obama would win. Was I smart or lucky? Early in 2011 I predicted Paul Ryan would be the Rep. Candidate. Early in 2015 and even into 2016 I predicted  that Trump would not get the nomination. After he got the nomination I predicted  he would not become president. So, in answer to my first question, I was lucky not smart. Having said all of this I predict that the Dem. candidate will be Warren. Note- this is an honest prediction, not one fueled by what I want to see happen. I predict Warren since she seems to be someone who can bridge the so-called establishment and the so-called left (I dislike the terms LEFT and RIGHT since issues and views change over time). Given my past record I would not take me too seriously. Also, since this prediction is not particularly unusual, if I am right this would NOT be impressive (My Obama prediction was impressive, and my Paul Ryan prediction would have been very impressive had I been right.)<br/>
<br/>
<i>Electability</i>: My spell checker doesn't think its a word. Actually it shouldn't be a word. It's a stupid concept. Recall<br/>
<br/>
JFK was unelectable since he was Catholic. <br/>
<br/>
Ronald Reagan was unelectable because he was too conservative.<br/>
<br/>
A draft dodging adulterer named Bill Clinton could not possible beat a sitting president who just won a popular war.<br/>
<br/>
Nobody named Barack Hussein Obama, who is half-black,  could possibly get the nomination, never mind the presidency. And Hillary had the nomination locked up in 2008--- she had no any serious challengers. <br/>
<br/>
(An article in <i>The New Republic</i> in 2007 predicted a brokered convention for the Republicans where Fred Thompson, Mitt Romney, and Rudy Guilliani would split the vote, and at the same time a cake walk for Hillary Clinton with<br/>
Barak Obama winning Illinois in the primaries but not much else. Recall that 2008 was McCain vs Obama.)<br/>
<br/>
Donald Trump will surely be stopped from getting the nomination because, in the end, <a href="https://www.amazon.com/Party-Decides-Presidential-Nominations-American/dp/0226112373/ref=cm_cr_arp_d_product_top?ie=UTF8">The Party Decides</a>.<br/>
<br/>
Republican voters in 2016  will prefer  Rubio to Trump since Marco is more electable AND more conservative. Hence, in the space of Rep. Candidates, Rubio dominates Trump. So, by simple game theory, Trump can't get the nomination.  The more electable Rubio, in the 2016 primaries, won Minnesota, Wash DC,  and Puerto Rico (Puerto Rico has a primary. Really!) One of my friends thought he also won Guam (Guam?) but I could not find evidence of that on the web. Okay, so why did Trump win? <i>Because voters are not game theorists.</i><br/>
<br/>
ANYWAY, my point is that how can anyone take the notion of electability seriously when unelectable people have gotten elected?<br/>
<br/>
<i>Primaries</i>: Dem primary  voters are torn between who they want to be president and who can beat Trump.  Since its so hard to tell who can beat who, I would recommend voting for who you like and not say stupid things like<br/>
<br/>
American would never elect  a 76 year old socialist whose recently had a heart attack.<br/>
<br/>
or<br/>
<br/>
Trump beat a women in 2016 so we can't nominate a women<br/>
<br/>
or<br/>
<br/>
America is not ready to elect a gay president yet. (America is never ready to do X until after it does X and then the pundits ret-con their opinions.For example, of course America is ready for Gay-Marriage. Duh.)<br/>
<br/>
<i>Who won the debate?<br/>
</i> Whoever didn't bother watching it :-). I think the question is stupid and has become who got out a clever sound bite. We need sound policy, not sound bites!</div>
    </content>
    <updated>2019-10-28T14:05:00Z</updated>
    <published>2019-10-28T14:05:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-10-31T10:02:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16320</id>
    <link href="https://rjlipton.wordpress.com/2019/10/27/quantum-supremacy-at-last/" rel="alternate" type="text/html"/>
    <title>Quantum Supremacy At Last?</title>
    <summary>What it takes to understand and verify the claim Cropped from 2014 Wired source John Martinis of U.C. Santa Barbara and Google is the last author of a paper published Wednesday in Nature that claims to have demonstrated a task executed with minimum effort by a quantum computer that no classical computer can emulate without […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>What it takes to understand and verify the claim</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2019/10/martinis1.png"><img alt="" class="alignright wp-image-16322" height="180" src="https://rjlipton.files.wordpress.com/2019/10/martinis1.png?w=153&amp;h=180" width="153"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from 2014 <i>Wired</i> <a href="https://www.wired.com/2014/09/martinis/">source</a></font></td>
</tr>
</tbody>
</table>
<p>
John Martinis of U.C. Santa Barbara and Google is the <em>last</em> author of a <a href="https://www.nature.com/articles/s41586-019-1666-5">paper</a> published Wednesday in <em>Nature</em> that claims to have demonstrated a task executed with minimum effort by a quantum computer that no classical computer can emulate without expending Herculean—or Sisyphean—effort. </p>
<p>
Today we present a lay understanding of the claim and discuss degrees of establishing it.</p>
<p>
There are 76 other authors of the paper. The first 75 are alphabetical, then comes Hartmut Neven before Martinis. Usually pride of place goes to the first author, but that depends on size. Martinis is also the corresponding author. The cox in a rowing race rides at the rear. We have discussed aspects of papers with a huge number of authors <a href="https://rjlipton.wordpress.com/2014/02/13/seeing-atoms/">here</a>. </p>
<p>
Three planks of a quantum supremacy claim are:</p>
<ol>
<li>
<em>Build a physical device capable of a nontrivial sampling task.</em> <p/>
</li><li>
<em>Prove that it gains advantage over known classical approaches.</em> <p/>
</li><li>
<em>Prove that comparable classical hardware cannot gain such advantage.</em>
</li></ol>
<p>
Scott Aaronson not only has made <a href="https://www.scottaaronson.com/blog/?p=4317">two</a> great <a href="https://www.scottaaronson.com/blog/?p=4372">posts</a> on these and many other aspects of the claim, he independently proposed in 2015 the sampling task that was programmed, and he analyzed it in a foundational <a href="https://arxiv.org/abs/1612.05903">paper</a> with Lijie Chen of MIT. Researchers at Google had already been thinking along those lines, and they anchored the team composed from numerous other institutions as well. As if on cue—just a couple days before Wednesday’s announcement—a group from IBM put out a <a href="https://www.ibm.com/blogs/research/2019/10/on-quantum-supremacy/">post</a> and <a href="https://arxiv.org/abs/1910.09534">paper</a> taking issue with the argument for the third plank.</p>
<p>
We’ll start with the task and go in order 1-3-2.</p>
<p>
</p><p/><h2> The Task </h2><p/>
<p/><p>
Any <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-qubit quantum circuit <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> and input <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> to <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> induces a probability distribution <img alt="{D_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_C}"/> on <img alt="{\{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{0,1\}^n}"/>. Because it will not matter if we prepend up to <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> NOT gates to <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>, we may suppose <img alt="{x = 0^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%3D+0%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x = 0^n}"/>. Then <img alt="{C(0^n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%280%5En%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C(0^n)}"/> is a unit complex vector of length <img alt="{N = 2^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN+%3D+2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N = 2^n}"/> with entries <img alt="{a_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a_z}"/> corresponding to possible outputs <img alt="{z \in \{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz+%5Cin+%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z \in \{0,1\}^n}"/>. Then the probability of getting <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> by a final measurement of all qubits is </p>
<p align="center"><img alt="\displaystyle  p_z = D_C(z) = |a_z|^2. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++p_z+%3D+D_C%28z%29+%3D+%7Ca_z%7C%5E2.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  p_z = D_C(z) = |a_z|^2. "/></p>
<p>
Next we consider probability distributions <img alt="{D_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_1}"/> that are generated uniformly at random by the following process, for some <img alt="{r \geq n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%5Cgeq+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r \geq n}"/> and taking <img alt="{R = 2^r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR+%3D+2%5Er%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R = 2^r}"/>:</p>
<blockquote><p><b> </b> <em> for <img alt="{i = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi+%3D+1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{i = 1}"/> to <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{R}"/>:<br/>
   choose a <img alt="{z \in \{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz+%5Cin+%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{z \in \{0,1\}^n}"/> uniformly at random;<br/>
   increment its probability <img alt="{D_1(z)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_1%28z%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{D_1(z)}"/> by <img alt="{\frac{1}{R}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7BR%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\frac{1}{R}}"/>. </em>
</p></blockquote>
<p/><p>
Here we intend <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r}"/> to be the number of binary nondeterministic gates in the circuit. In place of Hadamard gates the experimental circuits get their nondeterminism from these three single-qubit gates (ignoring global phase for <img alt="{\mathbf{Y}^{1/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BY%7D%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{Y}^{1/2}}"/> in particular): </p>
<p align="center"><img alt="\displaystyle  \mathbf{X}^{1/2} = \frac{1}{2}\begin{bmatrix} 1 + i &amp; 1 - i \\ 1 - i &amp; 1 + i \end{bmatrix},~ \mathbf{Y}^{1/2} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; -1 \\ 1 &amp; 1 \end{bmatrix},~ \mathbf{W}^{1/2} = \frac{1}{2}\begin{bmatrix} 1 + i &amp; - i\sqrt{2} \\ \sqrt{2} &amp; 1 + i \end{bmatrix}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathbf%7BX%7D%5E%7B1%2F2%7D+%3D+%5Cfrac%7B1%7D%7B2%7D%5Cbegin%7Bbmatrix%7D+1+%2B+i+%26+1+-+i+%5C%5C+1+-+i+%26+1+%2B+i+%5Cend%7Bbmatrix%7D%2C%7E+%5Cmathbf%7BY%7D%5E%7B1%2F2%7D+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%5Cbegin%7Bbmatrix%7D+1+%26+-1+%5C%5C+1+%26+1+%5Cend%7Bbmatrix%7D%2C%7E+%5Cmathbf%7BW%7D%5E%7B1%2F2%7D+%3D+%5Cfrac%7B1%7D%7B2%7D%5Cbegin%7Bbmatrix%7D+1+%2B+i+%26+-+i%5Csqrt%7B2%7D+%5C%5C+%5Csqrt%7B2%7D+%26+1+%2B+i+%5Cend%7Bbmatrix%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathbf{X}^{1/2} = \frac{1}{2}\begin{bmatrix} 1 + i &amp; 1 - i \\ 1 - i &amp; 1 + i \end{bmatrix},~ \mathbf{Y}^{1/2} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; -1 \\ 1 &amp; 1 \end{bmatrix},~ \mathbf{W}^{1/2} = \frac{1}{2}\begin{bmatrix} 1 + i &amp; - i\sqrt{2} \\ \sqrt{2} &amp; 1 + i \end{bmatrix}. "/></p>
<p>Here <img alt="{\mathbf{W} = \frac{1}{\sqrt{2}}(\mathbf{X} + \mathbf{Y})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BW%7D+%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%7D%7D%28%5Cmathbf%7BX%7D+%2B+%5Cmathbf%7BY%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{W} = \frac{1}{\sqrt{2}}(\mathbf{X} + \mathbf{Y})}"/> where <img alt="{\mathbf{Y} = \begin{bmatrix} 0 &amp; -i \\ i &amp; 0 \end{bmatrix}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BY%7D+%3D+%5Cbegin%7Bbmatrix%7D+0+%26+-i+%5C%5C+i+%26+0+%5Cend%7Bbmatrix%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{Y} = \begin{bmatrix} 0 &amp; -i \\ i &amp; 0 \end{bmatrix}}"/> and <img alt="{\mathbf{X}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BX%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{X}}"/> is another name for NOT. The difference from using Hadamard gates matters to technical analysis of the distributions <img alt="{D_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_C}"/> but the interplay between quantum nondeterministic gates and classical random coins remains in force. </p>
<p>
The choice of <img alt="{\mathbf{X}^{1/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BX%7D%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{X}^{1/2}}"/>, <img alt="{\mathbf{Y}^{1/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BY%7D%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{Y}^{1/2}}"/>, or <img alt="{\mathbf{W}^{1/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf%7BW%7D%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathbf{W}^{1/2}}"/> is itself uniformly random at each point where a single-qubit gate is used, except for not repeating the same gate on the same qubit, and those choices determine <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. Now we can give an initial statement of the task tailored to what the paper achieves:</p>
<blockquote><p><b> </b> <em> Given randomly-generated quantum circuits <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{C}"/> as inputs, distinguish <img alt="{D_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_C%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{D_C}"/> with high probability from any <img alt="{D_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_1%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{D_1}"/>. </em>
</p></blockquote>
<p/><p>
In more detail, the object is to take a number <img alt="{\delta &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta &gt; 0}"/> and moderately large integer <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>, both dictated by practical elements of the experiment, and fulfill this task statement:</p>
<blockquote><p><b> </b> <em> Given randomly-generated <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{C}"/>, generate samples <img alt="{z_1,...,z_k \in \{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz_1%2C...%2Cz_k+%5Cin+%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{z_1,...,z_k \in \{0,1\}^n}"/> such that <img alt="{\frac{1}{k}(D_C(z_1) + \cdots D_C(z_k)) \geq 1 + \delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7Bk%7D%28D_C%28z_1%29+%2B+%5Ccdots+D_C%28z_k%29%29+%5Cgeq+1+%2B+%5Cdelta%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\frac{1}{k}(D_C(z_1) + \cdots D_C(z_k)) \geq 1 + \delta}"/>. </em>
</p></blockquote>
<p/><p>
It’s important to note that there are two <em>stages</em> of randomness: one over <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> which chooses <img alt="{D_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_C}"/>, and then the stage of measuring after (perhaps imperfectly) executing <img alt="{C(0^n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%280%5En%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C(0^n)}"/>. The latter can be repeated to get a large sample of strings <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> for a given <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. The nature of the former stage matters most to justifying how to interpret tests of the samples and to closing loopholes. Our <img alt="{D_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_1}"/> does not signify having uniform distribution in the latter sampling, but rather covers classical alternatives in the former stage that (with overwhelming probability) belong to a class we call <img alt="{\mathcal{D}_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_1}"/>. The <img alt="{D_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_C}"/> for random <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> will (again w.o.p.) belong to a class <img alt="{\mathcal{D}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_2}"/> which we explain next.</p>
<p>
</p><p/><h2> The World Series of Quantum Computing </h2><p/>
<p/><p>
In honor of the baseball World Series, we offer a baseball analogy. To make differences sharper to see, we take <img alt="{r = n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r = n}"/>, so <img alt="{R = N = 2^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR+%3D+N+%3D+2%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R = N = 2^n}"/>. This is not what the experiment does: their biggest instance has 20 layers totaling <img alt="{r = 1,\!113}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+1%2C%5C%21113%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r = 1,\!113}"/> nondeterministic single-qubit gates (plus <img alt="{430}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B430%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{430}"/> two-qubit gates) on the <img alt="{n = 53}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+53%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n = 53}"/> qubits. But let us continue.</p>
<p>
We are distributing <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> units of probability among <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> “batters” <img alt="{z \in \{0,1\}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz+%5Cin+%5C%7B0%2C1%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z \in \{0,1\}^n}"/>. A batter who gets two units hits a double, three units makes a triple, and so on. The key distinction is between the familiar batting average and the <em>slugging average</em>, which averages all the bases scored with hits:</p>
<ul>
<li>
The chance of making an out—that is, getting no units—is <img alt="{(\frac{N-1}{N})^N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%5Cfrac%7BN-1%7D%7BN%7D%29%5EN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(\frac{N-1}{N})^N}"/> which is approximately <img alt="{\frac{1}{e} = 0.367879\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7Be%7D+%3D+0.367879%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{1}{e} = 0.367879\dots}"/> <p/>
</li><li>
The chance of hitting a single is also about <img alt="{\frac{1}{e}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B1%7D%7Be%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{1}{e}}"/>, leaving <img alt="{1 - \frac{2}{e}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+-+%5Cfrac%7B2%7D%7Be%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 - \frac{2}{e}}"/> as the frequency of getting an extra-base hit—which makes <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> a “heavy hitter.” <p/>
</li><li>
From <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> batters chosen uniformly at random, their expected batting average will be <img alt="{1 - \frac{1}{e} = 0.632\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+-+%5Cfrac%7B1%7D%7Be%7D+%3D+0.632%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 - \frac{1}{e} = 0.632\dots}"/>. <p/>
</li><li>
Their expected slugging average, however, will just be <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>: they expect <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> units to be distributed among them.
</li></ul>
<p>
Thus with respect to a random <img alt="{D_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_1}"/>, and without any knowledge of <img alt="{D_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_1}"/>, a chosen team of <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> hitters cannot expect to have a joint slugging average higher than <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. Moreover, for any fixed <img alt="{\delta &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta+%3E+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta &gt; 0}"/>, the chance of getting a slugging average higher than <img alt="{1 + \delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%2B+%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 + \delta}"/> tails away exponentially in <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/> (provided <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> also grows). </p>
<p>
With respect to <img alt="{D_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_C}"/>, however, a quantum device can do better. Google’s device programs itself given <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> as the blueprint. So it just executes <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> and measures all qubits to sample the output. Finding its own heavy hitters is what a quantum circuit is good at. The probability of getting a hitter who hits a triple is magnified by <img alt="{3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{3}"/> compared to a uniform choice. Moreover, <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> will never output a string with zero hits—a “can’t miss” property denied to a classical reader of <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. For large <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> the probability distribution approaches <img alt="{xe^{-x}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bxe%5E%7B-x%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{xe^{-x}}"/> and the slugging expectation is approximately </p>
<p align="center"><img alt="\displaystyle  \int_0^\infty x^2 e^{-x} = 2. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cint_0%5E%5Cinfty+x%5E2+e%5E%7B-x%7D+%3D+2.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \int_0^\infty x^2 e^{-x} = 2. "/></p>
<p>That is, a team <img alt="{z_1,\dots,z_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz_1%2C%5Cdots%2Cz_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z_1,\dots,z_k}"/> drafted by sampling from random quantum circuits <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> expects to have a slugging average near <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2}"/>. This defines the class <img alt="{\mathcal{D}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_2}"/>. If <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> works perfectly, the average will surpass <img alt="{1 + \delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%2B+%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 + \delta}"/> whenever <img alt="{0 &lt; \delta &lt; 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0+%3C+%5Cdelta+%3C+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0 &lt; \delta &lt; 1}"/> with near certainty as <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> grows. </p>
<p>
Google’s circuits have up to <img alt="{r = 20n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+20n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r = 20n}"/>, so <img alt="{R \gg N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR+%5Cgg+N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{R \gg N}"/>. Then the “can’t miss” aspect of the quantum advantage is less sharp but the <img alt="{xe^{-x}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bxe%5E%7B-x%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{xe^{-x}}"/> approximation is closer and the idea of <img alt="{\mathcal{D}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_2}"/> is the same. The nature of <img alt="{\mathcal{D}_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_2}"/> can actually be <em>seen</em> from point intensities in <a href="https://en.wikipedia.org/wiki/Speckle_pattern">speckle</a> patterns of laser light:</p>
<p>
<a href="https://rjlipton.files.wordpress.com/2019/10/375px-objective_speckle.jpg"><img alt="" class="aligncenter wp-image-16324" height="150" src="https://rjlipton.files.wordpress.com/2019/10/375px-objective_speckle.jpg?w=150&amp;h=150" width="150"/></a></p>
<p>
</p><p/><h2> Real-World Execution </h2><p/>
<p/><p>
The practical challenge is that the implementation of <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> is not perfect. The consequence of an error in the final output is severe. The heavy-hitter outputs <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> of a random <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> are generally not bit-wise similar, so sampling their neighbors is like sampling uniform distribution. As the paper says, “A single bit or phase flip over the course of the algorithm will completely shuffle the speckle pattern and result in close to zero fidelity.”</p>
<p>
Their circuits are sufficiently random that effects of sporadic errors over millions of samples can be modeled by a simple equation using quantum mixed states. We shortcut the paper’s physical analysis by drawing on John Preskill’s illustration of a <em>de-polarizing channel</em> in <a href="http://www.theory.caltech.edu/~preskill/ph219/chap3_15.pdf">chapter 3</a> of his wonderful online <a href="http://www.theory.caltech.edu/~preskill/ph219/ph219_2018-19">notes</a> on quantum computation to reach the same equation (<a href="https://rjlipton.wordpress.com/feed/#F">1</a>). The modeling has informative symmetry when the errors of a <b>bit flip</b>, <b>phase flip</b>, or both are considered equally likely with probability <img alt="{\frac{p}{3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7Bp%7D%7B3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{p}{3}}"/>. The action on the entangled pair <img alt="{|\Phi^+\rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%5CPhi%5E%2B%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|\Phi^+\rangle}"/> in the <a href="https://en.wikipedia.org/wiki/Bell_state#Bell_basis">Bell basis</a> is given by the density matrix evolution <img alt="{\rho = |\Phi^+\rangle\langle\Phi^+| \mapsto \rho'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crho+%3D+%7C%5CPhi%5E%2B%5Crangle%5Clangle%5CPhi%5E%2B%7C+%5Cmapsto+%5Crho%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\rho = |\Phi^+\rangle\langle\Phi^+| \mapsto \rho'}"/> where </p>
<p align="center"><img alt="\displaystyle  \begin{array}{rcl}  \rho' &amp;=&amp; (1 - p)|\Phi^+\rangle\langle\Phi^+| \;+\; \frac{p}{3}\left(|\Psi^+\rangle\langle\Psi^+| \;+\; |\Phi^-\rangle\langle\Phi^-| \;+\; |\Psi^-\rangle\langle\Psi^-|\right)\\ ~~~\\ &amp;=&amp; (1 - p') |\Phi^+\rangle\langle\Phi^+| \;+\; p'\frac{I}{4}, \end{array} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cbegin%7Barray%7D%7Brcl%7D++%5Crho%27+%26%3D%26+%281+-+p%29%7C%5CPhi%5E%2B%5Crangle%5Clangle%5CPhi%5E%2B%7C+%5C%3B%2B%5C%3B+%5Cfrac%7Bp%7D%7B3%7D%5Cleft%28%7C%5CPsi%5E%2B%5Crangle%5Clangle%5CPsi%5E%2B%7C+%5C%3B%2B%5C%3B+%7C%5CPhi%5E-%5Crangle%5Clangle%5CPhi%5E-%7C+%5C%3B%2B%5C%3B+%7C%5CPsi%5E-%5Crangle%5Clangle%5CPsi%5E-%7C%5Cright%29%5C%5C+%7E%7E%7E%5C%5C+%26%3D%26+%281+-+p%27%29+%7C%5CPhi%5E%2B%5Crangle%5Clangle%5CPhi%5E%2B%7C+%5C%3B%2B%5C%3B+p%27%5Cfrac%7BI%7D%7B4%7D%2C+%5Cend%7Barray%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \begin{array}{rcl}  \rho' &amp;=&amp; (1 - p)|\Phi^+\rangle\langle\Phi^+| \;+\; \frac{p}{3}\left(|\Psi^+\rangle\langle\Psi^+| \;+\; |\Phi^-\rangle\langle\Phi^-| \;+\; |\Psi^-\rangle\langle\Psi^-|\right)\\ ~~~\\ &amp;=&amp; (1 - p') |\Phi^+\rangle\langle\Phi^+| \;+\; p'\frac{I}{4}, \end{array} "/></p>
<p>where <img alt="{p' = \frac{4}{3} p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%27+%3D+%5Cfrac%7B4%7D%7B3%7D+p%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p' = \frac{4}{3} p}"/> and <img alt="{\frac{I}{4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7BI%7D%7B4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{I}{4}}"/> is the density matrix of the completely mixed two-qubit state which is just a classical distribution. This presumes <img alt="{p \leq \frac{3}{4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp+%5Cleq+%5Cfrac%7B3%7D%7B4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p \leq \frac{3}{4}}"/>; note that <img alt="{p = \frac{3}{4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp+%3D+%5Cfrac%7B3%7D%7B4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p = \frac{3}{4}}"/> completely mixes the Bell basis already. The <b>fidelity</b> of <img alt="{\vec{\rho}'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7B%5Crho%7D%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\vec{\rho}'}"/> to the original state is then given by </p>
<p align="center"><img alt="\displaystyle  F = \langle\Phi^+|\rho'|\Phi^+\rangle = 1 - p'. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F+%3D+%5Clangle%5CPhi%5E%2B%7C%5Crho%27%7C%5CPhi%5E%2B%5Crangle+%3D+1+-+p%27.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F = \langle\Phi^+|\rho'|\Phi^+\rangle = 1 - p'. "/></p>
<p>
This modeling already indicates that with <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> serial opportunities for error the fidelity will decay as <img alt="{(1 - p')^m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281+-+p%27%29%5Em%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(1 - p')^m}"/>.  The Google team found low ‘crosstalk’ between qubits and they used exactly this expression in the form <img alt="{(1 - \frac{e_1}{1 - 1/D^2})^m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%281+-+%5Cfrac%7Be_1%7D%7B1+-+1%2FD%5E2%7D%29%5Em%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(1 - \frac{e_1}{1 - 1/D^2})^m}"/>, evidently with <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p}"/> being the native gate error rate they call <img alt="{e_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e_1}"/> and <img alt="{D = 2^k,}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD+%3D+2%5Ek%2C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D = 2^k,}"/> where having <img alt="{k=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k=1}"/> for single-qubit gates supplies the factor <img alt="{\frac{2^{2k}}{2^{2k} - 1} = \frac{4}{3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B2%5E%7B2k%7D%7D%7B2%5E%7B2k%7D+-+1%7D+%3D+%5Cfrac%7B4%7D%7B3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac{2^{2k}}{2^{2k} - 1} = \frac{4}{3}}"/>.<br/>
The error <img alt="{e_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e_2}"/> for the two-qubit gates is similarly represented.  (The full modeling in the <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-019-1666-5/MediaObjects/41586_2019_1666_MOESM1_ESM.pdf">supplement</a>, section V, is more refined.)</p>
<p>
By observing their benchmarks (discussed below) for varying small <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> they could calculate the decay concretely and hence estimate values of <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> for the vast majority of runs with larger <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/>.  The random nature of the circuits <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> evidently makes covariance of errors that could systematically upset this modeling negligible.  Thus they can conclude that their device effectively samples from the distribution <a name="F"/></p><a name="F">
<p align="center"><img alt="\displaystyle  F|\langle z \;|\; C \;|\; 0^n\rangle|^2 \;\;+\;\; (1 - F)\frac{1}{N}. \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++F%7C%5Clangle+z+%5C%3B%7C%5C%3B+C+%5C%3B%7C%5C%3B+0%5En%5Crangle%7C%5E2+%5C%3B%5C%3B%2B%5C%3B%5C%3B+%281+-+F%29%5Cfrac%7B1%7D%7BN%7D.+%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  F|\langle z \;|\; C \;|\; 0^n\rangle|^2 \;\;+\;\; (1 - F)\frac{1}{N}. \ \ \ \ \ (1)"/></p>
</a><p><a name="F"/> Such distributions can be said to belong to the class <img alt="{\mathcal{D}_{1 + F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_%7B1+%2B+F%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_{1 + F}}"/>. The paper reports that their <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> is driven below <img alt="{0.01}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0.01%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0.01}"/> but stays above <img alt="{0.001}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0.001%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0.001}"/> in trials. This bounds the range of the <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta}"/> they can separate by. That <img alt="{\delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\delta}"/> is separated from zero achieves the first plank and starts on the second. The third needs attention first, however. </p>
<p>
</p><p/><h2> The Third Plank </h2><p/>
<p/><p>
Both <em>concrete</em> and <em>asymptotic</em> complexity evidence matter for the third plank, the former for now and the latter for how <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> and everything else may scale up in the future. In asymptotic complexity, we still don’t know that <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{P}}"/> and <img alt="{\mathsf{PSPACE}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BPSPACE%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{PSPACE}}"/>, which sandwich the quantum feasible class <img alt="{\mathsf{BQP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BBQP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{BQP}}"/>, are different. Thus asymptotic evidence about polynomial bounds must be conditional. Asymptotic evidence about linear time bounds can be sharper but then tends to be conditioned on forms of <a href="https://en.wikipedia.org/wiki/Exponential_time_hypothesis">SETH</a> in ways we still find <a href="https://rjlipton.wordpress.com/2015/06/01/puzzling-evidence/">puzzling</a>.</p>
<p>
Lower bounds in concrete complexity are less known and have a self-defeating aspect: We are trying to say that any program <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> run for less than an infeasible time <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> must fail. But we can’t run <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> for time <img alt="{T-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T-1}"/> to show that it fails because time <img alt="{T-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T-1}"/> is just as infeasible as time <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>. The best we can do is run <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> for a feasible <img alt="{T_0 \ll T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_0+%5Cll+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_0 \ll T}"/>, either (i) on a smaller task size, or (ii) on the original task but argue it doesn’t show <em>progress</em>. Neither is the same; we <a href="https://rjlipton.wordpress.com/2010/08/28/lower-bounds-and-progressive-algorithms/">made</a> some <a href="https://rjlipton.wordpress.com/2012/11/17/progress-on-progressive-algorithms/">attempts</a> on (ii). </p>
<p>
What the paper does instead is argue that a particular classical approach <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> (also from the Aaronson-Chen paper) would take 10,000 years on today’s hardware. This reminds us of a famous 1977 “Mathematical Games” <a href="https://simson.net/ref/1977/Gardner_RSA.pdf">column</a> by Martin Gardner, which quotes an estimate by Ron Rivest that for factoring a 126-digit number on then-current hardware, “the running time required would be about 40 quadrillion years!” It took only until <a href="https://en.wikipedia.org/wiki/The_Magic_Words_are_Squeamish_Ossifrage">1994</a> for this to be broken. Sure enough, IBM calculated that a more-clever implementation of <img alt="{P}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P}"/> on the <a href="https://en.wikipedia.org/wiki/Summit_(supercomputer)">Summit</a> supercomputer would take under 3 days. The point is not so much that the Summit hardware is comparable as that estimates based on what are currently thought to be the best possible (classical) methods need asterisks.</p>
<p>
On the asymptotic side, the last section (XI) of the paper’s 66-page <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-019-1666-5/MediaObjects/41586_2019_1666_MOESM1_ESM.pdf">supplement</a> proves a theorem toward showing that a classical simulation from <img alt="{\mathcal{D}_{1 + \delta}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_%7B1+%2B+%5Cdelta%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_{1 + \delta}}"/> that scales polynomially with <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> would collapse <img alt="{\mathsf{\#P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7B%5C%23P%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{\#P}}"/> to <img alt="{\mathsf{AM}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BAM%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{AM}}"/>, and similarly for sub-exponential running times. It does not get all the way there, however: improvements would need to be made in upper bounds for approximation and for worst-case to average-case equivalence. Moreover, there is a difference from what their statistical testing achieves that we try to explain next. </p>
<p>
</p><p/><h2> The Statistical Tests </h2><p/>
<p/><p>
We can cast the second plank in the general context of predictive modeling. Consider a forecaster who places estimates <img alt="{\{q_i\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7Bq_i%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{q_i\}}"/> on the true probabilities <img alt="{\{p_i\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7Bp_i%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{p_i\}}"/> of various events. In the quantum case, the <img alt="{p_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_i}"/> come from distributions in <img alt="{\mathcal{D}_{1+F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_%7B1%2BF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_{1+F}}"/>, where the <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> that applies to the latter sampling stage can be estimated based on the size and depth of <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. The <img alt="{q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i}"/> come from the physical quantum device—that is to say, from the strings <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> that it outputs. What’s needed is to compute the corresponding outcome probability <img alt="{q_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_z}"/> analytically based on the given circuit <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. This must be done <em>classically</em>, and incurs the “<img alt="{T_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_0}"/>-versus-<img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>” issue discussed above.  <b>[See Addendum below.]</b></p>
<p>
But before we get to that issue, let’s say more from the viewpoint of predictive modeling. We measure how well the forecasts <img alt="{q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i}"/> conform to the true <img alt="{p_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_i}"/> by applying a prediction scoring <a href="https://en.wikipedia.org/wiki/Scoring_rule">rule</a>. If outcome <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> happens, then the <em>log-likelihood rule</em> assesses a penalty of </p>
<p align="center"><img alt="\displaystyle  L_i = \log(\frac{1}{q_i}). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++L_i+%3D+%5Clog%28%5Cfrac%7B1%7D%7Bq_i%7D%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  L_i = \log(\frac{1}{q_i}). "/></p>
<p>This is zero if the outcome was predicted with certainty but goes to infinity if the individual <img alt="{q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i}"/> is very low—which is an issue in the quantum case. The expected score based on the true probabilities is <a name="XE"/></p><a name="XE">
<p align="center"><img alt="\displaystyle  E[L_i] = \sum_i p_i \log(\frac{1}{q_i}). \ \ \ \ \ (2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E%5BL_i%5D+%3D+%5Csum_i+p_i+%5Clog%28%5Cfrac%7B1%7D%7Bq_i%7D%29.+%5C+%5C+%5C+%5C+%5C+%282%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  E[L_i] = \sum_i p_i \log(\frac{1}{q_i}). \ \ \ \ \ (2)"/></p>
</a><p><a name="XE"/> The log-likelihood rule is <b>strictly proper</b> insofar as the unique way to minimize <img alt="{E[L_i]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE%5BL_i%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E[L_i]}"/> is to set <img alt="{q_i = p_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i+%3D+p_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i = p_i}"/> for each <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>. In human contexts this means the model has incentive to be as accurate as possible. For the quantum device, knowing the <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> that applies to its running of circuits <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> suffices to calculate <img alt="{E[L_i]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE%5BL_i%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E[L_i]}"/> as “<img alt="{E_{1+F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_%7B1%2BF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_{1+F}}"/>,” and hence to benchmark how accurately the device is conforming to the target.</p>
<p>
The formula (<a href="https://rjlipton.wordpress.com/feed/#XE">2</a>) is the <a href="https://en.wikipedia.org/wiki/Cross_entropy">cross-entropy</a> between the <img alt="{\vec{p}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\vec{p}}"/> and <img alt="{\vec{q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bq%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\vec{q}}"/> distributions. It is advocated in several predecessor papers on quantum supremacy experiments, but in fact the team shifted to something simpler they call “linear cross-entropy.” They simply show that the <img alt="{q_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_z}"/> from their samples collectively beat the “<img alt="{E_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_1}"/>” that applies to <img alt="{\mathcal{D}_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_1}"/>—more simply put, that when summed over <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>-many trials <img alt="{z_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z_t}"/>, </p>
<p align="center"><img alt="\displaystyle  \frac{1}{T} \sum_{t = 1}^T q_{z_t} &gt; \frac{1}{N} + \delta. \ \ \ \ \ (3)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7B1%7D%7BT%7D+%5Csum_%7Bt+%3D+1%7D%5ET+q_%7Bz_t%7D+%3E+%5Cfrac%7B1%7D%7BN%7D+%2B+%5Cdelta.+%5C+%5C+%5C+%5C+%5C+%283%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac{1}{T} \sum_{t = 1}^T q_{z_t} &gt; \frac{1}{N} + \delta. \ \ \ \ \ (3)"/></p>
<p>This just boils down to giving a <a href="https://en.wikipedia.org/wiki/Standard_score">z-score</a> based on the modeling for <img alt="{\mathcal{D}_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_1}"/>. It is analogous to how I (Ken writing this) test for cheating at chess. We are flagging the physical device as getting surreptitious input from quantum to achieve a strength of <img alt="{1 + \delta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1+%2B+%5Cdelta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1 + \delta}"/> compared to a “classical player” who is “rated” as having strength <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1}"/>. </p>
<p>
The difference from showing that the device’s score from (<a href="https://rjlipton.wordpress.com/feed/#XE">2</a>) is within a hair of <img alt="{E_{1+F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_%7B1%2BF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_{1+F}}"/> is that this is based on <img alt="{E_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_1}"/>. To be sure, the paper shows that their <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/>-scores conform to those one would expect an “<img alt="{E_{1+F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_%7B1%2BF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_{1+F}}"/>-rated” device to achieve. But this is still not the same as (<a href="https://rjlipton.wordpress.com/feed/#XE">2</a>). Whether it is tantamount for enough purposes—including the theorem about <img alt="{\mathsf{AM}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BAM%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathsf{AM}}"/>—is where we’re most unsure, and we note distinctions between fully (classically) sampling and “spoofing” the statistical tests(s) raised by Scott (including directly in reply to me <a href="https://www.scottaaronson.com/blog/?p=4372#comment-1822570">here</a>) and others. The authors say that using “linear cross-entropy” gave sharper results and that they tried other (unspecified) measures. We wonder how much of the space of scoring rules familiar in predictive modeling has been tried, and whether rules having more gentle tail behavior for tiny <img alt="{q_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_i}"/> than <img alt="{L_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L_i}"/> might do better.</p>
<p>
Finally, there is the issue that the team were able to verify <img alt="{q_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_z}"/> exactly only for circuits up to <img alt="{43}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B43%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{43}"/> qubits and/or with <img alt="{14}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B14%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{14}"/> levels, not <img alt="{53}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B53%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{53}"/> with <img alt="{20}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B20%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{20}"/> levels. This creates a dilemma in that IBM’s paper may push them toward <img alt="{n = 60}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%3D+60%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n = 60}"/> or <img alt="{70}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B70%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{70}"/>, but that increases the gap from instance sizes they can verify. This also pushes away from the possibly of observing the <img alt="{\mathcal{D}_{1+F}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BD%7D_%7B1%2BF%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\mathcal{D}_{1+F}}"/> nature of <img alt="{D_C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BD_C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{D_C}"/> more directly by finding repeated strings <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> in the second-stage sampling of a fixed <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/>. The “birthday paradox” threshold for repeats is roughly <img alt="{2^{n/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%7Bn%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^{n/2}}"/> samples, which might be feasible for <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> around <img alt="{50}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B50%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{50}"/> (given the classical work needed for each <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/>, which IBM’s cleverness might speed) but not above <img alt="{60}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B60%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{60}"/>. The distinguishing power of repeats drops further with <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/>. We intend to say more about these last few points, and we are sure there are many chapters still to write about supremacy experiments. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Is the evidence so far convincing to you? Is enough being done on the third plank to exclude possible clever classical use of the fact that the circuits <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> are given as “white boxes”? Are there possible loopholes? </p>
<p>
We would also be grateful to know where we may have oversimplified our characterization of the task and our analysis of the issues.</p>
<p/><p><br/>
<b>Addendum 10/28:</b> On further review, the “outcome probability” of a string <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> comes from first exhaustively computing the probability <img alt="{r_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{r_z}"/> that would result from error-free operation of <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> and plugging that in to make <img alt="{Fr_z + (1 - F)\frac{1}{N}.}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BFr_z+%2B+%281+-+F%29%5Cfrac%7B1%7D%7BN%7D.%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Fr_z + (1 - F)\frac{1}{N}.}"/>  Although derived from the estimate of <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{F}"/> and taking <img alt="{z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bz%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{z}"/> from the device, this seems better to regard as the “true probability” <img alt="{p_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_z}"/>, rather than “<img alt="{q_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_z}"/>” as stated above.  The actual quantity to regard as “<img alt="{q_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_z}"/>” is not calculable and estimating it would require observing repeats from the physical device.  Equation (2) remains correct on principle, but as explained in these <a href="https://www.cs.cmu.edu/~odonnell/quantum18/lecture25.pdf">notes</a> by Ryan O’Donnell, the reversed equation is used instead: </p>
<p align="center"><img alt="\displaystyle  E[L'_i] = \sum_z q_z \log(\frac{1}{p_z}). \ \ \ \ \ (2')" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E%5BL%27_i%5D+%3D+%5Csum_z+q_z+%5Clog%28%5Cfrac%7B1%7D%7Bp_z%7D%29.+%5C+%5C+%5C+%5C+%5C+%282%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  E[L'_i] = \sum_z q_z \log(\frac{1}{p_z}). \ \ \ \ \ (2')"/></p>
<p>
The difference is that <img alt="{\log(\frac{1}{p_z})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog%28%5Cfrac%7B1%7D%7Bp_z%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log(\frac{1}{p_z})}"/> can be calculated, and while <img alt="{q_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_z}"/> still cannot be, the act of sampling from the physical device estimates the idealized sum <img alt="{\sum_i q_i \log(\frac{1}{p_i})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_i+q_i+%5Clog%28%5Cfrac%7B1%7D%7Bp_i%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_i q_i \log(\frac{1}{p_i})}"/> closely enough.  This switches the roles of “forecaster” and “forecastee,” but the optimality of <img alt="{q_z = p_z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_z+%3D+p_z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_z = p_z}"/> remains valid and the target value is the same as before.  O’Donnell calls this inversion “slightly dicey” but (i) it was ultimately not used anyway, (ii) has an interpretation that regards the physical device as the ground truth, and (iii) may be equally amenable to asymptotic conditional hardness results.  Likewise “<img alt="{q_{z_t}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bq_%7Bz_t%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{q_{z_t}}"/>” should be re-named as “<img alt="{p_{z_t}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp_%7Bz_t%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{p_{z_t}}"/>” in (3).]</p>
<p/><p><br/>
[Added more error-modeling details to the real-world section; some minor word changes; clarified how X,Y,W are chosen; addendum to clarify modeling issues.]</p></font></font></div>
    </content>
    <updated>2019-10-27T13:19:04Z</updated>
    <published>2019-10-27T13:19:04Z</published>
    <category term="All Posts"/>
    <category term="detection"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="decoherence"/>
    <category term="Google"/>
    <category term="IBM"/>
    <category term="John Martinis"/>
    <category term="Physics"/>
    <category term="quantum"/>
    <category term="quantum computer"/>
    <category term="quantum supremacy"/>
    <category term="Scott Aaronson"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-10-31T14:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=18384</id>
    <link href="https://gilkalai.wordpress.com/2019/10/27/starting-today-kazhdan-sunday-seminar-computation-quantumness-symplectic-geometry-and-information/" rel="alternate" type="text/html"/>
    <title>Starting today: Kazhdan Sunday seminar: “Computation, quantumness, symplectic geometry, and information”</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Sunday, 27 October, 2019 – 14:00 to 16:00 Repeats every week every Sunday until Sat Feb 01 2020 Location: Ross 70 See also: Seminar announcement; previous post Symplectic Geometry, Quantization, and Quantum Noise. The Google supremacy claims are discussed (with … <a href="https://gilkalai.wordpress.com/2019/10/27/starting-today-kazhdan-sunday-seminar-computation-quantumness-symplectic-geometry-and-information/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Sunday, 27 October, 2019 – 14:00 to 16:00</p>
<p>Repeats every week every Sunday until Sat Feb 01 2020</p>
<p>Location: Ross 70</p>
<p>See also: <a href="https://mathematics.huji.ac.il/event/kazhdan-sunday-seminar-computation-quantumness-symplectic-geometry-and-information-gil?delta=0">Seminar announcement</a>; previous post <a href="https://gilkalai.wordpress.com/2013/01/01/symplectic-geometry-quantization-and-quantum-noise/" rel="bookmark">Symplectic Geometry, Quantization, and Quantum Noise.</a></p>
<p>The Google supremacy claims are discussed (with updates from time to time) in <a href="https://gilkalai.wordpress.com/2019/09/23/quantum-computers-amazing-progress-google-ibm-and-extraordinary-but-probably-false-supremacy-claims-google/">this earlier post</a>. Don’t miss <a href="https://gilkalai.wordpress.com/2019/10/13/gerard-cornuejolss-bakers-eighteen-5000-dollars-conjectures/">our previous post</a> on combinatorics.</p>
<h3>Tentative syllabus for “Computation, quantumness, symplectic geometry, and information”</h3>
<p>1. Mathematical models of classical and quantum mechanics.</p>
<p>2. Correspondence principle and quantization.</p>
<p>3. Classical and quantum computation: gates, circuits, algorithms (Shor, Grover). Solovay-Kitaev. Some ideas of cryptography</p>
<p>4. Quantum noise and measurement, and rigidity of the Poisson bracket.</p>
<p>5. Noisy classical and quantum computing and error correction, threshold theorem- quantum fault tolerance (small noise is good for quantum computation). Kitaev’s surface code.</p>
<p>6. Quantum speed limit/time-energy uncertainty vs symplectic displacement energy.</p>
<p>7. Time-energy uncertainty and quantum computation (Dorit or her student?)</p>
<p>8. Berezin transform, Markov chains, spectral gap, noise.</p>
<p>9. Adiabatic computation, quantum PCP (probabilistically checkable proofs) conjecture [? under discussion]</p>
<p>10. Noise stability and noise sensitivity of Boolean functions, noisy boson sampling</p>
<p>11. Connection to quantum field theory (Guy?).</p>
<p>Literature: Aharonov, D. Quantum computation, In “Annual Reviews of Computational Physics” VI, 1999 (pp. 259-346). <a href="https://arxiv.org/abs/quant-ph/9812037">https://arxiv.org/abs/quant-ph/9812037</a></p>
<p>Kalai, G., Three puzzles on mathematics computations, and games, Proc. Int Congress Math 2018, Rio de Janeiro, Vol. 1 pp. 551–606. <a href="https://arxiv.org/abs/1801.02602">https://arxiv.org/abs/1801.02602</a></p>
<p>Nielsen, M.A., and Chuang, I.L., Quantum computation and quantum information. Cambridge University Press, Cambridge, 2000.</p>
<p>Polterovich, L., Symplectic rigidity and quantum mechanics, European Congress of Mathematics, 155–179, Eur. Math. Soc., Zürich, 2018. <a href="https://sites.google.com/site/polterov/miscellaneoustexts/symplectic-rigidity-and-quantum-mechanics">https://sites.google.com/site/polterov/miscellaneoustexts/symplectic-rig…</a></p>
<p>Polterovich L., and Rosen D., Function theory on symplectic manifolds. American Mathematical Society; 2014. [Chapters 1,9] <a href="https://sites.google.com/site/polterov/miscellaneoustexts/function-theory-on-symplectic-manifolds">https://sites.google.com/site/polterov/miscellaneoustexts/function-theor…</a></p>
<p>Wigderson, A., Mathematics and computation, Princeton Univ. Press, 2019. <a href="https://www.math.ias.edu/files/mathandcomp.pdf">https://www.math.ias.edu/files/mathandcomp.pdf</a></p></div>
    </content>
    <updated>2019-10-27T06:09:19Z</updated>
    <published>2019-10-27T06:09:19Z</published>
    <category term="Combinatorics"/>
    <category term="Computer Science and Optimization"/>
    <category term="Geometry"/>
    <category term="Physics"/>
    <category term="Teaching"/>
    <category term="Quantization"/>
    <category term="Quantum computation"/>
    <category term="Quantum information"/>
    <category term="Symplectic geometry"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-10-31T14:20:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/143</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/143" rel="alternate" type="text/html"/>
    <title>TR19-143 |  Equivalence of Systematic Linear Data Structures and Matrix Rigidity | 

	Sivaramakrishnan Natarajan Ramamoorthy, 

	Cyrus Rashtchian</title>
    <summary>Recently, Dvir, Golovnev, and Weinstein have shown that sufficiently strong lower bounds for linear data structures would imply new bounds for rigid matrices. However, their result utilizes an algorithm that requires an $NP$ oracle, and hence, the rigid matrices are not explicit. In this work, we derive an equivalence between rigidity and the systematic linear model of data structures. For the $n$-dimensional inner product problem with $m$ queries, we prove that lower bounds on the query time imply rigidity lower bounds for the query set itself. In particular, an explicit lower bound of $\omega\left(\frac{n}{r}\log m\right)$ for $r$ redundant storage bits would yield better rigidity parameters than the best bounds due to Alon, Panigrahy, and Yekhanin. We also prove a converse result, showing that rigid matrices directly correspond to hard query sets for the systematic linear model. As an application, we prove that the set of vectors obtained from rank one binary matrices is rigid with parameters matching the known results for explicit sets. This implies that the vector-matrix-vector problem requires query time $\Omega(n^{3/2}/r)$ for redundancy $r \geq \sqrt{n}$ in the systematic linear model, improving a result of Chakraborty, Kamma, and Larsen. Finally, we prove a cell probe lower bound for the vector-matrix-vector problem in the  high error regime, improving a result of Chattopadhyay, Koucký, Loff, and Mukhopadhyay.</summary>
    <updated>2019-10-25T19:43:46Z</updated>
    <published>2019-10-25T19:43:46Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-10-31T14:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/10/25/tenure-track-faculties-at-krannert-school-of-management-purdue-university-apply-by-december-1-2019/</id>
    <link href="https://cstheory-jobs.org/2019/10/25/tenure-track-faculties-at-krannert-school-of-management-purdue-university-apply-by-december-1-2019/" rel="alternate" type="text/html"/>
    <title>Tenure track faculties at Krannert School of Management, Purdue University (apply by December 1, 2019)</title>
    <summary>Krannert School of Management invites applicants for two tenure-track faculty positions at the assistant professor level in the Quantitative Methods area, to begin in the fall semester of 2020. We welcome applicants from all research areas represented within the Quantitative Methods area. Website: https://career8.successfactors.com/sfcareer/jobreqcareer?jobId=8013&amp;company=purdueuniv&amp;userna%20me= Email: nguye161@purdue.edu</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Krannert School of Management invites applicants for two tenure-track faculty positions at the assistant professor level in the Quantitative Methods area, to begin in the fall semester of 2020. We welcome applicants from all research areas represented within the Quantitative Methods area.</p>
<p>Website: <a href="https://career8.successfactors.com/sfcareer/jobreqcareer?jobId=8013&amp;company=purdueuniv&amp;userna%20me=">https://career8.successfactors.com/sfcareer/jobreqcareer?jobId=8013&amp;company=purdueuniv&amp;userna%20me=</a><br/>
Email: nguye161@purdue.edu</p></div>
    </content>
    <updated>2019-10-25T18:11:05Z</updated>
    <published>2019-10-25T18:11:05Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-10-31T14:21:14Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/10/24/simons-berkeley-research-fellowship-at-simons-institute-for-the-theory-of-computing-apply-by-december-15-2019/</id>
    <link href="https://cstheory-jobs.org/2019/10/24/simons-berkeley-research-fellowship-at-simons-institute-for-the-theory-of-computing-apply-by-december-15-2019/" rel="alternate" type="text/html"/>
    <title>Simons-Berkeley Research Fellowship at Simons Institute for the Theory of Computing (apply by December 15, 2019)</title>
    <summary>The Simons Institute for the Theory of Computing invites applications for the Simons-Berkeley Research Fellowships to participate in one or more of the semester-long programs during the 2020-21 academic year: Probability, Geometry, and Computation in High Dimensions; Theory of Reinforcement Learning; Satisfiability: Theory, Practice, and Beyond; and Theoretical Foundations of Computer Systems Website: https://simons.berkeley.edu/fellows2020 Email: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Simons Institute for the Theory of Computing invites applications for the Simons-Berkeley Research Fellowships to participate in one or more of the semester-long programs during the 2020-21 academic year: Probability, Geometry, and Computation in High Dimensions; Theory of Reinforcement Learning; Satisfiability: Theory, Practice, and Beyond; and Theoretical Foundations of Computer Systems</p>
<p>Website: <a href="https://simons.berkeley.edu/fellows2020">https://simons.berkeley.edu/fellows2020</a><br/>
Email: simonsvisitorservices@berkeley.edu</p></div>
    </content>
    <updated>2019-10-24T23:20:27Z</updated>
    <published>2019-10-24T23:20:27Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-10-31T14:21:14Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1513</id>
    <link href="https://theorydish.blog/2019/10/24/shopping-for-grain-in-the-market-works-a-fine-job/" rel="alternate" type="text/html"/>
    <title>Shopping for Grain in the Market Works –   a Fine Job!</title>
    <summary>I’m excited to share the news of two upcoming workshops…   TCS Early Career Mentoring Workshop Yael Kalai, Matt Weinberg, and I are organizing a TCS mentoring workshop in upcoming FOCS with a focus on demystifying the job market. The program includes a senior panel featuring Shafi Goldwasser, Samir Khuller, Tim Roughgarden, and Eva Tardos, a junior panel starring Inbal Talgam-Cohen, Omri Weinstein, and Henry Yuen, and two exemplary job talks by Eshan Chattopadhyay and Pravesh Kothari. Visit our website to see the full program and most importantly suggest panel questions.   Fine-grained Complexity Workshop Amir Abboud and I are organizing a workshop on fine-grained complexity, to be held Jan 2nd 2020 at Tel-Aviv University, closing the first annual TAU Theory-Fest. The program includes a morning of plenary talks (Karl Bringmann, Seth Pettie, and Barna Saha) and shorter cutting-edge technical talks in the afternoon. (If you have something interesting to share with the fine-grained complexity community, and we haven’t contacted you yet about giving a talk, please let us know.)   Looking forward to seeing you in Baltimore and Tel-Aviv!</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I’m excited to share the news of two upcoming workshops…</p>
<p> </p>
<h3>TCS Early Career Mentoring Workshop</h3>
<p>Yael Kalai, Matt Weinberg, and I are organizing a TCS mentoring workshop in <a href="http://focs2019.cs.jhu.edu/">upcoming FOCS</a> with a focus on <strong>demystifying the job market</strong>.</p>
<p>The program includes a senior panel featuring Shafi Goldwasser, Samir Khuller, Tim Roughgarden, and Eva Tardos, a junior panel starring Inbal Talgam-Cohen, Omri Weinstein, and Henry Yuen, and two exemplary job talks by Eshan Chattopadhyay and Pravesh Kothari.</p>
<p>Visit our <a href="https://www.cs.princeton.edu/~smattw/FOCS19/index.html">website</a> to see the full program and most importantly <a href="https://forms.gle/5kw7Zydo4Cvw2D4r7">suggest panel questions</a>.</p>
<p> </p>
<h3>Fine-grained Complexity Workshop</h3>
<p>Amir Abboud and I are organizing a workshop on fine-grained complexity, to be held Jan 2nd 2020 at Tel-Aviv University, closing the first annual <a href="https://sites.google.com/view/tau-theory-fest/home">TAU Theory-Fest</a>.</p>
<p>The program includes a morning of plenary talks (Karl Bringmann, Seth Pettie, and Barna Saha) and shorter cutting-edge technical talks in the afternoon.</p>
<p>(If you have something interesting to share with the fine-grained complexity community, and we haven’t contacted you yet about giving a talk, please let us know.)</p>
<p> </p>
<p>Looking forward to seeing you in Baltimore and Tel-Aviv!</p></div>
    </content>
    <updated>2019-10-24T17:59:00Z</updated>
    <published>2019-10-24T17:59:00Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>aviad.rubinstein</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2019-10-31T14:21:28Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7561</id>
    <link href="https://windowsontheory.org/2019/10/24/boazs-inferior-classical-inferiority-faq/" rel="alternate" type="text/html"/>
    <title>Boaz’s inferior classical inferiority FAQ</title>
    <summary>(For better info, see Scott’s Supreme Quantum Superiority FAQ and also his latest post on the Google paper; also this is not really an FAQ but was inspired by a question about the Google paper from a former CS 121 student) “Suppose aliens invade the earth and threaten to obliterate it in a year’s time […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>(For better info, see <a href="https://www.scottaaronson.com/blog/?p=4317">Scott’s Supreme Quantum Superiority FAQ</a> and also his <a href="https://www.scottaaronson.com/blog/?p=4372">latest post on the Google paper</a>; also this is not really an FAQ but was inspired by a question about the Google paper from a former <a href="https://cs121.boazbarak.org/schedule/">CS 121 </a>student)</p>



<blockquote class="wp-block-quote"><p><em> “Suppose aliens invade the earth and threaten to obliterate it in a year’s time unless human beings can find the Ramsey number for red five and blue five. We could marshal the world’s best minds and fastest computers, and within a year we could probably calculate the value. If the aliens demanded the Ramsey number for red six and blue six, however, we would have no choice but to launch a preemptive attack.</em>“</p><cite>Paul Erdős (as quoted by Graham and Spencer, 1990, hat tip: <a href="https://windowsontheory.org/2017/10/30/the-different-forms-of-quantum-computing-skepticism/#comment-42659">Lamaze Tishallishmi</a>)</cite></blockquote>



<p>In a <a href="https://www.nature.com/articles/s41586-019-1666-5">Nature paper</a> published this week, a group of researchers from John Martinis’s lab at Google announced arguably the first demonstration of “quantum supremacy” – a computational task carried out by a 53 qubit quantum computer that would require a prohibitive amount of time to simulate classically. </p>



<p>Google’s calculations of the “classical computation time” might have been overly pessimistic (from the classical point of view), and there has been work from <a href="https://arxiv.org/abs/1910.09534">IBM</a> as well as some <a href="https://www.caltech.edu/campus-life-events/master-calendar/iqi-weekly-seminar-2019-10-01">work of Johnnie Gray</a> suggesting that there are significant savings to be made. Indeed, given the lessons that we learned from private key cryptography, where techniques such as linear and differential cryptanalysis were used to “shave factors from exponents”, we know that even if a problem requires exponential time in general, this does not mean that by being very clever we can’t make significant savings over the naive brute force algorithm. This holds doubly  so in this case, where, unlike the designers of block ciphers, the Google researchers were severely constrained by factors of geometry and the kind of gates they can reliably implement.</p>



<p>I would not be terribly surprised if we will see more savings and even an actual classical simulation of the same sampling task that Google achieved. In fact, I very much hope this happens, since it will allow us to independently verify the reliability of Google’s chip and whether it actually did in fact sample from the distribution it is supposed to have sampled from (or at least rule out some “null hypothesis”).  But this would not change the main point that the resources for classical simulation, as far as we know, scale exponentially with the number of qubits and their quality. While we could perhaps with great effort simulate a 53 qubit depth 20 circuit classically, once we reach something like 100 qubits and depth then all current approaches will be hopelessly behind.</p>



<p>In the language of <a href="https://windowsontheory.org/2017/10/30/the-different-forms-of-quantum-computing-skepticism/">my essay on quantum skepticism</a>, I think this latest result, and the rest of the significant experimental progress that has been going on, all but rules out the possibility of “Skepticland”  where there would be some fundamental physical reason why it is not possible to build quantum computers that offer exponential advantage in the amount of resources to achieve certain tasks over classical computers.</p>



<p>While the worlds of “Popscitopia”  (quantum computers can do everything) and “Classicatopia” (there is an efficient classical algorithm to simulate BQP) remain mathematical possiblities (just as P=NP is), most likely we live in <strong>“Superiorita”</strong> where quantum computers do offer exponential advantage for <em>some</em> computational problems.</p>



<p>Some people question whether these kind of “special purpose” devices that might be very expensive to build are worth the investment. First of all (and most importantly for me), as I argued in <a href="https://windowsontheory.org/2017/10/30/the-different-forms-of-quantum-computing-skepticism/">my essay</a>, exploring the limits of physically realizable computation is a grand scientific goal in its own right worthy of investment regardless  of applications. Second, technology is now a <a href="https://www.gartner.com/en/newsroom/press-releases/2019-01-28-gartner-says-global-it-spending-to-reach--3-8-trillio#targetText=Worldwide%20IT%20spending%20is%20projected,latest%20forecast%20by%20Gartner%2C%20Inc.">3.8 trillion dollar </a>per year industry, and quantum computers are in a very real sense the first qualitatively different computing devices since the days of Babbage and Turing. Spending a fraction of a percent of the industry’s worth to the economy on exploring the potential for quantum computing seems like a good investment, even if there will be no practical application in the next decade or two. (By the same token, spending a fraction of a percent on exploring algorithm design and the limitations of <em>classical </em>algorithms is a very good investment as well.)</p></div>
    </content>
    <updated>2019-10-24T13:32:18Z</updated>
    <published>2019-10-24T13:32:18Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2019-10-31T14:21:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/142</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/142" rel="alternate" type="text/html"/>
    <title>TR19-142 |  Semi-Algebraic Proofs, IPS Lower Bounds and the $\tau$-Conjecture: Can a Natural Number be Negative? | 

	Yaroslav Alekseev, 

	Dima Grigoriev, 

	Edward Hirsch, 

	Iddo  Tzameret</title>
    <summary>We introduce the `binary value principle' which is a simple subset-sum instance expressing that a natural number written in binary cannot be negative, relating it to central problems in proof and algebraic complexity. We prove conditional superpolynomial lower bounds on the Ideal Proof System (IPS) refutation size of this instance, based on a well-known hypothesis by Shub and Smale about the hardness of computing factorials, where IPS is the strong algebraic proof system introduced by Grochow and Pitassi (2018). Conversely, we show that short IPS refutations of this instance bridge the gap between sufficiently strong algebraic and semi-algebraic proof systems. Our results extend to full-fledged IPS the paradigm introduced in Forbes et al. (2016), whereby lower bounds against subsystems of IPS were obtained using restricted algebraic circuit lower bounds, and demonstrate that the binary value principle captures the advantage of semi-algebraic over algebraic reasoning, for sufficiently strong systems. Specifically, we show the following:

*Conditional IPS lower bounds:* The Shub-Smale hypothesis (1995) implies a superpolynomial lower bound on the size of IPS refutations of the binary value principle over the rationals defined as the unsatisfiable linear equation $\sum_{i=1}^{n} 2^{i-1}x_i = -1$, for boolean $x_i$'s. Further, the related $\tau$-conjecture (1995) implies a superpolynomial lower bound on the size of IPS refutations of a variant of the binary value principle over the ring of rational functions. No prior conditional lower bounds were known for IPS or for apparently much weaker propositional proof systems such as Frege.

*Algebraic vs. semi-algebraic proofs:* Admitting short refutations of the binary value principle is necessary for any algebraic proof system to fully simulate any known semi-algebraic proof system, and for strong enough algebraic proof systems it is also sufficient. In particular, we introduce a very strong proof system that simulates all known semi-algebraic proof systems (and most other known concrete propositional proof systems), under the name Cone Proof System (CPS), as a semi-algebraic analogue of the ideal proof system: CPS establishes the unsatisfiability of collections of polynomial equalities and inequalities over the reals, by representing sum-of-squares proofs (and extensions) as algebraic circuits. We prove that IPS is polynomially equivalent to CPS iff IPS admits polynomial-size refutations of the binary value principle (for the language of systems of equations that have no 0/1-solutions), over both $\mathbb{Z}$ and $\mathbb{Q}$.</summary>
    <updated>2019-10-24T00:45:43Z</updated>
    <published>2019-10-24T00:45:43Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-10-31T14:20:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/141</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/141" rel="alternate" type="text/html"/>
    <title>TR19-141 |  On Rich $2$-to-$1$ Games | 

	Mark Braverman, 

	Subhash Khot, 

	Dor Minzer</title>
    <summary>We propose a variant of the $2$-to-$1$ Games Conjecture that we call the Rich $2$-to-$1$ Games Conjecture and show that it is equivalent to the Unique Games Conjecture. We are motivated by two considerations. Firstly, in light of the recent proof of the $2$-to-$1$ Games Conjecture, we hope to understand how one might make further progress towards a proof of the Unique Games Conjecture. Secondly, the new variant along with perfect completeness in addition, might imply hardness of approximation results that necessarily require perfect completeness and (hence) are not implied by the Unique Games Conjecture.</summary>
    <updated>2019-10-24T00:42:15Z</updated>
    <published>2019-10-24T00:42:15Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-10-31T14:20:39Z</updated>
    </source>
  </entry>
</feed>
