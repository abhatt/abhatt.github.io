<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-01-02T08:38:47Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17937</id>
    <link href="https://rjlipton.wordpress.com/2021/01/01/peter-m-neumann-1940-2020/" rel="alternate" type="text/html"/>
    <title>Peter M. Neumann, 1940–2020</title>
    <summary>Memories from Oxford days Mathematical Institute src Peter Neumann, Professor of Mathematics at Queen’s College, Oxford University, passed away two weeks ago. Last Monday would have been his 80th birthday. Today we join others mourning in private and remembering in public, including the authors of these stories and memories, who are among Peter’s 39 graduated […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Memories from Oxford days</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/01/01/peter-m-neumann-1940-2020/neumann2/" rel="attachment wp-att-17939"><img alt="" class="alignright wp-image-17939" height="200" src="https://rjlipton.files.wordpress.com/2021/01/neumann2.jpg?w=200&amp;h=200" width="200"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Mathematical Institute <a href="https://www.maths.ox.ac.uk/people/peter.neumann">src</a></font></td>
</tr>
</tbody>
</table>
<p>
Peter Neumann, Professor of Mathematics at Queen’s College, Oxford University, passed away two weeks ago. Last Monday would have been his 80th birthday. </p>
<p>
Today we join others mourning in private and remembering in public, including the authors of these <a href="https://luit-pariya.blogspot.com/2020/12/eter-neumann-is-no-more.html">stories</a> and <a href="https://cameroncounts.wordpress.com/2020/12/24/memories-of-peter-neumann/">memories</a>, who are among Peter’s <a href="https://www.genealogy.math.ndsu.nodak.edu/id.php?id=28732">39</a> graduated doctoral students.</p>
<p>
Peter died from Covid-19 while in a care home for long treatment of a stroke that had paralyzed his left side two years ago—something he <a href="https://www.queens.ox.ac.uk/academics/neumann">wrote</a> he could recover from “no earlier than late 2020.” From the number of people we know who have been touched closely by this kind of happening, my family included, we have a small statistical window on a great tragedy. The one resolution we can most make for the New Year is that it bring a resolution, so that the progression Covid 19-20-21 stops there.</p>
<p>
The two words that most define Peter for me are <em>genial</em> and <em>acute</em>. I knew him from the beginning of my time as a graduate student at Oxford under a Marshall Scholarship in 1981. Every “don” at Oxford is associated to one of the thirty-nine residential colleges or six private halls. I joined Merton College to associate with my advisor, Dominic Welsh, but also there was Peter Cameron, whose memories are linked above and who joined Merton’s faculty after earning his doctorate under Peter in 1971. Perhaps that contributed to why Peter—that is, Neumann—kept up spelling his name Πeter. </p>
<p>
</p><p/><h2> The Kinderseminar </h2><p/>
<p/><p>
Early in my first term, Peter invited me to join a special weekly seminar hosted by Πeter in his expansive room in Queen’s College. It was already called the <em>Kinderseminar</em>, for it was directed to the graduate students for experience reading current research papers and, importantly, presenting them. It always began at 11am and had flexible time after noon because college lunches at Oxford typically began at 12:20 or 12:30 for students and 1pm for fellows.</p>
<p>
It is hard to replicate the atmosphere anywhere I have seen in the US. There was at least one big comfy chair, in which I remember Peter’s student Sarah Rees almost disappearing, and other chairs brought in around a coffee table for tea and—much more often—coffee. And a roll of <a href="https://en.wikipedia.org/wiki/McVitie's">McVitie’s</a> that was passed around. High windows let in enough light to work lamps-off except on the darkest days. Lugged to the head of the table when the time for chitchat was over was a portable chalkboard. Then began a masterclass in how to give a masterclass.</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<p>
<a href="https://rjlipton.wordpress.com/2021/01/01/peter-m-neumann-1940-2020/the_queens_college_front_quad/" rel="attachment wp-att-17941"><img alt="" class="aligncenter wp-image-17941" height="225" src="https://rjlipton.files.wordpress.com/2021/01/the_queens_college_front_quad.jpg?w=300&amp;h=225" width="300"/></a>
</p></td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Queen’s College toward his rooms at right. (Photo by Odicalmuse, <a href="https://commons.wikimedia.org/w/index.php?curid=94248927">own work</a>.)</font>
</td>
</tr>
</tbody></table>
<p>
We have already mentioned the <a href="https://rjlipton.wordpress.com/2013/09/27/a-dictionary-for-reading-proofs/">seminar</a> a <a href="https://rjlipton.wordpress.com/2019/02/28/phrases-that-drive-me-crazy/">number</a> of <a href="https://rjlipton.wordpress.com/2019/07/16/summer-reading-in-theory/">times</a> on this blog. Two of those mentions recall a lecture by Πeter himself on Évariste Galois, whose ulterior point was to warn the dangers of carelessly using phrases like “it is easy to see that…” in proofs. Well Galois, explained Πeter, was writing in haste the night before the duel that killed him, but most of us don’t have that excuse. Here is a preserved later <a href="https://www.youtube.com/watch?app=desktop&amp;v=xfJ4vwQ3vpo">lecture</a> by Πeter on Galois.</p>
<p>
The most recent of those posts featured Norman Biggs, who visited Oxford and took part in the seminar for some of my first year. My own first presentation in the seminar was on alternate proofs I’d found in my Princeton undergraduate thesis of theorems covered in a textbook by Biggs. He gave me the nice compliment of saying that if his text went to a second edition he could use my proofs. But Πeter—ah, Πeter took me aside afterward and pointed out my kinks in delivering them, including body positioning and organizing diagrams and algebra on the board. Encouragingly, of course.</p>
<p>
Also a regular was my fellow Mertonian and chess master Dugald Macpherson. Neither of us was the most illustrious chess player to enter that room, for grandmaster Colin McNab obtained his doctorate under Πeter shortly after I finished. I was more impressed by stories of playing first-class cricket in Jamaica by the visiting Ken Johnson. Thirty years later, when I emerged from my car in Princeton’s south campus parking lot and boarded a jitney going to the 2012 Alan Turing Centennial event covered <a href="https://rjlipton.wordpress.com/2012/05/17/turings-tiger-birthday-party/">here</a>, there was Johnson—who is now a professor at Penn State. Other students I have particular memories of there are Muhammad Audu, Jacinta Covington, and Tim Penttila.</p>
<p>
One quirky memory was coming in on Tuesday, Oct. 25, 1983, to find everyone around a radio bringing bulletins of the US invasion of Grenada which had begun at local dawn. After some pointed anti-imperialist disapproval they did get into math, but my mind stayed on Grenada as I composed in my head a steel-drum melody and the first verse of a still-unfinished <a href="https://cse.buffalo.edu/~regan/Writing/GrenadaAfter.txt">song</a> about the event. </p>
<p>
</p><p/><h2> Another Memory </h2><p/>
<p/><p>
Because his OBE from Queen Elizabeth in 2008 says Peter not Πeter, I will revert to the former. This was cited for services to mathematics education. He not only won other prizes, he has a prize named after him: the <a href="https://en.wikipedia.org/wiki/British_Society_for_the_History_of_Mathematics#Neumann_Prize">Neumann Prize</a> by the British Society for the History of Mathematics. Peter served once as its president and also led the British Mathematical Association—whose logo spells its name with a lowercase <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\alpha}"/>.</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/01/01/peter-m-neumann-1940-2020/ma-logo/" rel="attachment wp-att-17942"><img alt="" class="aligncenter size-full wp-image-17942" src="https://rjlipton.files.wordpress.com/2021/01/ma-logo.jpeg?w=600"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Mathematical Association <a href="https://www.m-a.org.uk/">home</a></font>
</td>
</tr>
</tbody></table>
<p>
From various experiences I can aver that Peter took a <em>whole-person</em> approach to education. He followed the lives of his former students and I heard stories of social occasions in both Britain and France. He also wrote the Oxford Mathematical Institute’s <a href="https://courses.maths.ox.ac.uk/node/view_material/1160">guide</a> to TeX and LaTeX for students. </p>
<p>
In May, 1984, the Institute organized an outing to a Mathematics Day at the University of Warwick near Coventry, about 55 miles north of Oxford. Peter organized about a dozen for the trip—whether this was everyone coming from Oxford, and whether we had one shuttle or separate cars, I do not recall. </p>
<p>
What I do recall is that he did not simply take us to and from the workshop. He planned a picnic en-route at the <a href="https://en.wikipedia.org/wiki/Rollright_Stones">Rollright Stones</a>, which are along a road at the Oxon-Warks border. I had not previously seen a Celtic stone circle. He told stories of its history. I also believe that after the meeting we took a drive into Coventry with a brief stop to walk around the cathedral to see the damage from World War II, but it is possible I am conflating that with a time the Oxford chess team played a match in Coventry.</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wordpress.com/2021/01/01/peter-m-neumann-1940-2020/kings-men-stone-circle-at-great-rollright/" rel="attachment wp-att-17943"><img alt="" class="aligncenter size-medium wp-image-17943" height="200" src="https://rjlipton.files.wordpress.com/2021/01/kings-men-stone-circle-at-great-rollright.jpg?w=300&amp;h=200" width="300"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">“Visit baby Stonehenge” <font size="-2"><a href="https://www.farmcottages.com/visit-stonehenge-great-rollright-stones-north-cotswolds/">src</a></font>
</font></td>
</tr>
</tbody></table>
<p>
</p><p/><h2> Instances and Problems in Ancient Complexity </h2><p/>
<p/><p>
Peter carried the Neumann pedigree in group theory, for it was the specialty of both his <a href="https://en.wikipedia.org/wiki/Hanna_Neumann">mother</a> and his <a href="https://en.wikipedia.org/wiki/Bernhard_Neumann">father</a>. He supervised another friend from Merton, Julia Tompson, in a thesis on the history of the Jordan-Hölder theorem. But I will discuss a contribution in which group theory is in the background—where I can put ideas from computational complexity in the foreground.</p>
<p>
Our May 2014 <a href="https://rjlipton.wordpress.com/2014/05/23/stoc-1500/">post</a> “STOC 1500” had a semi-serious intent of aligning famous classical problems of geometric (non-)constructibility with modern complexity theory. It has a fantasized keynote by the chess master and mathematician Luca Pacioli introducing the “<img alt="{\mathcal{NC=C}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BNC%3DC%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathcal{NC=C}}"/>?” problem: using Euclid’s ruler and compass alone, can one trisect an angle, double a cube, or (going beyond <img alt="{\mathcal{NC}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BNC%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathcal{NC}}"/>) square a circle? Much as we’re at pains to say that “<img alt="{\mathsf{NP}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BNP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathsf{NP}}"/>” does <em>not</em> stand for “non-polynomial,” the name “<img alt="{\mathcal{NC}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BNC%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathcal{NC}}"/>” does not stand for “non-constructible” but rather constructible by <a href="https://en.wikipedia.org/wiki/Neusis_construction">neusis</a>, meaning using fixed marks on a rotatable ruler. Of course, now we know that <img alt="{\mathcal{NC \neq C}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BNC+%5Cneq+C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathcal{NC \neq C}}"/>, a fact often attributed to Galois theory but, as Peter noted in his <a href="https://www.ime.usp.br/~pleite/pub/artigos/neumann/reflections_on_reflection_on_spherical_mirror.pdf">paper</a>, provable from simpler considerations about field extensions.</p>
<p>
Both “Ancient Complexity” and computational complexity have a key distinction between <em>instances</em> and <em>problems</em>. Trisecting a <img alt="{90^\circ}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B90%5E%5Ccirc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{90^\circ}"/> angle is an easy instance. If <img alt="{\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\alpha}"/> is a <em>constructible angle</em>, then the angle <img alt="{\beta = 3\alpha}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta+%3D+3%5Calpha%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\beta = 3\alpha}"/> can be trisected—but the method is <em>ad hoc</em> to that instance, not a single algorithm that given only <img alt="{\beta}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbeta%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\beta}"/> would efficiently discover how to trisect it. There are algorithms that work correctly on multiple angles but not on all of them.</p>
<p>
What is most different from computational complexity is that there are single instances that cannot be solved, such as <img alt="{\alpha = 60^\circ}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha+%3D+60%5E%5Ccirc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\alpha = 60^\circ}"/>. It is not a case of a Euclid-based algorithm taking too long: no <img alt="{\mathcal{C}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BC%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathcal{C}}"/> algorithm can construct a <img alt="{20^\circ}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B20%5E%5Ccirc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{20^\circ}"/> angle, period. This is akin to how a single string can be <em>incompressible</em> in Kolmogorov complexity, but without the latter’s dependence on a reference universal compression scheme.</p>
<p>
Another difference is problems, such as doubling the cube and squaring the circle, that essentially have only one instance. One can impose algebraic coordinates to represent different instances of them, but that is different from defining the coordinates from features of a given instance, as allowed when marking a ruler for <em>neusis</em>.  Peter’s paper speaks these problem/instance distinctions well.</p>
<p>
</p><p/><h2> The Reflection Problem </h2><p/>
<p/><p>
The <a href="https://en.wikipedia.org/wiki/Alhazen's_problem">problem</a> addressed in Peter’s paper was formulated by Ptolemy in the year 150. Given a circle <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{C}"/> and points <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{a}"/> and <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{b}"/>, both inside <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{C}"/> or both outside <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{C}"/>, construct a point <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{p}"/> on <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{C}"/> so that light starting from <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{a}"/> will reflect at <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{p}"/> and go to <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{b}"/>. When the points are inside, this is imagined as shooting a perfect billiard ball from <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{a}"/> off a circular rim to hit <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{b}"/>.</p>
<p/><p><br/>
<a href="https://rjlipton.wordpress.com/2021/01/01/peter-m-neumann-1940-2020/alhazen/" rel="attachment wp-att-17945"><img alt="" class="aligncenter size-medium wp-image-17945" height="234" src="https://rjlipton.files.wordpress.com/2021/01/alhazen.png?w=300&amp;h=234" width="300"/></a></p>
<p/><p><br/>
There are two algorithms in <img alt="{\mathcal{C}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BC%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathcal{C}}"/> that correctly solve an infinite set of instances, namely points <img alt="{a',b'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%27%2Cb%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{a',b'}"/> equidistant from the center <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{0}"/> of the circle. One constructs the midpoint <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{m}"/> of the line segment between <img alt="{a'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{a'}"/> and <img alt="{b'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{b'}"/>, projects the ray <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{R}"/> from <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{0}"/> to <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{m}"/>, and finds <img alt="{p'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{p'}"/> where the ray intersects <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{C}"/>. The other (shown in the above figure) first finds the intersections of the rays from <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{0}"/> to <img alt="{a',b'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%27%2Cb%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{a',b'}"/> with <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{C}"/>, bisects the line segment connecting them, and uses that to project outward to <img alt="{p'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{p'}"/>. The latter algorithm also works on points <img alt="{a'',b''}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%27%27%2Cb%27%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{a'',b''}"/> that are collinear with the rays to <img alt="{a'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{a'}"/> and <img alt="{b'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{b'}"/>, respectively, while the former is correct for pairs taken from the lines through <img alt="{a'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{a'}"/> and <img alt="{b'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{b'}"/> that are parallel to <img alt="{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{R}"/>. If <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{0}"/> is not available, the methods can be combined—to work on the original equidistant cases.</p>
<p>
Around the year 1020, Hasan Ibn al-Haytham presented a geometric algorithm that works in all cases, <a href="https://www2.kenyon.edu/Depts/Math/Aydin/Teach/128/AlHazenProblem.pdf">using</a> conic sections in ways equivalent to <em>neusis</em> and further <a href="http://nyjm.albany.edu/j/2000/6-8.pdf">characterized</a> this millennium by Roger Alperin of San José State. The problem was apparently not shown to be outside <img alt="{\mathcal{C}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BC%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathcal{C}}"/> until a 1965 <a href="https://www.jstor.org/stable/27968003?refreqid=excelsior_b4edd5f21fda45f74dd56724556a7a87&amp;seq=1">paper</a> by Jack Elkin. </p>
<p>
After considering solvable cases like the above in his paper, Elkin ended by giving a single non-<img alt="{\mathcal{C}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BC%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathcal{C}}"/> case, analogous to <img alt="{60^\circ}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B60%5E%5Ccirc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{60^\circ}"/> for the trisection problem. His example did show the general algebraic ingredients. What Peter did in his 1998 <a href="https://www.ime.usp.br/~pleite/pub/artigos/neumann/reflections_on_reflection_on_spherical_mirror.pdf">paper</a>, <a href="https://mathshistory.st-andrews.ac.uk/Obituaries/Al-Haytham_Telegraph/">evidently</a> <a href="https://www-jstor-org.gate.lib.buffalo.edu/stable/3620392">unaware</a> of Elkin’s, was to characterize all the <img alt="{\mathcal{C}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BC%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathcal{C}}"/>-solvable cases and show that they form a set of measure zero. Thus, he gave a rigorous sense in which <em>almost all</em> instances of the reflection problem are not <img alt="{\mathcal{C}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BC%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathcal{C}}"/>-solvable. The algebra and lemmas about field extensions and irreducibility are especially pretty in Peter’s paper.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Our condolences go to Peter’s wife Sylvia, his family, and all who knew him.  This <a href="https://www.lms.ac.uk/news-entry/21122020-0842/dr-peter-neumann-1940-2020">notice</a> by the London Mathematical Society includes at the end an invitation to send reminiscences for a collection being made by Queen’s College.</p>
<p>
Discovering Alperin’s equivalences while writing this post leads me to re-pose this question from the “STOC 1500” <a href="https://rjlipton.wordpress.com/2014/05/23/stoc-1500/">post</a>: is there a single problem that is <img alt="{\mathcal{NC}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BNC%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathcal{NC}}"/>-complete under <img alt="{\mathcal{C}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathcal%7BC%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathcal{C}}"/>-reductions?</p>
<p/><p><br/>
[added link to LMS notice]</p></font></font></div>
    </content>
    <updated>2021-01-02T02:51:27Z</updated>
    <published>2021-01-02T02:51:27Z</published>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="News"/>
    <category term="People"/>
    <category term="Teaching"/>
    <category term="&quot;Ancient Complexity&quot;"/>
    <category term="algebra"/>
    <category term="Alhazen's problem"/>
    <category term="constructibility"/>
    <category term="Euclid"/>
    <category term="group theory"/>
    <category term="memorial"/>
    <category term="New Year's"/>
    <category term="Oxford"/>
    <category term="Peter Cameron"/>
    <category term="Peter Neumann"/>
    <category term="reflection problem"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2021-01-02T08:37:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2012.15381</id>
    <link href="http://arxiv.org/abs/2012.15381" rel="alternate" type="text/html"/>
    <title>Faster Distance-Based Representative Skyline and $k$-Center Along Pareto Front in the Plane</title>
    <feedworld_mtime>1609459200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cabello:Sergio.html">Sergio Cabello</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2012.15381">PDF</a><br/><b>Abstract: </b>We consider the problem of computing the \emph{distance-based representative
skyline} in the plane, a problem introduced by Tao, Ding, Lin and Pei [Proc.
25th IEEE International Conference on Data Engineering (ICDE), 2009] and
independently considered by Dupin, Nielsen and Talbi [Optimization and Learning
- Third International Conference, OLA 2020] in the context of multi-objective
optimization. Given a set $P$ of $n$ points in the plane and a parameter $k$,
the task is to select $k$ points of the skyline defined by $P$ (also known as
Pareto front for $P$) to minimize the maximum distance from the points of the
skyline to the selected points. We show that the problem can be solved in
$O(n\log h)$ time, where $h$ is the number of points in the skyline of $P$. We
also show that the decision problem can be solved in $O(n\log k)$ time and the
optimization problem can be solved in $O(n \log k + n \operatorname{loglog} n)$
time. This improves previous algorithms and is optimal for a large range of
values of $k$.
</p></div>
    </summary>
    <updated>2021-01-01T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-01-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2012.15347</id>
    <link href="http://arxiv.org/abs/2012.15347" rel="alternate" type="text/html"/>
    <title>Satisfiability problems on sums of Kripke frames</title>
    <feedworld_mtime>1609459200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Ilya B. Shapirovsky <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2012.15347">PDF</a><br/><b>Abstract: </b>We consider the operation of sum on Kripke frames, where a family of
frames-summands is indexed by elements of another frame. In many cases, the
modal logic of sums inherits the finite model property and decidability from
the modal logic of summands. In this paper we show that, under a general
condition, the satisfiability problem on sums is polynomial space Turing
reducible to the satisfiability problem on summands. In particular, for many
modal logics decidability in PSPACE is an immediate corollary from the semantic
characterization of the logic.
</p></div>
    </summary>
    <updated>2021-01-01T22:37:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-01-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2012.15279</id>
    <link href="http://arxiv.org/abs/2012.15279" rel="alternate" type="text/html"/>
    <title>Some Algorithms on Exact, Approximate and Error-Tolerant Graph Matching</title>
    <feedworld_mtime>1609459200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dwivedi:Shri_Prakash.html">Shri Prakash Dwivedi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2012.15279">PDF</a><br/><b>Abstract: </b>The graph is one of the most widely used mathematical structures in
engineering and science because of its representational power and inherent
ability to demonstrate the relationship between objects. The objective of this
work is to introduce the novel graph matching techniques using the
representational power of the graph and apply it to structural pattern
recognition applications. We present an extensive survey of various exact and
inexact graph matching techniques. Graph matching using the concept of
homeomorphism is presented. A category of graph matching algorithms is
presented, which reduces the graph size by removing the less important nodes
using some measure of relevance. We present an approach to error-tolerant graph
matching using node contraction where the given graph is transformed into
another graph by contracting smaller degree nodes. We use this scheme to extend
the notion of graph edit distance, which can be used as a trade-off between
execution time and accuracy. We describe an approach to graph matching by
utilizing the various node centrality information, which reduces the graph size
by removing a fraction of nodes from both graphs based on a given centrality
measure. The graph matching problem is inherently linked to the geometry and
topology of graphs. We introduce a novel approach to measure graph similarity
using geometric graphs. We define the vertex distance between two geometric
graphs using the position of their vertices and show it to be a metric over the
set of all graphs with vertices only. We define edge distance between two
graphs based on the angular orientation, length and position of the edges. Then
we combine the notion of vertex distance and edge distance to define the graph
distance between two geometric graphs and show it to be a metric. Finally, we
use the proposed graph similarity framework to perform exact and error-tolerant
graph matching.
</p></div>
    </summary>
    <updated>2021-01-01T22:40:19Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-01-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2012.15194</id>
    <link href="http://arxiv.org/abs/2012.15194" rel="alternate" type="text/html"/>
    <title>Test Score Algorithms for Budgeted Stochastic Utility Maximization</title>
    <feedworld_mtime>1609459200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lee:Dabeen.html">Dabeen Lee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vojnovic:Milan.html">Milan Vojnovic</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yun:Se=Young.html">Se-Young Yun</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2012.15194">PDF</a><br/><b>Abstract: </b>Motivated by recent developments in designing algorithms based on individual
item scores for solving utility maximization problems, we study the framework
of using test scores, defined as a statistic of observed individual item
performance data, for solving the budgeted stochastic utility maximization
problem. We extend an existing scoring mechanism, namely the replication test
scores, to incorporate heterogeneous item costs as well as item values. We show
that a natural greedy algorithm that selects items solely based on their
replication test scores outputs solutions within a constant factor of the
optimum for a broad class of utility functions. Our algorithms and
approximation guarantees assume that test scores are noisy estimates of certain
expected values with respect to marginal distributions of individual item
values, thus making our algorithms practical and extending previous work that
assumes noiseless estimates. Moreover, we show how our algorithm can be adapted
to the setting where items arrive in a streaming fashion while maintaining the
same approximation guarantee. We present numerical results, using synthetic
data and data sets from the Academia.StackExchange Q&amp;A forum, which show that
our test score algorithm can achieve competitiveness, and in some cases better
performance than a benchmark algorithm that requires access to a value oracle
to evaluate function values.
</p></div>
    </summary>
    <updated>2021-01-01T22:40:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-01-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2012.15056</id>
    <link href="http://arxiv.org/abs/2012.15056" rel="alternate" type="text/html"/>
    <title>Improved Approximation Algorithms for Weighted Edge Coloring of Graphs</title>
    <feedworld_mtime>1609459200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Debarsho Sannyasi <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2012.15056">PDF</a><br/><b>Abstract: </b>We study weighted edge coloring of graphs, where we are given an undirected
edge-weighted general multi-graph $G := (V, E)$ with weights $w : E \rightarrow
[0, 1]$. The goal is to find a proper weighted coloring of the edges with as
few colors as possible. An edge coloring is called a proper weighted coloring
if the sum of the weights of the edges incident to a vertex of any color is at
most one. In the online setting, the edges are revealed one by one and have to
be colored irrevocably as soon as they are revealed. We show that $3.39m+o(m)$
colors are enough when the maximum number of neighbors of a vertex over all the
vertices is $o(m)$ and where $m$ is the maximum over all vertices of the
minimum number of unit-sized bins needed to pack the weights of the incident
edges to that vertex. We also prove the tightness of our analysis. This
improves upon the previous best upper bound of $5m$ by Correa and Goemans [STOC
2004]. For the offline case, we show that for a simple graph with edge disjoint
cycles, $m+1$ colors are sufficient and for a multi-graph tree, we show that
$1.693m+12$ colors are sufficient.
</p></div>
    </summary>
    <updated>2021-01-01T22:40:16Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-01-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2012.15002</id>
    <link href="http://arxiv.org/abs/2012.15002" rel="alternate" type="text/html"/>
    <title>New Partitioning Techniques and Faster Algorithms for Approximate Interval Scheduling</title>
    <feedworld_mtime>1609459200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Compton:Spencer.html">Spencer Compton</a>, Slobodan Mitrović, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rubinfeld:Ronitt.html">Ronitt Rubinfeld</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2012.15002">PDF</a><br/><b>Abstract: </b>Interval scheduling is a basic problem in the theory of algorithms and a
classical task in combinatorial optimization. We develop a set of techniques
for partitioning and grouping jobs based on their starting and ending times,
that enable us to view an instance of interval scheduling on many jobs as a
union of multiple interval scheduling instances, each containing only a few
jobs. Instantiating these techniques in dynamic and local settings of
computation leads to several new results.
</p>
<p>For $(1+\varepsilon)$-approximation of job scheduling of $n$ jobs on a single
machine, we obtain a fully dynamic algorithm with
$O(\frac{\log{n}}{\varepsilon})$ update and $O(\log{n})$ query worst-case time.
Further, we design a local computation algorithm that uses only
$O(\frac{\log{n}}{\varepsilon})$ queries. Our techniques are also applicable in
a setting where jobs have rewards/weights. For this case we obtain a fully
dynamic algorithm whose worst-case update and query time has only polynomial
dependence on $1/\varepsilon$, which is an exponential improvement over the
result of Henzinger et al. [SoCG, 2020].
</p>
<p>We extend our approaches for unweighted interval scheduling on a single
machine to the setting with $M$ machines, while achieving the same
approximation factor and only $M$ times slower update time in the dynamic
setting. In addition, we provide a general framework for reducing the task of
interval scheduling on $M$ machines to that of interval scheduling on a single
machine. In the unweighted case this approach incurs a multiplicative
approximation factor $2 - 1/M$.
</p></div>
    </summary>
    <updated>2021-01-01T22:37:33Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-01-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2012.14632</id>
    <link href="http://arxiv.org/abs/2012.14632" rel="alternate" type="text/html"/>
    <title>Testing Product Distributions: A Closer Look</title>
    <feedworld_mtime>1609459200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Arnab Bhattacharyya, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gayen:Sutanu.html">Sutanu Gayen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kandasamy:Saravanan.html">Saravanan Kandasamy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Vinodchandran:N=_V=.html">N. V. Vinodchandran</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2012.14632">PDF</a><br/><b>Abstract: </b>We study the problems of identity and closeness testing of $n$-dimensional
product distributions. Prior works by Canonne, Diakonikolas, Kane and Stewart
(COLT 2017) and Daskalakis and Pan (COLT 2017) have established tight sample
complexity bounds for non-tolerant testing over a binary alphabet: given two
product distributions $P$ and $Q$ over a binary alphabet, distinguish between
the cases $P = Q$ and $d_{\mathrm{TV}}(P, Q) &gt; \epsilon$. We build on this
prior work to give a more comprehensive map of the complexity of testing of
product distributions by investigating tolerant testing with respect to several
natural distance measures and over an arbitrary alphabet. Our study gives a
fine-grained understanding of how the sample complexity of tolerant testing
varies with the distance measures for product distributions. In addition, we
also extend one of our upper bounds on product distributions to bounded-degree
Bayes nets.
</p></div>
    </summary>
    <updated>2021-01-01T22:41:02Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-01-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2012.14542</id>
    <link href="http://arxiv.org/abs/2012.14542" rel="alternate" type="text/html"/>
    <title>NBR: Neutralization Based Reclamation</title>
    <feedworld_mtime>1609459200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Singh:Ajay.html">Ajay Singh</a>, Trevor Brown, Ali Mashtizadeh <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2012.14542">PDF</a><br/><b>Abstract: </b>Safe memory reclamation (SMR) algorithms suffer from a trade-off between
bounding unreclaimed memory and the speed of reclamation. Hazard pointer (HP)
based algorithms bound unreclaimed memory at all times, but tend to be slower
than other approaches. Epoch based reclamation (EBR) algorithms are faster, but
do not bound memory reclamation. Other algorithms follow hybrid approaches,
requiring special compiler or hardware support, changes to record layouts,
and/or extensive code changes. Not all SMR algorithms can be used to reclaim
memory for all data structures.
</p>
<p>We propose a new neutralization based reclamation (NBR) algorithm that is
faster than the best known EBR algorithms and achieves bounded unreclaimed
memory. It is non-blocking when used with a non-blocking operating system (OS)
kernel, and only requires atomic read, write and CAS. NBR is straightforward to
use with many different data structures, and in most cases, require similar
reasoning and programmer effort to two-phased locking. NBR is implemented using
OS signals and a lightweight handshaking mechanism between participating
threads to determine when it is safe to reclaim a record. Experiments on a
lock-based binary search tree and a lazy linked list show that NBR
significantly outperforms many state of the art reclamation algorithms. In the
tree NBR is faster than next best algorithm, DEBRA by upto 38% and HP by upto
17%. And, in the list NBR is 15% and 243% faster than DEBRA and HP,
respectively.
</p></div>
    </summary>
    <updated>2021-01-01T22:40:03Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-01-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2012.14540</id>
    <link href="http://arxiv.org/abs/2012.14540" rel="alternate" type="text/html"/>
    <title>Source Identification for Mixtures of Product Distributions</title>
    <feedworld_mtime>1609459200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Spencer L. Gordon, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mazaheri:Bijan.html">Bijan Mazaheri</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rabani:Yuval.html">Yuval Rabani</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Schulman:Leonard_J=.html">Leonard J. Schulman</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2012.14540">PDF</a><br/><b>Abstract: </b>We give an algorithm for source identification of a mixture of $k$ product
distributions on $n$ bits. This is a fundamental problem in machine learning
with many applications. Our algorithm identifies the source parameters of an
identifiable mixture, given, as input, approximate values of multilinear
moments (derived, for instance, from a sufficiently large sample), using
$2^{O(k^2)} n^{O(k)}$ arithmetic operations. Our result is the first explicit
bound on the computational complexity of source identification of such
mixtures. The running time improves previous results by Feldman, O'Donnell, and
Servedio (FOCS 2005) and Chen and Moitra (STOC 2019) that guaranteed only
learning the mixture (without parametric identification of the source). Our
analysis gives a quantitative version of a qualitative characterization of
identifiable sources that is due to Tahmasebi, Motahari, and Maddah-Ali (ISIT
2018).
</p></div>
    </summary>
    <updated>2021-01-01T22:38:48Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-01-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2012.14512</id>
    <link href="http://arxiv.org/abs/2012.14512" rel="alternate" type="text/html"/>
    <title>No-substitution k-means Clustering with Adversarial Order</title>
    <feedworld_mtime>1609459200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bhattacharjee:Robi.html">Robi Bhattacharjee</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Moshkovitz:Michal.html">Michal Moshkovitz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2012.14512">PDF</a><br/><b>Abstract: </b>We investigate $k$-means clustering in the online no-substitution setting
when the input arrives in \emph{arbitrary} order. In this setting, points
arrive one after another, and the algorithm is required to instantly decide
whether to take the current point as a center before observing the next point.
Decisions are irrevocable. The goal is to minimize both the number of centers
and the $k$-means cost. Previous works in this setting assume that the input's
order is random, or that the input's aspect ratio is bounded. It is known that
if the order is arbitrary and there is no assumption on the input, then any
algorithm must take all points as centers. Moreover, assuming a bounded aspect
ratio is too restrictive -- it does not include natural input generated from
mixture models.
</p>
<p>We introduce a new complexity measure that quantifies the difficulty of
clustering a dataset arriving in arbitrary order. We design a new random
algorithm and prove that if applied on data with complexity $d$, the algorithm
takes $O(d\log(n) k\log(k))$ centers and is an $O(k^3)$-approximation. We also
prove that if the data is sampled from a ``natural" distribution, such as a
mixture of $k$ Gaussians, then the new complexity measure is equal to
$O(k^2\log(n))$. This implies that for data generated from those distributions,
our new algorithm takes only $\text{poly}(k\log(n))$ centers and is a
$\text{poly}(k)$-approximation. In terms of negative results, we prove that the
number of centers needed to achieve an $\alpha$-approximation is at least
$\Omega\left(\frac{d}{k\log(n\alpha)}\right)$.
</p></div>
    </summary>
    <updated>2021-01-01T22:41:38Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-01-01T01:30:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/12/31/year-end-linkage</id>
    <link href="https://11011110.github.io/blog/2020/12/31/year-end-linkage.html" rel="alternate" type="text/html"/>
    <title>Year-end linkage</title>
    <summary>A mathematician’s unanticipated journey through the physical world (\(\mathbb{M}\)). Quanta profiles Lauren Williams and discusses her work on enumerating cells of Grassmannians and its unexpected connections with intersection patterns of solitons.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://www.quantamagazine.org/a-mathematicians-adventure-through-the-physical-world-20201216/">A mathematician’s unanticipated journey through the physical world</a> (<a href="https://mathstodon.xyz/@11011110/105394664282719115">\(\mathbb{M}\)</a>). <em>Quanta</em> profiles <a href="https://en.wikipedia.org/wiki/Lauren_Williams">Lauren Williams</a> and discusses her work on enumerating cells of Grassmannians and its unexpected connections with intersection patterns of solitons.</p>
  </li>
  <li>
    <p><a href="https://www.insidehighered.com/news/2020/12/15/draconian-contract-proposals-connecticut">Connecticut state university system faculty contract negotiations go sour</a> (<a href="https://mathstodon.xyz/@11011110/105402517620055997">\(\mathbb{M}\)</a>). Proposals include increasing units taught per term, adding a term, halving units counted per hour of teaching, doubling required office hours, killing a cap on part-timers, adding required weekend teaching, eliminating faculty ownership of course content, eliminating funds for faculty travel and research, eliminating committee review of personnel actions, and monitoring emails for union activity.</p>
  </li>
  <li>
    <p><a href="https://github.com/patmorin/lhp">Pat Morin implements the product structure for planar graphs in Python</a> (<a href="https://mathstodon.xyz/@patmorin/105396573803578266">\(\mathbb{M}\)</a>). This is part of a recent line of research in which planar graphs can be decomposed as subgraphs of strong products of paths with bounded-treewidth graphs. Pat writes: “Not exactly industrial-strength, and leans towards simplicity over performance.  Still, it can  decompose 100k-vertex triangulations in a few seconds.”</p>
  </li>
  <li>
    <p><a href="http://make-origami.com/RonaGurkewitz/home.php">Modular origami polyhedra systems</a> (<a href="https://mathstodon.xyz/@11011110/105408938256987767">\(\mathbb{M}\)</a>). An old link by <a href="https://en.wikipedia.org/wiki/Rona_Gurkewitz">Rona Gurkewitz</a> from my Geometry Junkyard, moved to a new address.</p>
  </li>
  <li>
    <p><a href="https://diagonalargument.com/2020/12/08/socrates-bad-guy/">Socrates as anti-democratic enabler of tyrannical coups</a> (<a href="https://mathstodon.xyz/@11011110/105419425427517260">\(\mathbb{M}\)</a>). None of this analysis is particularly new, but it’s not the version of Socrates you’ll see when you look at <a href="https://en.wikipedia.org/wiki/Socrates">the Wikipedia article</a>.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Blichfeldt%27s_theorem">Blichfeldt’s theorem</a> (<a href="https://mathstodon.xyz/@11011110/105422739565106313">\(\mathbb{M}\)</a>): Any set in the plane of area greater than one can be translated to contain two integer points. New article on Wikipedia, connected to an expansion of the <a href="https://en.wikipedia.org/wiki/Hans_Frederick_Blichfeldt">biography  of Hans Blichfeldt</a>, who came to the US from Denmark as a teenager in 1888 and worked for several years as a lumberman, railway worker, and surveyor before his mathematical talent was recognized and he became one of the first students at Stanford.</p>
  </li>
  <li>
    <p>I’ve been experimenting with <a href="https://git-scm.com/docs/git-svn">git-svn</a> (<a href="https://mathstodon.xyz/@11011110/105426263611221446">\(\mathbb{M}\)</a>), for a new project whose coauthors chose svn. It somehow manages to be clunkier than either svn or git by themselves. I understand why svn’s linear history forces compromise, but mostly I tend to keep a linear history anyway. I just want git pull and push to work, but instead I have to learn new un-mnemonic commands (for which <a href="https://git.wiki.kernel.org/images-git/7/78/Git-svn-cheatsheet.pdf">the cheatsheet of equivalences from svn to git-svn</a> was helpful). The only advantage over svn that I found was you have a local copy of the history, although in the comments David Bremner suggests more.</p>
  </li>
  <li>
    <p><a href="https://www.latimes.com/business/story/2020-12-22/agree-to-disagree">“Why I never ‘agree to disagree’ — I just tell you when you’re wrong”</a> (<a href="https://mathstodon.xyz/@11011110/105431453312440767">\(\mathbb{M}\)</a>). <em>Los Angeles Times</em> columnist Michael Hiltzik on truth versus neutrality, and why it is incorrect and intellectually lazy for public media to treat certain firmly-established facts — such as the existence of the COVID pandemic, the outcome of the recent US election, or human-driven climate change — as topics on which debate is still reasonable.</p>
  </li>
  <li>
    <p><a href="https://www.atlasobscura.com/articles/best-christmas-cookie-cutter">The perfect Christmas cookie cutter: one that tessellates your cookie dough sheet with Christmas trees</a> (<a href="https://mathstodon.xyz/@11011110/105436497912189975">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://hardmath123.github.io/chaos-game-fractal-foliage.html">Using gradient descent to find Christmas-tree-shaped fractals</a> (<a href="https://mathstodon.xyz/@11011110/105439870724073086">\(\mathbb{M}\)</a>).</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Lars_Arge">Lars Arge</a> died of cancer on December 23 (<a href="https://mathstodon.xyz/@11011110/105447919719671152">\(\mathbb{M}\)</a>). You can read an <a href="https://cs.au.dk/news-events/news/show-news/artikel/in-memoriam-professor-lars-arge/">obituary by his department chair</a> and <a href="http://blog.geomblog.org/2020/12/lars-arge.html">another by Suresh Venkatasubramanian</a>. Lars was a leading researcher in algorithms for massive data, and an anchor for algorithms and computational geometry in Denmark. As Suresh writes, he was a larger-than-life figure; we’ll miss him.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Reinhardt_polygon">Reinhardt polygons</a> (<a href="https://mathstodon.xyz/@11011110/105454229077474608">\(\mathbb{M}\)</a>). These polygons have equal side lengths and are inscribed in Reuleaux polygons. Among all convex polygons with the same number of sides (any number that is not a power of two), they have the largest possible perimeter for their diameter, the largest possible width for their diameter, and the largest possible width for their perimeter. New Wikipedia article.</p>

    <p style="text-align: center;"><img alt="Reinhardt 15-gons" src="https://11011110.github.io/blog/assets/2020/Reinhardt_15-gons.svg" width="50%"/></p>
  </li>
  <li>
    <p><a href="https://cacm.acm.org/magazines/2021/1/249441-reboot-the-computing-research-publication-systems/fulltext">Moshe Vardi suggests taking advantage of this year’s disruption to the dysfunctional computer science conference publication system by setting up a replacement that is more scalable and doesn’t involve large amounts of carbon-expensive travel</a> (<a href="https://mathstodon.xyz/@11011110/105460878037520050">\(\mathbb{M}\)</a>). He doesn’t really say what this new system (or old journal system?) should be, though, only that we should design it.</p>

    <p>Relatedly, <a href="https://statmodeling.stat.columbia.edu/2020/12/23/update-on-ieees-refusal-to-issue-corrections/">IEEE relents on corrections to conference papers</a>.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@christianp/105465163493715434">What winter looks like in different parts of the world</a>. Long photo-thread responding to a request by Christian Lawson-Perfect’s 3-year-old. Here’s <a href="https://mathstodon.xyz/@11011110/105466532506211765">my contribution</a>, with prickly pears and the distant snow-topped San Gabriel mountains in Southern California:</p>

    <p style="text-align: center;"><img alt="Coastal prickly pear (opuntia littoralis) with visible fruit in the University of California, Irvine Ecological Preserve, looking north towards the San Gabriel Mountains, covered in snow from a recent storm" src="https://www.ics.uci.edu/~eppstein/pix/pricklypearmountains/PricklyPearMountains-m.jpg" style="border-style: solid; border-color: black;" width="80%"/></p>
  </li>
  <li>
    <p><a href="https://boingboing.net/2020/12/29/game-of-life-running-on-penrose-tiles.html">Game of Life running on Penrose tiles</a> (<a href="https://mathstodon.xyz/@11011110/105473798576179397">\(\mathbb{M}\)</a>). With links to a <em>New York Times</em> feature of “<a href="https://www.nytimes.com/2020/12/28/science/math-conway-game-of-life.html">short reflections from big thinkers on why Conway’s famous cellular-automata gewgaw remains so fascinating</a>”. From which I found Kjetil Golid’s generative-art <a href="https://generated.space/sketch/crosshatch-automata/">crosshatch automata</a>.</p>
  </li>
  <li>
    <p>Two analyses of citation vs other impact in mathematics (<a href="https://mathstodon.xyz/@11011110/105477934629467245">\(\mathbb{M}\)</a>):</p>

    <ul>
      <li>
        <p>In “<a href="https://www.ams.org/journals/notices/202101/rnoti-p114.pdf">Don’t count on it</a>” in the <em>Notices</em>, Edward Dunne compares highly cited mathematicians to winners of multiple prizes; high citations clustered in few topics, while prize winners were widely distributed across research areas.</p>
      </li>
      <li>
        <p>In “<a href="https://arxiv.org/abs/2005.05389">Citations versus expert opinions</a>” (arXiv:2005.05389, <a href="https://retractionwatch.com/2020/12/19/weekend-reads-prof-sues-journal-school-after-demotion-following-retraction-researcher-fired-after-questioning-why-school-rejected-grant-the-authors-who-like-publish-or-perish/">via retractionwatch</a>), Smolinsky et al compare highly cited papers to MathSciNet featured reviews, again finding little overlap.</p>
      </li>
    </ul>
  </li>
</ul></div>
    </content>
    <updated>2020-12-31T21:32:00Z</updated>
    <published>2020-12-31T21:32:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-01-01T06:02:43Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-373991171790634666</id>
    <link href="https://blog.computationalcomplexity.org/feeds/373991171790634666/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/12/complexity-year-in-review-2020.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/373991171790634666" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/373991171790634666" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/12/complexity-year-in-review-2020.html" rel="alternate" type="text/html"/>
    <title>Complexity Year in Review 2020</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div>For the result of the year we go to all the way back to the "before times".</div><blockquote><div><a href="https://arxiv.org/abs/2001.04383">MIP*=RE</a> by Zhengfeng Ji, Anand Natarajan, Thomas Vidick, John Wright and Henry Yuen</div></blockquote><div>A follow up to last year's runner up <a href="https://arxiv.org/abs/1904.05870">NEEXP in MIP*</a>, the new paper shows how to prove everything computable and beyond to a polytime verifier with two provers who cannot communicate but share entangled quantum bits. The result had a bit of a scare but <a href="https://blog.computationalcomplexity.org/2020/10/mip-re-redux.html">fixed up this fall</a>.</div><div><br/></div><div>Honorary mention goes to <a href="https://arxiv.org/abs/2007.01409">beating the Christofides-Serdyukov approximation</a> for traveling salesman by Anna R. Karlin, Nathan Klein and Shayan Oveis Gharan. Nathan got his start in Bill's REU program.</div><div><br/></div><div>Of course when we think back at 2020 it won't be the theorems but the pandemic, a divisive election and racial reckoning. The word of the year for academia is "virtual", virtual teaching, virtual research collaborations, virtual talks, virtual conferences, virtual panels and virtual job visits. For the most part the the pandemic hasn't really created new trends but accelerated trends already in place. Given the far lower costs in terms of time, money and the environment, how much of "virtual" will remain post-corona?</div><div><br/></div><div>Bill's published a new book on <a href="https://amzn.to/3avH9J9">muffin problems</a>, his second book in two years. I started a new <a href="https://www.iit.edu/news/illinois-tech-creates-college-computing-fuel-chicagos-tech-rise">College of Computing</a>. </div><div><br/></div><div>We remember <a href="http://blog.geomblog.org/2020/12/lars-arge.html">Lars Arge</a>, <a href="https://www.scs.cmu.edu/news/edmund-clarke-pioneered-methods-detecting-software-hardware-errors">Ed Clarke</a>, <a href="https://blog.computationalcomplexity.org/2020/04/john-conway-dies-of-coronvirus.html">John Conway</a>, <a href="https://blog.computationalcomplexity.org/2020/05/obit-for-richard-dudley.html">Richard Dudley</a>, <a href="https://cpsc.yale.edu/news/memoriam-stanley-c-eisenstat-1944-2020">Stanley Eisenstat</a>, <a href="https://blog.computationalcomplexity.org/2020/07/reflections-on-ronald-graham-by-steve.html">Ron Graham</a>, <a href="https://blog.computationalcomplexity.org/2020/03/richard-guy-passed-away-at-age-of-103.html">Richard Guy</a>, <a href="https://www.nytimes.com/2020/02/24/science/katherine-johnson-dead.html">Katherine Johnson</a>, <a href="https://blog.computationalcomplexity.org/2020/11/vaughan-jones-and-kaikoura.html">Vaughn Jones</a>, <a href="https://blog.computationalcomplexity.org/2020/11/james-randi-magicians-author-skeptic.html">James Randi</a>, <a href="https://twitter.com/betanalpha/status/1343770446777495552">Arianna Rosenbluth</a>, <a href="https://www.indiatechonline.com/it-happened-in-india.php?id=3995">Joy Thomas</a>, <a href="https://blog.computationalcomplexity.org/2020/03/robin-thomas.html">Robin Thomas</a>, <a href="https://blog.computationalcomplexity.org/2020/11/alex-trebekwhat-is-todays-post-about.html">Alex Trebek</a>, <a href="https://gilkalai.wordpress.com/2020/04/03/trees-not-cubes-memories-of-boris-tsirelson/">Boris Tsirelson</a> and <a href="https://blog.computationalcomplexity.org/2020/03/theorist-paul-r-young-passed-away.html">Paul Young</a>. </div><div><div><br/></div><div>Thanks to our guest posters <a href="https://blog.computationalcomplexity.org/2020/12/dr-jill-biden.html">Gorjan Alagic, Andrew Childs, Tom Goldstein, Daniel Gottsman, Clyde Kruskal and Jon Katz</a>, <a href="https://blog.computationalcomplexity.org/2020/04/theoretical-computer-science-for-future.html">Antoine Amarilli, Thomas Colcombet, Hugo Férée and Thomas Schwentick</a>, <a href="https://blog.computationalcomplexity.org/2020/07/reflections-on-ronald-graham-by-steve.html">Steve Butler</a>, <a href="https://blog.computationalcomplexity.org/2020/06/on-chain-letters-and-pandemics.html">Varsha Dani</a>, <a href="https://blog.computationalcomplexity.org/2020/02/pre-publish-and-perish.html">Evangelos Georgiadis</a> and <a href="https://blog.computationalcomplexity.org/2020/04/a-guest-blog-on-pandemics-affect-on.html">Emily Kaplitz</a>.</div></div><div><br/></div><div>In 2021 we will likely see the end of the pandemic and most definitely see the 50th anniversary of P versus NP. Hope for a successful and healthy year for all. </div></div>
    </content>
    <updated>2020-12-31T12:24:00Z</updated>
    <published>2020-12-31T12:24:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-01-01T08:23:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5224</id>
    <link href="https://www.scottaaronson.com/blog/?p=5224" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5224#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5224" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">My vaccine crackpottery: a confession</title>
    <summary xml:lang="en-US">I hope everyone is enjoying a New Years’ as festive as the circumstances allow! I’ve heard from a bunch of you awaiting my next post on the continuum hypothesis, and it’s a-comin’, but I confess the new, faster-spreading covid variant is giving me the same sinking feeling that Covid 1.0 gave me in late February, […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>I hope everyone is enjoying a New Years’ as festive as the circumstances allow!</p>



<p>I’ve heard from a bunch of you awaiting my next post on the continuum hypothesis, and it’s a-comin’, but I confess the new, faster-spreading covid variant is giving me the same sinking feeling that Covid 1.0 gave me in late February, making it <em>really</em> hard to think about the eternal.  (For perspectives on Covid 2.0 from individuals who acquitted themselves well with their early warnings about Covid 1.0, see for example <a href="https://putanumonit.com/2020/12/26/seeing-the-new-covid-variant-smoke/">this by Jacob Falkovich</a>, or <a href="https://www.lesswrong.com/posts/CHtwDXy63BsLkQx4n/covid-12-24-we-re-f-ed-it-s-over">this by Zvi Mowshowitz</a>.)</p>



<p>So on that note: do you hold any opinions, on factual matters of practical importance, that most everyone around you sharply disagrees with?  Opinions that those who you respect consider ignorant, naïve, imprudent, and well outside your sphere of expertise?  Opinions that, nevertheless, you simply continue to hold, because you’ve learned that, <em>unless and until</em> someone shows you the light, you can no more will yourself to change what you think about the matter than change your blood type?</p>



<p>I try to have as few such opinions as possible.  Having run <em>Shtetl-Optimized</em> for fifteen years, I’m acutely aware of the success rate of those autodidacts who think they’ve solved P versus NP or quantum gravity or whatever.  It’s basically zero out of hundreds—and why wouldn’t it be?</p>



<p>And yet there’s one issue where I feel myself in the unhappy epistemic situation of those amateurs, spamming the professors in all-caps.  So, OK, here it is:</p>



<p><strong>I think that, in a well-run civilization, the first covid vaccines would’ve been tested and approved by around March or April 2020, while mass-manufacturing simultaneously ramped up with <em>trillions</em> of dollars’ investment.  I think almost everyone on earth could have, and should have, already been vaccinated by now.  I think a faster, “WWII-style” approach would’ve saved millions of lives, prevented economic destruction, and carried negligible risks compared to its benefits.  I think this will be clear to future generations, who’ll write PhD theses exploring how it was possible that we invented multiple effective covid vaccines in mere days or weeks, but then simply <a href="https://nymag.com/intelligencer/2020/12/moderna-covid-19-vaccine-design.html">sat on those vaccines for a year</a>, ticking off boxes called “Phase I,” “Phase II,” etc. while civilization hung in the balance.</strong></p>



<p>I’ve said similar things, on this blog and elsewhere, since the beginning of the pandemic, but part of me kept expecting events to teach me why I was wrong.  Instead events—including the staggering cost of delay, the spectacular failures of institutional authorities to adapt to the scientific realities of covid, and the long-awaited finding that <em>all</em> the major vaccines safely work (some better than others), just like the experts predicted back in February—all this only made me more confident of my original, stupid and naïve position.</p>



<p>I’m saying all this—clearly enough that no one will misunderstand—but I’m also scared to say it.  I’m scared because it sounds too much like colossal <em>ingratitude</em>, like Monday-morning quarterbacking of one of the great heroic achievements of our era by someone who played no part in it.</p>



<p>Let’s be clear: the ~11 months that it took to get from sequencing the novel coronavirus, to approving and mass-manufacturing vaccines, is a world record, soundly beating the previous record of 4 years.  Nobel Prizes and billions of dollars are the <em>least</em> that those who made it happen deserve.  Eternal praise is especially due to those like <a href="https://www.wired.co.uk/article/mrna-coronavirus-vaccine-pfizer-biontech?fbclid=IwAR0uTyPvEyhlia7tvlH3XFx8kVrCyx0A8IIy8ALVSrUJ8m7jV4b8E7mf-Ec">Katalin Karikó</a>, who risked their careers in the decades before covid to do the basic research on mRNA delivery that made the development of <em>these</em> mRNA vaccines so blindingly fast.</p>



<p>Furthermore, I could easily believe that there’s no one agent—neither Pfizer nor BioNTech nor Moderna, neither the CDC nor FDA nor other health or regulatory agencies, neither Bill Gates nor Moncef Slaoui—who could’ve unilaterally sped things up very much.  If one of them tried, they would’ve simply been ostracized by the other parts of the system, and they probably all understood that.  It might have taken a whole different civilization, with different attitudes about utility and risk.</p>



<p>And yet the fact remains that, historic though it was, a one-to-two-year turnaround time <em>wasn’t nearly good enough</em>.  Especially once we factor in the faster-spreading variant, by the time we’ve vaccinated everyone, we’ll already be a large fraction of the way to herd immunity and to the vaccine losing its purpose.  For all the advances in civilization, from believing in demonic spirits all the way to <a href="https://berthub.eu/articles/posts/reverse-engineering-source-code-of-the-biontech-pfizer-vaccine/">understanding mRNA at a machine-code level of detail</a>, covid is running wild much like it would have back in the Middle Ages—partly, yes, because modern transportation helps it spread, but partly also because our political and regulatory and public-health tools have lagged so breathtakingly behind our knowledge of molecular biology.</p>



<p>What could’ve been done faster?  For starters, as I said back in March, we could’ve had <a href="https://www.1daysooner.org/">human challenge trials</a> with willing volunteers, of whom there were tens of thousands.  We could’ve started mass-manufacturing months earlier, with funding commensurate with the problem’s scale (think trillions, not billions).  Today, we could give as many people as possible the first doses (which apparently already provide something like ~80% protection) before circling back to give the second doses (which boost the protection as high as ~95%).  We could <em>distribute the vaccines that are now <a href="https://www.washingtonpost.com/health/2020/12/30/covid-vaccine-delay/">sitting in warehouses, spoiling</a>, while people in the distribution chain take off for the holidays</em>—but that’s such low-hanging fruit that it feels unsporting even to mention it.</p>



<p>Let me now respond to three counterarguments that would surely come up in the comments if I didn’t address them.</p>



<ol><li><strong>The Argument from Actual Risk.</strong>  Every time this subject arises, someone patiently explains to me that, since a vaccine gets administered to billions of <em>healthy</em> people, the standards for its safety and efficacy need to be even higher than they are for ordinary medicines.  Of course that’s true, and it strikes me as an excellent reason not to inject people with a <em>completely</em> untested vaccine!  All I ask is that the people who are, or could be, harmed by a faulty vaccine, be weighed on the same moral scale as the people harmed by covid itself.  As an example, we know that the Phase III clinical trials were repeatedly halted for days or weeks because of a single participant developing strange symptoms—often a participant who’d received the placebo rather than the actual vaccine!  That person matters.  Any future vaccine recipient who might develop similar symptoms matters.  But <em>the 10,000 people who die of covid every single day we delay</em>, along with the hundreds of millions more impoverished, kept out of school, etc., matter equally.  If we threw them all onto the same utilitarian scale, would we be making the same tradeoffs that we are now?  I feel like the question answers itself.<br/></li><li><strong>The Argument from Perceived Risk.</strong>  Even <em>with</em> all the testing that’s been done, somewhere between 16% and 40% of Americans (depending on which poll you believe) say that they’ll refuse to get a covid vaccine, often because of anti-vaxx conspiracy theories.  How much higher would the percentage be had the vaccines been rushed out in a month or two?  And of course, if not enough people get vaccinated, then R<sub>0</sub> remains above 1 and the public-health campaign is a failure.  In this way of thinking, we need three phases of clinical trials the same way we need everyone to take off their shoes at airport security: it might not prevent a single terrorist, but the masses will be too scared to get on the planes if we don’t.  To me, this (if true) only underscores my broader point, that the year-long delay in getting vaccines out represents a failure of <em>our entire civilization</em>, rather than a failure of any one agent.  But also: people’s membership in the pro- or anti-vaxx camps is not static.  The percentage saying they’ll get a covid vaccine seems to have <em>already</em> gone up, as a formerly abstract question becomes a stark choice between wallowing in delusions and getting a deadly disease, or accepting reality and not getting it.  So while the Phase III trials were still underway—when the vaccines were already known to be safe, and experts thought it much more likely than not that they’d work—would it have been such a disaster to let Pfizer and Moderna sell the vaccines, for a hefty profit, <em>to those who wanted them</em>?  With the hope that, just like with the iPhone or any other successful consumer product, satisfied early adopters would inspire the more reticent to get in line too?<br/></li><li><strong>The Argument from Trump.</strong>  Now for the most awkward counterargument, which I’d like to address head-on rather than dodge.  If the vaccines had been approved faster in the US, it would’ve <em>looked</em> to many like Trump deserved credit for it, and he might well have been reelected.  And devastating though covid has been, Trump is plausibly worse!  Here’s my response: Trump has the mentality of a toddler, albeit with curiosity swapped out for cruelty and vindictiveness.  His and his cronies’ impulsivity, self-centeredness, and incompetence are likely responsible for at least ~200,000 of the 330,000 Americans now dead from covid.  But, yes, reversing his previous anti-vaxx stance, Trump <em>did</em> say that he wanted to see a covid vaccine in months, just like I’ve said.  Does it make me uncomfortable to have America’s worst president in my “camp”?  Only a little, because I have no problem admitting that sometimes toddlers are right and experts are wrong.  The solution, I’d say, is <em>not</em> to put toddlers in charge of the government!  As should be obvious by now—indeed, as should’ve been obvious back in 2016—that solution has some exceedingly severe downsides.  The solution, rather, is to work for a world where experts are unafraid to speak bluntly, so that it never <em>falls</em> to a mental toddler to say what the experts can’t say without jeopardizing their careers.</li></ol>



<p>Anyway, despite everything I’ve written, <a href="https://www.scottaaronson.com/blog/?p=2410">considerations of Aumann’s Agreement Theorem</a> still lead me to believe there’s an excellent chance that I’m wrong, and the vaccines couldn’t realistically have been rolled out any faster.  The trouble is, I don’t understand <em>why</em>.  And I don’t understand why compressing this process, from a year or two to at most a month or two, shouldn’t be civilization’s most urgent priority ahead of the <em>next</em> pandemic.  So go ahead, explain it to me!  I’ll be eternally grateful to whoever makes me retract this post in shame.</p>



<p><strong><span class="has-inline-color has-vivid-red-color">Update (Jan. 1, 2021):</span></strong> If you want a sense of the on-the-ground realities of administering the vaccine in the US, check out <a href="https://thezvi.wordpress.com/2020/12/31/covid-12-31-meet-the-new-year/">this long post by Zvi Mowshowitz</a>.  Briefly, it looks like in my post, I gave those in charge <em>way too much benefit of the doubt</em> <em>(!!)</em>.  The Trump administration pledged to administer 20 million vaccines by the end of 2020; instead it administered fewer than 3 million.  Crucially, this is not because of any problem with manufacturing or supply, but just because of pure bureaucratic blank-facedness.  Incredibly, even as the pandemic rages, most of the vaccines are sitting in storage, at severe risk of spoiling … and officials’ primary concern is not to administer the precious doses, but just to make sure no one gets a dose “out of turn.”  In contrast to Israel, where they’re now administering vaccines 24/7, including on Shabbat, with the goal being to get through the entire population as quickly as possible, in the US they’re moving at a snail’s pace and took off for the holidays.  In Wisconsin, a pharmacist intentionally spoiled hundreds of doses; in West Virginia, they mistakenly gave antibody treatments instead of vaccines.  There are no longer any terms to understand what’s happening other than those of black comedy.</p></div>
    </content>
    <updated>2020-12-31T09:16:32Z</updated>
    <published>2020-12-31T09:16:32Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Embarrassing Myself"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-01-02T02:18:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://adamsheffer.wordpress.com/?p=5584</id>
    <link href="https://adamsheffer.wordpress.com/2020/12/30/19th-century-precursors-of-polynomial-methods/" rel="alternate" type="text/html"/>
    <title>19th Century Precursors of Polynomial Methods</title>
    <summary>These days, I’m spending a lot of time revising my book about polynomial methods and incidences (an older draft is available here). This led me to the following conclusion: I am not sure what classifies a proof as a polynomial method. A Wikipedia page states “… the polynomial method is an algebraic approach to combinatorics problems that involves […]</summary>
    <updated>2020-12-30T21:05:54Z</updated>
    <published>2020-12-30T21:05:54Z</published>
    <category term="Elegant DG arguments"/>
    <category term="Incidences"/>
    <author>
      <name>Adam Sheffer</name>
    </author>
    <source>
      <id>https://adamsheffer.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://adamsheffer.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://adamsheffer.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://adamsheffer.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://adamsheffer.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Discrete geometry and other typos</subtitle>
      <title>Some Plane Truths</title>
      <updated>2021-01-02T08:38:12Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/193</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/193" rel="alternate" type="text/html"/>
    <title>TR20-193 |  Average-case rigidity lower bounds | 

	Xuangui Huang, 

	Emanuele Viola</title>
    <summary>It is shown that there exists $f : \{0,1\}^{n/2} \times \{0,1\}^{n/2} \to \{0,1\}$ in E$^\mathbf{NP}$ such that for every $2^{n/2} \times 2^{n/2}$ matrix $M$ of rank $\le \rho$ we have $\P_{x,y}[f(x,y)\ne M_{x,y}] \ge 1/2-2^{-\Omega(k)}$, where $k \leq \Theta(\sqrt{n})$ and $\log \rho \leq \delta n/k(\log n + k)$ for a sufficiently small $\delta &gt; 0$.  
This generalizes recent results which bound below the probability by $1/2-\Omega(1)$ or apply to constant-depth circuits.
The result is a step towards obtaining data-structure lower bounds for E$^\mathbf{NP}$: they would follow from a better trade-off between the probability bound and $\rho$.</summary>
    <updated>2020-12-29T16:17:22Z</updated>
    <published>2020-12-29T16:17:22Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-01-02T08:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4452</id>
    <link href="https://lucatrevisan.wordpress.com/2020/12/29/an-unusual-year-in-pictures/" rel="alternate" type="text/html"/>
    <title>An Unusual Year, in Pictures</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Some memories from 2020. When the year began I was in Hong Kong. I got to see the tail end of the latest round of pro-democracy and pro-freedom protests, which had started several months earlier in response to a proposed … <a href="https://lucatrevisan.wordpress.com/2020/12/29/an-unusual-year-in-pictures/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Some memories from 2020.</p>



<p>When the year began I was in Hong Kong.</p>



<p>I got to see the tail end of the latest round of pro-democracy and pro-freedom protests, which had started several months earlier in response to a proposed new extradition law. The proposal ignited protests because many people saw the point of the law as allowing the PRC to bring trumped-up charges against pro-democracy Hong Kongers, and then request their extradition, thus avoiding the extrajudicial kidnappings that had been the primary way of bringing dissidents to the mainland. (In June 2020, the PRC sidestepped the issue by throwing away whatever was left of the handover agreements, and passing its own anti-sedition law and imposing it on Hong Kong, making it possible to jail dissenters directly in Hong Kong.)</p>



<p>On January 1, I went to one of the big demonstrations, in Victoria Park, and saw Joshua Wong, the pro-democracy leader who is currently serving a jail term on the basis of the June 2020 laws.</p>



<figure class="wp-block-embed is-type-rich is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">

</div></figure>



<p>In the video below. the audio is not clear, but people are chanting “five demands, not one less” and “fight for freedom, stand with Hong Kong”. The five demands were to drop the extradition law, institute universal suffrage in elections, and the other three demands related to investigating and punishing police abuses against protesters.</p>



<figure class="wp-block-embed is-type-rich is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">

</div></figure>



<p>In those days, I was reading English-language Hong-Kong press to keep up to date on protests that could cause the subway to shut down, and I noticed some reporting on a cluster of pneumonia cases in Wuhan. Since the time of SARS, Hong Kongers have been quite paranoid about new respiratory diseases coming from the mainland, but the reports were that no human-to-human transmission had been confirmed. (Speaking of Hong Kong press, the publisher of the Apple Daily newspaper is now in jail on the basis of the June 2020 legislation, because of his pro-democracy position.)</p>



<p>The reason I remember this is that on January 2 I came down with a fever and a cough. On my flight back, several days later, I coughed for the whole flight, without a face mask. Those being more innocent times, nobody seemed to mind.</p>



<p>Between January 31 and February 3 I was in London for an event organized by Bocconi. The evening of January 31 happened to be the moment Brexit went into effect, after the negotiations had blown past several deadlines, and after being pushed back several times. As it happened, negotiations continued for the rest of the year, and were not resolved until a few days ago. Although Brexit was on everyone’s mind, there was concern about the novel Coronavirus that had been isolated in Wuhan, which had proved to transmit person-to-person, and that had led to a health emergency and a severe lockdown of the city of Wuhan.</p>



<div class="wp-block-media-text alignwide is-stacked-on-mobile"><figure class="wp-block-media-text__media"><img alt="" class="wp-image-4462 size-full" src="https://lucatrevisan.files.wordpress.com/2020/12/img_9314.png?w=1024"/></figure><div class="wp-block-media-text__content">
<p class="has-large-font-size"/>
</div></div>



<p>(Photo taken in London, Feb 2, 2020)</p>



<p>Back in Milan, I was looking forward to a Spring semester in which I was not teaching, and to the plans to take several trips and to host a number of academic guests.</p>



<p>Meanwhile, the Italian government had established a protocol according to which Covid19 testing was restricted to people who had had contact with a person known to suffer from Covid19 or who had recently traveled to China. Since nobody in Italy was known to suffer from Covid19, there was a bit of a chicken-and-egg problem going on, even as the virus (as became clear in retrospect) was spreading widely in Northern Italy. </p>



<p>Eventually, a person with Covid19 symptoms reported to have had dinner with a friend who had been to China. That person was tested, and, while he was already in intensive care, he became the first confirmed case of local transmission, on February 21. It then became clear that the friend who had been to China had never been infected, and that there must have already been a number of local infections. This was in the middle of Milan Fashion Week, which had already been scaled down due to concern about international travel. It was going to be the last major public event to take place in Milan for a while.</p>



<p>The following week, the Italian government settled on its response strategy: take some measures, back down after concern for the economic consequences, then double down when the situation gets worse. On March 1 I traveled to Rome in a mostly empty train. While a measure of panic was starting to gather in Milan (where it had become impossible to buy face masks and there were some shortages of other supplies in supermarkets), Romans were still mostly in denial. Tourism, however, had died down completely, and the city center was empty as I had never seen it.</p>



<div class="wp-block-media-text alignwide is-stacked-on-mobile"><figure class="wp-block-media-text__media"><img alt="" class="wp-image-4466 size-full" src="https://lucatrevisan.files.wordpress.com/2020/12/img_9367-1.png?w=1024"/></figure><div class="wp-block-media-text__content">
<p class="has-large-font-size"/>
</div></div>



<p>(Piazza di Spagna and Via dei Condotti seen from Trinità dei Monti on March 1, 2020. I had never seen Piazza di Spagna empty of people ever before).</p>



<p>The following week (see above on government strategy), the initial measures that had closed bars and restaurants in Milan were relaxed, and bars could open but could only do table service. </p>



<div class="wp-block-media-text alignwide is-stacked-on-mobile"><figure class="wp-block-media-text__media"><img alt="" class="wp-image-4468 size-full" src="https://lucatrevisan.files.wordpress.com/2020/12/img_9383.png?w=768"/></figure><div class="wp-block-media-text__content">
<p class="has-large-font-size"/>
</div></div>



<p>(A bar in the Navigli district of Milan on March 7, 2020. The counter area was roped-off, and they only provided table service.)</p>



<p>On the night of March 7, as I was having my sit-down drink with a friend, I started receiving text messages saying that the prime minister was about to speak on TV and that there were rumors that the government would lock down Northern Italy. As people literally ran to the train station to catch the last train out of Milan, the press conference was delayed until late at night, and he did announce a lockdown of Northern Italy, which would be extended to the whole country a few days later.</p>



<p>After that, time is a blur. I read a very interesting article on this topic (but I cannot find it again now), whose point was that when nothing interesting happens, time seems to stretch, and the days feel long and empty. But because nothing interesting happens, we do not form new long-term memory, so later it feels like that time went by very quickly. This warped perception is part of the sense of dislocation that some of us felt during the lockdown. </p>



<p>I looked at my pictures from those months for a clue as to what happened, and it’s basically pictures of things that I cooked and of the unfortunate results of cutting my hair with a beard trimmer. The lockdown was extremely strict until May, banning even taking a walk outside alone. In May we could again walk outside, but the city felt eery and empty.</p>



<div class="wp-block-media-text alignwide is-stacked-on-mobile"><figure class="wp-block-media-text__media"><img alt="" class="wp-image-4471 size-full" src="https://lucatrevisan.files.wordpress.com/2020/12/img_9581.png?w=1024"/></figure><div class="wp-block-media-text__content">
<p class="has-large-font-size"/>
</div></div>



<p>(The Italian stock exchange in Piazza Affari, Milan, on May 10, 2020. <a href="https://en.wikipedia.org/wiki/Maurizio_Cattelan">Maurizio Cattelan</a>‘s iconic sculpture is visible in the foreground.)</p>



<p>During the summer, Covid19 cases, and especially Covid19 deaths, dropped considerably, and most business were allowed to reopen. Movie theaters, concert halls, stadiums, conference centers, and other venues where large numbers of people congregate remained closed. Dance clubs, however, reopened, and schools reopened in September.</p>



<p>By mid-October, numbers were about half the thresholds that were considered alarming. There were more than a thousand Covid19 patients in intensive care, for example, and two thousand was considered the threshold at which there would be a shortage of ICU beds for other patients. Furthermore, the numbers were doubling roughly every ten days, and any new measures would take about two weeks to have any effect. I wasn’t teaching until the second week of November. I did the math and I moved to Rome. </p>



<p>By the end of October, Bocconi had moved almost all teaching online, and the government had instituted new measures, this time on a regional basis. Milan was in a “red” region, and got a lockdown almost as bad as the one in the Spring. Rome was in a “yellow” region and there was a bit more freedom: retail was open, and indoor dining was possible for lunch. </p>



<p>I went back to Milan just before Christmas, when there have been further restrictions to avoid the large gatherings that are common during the Christmas holidays. They might have actually overshot a bit with the restrictions.</p>



<figure class="wp-block-embed is-type-rich is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">

</div></figure>



<p>(This is Piazza Duomo in Milan, in the early evening of December 26, 2020. The emptiness and the tinny Christmas music made it feel like the setting of a horror movie.)</p>



<p>The day after I shot the above video, the European vaccine campaign got started. In July 2020, I was supposed to travel to Taipei. While all the other international events I had planned to attend in 2020 were canceled, the even in Taipei was moved to July 2021. I am looking ahead at what surely be another difficult Winter and Spring, but I am holding out hope to be in Taiwan in July and in <a href="https://simons.berkeley.edu/workshops/si2021-3">Berkeley in November</a>.</p>



<p>Best wishes to all readers, and may 2021 be a much less interesting year than the current one.</p></div>
    </content>
    <updated>2020-12-29T13:38:27Z</updated>
    <published>2020-12-29T13:38:27Z</published>
    <category term="Hong Kong"/>
    <category term="Milan"/>
    <category term="2020"/>
    <category term="things that are terrible"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2021-01-02T08:37:10Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=20635</id>
    <link href="https://gilkalai.wordpress.com/2020/12/29/the-argument-against-quantum-computers-a-very-short-introduction/" rel="alternate" type="text/html"/>
    <title>The Argument Against Quantum Computers – A Very Short Introduction</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Left: Gowers’s book Mathematics a very short introduction. Right C. elegans; Boson Sampling can be seen as the C. elegans of quantum computing. (See, this paper.) Happy new year to all my readers. Today, I will briefly explain the main … <a href="https://gilkalai.wordpress.com/2020/12/29/the-argument-against-quantum-computers-a-very-short-introduction/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h3><a href="https://gilkalai.files.wordpress.com/2020/12/4post.png"><img alt="" class="alignnone size-full wp-image-20648" height="327" src="https://gilkalai.files.wordpress.com/2020/12/4post.png" width="613"/></a></h3>
<p><span style="color: #ff0000;">Left: Gowers’s book Mathematics a very short introduction. Right <em>C. elegans; </em>Boson Sampling can be seen as the <em>C. elegans</em> of quantum computing. (See, <a href="https://arxiv.org/abs/1908.02499">this paper</a>.)</span></p>
<h3><span style="color: #0000ff;">Happy new year to all my readers.</span></h3>
<p>Today, I will briefly explain the main argument in my theory regarding quantum computers. In this post I will use the term HQCA (“Huge Quantum Computational Advantage”) instead of “quantum supremacy” that Google uses and “quantum advantage” that IBM and  Jiuzhang Boson Sampling group use. Before we move to the main topic of the post, here are two related updates.</p>
<h3>Boson Sampling update</h3>
<p>A group of researchers mainly from USTC in Hefei, China, claimed to achieve a fantastic quantum computational advantage using <strong><span style="color: #ff0000;">Jiuzhang,</span></strong> a photonic device for “Boson Sampling”.  It seems plausible now that level-<em>k</em> approximation from my paper <a href="https://arxiv.org/abs/1409.3093" rel="nofollow ugc">Gaussian Noise Sensitivity and BosonSampling</a> with Guy Kindler will “spoof” the claims, namely, will show an efficient way to give samples of similar fidelity on a digital computer. (There are other related approaches for spoofing the claim, mainly one by Jelmer Renema, and there are interesting connections between Jelmer’s approximations and ours. Sergio Boixo, John Martinis, and other researchers also raised concerns regarding the new HQCA claims.) While it will probably take several weeks or months to clarify it further, <strong>it is already clear, that in view of Kindler and mine 2014 results, the claims of achieving in 200 seconds samples that require millions of computing years on current supercomputers are unfounded.</strong></p>
<p>For more details  see <a href="https://gilkalai.wordpress.com/2020/12/06/photonic-huge-quantum-advantage/">my post</a> about the recent HQCA Boson Sampling experiment, and also Scott Aaronson’s <a href="https://www.scottaaronson.com/blog/?p=5159">recent post</a>.</p>
<h3>Scott Aaronson’s apology</h3>
<p>Let me mention that Scott Aaronson’s <a href="https://www.scottaaronson.com/blog/?p=5159">recent post</a> issued the following apology addressed to me, which I gladly welcome:</p>
<h3><a href="https://gilkalai.files.wordpress.com/2020/12/scott-aaronson.jpg"><img alt="" class="alignnone size-full wp-image-20715" height="360" src="https://gilkalai.files.wordpress.com/2020/12/scott-aaronson.jpg" width="640"/></a></h3>
<h2>The argument against quantum computers: a very short introduction</h2>
<p>Our starting point is the following <a href="https://www.scottaaronson.com/blog/?p=5122">recent assertion</a> by Scott Aaronson:</p>
<blockquote>
<p><span style="color: #0000ff;"><em>Many colleagues told me they thought experimental BosonSampling was a dead end, because of photon losses and the staggering difficulty of synchronizing 50-100 single-photon sources. They said that a convincing demonstration of quantum supremacy would have to await the arrival of quantum fault-tolerance—or at any rate, some hardware platform more robust than photonics. I always agreed that they might be right. — </em></span></p>
</blockquote>
<p>My argument for why all attempts to reach HQCA as well as quantum error-correction via NISQ systems will fail  is based on two claims.  </p>
<p><span style="color: #800000;"><strong>A) “A convincing demonstration of HQCA would have to await the arrival of quantum fault-tolerance”</strong></span></p>
<p><span style="color: #800000;"><strong>

</strong></span></p>
<p><span style="color: #800000;"><strong>B) The error rate required for quantum error correcting codes needed for quantum fault tolerance is even lower than that required for HQCA.</strong></span></p>
<p>If claims A) and B) are <span style="color: #800000;"><strong>both</strong></span> correct, then it is not possible to reach HQCA as well as good quality quantum error-correction via NISQ systems. In this case, all attempts to reach HQCA as well as quantum error-correction via NISQ systems will fail.</p>
<p>As Scott Aaronson himself asserted, many people in the quantum computing community agree (or used to agree) with claim A). And there is also wide agreement and experimental evidence for claim B). But fewer people see the logical consequence of claims A) and B) put together. </p>
<h2>A little more</h2>
<h3>The basis for Claim A</h3>
<p>We need to put claim A on theoretical grounds. And indeed it is based on a computational complexity assertion for the computational power of NISQ systems, PLUS a heuristic principle (“naturalness”)  on the relation between asymptotic statements and the behaviour of intermediate scale systems.   </p>
<p>My work with Guy Kindler shows that for constant level of noise, NISQ systems represent very low-level computational power that we call LDP. (We studied specifically BosonSampling but the argument extends.)</p>
<p>The naturalness principle is quite common when you relate theory to practice: E.g., when Scott Aaronson and  Sam Gunn <a href="https://arxiv.org/abs/1910.12085">argue</a>  that “spoofing” the Google statistical test for 53 qubits is probably very hard for digital computers, they make a similar leap from asymptotic insights to hardness insights for intermediate-scale systems.</p>
<h3>

</h3>
<h3>Non-stationary and chaotic behaviour</h3>
<p>There are several facts that strengthen the argument for claim A. Let me mention one: We already mentioned that for constant level of noise, NISQ systems represent very low-level computational power. Our work also asserts that for a wide range of <strong><em>lower</em></strong> levels of noise the empirical distribution from sampling based on NISQ systems will be very noise-sensitive: we can expect non-stationary and even chaotic empirical outcomes. <span style="color: #0000ff;">This is a great thing to test empirically!</span></p>
<h3>But how is it that classical computation is possible?</h3>
<p>This is a beautiful piece of the puzzle. The very low computational complexity class describing the power of NISQ systems <em>does</em> support a very rudimentary form of classical error-correction. This is the reason that robust classical information is possible so I can write  this post for you to read. (See also this <a href="https://gilkalai.wordpress.com/2017/10/16/if-quantum-computers-are-not-possible-why-are-classical-computers-possible/">cartoon post</a> from 2017.)</p>
<h3>Topological quantum computing (4 Nick)</h3>
<p>Topological quantum computing is a proposed alternative path to very stable quantum qubits and quantum computing. Accepting claims A) and B) does not directly imply that topological quantum computing will fail, but there are good reasons that the argument does extend.</p>
<h3>Was HQCA achieved already?</h3>
<p>There are two papers claiming it was! One describing the Sycamore Random Circuit Sampling experiment was published in <em>Nature</em> in 2019, and one describing the Jiuzhang Boson Sampling experiment was published in <em>Science</em> in 2020.  I  find both these claims unconvincing.</p>
<h2><span style="color: #ff0000;"><br/>An Open Problem</span></h2>
<p>One question that certainly comes to mind is whether the technological advances represented by <strong>Jiuzhang</strong> and earlier on by <strong>Sycamore</strong> and by other superconducting and ion trapped  NISQ systems, could have technological and scientific fruits even <em>without</em> achieving a computational advantage.</p>
<p>This  is especially interesting if my theory is correct, but it is also interesting if it is not.</p>
<h2>More information</h2>
<h3>A  few of my papers and lectures</h3>
<p><a href="https://arxiv.org/abs/1908.02499">The argument against quantum computers</a>, <em>Quantum, Probability, Logic: Itamar Pitowsky’s Work and Influence</em> (M. Hemmo and O. Shenker, eds.), pp. 399–422, Springer, 2019. (Section 3.5 refers to topological quantum computing.)</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/09/main-pr.pdf">Three puzzles on mathematics computations, and games,</a> Proc. ICM2018;  </p>
<p><a href="https://www.ams.org/journals/notices/201605/rnoti-p508.pdf">The quantum computer puzzle</a>, Notices AMS, May 2016</p>
<p>Boson Sampling was studied in a 2014 paper by Guy Kindler and me <a href="https://arxiv.org/abs/1409.3093" rel="nofollow ugc">Gaussian Noise Sensitivity and BosonSampling</a>. Our study of Boson Sampling gives the basis to understanding of general NISQ systems.</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/08/laws-blog2.pdf">The argument against quantum computers, the quantum laws of nature, and Google’s supremacy claims,</a><em> The Intercontinental Academia Laws: Rigidity and Dynamics </em>(M. J. Hannon and E. Z. Rabinovici, eds.), World Scientific, 2020. arXiv:2008.05188.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/09/cern.pptx">Slides</a> from my 2019 CERN lecture.</p>
<h3>Videotaped lectures</h3>
<p/>
<p><a href="https://youtu.be/BX_Xmz433vM">A recent colloquium at Harvard</a></p>
<p>My ICM 2018 <a href="https://www.youtube.com/watch?v=oR-ufBz13Eg">videotaped lecture</a>;  A very <a href="https://idc-il.zoom.us/rec/share/4v58MpbZ-CBJG7OO1E3SYYN-P426aaa82ndLr_cMyEgOuIwdOStnnIw18Xsg0dUr">easy-going videotaped zoom lecture at USTLC.</a> <a href="https://gilkalai.files.wordpress.com/2019/11/july1.pptx"> (slides)</a></p>
<p>Eralier posts <a href="https://gilkalai.wordpress.com/2019/11/13/gils-collegial-quantum-supremacy-skepticism-faq/">My collegial supremacy FAQ</a>; <a href="https://gilkalai.wordpress.com/2020/08/22/quantum-matters/">Quantum matters</a>.</p>
<h3>Boson Sampling as the<em> C. Elegans</em> of quantum computing</h3>
<p>Caenorhabditis elegans, or<em> C. elegans</em> for short, is a species of a free-living roundworm whose biological study shed much light on more complicated organisms. In my paper in Itamar Pitowsky’s memorial volume I regard BosonSampling as the <em>C. elegans</em> of quantum computing. Several technical and conceptual aspects of quantum computation are becoming much clearer for this model.</p>
<h2>A remark about the terminology</h2>
<p><span style="color: #008080;">John Preskill’s term “quantum supremacy” is problematic because the word “supremacy” (e.g. “the supremacy of the king”) often refers to superiority or dominance across the board, which is not the case for quantum computation. It is also unfortunate in terms of its connotations. I agree with Preskill that “advantage” is too weak e.g. when it is claimed that the new Boson Sampling experiment can achieve in 200 seconds what would take an ordinary supercomputer millions of years, this is not merely an “advantage.”  This is why I propose “huge quantum computational advantage” (HQCA). </span></p>
<p> </p>
<h2>Robert Aumann, Benjamin Netanyahu, and quantum computers</h2>
<p><a href="https://twitter.com/raz_shlomo/status/1310124893086846976">Here is an amusing story (in Hebrew) from an interview of Robert Aumann about  quantum computers and Benjamin Netanyahu</a></p>
<p><a href="https://twitter.com/raz_shlomo/status/1310124893086846976"><br/><br/></a></p>
<p> </p></div>
    </content>
    <updated>2020-12-29T13:10:40Z</updated>
    <published>2020-12-29T13:10:40Z</published>
    <category term="Combinatorics"/>
    <category term="Computer Science and Optimization"/>
    <category term="Physics"/>
    <category term="Probability"/>
    <category term="Quantum"/>
    <category term="Guy Kindler"/>
    <category term="quantum supremacy"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2021-01-02T08:37:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/192</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/192" rel="alternate" type="text/html"/>
    <title>TR20-192 |  Constructing Large Families of Pairwise Far Permutations: Good Permutation Codes Based on the Shuffle-Exchange Network | 

	Oded Goldreich, 

	Avi Wigderson</title>
    <summary>We consider the problem of efficiently constructing an as large as possible family of permutations such that each pair of permutations are far part (i.e., disagree on a constant fraction of their inputs).
Specifically, for every $n\in\N$, we present a collection of $N=N(n)=(n!)^{\Omega(1)}$ pairwise far apart permutations $\{\pi_i:[n]\to[n]\}_{i\in[N]}$ and a polynomial-time algorithm that on input $i\in[N]$ outputs an explicit description of $\pi_i$. 

From a coding theoretic perspective, we construct permutation codes of constant relative distance and constant rate along with efficient encoding (and decoding) algorithms. 
This construction is easily extended to produce codes on smaller alphabets in which every codeword is balanced; namely, each symbol appears the same number of times. 

Our construction combines routing on the Shuffle-Exchange network with any good binary error correcting code. 
Specifically, we uses codewords of a good binary code in order to determine the switching instructions in the Shuffle-Exchange network.</summary>
    <updated>2020-12-27T14:50:02Z</updated>
    <published>2020-12-27T14:50:02Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-01-02T08:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/191</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/191" rel="alternate" type="text/html"/>
    <title>TR20-191 |  Negations Provide Strongly Exponential Savings | 

	Arkadev Chattopadhyay, 

	Rajit Datta, 

	Partha Mukhopadhyay</title>
    <summary>We show that there is a family of monotone multilinear polynomials over $n$ variables in VP, such that any monotone arithmetic circuit for it would be of size $2^{\Omega(n)}$. Before our result, strongly exponential lower bounds on the size of monotone circuits were known only for computing explicit polynomials in VNP. The family of polynomials we prescribe are the spanning tree polynomials, also considered by Jerrum and Snir (JACM,1982), but this time defined over constant-degree expander graphs.</summary>
    <updated>2020-12-27T11:30:45Z</updated>
    <published>2020-12-27T11:30:45Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-01-02T08:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/190</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/190" rel="alternate" type="text/html"/>
    <title>TR20-190 |  Erasure-Resilient Sublinear-Time Graph Algorithms | 

	Ramesh Krishnan S. Pallavoor, 

	Amit Levi, 

	Sofya Raskhodnikova, 

	Nithin Varma</title>
    <summary>We investigate sublinear-time algorithms that take partially erased graphs represented by adjacency lists as input. Our algorithms make degree and neighbor queries to the input graph and work with a specified fraction of adversarial erasures in adjacency entries. We focus on two computational tasks: testing if a graph is connected or $\varepsilon$-far from connected and estimating the average degree. For testing connectedness, we discover a threshold phenomenon: when the fraction of erasures is less than $\varepsilon$, this property can be tested efficiently (in time independent of the size of the graph); when the fraction of erasures is at least $\varepsilon,$ then a number of queries linear in the size of the graph representation is required. Our erasure-resilient algorithm (for the special case with no erasures) is an improvement over the previously known algorithm for connectedness in the standard property testing model and has optimal dependence on the proximity parameter $\varepsilon$. For estimating the average degree, our results provide an "interpolation" between the query complexity for this computational task in the model with no erasures in two different settings: with only degree queries, investigated by Feige (SIAM J. Comput. `06), and with degree queries and neighbor queries, investigated by Goldreich and Ron (Random Struct. Algorithms `08) and Eden et al. (ICALP `17). We conclude with a discussion of our model and open questions raised by our work.</summary>
    <updated>2020-12-27T08:15:46Z</updated>
    <published>2020-12-27T08:15:46Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-01-02T08:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=20488</id>
    <link href="https://gilkalai.wordpress.com/2020/12/26/open-problem-session-of-huji-combsem-problem-4-eitan-bachmat-weighted-statistics-for-permulations/" rel="alternate" type="text/html"/>
    <title>Open problem session of HUJI-COMBSEM: Problem #4, Eitan Bachmat: Weighted Statistics for Permutations</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This is a continuation of our series of posts on the HUJI seminar 2020 open problems. This time the post was kindly written by Eitan Bachmat who proposed the problem.  My summary: understanding of the distribution of largest increasing subsequences … <a href="https://gilkalai.wordpress.com/2020/12/26/open-problem-session-of-huji-combsem-problem-4-eitan-bachmat-weighted-statistics-for-permulations/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://gilkalai.files.wordpress.com/2020/12/photo-bachmat.jpg"><img alt="" class="alignnone size-medium wp-image-20742" height="200" src="https://gilkalai.files.wordpress.com/2020/12/photo-bachmat.jpg?w=300" width="300"/></a></p>
<p><em>This is a continuation of our series of posts on the HUJI seminar 2020 open problems. This time the post was kindly written by <a href="https://www.cs.bgu.ac.il/~ebachmat/">Eitan Bachmat</a> who proposed the problem.  </em></p>
<p><em><span style="color: #ff0000;">My summary: understanding of the distribution of largest increasing subsequences for random permutations, and the combinatorics of largest increasing subsequences of general permutations is among the most fascinating stories in modern mathematics. When you also add weights this is largely an uncharted territory. </span><br/>
</em></p>
<h2>Heaviest increasing subsequences</h2>
<p>Setup: Let X be a bounded distribution with support on an interval [1,T]. Given a permutation <img alt="{\sigma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\sigma}"/> of size N, an increasing subsequence is a set of indices <img alt="{i_1&lt;i_2&lt;\cdots&lt;i_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi_1%3Ci_2%3C%5Ccdots%3Ci_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{i_1&lt;i_2&lt;\cdots&lt;i_k}"/> such that <img alt="{\sigma (i_1)&lt;\sigma (i_2)&lt;\cdots&lt;\sigma (i_k)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csigma+%28i_1%29%3C%5Csigma+%28i_2%29%3C%5Ccdots%3C%5Csigma+%28i_k%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\sigma (i_1)&lt;\sigma (i_2)&lt;\cdots&lt;\sigma (i_k)}"/>. If in addition, we attach to each index <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{i}"/> a weight <img alt="{w(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{w(i)}"/>, the weight of the subsequence is the sum <img alt="{w(i_1)+w(i_2)+\cdots+w(i_k)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%28i_1%29%2Bw%28i_2%29%2B%5Ccdots%2Bw%28i_k%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{w(i_1)+w(i_2)+\cdots+w(i_k)}"/>. We are interested in the asymptotic behavior as <img alt="{N\rightarrow \infty}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{N\rightarrow \infty}"/> of <img alt="{H_X(N)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BH_X%28N%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{H_X(N)}"/>, the random variable which is given by the weight of the heaviest increasing subsequence where <img alt="{\sigma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\sigma}"/> is chosen uniformly among permutations of size <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{N}"/> and the <img alt="{w_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{w_i}"/> are sampled i.i.d from X.</p>
<p>A standard sub-additivity argument shows that with high probability, as <img alt="{N\rightarrow \infty}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%5Crightarrow+%5Cinfty%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{N\rightarrow \infty}"/>, <img alt="{\frac{H_X(N)}{\sqrt{N}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7BH_X%28N%29%7D%7B%5Csqrt%7BN%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\frac{H_X(N)}{\sqrt{N}}}"/> approaches a constant <img alt="{\tau (X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctau+%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\tau (X)}"/>. The theorem of Vershik and Kerov on the longest increasing subsequence in a random permutation states that if X is the Dirac distribution which always takes the value <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{1}"/>, then <img alt="{\tau (X)=2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctau+%28X%29%3D2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\tau (X)=2}"/>. From this result, we can conclude that for any <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{X}"/>, <img alt="{\tau (X)\geq 2E(X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctau+%28X%29%5Cgeq+2E%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\tau (X)\geq 2E(X)}"/> (Take the longest increasing subsequence, disregarding the weights). Using similar elementary arguments it can be shown also that</p>
<p><img alt="\displaystyle \frac{\tau (X)}{2\sqrt{E(X^2)}},\frac{2\sqrt{E(X^2)}}{\tau (X)}\leq e\sqrt{ln(T)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Cfrac%7B%5Ctau+%28X%29%7D%7B2%5Csqrt%7BE%28X%5E2%29%7D%7D%2C%5Cfrac%7B2%5Csqrt%7BE%28X%5E2%29%7D%7D%7B%5Ctau+%28X%29%7D%5Cleq+e%5Csqrt%7Bln%28T%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle \frac{\tau (X)}{2\sqrt{E(X^2)}},\frac{2\sqrt{E(X^2)}}{\tau (X)}\leq e\sqrt{ln(T)}"/></p>
<p>Which means that <img alt="{2\sqrt{E(X^2)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Csqrt%7BE%28X%5E2%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{2\sqrt{E(X^2)}}"/> is a much better estimate of <img alt="{\tau (X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctau+%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\tau (X)}"/> than <img alt="{2E(X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2E%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{2E(X)}"/> (<img alt="{\frac{\sqrt{E(X^2)}}{E(X)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B%5Csqrt%7BE%28X%5E2%29%7D%7D%7BE%28X%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\frac{\sqrt{E(X^2)}}{E(X)}}"/> can be as large as <img alt="{\sqrt{T}/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7BT%7D%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\sqrt{T}/2}"/> for distributions with support in [1,T]).</p>
<p><strong>Problem 1:</strong> Is it true that for all X</p>
<p><img alt="\displaystyle \tau (X)\geq 2\sqrt{E(X^2)}." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Ctau+%28X%29%5Cgeq+2%5Csqrt%7BE%28X%5E2%29%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle \tau (X)\geq 2\sqrt{E(X^2)}."/></p>
<p>Numerical experimentation with many distributions has failed so far to find a counter example. Even the weaker bound</p>
<p><img alt="\displaystyle \tau (X)\geq c\sqrt{E(X^2)}." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Ctau+%28X%29%5Cgeq+c%5Csqrt%7BE%28X%5E2%29%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle \tau (X)\geq c\sqrt{E(X^2)}."/></p>
<p>for some constant <img alt="{c&gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{c&gt;0}"/> is unknown to the best of our knowledge and would also be very nice to have.</p>
<p>We can conjecture a stronger statement, by considering the space of all distributions (say on [1,T]) with its convex structure given by mixtures of distributions. We say that <img alt="{X=pY+(1-p)Z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%3DpY%2B%281-p%29Z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{X=pY+(1-p)Z}"/> is a mixture of <img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{Y}"/> and <img alt="{Z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{Z}"/>, if for all <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{t}"/>, <img alt="{Pr(X&lt;t)=pPr(Y&lt;t)+(1-p)Pr(Z&lt;t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BPr%28X%3Ct%29%3DpPr%28Y%3Ct%29%2B%281-p%29Pr%28Z%3Ct%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{Pr(X&lt;t)=pPr(Y&lt;t)+(1-p)Pr(Z&lt;t)}"/>.</p>
<p><strong> Problem 2:</strong> Is it true that <img alt="{\tau ^2(X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctau+%5E2%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\tau ^2(X)}"/> is a concave function with respect to mixtures, i.e., for any mixture <img alt="{X=pY+(1-p)Z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%3DpY%2B%281-p%29Z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{X=pY+(1-p)Z}"/> we have</p>
<p><img alt="\displaystyle \tau^2(X)\geq p\tau^2(Y)+(1-p)\tau^2(Z)." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5Ctau%5E2%28X%29%5Cgeq+p%5Ctau%5E2%28Y%29%2B%281-p%29%5Ctau%5E2%28Z%29.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle \tau^2(X)\geq p\tau^2(Y)+(1-p)\tau^2(Z)."/></p>
<p>The first problem is essentially this statement for the mixture which expresses <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{X}"/> as a generalized mixture of the extreme points which are Dirac measures on <img alt="{1\leq t\leq T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%5Cleq+t%5Cleq+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{1\leq t\leq T}"/>. We donít have counter examples for the stronger statement either.</p>
<p>The third problem concerns the upper bound.</p>
<p><strong>Problem 3:</strong> Is there a constant <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{C}"/> such that, <img alt="{\frac{\tau^2 (X)}{E(X^2)}\leq C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac%7B%5Ctau%5E2+%28X%29%7D%7BE%28X%5E2%29%7D%5Cleq+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\frac{\tau^2 (X)}{E(X^2)}\leq C}"/> for all <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{X}"/> bounded. We believe this is false. In particular, consider the family of Bounded Pareto distributions with parameter <img alt="{\alpha=2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Calpha%3D2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\alpha=2}"/>, with density given by <img alt="{f_T(t)=c_Tt^{-3}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_T%28t%29%3Dc_Tt%5E%7B-3%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{f_T(t)=c_Tt^{-3}}"/> for <img alt="{1\leq t\leq T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%5Cleq+t%5Cleq+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{1\leq t\leq T}"/> and <img alt="{c_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bc_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{c_T}"/> the normalizing constant. We believe that the ratio for this particular family is unbounded (and in some moral sense, thatís the only case).</p>
<p><strong>Problem 4:</strong> Compute <img alt="{\tau_X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctau_X%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\tau_X}"/> for ANY distribution which is not a Dirac distribution. Best chance seems to be the exponential or geometric distributions, where Kurt Johansson answers the corresponding last passage percolation problem, <a href="https://arxiv.org/pdf/math/9903134.pdf" rel="nofollow">https://arxiv.org/pdf/math/9903134.pdf</a></p>
<h2>Motivation</h2>
<p>The problems above are motivated by the study of the asymptotic behavior of certain finite particle (interval) systems which we call “`airplane boarding”‘. The system has a parameter <img alt="{k\geq 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{k\geq 0}"/>. At any given moment, the system consists of <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{N}"/> closed moving intervals <img alt="{I_1,\dots,I_N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BI_1%2C%5Cdots%2CI_N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{I_1,\dots,I_N}"/> all of length <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{k}"/> which we call “passengers” and which can only overlap at endpoints. The index of an interval/passenger is called its “queue location”. The ordering of the intervals according to “queue locations” never changes, i.e., passengers cannot pass each other while “boarding”. The interval <img alt="{[0,N]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5B0%2CN%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{[0,N]}"/> is called the “airplane aisle”. Each interval <img alt="{I_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BI_j%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{I_j}"/> has a target “row” (in the airplane aisle) <img alt="{\sigma (j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csigma+%28j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\sigma (j)}"/> where <img alt="{\sigma }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csigma+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\sigma }"/> is a permutation on <img alt="{1,...,N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2C...%2CN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{1,...,N}"/> (the system can be generalized to handle more than one “passenger” per “row”). To each “passenger” we also attach an “aisle clearing time” <img alt="{w_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{w_i}"/>. Initially, at time <img alt="{t=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{t=0}"/>, all the intervals are left of <img alt="{-k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B-k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{-k}"/> and in descending order of their index, for example, if <img alt="{ k=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+k%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{ k=1}"/>, we might have <img alt="{I_1=[-3,-2]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BI_1%3D%5B-3%2C-2%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{I_1=[-3,-2]}"/>, <img alt="{I_2=[-5.5,-4.5]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BI_2%3D%5B-5.5%2C-4.5%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{I_2=[-5.5,-4.5]}"/> etc., at any given time point, each interval/passenger, <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{i}"/>,moves at infinite speed towards its destination “row” and if it is not blocked by other intervals to its right it positions itself with the left endpoint at <img alt="{\sigma (i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csigma+%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\sigma (i)}"/>, i.e., occupies <img alt="{[\sigma_i-k,\sigma_i]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5B%5Csigma_i-k%2C%5Csigma_i%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{[\sigma_i-k,\sigma_i]}"/>. When a passenger does reach its target “row”, then it waits its aisle clearing time <img alt="{w_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{w_i}"/> before leaving the system and allowing the passengers behind to advance further towards their row. The “boarding time” is the time when all intervals have left the system and the process terminates. From the description, one can check that the boarding time when <img alt="{k=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{k=0}"/> is simply the heaviest increasing subsequence for <img alt="{\sigma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\sigma}"/> and weights <img alt="{ w_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+w_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{ w_i}"/>, so its clear that the problems above are relevant to the analysis of “airplane boarding”. To motivate them more specifically, lets introduce one more element, a “boarding policy” which makes the boarding time into a random variable. The simplest policy, which we have already encountered, is Random boarding. In Random boarding we assume that the queue position, the row and the weights are all independent random variables. The first two are chosen uniformly, generating a uniformly random permutation and we think of the <img alt="{w_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{w_i}"/> as drawn i.i.d. from a distribution <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{X}"/>, which is the aisle clearing time distribution of all passengers as a single group. We observe that <img alt="{\tau (X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctau+%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\tau (X)}"/> is a normalized asymptotic indicator of the boarding time of the Random policy. Suppose now that we have two (or more in general) subpopulations with different aisle clearing times, say, passengers with no hand luggage and the other passengers, or passengers who need assistant or are travelling with children and the other passengers as a second group. If we denote by Y and Z the aisle clearing time of the first and second subpopulations and by p, the portion of passengers from the first subpopulation, then the aisle clearing time of the whole population <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{X}"/> will be the mixture <img alt="{pY+(1-p)Z}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BpY%2B%281-p%29Z%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{pY+(1-p)Z}"/>. We use <img alt="{\tau}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctau%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\tau}"/> as a measure of the slowness or quickness of a group and will assume that the first group with aisle clearing time distribution <img alt="{Y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BY%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{Y}"/> is the group of ‘slow passengers”, i.e., <img alt="{\tau (Y)\geq \tau(Z)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctau+%28Y%29%5Cgeq+%5Ctau%28Z%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\tau (Y)\geq \tau(Z)}"/>. Given this scenario, we can naturally define two policies which are practiced by airlines. The first policy is the “Slow First” (SF) policy, where we use a uniformly random permutation <img alt="{\sigma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\sigma}"/>, but for the weights, those of the first <img alt="{[pN]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5BpN%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{[pN]}"/> passengers are drawn i.i.d. from Y and the others are drawn i.i.d. from Z. The other policy is “Fast First” (FF) which as the name suggests, draws the weights of the first <img alt="{[(1-p)N]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5B%281-p%29N%5D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{[(1-p)N]}"/> passengers i.i.d. from Z and the others from Y.</p>
<p>Now we come to the test your intuition portion of the post. <strong><span style="color: #0000ff;">Spoiler alert, please test your intuition before continuing to read below the fold.</span></strong> Assuming “normal” coach class conditions in a full occupancy airplane, <img alt="{k=4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%3D4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{k=4}"/>, with a realistic portion of slow passengers, say p=0.2, and a realistic effective aisle clearing time ratio <img alt="{\tau (Y)/\tau (Z)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctau+%28Y%29%2F%5Ctau+%28Z%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\tau (Y)/\tau (Z)}"/> of <img alt="{C=3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%3D3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{C=3}"/>, rank the 3 policies, Random, Slow First and Fast First, from fastest to slowest in terms of average asymptotic boarding time (ascending boarding time order).</p>
<p>\ FOLD \</p>
<p><a href="https://gilkalai.files.wordpress.com/2020/11/probs2.png"><img alt="" class="alignnone size-full wp-image-20576" height="397" src="https://gilkalai.files.wordpress.com/2020/11/probs2.png" width="640"/></a></p>
<p><span id="more-20488"/></p>
<p>The ordering is: Slow First, then Fast First, then Random. OK, but what happens if I choose other parameters <img alt="{k,p,c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%2Cp%2Cc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{k,p,c}"/>? We have the following result of Erland, Kaupuzs, Frette, Pugatch and Bachmat, <a href="https://arxiv.org/abs/1906.05018" rel="nofollow">https://arxiv.org/abs/1906.05018</a> \</p>
<p><strong>Theorem:</strong> (SF is better than FF universally) For any parameters <img alt="{k,p,c}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%2Cp%2Cc%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{k,p,c}"/>, SF has lower or equal average asymptotic boarding time compared to FF.</p>
<p>It turns out that we donít have a universal comparison between Random and FF. On realistic parameters, FF is faster, but for some small region of parameter space Random is actually faster than FF. We are left with the comparison of SF and Random and here comes problem 2 into play. We have the following result of Erland, Kaupusz, Steiner and Bachmat <a href="https://arxiv.org/abs/2012.11246" rel="nofollow">https://arxiv.org/abs/2012.11246</a></p>
<p><strong>Theorem:</strong> Slow first is universally (for all mixtures and k) faster than Random if and only if <img alt="{\tau^2 (X)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctau%5E2+%28X%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\tau^2 (X)}"/> is concave on the space of bounded distributions.</p>
<p>The concavity of <img alt="{\tau^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctau%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\tau^2}"/> has been verified on all mixtures coming from empirical observations of aisle clearing times (say, first group is passengers with 0,2,3 luggage items and the other those with 1 or 4).</p>
<p>The third problem and to some extent, the first, have a somewhat different motivation. There is a very different situation where we break a distribution as a mixture and that is express line queues, where the population of customers is broken into fast customers that go to the express line and the others which go to a regular line (assume there is one). We can consider the average waiting time in queue as a function of the mixture. Such systems have a parameter that does not exist in airplane boarding, the utilization <img alt="{\rho}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Crho%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\rho}"/>. On the other hand, airplane boarding has <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{k}"/> which does not have an express line counterpart. In addition, in airplane boarding there may be better policies for handling the slow and fast groups (say, slow passengers in back rows at the front of the queue and slow passengers from the front rows at the back of the queue, and more complicated ones).</p>
<p> </p>
<p>It has been shown, <a href="https://www.sciencedirect.com/science/article/pii/S0377221718310695" rel="nofollow">https://www.sciencedirect.com/science/article/pii/S0377221718310695</a> , that after proper normalizations to offset these non-mutual parameters, for any mixture, the average response time of the express line queue is approximately the square of the boarding time of the optimal policy for handling the slow and fast groups of the mixture (a complicated policy, the result does not hold for slow first nor fast first). The quality of the approximation essentially depends on the ratio <img alt="{\tau^2 (X)/E(X^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ctau%5E2+%28X%29%2FE%28X%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\tau^2 (X)/E(X^2)}"/>. If it is universally bounded then so is the approximation ratio.</p>
<p>Having such a tight relation between express line queues and airplane boarding is useful, since we can use it to analyze airplane boarding via the simpler and better studied express line queues. In addition, express line queues have an exact and non trivial duality on mixtures which preserves average waiting time via a formula of Riemann which was used in his proof of the functional equation of the zeta function. The duality simplifies the analysis of express line queues on self-dual distributions, these include log-normal distributions, Bounded Pareto distributions (as a family) and distributions whose densities are given by squaring a weight 2 Hecke cusp form (as in the Taniyama-Shimura conjecture) restricted to the positive imaginary axis. The approximate relation allows us to get a non-trivial approximate duality in airplane boarding.</p>
<p>The post is becoming long, so I will leave you with one final insight which is at the heart of the analysis of airplane boarding. Airplane boarding is asymptotically, most naturally understood in terms of evolution in (proper) time of particle clouds in space-time. As a process (consider the passengers which sit down at approximately the same time) it is essentially a discrete version of what is known as a Congruence with respect to proper time of particles, starting with a Cauchy hypersurface. That’s the kind of stuff that Penrose studied (much more deeply) when he proved the singularity theorems that won him a share of the 2020 Noble prize in physics. Maximal length or heaviest increasing subsequences in uniformly random permutations correspond to the simplest case of geodesics in special relativity (straight lines). I will explain some other time, what all this has to do with college admissions.</p>
<p> </p></div>
    </content>
    <updated>2020-12-26T15:19:56Z</updated>
    <published>2020-12-26T15:19:56Z</published>
    <category term="Combinatorics"/>
    <category term="Guest blogger"/>
    <category term="Probability"/>
    <category term="Eitan Bachmat"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2021-01-02T08:37:38Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-6555947.post-1034328608879992149</id>
    <link href="http://blog.geomblog.org/feeds/1034328608879992149/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.geomblog.org/2020/12/lars-arge.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/6555947/posts/default/1034328608879992149" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/6555947/posts/default/1034328608879992149" rel="self" type="application/atom+xml"/>
    <link href="http://feedproxy.google.com/~r/TheGeomblog/~3/ok1NCQJBVpM/lars-arge.html" rel="alternate" type="text/html"/>
    <title>Lars Arge.</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><i> Not a post I'd have wanted to make on Christmas day, but that's how it goes sometimes. </i></p><p>Lars Arge just passed away, on Dec 23. For those of us who've been following his battles with cancer, this might not come as a total shock, but there was always hope, and that's no longer an option. </p><p>It's hard to imagine this in 2020, but there was a time not that long ago (at least in my mind) when "big data" wasn't really a thing. Companies were acquiring lots of data, and "GIGA byte" was a thing, but there was no real appreciation of the computational challenge associated with big data. </p><p><a href="https://hal.inria.fr/inria-00075827">A paper by Aggarwal and Vitter</a> in 1998 made the first step towards changing that, introducing the <a href="https://en.wikipedia.org/wiki/External_memory_algorithm#:~:text=The%20model%20was%20introduced%20by,size%20and%20the%20cache%20size.&amp;text=Both%20the%20internal%20and%20external,into%20blocks%20of%20size%20B.">external memory model</a> as a way to think about computations when you have memory access that are cheap (in RAM) and expensive (on disk). </p><p>It's a diabolically simple model: all main memory access is free, and any disk access costs 1 unit (but you can get a block of data of size B for that one unit of access). It's not meant to be realistic, but like the best computational models, it's meant to isolate the key operations that are expensive so that we can study how algorithm design needs to change. </p><p>Lars was one of the foremost algorithm designers for this new world of external memory. His Ph.D thesis laid out ideas for how to build data structures that are external memory efficient, and his research over the next many decades, in true Tarjan/Hopcroft form, built the fundamental structures and concepts one would need to even think about efficient algorithm design, with many clever ideas around batching queries, processing data in main memory to prepare for queries, and streaming access to disk when appropriate. </p><p>Formal algorithmic models are often misunderstood. They look simplistic, miss many of the details that seem relevant in practice, and appear to encourage theoretical game playing divorced from reality. But a formal model at its best does its work invisibly. It shifts the way we think about a framework. It fosters the design of new paradigms for efficient algorithms, and it allows us to layer optimizations on that move a system from theory to practice without ever having to compromise the underlying design principles.</p><p>Lars was a force of nature in this area. I first remember meeting him in 1998 at AT&amp;T Labs when I was interning and he was visiting there. He had boundless energy for this space, and seemingly wanted to turn everything into an external memory algorithm, whether it was geometry, data structures, or even the most basic algorithms like sorting. His intuition was the best kind of algorithmic intuition: build up the core primitives, and the rest would follow. </p><p>And this is exactly what happened. The field exploded. For a while, "big data algorithms" WERE external memory algorithms. There was no other way to even talk about big data. And that spawned even more models. Streaming algorithms were inspired by external memory and the realization that a one pass stream was an effective way to work with large data. Cache-oblivious algorithms asked about what would happen if we took the same two-part hierarchy with main memory and disk and extended it to the cache. Semi-external memory models asked how we might modify the base model for graph computations. The MapReduce framework from the early 2000s generalized the external memory model to handle newer kinds of streaming/memory-limited architectures, in turn to be followed by Spark and so many other models. </p><p>I'd go as far as to say this: all of the conceptual developments we see today in big data computations at some level can be traced back to work on external memory algorithms, and that was driven by Lars (and his collaborators). </p><p>It wasn't just the papers he wrote. Lars was a leader in shaping the field. Early in the 2000s he moved back from Duke University to Aarhus University, and from there started to build what would become one of the foremost institutes for thinking about big data, first as a BRICS center and then as the appropriately named <a href="https://cs.au.dk/research/centers/madalgo/">MADALGO</a> Institute. </p><p>Many of us who had anything to do with big data visited MADALGO at some point in our careers. I spent one of the best summers of my life being hosted by him during my sabbatical - my children still remember that summer we spent in Aarhus and wish we could go back each year. He instinctively knew that the best way to foster the area was to facilitate a generation of researchers who would bring their own ideas to Aarhus, mix and exchange them,  and then go away and share them with the world. </p><p>And he wasn't merely content with that. He wanted to demonstrate the power of his perspective beyond just the realm of academia. He started a company <a href="https://scalgo.com/">SCALGO</a> that applied the principles of external memory algorithms (and so much more) to help with modeling geospatial data. I remember distinctly him telling me the first time he demonstrated SCALGO products in a forum with other companies doing GIS work and how the performance of their system blew the other products out of the water. For someone (at the time) deeply embedded in the theory of computer science, I was astounded and encouraged by this validation of formal thinking. </p><p>Lars was a giant in our field (his email address was always large@..., and this worked more appropriately than one would ever dream of). But he was also a giant both in real life and in his personality. He was the warmest, most fun person to be around. He seemed almost ego-free, and often downplayed his own accomplishments, claiming that his main talent was hanging around with smarter people. He was extremely generous with his time and resources (which is why so many of us were able to visit Aarhus and benefit from being at MADALGO)</p><p>He was the life of any party -- I still remember when he hosted the Symposium on Computational Geometry in Denmark. It felt like we were at a post-battle Viking celebration (and yes he got up on a table and shouted "SKÅL" over and over again while an actual pig was roasting on a spit nearby). I remember him taking me to a Denmark-Sweden soccer game and warning me not to wear anything with blue on it. I remember us going for go-kart racing and his stream of trash talking. </p><p>Lars was the entire package: a great person, a great researcher, a visionary leader, and a canny entrepreneur. I will miss him greatly. </p><img alt="" height="1" src="http://feeds.feedburner.com/~r/TheGeomblog/~4/ok1NCQJBVpM" width="1"/></div>
    </content>
    <updated>2020-12-25T18:47:00Z</updated>
    <published>2020-12-25T18:47:00Z</published><feedburner:origlink xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0">http://blog.geomblog.org/2020/12/lars-arge.html</feedburner:origlink>
    <author>
      <name>Suresh Venkatasubramanian</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/15898357513326041822</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-6555947</id>
      <category term="research"/>
      <category term="community"/>
      <category term="miscellaneous"/>
      <category term="soda"/>
      <category term="conferences"/>
      <category term="data-mining"/>
      <category term="socg"/>
      <category term="blogosphere"/>
      <category term="publishing"/>
      <category term="clustering"/>
      <category term="teaching"/>
      <category term="jobs"/>
      <category term="funding"/>
      <category term="humor"/>
      <category term="awards"/>
      <category term="outreach"/>
      <category term="stoc"/>
      <category term="cs.CG"/>
      <category term="focs"/>
      <category term="nsf"/>
      <category term="reviewing"/>
      <category term="socg-2010"/>
      <category term="fairness"/>
      <category term="academy"/>
      <category term="latex"/>
      <category term="stoc2017"/>
      <category term="theoryfest"/>
      <category term="workshops"/>
      <category term="acm"/>
      <category term="conf-blogs"/>
      <category term="writing"/>
      <category term="cs.DS"/>
      <category term="cs.LG"/>
      <category term="geometry"/>
      <category term="p-vs-nc"/>
      <category term="advising"/>
      <category term="sabbatical"/>
      <category term="simons foundation"/>
      <category term="announcement"/>
      <category term="big-data"/>
      <category term="deadline"/>
      <category term="jeff phillips"/>
      <category term="streaming"/>
      <category term="books"/>
      <category term="large-data"/>
      <category term="p-vs-np"/>
      <category term="cra"/>
      <category term="cstheory"/>
      <category term="focs2010"/>
      <category term="icdm"/>
      <category term="math.PR"/>
      <category term="memorial"/>
      <category term="personal"/>
      <category term="posters"/>
      <category term="potd"/>
      <category term="rajeev motwani"/>
      <category term="shonan"/>
      <category term="socg2012"/>
      <category term="software"/>
      <category term="stoc2012"/>
      <category term="GIA"/>
      <category term="SDM"/>
      <category term="alenex"/>
      <category term="alenex2011"/>
      <category term="arxiv"/>
      <category term="career"/>
      <category term="complexity"/>
      <category term="cs.CC"/>
      <category term="deolalikar"/>
      <category term="distributions"/>
      <category term="madalgo"/>
      <category term="nips"/>
      <category term="sdm2011"/>
      <category term="shape"/>
      <category term="talks"/>
      <category term="technology"/>
      <category term="theory.SE"/>
      <category term="travel"/>
      <category term="video"/>
      <category term="8f-cg"/>
      <category term="DBR"/>
      <category term="ICS"/>
      <category term="LISPI"/>
      <category term="acceptances"/>
      <category term="bibtex"/>
      <category term="bregman"/>
      <category term="cfp"/>
      <category term="clustering-book"/>
      <category term="column"/>
      <category term="combinatorial geometry"/>
      <category term="current-distance"/>
      <category term="ecml-pkdd"/>
      <category term="empirical"/>
      <category term="esa"/>
      <category term="fat*"/>
      <category term="focs2012"/>
      <category term="focs2014"/>
      <category term="fwcg"/>
      <category term="game theory"/>
      <category term="godel"/>
      <category term="graphs"/>
      <category term="implementation"/>
      <category term="journals"/>
      <category term="kernels"/>
      <category term="misc"/>
      <category term="models"/>
      <category term="obituary"/>
      <category term="productivity"/>
      <category term="programming"/>
      <category term="society"/>
      <category term="soda2011"/>
      <category term="topology"/>
      <category term="turing"/>
      <category term="tv"/>
      <category term="women-in-theory"/>
      <category term=".02"/>
      <category term="IMA"/>
      <category term="MOOC"/>
      <category term="PPAD"/>
      <category term="accountability"/>
      <category term="active-learning"/>
      <category term="aggregator"/>
      <category term="algorithms"/>
      <category term="ams"/>
      <category term="analco"/>
      <category term="barriers"/>
      <category term="beamer"/>
      <category term="blogging"/>
      <category term="candes"/>
      <category term="civil rights"/>
      <category term="classification"/>
      <category term="coding-theory"/>
      <category term="coffee"/>
      <category term="conjecture"/>
      <category term="cosmos"/>
      <category term="counting"/>
      <category term="cricket"/>
      <category term="cs.DC"/>
      <category term="dagstuhl"/>
      <category term="databuse"/>
      <category term="dimacs"/>
      <category term="dimensionality-reduction"/>
      <category term="distributed-learning"/>
      <category term="double-blind review"/>
      <category term="duality"/>
      <category term="eda"/>
      <category term="embarrassing"/>
      <category term="ethics"/>
      <category term="expanders"/>
      <category term="experiments"/>
      <category term="fake-news"/>
      <category term="fatml"/>
      <category term="fellowships"/>
      <category term="focs2013"/>
      <category term="fonts"/>
      <category term="gct"/>
      <category term="ggplot"/>
      <category term="gpu"/>
      <category term="graph minors"/>
      <category term="gt.game-theory"/>
      <category term="guest-post"/>
      <category term="guitar"/>
      <category term="hangouts"/>
      <category term="hirsch"/>
      <category term="history"/>
      <category term="ipe"/>
      <category term="ita"/>
      <category term="jmm"/>
      <category term="k-12"/>
      <category term="knuth"/>
      <category term="machine-learning"/>
      <category term="massive"/>
      <category term="math.ST"/>
      <category term="media"/>
      <category term="memes"/>
      <category term="metoo"/>
      <category term="metrics"/>
      <category term="morris"/>
      <category term="movies"/>
      <category term="multicore"/>
      <category term="music"/>
      <category term="narrative"/>
      <category term="networks"/>
      <category term="nih"/>
      <category term="parallelism"/>
      <category term="partha niyogi"/>
      <category term="polymath"/>
      <category term="polymath research"/>
      <category term="polytopes"/>
      <category term="postdocs"/>
      <category term="privacy"/>
      <category term="quant-ph"/>
      <category term="quantum"/>
      <category term="randomness"/>
      <category term="review"/>
      <category term="sampling"/>
      <category term="seminars"/>
      <category term="social-networking"/>
      <category term="soda2014"/>
      <category term="students"/>
      <category term="sublinear"/>
      <category term="submissions"/>
      <category term="summer-school"/>
      <category term="superbowl"/>
      <category term="surveys"/>
      <category term="svn"/>
      <category term="television"/>
      <category term="traffic"/>
      <category term="twitter"/>
      <category term="utah"/>
      <category term="wads"/>
      <category term="white elephant"/>
      <category term="xkcd"/>
      <author>
        <name>Suresh Venkatasubramanian</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/15898357513326041822</uri>
      </author>
      <link href="http://blog.geomblog.org/" rel="alternate" type="text/html"/>
      <link href="http://www.blogger.com/feeds/6555947/posts/default?alt=atom&amp;start-index=26&amp;max-results=25&amp;redirect=false" rel="next" type="application/atom+xml"/>
      <link href="http://feeds.feedburner.com/TheGeomblog" rel="self" type="application/atom+xml"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <subtitle>Ruminations on computational geometry, algorithms, theoretical computer science and life</subtitle>
      <title>The Geomblog</title>
      <updated>2020-12-26T12:35:57Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17925</id>
    <link href="https://rjlipton.wordpress.com/2020/12/24/p-np-in-our-stockings/" rel="alternate" type="text/html"/>
    <title>P &lt; NP In Our Stockings?</title>
    <summary>Brute force wins—sometimes Teespring.com source Santa Claus is on the way to visit those of us who have been good. Tonight is Christmas Eve, and we want to thank everyone who has been supporting Ken and me at GLL. Whether or not you believe in Santa Claus, whether or not you celebrate Xmas, we should […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Brute force wins—sometimes</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2020/12/24/p-np-in-our-stockings/tree-7/" rel="attachment wp-att-17929"><img alt="" class="alignright  wp-image-17929" src="https://rjlipton.files.wordpress.com/2020/12/tree.png?w=300" width="300"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Teespring.com <a href="https://teespring.com/shop/new-christmas-math-pi-tree-gif?aid=marketplace&amp;tsmac=marketplace&amp;tsmic=search&amp;pid=340&amp;cid=6412">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Santa Claus is on the way to visit those of us who have been good. </p>
<p/><p>
Tonight is Christmas Eve, and we want to thank everyone who has been supporting Ken and me at GLL.<br/>
<span id="more-17925"/></p>
<p>
Whether or not you believe in Santa Claus, whether or not you celebrate Xmas, we should agree that this has been some year. </p>
<p>
Indeed the the whole <a href="https://en.wikipedia.org/wiki/Santa_Claus">Santa Claus</a> issue raises some questions about truth telling. Children are typically told by their parents that Santa is real; tonight you can “watch” his progress as he travels around the world. Dr. Anthony Fauci even <a href="https://www.thedenverchannel.com/news/national/coronavirus/dr-fauci-reassures-children-says-he-personally-vaccinated-santa-claus">said</a> he traveled to the North Pole to give Santa the new Covid-19 vaccine.</p>
<p>
This raises some objections to presenting Santa Claus as a real person, rather than a story: </p>
<ul>
<li>
That lying is normally bad. <p/>
</li><li>
That parents intentionally lying to their children promotes distrust. <p/>
</li><li>
That it promotes selfishness, greed, and materialism. <p/>
</li><li>
That it associates good behavior with being materially rewarded with presents from Santa Claus. <p/>
</li><li>
That tricking children into believing falsehoods interferes with the development of critical thinking.
</li></ul>
<p>
Most agree that it is harmless, but see this for more on the <a href="https://en.wikipedia.org/wiki/Christmas_controversies#Rejection_among_certain_groups">controversy</a>. Ken finessed it by “superposing” the real Saint Nicholas on what told his children.</p>
<p>
Or read this <a href="https://www.amazon.com/Indisputable-Existence-Santa-Claus-Mathematics/dp/1468316125">book</a> on <i>The Indisputable Existence of Santa Claus: The Mathematics of Christmas</i> by Hannah Fry and Thomas Evans.</p>
<p>
</p><p/><h2> P <img alt="{\neq}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cneq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\neq}"/> NP </h2><p/>
<p/><p>
The belief that P is weaker than NP, that there is no efficient algorithm for many problems, is perhaps our version of Santa Claus. We cannot prove it, we cannot see how to even really approach it. </p>
<p>
But most of us believe that a resolution is <em>presently</em> out there, and that says there is no efficient algorithm for SAT and other NP-complete problems. Is this a story we tell each other like Santa Claus: Is P<img alt="{&lt;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{&lt;}"/>NP the same? Is it just a story we tell each other?</p>
<p>
</p><p/><h2> P <img alt="{=}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%3D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{=}"/> NP </h2><p/>
<p/><p>
Well not really: P is not equal to NP yet. But there are many important examples of hard problems that are solved every day. These problems are solved even though they are believed to be hard to solve.</p>
<p>
The algorithms that are used are brute force. The algorithms just try all the possible solutions and keep checking to see if this guess is correct. While complexity theory is filled with clever algorithms for hard problems that: </p>
<ul>
<li>
Solve special cases; <p/>
</li><li>
Give approximation solutions; <p/>
</li><li>
Solve some of the time; <p/>
</li><li>
And so on.
</li></ul>
<p>
Often only a perfect solution in the general case is what we sometimes needed. Two important examples of this are:</p>
<ol>
<li>
Bitcoin mining; <p/>
</li><li>
Password cracking.
</li></ol>
<p>
The first of these can be as precious as a diamond, but the latter is a lump of coal when it happens to us.</p>
<p>
</p><p/><h2> Password Cracking </h2><p/>
<p/><p>
A password is checked by a system by using a password file <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathsf{P}}"/> that consists of records like 	</p>
<p align="center"><img alt="\displaystyle  [name, h(name,p)]. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Bname%2C+h%28name%2Cp%29%5D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  [name, h(name,p)]. "/></p>
<p>A user who claims to be <img alt="{name}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bname%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{name}"/>, then proves this by giving an <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{w}"/> so that 	</p>
<p align="center"><img alt="\displaystyle  h(name,w) = h(name,p). " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++h%28name%2Cw%29+%3D+h%28name%2Cp%29.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  h(name,w) = h(name,p). "/></p>
<p>Here <img alt="{h(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{h(x)}"/> is hash function. This function is public and so is the file <img alt="{\mathsf{P}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BP%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\mathsf{P}}"/>. Well the file is not exactly public, but an attacker often does know the file. Note, the value <img alt="{name}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bname%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{name}"/> represents the name and other information that is used like <a href="https://en.wikipedia.org/wiki/Salt_(cryptography)">salt</a>. </p>
<p>
The hope is that it is hard to invert the hash function <img alt="{h(x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{h(x)}"/> and so an attacker cannot easily find a <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{w}"/>. The difficulty is that a brute force attack works when <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{p}"/> can be guessed. And this unfortunately is quite often. If we think Bob used the password “12345678,” then 	</p>
<p align="center"><img alt="\displaystyle  h(Bob,12345678) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++h%28Bob%2C12345678%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle  h(Bob,12345678) "/></p>
<p>will check correctly. </p>
<p>
Even with random passwords cracking them by brute force is <a href="https://en.wikipedia.org/wiki/Password_cracking">possible</a> </p>
<blockquote><p><b> </b> <em> Graphics processors can speed up password cracking by a factor of 50 to 100 over general purpose computers for specific hashing algorithms. As of 2011, available commercial products claim the ability to test up to 2,800,000,000 passwords a second on a standard desktop computer using a high-end graphics processor. Such a device can crack a 10 letter single-case password in one day. </em>
</p></blockquote>
<p/><p>
This is an amazing result. The algorithm that breaks passwords, the cracker, is just brute force. But the cleverness is all in the implementation. The cleverness is using GPU’s or FPGA’s to make the algorithm—brute force or not—run so fast. Here GPU’s are <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">graphics processors</a> and <a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a>‘s are field programmable arrays. Both of these devices are hardware that is well matched to computing the hash functions that are used. See this Hackaday <a href="https://hackaday.com/2020/05/15/all-your-passwords-are-belong-to-fpga/">item</a> for details.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Have a safe Xmas, and a safe rest of the year. Take care all.</p>
<p>
Dick and Ken.</p>
<p/></font></font></div>
    </content>
    <updated>2020-12-24T22:55:43Z</updated>
    <published>2020-12-24T22:55:43Z</published>
    <category term="Ideas"/>
    <category term="P=NP"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="#P"/>
    <category term="brute force"/>
    <category term="cracker"/>
    <category term="passwords"/>
    <category term="Santa"/>
    <category term="Xmas"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2021-01-02T08:37:42Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-1031792436674372594</id>
    <link href="https://blog.computationalcomplexity.org/feeds/1031792436674372594/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/12/slowest-sorting-algorithms.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1031792436674372594" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1031792436674372594" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/12/slowest-sorting-algorithms.html" rel="alternate" type="text/html"/>
    <title>Slowest Sorting Algorithms</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Radu Gigore <a href="https://twitter.com/rgrig/status/1342015488252063744">tweeted</a> "People are obsessed with finding the best algorithms. What about the worst?" So here's a Christmas gift that keeps giving, the slowest of sorting algorithms. </p><p>Before you read on, try to think of the slowest sorting algorithm. No fair spinning its wheels, sleeping or doing unrelated tasks. It should always make progress towards sorting. </p><p>Here are some <a href="https://www.geeksforgeeks.org/the-slowest-sorting-algorithms/">examples</a>, in particular bogosort that generates all permutations until it finds a sorted one. Takes n! time on average.</p><p>But we can do much worse. The following redicusort algorithm I got from Stuart Kurtz back in the 90's.</p><blockquote><p>Generate all permutations and then sort those permutations. The sort of the original permutation will be first on the list.</p></blockquote><p>The running time depends on the sorting algorithm you use after generating the permutations.</p><p>If you use bubblesort you get a (n!)<sup>2</sup> time algorithm. </p><p>If you use bogosort you get a (n!)! bound.</p><p>What if you just call redicusort recursively? The algorithm will never end. If you want to guarantee termination you need to bound the depth of the recursion. Choose something like an <a href="https://en.wikipedia.org/wiki/Ackermann_function">Ackermann function</a> to get a sorting algorithm that always makes progress but not primitively recursive. In general you can get a sorting algorithm that takes longer than any fixed computable function.</p></div>
    </content>
    <updated>2020-12-24T15:16:00Z</updated>
    <published>2020-12-24T15:16:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-01-01T08:23:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/189</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/189" rel="alternate" type="text/html"/>
    <title>TR20-189 |  Shadows of Newton polytopes | 

	Pavel Hrubes, 

	Amir Yehudayoff</title>
    <summary>We define the shadow complexity of a polytope P as the maximum number of vertices in a linear projection of $P$ to the plane. We describe connections to algebraic complexity and to parametrized optimization. We also provide several basic examples and constructions, and develop tools for bounding shadow complexity.</summary>
    <updated>2020-12-24T13:17:51Z</updated>
    <published>2020-12-24T13:17:51Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-01-02T08:37:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsmath.wordpress.com/?p=2300</id>
    <link href="https://tcsmath.wordpress.com/2020/12/23/itcs-2021-registration/" rel="alternate" type="text/html"/>
    <title>ITCS 2021 Registration</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Head to http://itcs-conf.org/ to register for ITCS 2021, being held virtually at the Simons Institute from January 6-8, 2021. One can find the list of accepted papers at the same site, and the schedule of talks will be posted shortly. Live talks will occur from 8:30AM-3:30PM PST, and longer versions of the talks will be … <a class="more-link" href="https://tcsmath.wordpress.com/2020/12/23/itcs-2021-registration/">Continue reading <span class="screen-reader-text">ITCS 2021 Registration</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Head to <a href="http://itcs-conf.org/">http://itcs-conf.org/</a> to register for ITCS 2021, being held <strong>virtually</strong> at the Simons Institute from January 6-8, 2021.  One can find the list of accepted papers at the same site, and the schedule of talks will be posted shortly.  Live talks will occur from 8:30AM-3:30PM PST, and longer versions of the talks will be available to stream before the conference.  Note that <em>registration is free!</em></p>



<p>If you would like to have the ability to be seen/heard during the talk sessions, you will need to check the corresponding box on the registration form.  Join us for the first virtual TCS conference of 2021!</p>



<p>James R. Lee, ITCS 2021 PC Chair</p></div>
    </content>
    <updated>2020-12-23T10:55:40Z</updated>
    <published>2020-12-23T10:55:40Z</published>
    <category term="Announcement"/>
    <author>
      <name>James</name>
    </author>
    <source>
      <id>https://tcsmath.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsmath.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsmath.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsmath.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsmath.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>some mathematics &amp; computation</subtitle>
      <title>tcs math</title>
      <updated>2021-01-02T08:37:14Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/12/22/postdoc-at-university-of-warwick-uk-apply-by-january-13-2021/</id>
    <link href="https://cstheory-jobs.org/2020/12/22/postdoc-at-university-of-warwick-uk-apply-by-january-13-2021/" rel="alternate" type="text/html"/>
    <title>Postdoc at University of Warwick, UK (apply by January 13, 2021)</title>
    <summary>I (Tom Gur) invite applications for a postdoc position, with generous travel support, flexible conditions, and full academic freedom. Relevant research interests include, but not limited to: complexity, sublinear-time algorithms, coding theory, PCP/IP, PAC learning, and quantum computing. To apply, please send me an email introducing yourself, and include your CV and 2-3 representative papers. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I (Tom Gur) invite applications for a postdoc position, with generous travel support, flexible conditions, and full academic freedom. Relevant research interests include, but not limited to: complexity, sublinear-time algorithms, coding theory, PCP/IP, PAC learning, and quantum computing.</p>
<p>To apply, please send me an email introducing yourself, and include your CV and 2-3 representative papers.</p>
<p>Website: <a href="https://www.dcs.warwick.ac.uk/~tomgur/">https://www.dcs.warwick.ac.uk/~tomgur/</a><br/>
Email: tom.gur@warwick.ac.uk</p></div>
    </content>
    <updated>2020-12-22T16:56:15Z</updated>
    <published>2020-12-22T16:56:15Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-01-02T08:37:46Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5209</id>
    <link href="https://www.scottaaronson.com/blog/?p=5209" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5209#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5209" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">The case for moving to a red state</title>
    <summary xml:lang="en-US">Update (Dec. 23): This post quickly attracted many of the most … colorful comments in this blog’s 15-year history. My moderation queue is overflowing right now with “gas the kikes,” “[f-word] [n-words],” “race war now,” “kikes deserve to burn in hell,” “a world without [n-words],” “the day of the rope approaches,” and countless similar contributions. […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong><span class="has-inline-color has-vivid-red-color">Update (Dec. 23):</span></strong> This post quickly attracted many of the most … colorful comments in this blog’s 15-year history.  My moderation queue is overflowing right now with “gas the kikes,” “[f-word] [n-words],” “race war now,” “kikes deserve to burn in hell,” “a world without [n-words],” “the day of the rope approaches,” and countless similar contributions.  One commenter focused on how hilarious he found my romantic difficulties earlier in life.</p>



<p>The puzzle, for me, is that I’d spent <em>years</em> denouncing Trump’s gleeful destruction of the country that I grew up believing in, using the strongest language I could muster.  So why am I only <em>now</em> getting all the hate-spam?</p>



<p>Then a possible explanation hit me: namely, the sort of person who’d leave such comments is <em>utterly impervious to moral condemnation</em>.  The only thing such a person cares about—indeed, as it turns out, feels a volcanic need to shout down—is someone articulating an actual plausible path to removing his resentment-fueled minority from power.  If this is right, then I’m proud to have hit a nerve. –SA</p>



<p/><hr/>



<ol><li>The US is now a failed democracy, with a president who’s considering <a href="https://www.cnn.com/2020/12/19/politics/trump-oval-office-meeting-special-counsel-martial-law/index.html">declaring martial law</a> to avoid conceding a lost election, and with the majority of his party eager to follow him arbitrarily far into the abyss.  Even assuming, as I do, that the immediate <em>putsch</em> will fail, the Republic will not magically return to normal.<br/></li><li>The survival of Enlightenment values on Earth now depends, in large part, on the total electoral humiliation and defeat of the forces that enabled Trump—something that the last election failed to deliver.<br/></li><li>Alas, ever since it absorbed the Southern racists in the 1960s, the Republican Party has maintained a grip on power wholly out of proportion to its numbers through anti-democratic means.  The most durable of these means are built into the Constitution itself: the Electoral College, the overrepresentation of sparsely-populated rural states in the Senate, and the gerrymandering of Congressional districts.  Every effort to fix these anachronisms, whether by legislation or Constitutional amendment, has been blocked for generations.  It’s fantasy to imagine the beneficiaries of these unjust advantages ever voluntarily giving them up.<br/></li><li>Accordingly, the survival of the nation might come down to whether enough Americans, in deep-blue areas like California and New York and Massachusetts, are willing to <em>pick up and move</em> to where their votes actually count.<br/></li><li>The pandemic has awoken tens of millions of people to the actual practical feasibility of working from home or in a different time zone from their employer.  The culture has finally caught up to the abridgment of distance that the Internet, smartphones, and videoconferencing achieved well over a decade ago.<br/></li><li>Still, one doesn’t expect Brooklynites to settle by the thousands on remote mountaintops.  And even if they did, there are <em>many</em> remote mountaintops, so the transplants’ power could be diluted to near nothing.  Better for the transplants to concentrate themselves in a few <a href="https://en.wikipedia.org/wiki/Focal_point_(game_theory)">Schelling points</a>: ideally, cities where they could both swing the national electoral calculus <em>and</em> actually want to live.<br/></li><li>There’s been a <a href="https://austonia.com/city/bay-area-austin-move">spate</a> of <a href="https://www.ktvu.com/news/silicon-valley-exodus-bay-area-tech-companies-leaving-for-texas?fbclid=IwAR0b3y6D8m0c1crdNDptyZaDNPeW6f01au_drfSmL7yEDgO7H-AlGUdJ1Uc">recent</a> <a href="https://www.techrepublic.com/article/silicon-valley-exodus-the-majority-of-professionals-said-theyd-follow-tech-leaders-to-emerging-tech-hubs/">articles</a> about the possible exodus of tech companies and professionals from the Bay Area, because of whatever combination of sky-high rents, NIMBYism, taxes, mismanagement, wildfires, blackouts, and the pandemic having removed the once-overwhelming reasons to be in the Bay.  Oft-mentioned alternatives include Miami, Denver, and of course my own adopted hometown of Austin, TX, where <a href="https://www.builtinaustin.com/2020/12/09/elon-musk-texas-move-tesla-austin-hiring">Elon Musk</a> and <a href="https://www.cnbc.com/2020/12/11/oracle-is-moving-its-headquarters-from-silicon-valley-to-austin-texas.html">Oracle</a> just announced they’re moving.<br/></li><li>If you were trying to optimize your environment for urban Blue-Tribeyness—indie music, craft beer, ironic tattoos, Bernie Sanders yard signs, etc. etc.—<em>but subject to living in an important red or purple state, where your vote could plausibly contribute to a historic political realignment of the US</em>—then you couldn’t do much better than Austin.  Where else is in the running?  Atlanta, Houston, San Antonio, Pittsburgh?<br/></li><li>It’s true that Texas is the state of <a href="https://www.expressnews.com/opinion/columnists/josh_brodesky/article/Brodesky-Stain-of-Paxton-s-lawsuit-will-be-15814916.php">Ken Paxton</a>, the corrupt and unhinged Attorney General who unsuccessfully petitioned the US Supreme Court to overturn Trump’s election loss.  But it’s also the state of MD Anderson, often considered the best oncology center on earth, and of Steven Weinberg, possibly the greatest living physicist.  It’s where the spike proteins of both the Pfizer and Moderna covid vaccines were developed.  It’s where <a href="https://en.wikipedia.org/wiki/Young_Sheldon">Sheldon Cooper grew up</a>—alright, he’s fictional, but I’ve worked with undergrads at UT Austin who almost <em>could’ve</em> been Sheldon.  Like the US as a whole, the state has potential.<br/></li><li>Accelerating the mass migration of blue Americans to cities like Austin isn’t <em>only</em> good for the country and the world.  The New Yorkers and San Franciscans left behind will thank the migrants for lower rents!<br/></li><li>But won’t climate change make Texas a living hell?  Alas, as recent wildfires and hurricanes remind us, there aren’t many places on earth that climate change <em>won’t</em> soon make various shades of hell.  At least Austin, like many red locales, is far inland.  For the summers, there are lots of swimming pools and lakes.<br/></li><li>If Austin gets overrun by Silicon Valley refugees, won’t they recreate whatever dysfunctional conditions caused them to flee Silicon Valley in the first place?  Maybe, eventually, but it would take quite a while.  One problem at a time!  And the “problems of Silicon Valley” are problems most places should desperately want.<br/></li><li>Is Texas winnable—or is a blue Texas like controlled nuclear fusion, forever a decade or two in the future?  Well, Trump’s 6-point margin in Texas this November, 3 points less than his margin in 2016, amounted to 630,000 votes out of 11.3 million cast.  Meanwhile, net migration to Texas over the past decade included 356,000 to Austin (growing its population by 20%), 687,000 to Dallas, 603,000 to Houston, 260,000 to San Antonio.  Let’s say we want two million more transplants.  The question is not whether they’re going to arrive but at what rate.<br/></li><li>Can the cities of Texas accommodate two million more people?  Well, traffic will get worse, rents will get higher … but the answer is an unequivocal yes.  Land, Texas has.<br/></li><li>Do the tech workers who I’d like to relocate even vote blue?  Given the unremitting scorn that the woke press now heaps on “racist, sexist, greedy Silicon Valley techbros,” it can be easy to forget this, but the answer to the question is: <em>yes, overwhelmingly, they do</em>.  Mountain View, CA, for example, <a href="https://mv-voice.com/news/2020/11/09/election-recap-mountain-view-swings-left-but-rejects-statewide-changes-to-rent-control?fbclid=IwAR17en2fsShjdcJREPbkSszEwU4Yuf1j2yU041-44Jn6ga0-_9B5vE4ylWM">went</a> 83% Biden and only 15% Trump in November.<br/></li><li>Even if everything I’ve said is obvious, in order for the Great Red-State Tech-Worker Migration happen at the rate I want, it needs to become <a href="https://www.scottaaronson.com/blog/?p=2410">common knowledge</a> that it’s happening—not merely known but known to be known, and so forth.  Closely related, it needs to become a serious <strong>status symbol</strong> for any blue-triber to relocate to a contested state.  (“You’re moving to Georgia to help save the Republic?  <em>And</em> you’ll be able to afford a four-bedroom house?  I’m <em>so</em> jealous!”)<br/></li><li>This has been the real purpose of this post: to make it clear that, if you help settle the wild frontier like my family did, then a tiny bit of the unattainable coolness of a stuttering quantum complexity theory blogger/professor could rub off on <em>you</em>.<br/></li><li>Think about it this way.  Many of our grandparents gave their lives to save the world from fascism.  Would you have done the same in their place?  OK now, what if you didn’t have to lose your life: you only had to live in Austin or Miami?<br/></li><li>If this post plays a role in any like-minded reader’s decision to move to Austin, then once covid is over, they should tell me to redeem a personal welcome celebration from me and Dana.  We’ll throw some extra brisket on the barbie.</li></ol>



<p/></div>
    </content>
    <updated>2020-12-22T08:05:19Z</updated>
    <published>2020-12-22T08:05:19Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-01-02T02:18:43Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://gradientscience.org/unadversarial/</id>
    <link href="https://gradientscience.org/unadversarial/" rel="alternate" type="text/html"/>
    <title>Unadversarial Examples: Designing Objects for Robust Vision</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a class="bbutton" href="https://arxiv.org/abs/2012.12235" style="float: left; width: 45%;">
<i class="fas fa-file-pdf"/>
    Paper
</a>
<a class="bbutton" href="https://git.io/unadversarial" style="float: left; width: 45%;">
<i class="fab fa-github"/>
   Code
</a>
<!-- <a class="bbutton" href="/breeds_class_hierarchy">
<i class="fa fa-tree"></i>
&nbsp;&nbsp; Hierarchies
</a> -->
<br/></p>


<div class="footnote">
    A video demo of an <em>unadversarial patch</em> for garbage trucks.
    Placing the patch on misclassified garbage trucks significantly
    improves performance and robustness.
</div>

<p>Modern computer vision systems are sensitive to transformations and corruptions
of their inputs, making reliability and safety real concerns at deployment time.
For example, fog or snow can severely impact self-driving cars’ perception
systems; in an <a href="https://gradientscience.org/background">earlier post</a>, we found that even changes in image
background are often enough to cause model misbehavior. Techniques like data
augmentation, domain randomization, and robust training can all improve
robustness, but tend to perform poorly in the face of the unfamiliar
distribution shifts that computer vision systems encounter in the wild.</p>

<h2 id="how-can-we-make-classifiers-more-robust">How can we make classifiers more robust?</h2>

<p>So far, most approaches to robust machine learning have focussed on better model
training methods. However, there is more than one way to peel a banana. Instead
of tackling the general problem of designing universally robust vision
algorithms, we leverage an additional degree of freedom present in many
real-world scenarios. Specifically, we observe that in many situations, the
system designer not only trains the model used to make predictions, but also has
the ability to modify objects of interest. Critically, such a system designer could
modify objects to improve the model’s performance! For example, a drone
operator training a landing pad detection model can modify the landing pad
surface for easier detection, and a roboticist manipulating custom objects could
alter objects’ texture or geometry to assist with perception. Indeed, a similar
insight motivates QR codes, which are patterns explicitly designed to encode
easily recoverable bits in photographs. In our work, we ask: can we reliably
create objects that <em>augment</em>Note that QR codes don't
really fit the bill here, since they require their own decoder. (For
completeness, we still tested QR codes in our paper and
found them not robust to distribution shift.) the
performance of existing machine learning models on vision tasks like
classification or regression?</p>

<h2 id="designing-objects-for-robust-vision">Designing objects for robust vision</h2>
<p>Research on <a href="https://gradientscience.org/intro_adversarial">adversarial examples</a> has demonstrated that modern vision
models can be extraordinarily sensitive to small changes in input: carefully
chosen yet imperceptible perturbations of natural images can cause models to
make wildly inaccurate predictions. In particular, previous work has shown that
we can perturb objects’ textures so that they induce misclassification when
photographed <a href="https://arxiv.org/abs/1607.02533">[Kurakin et al.]</a>. Indeed, 
<a href="https://arxiv.org/abs/1707.07397">Athalye et al.</a> 3D print a textured 
turtle that looks like a turtle, but classifies as a rifle when shown to a
classifier!</p>

<p>So, given that we can make objects <em>fool</em> ML models, can we also make them <em>help</em>?</p>

<p>The answer is yes! In our work, we turn the same oversensitivity behind
adversarial examples into a tool for robustly solving vision tasks.
Specifically, instead of optimizing inputs to mislead models (as in adversarial
attacks), we optimize inputs to reinforce correct behavior, yielding what we
refer to as <em>unadversarial examples</em> or <em>robust objects</em>.</p>

<p>We demonstrate that even a simple gradient-based algorithm can successfully
construct unadversarial examples for a variety of vision settings. In fact, by
optimizing objects for vision systems (rather than only vice-versa) we can
greatly improve performance on both in-distribution data and previously unseen
out-of-distribution data. For example, as illustrated below, optimizing the
texture of a 3D-modelled jet enables the image classifier to classify that jet
more robustly under various weather conditions, despite never being exposed to
such conditions in training:</p>

<p><img alt="Example of unadversarial example (jet)" src="https://gradientscience.org/assets/unadversarial/headline_fig.png"/></p>
<div class="footnote">
    In our work, we demonstrate that optimizing objects (e.g., the
    pictured jet) for pre-trained neural networks can significantly boost
    performance and robustness on computer vision tasks.
</div>

<p>The model correctly classifies both the original jet and its unadversarial
counterpart in standard conditions, but recognizes only the unadversarial jet in
foggy and dusty conditions.</p>

<h2 id="constructing-unadversarial-examples">Constructing unadversarial examples</h2>
<p>Our paper proposes two ways of designing robust objects: unadversarial
stickers (patches), and unadversarial textures:</p>

<p><img alt="Example of unadversarial patch and unadversarial texture" src="https://gradientscience.org/assets/unadversarial/patch_vs_texture.png"/></p>
<div class="footnote">
    Examples of the two considered methods for constructing unadversarial objects.
</div>

<p>As with adversarial examples, we construct both unadversarial patches and
textures via gradient descent. In both cases, we need a pre-trained vision model
to optimize objects for:</p>

<ul>
  <li><strong>Unadversarial patches</strong>: To construct/train unadversarial patches, we repeatedly
sample natural images from a given dataset, and place a patch onto the sampled
image (with random orientation and position) corresponding to the label of the
image. We then feed the augmented image into the pre-trained vision model, and
use gradient descent to minimize the standard classification loss of the model
with respect to the patch pixels. The procedure is almost identical to that of
<a href="https://arxiv.org/abs/1712.09665">Brown et al.</a> for constructing <em>adversarial</em>
patches; the main difference is that here we minimize, rather than maximize,
the loss of the pre-trained model.</li>
  <li><strong>Unadversarial textures</strong>: To construct adversarial textures, we start with a 3D
mesh of the object we would like to optimize, as well as a dataset of
background images. At each iteration, we render the current unadversarial
texture onto the 3D mesh, and overlay the rendering onto a random background
image. The result is fed to the pre-trained classifier, and we (again) use
gradient descent to minimize the classification loss with respect to the
texture. In previous work, <a href="https://arxiv.org/abs/1707.07397">Athalye et al.</a>
use an extremely similar approach to optimize <em>adversarial</em> textures (the most
famous example of which being the 
<a href="https://www.labsix.org/physical-objects-that-fool-neural-nets/">rifle turtle</a>).</li>
</ul>

<h2 id="evaluating-unadversarial-examples">Evaluating unadversarial examples</h2>
<p>After designing our unadversarial examples, we evaluate them in two ways. First,
we want to ensure that classifiers still maintain (or improve on) high accuracy
levels on standard benchmarks augmented with unadversarial examples. Second, we
want to understand the extent to which patches confer robustness to
<em>out-of-distribution</em> samples—how well do models perform on patches under
distribution shifts, despite such shifts never being considered in training?</p>

<p>To answer these questions, we first test our methods on standard datasets
(ImageNet and CIFAR) and robustness benchmarks (CIFAR10-C, ImageNet-C). It turns
out that adding unadversarial patches to ImageNet dataset boosts the ImageNet
accuracy as well as the robustness of a pretrained ResNet-18 model under various
corruptions!</p>

<p><img alt="Performance on ImageNet and ImageNet-C" src="https://gradientscience.org/assets/unadversarial/benchmark_performance.png"/></p>
<div class="footnote">
Accuracy on (a) clean ImageNet images and (b) synthetically corrupted
ImageNet-C images as a function of patch size (given as a percentage of image
area). In (b), each bar denotes the average accuracy over the five severities
in ImageNet-C, and the horizontal dashed lines report the accuracy on the
original (non-patched) datasets.
</div>

<p>In our <a href="https://arxiv.org/abs/2012.12235">paper</a>,
we also compare to some non-optimization based baselines,
like QR code-based augmentation and predetermined patches; unadversarial patches
comfortably outperform all of the baselines we tested.</p>

<h2 id="how-do-robust-objects-fare-in-more-realistic-settings">How do robust objects fare in more realistic settings?</h2>
<p>The promising results we observe on standard classification and synthetic
corruption benchmarks motivate us to consider more realistic tasks. To this end,
we extend our evaluation to consider unadversarial examples in three additional
settings: (a) classifying objects in a high-fidelity renderer, (b) localizing
objects in a drone-landing simulator, and (c) recognizing objects in the real
world. In the first two (simulated) settings, we optimize over objects’ textures
directly and simulate weather conditions in the renderings themselves. In the
final setting, we print out patches designed for ImageNet and study how well
they aid classification when photographed on real world objects.</p>

<h3 id="a-recognizing-objects-in-a-high-fidelity-simulator">A) Recognizing Objects in a High-Fidelity Simulator</h3>
<p>We first test how well unadversarial examples aid recognition of
three-dimensional objects in a high-fidelity simulator. Generating unadversarial
textures for objects corresponding to ImageNet classes, such as “warplane” and
“automobile,” we then use Microsoft AirSim to evaluate how well the
unadversarial textures help classification in both standard and severe weather
conditions—like fog and snow (note that in AirSim these weather conditions are
explicitly simulated in the 3D scene, and not applied through image
post-processing). We observe that our unadversarial models are much more
recognizable than their original counterparts by the pre-trained ImageNet model,
especially under distribution shift. For example, our unadversarial jet beats
the baselines “Bright Jet” and “Dark Jet,” textures manually designed with the
goal of visibility in severe weather) in a variety of weather conditions and
severities:</p>

<p><img alt="Classification of a standard and unadversarial jet in simulation" src="https://gradientscience.org/assets/unadversarial/jetplots.png"/></p>
<div class="footnote">
The jet unadversarial example task. We show example conditions under which we
evaluate the objects, along with aggregate statistics for how well an
ImageNet classifier classifies the objects in different conditions.
</div>

<h3 id="b-localization-for-simulated-drone-landing">B) Localization for (Simulated) Drone Landing</h3>
<p>We then take the realism of our simulations a step further by training
unadversarial patches for a simulated drone landing task. The drone is equipped
with a pre-trained regression model that localizes the landing pad. To improve
localization, we place an unadversarial patch on the pad and optimize it by
minimizing a squared error loss (instead of a classification loss as in the
previous experiments) corresponding to the drone’s error in predicting landing
pad location. We then evaluate the effectiveness of the patch by examining  the
landing success rate in clear, moderate, and severe (simulated) weather
conditions. As shown below, we find that the unadversarial landing pad improves
landing success rate across all conditions:</p>

<p><img alt="Simulated drone landing with unadversarial landing pad" src="https://gradientscience.org/assets/unadversarial/landing.png"/></p>
<div class="footnote">
Drone landing task. On the left we show the unadversarial versus standard landing pads. On the right we show the results for the task when both the standard and unadversarial landing pads are used.
</div>

<h3 id="c-physical-world-unadversarial-examples">C) Physical World Unadversarial Examples</h3>
<p>Finally, we study unadversarial examples’ generalization to the physical world.
Printing out unadversarial patches, we place them on top of real-world objects
and classify these object-patch pairs along a diverse set of viewpoints and
object orientations. Our results are detailed below; we find that the
unadversarial patches consistently improve performance despite printing
artifacts, lighting conditions, partial visibility, and other naturally-arising
distribution shifts.</p>

<p><img alt="Physical-world unadversarial examples" src="https://gradientscience.org/assets/unadversarial/physical_world_results.png"/></p>
<div class="footnote">
Physical-world experiments. We take pictures of objects at diverse orientations while varying the presence of a patch on the object. Note that we don't do any additional data augmentation on the patches, which are the same used in our previous ImageNet benchmark experiment.
</div>

<h2 id="conclusions">Conclusions</h2>
<p>We have shown that it is possible to design objects that improve the performance
of computer vision models, even under strong and unforeseen corruptions and
distribution shifts. By minimizing the standard loss of a pre-trained model,
without training for robustness to anything specific, we significantly boosted
both model performance and model robustness across a variety of tasks and
settings. Our results suggest that designing unadversarial inputs could be a
promising route towards increasing reliability and out-of-distribution
robustness of computer vision models.</p></div>
    </summary>
    <updated>2020-12-22T00:00:00Z</updated>
    <published>2020-12-22T00:00:00Z</published>
    <source>
      <id>https://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="https://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="https://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2021-01-01T22:43:47Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://kamathematics.wordpress.com/?p=205</id>
    <link href="https://kamathematics.wordpress.com/2020/12/21/soda-2021-funds-for-student-registration-fees/" rel="alternate" type="text/html"/>
    <title>SODA 2021: Funds for Student Registration Fees</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Please see below for a message from Shang-Hua Teng, regarding the possibility of waivers for SODA 2021 registration for students. Dear TCS students: By now, it is hard to overestimate the impact of the COVID19 pandemic to society. However, like every challenge, it has created some opportunities. For example, essentially all major conferences in TCS … <a class="more-link" href="https://kamathematics.wordpress.com/2020/12/21/soda-2021-funds-for-student-registration-fees/">Continue reading<span class="screen-reader-text"> "SODA 2021: Funds for Student Registration Fees"</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Please see below for a message from Shang-Hua Teng, regarding the possibility of waivers for SODA 2021 registration for students.</p>



<hr class="wp-block-separator"/>



<p>Dear TCS students:<br/><br/>By now, it is hard to overestimate the impact of the COVID19 pandemic to society. However, like every challenge, it has created some opportunities. For example, essentially all major conferences in TCS this year have been transformed into virtual ones, making them more accessible to scholars/students across the world (of course at the expense of traditional interactions). <br/><br/>ACM-SIAM Symposium on Discrete Algorithms (SODA21) will be held virtually this year, on Jan. 10 – 13, 2021. As you may know, this is the premier conference on algorithms .<br/><br/>See <a href="https://www.siam.org/conferences/cm/conference/soda21" rel="noreferrer noopener" target="_blank">https://www.siam.org/conferences/cm/conference/soda21</a><br/><br/>Thanks to our industry partners and ACM SIGACT group, SODA has some funds for covering student registrations. I am writing to informing you this opportunity and encourage you to apply:<br/> See: <br/>1. <a href="https://awards.siam.org/" rel="noreferrer noopener" target="_blank">https://awards.siam.org/</a> <br/>2. <a href="https://www.siam.org/conferences/cm/lodging-and-support/travel-support/soda21-travel-support" rel="noreferrer noopener" target="_blank">https://www.siam.org/conferences/cm/lodging-and-support/travel-support/soda21-travel-support</a><br/>That deadline is Dec. 27, 2020. Like before, having papers in SODA is not prerequisite.<br/><br/>Shang-Hua Teng<br/>On Behalf of SODA Steering Committee<br/></p></div>
    </content>
    <updated>2020-12-21T19:25:28Z</updated>
    <published>2020-12-21T19:25:28Z</published>
    <category term="Events"/>
    <author>
      <name>Gautam</name>
    </author>
    <source>
      <id>https://kamathematics.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://kamathematics.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://kamathematics.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://kamathematics.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://kamathematics.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Kamathematics</title>
      <updated>2021-01-02T08:38:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=20677</id>
    <link href="https://gilkalai.wordpress.com/2020/12/21/to-cheer-you-up-in-difficult-times-15-yuansi-chen-achieved-a-major-breakthrough-on-bourgains-slicing-problem-and-the-kannan-lovasz-and-simonovits-conjecture/" rel="alternate" type="text/html"/>
    <title>To Cheer You Up in Difficult Times 15: Yuansi Chen Achieved a Major Breakthrough on Bourgain’s Slicing Problem and the Kannan, Lovász and Simonovits Conjecture</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">This post gives some background to  a recent breakthrough  paper: An Almost Constant Lower Bound of the Isoperimetric Coefficient in the KLS Conjecture by Yuansi Chen. Congratulations Yuansi! The news Yuansi Chen gave an almost constant bounds for Bourgain’s 1984 … <a href="https://gilkalai.wordpress.com/2020/12/21/to-cheer-you-up-in-difficult-times-15-yuansi-chen-achieved-a-major-breakthrough-on-bourgains-slicing-problem-and-the-kannan-lovasz-and-simonovits-conjecture/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p/><div class="wp-caption alignnone" id="attachment_20687" style="width: 310px;"><a href="https://gilkalai.files.wordpress.com/2020/12/yuansi.jpg"><img alt="" class="size-medium wp-image-20687" height="225" src="https://gilkalai.files.wordpress.com/2020/12/yuansi.jpg?w=300" width="300"/></a><p class="wp-caption-text" id="caption-attachment-20687"><span style="color: #ff0000;"><strong>Yuansi Chen</strong></span></p></div><p/>
<h3>This post gives some background to  a recent breakthrough  paper: <a href="https://arxiv.org/abs/2011.13661">An Almost Constant Lower Bound of the Isoperimetric Coefficient in the KLS Conjecture</a> by <a href="https://people.math.ethz.ch/~chenyua/">Yuansi Chen</a>. <span style="color: #0000ff;">Congratulations Yuansi! </span></h3>
<h2>The news</h2>
<p>Yuansi Chen gave an almost constant bounds for Bourgain’s 1984 slicing problem and for the Kannan-Lovasz-Simonovits 1995 conjecture. In this post I will describe these conjectures.</p>
<p>Unrelated cheerful news: Here is <a href="https://www.quantamagazine.org/a-mathematicians-adventure-through-the-physical-world-20201216/">a very nice Quanta article</a> by Kevin Hartnett on Lauren Williams, the positive Grassmannian and connections to physics. (See <a href="https://gilkalai.wordpress.com/2015/02/16/the-simplex-the-cyclic-polytope-the-positroidron-the-amplituhedron-and-beyond/">this related 2015 post</a>.)</p>
<h2>Bourgain’s slicing problem</h2>
<p><strong>Bourgain’s slicing problem (1984):</strong>  Is there <em>c &gt; 0</em> such that for any dimension n and any centrally symmetric convex body <em>K ⊆</em> <img alt="\mathbb R^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="\mathbb R^n"/> of volume one, there exists a hyperplane <em>H</em> such that the<em> (n − 1)-</em>dimensional volume of <em>K ∩ H</em> is at least <em>c</em>?</p>
<p>Vitali Milman <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwiEobqGkNrtAhUJ3IUKHeerBUMQFjAAegQIAhAC&amp;url=https%3A%2F%2Fwww.weizmann.ac.il%2Fmath%2Fklartag%2Fsites%2Fmath.klartag%2Ffiles%2Fuploads%2Fbourgain_slicing_problem.pdf&amp;usg=AOvVaw3Rq7Rw9I4wbSUIyTUCAn08">wrote</a>: <span style="color: #0000ff;">“I was told once by Jean that he had spent more time on this problem and had dedicated more efforts to it than to any other problem he had ever worked on.”  </span></p>
<p>A positive answer to the problem is sometimes referred to as the hyperplane conjecture. Bourgain himself proved in the late 1980s that  the answer is yes if <img alt="c=1/n^{1/4}\log n" class="latex" src="https://s0.wp.com/latex.php?latex=c%3D1%2Fn%5E%7B1%2F4%7D%5Clog+n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="c=1/n^{1/4}\log n"/> and twenty years later Boaz Klartag shaved away the log<em>n </em>factor. For more on the problem see <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwiEobqGkNrtAhUJ3IUKHeerBUMQFjAAegQIAhAC&amp;url=https%3A%2F%2Fwww.weizmann.ac.il%2Fmath%2Fklartag%2Fsites%2Fmath.klartag%2Ffiles%2Fuploads%2Fbourgain_slicing_problem.pdf&amp;usg=AOvVaw3Rq7Rw9I4wbSUIyTUCAn08">this article by Boaz Klartag and Vitali Milman</a> and for the history see the moving foreword  by Vitali Milman.</p>
<p>Yuansi Chen’s result (combined with an earlier result of Klartag and Ronen) asserts that c can be taken as <img alt="n^{-o(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=n%5E%7B-o%281%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="n^{-o(1)}"/>.</p>
<h2>Computing the volume of convex sets and the 1995 Kannan-Lovasz-Simonovits conjecture</h2>
<p>Kannan, Lovász and Simonovits (KLS) conjectured  in 1995 that for any distribution that is<br/>
log-concave, the Cheeger isoperimetric coefficient equals to that achieved by half-spaces up to a universal constant factor.  The crucial point here is that the constant does not depend on the dimension.</p>
<p>Let <em>K</em> be a convex body of volume one in <img alt="\mathbb R^n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5En&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="\mathbb R^n"/> and consider the uniform probability distribution on <em>K</em>. Given a subset <em>A</em> of <em>K</em> of volume <em>t≤1/2</em> we can ask what is the surface <em>S(A)</em> area of <img alt="A \cap int K" class="latex" src="https://s0.wp.com/latex.php?latex=A+%5Ccap+int+K&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="A \cap int K"/>. (We don’t want to consider points in the surface of <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="K"/> itself.)   Let <img alt="f_K(t)" class="latex" src="https://s0.wp.com/latex.php?latex=f_K%28t%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="f_K(t)"/> be the minimum value of <em>S(A)</em> for subsets of volume <em>t</em>. The Cheeger constant or the expansion constant is the minimum value of <em>f(t)/t. </em>The KAS conjecture asserts that up to a universal constant independent from the dimension the Cheeger constant is realized by separating <em>K </em>with a hyperplane! (The conjecture is also called the Kannan, Lovász and Simonovits hyperplane conjecture.)</p>
<p>The motivation for the KMS conjecture came from a major 1990 breakthrough in the theory of algorithms. Dyer, Frieze, and Kannan found a polynomial-time algorithm to compute the volume of a convex set K in <img alt="\mathbb R^d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+R%5Ed&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="\mathbb R^d"/>.  Kannan, Lovász and Simonovich (later joined by Santosh Vampala) wrote a series of ingenious papers where they improved the exponents of the polynomial and in the process introduced new methods for proving rapid mixing of Markov chains and new Euclidean isoperimetric results and conjectures.</p>
<p>As it turned out the KLS conjecture implies Bourgain’s hyperplane conjecture and also the “thin-shell conjecture” that  I will not describe here.</p>
<p>Yuansi Chen’s proved  an almost constant lower bound <img alt="d^{o(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=d%5E%7Bo%281%29%7D&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" title="d^{o(1)}"/> of the isoperimetric coefficient in the KLS conjecture!</p>
<p>This also gives faster mixing time bounds of Markov chain Monte Carlo (MCMC) sampling algorithms on log-concave measures.</p>
<h2><a href="http://www.wisdom.weizmann.ac.il/~ronene/">Ronen Eldan</a>‘s  stochastic localization scheme and the strategy <span class="d2edcug0 hpfvmrgz qv66sw1b c1et5uql rrkovp55 a8c37x1j keod5gw0 nxhoafnm aigsh9s9 d3f4x2em fe6kdd0r mau55g9w c8b282yb iv3no6db jq4qci2q a3bd9o3v knj5qynh oo9gr5id" dir="auto">proposed by Yin Tat Lee and SantoshVempala</span></h2>
<p>Yuansi Chen’s proof relies on <a href="http://www.wisdom.weizmann.ac.il/~ronene/">Ronen Eldan</a>‘s  stochastic localization scheme that was introduced in this 2013 paper</p>
<p class="title mathjax">Ronen Eldan, <a href="https://arxiv.org/abs/1203.0893">Thin shell implies spectral gap up to polylog via a stochastic localization scheme</a></p>
<p>It follows a strategy laid by Yin Tat Lee and Santosh Vempala 2017 FOCS paper</p>
<p>Y. T. Lee and S. S. Vempala,  <a href="https://arxiv.org/abs/1612.01507">Eldan’s stochastic localization and the KLS hyperplane conjecture: an improved lower bound for expansion.</a></p>
<p>There is not much more I can tell you now about Ronen’s theory, about <span class="d2edcug0 hpfvmrgz qv66sw1b c1et5uql rrkovp55 a8c37x1j keod5gw0 nxhoafnm aigsh9s9 d3f4x2em fe6kdd0r mau55g9w c8b282yb iv3no6db jq4qci2q a3bd9o3v knj5qynh oo9gr5id" dir="auto">Yin Tat and Santosh’s </span>strategy, about Yuansi’s proof, and about further intermediate results. (<a href="https://gilkalai.wordpress.com/2020/02/28/remarkable-new-stochastic-methods-in-abf-ronen-eldan-and-renan-gross-found-a-new-proof-for-kkl-and-settled-a-conjecture-by-talagrand/">This post here might be somewhat related to Ronen’s method</a>.) But let me very briefly mention a few related matters.</p>
<ol>
<li>There are interesting negative results regarding volume computations, see the 1987 <a href="https://link.springer.com/article/10.1007/BF02187886">Barany Furedi’s paper</a> and <a href="https://arxiv.org/abs/0903.2634">Ronen Eldan’s 2009 paper</a></li>
<li>It is a very interesting question (that I heard from Moshe Vardi) to understand the connection between practice and theory regarding Markov chain Monte Carlo<br/>
(MCMC) sampling algorithms e.g. for approximating permanents and volumes.</li>
<li>Related topics are: <a href="https://en.wikipedia.org/wiki/Dvoretzky%27s_theorem">Dvoretzky’s theorem</a>, Milman’s theorem, isotropic position, the<a href="https://en.wikipedia.org/wiki/Concentration_of_measure"> concentration of measure phenomenon,</a> asymptotic convex geometry.</li>
</ol>
<h3>The Busseman-Petty problem</h3>
<p><a href="https://en.wikipedia.org/wiki/Busemann%E2%80%93Petty_problem">The Busseman-Petty problem</a> from 1956 asks whether it is true that a symmetric <a href="https://en.wikipedia.org/wiki/Convex_body" title="Convex body">convex body</a> with larger central hyperplane sections has a larger volume. So the question was  if <em>K</em> and <em>L</em> are centrally symmetric problems and all hyperplane sections of <em>K</em> (through the origin)  have a larger volume than those of L, is it true that the volume of <em>K</em> is larger than that of L.  Busemann and Petty showed that the answer is positive if <i>K</i> is a ball. (A positive answer when L is a ball would have given a very strong version of the slicing conjecture.) But the answer is negative for dimensions larger than or equal to 5.</p>
<p>Here is a brief history:</p>
<p>Larman and Rogers constructed a counterexample in dimension 12 in their example L was a ball and K was a perturbation of a ball. Ball showed that taking L to be a ball and K the standard cube gives a counterexample in dimension 10;  <span class="text">Giannopoulos and</span> Bourgain gave counterexamples in dimension 7   and Papadimitrakis finally found a counterexample in dimension 5. Gardner gave a  positive for answer for dimensions 3  Zhang gave a positive solution for dimension 4. Erwin Lutwak’s theory of intersection bodies plays an important role in the solution.  Gardner, Koldobskiy, and Schlumprecht presented a unified solution in all dimensions.</p>
<p>This is a really beautiful story and was saw a slice of the story in our <a href="https://gilkalai.wordpress.com/2008/12/09/test-your-intuition2/">second “test your intuition”</a>.  In the late 90s I had a “what is new” corner on my homepage and I remember devoting it to some of these developments.</p>
<p>Sources: Gardner’s book “<a href="https://www.cambridge.org/core/books/geometric-tomography/FA1A47CCEF63CEBA228520A57294CF7A">Geometric tomography</a>“, and Koldobskiy’s book <a href="https://books.google.co.il/books/about/Fourier_Analysis_in_Convex_Geometry.html?id=tlXzBwAAQBAJ&amp;printsec=frontcover&amp;source=kp_read_button&amp;redir_esc=y#v=onepage&amp;q&amp;f=false">“Fourier analysis in convex geometry”.</a></p>
<p>Here are<a href="https://gilkalai.files.wordpress.com/2020/12/talk-busemannpetty.pdf"> very nice slides from a lecture of Alexandr Koldobskiy</a> on the problem that also contain some information on the slicing problem.</p>
<p><a href="https://simons.berkeley.edu/workshops/counting2016-1">2016 meeting: Approximate Counting, Markov Chains and Phase Transitions</a>;  Series of 2019 lectures in Atlanta on related matters. ( <a href="https://mediaspace.gatech.edu/esearch/search?keyword=Koldobsky">Koldobsky</a> <a href="https://mediaspace.gatech.edu/media/0_iwzhee9l">I</a>,<a href="https://mediaspace.gatech.edu/media/0_mvtjxzhg">II</a>,<a href="https://mediaspace.gatech.edu/media/1_ep4j7so2">III</a> ; <a href="https://mediaspace.gatech.edu/esearch/search?keyword=Elisabeth%20Werner">Werner;</a> <a href="https://mediaspace.gatech.edu/esearch/search?keyword=Paouris">Paouris</a>)</p></div>
    </content>
    <updated>2020-12-21T16:55:06Z</updated>
    <published>2020-12-21T16:55:06Z</published>
    <category term="Combinatorics"/>
    <category term="Computer Science and Optimization"/>
    <category term="Convexity"/>
    <category term="Geometry"/>
    <category term="Yuansi Chen"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2021-01-02T08:37:37Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2020/188</id>
    <link href="https://eccc.weizmann.ac.il/report/2020/188" rel="alternate" type="text/html"/>
    <title>TR20-188 |  Hard QBFs for Merge Resolution | 

	Olaf Beyersdorff, 

	Joshua Blinkhorn, 

	Meena Mahajan, 

	Tomáš Peitl, 

	Gaurav Sood</title>
    <summary>We prove the first proof size lower bounds for the proof system Merge Resolution (MRes [Olaf Beyersdorff et al., 2020]), a refutational proof system for prenex quantified Boolean formulas (QBF) with a CNF matrix. Unlike most QBF resolution systems in the literature, proofs in MRes consist of resolution steps together with information on countermodels, which are syntactically stored in the proofs as merge maps. As demonstrated in [Olaf Beyersdorff et al., 2020], this makes MRes quite powerful: it has strategy extraction by design and allows short proofs for formulas which are hard for classical QBF resolution systems.

Here we show the first exponential lower bounds for MRes, thereby uncovering limitations of MRes. Technically, the results are either transferred from bounds from circuit complexity (for restricted versions of MRes) or directly obtained by combinatorial arguments (for full MRes). Our results imply that the MRes approach is largely orthogonal to other QBF resolution models such as the QCDCL resolution systems QRes and QURes and the expansion systems ?Exp+Res and IR.</summary>
    <updated>2020-12-20T11:56:21Z</updated>
    <published>2020-12-20T11:56:21Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-01-02T08:37:33Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-985389002707236859</id>
    <link href="https://blog.computationalcomplexity.org/feeds/985389002707236859/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/12/dr-jill-biden.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/985389002707236859" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/985389002707236859" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/12/dr-jill-biden.html" rel="alternate" type="text/html"/>
    <title>Dr Jill Biden</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>(I was helped on this post by Gorjan Alagic, Andrew Childs,  Tom Goldstein, Daniel Gottsman, Clyde Kruskal,  Jon Katz. I emailed them for their thoughts on the issue and some of those thoughts are embedded in the post. ) </p><p>The First First Lady to have a college degree was Lucy Hayes (Rutherford B Hayes was Prez 1876-1880). Nickname: Lemonade Lucy since she did not serve alcohol. </p><p>Trivia: who was the last first lady to not have a college degree? I'll answer that one at the end of this post. </p><p>The First First Lady to keep her day job was Abigail Fillmore who was a teacher. (Millard Fillmore was Prez in 1850-1852. He became prez  after Zachary Taylor died in office) . </p><p>In recent times it is  uncommon for a first lady to have a day job. So much so that it was notable when Elizabeth Dole said that if her husband (Bob Dole) won in 1996 she would keep her job at the Red Cross. </p><p>For other first lady firsts  see <a href="https://en.wikipedia.org/wiki/List_of_United_States_First_Lady_firsts#:~:text=Grace%20Coolidge,-Main%20article%3A%20Grace&amp;text=First%20first%20lady%20to%20earn%20a%20four%2Dyear%20undergraduate%20degree.">here</a>.</p><p>Jill Biden is the First First Lady to have a PhD. (ADDED LATER- one of the comments pointed out that she has an Ed. D, Doctor of Education.)   She says she will keep her day job as a professor.  Four other First ladies had advanced degrees: Pat Nixon (MS in Education), Laura Bush (MS in Library Science), Hillary Clinton (Law degree), Michelle Obama (Law degree). If I missed any, let me know. </p><p>The WSJ had an op-ed  that said Jill Biden should not call herself `Dr'.  Inspired by that, here are thoughts on the use of the word `Dr'</p><p>1) At the 1986 Structures Conference (now Complexity Conference ) Lane Hemachandra (now Lane Hemaspaandra) gave a talk. He had just gotten his PhD a few weeks ago, so he was introduced as `DOCTOR Lane Hemchandra' Gee, most of the talks were by people with PhD's but were not introduced as such.</p><p>2) Most people I know within academia do not call themselves Dr since it sounds pretentious. However, speaking in public about an issue one might want to use `Dr' to signal that you...know stuff. However, it would be odd for a PhD in (say) linguistics to claim he knows a lot about (say) politics. It has been said: an Intellectual is someone who is an expert in one field and pontificates in another field. </p><p>3) Does the general public think of DOCTOR as Medical Doctor? Probably yes. There are some exceptions: Dr. Martin Luther King and Dr. Henry Kissinger. Also, I think it is  more common in Psychology, pharmacy, education, and counseling to call yourself `Doctor'  </p><p>4) The article also criticized her PhD (in education, about community colleges) as `useless' . If that's the reason to not call her doctor than I shudder to think what the WSJ would think of degrees in, say, set theory with an emphasis on Large Cardinals. GEE, you can't call yourself  DOCTOR since your degree is on Ramsey Cardinals. OH, now they found an application, so now you CAN call yourself DOCTOR. OH, the application is to extending the Canonical Ramsey Theory from Polish spaces  to meta- compact  cardinals, so we can't call you DOCTOR after all. Do we really want to go down this path? </p><p>5) I ask all of the following non-rhetorically:  Did the author read her PhD thesis? Is he qualified to judge it? Did he (as one should do) look at her entire body of work to judge her? What point is he trying to make anyway? </p><p>6) Should Dr. Who call themselves a doctor? Are they  a medical doctor? PhD? If so, in what? Is `Who' part of their name? For other TV and movie tropes about the use of the word doctor, see <a href="https://tvtropes.org/pmwiki/pmwiki.php/Quotes/NotThatKindOfDoctor">here</a>.</p><p>7) I avoid saying I am a doctor since people will then ask me about the medical condition.</p><p>I avoid saying I am a computer scientist since people will then ask me how to help them with their Facebook privacy settings. </p><p>I avoid saying I am a mathematician since people will ask me to help their daughter with her trigonometry. </p><p>8) The answer to my trivia question: The last first lady to not have a college degree: Melania Trump. She went to college for a year and then left. The one before her was Barbara Bush who also went for a year and then left. </p><p>ADDED LATER: Many supervillians who don't have a PhD or an MD call themselves `Doctor', see <a href="https://www.buzzfeed.com/donnad/11-super-villains-masquerading-as-doctors">here</a>. Why no outrage about this? Because (1) they are fictional, and (2) imagine the scenario: Not only does Dr. Doom want to take over the world, he also doesn't even have a PhD or an MD!</p><p><br/></p><p>ADDED LATER: Where does Dr. Pepper fit into this? </p><p><br/></p></div>
    </content>
    <updated>2020-12-20T06:22:00Z</updated>
    <published>2020-12-20T06:22:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-01-01T08:23:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=5129</id>
    <link href="https://francisbach.com/optimization-is-as-hard-as-approximation/" rel="alternate" type="text/html"/>
    <title>Optimization is as hard as approximation</title>
    <summary>Optimization is a key tool in machine learning, where the goal is to achieve the best possible objective function value in a minimum amount of time. Obtaining any form of global guarantees can usually be done with convex objective functions, or with special cases such as risk minimization with one-hidden over-parameterized layer neural networks (see...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">Optimization is a key tool in machine learning, where the goal is to achieve the best possible objective function value in a minimum amount of time. Obtaining any form of global guarantees can usually be done with <a href="https://en.wikipedia.org/wiki/Convex_optimization">convex</a> objective functions, or with special cases such as risk minimization with one-hidden over-parameterized layer neural networks (see the <a href="https://francisbach.com/gradient-descent-neural-networks-global-convergence/">June post</a>). In this post, I will consider low-dimensional problems (imagine 10 or 20), with no constraint on running time (thus get ready for some running-times that are exponential in dimension!).</p>



<p class="justify-text">We consider minimizing a function \(f\) on a bounded subset  \(\mathcal{X}\) of \(\mathbb{R}^d\), based only on function evaluations, a problem often referred to as zero-th order optimization or <a href="https://en.wikipedia.org/wiki/Derivative-free_optimization">derivative-free optimization</a>. No convexity is assumed, so we should not expect fast rates, and, again, no efficient algorithms that can provably find a global minimizer. Good references on what I am going to cover in this post are [1, 2, 5].</p>



<p class="justify-text">One may wonder why this is interesting at all. Clearly, such algorithms are not made to be used to find millions of parameters for logistic regression or neural networks, but they are often used for hyperparameter tuning (regularization parameters, size of neural network layer, etc.). See, e.g., [<a href="https://papers.nips.cc/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf">6</a>] for applications.</p>



<p class="justify-text">We are going to assume some regularity for the functions we want to minimize, typically bounded derivatives. We will thus assume that \(f \in \mathcal{F}\), for a space \(\mathcal{F}\) of functions from \(\mathcal{X}\) to \(\mathbb{R}\). We are going to take a worst-case approach, where we characterize convergence over all members of \(\mathcal{F}\).  That is, we want our guarantees to hold for <em>all</em> functions in \(\mathcal{F}\). Note that this worst-case analysis may not predict well what’s happening for a particular function; in particular, it is (by design) pessimistic.</p>



<p class="justify-text">An algorithm \(\mathcal{O}\) will be characterized by (a) the choice of points \(x_1,\dots,x_n \in \mathcal{X}\) to query the function, and (b) the algorithm to output a candidate \(\hat{x} \in \mathcal{X}\) such that \(f(\hat{x}) \ – \inf_{x \in \mathcal{X}} f(x)  \) is small. The estimate \(\hat{x}\) can only depend on \((x_i,f(x_i))\), for \(i \in \{1,\dots,n\}\). In most of this post, the choice of points \(x_1,\dots,x_n\) is made once (without seeing any function values). We show later in this blog post that going <em>adaptive</em>, where the point \(x_{i+1}\) is selected after seeing \((x_j,f(x_j))\) for all \(j \leqslant i\), does not bring much (at least in the worst-case sense).</p>



<p class="justify-text">Given a selection of points and the algorithm \(\mathcal{O}\), the rate of convergence is the supremum over all functions \(f \in \mathcal{F}\) of the error \(f(\hat{x}) \ – \inf_{x \in \mathcal{X}} f(x)\). This is a function \(\varepsilon_n(\mathcal{O})\) of the number \(n\) of sampled points (and of the the class of functions \(\mathcal{F}\)). The optimal algorithm (minimizing \(\varepsilon_n(\mathcal{O})\)) will lead to a rate we denote \(\varepsilon_n^{\rm opt}\), and which we aim to characterize.</p>



<h2>Direct lower/upper bounds for Lipschitz-continuous functions</h2>



<p class="justify-text">The argument is particularly simple for a bounded metric space \(\mathcal{X}\) with distance \(d\), and \(\mathcal{F}\) the class of \(L\)-Lipschitz-continuous functions, that is, such that for all \(x,x’ \in \mathcal{X}\), \(|f(x) -f(x’)| \leqslant L d(x,x’)\). This is a very large set of functions, so expect weak convergence rates.</p>



<p class="justify-text"><strong>Set covers. </strong>We will need to cover the set \(\mathcal{X}\) with balls of a given radius. The minimal radius \(r\) of a cover of \(\mathcal{X}\) by \(n\) balls of radius \(r\) is denoted \(r_n(\mathcal{X},d)\). This corresponds to \(n\) ball centers \(x_1,\dots,x_n\). See example below for the unit cube \(\mathcal{X} = [0,1]^2\) and the metric obtained from the \(\ell_\infty\)-norm, with \(n = 16\), and \(r_n([0,1]^2,\ell_\infty) = 1/8\). See more details on covering numbers <a href="https://en.wikipedia.org/wiki/Covering_number">here</a>.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-5187" height="269" src="https://francisbach.com/wp-content/uploads/2020/12/cover-1024x1024.png" width="269"/></figure></div>



<p class="justify-text">More generally, for the unit cube \(\mathcal{X} = [0,1]^d\), we have \(r_n([0,1]^d,\ell_\infty) \approx \frac{1}{2} n^{-1/d}\) (which is not an approximation where \(n\) is the \(d\)-th power of an integer). For other normed metrics, (since all norms are equivalent) the scaling as \(r_n \sim {\rm diam} (\mathcal{X}) n^{-1/d}\) is the same on any bounded set in \(\mathbb{R}^d\) (with an extra constant that depends on \(d\)).</p>



<p class="justify-text"><strong>Algorithm. </strong>Given the ball centers \(x_1,\dots,x_n\), outputting the minimum of function values \(f(x_i)\) for \(i=1,\dots,n\), leads to an error which is less than \(L r_n(\mathcal{X},d)\), as the optimal \(x_\ast \in \mathcal{X}\) is at most at distance \(r_n(\mathcal{X},d)\) from one of the cluster centers, let’s say \(x_k\), and thus \(f(x_k)\  – f(x_\ast) \leqslant L d(x_k,x_\ast) \leqslant L r_n(\mathcal{X},d)\). This provides an upper-bound on \(\varepsilon_n^{\rm opt}\). The algorithm we just described seems naive, but it turns out to be optimal for this class of problems.</p>



<p class="justify-text"><strong>Lower-bound.</strong> Consider any optimization algorithm, with its first \(n\) point queries and its estimate \(\hat{x}\). By considering the functions which are zero in these \(n+1\) points, the algorithm outputs zero. We now simply need to construct a function \(f \in \mathcal{F}\) such that \(f\) is zero at these points, but maximally smaller than zero at a different point. </p>



<p class="justify-text">Consider a cover of \(\mathcal{X}\) with \(n+2\) balls of minimal radius  (equal to \(r_{n+2}(\mathcal{X},d)\)), there has to exist at least one of the \(n+2\) corresponding ball centers such that the corresponding ball contains no points from the algorithm (denote by \(y\) its center). We can then construct the function $$ f(x)  = \ – L \big( r_{n+2}(\mathcal{X},d) \ – d(x,y) \big)_+ = \ – L \max \big\{  r_{n+2}(\mathcal{X},d) \ – d(x,y) ,0 \big\}, $$ which is zero on all points of the algorithm and the output point \(\hat{x}\), and with minimum value \(– L r_{n+2}(\mathcal{X},d)\) attained at \(y\). Thus, we must have \(\varepsilon_n^{\rm opt} \geqslant 0 \ – (\ – L r_{n+2}(\mathcal{X},d) ) = L r_{n+2}(\mathcal{X},d)\). This difficult function is plotted below in one dimension.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-5260" height="73" src="https://francisbach.com/wp-content/uploads/2020/12/lowerbound-1-1024x178.png" width="423"/></figure></div>



<p class="justify-text">Thus, the performance of any algorithm from \(n\) function values has to be larger than \(L r_{n+2}(\mathcal{X},d)\).  Thus, so far, we have shown that $$L r_{n+2}(\mathcal{X},d) \leqslant \varepsilon_n^{\rm opt} \leqslant L r_{n}(\mathcal{X},d).$$  For \(\mathcal{X} \subset \mathbb{R}^d\), \(r_n(\mathcal{X},d)\) is typically of order \({\rm diam}(\mathcal{X}) n^{-1/d}\), and thus the difference between \(n\) and \(n+2\) above is negligible. Note that the rate in \(n^{-1/d}\) is <strong>very</strong> slow, and symptomatic of the classical curse of dimensionality. The appearance of a covering number is not totally random here, as we will see below.</p>



<p class="justify-text"><strong>Random search.</strong> We can have a similar bound up to logarithmic terms for random search, that is, after selecting independently \(n\) points \(x_1,\dots,x_n\), uniformly at random in \(\mathcal{X}\), and selecting the points with smallest function value \(f(x_i)\). The performance can be shown to be proportional to \(L {\rm diam}(\mathcal{X}) ( \log n )^{1/d} n^{-1/d}\) in high probability, leading to an extra logarithmic term (the proof can be obtained with a simple covering argument, as shown at the end of the post). Therefore, random search is optimal up to logarithmic terms for this very large class of functions to optimize.</p>



<p class="justify-text">We would like to go beyond Lipschitz-continuous functions, and study if we can leverage smoothness, and hopefully avoid the dependence in \(n^{-1/d}\). This can be done by a somewhat surprising equivalence between worst case guarantees from optimization and worst case guarantees for uniform approximation.</p>



<h2>Optimization is as hard as uniform function approximation</h2>



<p class="justify-text">We now also consider the problem of outputting a whole function \(\hat{f} \in \mathcal{F}\), such that \(\|f – \hat{f}\|_\infty = \max_{x \in \mathcal{X}} | \hat{f}(x)\ – f(x)|\) is as small as possible. For any approximation algorithm \(\mathcal{A}\) that builds an estimate \(\hat{f} \in \mathcal{F}\) from \(n\) function values, we can define its convergence rate in the same way as for optimization algorithm \(\varepsilon_n(\mathcal{A})\), as a function of \(n\). The optimal approximation algorithm has a convergence rate denoted by \(\varepsilon_n^{\rm app}\).</p>



<p class="justify-text"><strong>From approximation to optimization. </strong>Clearly, an approximation algorithm \(\mathcal{A}\) leads to an optimization algorithm \(\mathcal{O}\) with at most twice the same rate, that is, $$  \varepsilon_n(\mathcal{O}) \leqslant 2\varepsilon_n(\mathcal{A}),$$ by simply approximating \(f\) by \(\hat{f}\) and outputting any \(\hat{x} \in \arg \min_{x \in \mathcal{X}} \hat{f}(x)\), for which $$f(\hat{x})  \leqslant \hat{f}(\hat{x}) + \| \hat{f}\ – f\|_\infty =  \min_{x \in \mathcal{X}} \hat{f}(x) + \| \hat{f}\  – f\|_\infty \leqslant \min_{x \in \mathcal{X}} {f}(x) +2  \| \hat{f} \ – f\|_\infty. $$ See an illustration below (with a function estimated from the values at green points), with the candidate minimizer in orange.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-5268" height="254" src="https://francisbach.com/wp-content/uploads/2020/12/optim_approx-1024x553.png" width="470"/></figure></div>



<p class="justify-text">Thus in terms of worst-case performance, we get $$ \varepsilon_n^{\rm opt} \leqslant 2\varepsilon_n^{\rm app}.$$ Intuitively, it seems that this upper-bound should be very loose, as approximating uniformly a function, in particular far from its minimum, seems useless for minimization. For worst-case performance, this intuition is incorrect… Indeed, the optimal rate for optimization with \(n\) function evaluations happens to be greater than the optimal rate for approximation with \(n+1\) function evaluations, which we will now show.</p>



<p class="justify-text"><strong>Lower-bound on optimization performance. </strong>We will need an extra assumption, namely that the function space \(\mathcal{F}\) is convex and symmetric (and bounded in uniform norm).</p>



<p class="justify-text">We consider an optimization algorithm over the class of function \(\mathcal{F}\), that is considering \(n\) observation points \(x_1,\dots,x_n\), and an estimate \(\hat{x}\). The worst-case performance over all functions in \(\mathcal{F}\) is greater than over the (smaller) class of functions for which \(f(x_1) = \cdots = f(x_n) = f(\hat{x}) = 0\). Given that the performance measure is \(f(\hat{x})\  – \inf_{x \in \mathcal{X}} f(x)\), the performance is greater than the supremum of  \(– \inf_{x \in \mathcal{X}} f(x)\) over functions in \(\mathcal{F}\) such that \(f(x_1) = \cdots = f(x_n) = f(\hat{x}) = 0\). For Lipschitz-continuous functions above, we built explicitly a hard function. In the general case, an explicit construction is not that easy, but we will relate the construction of such a function to the general approximation problem.</p>



<p class="justify-text">As we just saw, the optimal rate of the optimization algorithm is greater than $$\inf_{x_1,\dots,\, x_{n+1} \in \, \mathcal{X}} \sup_{f \in \mathcal{F}, \ f(x_1)\ =\ \cdots \ = \ f(x_{n+1}) \ =\ 0} – \inf_{x \in \mathcal{X}} f(x) . $$ The quantity above characterizes how small a function can be when equal to zero on \(n+1\) points. When the set of function \(\mathcal{F}\) is centrally symmetric, that is \(f \in \mathcal{F} \Rightarrow \ – f \in \mathcal{F}\), we can replace \(– \inf_{x \in \mathcal{X}} f(x)\) in the expression above by \(\| f\|_\infty\).</p>



<p class="justify-text">It turns out that for convex and centrally symmetric spaces \(\mathcal{F}\) of functions, this happens to be the optimal rate of approximation with \(n+1\) function evaluations. This is sometimes referred to as Smolyak’s lemma [3], which we state here (see a very nice and short proof in [4]).</p>



<p class="justify-text"><strong>Smolyak’s lemma. </strong>We consider a space of functions \(\mathcal{F}\) which is convex and centrally symmetric. Then:</p>



<ul class="justify-text"><li>For any \(x_1,\dots,x_n \in \mathcal{X}\), the optimal approximation method, that is the optimal map \(\mathcal{S}: \mathbb{R}^n \to \mathcal{F}\), i.e., an algorithm that computes \(\hat{f} = \mathcal{S}(f(x_1),\dots,f(x_n)) \in \mathcal{F}\), is <em>linear</em>, that is, there exist functions \(g_1,\dots,g_n: \mathcal{X} \to \mathbb{R}\) such that $$\mathcal{S}(f(x_1),\dots,f(x_n)) = \sum_{i=1}^n f(x_i) g_i.$$</li><li>The optimal rate of uniform approximation in \(\mathcal{F}\) is equal to $$ \displaystyle \varepsilon_n^{\rm app} = \inf_{x_1,\dots,\, x_{n} \in \, \mathcal{X}} \sup_{f \in \mathcal{F}, \ f(x_1)\ =\ \cdots \ = \ f(x_{n}) \ =\ 0} \| f \|_\infty.$$</li></ul>



<p class="justify-text">Thus, we have “shown” that: $$ \varepsilon_{n+1}^{\rm app} \leqslant \varepsilon_n^{\rm opt} \leqslant 2\varepsilon_n^{\rm app}, $$ that is, optimization is at most twice as hard as uniform approximation (still in the worst-case sense). We can now consider examples of uniform approximation algorithms to get optimization algorithms.</p>



<h2>Examples and optimal algorithms</h2>



<p class="justify-text">As seen above, we simply need algorithms that approximate the function in uniform norm, and then we minimize the approximation. This is computationally optimal in terms of access to function evaluations (but clearly not in terms of computational complexity).</p>



<p class="justify-text"><strong>Lipschitz-continuous functions.</strong> One can check that with \(x_1,\dots,x_n\) the centers of the cover above, and a piecewise constant function on each of the ball (with arbitrary values at intersections), then we have $$ \| \hat{f} \, – f \|_\infty \leqslant L   r_n(\mathcal{X},d),$$ which is a standard approximation result using covering numbers. We recover the result above directly from the cover argument (and here the sharper result that the rates of uniform approximation and optimization are asymptotically the same for this class of functions).</p>



<p class="justify-text"><strong>Smooth functions in one dimension.</strong> For simplicity, I will focus on the one-dimensional problem, where all concepts are simpler, some of them directly extend to higher dimensions, some of them don’t.</p>



<p class="justify-text">We consider \(\mathcal{X} = [0,1]\). The simplest interpolation techniques are piecewise constant and piecewise affine interpolations. That is, if we observe \(0 = x_1 \leqslant \cdots \leqslant x_n = 1\), we consider \(\hat{f}\) defined on \([x_i,x_{i+1}]\) as $$\hat{f}(x) = \frac{1}{2}f(x_i) + \frac{1}{2}f(x_{i+1}) $$ </p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-5286" height="152" src="https://francisbach.com/wp-content/uploads/2020/12/interpolation_constant-2-1024x451.png" width="339"/></figure></div>



<p class="justify-text">or $$ \hat{f}(x) = f(x_i) + \frac{ x\  – x_i}{x_{i+1}-x_i}\big[ f(x_{i+1})\ – f(x_i) \big].$$</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img alt="" class="wp-image-5287" height="144" src="https://francisbach.com/wp-content/uploads/2020/12/interpolation_affine-1024x449.png" width="324"/></figure></div>



<p class="justify-text">If we assume that \(f\) is twice differentiable with second-derivative bounded by \(M_2\), then, we can show that on \([x_i,x_{i+1}]\), we have for piecewise affine interpolation: $$ | f(x) \ – \hat{f}(x) | \leqslant \frac{M_2}{8} | x_{i+1} – x_i |^2.$$</p>



<p class="justify-text">If we assume that \(f\) is differentiable with first-derivative bounded by \(M_1\), then, we can show that on \([x_i,x_{i+1}]\), we have for piecewise affine or constant interpolation: $$ | f(x) \ – \hat{f}(x) | \leqslant \frac{M_1}{2} | x_{i+1} – x_i |.$$</p>



<p class="justify-text">This leads to, when \(x_i = (i-1)/(n-1)\), to uniform error in \(O(1/n)\) for functions with a bound on a single derivative and or \(O(1/n^2)\) for two derivatives. Thus, we see two effects, that are common in approximations problems: (a) the more regular the function to approximate, the better the approximation rate, (b) If we consider a method tailored to smoother functions, it often works well for less smooth functions.</p>



<p class="justify-text">Explicit piecewise affine interpolation is harder to perform in higher dimensions, where other techniques can be used as presented below, such as <a href="https://en.wikipedia.org/wiki/Kernel_method">kernel methods</a> (again!).</p>



<p class="justify-text"><strong>Going high-dimensional with kernel methods.</strong> We assume that we have a positive definite kernel \(k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}\) on \(\mathcal{X}\), and, given the \(n\) elements \(x_1,\dots,x_n\) of \(\mathcal{X}\) and function values \(f(x_1),\dots,f(x_n)\), we look for the interpolating function \(f\) in the corresponding reproducing kernel Hilbert space with minimum norm. By the <a href="https://en.wikipedia.org/wiki/Representer_theorem">representer theorem</a>, it has to be of the form \(\sum_{i=1}^n  \alpha_i k(x,x_i)\). Since it has to interpolate, we must have \(K\alpha = y\) and thus \(\alpha = K^{-1} y\), where \(y_i = f(x_i)\) for \(i \in \{1,\dots,n\}\), and \(K\) the \(n \times n\) kernel matrix. Thus, interpolation can then be easily done in higher dimensions.</p>



<p class="justify-text"><a href="https://en.wikipedia.org/wiki/Sobolev_space">Sobolev spaces</a> are commonly used in the interpolation context, and for \(s\) square-integrable derivatives, for \(s &gt; d/2\), they are reproducing kernel Hilbert spaces. See below for examples of interpolations with various Sobolev spaces and increasing number of interpolating points, where the approximation errors (in uniform norm) are computed. With \(s=1\), we recover piecewise affine interpolation, but smoother functions are obtained for \(s=2\).</p>



<div class="wp-block-image"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-5238" height="250" src="https://francisbach.com/wp-content/uploads/2020/12/interpolation-2.gif" width="468"/></figure></div>



<p class="justify-text"><strong>Optimal rates of approximation.</strong> With the method defined above, we can obtain the optimal rates of approximation (and hence of optimization) of \(\varepsilon_n^{\rm app} \approx \displaystyle \frac{1}{n^{m/d}}\), for the set of functions with \(m\) bounded derivatives. Thus, for very smooth functions, we can escape the curse of dimensionality (that is, obtain a power of \(n\) that does not decay too slowly). See [1] for more details.</p>



<h2>The powerlessness of adaptivity</h2>



<p class="justify-text">The critical reader may argue that the algorithm set-up described in this post is too simple: the points at which we take function values are decided once and for all, independently of the observed function values. Clearly, some form of adaptivity should beat random search. The sad truth is that the bound for worst-case performance is the same… up to a factor of 2 at most (still in the worst-case sense).</p>



<p class="justify-text">The argument is essentially the same as for non-adaptive algorithms: for the Lipschitz-continuous example, our hard function can also be built for adaptive algorithms, while for the general case, it simply turns out that the rate for adaptive approximation is exactly the same than for adaptive approximation [4].</p>



<p class="justify-text">Thus, adaptive and non-adaptive approximations are just a factor of two of each other. Note that given the practical success of <a href="https://en.wikipedia.org/wiki/Bayesian_optimization">Bayesian optimization</a> (which is one instance of adaptive optimization) on some problems, there could be at least two explanations (choose the one you prefer, or find a new one): (1) the worst-case analysis can be too pessimistic, or (2) what is crucial in Bayesian optimization is not the adaptive choice of points to evaluate the function, but the adaptivity to smoothness of the function to optimize (that is, if the function has \(m\) derivatives, then the rate is \(n^{-m/d}\), which can be much better than \(n^{-1/d}\)).</p>



<h2>Conclusion</h2>



<p class="justify-text">In this post, I highlighted the strong links between approximation problems and optimization problems. It turns out that the parallel between worst-case performances goes beyond: computing integrals from function evaluations, a problem typically referred to as <a href="https://en.wikipedia.org/wiki/Numerical_integration">quadrature</a>, is also as hard. More on this in a future post.</p>



<p class="justify-text">As warned early in the post, the algorithms presented here all have running time that are exponential in dimension, as they either perform random sampling or need to minimize a model of the function to optimize. </p>



<p class="justify-text">The only good news in this post: optimization is hard as approximation, but the scenario is more varied than expected. Indeed approximation can be fast and avoid the curse of dimensionality, at least in the exponent of the rate, when the function is very smooth (indeed, for \(m\)-times differentiable functions, the constant in front of \(n^{-m/d}\) still depends exponentially in \(d\)). It would be nice to catch this property also in an optimization algorithm, with a running time complexity depending polynomially on \(n\) only, and not exponentially in \(d\). Next month, I will present recent work with Alessandro Rudi and Ulysse Marteau-Ferey [<a href="https://arxiv.org/pdf/2012.11978">7</a>] that does exactly this, by combining the tasks of interpolation and optimization in a single convex optimization problem.</p>



<p class="justify-text"><strong>Acknowledgements</strong>. I would like to thank Alessandro Rudi and Ulysse Marteau-Ferey for proofreading this blog post and making good clarifying suggestions.</p>



<h2>References</h2>



<p class="justify-text">[1] Erich Novak. <em>Deterministic and stochastic error bounds in numerical analysis</em>. Springer, 2006.<br/>[2] Erich Novak, Henryk Woźniakowski. Tractability of Multivariate Problems: Standard information for functionals, European Mathematical Society, 2008.<br/>[3] S. A. Smolyak. On optimal restoration of functions and functionals of them, Candidate Dissertation, Moscow State University, 1965.<br/>[4] Nikolai Sergeevich Bakhvalov. <a href="https://www.sciencedirect.com/science/article/abs/pii/0041555371900176">On the optimality of linear methods for operator approximation in convex classes of functions</a>. USSR Computational Mathematics and Mathematical Physics. 11(4): 244-249, 1971.<br/>[5] Yurii Nesterov. <em>Lectures on Convex Optimization</em>. Springer, 2018.<br/>[<a href="https://papers.nips.cc/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf">6</a>] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. <a href="https://papers.nips.cc/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf">Practical Bayesian optimization of machine learning algorithms</a>. <em>Advances in Neural Information Processing Systems</em>, 2015.<br/>[7] Alessandro Rudi, Ulysse Marteau-Ferey, Francis Bach. <a href="https://www.di.ens.fr/~fbach/gloptikernel.pdf">Finding Global Minima via Kernel </a><a href="https://arxiv.org/pdf/2012.11978">Approximations</a>. Technical report arXiv 2012.11978, 2020.</p>



<h2>Performance of random search</h2>



<p class="justify-text">We consider sampling independently and uniformly in \(\mathcal{X}\) \(n\) points \(x_1,\dots,x_n\). For a given \(L\)-Lipschitz-continuous function \(f\), with global minimizer \(x\) on \(\mathcal{X}\), we have $$\min_{i \in \{1,\dots,n\}} f(x_i)\  – f(x) \leqslant L \min_{i \in \{1,\dots,n\}} d(x,x_i).$$ Thus, to obtain an upper-bound on performance over all functions \(f\) (and potentially all locations of the global minimizer \(x\)), we need to bound $$\max_{x \in \mathcal{X}} \min_{i \in \{1,\dots,n\}} d(x,x_i).$$ If we have a cover with \(m\) points \(y_1,\dots,y_m\), and radius \(r = r_m(\mathcal{X},d)\), we have $$\max_{x \in \mathcal{X}} \min_{i \in \{1,\dots,n\}} d(x,x_i) \leqslant r + \max_{j  \in \{1,\dots,m\}} \min_{i \in \{1,\dots,n\}} d(y_j,x_i) .$$ We can then bound the probability that $$ \max_{j \in \{1,\dots,m\}} \min_{i \in \{1,\dots,n\}} d(y_j,x_i) \geqslant r$$ by the union bound (and using the independence of the \(x_i\)’s) as \(m\) times the \(n\)-th power of the probability that one of the \(x_i\) is not in a given ball of radius \(r\). By a simple volume argument (and assuming that all balls of a given radius have the same volume), this probability is less than \(( 1 – 1/m)\). Thus with probability greater than \(1 – m(1 – 1/m)^n \geqslant 1 – m e^{ – n /m}\) the worst-case performance is less than \(2Lr\).</p>



<p class="justify-text">Since for normed metrics in \(\mathbb{R}^d\), \(r  \sim m^{-1/d} {\rm diam}(\mathcal{X})\), by selecting \(m = 2n \log n\), we get an overall performance proportional to \( L \big( \frac{ \log n}{n} \big)^{1/d}\) with probability greater than \(1 – \frac{ \log n}{n}\) (which tends to one when \(n\) grows).</p></div>
    </content>
    <updated>2020-12-18T16:07:10Z</updated>
    <published>2020-12-18T16:07:10Z</published>
    <category term="Machine learning"/>
    <author>
      <name>Francis Bach</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2021-01-02T08:38:45Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=17896</id>
    <link href="https://rjlipton.wordpress.com/2020/12/17/database-and-theory/" rel="alternate" type="text/html"/>
    <title>Database and Theory</title>
    <summary>A dean of engineering Jennifer Widom was appointed as the Dean of Engineering at Stanford not long ago. No doubt her research in databases and her teaching played a major role in her selection. For example, she is known for her contributions to online education including teaching one of the first massive open courses—MOOCS—which had […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><span style="color: #0044cc;"><br/>
<em>A dean of engineering</em><br/>
</span></p>
<p><a href="https://rjlipton.wordpress.com/2020/12/17/database-and-theory/dean-2/" rel="attachment wp-att-17922"><img alt="" class="alignright wp-image-17922" height="173" src="https://rjlipton.files.wordpress.com/2020/12/dean-1.jpg?w=180&amp;h=173" width="180"/></a></p>
<p>Jennifer Widom was appointed as the <a href="https://news.stanford.edu/2017/02/27/computer-scientist-jennifer-widom-named-dean-stanford-school-engineering/">Dean</a> of Engineering at Stanford not long ago. No doubt her research in databases and her teaching played a major role in her selection. For example, she is known for her contributions to online education including teaching one of the first massive open courses—<a href="https://en.wikipedia.org/wiki/Massive_open_online_course">MOOCS</a>—which had over 100,000 students.</p>
<p>Today I thought we might highlight her area, databases, and one of its connections to theory.</p>
<p>But first, I was impressed to hear her say in an ACM <a href="https://learning.acm.org/bytecast">podcast</a> that she used punch cards during the start of her career:</p>
<blockquote><p><b> </b> <em> So my passion in high school actually became music, and when it came time to select a college, I chose to go to music school. I’m pretty sure at this point, I’m the only dean of engineering anywhere who has a bachelor’s degree in trumpet performance, but that’s actually what my undergraduate degree is in. Late in my music education, I just sort of randomly took a class called computer applications and music research. And it was a class in the music school about using programming to analyze music, and it was my first exposure to computer programming. I have to say, it’ll reveal my age, but I used punch cards in that class. It was sort of the end of the punch card era. </em></p></blockquote>
<p>I admit to having used these too at the start of my days as an undergrad at <a href="https://www.fourmilab.ch/documents/univac/cards.html">Case</a> Institute. Do you know what a punch card is?</p>
<p><a href="https://rjlipton.wordpress.com/2020/12/17/database-and-theory/case/" rel="attachment wp-att-17901"><img alt="" class="aligncenter size-full wp-image-17901" src="https://rjlipton.files.wordpress.com/2020/12/case.png?w=600"/></a></p>
<p>Ken once had his own experience with punch cards and music. While taking an intro programming course during his sophomore year—taught then by Jeffrey Ullman—Ken and his roommate joined an excursion to NYC for dinner and the New York Philharmonic. Still dressed in suits, on return to Princeton, they descended into the computer center to do their assignment—via punch cards on a batch server. Ken’s roommate got out before 2am, but Ken says he was still fixing punch card typos that ruined runs as late as 4am.</p>
<h2>Data Management</h2>
<p>Widom’s <a href="https://cs.stanford.edu/people/widom/">research</a> has always been in the area of non-traditional databases: semi-structured data, data streams, uncertain data and data provenance.</p>
<p>We theorists view the world as made of questions like:</p>
<ul>
<li>Upper Bounds Can we find a faster algorithm for problem X?</li>
<li>Lower Bounds Can we prove there are no faster algorithms for X? Sometimes we replace “faster” by other complexity measures, but these are the dominant questions we ask.
<p>Widom proposed a notion that is called <a href="https://www.sciencedirect.com/topics/computer-science/data-provenance">lineage</a>. Theorists worry about the performance of searching, but this issue is about where did the data come from? See also <a href="http://db.cis.upenn.edu/DL/fsttcs.pdf">provenance</a>.</p>
<p>The insight boils down to garbage in and garbage out, or <a href="https://en.wikipedia.org/wiki/Garbage_in,_garbage_out">GIGO</a>:</p>
<blockquote><p><b> </b> <em> On two occasions I have been asked, “Pray, Mr. Babbage, if you put into the machine wrong figures, will the right answers come out?”</em></p>
<p><em> Charles Babbage (1864). </em></p></blockquote>
<p>The issue of lineage and provenance is exactly this made modern. How can one track where the data comes from? Of course, finding fast algorithms to do the tracking is still an important question. Oh well.</p>
<h2>Databases Meet Theory</h2>
<p>Of course databases store and allow us to retrieve information—they are everywhere these days. One reason for their dominance is they support complex queries. For example, we can use databases to obtain all <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{x}"/> so that <img alt="{R(x,y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{R(x,y)}"/> is true for some <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{y}"/>:</p>
<p align="center"><img alt="\displaystyle \{ x \mid \exists y R(x,y) \}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+%5C%7B+x+%5Cmid+%5Cexists+y+R%28x%2Cy%29+%5C%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle \{ x \mid \exists y R(x,y) \}. "/></p>
<p>From a theory point of view any first-order question is allowed.</p>
<p>But searches often use approximate matches. Thus one might wish all <img alt="{R(x',y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28x%27%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{R(x',y)}"/> so that <img alt="{x'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{x'}"/> is near <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{x}"/>. We might want all <img alt="{R(x',y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BR%28x%27%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{R(x',y)}"/> so that <img alt="{x'}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{x'}"/> is not “Michael” but also includes “Micheal”. This type of error is not too hard to handle, but a worse type of error is</p>
<p align="center"><img alt="\displaystyle Michael \iff Michal " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle+Michael+%5Ciff+Michal+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="\displaystyle Michael \iff Michal "/></p>
<p>Formally changing a letter is not as difficult as deleting or adding letters. This is the <a href="https://web.stanford.edu/class/cs124/lec/med.pdf">edit distance</a> between two strings is defined as the minimum number of insertions, deletions or substitutions of symbols needed to transform one string into another. An important application is to biology.</p>
<table style="margin: auto;">
<tbody>
<tr>
<td><a href="https://rjlipton.wordpress.com/2020/12/17/database-and-theory/edit/" rel="attachment wp-att-17903"><img alt="" class="aligncenter size-full wp-image-17903" height="289" src="https://rjlipton.files.wordpress.com/2020/12/edit.png?w=600&amp;h=289" width="600"/></a></td>
</tr>
</tbody>
</table>
<p>This leads to into the more-general notion of <em>entity resolution</em> (ER). Widom co-leads the <em>Stanford Entity Resolution Framework</em> (<a href="http://infolab.stanford.edu/serf/">SERF</a>) with Hector Garcia-Molina; they were two of the authors on its earliest <a href="http://ilpubs.stanford.edu:8090/779/">paper</a> in 2006. The page highlights the lexical nature of ER:</p>
<blockquote><p><b> </b> <em> For instance, two records on the same person may provide different name spellings, and addresses may differ. The goal of ER is to “resolve” entities, by identifying the records that represent the same entity and reconciling them to obtain one record per entity. </em></p></blockquote>
<p>Their approach “involves functions that `match’ records (i.e., decide whether they represent the same entity) and `merge’ them.” Their framework has an outer layer that treats the functions as generic. Widom’s emphasis has been on axiomatizing properties of the functions to enable efficient and extensible manipulation by the outer layer.</p>
<p>Necessarily under the hood is a measure of edit distance. Ken recently monitored a large tournament where two players had the same surname and first three prenames, differing only in the last four letters of their last prename. The widget used to download the game file originally given to Ken truncated the names so that there seemed to be one person playing twice as many games. They could also have been distinguished by other database information. More common is when the same player is referenced with different spellings, such as (grandmaster Artur) Jussupow or Yusupov. The similarity metric needs to be extended beyond the name. This is when computation time becomes more a factor and theory enters in.</p>
<h2>Edit Distance</h2>
<p>Computing the <a href="https://en.wikipedia.org/wiki/Edit_distance">edit distance</a> between two strings is a long-studied problem. The best algorithm known is order quadratic time, as we covered <a href="https://rjlipton.wordpress.com/2009/03/22/bellman-dynamic-programming-and-edit-distance/">here</a>. There are better running times if we allow approximate algorithms or quantum algorithms. See <a href="https://arxiv.org/pdf/1804.04178.pdf">this</a> for the latter and this <a href="https://rjlipton.wordpress.com/2015/07/22/alberto-apostolico-1948-2015/">post</a> for work related to the former.</p>
<p>Theorists have tried to improve the time needed for classical exact edit distance. It remains at quadratic time, however, and there is <a href="https://www.quantamagazine.org/edit-distance-reveals-hard-computational-problems-20150929/">evidence</a> of inability to do better that we have remarked as <a href="https://rjlipton.wordpress.com/2015/06/01/puzzling-evidence/">puzzling</a>. One of the strange traits of theorists is that when something is hard we try to <a href="https://www.theidioms.com/when-life-gives-you-lemons/">exploit</a> the hardness.</p>
<blockquote><p><b> </b> <em> When life gives you lemons, make lemonade. </em></p></blockquote>
<p>For instance, factoring integers is hard—so let us make crypto systems. The lemonade in the case of edit distance is a connection to the hardness of really-hard problems. To quote the seminal <a href="https://arxiv.org/pdf/1412.0348.pdf">paper</a> by Arturs Backurs and Piotr Indyk:</p>
<blockquote><p><b> </b> <em> In this paper we provide evidence that the near-quadratic running time bounds known for the problem of computing edit distance might be tight. Specifically, we show that, if the edit distance can be computed in time <img alt="{O(n^{2-\epsilon})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%7B2-%5Cepsilon%7D%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{O(n^{2-\epsilon})}"/> for some constant <img alt="{\epsilon &gt; 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{\epsilon &gt; 0}"/>, then the satisfiability of conjunctive normal form formulas with <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{N}"/> variables and <img alt="{M}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BM%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002" title="{M}"/> clauses can be solved in time subexp. The latter result would violate the Strong Exponential Time Hypothesis, which postulates that such algorithms do not exist. </em></p></blockquote>
<h2>Edit Distance—Searching</h2>
<p>An open problem that is closer to databases is not computing edit distance. Instead, the problem how to search for strings that are <em>near</em> a given string. This can be a generic use of an edit-distance function. A larger question is whether this can be approached more directly.</p>
<p>I believe that while edit distance may indeed be quadratic time, it still is open how fast searches can be done for approximate matches. The problem is given a large collection of strings <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{S}"/> how fast can we find the closest string in the collection to some word <img alt="{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002" title="{w}"/>? That the best algorithms for edit distance are probably quadratic does not—I believe—entail that this search question is difficult.</p>
<h2>Open Problems</h2>
<p>By the way, see <a href="https://databasetheory.org/node/76">this</a> for other open problems in databases.</p>
<p>[deleted stray line]</p></li>
</ul></div>
    </content>
    <updated>2020-12-17T16:55:11Z</updated>
    <published>2020-12-17T16:55:11Z</published>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="distance"/>
    <category term="edit"/>
    <category term="optimal"/>
    <category term="quadratic time"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2021-01-02T08:37:42Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-411297283565155244</id>
    <link href="https://blog.computationalcomplexity.org/feeds/411297283565155244/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/12/optiland.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/411297283565155244" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/411297283565155244" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2020/12/optiland.html" rel="alternate" type="text/html"/>
    <title>Optiland</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Many of you have heard of Russell Impagliazzo's <a href="https://blog.computationalcomplexity.org/2004/06/impagliazzos-five-worlds.html">five worlds</a> from his 1995 classic <a href="https://doi.org/10.1109/SCT.1995.514853">A personal view of average-case complexity</a> In short </p><ul><li><i>Algorithmica</i>: P = NP or something "morally equivalent" like fast probabilistic algorithms for NP. </li><li><i>Heuristica</i>: NP problems are hard in the worst case but easy on average.</li><li><i>Pessiland</i>: NP problems hard on average but no one-way functions exist. We can easily create hard NP problems, but not hard NP problems where we know the solution. </li><li><i>Minicrypt</i>: One-way functions exist but we do not have public-key cryptography.</li><li><i>Cryptomania</i>: Public-key cryptography is possible, i.e. two parties can exchange secret messages over open channels.</li></ul><div>Impagliazzo's world has an explicit "you can't have your cake and eat it too", either you can solve NP-hard problems on average, or have cryptography but not both (neither is possible). That's the mathematical world of P v NP. </div><div><br/></div><div>The reality is looking more and more like <i>Optiland</i>, where we can solve difficult NP problems and still have cryptography thanks to vast improvements in machine learning and optimization on faster computers with specialized hardware.</div><div><br/></div><div>Back in 2004 I <a href="https://blog.computationalcomplexity.org/2004/05/what-if-p-np.html">gave my guess</a> of the world of P = NP</div><div><blockquote>Learning becomes easy by using the principle of Occam's razor--we simply find the smallest program consistent with the data. Near perfect vision recognition, language comprehension and translation and all other learning tasks become trivial. We will also have much better predictions of weather and earthquakes and other natural phenomenon.</blockquote><p>Today you can take your smartphone, unlock it by having the phone scan your face, and ask it a question by talking and often get a reasonable answer, or have your question translated into a different language. You get alerts on your phone for weather and earthquakes, with far better predictions than we would have though possible a dozen years ago.</p><p>We have computed the <a href="http://www.math.uwaterloo.ca/tsp/uk/index.html">shortest traveling-salesman tour</a> through nearly 50K UK pubs. <a href="https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology">AlphaFold</a> can simulate protein folding with an accuracy nearly as good as what we get with real-world experiments. You can view GPT-3 as generating a form of a universal prior. Even beyond P = NP, we have self-trained computers easily besting humans in Chess, Go and Poker.</p><p>Meanwhile these techniques have done little to break cryptographic functions. Plenty of cybersecurity attacks but rarely by breaking the cryptography. </p><p>Not all is rosy--there is still much more we could do positively if P = NP  and we are already seeing some of the negative effects of learning such as loss of privacy. Nevertheless we are heading to a de facto best of both worlds when complexity theory tells us those worlds are incompatible. </p></div><p/></div>
    </content>
    <updated>2020-12-16T14:59:00Z</updated>
    <published>2020-12-16T14:59:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-01-01T08:23:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5159</id>
    <link href="https://www.scottaaronson.com/blog/?p=5159" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5159#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5159" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Chinese BosonSampling experiment: the gloves are off</title>
    <summary xml:lang="en-US">Two weeks ago, I blogged about the striking claim, by the group headed by Chaoyang Lu and Jianwei Pan at USTC in China, to have achieved quantum supremacy via BosonSampling with 50-70 detected photons. I also did a four-part interview on the subject with Jonathan Tennenbaum at Asia Times, and other interviews elsewhere. None of […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Two weeks ago, I <a href="https://www.scottaaronson.com/blog/?p=5122">blogged about</a> the striking claim, by the group headed by Chaoyang Lu and Jianwei Pan at USTC in China, to have achieved quantum supremacy via BosonSampling with 50-70 detected photons.  I also did a <a href="https://asiatimes.com/2020/12/chinas-quantum-computer-a-step-not-a-leap/">four-part interview</a> on the subject with Jonathan Tennenbaum at Asia Times, and other interviews elsewhere.  None of that stopped some people, who I guess didn’t google, from writing to tell me how disappointed they were by my silence!</p>



<p>The reality, though, is that a lot has happened since the original announcement, so it’s way past time for an update.</p>



<p><strong>I. The Quest to Spoof</strong></p>



<p>Most importantly, other groups almost immediately went to work trying to refute the quantum supremacy claim, by finding some efficient classical algorithm to spoof the reported results.  It’s important to understand that this is exactly how the process is <em>supposed</em> to work: as I’ve often stressed, a quantum supremacy claim is credible only if it’s open to the community to refute and if no one can.  It’s also important to understand that, for reasons we’ll go into, there’s a decent chance that people <em>will</em> succeed in simulating the new experiment classically, although they haven’t yet.  All parties to the discussion agree that the new experiment is, far and away, the closest any BosonSampling experiment has ever gotten to the quantum supremacy regime; the hard part is to figure out if it’s already there.</p>



<p>Part of me feels guilty that, as one of reviewers on the <em>Science</em> paper—albeit, one stressed and harried by kids and covid—it’s now clear that I didn’t exercise the amount of diligence that I could have, in searching for ways to kill the new supremacy claim.  But another part of me feels that, with quantum supremacy claims, much like with proposals for new cryptographic codes, vetting <em>can’t be the responsibility of one or two reviewers</em>.  Instead, provided the claim is serious—as this one obviously is—the only thing to do is to get the paper out, so that the <em>entire community</em> can then work to knock it down.  Communication between authors and skeptics is also a hell of a lot faster when it doesn’t need to go through a journal’s editorial system.</p>



<p>Not surprisingly, one skeptic of the new quantum supremacy claim is Gil Kalai, who (despite Google’s result last year, which Gil still believes must be in error) rejects the entire possibility of quantum supremacy on quasi-metaphysical grounds.  But other skeptics are current and former members of the Google team, including Sergio Boixo and John Martinis!  And—pause to enjoy the irony—<em>Gil has effectively teamed up with the Google folks</em> on questioning the new claim.  Another central figure in the vetting effort—one from whom I’ve learned much of what I know about the relevant issues over the last week—is Dutch quantum optics professor and frequent <em>Shtetl-Optimized</em> commenter <a href="https://people.utwente.nl/j.j.renema">Jelmer Renema</a>.</p>



<p>Without further ado, why might the new experiment, impressive though it was, be efficiently simulable classically?  A central reason for concern is photon loss: as Chaoyang Lu has now explicitly confirmed (it was implicit in the paper), up to ~70% of the photons get lost on their way through the beamsplitter network, leaving only ~30% to be detected.  At least with “Fock state” BosonSampling—i.e., the original kind, the kind with single-photon inputs that Alex Arkhipov and I proposed in 2011—it seems likely to me that such a loss rate would be fatal for quantum supremacy; see for example <a href="https://arxiv.org/abs/1809.01953">this 2019 paper</a> by Renema, Shchesnovich, and Garcia-Patron.</p>



<p>Incidentally, if anything’s become clear over the last two weeks, it’s that I, the co-inventor of BosonSampling, am no longer any sort of expert on the subject’s literature!</p>



<p>Anyway, one source of uncertainty regarding the photon loss issue is that, as I said in my last post, the USTC experiment implemented a 2016 variant of BosonSampling called <a href="https://arxiv.org/abs/1612.01199">Gaussian BosonSampling (GBS)</a>—and Jelmer tells me that the computational complexity of GBS in the presence of losses hasn’t yet been analyzed in the relevant regime, though there’s been <a href="https://arxiv.org/abs/1905.12075">work aiming in that direction</a>.  A second source of uncertainty is simply that the classical simulations work in a certain limit—namely, fixing the rate of noise and then letting the numbers of photons and modes go to infinity—but any real experiment has a fixed number of photons and modes (in USTC’s case, they’re ~50 and ~100 respectively).  It wouldn’t do to reject USTC’s claim via a theoretical asymptotic argument that would equally well apply to <em>any</em> non-error-corrected quantum supremacy demonstration!</p>



<p>OK, but if an efficient classical simulation of lossy GBS experiments exists, then <em>what is it?</em>  How does it work?  It turns out that we have a plausible candidate for the answer to that, originating with a <a href="https://arxiv.org/abs/1409.3093">2014 paper</a> by Gil Kalai and Guy Kindler.  Given a beamsplitter network, Kalai and Kindler considered an infinite hierarchy of better and better approximations to the BosonSampling distribution for that network.  Roughly speaking, at the first level (k=1), one pretends that the photons are just classical distinguishable particles.  At the second level (k=2), one correctly models quantum interference involving <em>pairs</em> of photons, but none of the higher-order interference.  At the third level (k=3), one correctly models three-photon interference, and so on until k=n (where n is the total number of photons), when one has reproduced the original BosonSampling distribution.  At least when k is small, the time needed to spoof outputs at the k<sup>th</sup> level of the hierarchy should grow like n<sup>k</sup>.  As theoretical computer scientists, Kalai and Kindler didn’t care whether their hierarchy produced any physically realistic kind of noise, but later work, by Shchesnovich, Renema, and others, showed that (as it happens) it does.</p>



<p>In its original paper, the USTC team ruled out the possibility that the first, k=1 level of this hierarchy could explain its experimental results.  More recently, in response to inquiries by Sergio, Gil, Jelmer, and others, Chaoyang tells me they’ve ruled out the possibility that the k=2 level can explain their results either.  We’re now eagerly awaiting the answer for larger values of k.</p>



<p>Let me add that I owe Gil Kalai the following public <em>mea culpa</em>.  While his objections to QC have often struck me as unmotivated and weird, in the case at hand, Gil’s 2014 work with Kindler is clearly helping drive the scientific discussion forward.  In other words, at least with BosonSampling, it turns out that Gil put his finger precisely on a key issue.  He did exactly what every QC skeptic should do, and what I’ve always implored the skeptics to do.</p>



<p><strong>II. BosonSampling vs. Random Circuit Sampling: A Tale of HOG and CHOG and LXEB</strong></p>



<p>There’s a broader question: why should skeptics of a BosonSampling experiment even have to <em>think</em> about messy details like the rate of photon losses?  Why shouldn’t that be solely the <em>experimenters’</em> job?</p>



<p>To understand what I mean, consider the situation with Random Circuit Sampling, the task Google demonstrated last year with 53 qubits.  There, the Google team simply collected the output samples and fed them into a benchmark that they called “Linear Cross-Entropy” (LXEB), closely related to what Lijie Chen and I called “Heavy Output Generation” (HOG) in a <a href="https://arxiv.org/abs/1612.05903">2017 paper</a>.  With suitable normalization, an ideal quantum computer would achieve an LXEB score of 2, while classical random guessing would achieve an LXEB score of 1.  Crucially, according to a <a href="https://arxiv.org/abs/1910.12085">2019 result</a> by me and Sam Gunn, under a plausible (albeit strong) complexity assumption, <em>no</em> subexponential-time classical spoofing algorithm should be able to achieve an LXEB score that’s even slightly higher than 1.  In its experiment, Google reported an LXEB score of about 1.002, with a confidence interval <em>much</em> smaller than 0.002.  Hence: quantum supremacy (subject to our computational assumption), with no further need to know anything about the sources of noise in Google’s chip!  (More explicitly, Boixo, Smelyansky, and Neven <a href="https://arxiv.org/abs/1708.01875">did a calculation</a> in 2017 to show that the Kalai-Kindler type of spoofing strategy <em>definitely</em> isn’t going to work against RCS and Linear XEB, with no computational assumption needed.)</p>



<p>So then why couldn’t the USTC team do something analogous with BosonSampling?  Well, they tried to.   They defined a measure that they called “HOG,” although it’s different from my and Lijie Chen’s HOG, more similar to a cross-entropy.  Following Jelmer, let me call their measure CHOG, where the C could stand for Chinese, Chaoyang’s, or Changed.  They calculated the CHOG for their experimental samples, and showed that it exceeds the CHOG that you’d get from the k=1 and k=2 levels of the Kalai-Kindler hierarchy, as well as from various other spoofing strategies, thereby ruling those out as classical explanations for their results.</p>



<p>The trouble is this: <em>unlike </em>with Random Circuit Sampling and LXEB, with BosonSampling and CHOG, we <em>know</em> that there are fast classical algorithms that achieve better scores than the trivial algorithm, the algorithm that just picks samples at random.  That follows from Kalai and Kindler’s work, and it even more simply follows from a <a href="https://arxiv.org/abs/1309.7460">2013 paper</a> by me and Arkhipov, entitled “BosonSampling Is Far From Uniform.”  Worse yet, with BosonSampling, we currently have no analogue of my 2019 result with Sam Gunn: that is, a result that would tell us (under suitable complexity assumptions) the highest possible CHOG score that we expect any efficient classical algorithm to be able to get.  And since we don’t know exactly where that ceiling is, we can’t tell the experimentalists exactly what target they need to surpass in order to claim quantum supremacy.  Absent such definitive guidance from us, the experimentalists are left playing whac-a-mole against <em>this</em> possible classical spoofing strategy, and <em>that</em> one, and <em>that</em> one.</p>



<p>This is an issue that I and others were aware of for years, although the new experiment has certainly underscored it.  Had I understood <em>just how serious</em> the USTC group was about scaling up BosonSampling, and fast, I might’ve given the issue some more attention!</p>



<p><strong>III. Fock vs. Gaussian BosonSampling</strong></p>



<p>Above, I mentioned another complication in understanding the USTC experiment: namely, their reliance on Gaussian BosonSampling (GBS) rather than Fock BosonSampling (FBS), sometimes also called Aaronson-Arkhipov BosonSampling (AABS).  Since I gave this issue short shrift in my previous post, let me make up for it now.</p>



<p>In FBS, the initial state consists of either 0 or 1 photons in each input mode, like so: |1,…,1,0,…,0⟩.  We then pass the photons through our beamsplitter network, and measure the number of photons in each output mode.  The result is that the amplitude of each possible output configuration can be expressed as the <a href="https://en.wikipedia.org/wiki/Permanent_(mathematics)">permanent</a> of some n×n matrix, where n is the total number of photons.  It was interest in the permanent, which plays a central role in classical computational complexity, that led me and Arkhipov to study BosonSampling in the first place.</p>



<p>The trouble is, preparing initial states like |1,…,1,0,…,0⟩ turns out to be really hard.  No one has yet build a source that reliably outputs <em>one and only one photon</em> at exactly a specified time.  This led two experimental groups to propose an idea that, in a <a href="https://www.scottaaronson.com/blog/?p=1579">2013 post on this blog</a>, I named Scattershot BosonSampling (SBS).  In SBS, you get to use the more readily available “Spontaneous Parametric Down-Conversion” (SPDC) photon sources, which output superpositions over different numbers of photons, of the form $$\sum_{n=0}^{\infty} \alpha_n |n \rangle |n \rangle, $$ where α<sub>n</sub> decreases exponentially with n.  You then measure the left half of each entangled pair, <em>hope</em> to see exactly one photon, and are guaranteed that if you do, then there’s also exactly one photon in the right half.  Crucially, one can show that, if Fock BosonSampling is hard to simulate approximately using a classical computer, then the Scattershot kind must be as well.</p>



<p>OK, so what’s <em>Gaussian</em> BosonSampling?  It’s simply the generalization of SBS where, instead of SPDC states, our input can be an arbitrary “Gaussian state”: for those in the know, a state that’s exponential in some quadratic polynomial in the creation operators.  If there are m modes, then such a state requires ~m<sup>2</sup> independent parameters to specify.  The quantum optics people have a much easier time creating these Gaussian states than they do creating single-photon Fock states.</p>



<p>While the amplitudes in FBS are given by permanents of matrices (and thus, the probabilities by the absolute squares of permanents), the probabilities in GBS are given by a more complicated matrix function called the <a href="https://en.wikipedia.org/wiki/Hafnian">Hafnian</a>.  Roughly speaking, while the permanent counts the number of perfect matchings in a bipartite graph, the Hafnian counts the number of perfect matchings in an <em>arbitrary</em> graph.  The permanent and the Hafnian are both #P-complete.  In the USTC paper, they talk about yet another matrix function called the “Torontonian,” which was <a href="https://arxiv.org/pdf/1807.01639.pdf">invented two years ago</a>.  I gather that the Torontonian is just the modification of the Hafnian for the situation where you only have “threshold detectors” (which decide whether one or more photons are present in a given mode), rather than “number-resolving detectors” (which <em>count</em> how many photons are present).</p>



<p>If Gaussian BosonSampling includes Scattershot BosonSampling as a special case, and if Scattershot BosonSampling is at least as hard to simulate classically as the original BosonSampling, then you might hope that GBS would <em>also</em> be at least as hard to simulate classically as the original BosonSampling.  Alas, this doesn’t follow.  Why not?  Because for all we know, a <em>random</em> GBS instance might be a lot easier than a <em>random</em> SBS instance.  Just because permanents can be expressed using Hafnians, doesn’t mean that a random Hafnian is as hard as a random permanent.</p>



<p>Nevertheless, I think it’s very likely that the sort of analysis Arkhipov and I did back in 2011 could be mirrored in the Gaussian case.  I.e., instead of starting with reasonable assumptions about the distribution and hardness of random permanents, and then concluding the classical hardness of approximate BosonSampling, one would start with reasonable assumptions about the distribution and hardness of random Hafnians (or “Torontonians”), and conclude the classical hardness of approximate GBS.  But this is theoretical work that remains to be done!</p>



<p><strong>IV. Application to Molecular Vibronic Spectra?</strong></p>



<p>In 2014, Alan Aspuru-Guzik and collaborators put out a <a href="https://arxiv.org/abs/1412.8427">paper</a> that made an amazing claim: namely that, contrary to what I and others had said, BosonSampling was <em>not</em> an intrinsically useless model of computation, good only for refuting QC skeptics like Gil Kalai!  Instead, they said, a BosonSampling device (specifically, what would later be called a GBS device) could be directly applied to solve a practical problem in quantum chemistry.  This is the computation of “molecular vibronic spectra,” also known as “Franck-Condon profiles,” whatever those are.</p>



<p>I never understood nearly enough about chemistry to evaluate this striking proposal, but I was always a bit skeptical of it, for the following reason.  Nothing in the proposal seemed to take seriously that BosonSampling is a <em>sampling</em> task!  A chemist would typically have some <em>specific numbers</em> that she wants to estimate, of which these “vibronic spectra” seemed to be an example.  But while it’s often convenient to estimate physical quantities via Monte Carlo sampling over simulated observations of the physical system you care about, that’s not the <em>only</em> way to estimate physical quantities!  And worryingly, in all the other examples we’d seen where BosonSampling could be used to estimate a number, the same number could <em>also</em> be estimated using one of several polynomial-time classical algorithms invented by Leonid Gurvits.  So why should vibronic spectra be an exception?</p>



<p>After an email exchange with Alex Arkhipov, Juan Miguel Arrazola, Leonardo Novo, and Raul Garcia-Patron, I believe we finally got to the bottom of it, and the answer is: vibronic spectra are <em>not</em> an exception.</p>



<p>In terms of BosonSampling, the vibronic spectra task is simply to estimate the probability histogram of some weighted sum like $$ w_1 s_1 + \cdots + w_ m s_m, $$ where w<sub>1</sub>,…,w<sub>m</sub> are fixed real numbers, and (s<sub>1</sub>,…,s<sub>m</sub>) is a possible outcome of the BosonSampling experiment, s<sub>i</sub> representing the number of photons observed in mode i.  Alas, while it takes some work, it turns out that Gurvits’s classical algorithms can be adapted to estimate these histograms.  Granted, running the actual BosonSampling experiment would provide <em>slightly</em> more detailed information—namely, some exact sampled values of $$ w_1 s_1 + \cdots + w_ m s_m, $$ rather than merely additive approximations to the values—but since we’d still need to sort those sampled values into coarse “bins” in order to compute a histogram, it’s not clear why that additional precision would ever be of chemical interest.</p>



<p>This is a pity, since if the vibronic spectra application <em>had</em> beaten what was doable classically, then it would’ve provided not merely a first practical use for BosonSampling, but also a lovely way to <em>verify</em> that a BosonSampling device was working as intended.</p>



<p><strong>V. Application to Finding Dense Subgraphs?</strong></p>



<p>A different potential application of Gaussian BosonSampling, first suggested by the Toronto-based startup <a href="https://www.xanadu.ai/">Xanadu</a>, is <a href="https://arxiv.org/abs/1803.10730">finding dense subgraphs in a graph</a>.  (Or at least, providing an initial seed to classical optimization methods that search for dense subgraphs.)</p>



<p>This is an NP-hard problem, so to say that I was skeptical of the proposal would be a gross understatement.  Nevertheless, it turns out that there <em>is</em> a striking observation by the Xanadu team at the core of their proposal: namely that, given a graph G and a positive even integer k, a GBS device can be used to sample a random subgraph of G of size k, with probability <em>proportional to the square of the number of perfect matchings in that subgraph</em>.  Cool, right?  And potentially even useful, especially if the number of perfect matchings could serve as a rough indicator of the subgraph’s density!  Alas, Xanadu’s Juan Miguel Arrazola himself recently told me that there’s a cubic-time classical algorithm for the same sampling task, so that the possible quantum speedup that one could get from GBS in this way is at most polynomial.  The search for a useful application of BosonSampling continues!</p>



<p/><hr/><p/>



<p>And that’s all for now!  I’m grateful to all the colleagues I talked to over the last couple weeks, including Alex Arkhipov, Juan Miguel Arrazola, Sergio Boixo, Raul Garcia-Patron, Leonid Gurvits, Gil Kalai, Chaoyang Lu, John Martinis, and Jelmer Renema, while obviously taking sole responsibility for any errors in the above.  I look forward to a spirited discussion in the comments, and of course I’ll post updates as I learn more!</p></div>
    </content>
    <updated>2020-12-16T08:16:30Z</updated>
    <published>2020-12-16T08:16:30Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-01-02T02:18:43Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2020/12/15/linkage</id>
    <link href="https://11011110.github.io/blog/2020/12/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>3d-printed models of the chaotic attractors from dynamical systems (\(\mathbb{M}\)). Stephen K. Lucas, Evelyn Sander, and Laura Taalman in the cover article of the latest Notices.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://www.ams.org/journals/notices/202011/rnoti-p1692.pdf">3d-printed models of the chaotic attractors from dynamical systems</a> (<a href="https://mathstodon.xyz/@11011110/105309562849621245">\(\mathbb{M}\)</a>). Stephen K. Lucas, Evelyn Sander, and Laura Taalman in the cover article of the latest <em>Notices</em>.</p>
  </li>
  <li>
    <p><a href="https://threadreaderapp.com/thread/1333670741590503425.html">Complete classification of tetrahedra whose angles are all rational multiples of \(\pi\)</a> (<a href="https://mathstodon.xyz/@11011110/105311921075649463">\(\mathbb{M}\)</a>, <a href="https://aperiodical.com/2020/12/aperiodical-news-roundup-november-2020/">via</a>). The original paper is “<a href="https://arxiv.org/abs/2011.14232">Space vectors forming rational angles</a>”, by Kiran S. Kedlaya, Alexander Kolpakov, Bjorn Poonen, and Michael Rubinstein.</p>
  </li>
  <li>
    <p><a href="https://www.maa.org/programs/faculty-and-departments/classroom-capsules-and-notes/geometry-strikes-again">Geometry strikes again</a> (<a href="https://mathstodon.xyz/@11011110/105320692550081128">\(\mathbb{M}\)</a>, <a href="https://www.metafilter.com/189571/slaps-roof-this-bad-boy-can-fit-so-many-fucking-polyhedra-in-it">via</a>), Branko Grünbaum, <em>Math. Mag.</em> 1985. Somehow I don’t think I’d encountered this short paper before but it’s filled with many examples of horribly-drawn mathematics, one in the logo of the MAA. Worth reading as a warning for what not to do. Also for clear instructions on how to draw regular icosahedra correctly.</p>
  </li>
  <li>
    <p><a href="https://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru">Ethical issues in large-corpus natural language processing, or what’s behind the research that got Timnit Gebru kicked out of Google</a> (<a href="https://mathstodon.xyz/@11011110/105326128589157740">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=25311402">via</a>).</p>
  </li>
  <li>
    <p><a href="https://jack.wrenn.fyi/blog/brown-location-surveillance/">How one university (Brown) tracks the physical locations of its students to ensure compliance with its pandemic safety policies</a> (<a href="https://mathstodon.xyz/@11011110/105330487336321470">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=25319392">via</a>). Most of it is pretty obvious: if you use a campus keycard or connect to a campus wireless network, they know you’re on campus.</p>
  </li>
  <li>
    <p><a href="https://www.forbes.com/sites/madhukarpai/2020/11/30/how-prestige-journals-remain-elite-exclusive-and-exclusionary/?sh=1c14baef4d48">How prestige journals remain elite, exclusive and exclusionary</a> (<a href="https://mathstodon.xyz/@11011110/105345990074471874">\(\mathbb{M}\)</a>, <a href="https://retractionwatch.com/2020/12/05/weekend-reads-google-ai-researcher-fired-after-being-asked-to-retract-paper-journal-accused-of-stonewalling-on-paper-used-to-justify-human-rights-violations-reflecting-on-a-covid-19-retraction/">via</a>). Nature is charging up to €9,500 per paper in open-access fees, as much as some scientists in third-world countries earn in a year, making open-access publication inaccessible to people from low- and middle-income countries.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean distance</a> (<a href="https://mathstodon.xyz/@11011110/105348855359182047">\(\mathbb{M}\)</a>), now a Good Article on Wikipedia.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/mathematician-solves-centuries-old-grazing-goat-problem-exactly-20201209/">Ingo Ullisch and the goats</a> (<a href="https://mathstodon.xyz/@btcprox/105354303333088090">\(\mathbb{M}\)</a>). A new solution to the problem of how to bisect the area of a circle by another circular arc centered on the first circle. But, given that it involves integrals and trig, is it really fair to call it “more exact” than the previous solution? I don’t think we even know whether the solution radius is transcendental (or transcendental over \(\pi\)).</p>
  </li>
  <li>
    <p><a href="http://www.anilaagha.com/sculpturelooksee">Anila Quayyum Agha’s openwork sculptures cast intricate tessellated shadows on the surrounding surfaces</a> (<a href="https://mathstodon.xyz/@11011110/105360602658987905">\(\mathbb{M}\)</a>). See also <a href="https://en.wikipedia.org/wiki/Anila_Quayyum_Agha">her Wikipedia article</a> and two stories on her work, “<a href="http://canjournal.org/2019/11/between-light-and-shadow-at-the-toledo-museum-of-art/">Between light and shadow at the Toledo Museum of Art</a>” and “<a href="https://news.artnet.com/art-world/anila-quayyum-agha-interview-741371">Anila Quayyum Agha on drawing inspiration from darkness</a>”.</p>
  </li>
  <li>
    <p>Pat Morin notes that it’s “good to see that the pandemic hasn’t affected every aspect of our lives”: <a href="https://mathstodon.xyz/@patmorin/105362925062596140">the registration fees for the online SODA conference are still way too high</a>.</p>
  </li>
  <li>
    <p><a href="https://statmodeling.stat.columbia.edu/2020/12/10/ieees-refusal-to-issue-corrections/">IEEE has no mechanism to publish corrections or errata to conference proceedings papers</a> (<a href="https://mathstodon.xyz/@11011110/105369247233466112">\(\mathbb{M}\)</a>, <a href="https://retractionwatch.com/2020/12/12/weekend-reads-p-hacking-the-us-election-an-apparently-fake-author-sinks-a-stock-sued-for-using-a-research-tool/">via</a>), violating IEEE’s own code of ethics requiring authors “to acknowledge and correct errors”:  Probably many other conference proceedings have similar issues.</p>
  </li>
  <li>
    <p><a href="https://cp4space.hatsya.com/2020/12/13/shallow-trees-with-heavy-leaves/">Shallow trees with heavy leaves</a> (<a href="https://mathstodon.xyz/@11011110/105375238890363766">\(\mathbb{M}\)</a>). On “the general strategy of searching much fewer positions and expending more effort on each position”, and its application in using SAT solvers to find new spaceships in cellular automata.</p>
  </li>
  <li>
    <p><a href="https://www.flyingcoloursmaths.co.uk/dictionary-of-mathematical-eponymy-the-xuong-tree">Dictionary of mathematical eponymy: The Xuong tree</a> (<a href="https://mathstodon.xyz/@11011110/105382771623486905">\(\mathbb{M}\)</a>, <a href="https://en.wikipedia.org/wiki/Xuong_tree">see also</a>), a special kind of spanning tree in graphs, used to embed them into surfaces with as high a genus as possible.</p>
  </li>
  <li>
    <p><a href="https://journals.carleton.ca/jocg/index.php/jocg/article/view/461">An explicit PL-embedding of the square flat torus into \(\mathbb{E}^3\)</a> (<a href="https://mathstodon.xyz/@11011110/105385674493868301">\(\mathbb{M}\)</a>). The square torus is like the old Asteroids arcade game: a Euclidean square with boundary conditions that wrap around so if you move off one edge you re-enter at the corresponding point of the opposite edge. In 4d, it has a nice representation as the set \(\{(a,b,c,d)\mid a^2+b^2=c^2+d^2=1\}\), the Cartesian product of two circles. The <a href="https://en.wikipedia.org/wiki/Nash_embedding_theorem">Nash embedding theorem</a> gives it fractal embeddings in 3d, but Tanessi Quintanar finds it as a bona fide polyhedron.</p>
  </li>
</ul></div>
    </content>
    <updated>2020-12-15T21:28:00Z</updated>
    <published>2020-12-15T21:28:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-01-01T06:02:43Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2020/12/15/postdoc-at-microsoft-research-apply-by-january-15-2021/</id>
    <link href="https://cstheory-jobs.org/2020/12/15/postdoc-at-microsoft-research-apply-by-january-15-2021/" rel="alternate" type="text/html"/>
    <title>Postdoc at Microsoft Research (apply by January 15, 2021)</title>
    <summary>The Algorithms group at Microsoft Research Redmond seeks exceptional researchers who are passionate about advancing the state of the art in theoretical computer science and having impact on the industry. Applicants must have a demonstrated ability for independent research and a strong academic publication record in theoretical computer science. Website: https://www.microsoft.com/en-us/research/group/algorithms-redmond/ Email: yekhanin@microsoft.com</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Algorithms group at Microsoft Research Redmond seeks exceptional researchers who are passionate about advancing the state of the art in theoretical computer science and having impact on the industry. Applicants must have a demonstrated ability for independent research and a strong academic publication record in theoretical computer science.</p>
<p>Website: <a href="https://www.microsoft.com/en-us/research/group/algorithms-redmond/">https://www.microsoft.com/en-us/research/group/algorithms-redmond/</a><br/>
Email: yekhanin@microsoft.com</p></div>
    </content>
    <updated>2020-12-15T20:30:15Z</updated>
    <published>2020-12-15T20:30:15Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-01-02T08:37:46Z</updated>
    </source>
  </entry>
</feed>
