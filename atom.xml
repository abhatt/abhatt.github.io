<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-04-14T02:22:05Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/053</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/053" rel="alternate" type="text/html"/>
    <title>TR21-053 |  Information in propositional proofs and algorithmic proof search | 

	Jan  Krajicek</title>
    <summary>We study from the proof complexity perspective the (informal) proof search problem:
Is there an optimal way to search for propositional proofs?
We note that for any fixed proof system there exists a time-optimal proof search algorithm. Using classical proof complexity results about reflection principles we prove that a time-optimal proof search algorithm exists w.r.t. all proof systems iff a p-optimal proof system exists.
To characterize precisely the time proof search algorithms need for individual formulas we introduce a new proof complexity measure based on algorithmic information concepts. In particular, to a proof system P we attach {\bf information-efficiency function} $i_P(\tau)$ assigning to a tautology a natural number, and we show that:
- $i_P(\tau)$ characterizes time any $P$-proof search algorithm has to use on $\tau$ and that for a fixed $P$ there is such an information-optimal algorithm,
- a proof system is information-efficiency optimal iff it is p-optimal,
- for non-automatizable systems $P$ there are formulas $\tau$ with short proofs but having large information measure $i_P(\tau)$.
We isolate and motivate the problem to establish {\em unconditional} super-logarithmic lower bounds for $i_P(\tau)$ where no super-polynomial size lower bounds are known. We also point out connections of the new measure with some topics in proof complexity other than proof search.</summary>
    <updated>2021-04-13T07:34:11Z</updated>
    <published>2021-04-13T07:34:11Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-04-14T02:20:28Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2104.05093</id>
    <link href="http://arxiv.org/abs/2104.05093" rel="alternate" type="text/html"/>
    <title>Load Balancing with Dynamic Set of Balls and Bins</title>
    <feedworld_mtime>1618272000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Aamand:Anders.html">Anders Aamand</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Knudsen:Jakob_B=aelig=k_Tejs.html">Jakob BÃ¦k Tejs Knudsen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Thorup:Mikkel.html">Mikkel Thorup</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2104.05093">PDF</a><br/><b>Abstract: </b>In dynamic load balancing, we wish to distribute balls into bins in an
environment where both balls and bins can be added and removed. We want to
minimize the maximum load of any bin but we also want to minimize the number of
balls and bins affected when adding or removing a ball or a bin. We want a
hashing-style solution where we given the ID of a ball can find its bin
efficiently.
</p>
<p>We are given a balancing parameter $c=1+\epsilon$, where $\epsilon\in (0,1)$.
With $n$ and $m$ the current numbers of balls and bins, we want no bin with
load above $C=\lceil c n/m\rceil$, referred to as the capacity of the bins.
</p>
<p>We present a scheme where we can locate a ball checking $1+O(\log
1/\epsilon)$ bins in expectation. When inserting or deleting a ball, we expect
to move $O(1/\epsilon)$ balls, and when inserting or deleting a bin, we expect
to move $O(C/\epsilon)$ balls. Previous bounds were off by a factor
$1/\epsilon$.
</p>
<p>These bounds are best possible when $C=O(1)$ but for larger $C$, we can do
much better: Let $f=\epsilon C$ if $C\leq \log 1/\epsilon$,
$f=\epsilon\sqrt{C}\cdot \sqrt{\log(1/(\epsilon\sqrt{C}))}$ if $\log
1/\epsilon\leq C&lt;\tfrac{1}{2\epsilon^2}$, and $C=1$ if $C\geq
\tfrac{1}{2\epsilon^2}$. We show that we expect to move $O(1/f)$ balls when
inserting or deleting a ball, and $O(C/f)$ balls when inserting or deleting a
bin.
</p>
<p>For the bounds with larger $C$, we first have to resolve a much simpler
probabilistic problem. Place $n$ balls in $m$ bins of capacity $C$, one ball at
the time. Each ball picks a uniformly random non-full bin. We show that in
expectation and with high probability, the fraction of non-full bins is
$\Theta(f)$. Then the expected number of bins that a new ball would have to
visit to find one that is not full is $\Theta(1/f)$. As it turns out, we obtain
the same complexity in our more complicated scheme where both balls and bins
can be added and removed.
</p></div>
    </summary>
    <updated>2021-04-13T22:38:56Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-04-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2104.05091</id>
    <link href="http://arxiv.org/abs/2104.05091" rel="alternate" type="text/html"/>
    <title>Simple, Optimal Algorithms for Random Sampling Without Replacement</title>
    <feedworld_mtime>1618272000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Ting:Daniel.html">Daniel Ting</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2104.05091">PDF</a><br/><b>Abstract: </b>Consider the fundamental problem of drawing a simple random sample of size k
without replacement from [n] := {1, . . . , n}. Although a number of classical
algorithms exist for this problem, we construct algorithms that are even
simpler, easier to implement, and have optimal space and time complexity.
</p></div>
    </summary>
    <updated>2021-04-13T22:38:48Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-04-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2104.05065</id>
    <link href="http://arxiv.org/abs/2104.05065" rel="alternate" type="text/html"/>
    <title>The algebraic structure of the densification and the sparsification tasks for CSPs</title>
    <feedworld_mtime>1618272000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Takhanov:Rustem.html">Rustem Takhanov</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2104.05065">PDF</a><br/><b>Abstract: </b>The tractability of certain CSPs for dense or sparse instances is known from
the 90s. Recently, the densification and the sparsification of CSPs were
formulated as computational tasks and the systematical study of their
computational complexity was initiated.
</p>
<p>We approach this problem by introducing the densification operator, i.e. the
closure operator that, given an instance of a CSP, outputs all constraints that
are satisfied by all of its solutions. According to the Galois theory of
closure operators, any such operator is related to a certain implicational
system (or, a functional dependency) $\Sigma$. We are specifically interested
in those classes of fixed-template CSPs, parameterized by constraint languages
$\Gamma$, for which the size of an implicational system $\Sigma$ is a
polynomial in the number of variables $n$. We show that in the Boolean case,
$\Sigma$ is of polynomial size if and only if $\Gamma$ is of bounded width. For
such languages, $\Sigma$ can be computed in log-space or in a logarithmic time
with a polynomial number of processors. Given an implicational system $\Sigma$,
the densification task is equivalent to the computation of the closure of input
constraints. The sparsification task is equivalent to the computation of the
minimal key. This leads to ${\mathcal O}({\rm poly}(n)\cdot N^2)$-algorithm for
the sparsification task where $N$ is the number of non-redundant
sparsifications of an original CSP.
</p>
<p>Finally, we give a complete classification of constraint languages over the
Boolean domain for which the densification problem is tractable.
</p></div>
    </summary>
    <updated>2021-04-13T22:37:31Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-04-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2104.04940</id>
    <link href="http://arxiv.org/abs/2104.04940" rel="alternate" type="text/html"/>
    <title>Dissecting the square into seven congruent parts</title>
    <feedworld_mtime>1618272000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Gerardo L. Maldonado, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rold=aacute=n=Pensado:Edgardo.html">Edgardo RoldÃ¡n-Pensado</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2104.04940">PDF</a><br/><b>Abstract: </b>We give a computer-based proof of the following fact: If a square is tiled by
seven convex tiles which are congruent among themselves, then the tiles are
rectangles. This confirms a new case of a conjecture posed by Yuen, Zamfirescu
and Zamfirescu.
</p></div>
    </summary>
    <updated>2021-04-13T22:48:24Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-04-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2104.04908</id>
    <link href="http://arxiv.org/abs/2104.04908" rel="alternate" type="text/html"/>
    <title>Graph Streaming Lower Bounds for Parameter Estimation and Property Testing via a Streaming XOR Lemma</title>
    <feedworld_mtime>1618272000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Assadi:Sepehr.html">Sepehr Assadi</a>, Vishvajeet N <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2104.04908">PDF</a><br/><b>Abstract: </b>We study space-pass tradeoffs in graph streaming algorithms for parameter
estimation and property testing problems such as estimating the size of maximum
matchings and maximum cuts, weight of minimum spanning trees, or testing if a
graph is connected or cycle-free versus being far from these properties. We
develop a new lower bound technique that proves that for many problems of
interest, including all the above, obtaining a $(1+\epsilon)$-approximation
requires either $n^{\Omega(1)}$ space or $\Omega(1/\epsilon)$ passes, even on
highly restricted families of graphs such as bounded-degree planar graphs. For
multiple of these problems, this bound matches those of existing algorithms and
is thus (asymptotically) optimal.
</p>
<p>Our results considerably strengthen prior lower bounds even for arbitrary
graphs: starting from the influential work of [Verbin, Yu; SODA 2011], there
has been a plethora of lower bounds for single-pass algorithms for these
problems; however, the only multi-pass lower bounds proven very recently in
[Assadi, Kol, Saxena, Yu; FOCS 2020] rules out sublinear-space algorithms with
exponentially smaller $o(\log{(1/\epsilon)})$ passes for these problems.
</p>
<p>One key ingredient of our proofs is a simple streaming XOR Lemma, a generic
hardness amplification result, that we prove: informally speaking, if a
$p$-pass $s$-space streaming algorithm can only solve a decision problem with
advantage $\delta &gt; 0$ over random guessing, then it cannot solve XOR of $\ell$
independent copies of the problem with advantage much better than
$\delta^{\ell}$. This result can be of independent interest and useful for
other streaming lower bounds as well.
</p></div>
    </summary>
    <updated>2021-04-13T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-04-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2104.04853</id>
    <link href="http://arxiv.org/abs/2104.04853" rel="alternate" type="text/html"/>
    <title>Beyond Pointwise Submodularity: Non-Monotone Adaptive Submodular Maximization subject to a Knapsack Constraint</title>
    <feedworld_mtime>1618272000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tang:Shaojie.html">Shaojie Tang</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2104.04853">PDF</a><br/><b>Abstract: </b>In this paper, we study the non-monotone adaptive submodular maximization
problem subject to a knapsack constraint. The input of our problem is a set of
items, where each item has a particular state drawn from a known prior
distribution. However, the state of an item is initially unknown, one must
select an item in order to reveal the state of that item. Moreover, each item
has a fixed cost. There is a utility function which is defined over items and
states. Our objective is to sequentially select a group of items to maximize
the expected utility subject to a knapsack constraint. Although the
cardinality-constrained, as well as the more general matroid-constrained,
adaptive submodular maximization has been well studied in the literature,
whether there exists a constant approximation solution for the
knapsack-constrained adaptive submodular maximization problem remains an open
problem. We fill this gap by proposing the first constant approximation
solution. In particular, our main contribution is to develop a sampling-based
randomized algorithm that achieves a $\frac{1}{10}$ approximation for
maximizing an adaptive submodular function subject to a knapsack constraint.
</p></div>
    </summary>
    <updated>2021-04-13T22:42:22Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-04-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2104.04711</id>
    <link href="http://arxiv.org/abs/2104.04711" rel="alternate" type="text/html"/>
    <title>Information in propositional proofs and algorithmic proof search</title>
    <feedworld_mtime>1618272000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Jan Krajicek <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2104.04711">PDF</a><br/><b>Abstract: </b>We study from the proof complexity perspective the (informal) proof search
problem:
</p>
<p>Is there an optimal way to search for propositional proofs?
</p>
<p>We note that for any fixed proof system there exists a time-optimal proof
search algorithm. Using classical proof complexity results about reflection
principles we prove that a time-optimal proof search algorithm exists w.r.t.
all proof systems iff a p-optimal proof system exists.
</p>
<p>To characterize precisely the time proof search algorithms need for
individual formulas we introduce a new proof complexity measure based on
algorithmic information concepts. In particular, to a proof system $P$ we
attach {\bf information-efficiency function} $i_P(\tau)$ assigning to a
tautology a natural number, and we show that:
</p>
<p>- $i_P(\tau)$ characterizes time any $P$-proof search algorithm has to use on
$\tau$ and that for a fixed $P$ there is such an information-optimal algorithm,
</p>
<p>- a proof system is information-efficiency optimal iff it is p-optimal,
</p>
<p>- for non-automatizable systems $P$ there are formulas $\tau$ with short
proofs but having large information measure $i_P(\tau)$.
</p>
<p>We isolate and motivate the problem to establish {\em unconditional}
super-logarithmic lower bounds for $i_P(\tau)$ where no super-polynomial size
lower bounds are known. We also point out connections of the new measure with
some topics in proof complexity other than proof search.
</p></div>
    </summary>
    <updated>2021-04-13T22:38:10Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-04-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2104.04614</id>
    <link href="http://arxiv.org/abs/2104.04614" rel="alternate" type="text/html"/>
    <title>Efficient and Robust Discrete Conformal Equivalence with Boundary</title>
    <feedworld_mtime>1618272000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Campen:Marcel.html">Marcel Campen</a>, Ryan Capouellez, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shen:Hanxiao.html">Hanxiao Shen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhu:Leyi.html">Leyi Zhu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Panozzo:Daniele.html">Daniele Panozzo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zorin:Denis.html">Denis Zorin</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2104.04614">PDF</a><br/><b>Abstract: </b>We describe an efficient algorithm to compute a conformally equivalent metric
for a discrete surface, possibly with boundary, exhibiting prescribed Gaussian
curvature at all interior vertices and prescribed geodesic curvature along the
boundary. Our construction is based on the theory developed in [Gu et al. 2018;
Springborn 2020], and in particular relies on results on hyperbolic Delaunay
triangulations. Generality is achieved by considering the surface's intrinsic
triangulation as a degree of freedom, and particular attention is paid to the
proper treatment of surface boundaries. While via a double cover approach the
boundary case can be reduced to the closed case quite naturally, the implied
symmetry of the setting causes additional challenges related to stable
Delaunay-critical configurations that we address explicitly in this work.
</p></div>
    </summary>
    <updated>2021-04-13T22:48:47Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-04-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2104.04525</id>
    <link href="http://arxiv.org/abs/2104.04525" rel="alternate" type="text/html"/>
    <title>Coordinate descent heuristics for the irregular strip packing problem of rasterized shapes</title>
    <feedworld_mtime>1618272000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Umetani:Shunji.html">Shunji Umetani</a>, Shohei Murakami <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2104.04525">PDF</a><br/><b>Abstract: </b>We consider the irregular strip packing problem of rasterized shapes, where a
given set of pieces of irregular shapes represented in pixels should be placed
into a rectangular container without overlap. The rasterized shapes enable us
to check overlap without any exceptional handling due to geometric issues,
while they often require much memory and computational effort in
high-resolution. We develop an efficient algorithm to check overlap using a
pair of scanlines that reduces the complexity of rasterized shapes by merging
consecutive pixels in each row and column into strips with unit width,
respectively. Based on this, we develop coordinate descent heuristics that
repeat a line search in the horizontal and vertical directions alternately.
Computational results for test instances show that the proposed algorithm
obtains sufficiently dense layouts of rasterized shapes in high-resolution
within a reasonable computation time.
</p></div>
    </summary>
    <updated>2021-04-13T22:48:33Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-04-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2104.04324</id>
    <link href="http://arxiv.org/abs/2104.04324" rel="alternate" type="text/html"/>
    <title>Ranking Bracelets in Polynomial Time</title>
    <feedworld_mtime>1618272000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Adamson:Duncan.html">Duncan Adamson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Deligkas:Argyrios.html">Argyrios Deligkas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gusev:Vladimir_V=.html">Vladimir V. Gusev</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Potapov:Igor.html">Igor Potapov</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2104.04324">PDF</a><br/><b>Abstract: </b>The main result of the paper is the first polynomial-time algorithm for
ranking bracelets. The time-complexity of the algorithm is O(k^2 n^4), where k
is the size of the alphabet and n is the length of the considered bracelets.
The key part of the algorithm is to compute the rank of any word with respect
to the set of bracelets by finding three other ranks: the rank over all
necklaces, the rank over palindromic necklaces, and the rank over enclosing
apalindromic necklaces. The last two concepts are introduced in this paper.
These ranks are key components to our algorithm in order to decompose the
problem into parts. Additionally, this ranking procedure is used to build a
polynomial-time unranking algorithm.
</p></div>
    </summary>
    <updated>2021-04-13T22:47:09Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-04-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2103.10315</id>
    <link href="http://arxiv.org/abs/2103.10315" rel="alternate" type="text/html"/>
    <title>Lorentz Quantum Computer</title>
    <feedworld_mtime>1618272000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/He:Wenhao.html">Wenhao He</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Zhenduo.html">Zhenduo Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wu:Biao.html">Biao Wu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2103.10315">PDF</a><br/><b>Abstract: </b>A theoretical model of computation is proposed based on Lorentz quantum
mechanics. Besides the standard qubits, this model has an additional bit, which
we call hyperbolic bit (or hybit in short). A set of universal gates are
constructed and their universality is proved rigorously. As an application, a
search algorithm is designed for this computer model and is found to be faster
than the Grover's search algorithm for the standard quantum computer. Physical
implementation of this computation model is discussed.
</p></div>
    </summary>
    <updated>2021-04-13T22:38:13Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2021-04-13T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18514</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/" rel="alternate" type="text/html"/>
    <title>Wobble in the Standard Model</title>
    <summary>Prediction is very difficult, especially if itâs about the futureâNiels Bohr Boston Globe âUncertaintyâ source Lisa Randall is a professor of theoretical physics at Harvard. Her research has touched on many of the basic questions of modern physics: supersymmetry, Standard Model observables, cosmological inflation, baryogenesis, grand unified theories, and general relativity. She has also written [â¦]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Prediction is very difficult, especially if itâs about the futureâNiels Bohr</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/randalluncertainty/" rel="attachment wp-att-18526"><img alt="" class="alignright wp-image-18526" height="210" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/RandallUncertainty.jpg?resize=153%2C210&amp;ssl=1" width="153"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Boston Globe âUncertaintyâ <a href="https://www.bostonglobe.com/ideas/2011/10/22/lisa-randall-physics-universe-uncertainty/BfSYjipZy7HkQPmmRs8ZRI/story.html">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Lisa Randall is a professor of theoretical physics at Harvard. Her research has touched on many of the basic questions of modern physics: supersymmetry, Standard Model observables, cosmological inflation, baryogenesis, grand unified theories, and general relativity. She has also written popular books about her work and science in general. Thus she has a handle on aspects of science that overlap my expertiseânot to mention those of her sister Dana Randall, whom I have known as a colleague for many years.</p>
<p>
Today, Ken and I thought we would talk about recent developments in particle physics, and their connection to two topics dear to us. </p>
<p>
Randallâs most recent popular book is <a href="https://en.wikipedia.org/wiki/Dark_Matter_and_the_Dinosaurs">Dark Matter and the Dinosaurs</a>. The idea she advances is that the periodic extinctions in Earthâs history may have been caused when the solar system passes through a plane of dark matter within our galaxy. But dark matter and also dark energy have come under <a href="https://www.sciencenews.org/article/dark-matter-mystery-deepens-demise-reported-detection">increasing</a> <a href="https://phys.org/news/2021-03-composition-percent-universe.html">recent</a> <a href="https://www.nbcnews.com/science/space/maybe-dark-matter-doesn-t-exist-after-all-new-research-n1252995">doubt</a>, even from their original <a href="https://www.newscientist.com/article/mg24632851-400-why-the-universe-i-invented-is-right-but-still-not-the-final-answer/">formulator</a>. Maybe Niels Bohrâs quote should also say: </p>
<blockquote><p><b> </b> <em> <i>Prediction is very difficult, especially if itâs about the past.</i> </em>
</p></blockquote>
<p>
Randallâs previous book, <a href="https://en.wikipedia.org/wiki/Knocking_on_Heaven's_Door_(book)">Knocking on Heavenâs Door</a>, is most relevant to this post. The 1973 Bob Dylan <a href="https://en.wikipedia.org/wiki/Knockin'_on_Heaven's_Door">song</a> title it pinches describes the feeling of doing frontier physical science. Insofar as her own work is mostly theoretical, much of it connects to feelings we have in computer scienceâespecially complexity lower bounds where the door mostly feels slammed shut. </p>
<p>
But the book is also about the practice of experimental scienceânot only how to gather knowledge but when and how we can have confidence in it. Its long middle part is titled, âMachines, Measurements, and Probability.â All three elements are foremost in considering a new development that involves two measurements taken 20 years apart.</p>
<p>
<a href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/randallbooks/" rel="attachment wp-att-18528"><img alt="" class="aligncenter wp-image-18528" height="254" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/RandallBooks.jpg?resize=406%2C254&amp;ssl=1" width="406"/></a></p>
<p/><h2> Muons </h2><p/>
<p>
Last Tuesdayâs New York Times <a href="https://www.nytimes.com/2021/04/07/science/particle-physics-muon-fermilab-brookhaven.html">highlighted</a> a potential discovery in particle physics. It was in their Tuesday science section. </p>
<p>
The result is an experimental discovery that could show that the current model of matter is wrong. </p>
<blockquote><p><b> </b> <em> âThis is our Mars rover landing moment,â said Chris Polly, a physicist at the Fermi National Accelerator Laboratory, or Fermilab, in Batavia, Ill., who has been working toward this finding for most of his career. </em>
</p></blockquote>
<p>
Indeed. It is a Mars landing moment. They both involved many people, lots of exotic machinery, lots of money, many years. Say three billion dollars or so for Mars. Say nearly the same amount for muonsâthe annual budget for Fermilab is over one half billion dollars. It was certainly enough to reassemble and upgrade a huge accelerator ring that was <a href="https://www.bnl.gov/newsroom/news.php?a=112259">first used</a> at Brookhaven National Lab on Long Island in 2001:</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/fermilabring/" rel="attachment wp-att-18539"><img alt="" class="aligncenter wp-image-18539" height="301" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/FermilabRing.jpg?resize=450%2C301&amp;ssl=1" width="450"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">NY Times <a href="https://www.nytimes.com/2021/04/07/science/particle-physics-muon-fermilab-brookhaven.html">src</a></font>
</td>
</tr>
</tbody></table>
<p>
The study used the muon to probe the Standard Model of physics. Muons are useful because they are charged like an electron, which helps control them in an accelerator. Yet their mass is roughly 207 times larger than an electron. The same charge helps control their motion and the large mass makes collisions more interesting. As Polly stated in Natalie Wolchoverâs <a href="https://www.quantamagazine.org/muon-g-2-experiment-at-fermilab-finds-hint-of-new-particles-20210407/">story</a> for <em>Quanta</em>:</p>
<blockquote><p><b> </b> <em> â[I]f youâre looking for particles that could explain the missing mass of the universeâdark matterâor youâre looking for [supersymmetry], thatâs where the muon has a unique role.â </em>
</p></blockquote>
<p/><h2> Theory and Jealousy </h2><p/>
<p>
Computer science theory is so different from high end physics. We are closer to the type of research that Randall does. We involve few people, no exotic machinery, and small amounts of money. Maybe the closest attribute we have to high-end research is we also take years and years.</p>
<p>
Perhaps we are also jealous of high-end physics. Not just for money, but for the ability of particle physicists to get announcements into the New York Times. Polly said the <a href="https://www.energy.gov/science/articles/first-person-science-chris-polly-muon-physics">following</a> about the ending day of the muon experiment two decades ago:</p>
<blockquote><p><b> </b> <em> When we revealed the results, people from all over the world flew in to visit the lab. These experiments take decades to build and analyze, so you donât get to go to very many of these events. We did a little âDrumroll, pleaseâ and then had the postdoc managing the spreadsheet hit the button to show it on the projector. Lo and behold, you could see that there was still a three-sigma discrepancy! </em>
</p></blockquote>
<p>
At the time he was a graduate assistant assigned to machinery for measuring particle energies. He had fixed a problem where someone had touched a component with bare hands and thereby ruined its sheathing. All such problems were meant to be ironed-out by the drum-roll event. But all this raises two further interesting issues that connect the muon results with issues we think about in computer science. Letâs look at them next.</p>
<p/><h2> Three to Four Sigma </h2><p/>
<p>
In an experimental science one must be aware that results are not exact. They are samples from some random process. Flip a coin 10 times in a row. If they all come up heads what does that mean? Could be the coin is fair but this happens about one time in a thousand. Or the coin is biased. Or something else. </p>
<p>
Flip a muon many times. That is sample some muon experiment. The outcome is from a random process. Some of it comes from properties of the natural processes themselves and others from incidentals of the measurement apparatus. How do we decide if the experiment means what we think it does?</p>
<p>
The theory developed by Carl Gauss and others before and after to delineate the normal distribution was largely prompted by analysis of measurement errors <a href="https://www.maa.org/sites/default/files/images/upload_library/22/Allendoerfer/stahl96.pdf">to begin with</a>. This yields the â<a href="https://en.wikipedia.org/wiki/68-95-99.7_rule">rule of three</a>,â about the percentage of values that naturally lie within an interval estimate in a normal distribution: 68%, 95%, and 99.7% of the values lie within one, two, and three standard deviations of the mean, respectively.</p>
<p>
The question is, how to assess cases where the measurement result is well outside these intervalsâwhen can we conclude it is more than a deviation by natural chance? In social sciences a result is âsignificantâ provided it lies outside two-sigma. In particle physics, there is a convention of a five-sigma effect (99.99994% confidence) being required to qualify as a discovery. No Nobel prize for less. </p>
<p>
The situation with the muons has an extra factor of repeated measurementsâbut there have been only two measurements so far:</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/12/wobble-in-the-standard-model/muoncharts/" rel="attachment wp-att-18531"><img alt="" class="aligncenter wp-image-18531" height="247" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/MuonCharts.png?resize=550%2C247&amp;ssl=1" width="550"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite of <a href="https://news.fnal.gov/2021/04/first-results-from-fermilabs-muon-g-2-experiment-strengthen-evidence-of-new-physics/">src1</a>, <a href="https://www.sciencemag.org/news/2021/04/particle-mystery-deepens-physicists-confirm-muon-more-magnetic-predicted">src2</a></font>
</td>
</tr>
</tbody></table>
<p>
The blue line in the left figure is the original Brookhaven measurement; the red is the new one. There is also <a href="https://4gravitons.com/2021/04/09/theoretical-uncertainty-and-uncertain-theory/">theoretical uncertainty</a> in the calculation of the Standard Model prediction, and that combines with the measurement error bars to give the sigma baseline. The chart at right normalizes the deviation to parts per billionâthe measurements need to be incredibly fine. This scale appears to be about 25% under the current <img alt="{\sigma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-scale (it shows about <img alt="{2.8}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2.8%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for Brookhaven compared to its <img alt="{3.7\sigma}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3.7%5Csigma%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> after <a href="https://arxiv.org/pdf/2006.04822.pdf">revised</a> uncertainty) but it is close enough to get the picture.</p>
<p>
Although the new Fermilab result by itself deviates slightly less from the Standard Model, it corroborates the earlier measurement. It is not fully independent from it, but the combination is enough to raise the current claimed deviation to about <img alt="{4.2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B4.2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> sigmas. This is well above social science level but below Nobel level. This is with respect to the probability that the effects are real. </p>
<p>
The social significance of <img alt="{4.2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B4.2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is that it is above the â<img alt="{3+}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B3%2B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>â level where hoped-for anomalies have subsequently disappeared for reasons chalked up to natural chance. This is because physicists around the world do many a hundredfold amount of hopeful measurements. Some measurements get initial âbumpsâ up just because of the numbers. But <img alt="{4.2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B4.2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> reduces the natural frequency under 1-in-40,000. This is why reproducing measurements is so important, why the new Fermilab team devoted all the expense and effort. A more independent measurement on other machines could give a higher boost that might get over the <img alt="{5.0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B5.0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> line. Time to break out the wallets and hammers?</p>
<p>
The <img alt="{4.2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B4.2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is not, however, beyond the realm of <a href="https://en.wikipedia.org/wiki/Faster-than-light_neutrino_anomaly">recent</a> <a href="https://physics.aps.org/articles/v11/s78">experience</a> with apparatus faults and modeling error. On the latter, there is <a href="https://www.math.columbia.edu/~woit/wordpress/?p=12292">still</a> some <a href="http://resonaances.blogspot.com/2021/04/why-is-it-when-something-happens-it-is.html">doubt</a> about the theoretical prediction for the muonâs magnetic moment. In any event, the muon results are exciting but still below what is required for a true discovery. Time will tell.</p>
<p>
The second factor we draw attention to concerns the human âhopingâ directly.</p>
<p/><h2> Blinding </h2><p/>
<p>
In any experimental science one must also be aware that people are not unbiased. Scientists have much invested in the outcome of their experiments. Think jobs, tenure even, funding, and more. So a big physics experiment like the muon one must be careful. They follow standard practice to perform <a href="https://en.wikipedia.org/wiki/blinded_experiment">blinded</a> data analysis. </p>
<p>
This surprised me. This blinding is a crypto-type protocol, which is something we computer scientists study. The muon team performed a protocol that protected against cheating. Here is how they did it:</p>
<blockquote><p><b> </b> <em> In this case, the master clock that keeps track of the muonsâ wobble had been set to a rate unknown to the researchers. The figure was sealed in envelopes that were locked in the offices at Fermilab and the University of Washington in Seattle.</em></p><em>
<p>
In a ceremony on Feb. 25 that was recorded on video and watched around the world on Zoom, Dr. Polly opened the Fermilab envelope and David Hertzog from the University of Washington opened the Seattle envelope. The number inside was entered into a spreadsheet, providing a key to all the data, and the result popped out to a chorus of wows.</p>
</em><p><em>
âThat really led to a really exciting moment, because nobody on the collaboration knew the answer until the same moment,â said Saskia Charity, a Fermilab postdoctoral fellow who has been working remotely from Liverpool, England, during the pandemic. </em>
</p></blockquote>
<p>
This mechanism for blinding suggests possible crypto questions. They hid the master clock rate. Can this be modeled as one of our crypto problems? Can we prove some security bounds? If they claim that hiding the rate protects against cheating then they should be able to make this claim precise. The <a href="https://en.wikipedia.org/wiki/First_observation_of_gravitational_waves">discovery</a> of gravitational waves used a <a href="https://www.ligo.org/news/blind-injection.php">blind injection</a> scheme tailored for that experiment. How can this be generalized?</p>
<p/><h2> Open Problems </h2><p/>
<p>
We have discussed two aspects that involve soft numbers rather than hard machines and hard-shelled particles. Perhaps they are interesting new problems for us? What do you think?</p>
<p/></font></font></div>
    </content>
    <updated>2021-04-12T23:11:21Z</updated>
    <published>2021-04-12T23:11:21Z</published>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="blind injection"/>
    <category term="Chris Polly"/>
    <category term="Lisa Randall"/>
    <category term="muon"/>
    <category term="particles"/>
    <category term="Physics"/>
    <category term="result"/>
    <category term="standard model"/>
    <category term="uncertainty"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>GÃ¶del's Lost Letter and P=NP</title>
      <updated>2021-04-14T02:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/052</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/052" rel="alternate" type="text/html"/>
    <title>TR21-052 |  Upslices, Downslices, and Secret-Sharing with Complexity of $1.5^n$ | 

	Oded Nir, 

	Benny Applebaum</title>
    <summary>A secret-sharing scheme allows to distribute a secret $s$ among $n$ parties such that only some predefined ``authorized'' sets of parties can reconstruct the secret, and all other ``unauthorized'' sets learn nothing about $s$. 
The collection of authorized/unauthorized sets can be captured by a monotone function $f:\{0,1\}^n\rightarrow \{0,1\}$. 
In this paper, we focus on monotone functions that all their min-terms are sets of size $a$, and on their duals -- monotone functions whose max-terms are of size $b$. We refer to these classes as $(a,n)$-upslices and $(b,n)$-downslices, and note that these natural families correspond to monotone $a$-regular DNFs and monotone $(n-b)$-regular CNFs. We derive the following results.

1. (General downslices) Every downslice can be realized with total share size of $1.5^{n+o(n)}&lt;2^{0.585 n}$. Since every monotone function can be cheaply decomposed into $n$ downslices, we obtain a similar result for general access structures improving the previously known $2^{0.637n+o(n)}$ complexity of Applebaum, Beimel, Nir and Peter (STOC 2020). We also achieve a minor improvement in the exponent of linear secrets sharing schemes. 

2. (Random mixture of upslices) Following Beimel and Farras (TCC 2020) who studied the complexity of random DNFs with constant-size terms, we consider the following general distribution $F$ over monotone DNFs: For each width value $a\in [n]$, uniformly sample $k_a$ monotone terms of size $a$, where $k=(k_1,\ldots,k_n)$ is an arbitrary vector of non-negative integers. We show that, except with exponentially small probability, $F$ can be realized with share size of $2^{0.5 n+o(n)}$ and
    can be linearly realized with an exponent strictly smaller than $2/3$. Our proof also provides a candidate distribution for ``exponentially-hard'' access structure. 
    
We use our results to explore connections between several seemingly unrelated questions about the complexity of secret-sharing schemes such as worst-case vs. average-case, linear vs. non-linear and primal vs. dual access structures. We prove that, in at least one of these settings, there is a significant gap in secret-sharing complexity.</summary>
    <updated>2021-04-12T17:17:39Z</updated>
    <published>2021-04-12T17:17:39Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-04-14T02:20:28Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8147147831052480535</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8147147831052480535/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/04/is-following-reaction-to-getting-first.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8147147831052480535" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8147147831052480535" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/04/is-following-reaction-to-getting-first.html" rel="alternate" type="text/html"/>
    <title>Is the following reaction to getting the first COVID shot logical?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Â Alice works at a charity that puts together bag and box lunches for children.</p><p><br/>They all wear masks and they are 12 feet apart and very careful, and nobody there has gotten COVID.</p><p>Then Alice gets here first COVID shot and says:</p><p><br/></p><p><i>I am not going to work for that charity until I have had my second shot and waitedÂ  4 weeks so I am immune.Â </i></p><p><i><br/></i></p><p>She is really scared of getting COVID NOW thatÂ  she is on the verge of being immune.Â </p><p><br/></p><p>Is that logical? She was not scared before. So does it make sense to be scared now? I see where she is coming from emotionally, but is there a logical argument for her viewpoint? I ask nonrhetorically.</p><p><br/></p><p>bill g.Â </p></div>
    </content>
    <updated>2021-04-12T04:12:00Z</updated>
    <published>2021-04-12T04:12:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-04-14T02:01:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5437</id>
    <link href="https://www.scottaaronson.com/blog/?p=5437" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5437#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5437" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Just some prizes</title>
    <summary xml:lang="en-US">Oded Goldreich is a theoretical computer scientist at the Weizmann Institute in Rehovot, Israel. Heâs best known for helping to lay the rigorous foundations of cryptography in the 1980s, through seminal results like the Goldreich-Levin Theorem (every one-way function can be modified to have a hard-core predicate), the Goldreich-Goldwasser-Micali Theorem (every pseudorandom generator can be [â¦]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://en.wikipedia.org/wiki/Oded_Goldreich">Oded Goldreich</a> is a theoretical computer scientist at the Weizmann Institute in Rehovot, Israel.  Heâs best known for helping to lay the rigorous foundations of cryptography in the 1980s, through seminal results like the <a href="https://en.wikipedia.org/wiki/Hard-core_predicate">Goldreich-Levin Theorem</a> (every one-way function can be modified to have a hard-core predicate), the <a href="https://people.csail.mit.edu/silvio/Selected%20Scientific%20Papers/Pseudo%20Randomness/How%20To%20Construct%20Random%20Functions.pdf">Goldreich-Goldwasser-Micali Theorem</a> (every pseudorandom generator can be made into a pseudorandom function), and the <a href="https://www.cs.purdue.edu/homes/hmaji/teaching/Fall%202017/lectures/39.pdf">Goldreich-Micali-Wigderson protocol</a> for secure multi-party computation.  I first met Oded more than 20 years ago, when he lectured at a summer school at the Institute for Advanced Study in Princeton, barefoot and wearing a tank top and what looked like pajama pants.  It was a bracing introduction to complexity-theoretic cryptography.  Since then, Iâve interacted with Oded from time to time, partly around his <a href="http://www.wisdom.weizmann.ac.il/~oded/on-qc.html">firm belief</a> that quantum computing is impossible.</p>



<p>Last month a committee in Israel voted to award Goldreich the <a href="https://en.wikipedia.org/wiki/Israel_Prize">Israel Prize</a> (roughly analogous to the US National Medal of Science), for which Iâd say Goldreich had been a plausible candidate for decades.  But alas, Yoav Gallant, Netanyahuâs Education Minister, then rather <a href="https://www.jpost.com/israel-news/high-court-revokes-israel-prize-in-math-to-pro-bds-professor-664538">non-gallantly blocked the award</a>, solely because he objected to Goldreichâs far-left political views (and apparently because of various statements Goldreich signed, including in support of a boycott of Ariel University, which is in the West Bank).  The case went all the way to the Israeli Supreme Court (!), which <a href="https://www.washingtonpost.com/world/middle_east/israeli-academic-wont-receive-prize-after-signing-petition/2021/04/08/d1e987ca-987b-11eb-8f0a-3384cf4fb399_story.html">ruled two days ago</a> in Gallantâs favor: he gets to âdelayâ the award to investigate the matter further, and in the meantime has apparently sent out invitations for an award ceremony next week that doesnât include Goldreich.  Some are now calling for the other winners to boycott the prize in solidarity until this is righted.</p>



<p>I doubt readers of this blog need convincing that this is a travesty and an embarrassment, a <em><a href="https://en.wiktionary.org/wiki/shanda#:~:text=shanda%20(uncountable),(Jewish)%20shame%3B%20disgrace.">shanda</a></em>, for the Netanyahu government itself.  That I disagree with Goldreichâs far-left views (or <em>might</em> disagree, if I knew in any detail what they were) is totally immaterial to that judgment.  In my opinion, not even Goldreichâs belief in the impossibility of quantum computers should affect his eligibility for the prize. <img alt="&#x1F642;" class="wp-smiley" src="https://s.w.org/images/core/emoji/13.0.1/72x72/1f642.png" style="height: 1em;"/></p>



<p>Maybe it would be better to say that, as far as his academic colleagues in Israel and beyond are concerned, Goldreich <em>has</em> won the Israel Prize; itâs only some irrelevant external agent whoâs blocking his receipt of it.  Ironically, though, among Goldreichâs many heterodox beliefs is a <a href="http://www.wisdom.weizmann.ac.il/~oded/on-awards1.html">total rejection of the value of scientific prizes</a> (although Goldreich has also said he wouldnât refuse the Israel Prize if offered it!).</p>



<p/><hr/><p/>



<p>In unrelated news, the 2020 Turing Award has been given to <a href="https://en.wikipedia.org/wiki/Alfred_Aho">Al Aho</a> and <a href="https://en.wikipedia.org/wiki/Jeffrey_Ullman">Jeff Ullman</a>.  Aho and Ullman have both been celebrated leaders in CS for half a century, having laid many of the foundations of formal languages and compilers, and having coauthored one of CSâs <a href="https://www.amazon.com/Design-Analysis-Computer-Algorithms/dp/0201000296/ref=pd_lpo_14_t_1/140-9181226-0879049?_encoding=UTF8&amp;pd_rd_i=0201000296&amp;pd_rd_r=4c6cd308-4669-45ee-ad4d-292ee24e043f&amp;pd_rd_w=WxNWv&amp;pd_rd_wg=AAt3G&amp;pf_rd_p=337be819-13af-4fb9-8b3e-a5291c097ebb&amp;pf_rd_r=T95EJ78DHVE1V1G604D1&amp;psc=1&amp;refRID=T95EJ78DHVE1V1G604D1">defining textbooks</a> with <a href="https://en.wikipedia.org/wiki/John_Hopcroft">John Hopcroft</a> (who already received a different Turing Award).</p>



<p>But again thereâs a controversy.  <a href="https://lobelog.com/niac-calls-out-anti-iranian-stanford-professor/">Apparently</a>, in 2011, Ullman wrote to an Iranian student who wanted to work with him, saying that as âa matter of principle,â he would not accept Iranian students until the Iranian government recognized Israel.  Maybe I should say that I, like Ullman, am both a Jew and a Zionist, but I find it hard to imagine the state of mind that would cause me to hold some hapless student responsible for the misdeeds of their birth-countryâs government.  Ironically, this is a mirror-image of the <a href="https://en.wikipedia.org/wiki/Academic_boycott_of_Israel#Mona_Baker,_Miriam_Shlesinger_and_Gideon_Toury">tactics</a> that the BDS movement has wielded against Israeli academics.  Unlike Goldreich, though, Ullman seems to have gone beyond merely expressing his beliefs, actually turning them into a one-man foreign policy.</p>



<p>Iâm <a href="https://www.scottaaronson.com/blog/?p=3167">proud</a> of the Iranian students Iâve mentored and hope to mentor more.  While I donât think this issue should affect Ullmanâs Turing Award (and I havenât seen anyone claim that it should), I do think itâs appropriate to use the occasion to express our opposition to all forms of discrimination.  I fully endorse Shafi Goldwasserâs <a href="https://www.facebook.com/SimonsInstitute/">response</a> in her capacity as Director of the Simons Institute for Theory of Computing in Berkeley:</p>



<blockquote class="wp-block-quote"><p>As a senior member of the computer science community and an American-Israeli, I stand with our Iranian students and scholars and outright reject any notion by which admission, support, or promotion of individuals in academic settings should be impeded by national origin or politics. Individuals should not be conflated with the countries or institutions they come from. Statements and actions to the contrary have no place in our computer science community. Anyone experiencing such behavior will find a committed ally in me.</p></blockquote>



<p>As for Al Aho?  I knew him fifteen years ago, when he became interested in quantum computing, in part due to his then-student <a href="https://www.microsoft.com/en-us/research/people/ksvore/">Krysta Svore</a> (whoâs now the head of Microsoftâs quantum computing efforts).  Al struck me as not only a famous scientist but a gentleman who radiated kindness everywhere.  Iâm not aware of any controversies heâs been involved in and never heard anyone say a bad word about him.</p>



<p>Anyway, this seems like a good occasion to recognize some foundational achievements in computer science, as well as the complex human beings who produce them!</p></div>
    </content>
    <updated>2021-04-09T18:15:33Z</updated>
    <published>2021-04-09T18:15:33Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-04-09T18:27:57Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/051</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/051" rel="alternate" type="text/html"/>
    <title>TR21-051 |  Binary Interactive Error Resilience Beyond $1/8$ (or why $(1/2)^3 &amp;gt; 1/8$) | 

	Raghuvansh Saxena, 

	Klim Efremenko, 

	Gillat Kol</title>
    <summary>Interactive error correcting codes are codes that encode a two party communication protocol to an error-resilient protocol that succeeds even if a constant fraction of the communicated symbols are adversarially corrupted, at the cost of increasing the communication by a constant factor. What is the largest fraction of corruptions that such codes can protect against? 

If the error-resilient protocol is allowed to communicate large (constant sized) symbols, Braverman and Rao  (STOC, 2011) show that the maximum rate of corruptions that can be tolerated is $1/4$. They also give a binary interactive error correcting protocol that only communicates bits and is resilient to $1/8$ fraction of errors, but leave the optimality of this scheme as an open problem.

We answer this question in the negative, breaking the $1/8$ barrier. Specifically, we give a binary interactive error correcting scheme that is resilient to $5/39 &gt; 1/8$ fraction of adversarial errors. Our scheme builds upon a novel construction of binary list-decodable interactive codes with small list size.</summary>
    <updated>2021-04-09T04:15:46Z</updated>
    <published>2021-04-09T04:15:46Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-04-14T02:20:28Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-709704913623244646</id>
    <link href="https://blog.computationalcomplexity.org/feeds/709704913623244646/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/04/quantum-stories.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/709704913623244646" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/709704913623244646" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/04/quantum-stories.html" rel="alternate" type="text/html"/>
    <title>Quantum Stories</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Scott AaronsonÂ <a href="https://www.scottaaronson.com/blog/?p=5387">wrote last month</a> about the hype over quantum computing. I'd thought I'd drop a few stories.</p><p>I was once asked to review a grant proposal (outside the US) that claimed it would find a quantum algorithm for NP-hard problems. I wrote a scathing review but the grant was funded because I failed to prove that it was impossible. I replied that they should fund my research to teleport people from Chicago to Paris because they couldn't prove I couldn't do it. I never got a response.</p><div>I was at an NSF sponsored meeting on quantum computing. I suggested, as a complexity theorist, that we need to explore the limits of quantum computing. A senior researcher said we shouldn't mention that in the report or it might hurt our chances of funding the field if they think quantum computing might not be a complete success.</div><p>I went to a Microsoft Faculty Research Summit which had a big focus on quantum computing. I complained of the quantum computing hype. My friends in the field denied the hype. Later at the summit a research head said that Microsoft will solve world hunger with quantum computing.</p><p>I was meeting with a congressional staffer who had worked on the National Quantum Initiative which coincidentally was being announced that day. I said something about high risk, high reward. He looked shocked--nobody had told him before that quantum computing is a speculative technology.</p><p>Quantum computing has generated a large number of beautiful and challenging scientific questions. Thinking about quantum has helped generate classical complexity and algorithmic results. But quantum computing having a real-world impact in the near or mid-term is unlikely. Most scientists I know working directly in quantum research are honest about the limitations and challenges in quantum computing. But somehow that message is not often getting to the next layers up, the policy makers, the research managers, the university administrators, the media and the venture capitalists.Â </p><p>But who knows, maybe some quantum heuristic that doesn't need much entanglement will change the world tomorrow. I can't prove it's impossible.</p></div>
    </content>
    <updated>2021-04-08T12:57:00Z</updated>
    <published>2021-04-08T12:57:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-04-14T02:01:29Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-21129445.post-1274107059735040105</id>
    <link href="http://mysliceofpizza.blogspot.com/feeds/1274107059735040105/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://www.blogger.com/comment.g?blogID=21129445&amp;postID=1274107059735040105" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/21129445/posts/default/1274107059735040105" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/21129445/posts/default/1274107059735040105" rel="self" type="application/atom+xml"/>
    <link href="http://mysliceofpizza.blogspot.com/2021/04/postdoctoral-openings-at-amazon.html" rel="alternate" type="text/html"/>
    <title>Postdoctoral openings at Amazon</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><a href="https://www.amazon.science/amazon-advertising-opens-applications-for-early-career-scientists" target="_blank">Amazon Advertising opens applications for early career scientists.</a> The new program, which offers full-time two-year positions, is aimed at recent PhD graduates who want to innovate, publish, and have their work impact millions of customers. The application deadline is May 14.</p></div>
    </content>
    <updated>2021-04-08T04:56:00Z</updated>
    <published>2021-04-08T04:56:00Z</published>
    <category scheme="http://www.blogger.com/atom/ns#" term="aggregator"/>
    <author>
      <name>metoo</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/07192519900962182610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-21129445</id>
      <category term="aggregator"/>
      <category term="Non-CS"/>
      <author>
        <name>metoo</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/07192519900962182610</uri>
      </author>
      <link href="http://mysliceofpizza.blogspot.com/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="http://www.blogger.com/feeds/21129445/posts/default/-/aggregator" rel="self" type="application/atom+xml"/>
      <link href="http://mysliceofpizza.blogspot.com/search/label/aggregator" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="http://www.blogger.com/feeds/21129445/posts/default/-/aggregator/-/aggregator?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>books, stories, poems, algorithms, math and computer science. 

some art and anecdotes too.</subtitle>
      <title>my slice of pizza</title>
      <updated>2021-04-08T21:32:16Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://offconvex.github.io/2021/04/07/ripvanwinkle/</id>
    <link href="http://offconvex.github.io/2021/04/07/ripvanwinkle/" rel="alternate" type="text/html"/>
    <title>Rip van Winkle's Razor, a Simple New Estimate for Adaptive Data Analysis</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>Can you trust a model whose designer had access to the test/holdout set?</em> This implicit question 
in <a href="https://science.sciencemag.org/content/349/6248/636.full">Dwork et al 2015</a> launched a new field, <em>adaptive data analysis</em>.
The question referred to the fact that in many scientific settings as well as modern machine learning (with its standardized datasets like CIFAR, 
ImageNet etc.) the model designer has full access to the holdout set and is free to ignore the</p>

<blockquote>
  <p>(Basic Dictum of Data Science) âThou shalt not train on the test/holdout set.â</p>
</blockquote>

<p>Furthermore, even researchers who scrupulously follow the Basic Dictum may be unknowingly violating it when they take inspiration (and design choices) 
from published works by others who presumably published <em>only the best of the many models they evaluated on the test set.</em></p>

<p>Dwork et al. showed that if the test set has size $N$, and the designer is allowed to see the error of the first $i-1$ models on the test set before designing the $i$âth model, then a clever designer can  use so-called <a href="https://arxiv.org/pdf/1502.04585.pdf"><em>wacky boosting</em></a> (see this <a href="http://blog.mrtz.org/2015/03/09/competition.html">blog post</a>) to ensure the accuracy of the $t$âth model on the test set as high as $\Omega(\sqrt{t/N})$. In other words, the test set could become essentially useless once $t \gg N$, a 
condition that holds in ML, whereby in popular datasets (CIFAR10, CIFAR100, ImageNet etc.)  $N$ is no more than $100,000$ and the total number of models being trained 
world-wide is well in the millions if not higher (once you include hyperparameter searches).</p>

<blockquote>
  <p><strong>Meta-overfitting Error (MOE)</strong> of a model is the difference between its average error on the test data and its expected error on the full distribution.
(It is closely related to <a href="https://en.wikipedia.org/wiki/False_discovery_rate"><em>false discovery rate</em></a> in statistics.)</p>
</blockquote>

<p>This blog post concerns <a href="https://arxiv.org/pdf/2102.13189.pdf">our new paper</a>, which gives meaningful upper bounds on this sort of trouble for popular 
deep net architectures, whereas prior ideas from adaptive data analysis gave no nontrivial estimates. We call our estimate  <em>Rip van Winkleâs Razor</em> 
which combines references to <a href="https://en.wikipedia.org/wiki/Occam%27s_razor">Occamâs Razor</a> and the 
<a href="https://en.wikipedia.org/wiki/Rip_Van_Winkle">mythical person who fell asleep for  20 years</a>.</p>

<figure align="center">
<img alt="drawing" src="http://www.offconvex.org/assets/ripvanwinkle.jpg" width="50%"/>
   Rip Van Winkle wakes up from 20 years of sleep, clearly needing a Razor 
</figure>

<h2 id="adaptive-data-analysis-brief-tour">Adaptive Data Analysis: Brief tour</h2>

<p>It is well-known that for a model trained <strong>without</strong> ever querying the test set, MOE scales (with high probability over choice of the test set) as $1/\sqrt{N}$ where $N$ 
is the size of the test set.  Furthermore standard concentration bounds imply that even if we train $t$ models without ever referring to the test set (in other words, 
using proper data hygiene) then the maximum meta-overfitting error among the $t$ models scales whp as $O(\sqrt{\log(t)/ N})$. The trouble pinpointed by Dwork et al. 
can happen only if models are designed adaptively, with test error of the previous models shaping the design of the next model.</p>

<p>Adaptive Data Analysis has come up with many good practices for honest researchers to mitigate such issues. For instance, Dwork et al. showed that using 
Differential Privacy on labels while evaluating models can lower MOE. Or the <a href="https://arxiv.org/pdf/1502.04585.pdf">Ladder mechanism</a> helps in Kaggle-like 
settings where the test dataset resides on a server that can choose to answers only a  selected subset of queries, which essentially takes away the MOE issue.</p>

<p>For several good practices  matching lower bounds exist showing a way to construct cheating models with MOE matching the upper bound.</p>

<p>However such recommended best practices do not help with understanding the MOE in the performance numbers of a new model  since there is no guarantee that the 
inventors never tuned models using the test set, or didnât get inspiration from existing models that may have been designed that way.  Thus statistically 
speaking the above results still give no reason to believe that a modern deep net such as ResNet152 has low MOE.</p>

<p><a href="http://proceedings.mlr.press/v97/recht19a/recht19a.pdf">Recht et al. 2019</a> summed up the MOE issue in a catchy title: <em>Do ImageNet Classifiers Generalize to ImageNet?</em>  They tried to answer their question experimentally by creating new test sets from scratch âwe discuss their results later.</p>

<h2 id="moe-bounds-and-description-length">MOE bounds and description length</h2>

<p>The starting point of our work is the following classical concentration bounds:</p>

<blockquote>
  <p><strong>Folklore Theorem</strong> With high probability over the choice of a test set of size $N$, the MOE of <em>all</em> models with description length at most $k$ bits is  $O(\sqrt{k/N})$.</p>
</blockquote>

<p>At first sight this doesnât seem to help us because one cannot imagine modern deep nets having a short description. The most obvious description involves reporting 
values of the net parameters, which requires millions or even hundreds of millions of bits, resulting in a vacuous upper bound  on MOE.</p>

<p>Another obvious description would be the computer program used to produce the model using the (publicly available) training and validation sets. However, these 
programs usually rely on imported libraries through layers of encapsulation and so the effective program size is pretty large as well.</p>

<h2 id="rip-van-winkles-razor">Rip van Winkleâs Razor</h2>
<p>Our new upper bound involves a more careful definition of <em>Description Length</em>: it is the smallest description that allows a referee  to reproduce a model of 
similar performance using the (universally available) training and validation datasets.</p>

<p>While this phrasing may appear reminiscent of the review process for conferences and journals, there is a subtle difference  with respect to what the referee 
can or cannot be assumed to know. (Clearly, assumptions about the referee can greatly affect description length âe.g,  a referee ignorant of even basic 
calculus might need a very long explanation!)</p>

<blockquote>
  <p><strong>Informed Referee:</strong> âKnows everything that was known to humanity (e.g., about deep learning, mathematics,optimization, statistics etc.) right up to the 
moment of creation of the Test set.â</p>
</blockquote>

<blockquote>
  <p><strong>Unbiased Referee:</strong> Knows nothing discovered since the Test set was created.</p>
</blockquote>

<p>Thus <em>Description Length</em> of a model is the number of bits in the shortest description that allows an informed but unbiased referee to reproduce the claimed result.</p>

<p>Note that informed referees let descriptions get shorter. Unbiased require longer descriptions that rule out any statistical âcontaminationâ due to any interaction whatsoever with the test set. For example, momentum techniques in optimization were 
well-studied before the creation of ImageNet test set, so informed referees can be expected to understand a line like âSGD with momentum 0.9.â But a 
line like âUse Batch Normalizationâ cannot be understood by unbiased referees since conceivably this technique (invented after 2012) might have
 become popular precisely because it leads to better performance on the test set of ImageNet.</p>

<p>By now it should be clear why the estimate is named after  <a href="https://en.wikipedia.org/wiki/Rip_Van_Winkle">âRip van Winkleâ</a>: the referee can be thought 
of as an infinitely well-informed researcher who went into deep sleep at the moment of creation of the test set, and has just been woken up years later 
to start refereeing the  latest papers.  Real-life journal referees who luckily did not suffer this way should try to simulate the idealized Rip van Winkle 
in their heads while perusing the description submitted by the researcher.</p>

<p>To allow as short a  description as possible the researcher is allowed to compress the description of their new deep net non-destructively using any compression  that would make sense to Rip van Winkle (e.g., <a href="https://en.wikipedia.org/wiki/Huffman_coding">Huffman Coding</a>). The description of the compression method itself 
is not counted towards the description length â provided the same method is used for all papers submitted to Rip van Winkle. To give an example, a 
technique appearing in a text known to Rip van Winkle could be succinctly referred to using the bookâs ISBN number and page number.</p>

<h2 id="estimating-moe-of-resnet-152">Estimating MOE of ResNet-152</h2>
<p>As an illustration, here we provide a suitable description allowing  Rip van Winkle to reproduce a mainstream ImageNet model, ResNet-152, which achieves $4.49\%$ top-5 
test error.</p>

<p>The description consists of three types of expressions: English phrases, Math equations, and directed graphs. In the paper, we describe in detail how to encode 
each of them into binary strings and count their lengths.  The allowed vocabulary includes primitive concepts that were known before 2012, such 
as <em>CONV, MaxPool, ReLU, SGD</em> etc., as well as a graph-theoretic notation/shorthand  for describing net architecture. The newly introduced concepts 
including <em>Batch-Norm</em>, <em>Layer, Block</em> are defined precisely using Math, English, and other primitive concepts.</p>

<figure align="center">
<img alt="drawing" src="http://www.offconvex.org/assets/resnet_description.png" width="80%"/>
  <b>Description for reproducing ResNet-152</b>
</figure>

<p>According to our estimate, the length of the above description is $1032$ bits, which translates into a upper bound on meta-overfitting error of merely $5\%$! 
This suggests the real top-5 error of the model on full distribution is at most $9.49\%$. In the paper we also provide a $980$-bit long description for 
reproducing DenseNet-264, which leads to $5.06\%$ upper bound on its meta-overfitting error.</p>

<p>Note that the number $5.06$ suggests higher precision than actually given by the method, since it is possible to quibble about the coding assumptions 
that led to it.  Perhaps others might use a more classical coding mechanism and obtain an estimate of $6\%$ or $7\%$.</p>

<p>But the important point is that unlike  existing bounds in Adaptive Data Analysis, there is <strong>no</strong> dependence on $t$, the number of models that have been tested before, and the bound is non-vacuous.</p>

<h2 id="empirical-evidence-about-lack-of-meta-overfitting">Empirical evidence about lack of meta-overfitting</h2>

<p>Our estimates indicate that the issue of meta-overfitting on ImageNet for these mainstream models is mild. The reason is that despite the vast number
 of parameters and hyper-parameters in todayâs deep nets, the <em>information content</em> of these models is not high given  knowledge circa 2012.</p>

<p>Recently Recht et al. <a href="https://arxiv.org/abs/1902.10811">tried to reach an empirical upper bound on MOE</a> for
ImageNet and <a href="https://arxiv.org/abs/1806.00451">CIFAR-10</a>. They created new tests sets by carefully replicating the methodology used for constructing the original ones. They found that error of famous published models of the past seven years is as much as 10-15% higher on the new test set as compared to the original.  On the face of it, this seemed to confirm a case of bad meta-overfitting. But they  also presented evidence  that the swing in test error was due to systemic effects during test set creation. For instance, a comparable swing happens also for models that predated the creation of ImageNet (and thus were not overfitted to the ImageNet test set). 
<a href="https://proceedings.neurips.cc/paper/2019/hash/ee39e503b6bedf0c98c388b7e8589aca-Abstract.html">A followup study</a> of a hundred Kaggle competitions used fresh, 
identically distributed test sets that were available from the official competition organizers. The authors concluded that MOE does not appear to be significant in modern ML.</p>

<h2 id="conclusions">Conclusions</h2>
<p>To us the  disquieting takeaway from Recht et al.âs results was that  estimating MOE by creating a new test set is rife with systematic bias at best, and perhaps impossible, especially in datasets concerning rare or one-time phenomena (e.g., stock prices).  Thus their work still left a pressing need for effective upper bounds on  meta-overfitting error. Our Rip van Winkleâs Razor is elementary, and easily deployable by the average researcher. We hope it becomes part of the standard toolbox in Adaptive Data Analysis.</p></div>
    </summary>
    <updated>2021-04-07T21:00:00Z</updated>
    <published>2021-04-07T21:00:00Z</published>
    <source>
      <id>http://offconvex.github.io/</id>
      <author>
        <name>Off the Convex Path</name>
      </author>
      <link href="http://offconvex.github.io/" rel="alternate" type="text/html"/>
      <link href="http://offconvex.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Algorithms off the convex path.</subtitle>
      <title>Off the convex path</title>
      <updated>2021-04-13T22:52:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://toc4fairness.org/?p=1613</id>
    <link href="https://toc4fairness.org/ensuring-equity-in-high-stakes-online-advertising/" rel="alternate" type="text/html"/>
    <title>Ensuring equity in high-stakes online advertising</title>
    <summary>In this blog post, I outline how existing advertising platforms do not prevent high-stakes ads from reaching different demographics at different rates. The post then describes how pushing this responsibility ...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In this blog post, I outline how existing advertising platforms do not prevent high-stakes ads from reaching different demographics at different rates. The post then describes how pushing this responsibility down to advertisers rather than addressing it at the platform leaves manipulating a complex system to those least aware of the systemâs inner workings. She then proposes a simpler, more unified solution to this problem: advertising slots should be either targetable or untargetable, and high-stakes ads should be in the untargeted segment. Finally, the post concludes with a discussion of how this segmentation need not cost these systems substantial revenue if reserve prices are used appropriately.</p>



<p/>



<p><a href="https://jamiemmt-cs.medium.com/ensuring-equity-in-online-advertising-for-employment-housing-and-credit-82931668c420">https://jamiemmt-cs.medium.com/ensuring-equity-in-online-advertising-for-employment-housing-and-credit-82931668c420</a></p></div>
    </content>
    <updated>2021-04-07T20:47:28Z</updated>
    <published>2021-04-07T20:47:28Z</published>
    <category term="Blog"/>
    <author>
      <name>jamiemorgenstern</name>
    </author>
    <source>
      <id>https://toc4fairness.org</id>
      <logo>https://i1.wp.com/toc4fairness.org/wp-content/uploads/2020/10/cropped-favicon.png?fit=32%2C32&amp;ssl=1</logo>
      <link href="https://toc4fairness.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://toc4fairness.org" rel="alternate" type="text/html"/>
      <subtitle>a simons collaboration project</subtitle>
      <title>TOC for Fairness</title>
      <updated>2021-04-14T02:22:05Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=547</id>
    <link href="https://tcsplus.wordpress.com/2021/04/06/tcs-talk-wednesday-april-14-andrea-lincoln-uc-berkeley/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, April 14 â Andrea Lincoln, UC Berkeley</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, April 14th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Andrea Lincoln from UC Berkeley will speak about âNewÂ Techniques for Proving Fine-Grained Average-Case Hardnessâ (abstract below). You can reserve a spot as an individual or a group to [â¦]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p style="text-align: left;">The next TCS+ talk will take place this coming Wednesday, April 14th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="https://sites.google.com/site/andrealiresume/home"><strong>Andrea Lincoln</strong></a> from UC Berkeley will speak about â<em>NewÂ Techniques for Proving Fine-Grained Average-Case Hardness</em>â (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards, so people who did not sign up will still be able to watch the talk)</p>
<p>As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: In this talk I will cover a new technique for worst-case to average-case reductions. There are two primary concepts introduced in this talk: âfactoredâ problems and a framework for worst-case to average-case fine-grained (WCtoACFG) self reductions.</p>
<p>We will define new versions of OV, kSUM and zero-k-clique that are both worst-case and average-case fine-grained hard assuming the core hypotheses of fine-grained complexity. We then use these as a basis for fine-grained hardness and average-case hardness of other problems. Our hard factored problems are also simple enough that we can reduce them to many other problems, e.g. to edit distance, k-LCS and versions of Max-Flow. We further consider counting variants of the factored problems and give WCtoACFG reductions for them for a natural distribution.</p>
<p>To show hardness for these factored problems we formalize the framework of [Boix-Adsera et al. 2019] that was used to give a WCtoACFG reduction for counting k-cliques. We define an explicit property of problems such that if a problem has that property one can use the framework on the problem to get a WCtoACFG self reduction. In total these factored problems and the framework together give tight fine-grained average-case hardness for various problems including the counting variant of regular expression matching.</p>
<p>Based on joint work with Mina Dalirrooyfard and Virginia Vassilevska Williams.</p></blockquote></div>
    </content>
    <updated>2021-04-06T20:51:50Z</updated>
    <published>2021-04-06T20:51:50Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2021-04-14T02:21:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8076</id>
    <link href="https://windowsontheory.org/2021/04/06/tcs-summer-school-call-for-tas/" rel="alternate" type="text/html"/>
    <title>TCS summer school â call for TAs</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">TL;DR: The summer school we are organizing is looking for TAs. Please forward this to your students as well as any departmental mailing lists. Are you passionate about teaching? Or about increasing diversity within TCS? If so, we need your help! The committee for advancement of theoretical computer science (CATCS) is organizing an online summer â¦ <a class="more-link" href="https://windowsontheory.org/2021/04/06/tcs-summer-school-call-for-tas/">Continue reading <span class="screen-reader-text">TCS summer school â call forÂ TAs</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>TL;DR: The <a href="https://boazbk.github.io/tcs-summerschool/">summer school</a> we are organizing is looking for TAs.  Please forward this to your students as well as any departmental mailing lists.<br/></p>



<p>Are you passionate about teaching? Or about increasing diversity within TCS? If so, we need your help!</p>



<p>The committee for advancement of theoretical computer science (CATCS) is organizing an online summer course that will take place on May 31 till June 4, 2021. New horizons in theoretical computer science is a week-long online summer school which will expose undergraduates to exciting research areas in the area of theoretical computer science and its applications. The school will contain several mini-courses from top researchers in the field. We particularly encourage participants from groups that are currently under-represented in TCS. SeeÂ <a href="https://boazbk.github.io/tcs-summerschool/" rel="noreferrer noopener" target="_blank">https://boazbk.github.io/tcs-summerschool/</a>Â for more details.</p>



<p>We are looking for TAs to help run the school.</p>



<p>TAs will have the following responsibilities:<br/>Â  Â  Â  Â  â¢ Plan team building and ice breaking activities and social events for the summer school<br/>Â  Â  Â  Â  â¢ Lead small groups during the week<br/>Â  Â  Â  Â  â¢ Monitor questions in chat during lectures<br/>Â  Â  Â  Â  â¢ Work with one of the instructors to prepare one homework<br/>Â  Â  Â  Â  â¢ Grade homework<br/>Â  Â  Â  Â  â¢ Provide mentorship to students<br/>Â  Â  Â  Â  â¢ Possibly assist with reviewing applications and other technical/admin aspects of running the school</p>



<p>The time commitment will be ~20 hours during the week of May 31-June 4; ~5-10 hours prior to that week; and ~2-3 hours following that week. We are hoping to pay an amount of $500 to each TA (please note that international students will need a CPT for this).</p>



<p>To apply for a TA position, please fill in the application form atÂ <a href="https://forms.gle/QCxLn8R81Ga4JQLH8" rel="noreferrer noopener" target="_blank">https://forms.gle/QCxLn8R81Ga4JQLH8</a> Â by April 15, 2021. Please also have a faculty advisor send a short recommendation toÂ <a href="mailto:summer-school-admin@boazbarak.org" rel="noreferrer noopener" target="_blank">summer-school-admin@boazbarak.org</a>. Please ask them to use the subject âTA recommendation for &lt;&lt;Your Name&gt;&gt;â.</p>



<p>Course organizers: Boaz Barak (Harvard), Shuchi Chawla (UT Austin), Madhur Tulsiani (TTI-Chicago)</p>



<p>Current list of confirmed instructors: Antonio Blanca (Penn State University), Ashia Wilson (MIT), Jelani Nelson (UC Berkeley), Nicole Immorlica (Microsoft Research), Yael Kalai (Microsoft research).</p>



<p>Please emailÂ Â <a href="mailto:summer-school-admin@boazbarak.org" rel="noreferrer noopener" target="_blank">summer-school-admin@boazbarak.org</a>Â with any questions.</p></div>
    </content>
    <updated>2021-04-06T17:32:35Z</updated>
    <published>2021-04-06T17:32:35Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-04-14T02:21:01Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/04/06/ci-fellows-at-any-apply-by-may-1-2021/</id>
    <link href="https://cstheory-jobs.org/2021/04/06/ci-fellows-at-any-apply-by-may-1-2021/" rel="alternate" type="text/html"/>
    <title>CI Fellows at Any (apply by May 1, 2021)</title>
    <summary>CIFellows is a general CRA/CCC program for matching postdocs with advisors, and supports 2-year postdocs. Applications will open mid-April, and are due early May. It is competitive across all areas of CS. Website: https://cifellows2021.org/ Email: shachar.lovett@gmail.com</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>CIFellows is a general CRA/CCC program for matching postdocs with advisors, and supports 2-year postdocs. Applications will open mid-April, and are due early May. It is competitive across all areas of CS.</p>
<p>Website: <a href="https://cifellows2021.org/">https://cifellows2021.org/</a><br/>
Email: shachar.lovett@gmail.com</p></div>
    </content>
    <updated>2021-04-06T13:33:14Z</updated>
    <published>2021-04-06T13:33:14Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-04-14T02:20:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18492</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/04/06/relaxing-the-primes/" rel="alternate" type="text/html"/>
    <title>Relaxing the Primes</title>
    <summary>Although the prime numbers are rigidly determined, they somehow feel like experimental dataâTim Gowers Finnish Scientific Courage Award source Kaisa MatomÃ¤ki is a Finnish mathematician working in number theory. She has some terrific results on prime numbersâresults that have won several important prizes including the 2021 Ruth Lyttle Satter Prize: It is presented to a [â¦]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>Although the prime numbers are rigidly determined, they somehow feel like experimental dataâTim Gowers</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/06/relaxing-the-primes/sm/" rel="attachment wp-att-18494"><img alt="" class="alignright wp-image-18494" height="102" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/sm.jpg?resize=181%2C102&amp;ssl=1" width="181"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Finnish Scientific Courage Award <a href="https://www.utu.fi/en/news/news/2016-academy-of-finland-award-granted-to-kaisa-matomaki">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Kaisa MatomÃ¤ki is a Finnish mathematician working in <a href="https://en.wikipedia.org/wiki/Kaisa_Matomaki">number theory</a>. She has some terrific results on prime numbersâresults that have won several important prizes including the 2021 Ruth Lyttle Satter <a href="https://en.wikipedia.org/wiki/Ruth_Lyttle_Satter_Prize_in_Mathematics">Prize</a>: It is presented to a woman who has made an outstanding contribution to mathematics research. The prize <a href="https://www.ams.org/news?news_id=6455">citation</a> specifically mentions a 2015 <a href="https://arxiv.org/abs/1501.04585">paper</a> with Maksym Radziwill that contributed to Terence Taoâs resolution of the ErdÅs discrepancy problemâand indeed we highlighted Taoâs followup work with her and Radziwill in our 2015 <a href="https://rjlipton.wpcomstaging.com/2015/09/24/frogs-and-lily-pads-and-discrepancy/">post</a> on this.</p>
<p>
Today, Ken and I thought we would combine one of her new deep theorems with some shallow observations.<br/>
<span id="more-18492"/></p>
<p>
The set of primes is denoted by <img alt="{\mathsf{PRIMES}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BPRIMES%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, as usual. As complexity theorists we have known that for decades that the Boolean circuit complexity of <img alt="{\mathsf{PRIMES}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BPRIMES%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is polynomial. This follows from the existence of a polynomial random algorithm for <img alt="{\mathsf{PRIMES}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BPRIMES%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> due to Robert Solovay and Volker Strassen, and then applying Len Adlemanâs connection between such algorithms and Boolean circuit complexity.</p>
<p>
MatomÃ¤ki views <img alt="{\mathsf{PRIMES}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BPRIMES%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in a different way. As an analytic number theorist she is concerned with the density and structure of primes, not so much the complexity of recognizing or generating a prime.</p>
<p>
</p><h2> Relaxations </h2><p/>
<p>
Exact results on the set of primes are hard to come by. </p>
<p>
<b>Twin Primes</b>: We know that there are infinitely many <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> such that <img alt="{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{p+2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bp%2B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are prime. But not proved.</p>
<p>
<b>Goldbach</b>: We know that every even number from <img alt="{4}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B4%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> onward is the sum of two primes. But not proved.</p>
<p>
<b>Density of Primes</b>: We known that the gap between consecutive primes is at most <img alt="{\dots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cdots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> But not proved.</p>
<p>
The Riemann Hypothesis attempts to say more about density of the primes than the Prime Number Theorem does. It has been proved equivalent to some statements about approximate growth rates, yet even these forms have not been touched.</p>
<p>
Instead of trying for better approximate results about primes, can we learn more by relaxing the notion of âprimeâ itself? For each <img alt="{k \geq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk+%5Cgeq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, define <img alt="{P_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to be the set of numbers that are products of at most <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> primes. Then <img alt="{P_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is the set of primes (not the prime powers) and <img alt="{P_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is the set of products of two primesâwhich include the <a href="https://en.wikipedia.org/wiki/Blum_integer">Blum integers</a> of special interest to cryptography. Collectively the <img alt="{P_k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP_k%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> sets represent different levels of being â<a href="https://en.wikipedia.org/wiki/Almost_prime">almost prime</a>.â The motivating question is:</p>
<blockquote><p><b> </b> <em> How well and in what ways does the structure of the sets of almost-primes model the structure of the primes? </em>
</p></blockquote>
<p>
In 2010, John Friedlander and Henryk Iwaniec wrote a <a href="https://www.ams.org/books/coll/057/coll057-endmatter.pdf">monograph</a> titled <em>Opera de Cribro</em>, which is Latin for âWorks of the Sieve.â They proved certain results about <img alt="{P_{13}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP_%7B13%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and speculated whether their methods improve them to work at least for <img alt="{P_3}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP_3%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Then they say, âIt would be interesting to get integers with at most two prime divisors.â This is where MatomÃ¤ki comes inâwith a second kind of relaxing, one applying to âalmost allâ members of <img alt="{P_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>
</p><h2> New Result </h2><p/>
<p>
MatomÃ¤ki has a brand new <a href="https://arxiv.org/abs/2012.11565">paper</a> (revised two weeks ago from a December original) titled, âAlmost primes in almost all very short intervals.â It follows on from a <a href="https://arxiv.org/abs/1510.06005">paper</a> by one of her students, Joni TerÃ¤vÃ¤inen, titled âAlmost all primes in almost all short intervals.â </p>
<p>
Her main result is the following theorem. </p>
<blockquote><p><b>Theorem 1</b> <em> Suppose <img alt="{h_X \rightarrow \infty}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bh_X+%5Crightarrow+%5Cinfty%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> as <img alt="{X \rightarrow \infty}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX+%5Crightarrow+%5Cinfty%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> and put <img alt="{\Delta_X = h\log X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CDelta_X+%3D+h%5Clog+X%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Then for almost all <img alt="{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{x \in (\frac{X}{2},X]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cin+%28%5Cfrac%7BX%7D%7B2%7D%2CX%5D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/>, the interval <img alt="{(x-\Delta_X,x]}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28x-%5CDelta_X%2Cx%5D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> contains a <img alt="{P_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP_2%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/>. </em>
</p></blockquote>
<p>
Roger Heath-Brown had established the corresponding result for primes (that is, for <img alt="{P_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>) but only assuming simultaneously the Riemann hypothesis and the pair correlation conjecture for the zeros of the Riemann zeta function. With no hypotheses, the presence of primes in almost all intervals is known only when the intervals have length <img alt="{X^{\Omega(1)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%5E%7B%5COmega%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>âconcretely, <img alt="{X^{1/20}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BX%5E%7B1%2F20%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is known. Thus, MatomÃ¤ki has traded off the use of conjectures for the relaxed notion of prime.</p>
<p>
The âalmost callâ is in the sense of average density of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> whose short interval is populated. This average instead of worst case mirrors our difficulties with circuit lower bounds. We can easily show that on average Boolean function have huge circuit lower bounds. But worst case bounds for specific functions is beyond reach. Hmmm.</p>
<p>
</p><h2> Lower Bounds </h2><p/>
<p>
We <a href="https://rjlipton.wordpress.com/2021/01/14/priming-random-restrictions/">previously</a> proved: </p>
<blockquote><p><b>Theorem 2 (Lower Bound)</b> <em> For any <img alt="{\epsilon&gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/>, the depth of a DeMorgan boolean circuit for <img alt="{\mathsf{PRIMES}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BPRIMES%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> is at least <img alt="{(2-\epsilon)\log_2 n + O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%282-%5Cepsilon%29%5Clog_2+n+%2B+O%281%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/>. </em>
</p></blockquote>
<p>
Recall a DeMorgan formula <img alt="{F}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BF%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> on <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> variables <img alt="{x_1,\dots,x_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_1%2C%5Cdots%2Cx_n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a binary tree whose leaves are labeled with variables or their negations, and whose internal nodes are labeled with either <img alt="{\vee}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvee%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for OR and <img alt="{\wedge}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cwedge%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for AND gates. </p>
<p>
Here is a high level view of this result. See <a href="https://rjlipton.wordpress.com/2021/01/14/priming-random-restrictions/">here</a> for details.</p>
<p>
Assume there is a DeMorgan Boolean circuit for <img alt="{\mathsf{PRIMES}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BPRIMES%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is at most <img alt="{(2-\epsilon)\log_2 n + O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%282-%5Cepsilon%29%5Clog_2+n+%2B+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> depth. We can use the random restriction method to set lots of the inputs to random values <img alt="{0/1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%2F1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. </p>
<p>
We claim that with high probability the input looks like 	</p>
<p align="center"><img alt="\displaystyle  x_1,\dots,x_m, x_{m+1},\dots,x_n. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_1%2C%5Cdots%2Cx_m%2C+x_%7Bm%2B1%7D%2C%5Cdots%2Cx_n.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>The right-most bits 	</p>
<p align="center"><img alt="\displaystyle  x_{m+1},\dots,x_{n} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_%7Bm%2B1%7D%2C%5Cdots%2Cx_%7Bn%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>are left unset, and the other bits to the left have some bits randomly set. Moreover, the number of bits in the above right is order <img alt="{n^{\epsilon}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B%5Cepsilon%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.</p>
<p>
Now set all the bits in the left group that are unset also to random values <img alt="{0/1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%2F1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Leave the right group unset.</p>
<p>
The point of this is that if we assume that the formula had size at most order <img alt="{n^{2-\epsilon}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7B2-%5Cepsilon%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, then we have that the formula is likely to be constant. But this contradicts the density of primes and composites. </p>
<p>
</p><h2> Open Problems </h2><p/>
<p>
Can MatomÃ¤kiâs theorem be used to get stronger lower bounds on the complexity of <img alt="{\mathsf{PRIMES}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathsf%7BPRIMES%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>? Can it go beyond the quadratic limit?</p>
<p/></font></font></div>
    </content>
    <updated>2021-04-06T04:43:53Z</updated>
    <published>2021-04-06T04:43:53Z</published>
    <category term="All Posts"/>
    <category term="News"/>
    <category term="Open Problems"/>
    <category term="primes"/>
    <category term="Results"/>
    <category term="almost primes"/>
    <category term="approximation"/>
    <category term="complexity"/>
    <category term="Kaisa Matomaki"/>
    <category term="primes density ideas"/>
    <category term="Prize"/>
    <category term="Satter Prize"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>GÃ¶del's Lost Letter and P=NP</title>
      <updated>2021-04-14T02:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://francisbach.com/?p=5900</id>
    <link href="https://francisbach.com/i-am-writing-a-book/" rel="alternate" type="text/html"/>
    <title>I am writing a book!</title>
    <summary>After several attempts, I finally found the energy to start writing a book. It grew out of lecture notes for a graduate class I taught last semester. I make the draft available so that I can get feedback before a (hopefully) final effort next semester. The goal of the book is to present old and...</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p class="justify-text">After several attempts, I finally found the energy to start writing a book. It grew out of lecture notes for a graduate class I taught last semester. I make the <a href="https://www.di.ens.fr/~fbach/ltfp_book.pdf">draft</a> available so that I can get feedback before a (hopefully) final effort next semester.</p>



<p class="justify-text">The goal of the book is to present old and recent results in learning theory, for the most widely-used learning architectures. This book is geared towards theory-oriented students as well as students who want to acquire a basic mathematical understanding of algorithms used throughout machine learning and associated fields that are large users of learning methods, such as computer vision or natural language processing.</p>



<p class="justify-text">A particular effort is made to prove <strong>many results from first principles</strong>, while keeping the exposition as simple as possible. This will naturally lead to a choice of key results that show-case in simple but relevant instances the important concepts in learning theory. Some general results will also be presented without proofs. Of course the concept of first principles is subjective, and a good knowledge of linear algebra, probability theory and differential calculus will be assumed.</p>



<p class="justify-text">Moreover, I will focus on the part of learning theory that does not exist outside of algorithms that can be run in practice, and thus all algorithmic frameworks described in this book are routinely used. For most learning methods, some simple <strong>illustrative experiments</strong> are presented, with the plan to have accompanying code (Matlab, Julia, and Python) so that students can see for themselves that the algorithms are simple and effective in synthetic experiments.</p>



<p class="justify-text">This is <em>not</em> an introductory textbook on machine learning. There are already several good ones in several languages [1, 2]. Many topics are not covered, and many more are not covered in much depth. There are many good textbooks on learning theory that go deeper [3, 4, 5].</p>



<p class="justify-text">The choice of topics is arbitrary (and thus personal). Many important algorithmic frameworks are forgotten (e.g.,Â  reinforcement learning, unsupervised learning, etc.). Suggestions of extra themes are welcome! A few additional chapters are currently being written such as: ensemble learning, bandit optimization, probabilistic methods, structured prediction.</p>



<h2>Help wanted!</h2>



<p class="justify-text">This is still work in progress. In particular, there are still a lot of typos, probably some mistakes, and almost surely places where more details are needed; readers are most welcome to report them to me (and then get credit for it).Â Also, the bibliography is currently quite short and would benefit from some expansion (all suggestions welcome, in particular for giving proper credit).</p>



<p class="justify-text">Moreover, I am convinced that simpler mathematical arguments are possible in many places in the book. If you are aware of elegant and simple ideas that I have overlooked, please let me know.</p>



<h2>References</h2>



<p class="justify-text">[1] ChloÃ©-Agathe Azencott. Introduction au Machine Learning. Dunod, 2019.<br/>[2] Ethem Alpaydin. Introduction to Machine Learning. MIT Press, 2020.<br/>[3] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning. MIT Press, 2018.<br/>[4] Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press, 2014.<br/>[5] Andreas Christmann and Ingo Steinwart. Support Vector Machines. Springer, 2008.</p></div>
    </content>
    <updated>2021-04-05T08:18:58Z</updated>
    <published>2021-04-05T08:18:58Z</published>
    <category term="Machine learning"/>
    <category term="Opinions"/>
    <author>
      <name>Francis Bach</name>
    </author>
    <source>
      <id>https://francisbach.com</id>
      <link href="https://francisbach.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://francisbach.com" rel="alternate" type="text/html"/>
      <subtitle>Francis Bach</subtitle>
      <title>Machine Learning Research Blog</title>
      <updated>2021-04-14T02:22:03Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://ptreview.sublinear.info/?p=1500</id>
    <link href="https://ptreview.sublinear.info/?p=1500" rel="alternate" type="text/html"/>
    <title>News for March 2021</title>
    <summary>A somewhat quieter month by recent standards. Three Two papers: graph property testing and quantum distribution testing. (Ed: The distribution testing paper was a revision of a paper we already covered in Sept 2020.) Robust Self-Ordering versus Local Self-Ordering by Oded Goldreich (ECCC). In Nov 2020, we covered a paper that uses a tool called [â¦]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A somewhat quieter month by recent standards. <s>Three</s> Two papers: graph property testing and quantum distribution testing. <em>(Ed: The distribution testing paper was a revision of a paper we already covered in Sept 2020.)</em></p>



<p><strong>Robust Self-Ordering versus Local Self-Ordering</strong> by Oded Goldreich (<a href="https://eccc.weizmann.ac.il/report/2021/034">ECCC</a>). In Nov 2020, we covered a <a href="https://eccc.weizmann.ac.il/report/2020/160/">paper</a> that uses a tool called <em>self-ordered graphs, </em>that transferred bit string function lower bounds to graph property testing. Consider a labeled graph. A graph is self-ordered if its automorphism group only contains the identity element (it has no non-trivial isomorphisms). A graph is robustly self-ordered, if every permutation of the vertices leads to a (labeled) graph that is sufficiently âfarâ according to edit distance. Given a self-ordered graph \(G\), a local self-ordering procedure is the following. Given access to a copy \(Gâ\) of \(G\) and a vertex \(v \in V(Gâ)\), this procedure determines the (unique) vertex in \(V(G)\) that corresponds to \(v\) with sublinear queries to \(G\). In other words, it can locally âlabelâ the graph. Intuitively, one would think that more robustly self-ordered graphs will be easier to locally label. This paper studies the relation between robust and local self-ordering. Curiously, this paper refutes the above intuition for bounded-degree graphs, and (weakly) confirms it for dense graphs. Roughly speaking, there are bounded degree graphs that are highly robustly self-ordered, for which any local self-ordering procedure requires \(\omega(\sqrt{n})\) queries. Moreover, there are bounded degree graphs with \(O(\log n)\)-query local self-ordering procedures, yet are not robustly self-ordered even for weak parameters. For dense graphs, the existence of fast non-adaptive local self-ordering procedures implies robust self-ordering.</p>



<p><strong>Testing identity of collections of quantum states: sample complexity analysis</strong> by Marco Fanizza,Â Raffaele Salvia,Â and Vittorio Giovannetti (<a href="https://arxiv.org/abs/2103.14511">arXiv</a>). This paper takes identity testing to the quantum setting. One should think of a \(d\)-dimensional quantum state as a \(d \times d\) density matrix (with some special properties). To learn the state entirely up to error \(\varepsilon\) would require \(O(\varepsilon^{-2} d^2)\) samples/measurements. A recent result of <a href="https://arxiv.org/pdf/1708.06002.pdf">Badescu-OâDonnell-Wright</a> proves that identity testing to a known state can be done significantly faster using \(O(\varepsilon^{-2} d)\) measurements. This paper takes this result a step further by consider a set of \(N\) quantum states. A âsampleâ is like a classical sample, where one gets a sample from a distribution of quantum states. The YES (âuniformâ) case is when all the states are identical. The NO (âfar from uniformâ) case is when they are âfarâ from being the same state. This paper proves that \(O(\varepsilon^{-2}\sqrt{N}d)\) samples suffices for distinguishing these cases.</p></div>
    </content>
    <updated>2021-04-05T04:35:07Z</updated>
    <published>2021-04-05T04:35:07Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Seshadhri</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2021-04-13T22:53:27Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-7501643626120846375</id>
    <link href="https://blog.computationalcomplexity.org/feeds/7501643626120846375/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/04/do-any-np-reductions-use-deep.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7501643626120846375" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7501643626120846375" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/04/do-any-np-reductions-use-deep.html" rel="alternate" type="text/html"/>
    <title>Do any NP-reductions use deep mathematics? Non-rhetically</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">BILL: Lets say we didn't know that FACTORING NPC --&gt; NP=coNP.<div>then what direction would we think Factoring in P or NPC?Â </div><div><br/>STUDENT: In P. After all, Number Theory is a deep subject and I can imagine some deep Theorem in it leading to FACTORING in P.Â </div><div><br/></div><div>BILL: That cuts both ways. I can imagine some deep theorem in NT being the key to showingÂ </div><div><br/></div><div>FACTORING poly-reduces toÂ  SAT</div><div><br/></div><div>STUDENT:Â </div><div>Deep mathematics HAS been used for algorithms. Factoring algs is one example, The graph minor theorem yielding MANY problems in P is another. Can you give me an example where deep mathematics has been used for an NPC reduction?</div><div><br/></div><div>BILL: Oh. Gee. I do not know of any.Â </div><div><br/></div><div>STUDENT: If only you had some mechanism to ask the theory community. Maybe you could call it a web log, or weblog.</div><div><br/></div><div>BILL: If only...</div><div><br/>QUESTIONS</div><div>1) Are there any NPC reductions that use deep math? (I realize that the phrase `deep math' is not well defined,)</div><div><br/></div><div>2) Are there other reductions that use deep math?</div><div><br/></div><div>3) If P NE NP then:Â </div><div>For all epsilon there is no approx-alg for MAX3SAT which yieldsÂ  \geÂ  (7/8 + epsilon)OPT</div><div><br/></div><div>For all deltaÂ  there is no approx-alg for CLIQ which yields &gt; n^{-delta} OPT</div><div><br/></div><div>No approx-alg for SET COVER which yields \ge (ln n + o(1))OPT.Â </div><div><br/></div><div>All of these proofs useÂ  the PCP machinery or something similar. My impression is that the original PCP theorem, while long, hard, and impressive, did not use deep math. I have a vague memory of some paper or PhD thesis stating that the ONLY theorem needed was that a poly of degree d over a finite field has \le d roots.Â </div><div><br/></div><div>However to get the optimal lower bounds seemed to use some deep math. But I am more ASKING than telling.Â </div></div>
    </content>
    <updated>2021-04-05T04:30:00Z</updated>
    <published>2021-04-05T04:30:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-04-14T02:01:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5402</id>
    <link href="https://www.scottaaronson.com/blog/?p=5402" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/train1.MOV" length="1003960" rel="enclosure" type="video/quicktime"/>
    <link href="https://www.scottaaronson.com/yjunctions.MOV" length="22264409" rel="enclosure" type="video/quicktime"/>
    <link href="https://www.scottaaronson.com/blog/?p=5402#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5402" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">The Computational Expressiveness of a Model Train Set: A Paperlet</title>
    <summary xml:lang="en-US">Update (April 5, 2021): So it turns out that Adam Chalcraft and Michael Greene already proved the essential result of this post back in 1994 (hat tip to commenter Dylan). Not terribly surprising in retrospect! My son Daniel had his fourth birthday a couple weeks ago. For a present, he got an electric train set. [â¦]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p><strong><span class="has-inline-color has-vivid-red-color">Update (April 5, 2021):</span></strong> So it turns out that Adam Chalcraft and Michael Greene already <a href="http://www.monochrom.at/turingtrainterminal/Chalcraft.pdf">proved the essential result of this post back in 1994</a> (hat tip to commenter Dylan).  Not terribly surprising in retrospect!</p>



<p/><hr/><p/>



<p>My son Daniel had his fourth birthday a couple weeks ago.  For a present, he got an electric train set.  (For completenessâand since the details of the train set will be rather important to the postâitâs called <a href="https://www.amazon.com/Dinosaur-Flexible-Tracks-Create-Children/dp/B07ZDLXSXK">âWESPREX Create a Dinosaur Trackâ</a>, but this is not an ad and Iâm not getting a kickback for it.)</p>



<figure class="wp-block-video"><video src="https://www.scottaaronson.com/yjunctions.MOV"/></figure>



<p>As you can see, the main feature of this set is a Y-shaped junction, which has a flap that can control which direction the train goes.  The logic is as follows:</p>



<ul><li>If the train is coming up from the âbottomâ of the Y, then it continues to either the left arm or the right arm, depending on where the flap is.  It leaves the flap as it was.</li></ul>



<ul><li>If the train is coming down the left or right arms of the Y, then it continues to the bottom of the Y, <em>pushing the flap out of its way if itâs in the way</em>.  (Thus, if the train were ever to return to this Y-junction coming up from the bottom, not having passed the junction in the interim, it would necessarily go to the same arm, left or right, that it came down from.)</li></ul>



<p>The train set also comes with bridges and tunnels; thus, thereâs no restriction of planarity.  Finally, the train set comes with little gadgets that can reverse the trainâs direction, sending it back in the direction that it came from:</p>



<figure class="wp-block-video"><video src="https://www.scottaaronson.com/train1.MOV"/></figure>



<p>These gadgets donât seem particularly important, though, since we could always replace them if we wanted by a Y-junction together with a loop.</p>



<p>Notice that, at each Y-junction, the position of the flap stores one bit of internal state, and that the train can both âreadâ and âwriteâ these bits as it moves around.  Thus, a question naturally arises: can this train set do any nontrivial computations?  If there are <em>n</em> Y-junctions, then can it cycle through exp(<em>n</em>) different states?  Could it even solve <strong>PSPACE</strong>-complete problems, if we let it run for exponential time?  (For a very different example of a model-train-like system that, as it turns out, <em>is</em> able to express <strong>PSPACE</strong>-complete problems, see <a href="https://arxiv.org/abs/1905.00518">this recent paper</a> by Erik Demaine et al.)</p>



<p>Whatever the answers regarding Danielâs train set, I knew immediately on watching the thing go that Iâd have to write a âpaperletâ on the problem and publish it on my blog (no, I donât inflict such things on journals!).  Todayâs post constitutes my third âpaperlet,â on the general theme of a discrete dynamical system that someone showed me in real life (e.g. in a childrenâs toy or in biology) having more structure and regularity than one might naÃ¯vely expect.  My first such paperlet, from 2014, was <a href="https://www.scottaaronson.com/blog/?p=1902">on a 1960s toy called the Digi-Comp II</a>; my second, from 2016, was <a href="https://www.scottaaronson.com/blog/?p=2862">on DNA strings acted on by recombinase</a> (OK, that one <em>was</em> associated with a <a href="https://science.sciencemag.org/content/353/6297/aad8559.full?ijkey=wzroPPh1eIu9k&amp;keytype=ref&amp;siteid=sci">paper in <em>Science</em></a>, but my combinatorial analysis wasnât the main point of the paper).</p>



<p>Anyway, after spending an enjoyable evening on the problem of Danielâs train set, I was able to prove that, alas, the possible behaviors are quite limited (I classified them all), falling far short of computational universality.</p>



<p>If you feel like Iâm wasting your time with trivialities (or if you simply enjoy puzzles), then before you read any further, I encourage you to stop and try to prove this for yourself!</p>



<p>Back yet?  OK thenâ¦</p>



<p/><hr/><p/>



<p><strong>Theorem:</strong> Assume a finite amount of train track.  Then after a linear amount of time, the train will necessarily enter a âboring infinite loopââi.e., an attractor state in which at most two of the flaps keep getting toggled, and the rest of the flaps are fixed in place.  In more detail, the attractor must take one of four forms:</p>



<p>I. a line (with reversing gadgets on both ends),<br/>II. a simple cycle,<br/>III. a âlollipopâ (with one reversing gadget and one flap that keeps getting toggled), or<br/>IV. a âdumbbellâ (with two flaps that keep getting toggled).</p>



<p>In more detail still, there are seven possible topologically distinct trajectories for the train, as shown in the figure below.</p>



<figure class="wp-block-image size-large"><a href="https://www.scottaaronson.com/trajectories.png"><img alt="" src="https://www.scottaaronson.com/trajectories.png"/></a></figure>



<p>Here the red paths represent the attractors, where the train loops around and around for an unlimited amount of time, while the blue paths represent ârunwaysâ where the train spends a limited amount of time on its way into the attractor.  Every degree-3 vertex is assumed to have a Y-junction, while every degree-1 vertex is assumed to have a reversing gadget, unless (in IIb) the train starts at that vertex and never returns to it.</p>



<p>The proof of the theorem rests on two simple observations.</p>



<p><strong>Observation 1:</strong> While the Y-junctions correspond to vertices of degree 3, there are no vertices of degree 4 or higher.  This means that, if the train ever revisits a vertex <em>v</em> (other than the start vertex) for a second time, then there must be some edge <em>e</em> incident to <em>v</em> that it also traverses for a second time immediately afterward.</p>



<p><strong>Observation 2:</strong> Suppose the train traverses some edge <em>e</em>, then goes around a simple cycle (meaning, one where no edges or vertices are reused), and then traverses <em>e</em> again, <em>going in the same direction as the first time</em>.  Then from that point forward, the train will just continue around the same simple cycle forever.</p>



<p>The proof of Observation 2 is simply that, if there were any flap that might be in the trainâs way as it continued around the simple cycle, then the train would already have pushed it out of the way its <em>first</em> time around the cycle, and nothing that happened thereafter could possibly change the flapâs position.</p>



<p>Using the two observations above, letâs now prove the theorem.  Let the train start where it will, and follow it as it traces out a path.  Since the graph is finite, at some point some already-traversed edge must be traversed a second time.  Let <em>e</em> be the first such edge.  By Observation 1, this will also be the first time the trainâs path intersects itself at all.  There are then three cases:</p>



<p><strong>Case 1:</strong> The train traverses <em>e</em> in the same direction as it did the first time.  By Observation 2, the train is now stuck in a simple cycle forever after.  So the only question is what the train couldâve done <em>before</em> entering the simple cycle.  We claim that at most, it couldâve traversed a simple path.  For otherwise, weâd contradict the assumption that <em>e</em> was the first edge that the train visited twice on its journey.  So the trajectory must have type IIa, IIb, or IIc in the figure.</p>



<p><strong>Case 2:</strong> Immediately after traversing e, the train hits a reversing gadget and traverses <em>e</em> again the other way.  In this case, the train will clearly retrace its entire path and then continue past its starting point; the question is what happens next.  If it hits another reversing gadget, then the trajectory will have type I in the figure.  If it enters a simple cycle and stays in it, then the trajectory will have type IIb in the figure.  If, finally, it makes a simple cycle and then <em>exits</em> the cycle, then the trajectory will have type III in the figure.  In this last case, the trainâs trajectory will form a âlollipopâ shape.  Note that there must be a Y-junction where the âstickâ of the lollipop meets the âcandyâ (i.e., the simple cycle), with the base of the Y aligned with the stick (since otherwise the train wouldâve continued around and around the candy).  From this, we deduce that every time the train goes around the candy, it does so in a different orientation (clockwise or counterclockwise) than the time before; and that the train toggles the Y-junctionâs flap every time it exits the candy (although not when it enters the candy).</p>



<p><strong>Case 3:</strong> At some point after traversing <em>e</em> in the forward direction (but not <em>immediately</em> after), the train traverses <em>e</em> in the reverse direction.  In this case, the broad picture is analogous to Case 2.  So far, the train has made a lollipop with a Y-junction connecting the stick to the candy (i.e. cycle), the base of the Y aligned with the stick, and <em>e</em> at the very top of the stick.  The question is what happens next.  If the train next hits a reversing gadget, the trajectory will have type III in the figure.  If it enters a new simple cycle, disjoint from the first cycle, and never leaves it, the trajectory will have type IId in the figure.  If it enters a new simple cycle, disjoint from the first cycle, and <em>does</em> leave it, then the trajectory now has a âdumbbellâ pattern, type IV in the figure (also shown in the first video).  Thereâs only one other situation to worry about: namely, that the train makes a new cycle that <em>intersects</em> the first cycle, forming a âthetaâ (Î¸) shaped trajectory.  In this case, there must be a Y-junction at the point where the new cycle bumps into the old cycle.  Now, if the base of the Y isnât part of the old cycle, then the train never couldâve made it all the way around the old cycle in the first place (it wouldâve exited the old cycle at this Y-junction), contradiction.  If the base of the Y <em>is</em> part of the old cycle, then the flap must have been initially set to let the train make it all the way around the old cycle; when the train then reenters the old cycle, the flap must be moved so that the train will never make it all the way around the old cycle again.  So now the train is stuck in a new simple cycle (sharing some edges with the old cycle), and the trajectory has type IIc in the figure.</p>



<p>This completes the proof of the theorem.</p>



<p/><hr/><p/>



<p>We might wonder: <em>why</em> isnât this model train set capable of universal computation, of AND, OR, and NOT gatesâor at any rate, of <em>some</em> computation more interesting than repeatedly toggling one or two flaps?  My answer might sound tautological: itâs simply that the logic of the Y-junctions is too limited.  Yes, the flaps can get pushed out of the wayâthatâs a âbit flipââbut every time such a flip happens, it helps to set up a âgrooveâ in which the train just wants to continue around and around forever, not flipping any additional bits, with only the minor complications of the lollipop and dumbbell structures to deal with.  Even though my proof of the theorem mightâve seemed like a tedious case analysis, it had this as its unifying message.</p>



<p>Itâs interesting to think about what gadgets would need to be added to the train set to <em>make</em> it computationally universal, or at least expressively richerâable, as <a href="https://www.scottaaronson.com/blog/?p=1902">turned out</a> to be the case for the Digi-Comp II, to express some nontrivial complexity class falling short of <strong>P</strong>.  So for example, what if we had degree-4 vertices, with little turnstile gadgets?  Or multiple trains, which could be synchronized to the millisecond to control how they interacted with each other via the flaps, or which could even crash into each other?  I look forward to reading your ideas in the comment section!</p>



<p>For the truth is this: quantum complexity classes, BosonSampling, closed timelike curves, circuit complexity in black holes and AdS/CFT, etc. etc.âall these topics are great, but the same models and problems do get stale after a while.  I aspire for my research agenda to chug forward, full steam ahead, into new computational domains.</p>



<p>PS. Happy Easter to those who celebrate!</p></div>
    </content>
    <updated>2021-04-04T18:37:49Z</updated>
    <published>2021-04-04T18:37:49Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Complexity"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Embarrassing Myself"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Procrastination"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-04-09T18:27:57Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/050</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/050" rel="alternate" type="text/html"/>
    <title>TR21-050 |  Linear Threshold Secret-Sharing with Binary Reconstruction | 

	Marshall Ball, 

	Alper Cakan, 

	Tal Malkin</title>
    <summary>Motivated in part by applications in lattice-based cryptography, we initiate the study of the size of linear threshold (`$t$-out-of-$n$') secret-sharing where the linear reconstruction function is restricted to coefficients in $\{0,1\}$. We prove upper and lower bounds on the share size of such schemes. One ramification of our results is that a natural variant of Shamir's classic scheme [Comm. of ACM, 1979], where bit-decomposition is applied to each share, is optimal for when the underlying field has characteristic 2. Another ramification is that schemes obtained from some monotone formulae are optimal for certain threshold values when the field's characteristic is any constant. We prove our results by defining and investigating an equivalent variant of Karchmer and Wigderson's Monotone Span Programs [CCC, 1993]. 

We also study the complexity such schemes with the additional requirement that the joint distribution of the shares of any unauthorized set of parties is not only independent of the secret, but also uniformly distributed. This property is critical for security of certain applications in lattice-based cryptography. We show that any such scheme must use $\Omega(n\log n)$ field elements, regardless of the field. Moreover, for any field this is tight up to constant factors for the special cases where any $t=n-1$ parties can reconstruct, as well as for any threshold when the field characteristic is 2.</summary>
    <updated>2021-04-04T06:36:04Z</updated>
    <published>2021-04-04T06:36:04Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-04-14T02:20:28Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8067</id>
    <link href="https://windowsontheory.org/2021/04/03/natural-language-processing-guest-lecture-by-sasha-rush/" rel="alternate" type="text/html"/>
    <title>Natural Language Processing (guest lecture by Sasha Rush)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Scribe notes by Benjamin Basseri and Richard Xu Previous post: Inference and statistical physics Next post: TBD. See also all seminar posts and course webpage. Alexander (Sasha) Rush is a professor at Cornell working in in Deep Learning / NLP. He applies machine learning to problems of text generation, summarizing long documents, and interactions between â¦ <a class="more-link" href="https://windowsontheory.org/2021/04/03/natural-language-processing-guest-lecture-by-sasha-rush/">Continue reading <span class="screen-reader-text">Natural Language Processing (guest lecture by SashaÂ Rush)</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>Scribe notes by Benjamin Basseri and <a href="https://github.com/rxu18">Richard Xu</a></em></p>



<p><strong>Previous post:</strong> <a href="https://windowsontheory.org/2021/04/02/inference-and-statistical-physics/">Inference and statistical physics</a> <strong>Next post:</strong> TBD. See also <a href="https://windowsontheory.org/category/ml-theory-seminar/">all seminar posts</a> and <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">course webpage</a>.</p>



<p><a href="http://rush-nlp.com/">Alexander (Sasha) Rush</a> is a professor at Cornell working in in Deep Learning / NLP. He applies machine learning to problems of text generation, summarizing long documents, and interactions between character and word-based models. Sasha is previously at Harvard, where he taught an awesome NLP class, and we are excited to have him as our guest! (Note: some of the figures in this lecture are taken from other papers or presentations.)</p>



<p>The first half of the talk will focus on how NLP works and what makes it interesting: a birdâs-eye view of the field. The second half of the talk will focus on current research.</p>



<h2>Basics of NLP</h2>



<p>Textual data has many different challenges that differ from computer vision (CV), since it is a human phenomenon. There are methods that work in computer vision / other ML models that just donât work for NLP (e.g. GANs). As effective methods were found for computer vision around 2009-2014, we thought that these methods would also work well for NLP. While this was sometimes the case, it has not been true in general.</p>



<p>What are the difficulties of working with natural language? Language works at different scales:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">word &lt; phrase &lt; sentence &lt; document &lt; ...
</pre></div>


<p>Here are examples of structure at each level:</p>



<ol><li>Zipfâs Law: The frequency of any word is inversely proportional to its popularity rank.</li><li>Given the last <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> symbols, it is often possible to predict the next one (The Shannon Game).</li><li>Linguists have found many rules about syntax and semantics of a language.</li><li>In a document, we have lots of discourse between different sentences. For example, âitâ or other pronouns are context dependent.</li></ol>



<p>In NLP, we will talk about the <em>syntax</em> and <em>semantics</em> of a document. The syntax refers to how words can fit together, and semantics refers to the meaning of these words.</p>



<h2>Language Modeling</h2>



<p>There are many different NLP tasks such as sentiment analysis, question answering, named entity recognition, and translation. However, recent research shows that these tasks are often related to language modeling.</p>



<p>Language modeling, as explained in Shannon 1948, aims to answer the following question: <em>Think of language as a stochastic process producing symbols. Given the last <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> symbols, can we predict the next one?</em></p>



<p>This question is challenging as is. Consider the following example:</p>


<div class="wp-block-syntaxhighlighter-code "><pre class="brush: plain; title: ; notranslate">A reading lamp on the desk shed glow on polished ___
</pre></div>


<p>There are many options: marble/desk/stone/engraving/etc., and it is already difficult to give a probability here. In general, language is hard to model because the next word can be connected to words from a long time ago.</p>



<p>Shannon proposes variants of Markov models to perform this prediction, based on the last couple characters or the context in general.</p>



<p>Since local context matters most, we assume that only the <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> most recent words matter. Then, we get the model<br/><img alt="p(w_{t+1}|w_{t-n},\ldots,w_t)=\frac{p(w_{t-n},\ldots,w_t,w_{t+1})}{p(w_{t-n},\ldots,w_t)}." class="latex" src="https://s0.wp.com/latex.php?latex=p%28w_%7Bt%2B1%7D%7Cw_%7Bt-n%7D%2C%5Cldots%2Cw_t%29%3D%5Cfrac%7Bp%28w_%7Bt-n%7D%2C%5Cldots%2Cw_t%2Cw_%7Bt%2B1%7D%29%7D%7Bp%28w_%7Bt-n%7D%2C%5Cldots%2Cw_t%29%7D.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<h3>Measuring Performance</h3>



<p>As <a href="https://windowsontheory.org/2021/02/24/unsupervised-learning-and-generative-models/">we have seen in the generative models lecture</a>, we can use cross entropy as a loss function for density estimation models. Given model density distribution <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and true distribution <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the cross entropy (which equals negative expected log-likelihood) is defined as follows:</p>



<p><img alt="H(p, q) = - E_p \log q(w_t | w_1, \ldots , w_{t-1})" class="latex" src="https://s0.wp.com/latex.php?latex=H%28p%2C+q%29+%3D+-+E_p+%5Clog+q%28w_t+%7C+w_1%2C+%5Cldots+%2C+w_%7Bt-1%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>In NLP we tend to use the metric âperplexityâ, which is the exponentiated negative cross entropy:</p>



<p><img alt="ppl(p, q) = \exp -H(p, q)." class="latex" src="https://s0.wp.com/latex.php?latex=ppl%28p%2C+q%29+%3D+%5Cexp+-H%28p%2C+q%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>This corresponds to the equivalent vocabulary size of a uniformly distributed model. Lower perplexity means our model was closer to the underlying text distribution. As an example, the perplexity of the perfect dice-roll model would be 6.</p>



<p>Why do we care about perplexity anyway?</p>



<ol><li>With a good model we can determine the natural perplexity of a language, which is interesting.</li><li>Many NLP questions are language modeling with conditioning. Speech recognition is language modeling conditioned on some sound signal, and translation is language modeling conditioned on text from another language.</li><li>More importantly, we have found recent applications in <em>transfer learning</em>. That is, a language model can be trained on some (small) input data for a specific task. Then, such a model becomes effective at the given task!</li></ol>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2021/04/model_task.png"><img alt="" class="wp-image-8081" src="https://windowsontheory.files.wordpress.com/2021/04/model_task.png?w=1024"/></a></figure>



<p>A few years ago, the best perplexity on WSJ text was 150. Nowadays, it is about 20! To understand how we got here, we look at modern language modeling.</p>



<h2>Predicting the next word</h2>



<p>We start with the model</p>



<p><img alt="p(w_t | w_{1:t-1}; \theta) = softmax(\mathbf{W}\phi(w_{1:t-1}; \theta))." class="latex" src="https://s0.wp.com/latex.php?latex=p%28w_t+%7C+w_%7B1%3At-1%7D%3B+%5Ctheta%29+%3D+softmax%28%5Cmathbf%7BW%7D%5Cphi%28w_%7B1%3At-1%7D%3B+%5Ctheta%29%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>(The softmax function maps a vector <img alt="\vec{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bv%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> into the probability distribution <img alt="\vec{p}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bp%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> such that <img alt="p_i \propto \exp(v_i)" class="latex" src="https://s0.wp.com/latex.php?latex=p_i+%5Cpropto+%5Cexp%28v_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. That is, <img alt="p_i = \exp(v_i)/\sum_j \exp(v_j)" class="latex" src="https://s0.wp.com/latex.php?latex=p_i+%3D+%5Cexp%28v_i%29%2F%5Csum_j+%5Cexp%28v_j%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Note that this is a Boltzman distribution of the type we saw in the <a href="https://windowsontheory.org/2021/04/02/inference-and-statistical-physics/">statistical physics and variational algorithms</a> lecture)</p>



<p>We call <strong>W</strong> the output word embeddings, <img alt="\phi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is some neural basis (e.g. <img alt="\phi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are all but the final layers of a neural net with weights <img alt="\theta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, and <strong>W</strong> is the final layer). However, this means to use softmax to predict requires computing softmax over every word in your vocabulary (tens or hundreds of thousands). This was often infeasible until GPU computing became available.</p>



<p>As an aside, why not predict characters instead of words? The advantage is that there are much fewer characters than words. However, computation with characters is slower. Empirically, character-based models tend to perform worse than word-based. However, character-based models can handle words outside the vocabulary.</p>



<p><a href="https://en.wikipedia.org/wiki/Byte_pair_encoding">Byte-pair encoding</a> offers a bridge between character and word models. This greedily builds up new tokens as repetitive patterns are found in the original text.</p>



<p>In the last decade NLP has seen a few dominant architectures, all using SGD but with varying bases. First, we must cast the words as one-hot vectors, then embed them into vector space:<br/><img alt="x_t = Vw_t," class="latex" src="https://s0.wp.com/latex.php?latex=x_t+%3D+Vw_t%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/><br/>where <img alt="w_t" class="latex" src="https://s0.wp.com/latex.php?latex=w_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the one-hot encoded vector and <img alt="V" class="latex" src="https://s0.wp.com/latex.php?latex=V&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the learned embedding transformation.</p>



<h3>NNLM</h3>



<p>NNLM (Neural Network Language Model) is like a CNN. The model predicts on possibly multiple NN transformations:<br/><img alt="\phi(w_{1:t-1}; \theta) = \sigma(U[x_{t-k-1} \oplus \ldots \oplus x_{t-1}]])," class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi%28w_%7B1%3At-1%7D%3B+%5Ctheta%29+%3D+%5Csigma%28U%5Bx_%7Bt-k-1%7D+%5Coplus+%5Cldots+%5Coplus+x_%7Bt-1%7D%5D%5D%29%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="\oplus" class="latex" src="https://s0.wp.com/latex.php?latex=%5Coplus&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> denotes concatenation, <img alt="U" class="latex" src="https://s0.wp.com/latex.php?latex=U&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is some convolutional filter and <img alt="\sigma" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csigma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the activation function. This has the benefit of learning fast. The matrices it learns also transfer well.</p>



<p>As an example, GloVe is a NNLM-inspired model. It stores the words in 300-dimensional space. When we project the some words to 2-dimensions using PCA, we find semantic information in the language model.</p>



<figure class="wp-block-image"><img alt="Language structure" src="https://i.imgur.com/wesRAtM.png"/></figure>



<h3>Recurrent Models</h3>



<p>A recurrent model uses a fixed set of previous words to predict the next word. A recurrent network uses all previous words:<br/><img alt="\phi(w_{1:t-1}; \theta) = \sigma(U[x_{t-1}\oplus \phi(w_{1:t-2};\theta)])." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi%28w_%7B1%3At-1%7D%3B+%5Ctheta%29+%3D+%5Csigma%28U%5Bx_%7Bt-1%7D%5Coplus+%5Cphi%28w_%7B1%3At-2%7D%3B%5Ctheta%29%5D%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Previous information is âsummarizedâ in the <img alt="\phi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> on the right, so this model uses finite memory. Below is an illustration of the recurrent neural network.</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2021/04/rnn.png"><img alt="" class="wp-image-8080" src="https://windowsontheory.files.wordpress.com/2021/04/rnn.png?w=1024"/></a></figure>



<p>Since the recurrent model uses the full context, it is a more plausible model for how we really process language. Furthermore, the introduction of RNN saw drastically improved performance. In the graph below, the items in the chart are performances from previous NNLMs. The recurrent network performance is âoff the chartâ.</p>



<figure class="wp-block-image"><img alt="RNN Performance" src="https://i.imgur.com/v5rgNeQ.png"/></figure>



<p>However, the model grows with sequence length. This requires gradient flow to backpropagate over arbitrarily long sequences, and often required baroque network designs to facilitate longer sequences while avoiding exploding/vanishing gradients.</p>



<h3>Attentional Models</h3>



<p>To understand modern NLP we must look at <a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)">attention</a>. For a set of vectors <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, keys <img alt="k_1, \ldots, k_T" class="latex" src="https://s0.wp.com/latex.php?latex=k_1%2C+%5Cldots%2C+k_T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and a query <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we define attention as</p>



<p><img alt="Att(q, k, v) = \sum_t \alpha_t v_t" class="latex" src="https://s0.wp.com/latex.php?latex=Att%28q%2C+k%2C+v%29+%3D+%5Csum_t+%5Calpha_t+v_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="\alpha = softmax(q\cdot k_1 \oplus \ldots \oplus q \cdot k_T)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha+%3D+softmax%28q%5Ccdot+k_1+%5Coplus+%5Cldots+%5Coplus+q+%5Ccdot+k_T%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>Here, <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> can be considered a probability density of the modelâs âmemoryâ of the sequence. The model decides which words are important by combining <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>The <em>attentional model</em> can be fully autoregressive (use all previously seen words), and the query <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> can be learned or be specific to an input. In general, we have:<br/><img alt="\phi(w_{1:t-1}; \theta) = \sigma(U[Att(q, x_{1:t-1}, x_{1:t-1})])" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi%28w_%7B1%3At-1%7D%3B+%5Ctheta%29+%3D+%5Csigma%28U%5BAtt%28q%2C+x_%7B1%3At-1%7D%2C+x_%7B1%3At-1%7D%29%5D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Since we condense all previous information into the attention mechanism, it is simpler to backpropagate.<br/>In particular attention enables looking at information from a large context without paying for this in the depth of the network and hence in the depth of back-propagation you need to cover. (Boazâs note: With my crypto background, attention reminds me of the design of <a href="https://en.wikipedia.org/wiki/Block_cipher">block ciphers</a>, which use linear operations to mix between far away parts of the inputs, and then apply non-liearity locally to each small parts.)</p>



<p>Note that attention is defined with respect to a set of vectors. There is no idea of positional information in the attentional model. How do we encode positional information for the model? One way to do this is using <em>sinusoidal encoding</em> in the keys. We store the word <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as <img alt="\cos(\pi n/k)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ccos%28%5Cpi+n%2Fk%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for some period <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Notice that if we choose many different periods, then the cosine ways will almost never meet at the same point. As a result, only recent points will have high dot products between the different cosine values.</p>



<figure class="wp-block-image"><img alt="Sinusoidal" src="https://i.imgur.com/kAnYjAP.png"/></figure>



<h3>Transformers</h3>



<p>A transformer is a stacked attention model. Computation in one layer becomes query, keys and values for the next layer. This is a multiheaded attention model. We learn <img alt="H" class="latex" src="https://s0.wp.com/latex.php?latex=H&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> projections for each query/key (generally between 8 and 32) then do softmax across these projections:<br/><img alt="\alpha^h = softmax((W^{(h)}q)\cdot(V^{(h)}k_1)\oplus \ldots \oplus (W^{(h)}q)\cdot(V^{(h)}k_T))." class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%5Eh+%3D+softmax%28%28W%5E%7B%28h%29%7Dq%29%5Ccdot%28V%5E%7B%28h%29%7Dk_1%29%5Coplus+%5Cldots+%5Coplus+%28W%5E%7B%28h%29%7Dq%29%5Ccdot%28V%5E%7B%28h%29%7Dk_T%29%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<figure class="wp-block-image"><img alt="Transformer Architecture" src="https://i.imgur.com/uUp2elD.png"/></figure>



<p>These heads can be computed in parallel and can be implemented with batch matrix multiplication. As a result, transformers can be massively scaled and are extremely efficient in modern hardware. This has led these models to be very dominant in the field. Here are some example models:</p>



<ol><li>GPT-1,2,3 are able to generate text that is quite convincing to a human. They also handle the syntax and semantics of a language quite well.</li><li>BERT is a transformer-based model that examines text both forwards and backwards in making its predictions. It works well with transfer fining tuning: train on a large data set, then take the feature representations and train a task on top of the learned representation.</li></ol>



<h2>Scaling</h2>



<p>In recent years we have had larger and larger models, from GPT1âs 110 million to GPT3âs 175 billion.</p>



<figure class="wp-block-image"><img alt="Scaling Up" src="https://i.imgur.com/8drDknV.png"/></figure>



<p>On these massive scales, scaling has become a very interesting issue: how do we process more samples? How do we run distributed computation? How much autoregressive input should each model looks at? (Boazâs note: To get a sense of how big these models are, GPT-3 was trained on about <a href="https://lambdalabs.com/blog/demystifying-gpt-3/">1000B tokens</a>. The total <a href="https://www.prb.org/howmanypeoplehaveeverlivedonearth/">number of people that ever lived</a> is about 100B, and only about half since the invention of the printing press, so this is arguably a non-negligible fraction of all text produced in human history.)</p>



<p>For a model like BERT, most of the cost still comes from feed-forward network â mostly matrix multiplications. These are tasks we are familiar with and can scale up.</p>



<p>One question is to have long-range attentional lookup, which is important for language modeling. For now, models often look at most 512 words back. Can we do longer range lookups? One approach to this is kernel feature attention.</p>



<h3>Kernel Feature Attention</h3>



<p>Recall that we have <img alt="\alpha=\mathrm{softmax}(q\cdot k_i)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%3D%5Cmathrm%7Bsoftmax%7D%28q%5Ccdot+k_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Can we approximate this with some kernel <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>? The main approach is that <img alt="\alpha\propto \exp(q\cdot k_i)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha%5Cpropto+%5Cexp%28q%5Ccdot+k_i%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, which we approximate with the kernel <img alt="\exp(v_1\cdot v_2)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cexp%28v_1%5Ccdot+v_2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. There is a rich literature on approximating <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> where<br/><img alt="K(v_1, v_2) \approx \phi(v_1)\cdot\phi(v_2)" class="latex" src="https://s0.wp.com/latex.php?latex=K%28v_1%2C+v_2%29+%5Capprox+%5Cphi%28v_1%29%5Ccdot%5Cphi%28v_2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>for some transfomration <img alt="\phi" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cphi&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Then, we can try to approximate <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> with linear features.</p>



<p>Practically, transformers do well but are slower. For longer texts, we have faster models that do slightly worse. A recent model called <a href="https://arxiv.org/abs/2009.14794">performer</a> is such an example.<br/><img alt="LRA Performance" src="https://i.imgur.com/S3vs2o5.png"/></p>



<h3>Scaling Down</h3>



<p>Ultimately, we want to make models run on ânon Google scaleâ hardware once it has been trained to a specific task. This can often require scaling down.</p>



<p>One approach is to prune weights according to their magnitude. However, since models are often overparameterized and weights do not move much, the weights that get pruned according to this method are usually the weights that were simply initialized closest to 0. In the diagram below, we can consider only leaving the orange weights and cutting out the gray.</p>



<figure class="wp-block-image"><img alt="Pruning" src="https://i.imgur.com/MqY5jK2.png"/></figure>



<p>Another approach is to mask out the weights that are unnecessary for a specific task, if youâre trying to ship a model for a specific task.</p>



<h2>Research Questions</h2>



<p>Two major lines of research dominate NLP today:</p>



<ol><li>Attention / Transformer architecture,</li><li>Pretraining with language modeling as a deep learning paradigm.</li></ol>



<p>We are also in a space race to produce efficient models with more parameters, given how much scaling has been effective.</p>



<p>The paper <a href="https://arxiv.org/abs/2010.00854">Which BERT</a> classifies modern questions in NLP into the following categories:</p>



<ol><li><strong>Tasks:</strong> Is (masked) language modeling the best task for pretraining? Language modeling emphasizes local information. We could imagine doing other types of denoising. See also <a href="https://openai.com/blog/dall-e/">DALL-E</a>.</li><li><strong>Efficiency:</strong> We see that much fewer parameters are needed in practice after pruning. Does pruning lose something? Pruned models tend to do well on in-sample data, but out-of-sample data tends to make the pruned model do worse.</li><li><strong>Data:</strong> How does data used in pretraining impact task accuracy? How does the data of pretraining impact task bias?</li><li><strong>Interpretability:</strong> What does BERT know, and how can we determine this? Does interpretability need to come at a cost to performance?</li><li><strong>Multilinguality:</strong> Many languages donât have the same amount of data as English. What methods apply when we have less data?</li></ol>



<h2>Q&amp;A</h2>



<p>We have many questions asked during and after lecture. Here are some of the questions.</p>



<ol><li><strong>Q:</strong> Should we say GANs fail at NLP or that other generative models are more advanced in NLP than in CV? <strong>A:</strong> One argument is that language is a human generated system, there are some inherent structures that help with generation. We can do language in left-to-right, but in CV this would be a lot more difficult. At the same time, this can change in the future!</li><li><strong>Q:</strong> Why are computer vision and NLP somewhat close to each other? <strong>A:</strong> classically, they are both perception-style tasks under AI. Also, around 2014 we had lots of ideas that come from porting CV ideas into NLP, and recently we have seen NLP ideas ported to CV.</li><li><strong>Q:</strong> Since languages have unique grammars, is NLP better at some languages? Do we have to translate language to an âNLP-effectiveâ language and back? <strong>A:</strong> In the past, some languages are better. Ex: we used to struggle with Japanese to other languages but do well with English to other languages. However, modern models are <em>extremely</em> data driven, so we have needed much less hardcoding.</li><li><strong>Q:</strong> Have we done any scatter plot of the form (data available for language X, performance on X) to see if performance is just a function of available data? <strong>A:</strong> Not right now, but these plots can potentially be really cool! Multilinguality is a broad area of research in general.</li><li><strong>Q:</strong> What are some NLP techniques for low-resource languages? <strong>A:</strong> Bridging is commonly used. Iterative models (translate and translate back with some consistency) is also used to augment the data.</li><li><strong>Q:</strong> Do you think old-school parsers will make a comeback? <strong>A:</strong> Unlikely to deploy parsers, but the techniques of parsing is interesting.</li><li><strong>Q:</strong> Given the large number of possible âcorrectâ answers, has there been work on which âcontextsâ are most informative? <strong>A:</strong> Best context is the closest context, which is expected. The other words matter but matter a lot less.</li><li><strong>Q:</strong> Is there any model that captures the global structure first (e.g. an outline) before writing sequentially, like humans do when they write longer texts? <strong>A:</strong> Currently no. Should be possible, but we do not have data about the global structure of writing.</li><li><strong>Q:</strong> Why is our goal density estimation? <strong>A:</strong> It is useful because it tells us how âsurprisingâ the next word is. This is also related to why a language feels âfastâ when you first learn it: because you are not familiar with the words, you cannot anticipate the next words.</li><li><strong>Q:</strong> Why is lower perplexity better? <strong>A:</strong> Recall from past talk that lower cross-entropy means less distance between <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, and intuitive you have more âcertaintyâ.</li><li><strong>Q:</strong> Is the reason LM so important because evaluations are syntax-focused? <strong>A:</strong> Evaluations are actually more semantically focused, but syntax and semantics are quite connected.</li><li><strong>Q:</strong> Do we see attentional models in CV? <strong>A:</strong> Yes, we have seen more use of transformer in CV. In a standard model we use data only recently, and here we get to work with data across space and time. As such, we will need to encode time positionally.</li><li><strong>Q:</strong> Why is attention generalized convolution? <strong>A:</strong> If you have attention with all mass in the local area, thatâs probably like convolution.</li><li><strong>Q:</strong> How do we compare heads with depth? e.g. Is having 5 heads better than 5x depth? <strong>A:</strong> when we use heads we add a lot less parameters. As such, we can parallelize heads and increase performance.</li><li><strong>Q:</strong> Do you think transformers are the be-all end-all of NLP models? <strong>A:</strong> Maybe. To dethrone transformers, you have to both show similar work on small-scale and show that it can be scaled easily.</li><li><strong>Q:</strong> How does simplicity bias affect these transferrable models? <strong>A:</strong> surprising and we are not sure. In CV we found that the models quickly notice peculiarities in the data (e.g. how mechanical turks are grouped), but the models do work.</li><li><strong>Q:</strong> We get bigger models every year and better performance. Will this end soon? <strong>A:</strong> Probably not, as it seems like having more parameters helps it recognize some additional features.</li><li><strong>Q:</strong> If we prune models to the same size, will they have the same performance? <strong>A:</strong> for small models we can seem to prune them, but for the bigger models it is hard to run them in academica given the computational resource constraints.</li><li><strong>Q:</strong> When we try to remember something from a long time ago we would look up a textbook / etc. Have we had similar approaches in practice? <strong>A:</strong> transformer training is static at first, and tasks happen later. So, we have to decide how to throw away information before we train on the tasks.</li><li><strong>Q:</strong> Are better evaluation metrics an important direction for future research? <strong>A:</strong> Yes â this has been the case for the past few years in academia.</li><li><strong>Q:</strong> What is a benchmark/task where you think current models show deep lack of capability? <strong>A:</strong> During generation, models donât seem to distinguish between information that makes it âsound goodâ and factually correct information.</li></ol></div>
    </content>
    <updated>2021-04-03T14:37:39Z</updated>
    <published>2021-04-03T14:37:39Z</published>
    <category term="ML Theory seminar"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-04-14T02:21:00Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/04/02/islands</id>
    <link href="https://11011110.github.io/blog/2021/04/02/islands.html" rel="alternate" type="text/html"/>
    <title>Islands</title>
    <summary>In the neighborhood where I live, fire safety regulations require the streets to be super-wide (so wide that two fire trucks can pass even with cars parked along both sides of the street), and to have even wider turnarounds at the ends of the culs-de-sac. To break up the resulting vast expanses of pavement, we have occasional islands of green, public gardens too small to name as a park. They come in several different types: medians to separate the incoming and outgoing lanes at junctions with larger roads,</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>In the neighborhood where I live, fire safety regulations require the streets to be super-wide (so wide that two fire trucks can pass even with cars parked along both sides of the street), and to have even wider turnarounds at the ends of the culs-de-sac. To break up the resulting vast expanses of pavement, we have occasional islands of green, public gardens too small to name as a park. They come in several different types: medians to separate the incoming and outgoing lanes at junctions with larger roads,</p>

<p style="text-align: center;"><img alt="Traffic island at Frost and Gabrielino, Irvine" src="https://www.ics.uci.edu/~eppstein/pix/islands/Frost-m.jpg" style="border-style: solid; border-color: black;"/></p>

<p>barriers to separate small groups of houses from the main flow of the road,</p>

<p style="text-align: center;"><img alt="Traffic island on Los Trancos Drive, Irvine" src="https://www.ics.uci.edu/~eppstein/pix/islands/LosTrancos-m.jpg" style="border-style: solid; border-color: black;"/></p>

<p>or giving some shape to the turnaround at the end of a cul-de-sac.</p>

<p style="text-align: center;"><img alt="Traffic island on Harvey Court, Irvine" src="https://www.ics.uci.edu/~eppstein/pix/islands/Harvey-m.jpg" style="border-style: solid; border-color: black;"/></p>

<p>Most are ignored except by the community associationâs gardeners and by passing cars, but I did find one set up as a neighborhood basketball court:</p>

<p style="text-align: center;"><img alt="Traffic island on Perkins Court, Irvine" src="https://www.ics.uci.edu/~eppstein/pix/islands/Perkins-m.jpg" style="border-style: solid; border-color: black;"/></p>

<p><a href="https://www.ics.uci.edu/~eppstein/pix/islands/">The rest of the gallery</a>.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/105998376302792384">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-04-02T16:34:00Z</updated>
    <published>2021-04-02T16:34:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-04-02T23:41:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8054</id>
    <link href="https://windowsontheory.org/2021/04/02/inference-and-statistical-physics/" rel="alternate" type="text/html"/>
    <title>Inference and statistical physics</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Scribe notes by Franklyn Wang Previous post: Robustness in train and test time Next post: Natural Language Processing (guest lecture by Sasha Rush). See also all seminar posts and course webpage. lecture slides (pdf) â lecture slides (Powerpoint with animation and annotation) â video Digression: Frequentism vs Bayesianism Before getting started, weâll discuss the difference â¦ <a class="more-link" href="https://windowsontheory.org/2021/04/02/inference-and-statistical-physics/">Continue reading <span class="screen-reader-text">Inference and statisticalÂ physics</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>Scribe notes by Franklyn Wang</em></p>



<p><strong>Previous post:</strong> <a href="https://windowsontheory.org/2021/04/01/robustness-in-train-and-test-time/">Robustness in train and test time</a> <strong>Next post:</strong> <a href="https://windowsontheory.org/2021/04/03/natural-language-processing-guest-lecture-by-sasha-rush/">Natural Language Processing (guest lecture by Sasha Rush)</a>. See also <a href="https://windowsontheory.org/category/ml-theory-seminar/">all seminar posts</a> and <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">course webpage</a>.</p>



<p><a href="http://files.boazbarak.org/misc/mltheory/ML_seminar_lecture_5.pdf">lecture slides (pdf)</a> â <a href="http://files.boazbarak.org/misc/mltheory/ML_seminar_lecture_5.pptx">lecture slides (Powerpoint with animation and annotation)</a> â <a href="https://harvard.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=794333ec-0897-48ea-9bc5-ace600facb9f">video</a></p>



<h2>Digression: Frequentism vs Bayesianism</h2>



<p>Before getting started, weâll discuss the difference between the two dominant schools of thought in probability: Frequentism and Bayesianism.</p>



<p>Frequentism holds that the probability of an event is the long-run average proportion of something that happens many, many times, so a coin has 50% probability of being heads if on average half of the flips are heads. One consequence of this framework is that a one-off event like Biden winning the election doesnât have any probability as by definition they can only be observed once.</p>



<p>Bayesians reject this model of the world. For example, a famous Bayesian, Jaynes, even wrote that âprobabilities are frequencies only in an imaginary universe.â</p>



<p>While these branches of thought are different, generally the answers to most questions are the same, so the distinction will not matter for this class. However, these branches of thought inspire different types of methods, which we now discuss.</p>



<p>For example, suppose that we have a probability distribution <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> that we can get samples from in the form of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. In statistics, our goal is to calculate a hypothesis <img alt="h \in \mathcal{H}" class="latex" src="https://s0.wp.com/latex.php?latex=h+%5Cin+%5Cmathcal%7BH%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> which minimizes <img alt="\mathcal{L}_{D}(h)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D_%7BD%7D%28h%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (possibly a loss function, but any minimand will do) where we estimate <img alt="\mathcal{L}_D" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D_D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> with our samples <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/Wvujokc.png"/></figure>



<p>A <strong>frequentist</strong> does this by defining a family <img alt="\mathcal{D}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> of potential distributions, and we need to find a transformation <img alt="\vec{x} \mapsto h(\vec{x})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bx%7D+%5Cmapsto+h%28%5Cvec%7Bx%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> which minimizes the cost for all <img alt="D \in \mathcal{D}" class="latex" src="https://s0.wp.com/latex.php?latex=D+%5Cin+%5Cmathcal%7BD%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, where</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/CHDrLRW.png"/></figure>



<p>These equations amount to saying that <img alt="h" class="latex" src="https://s0.wp.com/latex.php?latex=h&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is minimizing the worst-case loss over all distributions.</p>



<p>By contrast, a <strong>Bayesian</strong> approaches this task by assuming that <img alt="D = D_{\vec{w}}" class="latex" src="https://s0.wp.com/latex.php?latex=D+%3D+D_%7B%5Cvec%7Bw%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> where <img alt="\vec{w} \sim P" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bw%7D+%5Csim+P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are latent variables sampled from a prior.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/GDhqJTT.png"/></figure>



<p>Then, let <img alt="Q_{\vec{x}}(\vec{w}) = P(w | D_{\vec{w}} = x)" class="latex" src="https://s0.wp.com/latex.php?latex=Q_%7B%5Cvec%7Bx%7D%7D%28%5Cvec%7Bw%7D%29+%3D+P%28w+%7C+D_%7B%5Cvec%7Bw%7D%7D+%3D+x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> be the <em>posterior</em> distribution of <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> conditioned on the observations <img alt="\vec{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bx%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. We now minimize</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/dNTPavw.png"/></figure>



<p>In fact, Bayesian approaches extend beyond this, as if you can sample from a posterior you can do more than just minimize a loss function.</p>



<p>In Bayesian inference, chosing the prior <img alt="P" class="latex" src="https://s0.wp.com/latex.php?latex=P&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is often very important. One classical approach is a <em>maximum entropy prior</em>, because itâs the least informative (hence, the most entropy) and requires the fewest assumptions.</p>



<p>These two minimization equations con sometimes lead to identical results, but often in practice they work out differently once we introduce <strong>computational constraints</strong> into the picture. In the frequentist approach, we generally constrain the family of mappings <img alt="\vec{x} \rightarrow \vec{h}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bx%7D+%5Crightarrow+%5Cvec%7Bh%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to be efficiently computable. In the Bayesian approach, we typically approximate the posterior instead and either use approximate sampling or a restricted hypothesis class for the posterior to be able to efficiently sample from it.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/KgOrLqR.png"/></figure>



<h2>Part 1. An introduction to Statistical Physics</h2>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/FOIcEVs.jpg"/></figure>



<p>Compare water and ice. Water is hotter, and the molecules move around more. In ice, by contrast, the molecules are more stationary. When the temperature increases, the objects move more quickly, and when they decrease the objects have less energy and stop moving. There are also phase transitions, where certain temperatures cause qualitative discontinuities in behavior, like solid to liquid or liquid to gas.<br/></p>



<p>See video from <a href="https://phet.colorado.edu/">here</a>:</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2021/04/ezgif.com-gif-maker-1.gif"><img alt="" class="wp-image-8064" src="https://windowsontheory.files.wordpress.com/2021/04/ezgif.com-gif-maker-1.gif?w=702"/></a></figure>



<p>An atomic state can be thought of as <img alt="x \in {0, 1}^{n}" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%7B0%2C+1%7D%5E%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Now, how do we represent the systemâs state? The crucial observation that makes statistical physics different from âvanilla physicsâ is the insight to represent the system state as a probability distribution <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> supported on <img alt="{0, 1}^{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%2C+1%7D%5E%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, rather than in a single state.</p>



<p>Each atomic state has a negative energy function (which we will think of as a utility function) <img alt="W: {0, 1}^{n} \rightarrow \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=W%3A+%7B0%2C+1%7D%5E%7Bn%7D+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. In some sense, the system âwantsâ to have a high value of <img alt="\mathbb{E}_{x \sim p}[ W(x)]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bx+%5Csim+p%7D%5B+W%28x%29%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. In addition, when the temperature is high, the system âwantsâ to have a high value of entropy.</p>



<p>Thus, an axiom of thermodynamics states that to find the true distribution, we need only look at the maximizer of</p>



<p><img alt="p = \arg\max_{q} \mathbb{E}_{x \sim q}[W(x)] + \tau \cdot H(q)." class="latex" src="https://s0.wp.com/latex.php?latex=p+%3D+%5Carg%5Cmax_%7Bq%7D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+q%7D%5BW%28x%29%5D+%2B+%5Ctau+%5Ccdot+H%28q%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>That is, it is the probability distribution which maximizes a linear combination of the expected negative energy and the entropy, with the temperature controlling the coefficient of entropy in this combination (the higher the temperature, the more the system prioritizes having high entropy).</p>



<p>The <strong>variational principle</strong>, which we prove later, states that the <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> which maximizes this satisfies</p>



<p><img alt="p(x) \propto \exp(\tau^{-1} \cdot W(x))" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29+%5Cpropto+%5Cexp%28%5Ctau%5E%7B-1%7D+%5Ccdot+W%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>which is known as the <strong>Boltzmann Distribution</strong>. We often write this with a normalizing constant, so <img alt="p(x) = \exp(\tau^{-1} \cdot W(x) - A_{\tau}(W))" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29+%3D+%5Cexp%28%5Ctau%5E%7B-1%7D+%5Ccdot+W%28x%29+-+A_%7B%5Ctau%7D%28W%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, where <img alt="A_{\tau}(W) = \log (\int \exp(\tau^{1} \cdot W(x)))" class="latex" src="https://s0.wp.com/latex.php?latex=A_%7B%5Ctau%7D%28W%29+%3D+%5Clog+%28%5Cint+%5Cexp%28%5Ctau%5E%7B1%7D+%5Ccdot+W%28x%29%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>Before proving the variational principle, we will go through some examples of statistical physics.</p>



<h3>Example: Ising Model</h3>



<p>In the Ising model, we have <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> magnets that are connected in a square grid. The atomic state is each <img alt="x \in {\pm 1}^{n}" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%7B%5Cpm+1%7D%5E%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, which represents âspinâ. The value of <img alt="W(x)" class="latex" src="https://s0.wp.com/latex.php?latex=W%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is <img alt="J\sum_{i \sim j} x_i x_j + J'\sum_{i} x_i" class="latex" src="https://s0.wp.com/latex.php?latex=J%5Csum_%7Bi+%5Csim+j%7D+x_i+x_j+%2B+J%27%5Csum_%7Bi%7D+x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, where <img alt="i \sim j" class="latex" src="https://s0.wp.com/latex.php?latex=i+%5Csim+j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> denotes that <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="j" class="latex" src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are adjacent magnets. This encourages adjacent magnets to have aligned spins. One important concept, which we will return to later, is that the values of <img alt="{x_i, x_ix_j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_i%2C+x_ix_j%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are sufficient to calculate the value of <img alt="W(x)" class="latex" src="https://s0.wp.com/latex.php?latex=W%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (these are known as <em>sufficient statistics</em>). Furthermore, if we wanted to calculate <img alt="\mathbb{E}[W(x)]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5BW%28x%29%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, it would be enough to calculate the values of <img alt="\mathbb{E}[x_i]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Bx_i%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="\mathbb{E}[x_i x_j]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Bx_i+x_j%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and then apply linearity of expectation. An illustration of an Ising model that we cool down slowly can be found here:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/VG1ION8.jpg"/></figure>



<p>and a video can be found <a href="https://www.youtube.com/watch?v=kjwKgpQ-l1s">here</a>.</p>



<p>This is what a low-temperature Ising model looks like â note that <img alt="W(x)" class="latex" src="https://s0.wp.com/latex.php?latex=W%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is high because almost all adjacent spins are aligned (hence the well-defined regions).</p>



<p>The <a href="https://arxiv.org/abs/1211.1094">Sherrington-Kirkpatrick model</a> is a generalization of the Ising model to a random graph, which represents a disordered mean field model. Here, <img alt="x \in {\pm 1}^{n}" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%7B%5Cpm+1%7D%5E%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> still represents spin, and <img alt="W(x) = \sum w_{i, j} x_i x_j" class="latex" src="https://s0.wp.com/latex.php?latex=W%28x%29+%3D+%5Csum+w_%7Bi%2C+j%7D+x_i+x_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> where <img alt="w_{i, j} \sim \mathcal{N}(0, 1)" class="latex" src="https://s0.wp.com/latex.php?latex=w_%7Bi%2C+j%7D+%5Csim+%5Cmathcal%7BN%7D%280%2C+1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. The SK-model is deeply influential in statistical physics. We say that it is <em>disordered</em> because the utility function <img alt="W" class="latex" src="https://s0.wp.com/latex.php?latex=W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is chosen at random and that it is a <em>mean field</em> model because, unlike the Ising model, there is no geometry in the sense that every pair of individual variables <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="x_j" class="latex" src="https://s0.wp.com/latex.php?latex=x_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are equally likely to be connected.</p>



<p>Our third example is the posterior distribution, where <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a hidden variable with a uniform prior. We now makes, say, <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> independent observations <img alt="O_1, O_2, \ldots O_k" class="latex" src="https://s0.wp.com/latex.php?latex=O_1%2C+O_2%2C+%5Cldots+O_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. The probability of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is given by</p>



<p><img alt="p(x) \propto p(x\text{ satisfies } O_1) \cdot p(x\text{ satisfies } O_2) \ldots p(x\text{ satisfies } O_k)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29+%5Cpropto+p%28x%5Ctext%7B+satisfies+%7D+O_1%29+%5Ccdot+p%28x%5Ctext%7B+satisfies+%7D+O_2%29+%5Cldots+p%28x%5Ctext%7B+satisfies+%7D+O_k%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>so <img alt="p(x) \propto \exp(\log \sum \mathbb{P}(x\text{ satisfies } O_i))" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29+%5Cpropto+%5Cexp%28%5Clog+%5Csum+%5Cmathbb%7BP%7D%28x%5Ctext%7B+satisfies+%7D+O_i%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Note that the RHS is easy to calculate, but in practice the normalizing factor (also known as the partition function) can be difficult to calculate and often represents a large barrier, in the sense that computing the partition function makes many of these questions far easier.</p>



<h3>Proof of the Variational Principle</h3>



<p>Now, we prove the variational principle, which states that if <img alt="p(x) = \exp(\tau^{-1} \cdot W(x) - A_\tau(W))" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29+%3D+%5Cexp%28%5Ctau%5E%7B-1%7D+%5Ccdot+W%28x%29+-+A_%5Ctau%28W%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, where <img alt="A_\tau(W) = \log \int \exp(\tau^{-1} \cdot W(x))" class="latex" src="https://s0.wp.com/latex.php?latex=A_%5Ctau%28W%29+%3D+%5Clog+%5Cint+%5Cexp%28%5Ctau%5E%7B-1%7D+%5Ccdot+W%28x%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the normalizing factor, then</p>



<p><img alt="p = \arg\max_{q} \mathbb{E}_{x \sim q} W(x) + \tau \cdot H(q)" class="latex" src="https://s0.wp.com/latex.php?latex=p+%3D+%5Carg%5Cmax_%7Bq%7D+%5Cmathbb%7BE%7D_%7Bx+%5Csim+q%7D+W%28x%29+%2B+%5Ctau+%5Ccdot+H%28q%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Before proving the variational principle, we make the following claim: if <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is defined as <img alt="p(x) = \exp(\tau^{-1} \cdot W(x) - A_\tau(W))" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29+%3D+%5Cexp%28%5Ctau%5E%7B-1%7D+%5Ccdot+W%28x%29+-+A_%5Ctau%28W%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> then</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/yACUUCV.png"/></figure>



<p><strong>Proof of claim:</strong><br/>Write<br/><img alt="H(p) = -\int \log p(x) p(x) dx = -\int (\tau^{-1} \cdot W(x) - A_{\tau}(W)) p(x) dx" class="latex" src="https://s0.wp.com/latex.php?latex=H%28p%29+%3D+-%5Cint+%5Clog+p%28x%29+p%28x%29+dx+%3D+-%5Cint+%28%5Ctau%5E%7B-1%7D+%5Ccdot+W%28x%29+-+A_%7B%5Ctau%7D%28W%29%29+p%28x%29+dx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/><br/>(where we plugged in the definition of <img alt="p(x)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> in <img alt="\log p(x)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+p%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>).</p>



<p>We can now rewrite this as</p>



<p><img alt="H(p) = -\tau^{-1} \cdot \mathbb{E}_{x \sim p} W(x) + \mathbb{E}_{x \sim p} A_{\tau}(W)" class="latex" src="https://s0.wp.com/latex.php?latex=H%28p%29+%3D+-%5Ctau%5E%7B-1%7D+%5Ccdot+%5Cmathbb%7BE%7D_%7Bx+%5Csim+p%7D+W%28x%29+%2B+%5Cmathbb%7BE%7D_%7Bx+%5Csim+p%7D+A_%7B%5Ctau%7D%28W%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>which means that</p>



<p><img alt="H(p) = -\tau^{-1} \cdot \mathbb{E}_{x \sim p} W(x) + A{\tau}(W)" class="latex" src="https://s0.wp.com/latex.php?latex=H%28p%29+%3D+-%5Ctau%5E%7B-1%7D+%5Ccdot+%5Cmathbb%7BE%7D_%7Bx+%5Csim+p%7D+W%28x%29+%2B+A%7B%5Ctau%7D%28W%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>using the fact that <img alt="A_\tau(W)" class="latex" src="https://s0.wp.com/latex.php?latex=A_%5Ctau%28W%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a constant (independent of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>). Multiplying by <img alt="\tau" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctau&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and rearranging, we obtain the claim. <img alt="\blacksquare" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cblacksquare&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p><strong>Proof of variational principle:</strong><br/>Given the claim, we can now prove the variational principle.<br/>Let <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> be any distribution. We write</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/KVij2G2.png"/></figure>



<p>where the first equation uses our likelihood expression. This implies that <img alt="\mathbb{E}_{x \sim q} W(x) + \tau \cdot H(q)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bx+%5Csim+q%7D+W%28x%29+%2B+%5Ctau+%5Ccdot+H%28q%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is maximized at <img alt="q = p" class="latex" src="https://s0.wp.com/latex.php?latex=q+%3D+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, as desired. <img alt="\blacksquare" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cblacksquare&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p><strong>Remarks:</strong></p>



<ul><li>When <img alt="\tau = 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctau+%3D+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we have <img alt="A(W) = \max_{q} H(q) + \mathbb{E}_{x \sim q} W(x)" class="latex" src="https://s0.wp.com/latex.php?latex=A%28W%29+%3D+%5Cmax_%7Bq%7D+H%28q%29+%2B+%5Cmathbb%7BE%7D_%7Bx+%5Csim+q%7D+W%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</li><li>In particular, for every value of <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we have that<br/><img alt="" src="https://i.imgur.com/F0NNlGQ.png"/></li></ul>



<p>which <a href="https://windowsontheory.org/2021/02/24/unsupervised-learning-and-generative-models/">weâve seen before</a>! Note that equality holds when <img alt="q = p" class="latex" src="https://s0.wp.com/latex.php?latex=q+%3D+p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, but to approximate <img alt="A" class="latex" src="https://s0.wp.com/latex.php?latex=A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> we can simply take more tractable values of <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>In many situations, we can compute <img alt="W(x)" class="latex" src="https://s0.wp.com/latex.php?latex=W%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, but canât compute <img alt="A_{\tau}(W)" class="latex" src="https://s0.wp.com/latex.php?latex=A_%7B%5Ctau%7D%28W%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, which stifles many applications. One upshot of this, though, is that we can calculate ratios of <img alt="p(x)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="p(x')" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%27%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, which is good enough for some applications, like Markov Chain Monte Carlo.</p>



<h3>Markov Chain Monte Carlo (MCMC)</h3>



<p>An important task in statistics is to sample from a distribution <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, for very complicated values of <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. MCMC does this by constructing a Markov Chain whose stationary distribution is <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. The most common instantiation of MCMC is Metropolis-Hastings, which we now describe. First, we assume that there exists an undirected graph on the states <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, so that <img alt="x \sim x'" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Csim+x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> iff <img alt="x' \sim x" class="latex" src="https://s0.wp.com/latex.php?latex=x%27+%5Csim+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and that for <img alt="x \sim x'" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Csim+x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the probabilities of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="x'" class="latex" src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are similar in the sense that <img alt="W(x)/W(x')" class="latex" src="https://s0.wp.com/latex.php?latex=W%28x%29%2FW%28x%27%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is neither too small nor too large. Then the Metropolis-Hastings algortihm is as follows.</p>



<p><strong>Metropolis-Hastings Algorithm</strong>:</p>



<ol><li>Draw <img alt="x_0" class="latex" src="https://s0.wp.com/latex.php?latex=x_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> at random.</li><li>For <img alt="i = 1, 2, \ldots" class="latex" src="https://s0.wp.com/latex.php?latex=i+%3D+1%2C+2%2C+%5Cldots&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, choose an arbitrary <img alt="x' \sim x" class="latex" src="https://s0.wp.com/latex.php?latex=x%27+%5Csim+x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, and let</li></ol>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/HQBwr77.png"/></figure>



<p>Then, <img alt="x_t" class="latex" src="https://s0.wp.com/latex.php?latex=x_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> eventually is distributed as <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>! To show that this samples from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we show that the stationary distribution is of this Markov chain is <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. In this case, this turns out to be easy, since we can check the <em>detailed balance conditions</em> <img alt="p(x' | x)p(x) = \min{(p(x), p(x'))} = p(x | x')p(x')" class="latex" src="https://s0.wp.com/latex.php?latex=p%28x%27+%7C+x%29p%28x%29+%3D+%5Cmin%7B%28p%28x%29%2C+p%28x%27%29%29%7D+%3D+p%28x+%7C+x%27%29p%28x%27%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> so <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the stationary distribution of the Markov Chain.</p>



<p>The stationary distribution is often unique, so weâve proven that this samples from <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> eventually. In MCMC algorithms, however, often an important question is how fast we converge to the stationary distribution. Often this is rather slow, which is especially dissappointing because if it were faster many very difficult problems could be solved much more easily, like generic optimization.<br/>Indeed, there are examples where convergence to the stationary distribution would take exponential time.</p>



<p><strong>Note:</strong> One way to make MCMC faster is to let <img alt="x'" class="latex" src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> be really close from <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, because the likelihood ratio will be closer to <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> will spend less time stuck at its original location. Thereâs a tradeoff, however. If <img alt="x'" class="latex" src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is too close to <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, then the chain will not <em>mix</em> as quickly, where mixing is the convergence of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to the stationary distribution, because generally to mix the value <img alt="x_t" class="latex" src="https://s0.wp.com/latex.php?latex=x_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> must get close to every point, and making each of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>âs steps smaller will make that more difficult.</p>



<h3>Applications of MCMC: Simulated Annealing</h3>



<p>One application of MCMC is in simulated annealing. Suppose that we have <img alt="W: {0, 1}^{n} \rightarrow \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=W%3A+%7B0%2C+1%7D%5E%7Bn%7D+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, and we want to find <img alt="x = \arg \max_{x} W(x)" class="latex" src="https://s0.wp.com/latex.php?latex=x+%3D+%5Carg+%5Cmax_%7Bx%7D+W%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. The most direct attempt at solving this problem is creating a Markov Chain that simply samples from <img alt="x \in \arg\max_{x} W(x)" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%5Carg%5Cmax_%7Bx%7D+W%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. However, this is impractical, at least directly. It is like we have a domino that is far away from us, and we canât knock it down. What do we do in this case? We put some dominoes in between us!</p>



<p>To this end, we now create a sequence of Markov chains supported on <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, whose stationary distribution <img alt="p_{\tau}(x) \sim W(x)^{1/\tau}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7B%5Ctau%7D%28x%29+%5Csim+W%28x%29%5E%7B1%2F%5Ctau%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, as <img alt="\tau" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctau&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> gets smaller and smaller. This corresponds to cooling a system from a high-temperature to a low-temperature. Essentially, we want to sample from <img alt="p_0(x)" class="latex" src="https://s0.wp.com/latex.php?latex=p_0%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>Simulated annealing lets us begin by sampling from <img alt="p_{\infty}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7B%5Cinfty%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, which is uniform on the support. Then we can slowly reduce <img alt="\tau" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctau&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> from <img alt="\infty" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. When cooling a system, it will be helpful to think of two stages. First, a stage in which the object cools down. Second, a settling stage in which the system settles into a more stable state. The settling stage is simulated via MCMC on <img alt="p_{\tau}(x)" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7B%5Ctau%7D%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, so the transition probability from <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to <img alt="x'" class="latex" src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is <img alt="\min(1, \text{exp}(\tau^{-1} \cdot (W(x') - W(x))))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmin%281%2C+%5Ctext%7Bexp%7D%28%5Ctau%5E%7B-1%7D+%5Ccdot+%28W%28x%27%29+-+W%28x%29%29%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p><strong>Simulated Annealing Algorithm</strong>:</p>



<ol><li><em>Cooling</em> Begin <img alt="\tau" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctau&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> at <img alt="\infty" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cinfty&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (or very large), and lower the value of <img alt="\tau" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctau&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to zero according to some schedule.<br/>1a. <em>Settling</em> Now, repeatedly move from <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to <img alt="x'" class="latex" src="https://s0.wp.com/latex.php?latex=x%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> with probability <img alt="\min(1, \text{exp}(\tau^{-1} \cdot (W(x') - W(x))))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmin%281%2C+%5Ctext%7Bexp%7D%28%5Ctau%5E%7B-1%7D+%5Ccdot+%28W%28x%27%29+-+W%28x%29%29%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</li></ol>



<p>Since simulated annealing is inspired by physical intuition, it turns out that its shortcomings can be interpreted physically too. Namely, when you cool something too quickly it becomes a glassy state instead of a ground state, which often causes simulated annealing to fail, and for this algorithm to get stuck in local minima. Note how this is fundamentally a failure mode with MCMC â it canât mix quickly enough.</p>



<p>See <a href="https://www.youtube.com/watch?v=iaq_Fpr4KZc">this video</a> for an illustration of simulated annealing.</p>



<figure class="wp-block-image size-large"><a href="https://windowsontheory.files.wordpress.com/2021/04/ezgif.com-gif-maker.gif"><img alt="" class="wp-image-8062" src="https://windowsontheory.files.wordpress.com/2021/04/ezgif.com-gif-maker.gif?w=818"/></a></figure>



<h2>Part 2: Bayesian Analysis</h2>



<p>Note that the posterior of <img alt="\vec{w}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cvec%7Bw%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> conditioned on <img alt="x_1, \ldots x_n" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C+%5Cldots+x_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is</p>



<p><img alt="p(\vec{w} | x_1, \ldots x_n) = \frac{p(\vec{w})}{p(\vec{x})} \cdot p(x_1 | \vec{w}) p(x_2 | \vec{w}) \ldots p(x_n | \vec{w}) \propto \exp\left(\sum X_i(\vec{w})\right)" class="latex" src="https://s0.wp.com/latex.php?latex=p%28%5Cvec%7Bw%7D+%7C+x_1%2C+%5Cldots+x_n%29+%3D+%5Cfrac%7Bp%28%5Cvec%7Bw%7D%29%7D%7Bp%28%5Cvec%7Bx%7D%29%7D+%5Ccdot+p%28x_1+%7C+%5Cvec%7Bw%7D%29+p%28x_2+%7C+%5Cvec%7Bw%7D%29+%5Cldots+p%28x_n+%7C+%5Cvec%7Bw%7D%29+%5Cpropto+%5Cexp%5Cleft%28%5Csum+X_i%28%5Cvec%7Bw%7D%29%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>We now have <img alt="p_{W}(x) = \exp(W(x) - A(W))" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7BW%7D%28x%29+%3D+%5Cexp%28W%28x%29+-+A%28W%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, an exponential distribution. Now suppose that <img alt="W(x) = \langle w, \hat{x} \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=W%28x%29+%3D+%5Clangle+w%2C+%5Chat%7Bx%7D+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, where <img alt="\hat{x} \in \mathbb{R}^{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7Bx%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bm%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are the sufficient statistics of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. For example, the energy function could follow that of a tree, so <img alt="W(x) = \sum_{(i, j) \in T} w_{i, j} x_i x_j" class="latex" src="https://s0.wp.com/latex.php?latex=W%28x%29+%3D+%5Csum_%7B%28i%2C+j%29+%5Cin+T%7D+w_%7Bi%2C+j%7D+x_i+x_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. If you want to find the expected value of $latex \mathbb{E}<em>{x \sim p</em>{W}}[W(x)]&amp;bg=ffffff$ its enough to know $latex \mu = \mathbb{E}<em>{x \sim p</em>{W} }[\hat{x}]&amp;bg=ffffff$. (By following a tree, we mean that the undirected graphical model of the distribution is that of a tree.)</p>



<h3>Sampling from a Tree-Structured Distribution</h3>



<p>Now how do we sample from a tree-structured distribution? The most direct way is by calculating the marginals. While marginalization is generally quite difficult, it is much more tractable on certain types of graphs. For a sense of what this entails, suppose that we have the following tree-structured statistical model:</p>



<p><img alt="" src="https://i.imgur.com/mnC1vkM.png"/>,<br/>whose joint PDF is <img alt="W(x) = \exp(\sum_{i, j \in E} -w_{i, j} (x_i - x_j)^2)" class="latex" src="https://s0.wp.com/latex.php?latex=W%28x%29+%3D+%5Cexp%28%5Csum_%7Bi%2C+j+%5Cin+E%7D+-w_%7Bi%2C+j%7D+%28x_i+-+x_j%29%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> where <img alt="x \in {0, 1}^{n}" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Cin+%7B0%2C+1%7D%5E%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.<br/>(The unusual notation is chosen so that the relationships between the marginals is clean. The log-density in question is a Laplacian Quadratic Form.)</p>



<p>Then one can show that if the marginal of <img alt="x_3" class="latex" src="https://s0.wp.com/latex.php?latex=x_3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is <img alt="\mu_3 = \mathbb{P}(x_3 = 1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu_3+%3D+%5Cmathbb%7BP%7D%28x_3+%3D+1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, then we have that</p>



<p><img alt="\mu_6 = \mu_3 \cdot \frac{e^{w_{3,6}}}{1 + e^{w_{3, 6}}} + (1 - \mu_3) \cdot \frac{1}{1 + e^{w_{3, 6}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu_6+%3D+%5Cmu_3+%5Ccdot+%5Cfrac%7Be%5E%7Bw_%7B3%2C6%7D%7D%7D%7B1+%2B+e%5E%7Bw_%7B3%2C+6%7D%7D%7D+%2B+%281+-+%5Cmu_3%29+%5Ccdot+%5Cfrac%7B1%7D%7B1+%2B+e%5E%7Bw_%7B3%2C+6%7D%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>This will give six linear equations in six unknowns, which we can solve. Once we can marginalize a variable, say, <img alt="x_6" class="latex" src="https://s0.wp.com/latex.php?latex=x_6&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we can then simply sample from the marginal, then sample from the conditional distribution on <img alt="x_6" class="latex" src="https://s0.wp.com/latex.php?latex=x_6&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to sample from the entire distribution. This method is known as belief propogation.</p>



<p>This algorithm (with some modifications) also works for general graphs, but what it represents on general graphs is not the exact marginal, but rather an extension of that graph to a tree.</p>



<h3>General Exponential Distributions</h3>



<p>Now letâs try to develop more theory for exponential distributions. Let the PDF of this distribution be <img alt="p_W(x) = \exp(\langle w, \hat{x} \rangle - A(w))" class="latex" src="https://s0.wp.com/latex.php?latex=p_W%28x%29+%3D+%5Cexp%28%5Clangle+w%2C+%5Chat%7Bx%7D+%5Crangle+-+A%28w%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. A few useful facts about these distributions that often appear in calculations: <img alt="\nabla A(w) = \mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla+A%28w%29+%3D+%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and $latex \nabla^2 (A(w)) = \text{Cov}<em>{p</em>{W}}(x) \succeq 0&amp;bg=ffffff$.</p>



<p>By the variational principle, we have that among all distributions with the same sufficient statistics as a given Boltzmann distribution, the true one maximizes the entropy, so</p>



<p><img alt="p_w = \arg\max_{\mathbb{E}_{q} \hat{x} = \mu} H(q)." class="latex" src="https://s0.wp.com/latex.php?latex=p_w+%3D+%5Carg%5Cmax_%7B%5Cmathbb%7BE%7D_%7Bq%7D+%5Chat%7Bx%7D+%3D+%5Cmu%7D+H%28q%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>An analagous idea gives us<br/><img alt="\mu = \arg \max_{H(\mu) \ge H(p_{w})} \langle w, \mu \rangle," class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu+%3D+%5Carg+%5Cmax_%7BH%28%5Cmu%29+%5Cge+H%28p_%7Bw%7D%29%7D+%5Clangle+w%2C+%5Cmu+%5Crangle%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>so <img alt="p_w" class="latex" src="https://s0.wp.com/latex.php?latex=p_w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the <em>maximum entropy</em> distribution consistent with observations <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>The important consequence of these two facts is that in principle, <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> determines <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and vice versa. (Up to fine print of using <em>minimal</em> sufficient statistics, which we ignore here.) We will refer to <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as the âcanonical parameter spaceâ and <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as the âmean representation spaceâ. Now note that the first equation gives us a way of going from <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, and the second equation gives us a way to go from <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, at least information-theoretically.</p>



<p>Now, how do we do this algorithmically? Using <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, if were able to sample from <img alt="p_w" class="latex" src="https://s0.wp.com/latex.php?latex=p_w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (which is generally possible if we can evaluate <img alt="A(w)" class="latex" src="https://s0.wp.com/latex.php?latex=A%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> then we can estimate the mean <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> through samples. We can also obtain <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> from <img alt="A(w)" class="latex" src="https://s0.wp.com/latex.php?latex=A%28w%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> by estimating <img alt="\nabla A" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla+A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/qFKGiMa.png"/></figure>



<p>On the other hand, if you have <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, to obtain <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> you first consider <img alt="A^{\ast}(\mu) = \sup_{w \in \Omega} {\langle \mu, w \rangle - A(w)}" class="latex" src="https://s0.wp.com/latex.php?latex=A%5E%7B%5Cast%7D%28%5Cmu%29+%3D+%5Csup_%7Bw+%5Cin+%5COmega%7D+%7B%5Clangle+%5Cmu%2C+w+%5Crangle+-+A%28w%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and then note that the desired value is <img alt="\arg\max_{w \in \Omega}{\langle \mu, w \rangle - A(w)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Carg%5Cmax_%7Bw+%5Cin+%5COmega%7D%7B%5Clangle+%5Cmu%2C+w+%5Crangle+-+A%28w%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, so solving this problem boils down to estimating <img alt="\nabla A^{\ast}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla+A%5E%7B%5Cast%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> efficiently, because setting it to zero will give us <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>Thus going from <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> requires estimating <img alt="\nabla A" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla+A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, whereas going from <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> requires estimating <img alt="\nabla A^{\ast}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla+A%5E%7B%5Cast%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>When <img alt="p" class="latex" src="https://s0.wp.com/latex.php?latex=p&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a <em>posterior distribution</em>, the observed data typically gives us the weights <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, and hence the inference problem becomes to use that to sample from the posterior.</p>



<h4>Examples of Exponential Distributions</h4>



<p>There are many examples of exponential distributions, which we now give.</p>



<ul><li>High-Dimensional Normals: <img alt="x\in \mathbb{R}^{d}" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Cin+%5Cmathbb%7BR%7D%5E%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, <img alt="W(x) = -(x - \mu)^{\top} \Sigma^{-1} (x-\mu)" class="latex" src="https://s0.wp.com/latex.php?latex=W%28x%29+%3D+-%28x+-+%5Cmu%29%5E%7B%5Ctop%7D+%5CSigma%5E%7B-1%7D+%28x-%5Cmu%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></li><li>Ising Model: <img alt="x\in {0, 1}^{d}" class="latex" src="https://s0.wp.com/latex.php?latex=x%5Cin+%7B0%2C+1%7D%5E%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, <img alt="W(x) = \sum w_i x_I + \sum_{i, j \in E} w_{i, j} x_i x_j" class="latex" src="https://s0.wp.com/latex.php?latex=W%28x%29+%3D+%5Csum+w_i+x_I+%2B+%5Csum_%7Bi%2C+j+%5Cin+E%7D+w_%7Bi%2C+j%7D+x_i+x_j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. A sufficient statistic for these is <img alt="\hat{x} = (x, xx^{\top})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7Bx%7D+%3D+%28x%2C+xx%5E%7B%5Ctop%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, a fact which we will invoke repeatedly.</li><li>Thereâs many many more, including Gaussian Markov Random Fields, Latend Dirchlet Allocation, and Mixtures of Gaussians.</li></ul>



<h4>Going from canonical parameters to mean parameters</h4>



<p>Now, we show how to go from <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> in a special case, namely in the mean-field approximation. The mean field approximation is the approximation of distributions by product distributions over <img alt="x_1, \ldots, x_d \in {0, 1}^{d}" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C+%5Cldots%2C+x_d+%5Cin+%7B0%2C+1%7D%5E%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, for which</p>



<p><img alt="\mathbb{P}(x_1, \ldots x_n) = \mathbb{P}(x_1)\mathbb{P}(x_2) \ldots \mathbb{P}(x_n)." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BP%7D%28x_1%2C+%5Cldots+x_n%29+%3D+%5Cmathbb%7BP%7D%28x_1%29%5Cmathbb%7BP%7D%28x_2%29+%5Cldots+%5Cmathbb%7BP%7D%28x_n%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Recall that the partition function can be computed as</p>



<p><img alt="A(w) = \max_{q \in \mathcal{Q}} \langle w, \mathbb{E}_{q} \hat{x} \rangle + H(q)." class="latex" src="https://s0.wp.com/latex.php?latex=A%28w%29+%3D+%5Cmax_%7Bq+%5Cin+%5Cmathcal%7BQ%7D%7D+%5Clangle+w%2C+%5Cmathbb%7BE%7D_%7Bq%7D+%5Chat%7Bx%7D+%5Crangle+%2B+H%28q%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="\mathcal{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the set of all probability distributions. If we instead write <img alt="\mathcal{Q}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BQ%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as the set of product distributions (parametrized by <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, or the probability that each variable is <img alt="1" class="latex" src="https://s0.wp.com/latex.php?latex=1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>), we get</p>



<p><img alt="A(w) \ge \max_{\mu \in \mathcal{Q}} \sum w_i \mu_i + \sum w_{i,j} \mu_i \mu_j + \sum H(\mu_i)," class="latex" src="https://s0.wp.com/latex.php?latex=A%28w%29+%5Cge+%5Cmax_%7B%5Cmu+%5Cin+%5Cmathcal%7BQ%7D%7D+%5Csum+w_i+%5Cmu_i+%2B+%5Csum+w_%7Bi%2Cj%7D+%5Cmu_i+%5Cmu_j+%2B+%5Csum+H%28%5Cmu_i%29%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="H(\mu_i) = -\mu_i \log \mu_i - (1 - \mu_i) \log{1 - \mu_i}" class="latex" src="https://s0.wp.com/latex.php?latex=H%28%5Cmu_i%29+%3D+-%5Cmu_i+%5Clog+%5Cmu_i+-+%281+-+%5Cmu_i%29+%5Clog%7B1+-+%5Cmu_i%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and it now suffices to maximize the right hand function over the set <img alt="K = { \mu\in [0,1]^d | \sum \mu_i = 1 }" class="latex" src="https://s0.wp.com/latex.php?latex=K+%3D+%7B+%5Cmu%5Cin+%5B0%2C1%5D%5Ed+%7C+%5Csum+%5Cmu_i+%3D+1+%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>We can generally maximize concave functions over a convex set such as <img alt="K" class="latex" src="https://s0.wp.com/latex.php?latex=K&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Unfortunately, this function is not concave. However, it is concave in every coordinate. This suggests the following algorithm: fix all but one variable and maximize over that variable, and repeat. This approach is known as Coordinate Ascent Variational Inference (CAVI), and its pseudocode is given below.</p>



<p><strong>CAVI Algorithm</strong></p>



<ol><li>Let <img alt="\mu = 1/2 \cdot (1, 1, \ldots 1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu+%3D+1%2F2+%5Ccdot+%281%2C+1%2C+%5Cldots+1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></li><li>Repatedly choose values of <img alt="j" class="latex" src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> in <img alt="[n]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bn%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (possibly at random, or in a loop.)<br/>2a. Update <img alt="\mu_{j} = \arg\max_{x_j} (f(\mu_{-j}, x_j))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu_%7Bj%7D+%3D+%5Carg%5Cmax_%7Bx_j%7D+%28f%28%5Cmu_%7B-j%7D%2C+x_j%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (where <img alt="\mu_{-j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu_%7B-j%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> represents the non-<img alt="j" class="latex" src="https://s0.wp.com/latex.php?latex=j&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> coordinates of <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>)</li></ol>



<h2>Part 3. Solution landscapes and the replica method</h2>



<p>This part is best explained visually. Suppose that we have a Boltzman distribution. In the infinite temperature limit the this is the uniform distribution, distributed over the entire domain. As you decrease the temperature, the supportâs size decreases. (<em>Note:</em> The gifs below are just cartoons. These pretend as if the probability distribution is always uniform over a subset. Also, in most cases of interest for learning, only higher order derivatives of entropy will be discontinuous.)<br/></p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/rPb67cG.gif"/></figure>



<p>Sometimes thereâs a discontinuity in entropy, and the entropy âsuddenly dropsâ in a discontinuity. Often the entropy function itself is continuous, but the derivatives are not continuous at that point â a higher order phase transition.</p>



<p>Sometimes the geometry of the solution space undergoes a phase transition as well, with it âshatteringâ to several distinct clusters.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/itUWxrB.gif"/></figure>



<h3>The replica method</h3>



<p><strong>Note:</strong> We will have an extended post on the replica method later on.</p>



<p>If you sampled <img alt="x_1, \ldots x_n" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C+%5Cldots+x_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> from <img alt="p_{w}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bw%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, where <img alt="p_w" class="latex" src="https://s0.wp.com/latex.php?latex=p_w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a high-dimensional distribution, youâd expect the distances to all be approximately the same. The overlap matrix, or </p>



<p><img alt="\begin{bmatrix} \langle x_1, x_1 \rangle &amp; \cdots &amp; \langle x_1, x_n \rangle \ \vdots &amp; \ddots &amp; \vdots \ \langle x_n, x_1 \rangle &amp; \cdots &amp; \langle x_n, x_n \rangle \end{bmatrix}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Bbmatrix%7D+%5Clangle+x_1%2C+x_1+%5Crangle+%26+%5Ccdots+%26+%5Clangle+x_1%2C+x_n+%5Crangle+%5C+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%5C+%5Clangle+x_n%2C+x_1+%5Crangle+%26+%5Ccdots+%26+%5Clangle+x_n%2C+x_n+%5Crangle+%5Cend%7Bbmatrix%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> </p>



<p>we approximate as </p>



<p><img alt="\begin{bmatrix} 1 &amp; \cdots &amp; q \ \vdots &amp; \ddots &amp; \vdots \ q&amp; \cdots &amp; 1 \end{bmatrix}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Bbmatrix%7D+1+%26+%5Ccdots+%26+q+%5C+%5Cvdots+%26+%5Cddots+%26+%5Cvdots+%5C+q%26+%5Ccdots+%26+1+%5Cend%7Bbmatrix%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> </p>



<p>where <img alt="q" class="latex" src="https://s0.wp.com/latex.php?latex=q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is some constant.</p>



<p>Suppose <img alt="w" class="latex" src="https://s0.wp.com/latex.php?latex=w&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> comes from a probability distribution <img alt="W" class="latex" src="https://s0.wp.com/latex.php?latex=W&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Then a very common problem is computing</p>



<p><img alt="\mathbb{E}_{w}[A(w)] = \mathbb{E}_{w}\left[ \log \int \exp(\langle w, \hat{x} \rangle)\right]," class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bw%7D%5BA%28w%29%5D+%3D+%5Cmathbb%7BE%7D_%7Bw%7D%5Cleft%5B+%5Clog+%5Cint+%5Cexp%28%5Clangle+w%2C+%5Chat%7Bx%7D+%5Crangle%29%5Cright%5D%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>which is the expected free energy. However, it turns out to be much easier to find <img alt="\log (\int E_{w} \exp(\langle w, \hat{x} \rangle))" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog+%28%5Cint+E_%7Bw%7D+%5Cexp%28%5Clangle+w%2C+%5Chat%7Bx%7D+%5Crangle%29%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. So hereâs what we do. We find</p>



<p><img alt="\mathbb{E}_{w} [A(w)] = \lim{n \rightarrow 0} \frac{\mathbb{E}_{w} [\text{exp}(n \cdot A(w))] - 1}{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bw%7D+%5BA%28w%29%5D+%3D+%5Clim%7Bn+%5Crightarrow+0%7D+%5Cfrac%7B%5Cmathbb%7BE%7D_%7Bw%7D+%5B%5Ctext%7Bexp%7D%28n+%5Ccdot+A%28w%29%29%5D+-+1%7D%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>This should already smell weird, because <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is going to zero, an unusal notational choice. We can now write this as<br/><img alt="\mathbb{E}{w} A(w) = \lim_{n \rightarrow 0} \frac{\mathbb{E}_{w}\left( \int{x} \exp(\langle w, \hat{x} \rangle)\right)^{n} - 1}{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%7Bw%7D+A%28w%29+%3D+%5Clim_%7Bn+%5Crightarrow+0%7D+%5Cfrac%7B%5Cmathbb%7BE%7D_%7Bw%7D%5Cleft%28+%5Cint%7Bx%7D+%5Cexp%28%5Clangle+w%2C+%5Chat%7Bx%7D+%5Crangle%29%5Cright%29%5E%7Bn%7D+-+1%7D%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>which we can write as</p>



<p><img alt="\lim_{n \rightarrow 0} \frac{\int_{x_1, \ldots x_n} \mathbb{E}_{w} \exp(\langle w, \hat{x}_1 + \cdots + \hat{x}_n \rangle) - 1}{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clim_%7Bn+%5Crightarrow+0%7D+%5Cfrac%7B%5Cint_%7Bx_1%2C+%5Cldots+x_n%7D+%5Cmathbb%7BE%7D_%7Bw%7D+%5Cexp%28%5Clangle+w%2C+%5Chat%7Bx%7D_1+%2B+%5Ccdots+%2B+%5Chat%7Bx%7D_n+%5Crangle%29+-+1%7D%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Now these <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>âs represent the replicas! Now, weâd hope <img alt="\psi(n) = \int_{x_1, \ldots x_n} \mathbb{E}_w \exp(\langle w, \hat{x}_1 + \hat{x}_2 + \ldots + \hat{x}_n \rangle)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cpsi%28n%29+%3D+%5Cint_%7Bx_1%2C+%5Cldots+x_n%7D+%5Cmathbb%7BE%7D_w+%5Cexp%28%5Clangle+w%2C+%5Chat%7Bx%7D_1+%2B+%5Chat%7Bx%7D_2+%2B+%5Cldots+%2B+%5Chat%7Bx%7D_n+%5Crangle%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is an analytic function, and we can now write this as <img alt="\frac{\psi(n) - 1}{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpsi%28n%29+-+1%7D%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> goes to zero.</p>



<p>Generally speaking, <img alt="\mathbb{E}_{w} [\exp(w, \hat{x}_1 + \cdots + \hat{x}_n)]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7Bw%7D+%5B%5Cexp%28w%2C+%5Chat%7Bx%7D_1+%2B+%5Ccdots+%2B+%5Chat%7Bx%7D_n%29%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> only depends on overlaps of <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, so we often guess the value of <img alt="Q" class="latex" src="https://s0.wp.com/latex.php?latex=Q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and calculate this expectation.</p>



<h4>Examples:</h4>



<p>Weâll now give an example of how this is useful. Consider a spiked matrix/tensor model, where we observe <img alt="Y" class="latex" src="https://s0.wp.com/latex.php?latex=Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> so that <img alt="Y = \lambda S + N" class="latex" src="https://s0.wp.com/latex.php?latex=Y+%3D+%5Clambda+S+%2B+N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, where <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the signal and <img alt="N" class="latex" src="https://s0.wp.com/latex.php?latex=N&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the noise. Thus here we have</p>



<p><img alt="p(S', N' | Y) \propto \exp(\beta \langle Y - \lambda S', N' \rangle^2) \cdot p(S')," class="latex" src="https://s0.wp.com/latex.php?latex=p%28S%27%2C+N%27+%7C+Y%29+%5Cpropto+%5Cexp%28%5Cbeta+%5Clangle+Y+-+%5Clambda+S%27%2C+N%27+%5Crangle%5E2%29+%5Ccdot+p%28S%27%29%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>and we want to analyze <img alt="\mathbb{E}_{S' \sim p(\cdot | Y)} \langle S, S' \rangle^2" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D_%7BS%27+%5Csim+p%28%5Ccdot+%7C+Y%29%7D+%5Clangle+S%2C+S%27+%5Crangle%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as a function of <img alt="\lambda" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clambda&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, which can be done by the <a href="https://arxiv.org/pdf/1612.07728.pdf">replica method</a> and exhibits a phase transition.<br/><img alt="" src="https://i.imgur.com/ec3tC8C.png"/></p>



<p>Other examples include Teacher-student models, where <img alt="X, Y = (X, f_S(X) + N)" class="latex" src="https://s0.wp.com/latex.php?latex=X%2C+Y+%3D+%28X%2C+f_S%28X%29+%2B+N%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Then, <a href="https://arxiv.org/abs/2102.08127">a recent work</a> calculates the training losses and classification errors when training on this dataset, which closely match empirical values.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/x9bdDv7.png"/></figure>



<h3>Symmetry breaking</h3>



<p>Lastly, weâll talk about replica symmetry breaking. Sometimes, discontinuities donât just make the support smaller, but actually break the support into many different parts. For example, at this transition the circle becomes the three green circles.<br/><img alt="" src="https://i.imgur.com/ufck69X.png"/></p>



<p>When this happens, we can no longer make our overlap matrix assumption, as there are now asymmetries between the points. This leads to the overlap matrix being striped, in the fashion one would anticipate.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/0Zua7m9.png"/></figure>



<p>Sometimes, the support actually breaks into infinitely many parts, in a phenomenon called full replica symmetry breakng.</p>



<p>Finally, some historical notes. This âplugging in zeroâ trick was introduced by Parisi approximately 30 years ago, receiving a standing ovation at the ICM. Since then, some of those conjectures have been rigorously formalized, but many havenât. It is still very impressive that Parisi was able to do it.</p></div>
    </content>
    <updated>2021-04-02T15:04:30Z</updated>
    <published>2021-04-02T15:04:30Z</published>
    <category term="ML Theory seminar"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-04-14T02:21:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4509</id>
    <link href="https://lucatrevisan.wordpress.com/2021/04/02/bocconi-hired-poorly-qualified-computer-scientist/" rel="alternate" type="text/html"/>
    <title>Bocconi Hired Poorly Qualified Computer Scientist</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Today I received an interesting email from our compliance office that is working on the accreditation of our PhD program in Statistics and Computer Science. One of the requisites for accreditation is to have a certain number of affiliated faculty. â¦ <a href="https://lucatrevisan.wordpress.com/2021/04/02/bocconi-hired-poorly-qualified-computer-scientist/">Continue reading <span class="meta-nav">â</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Today I received an interesting email from our compliance office that is working on the accreditation of our PhD program in Statistics and Computer Science.</p>



<p>One of the requisites for accreditation is to have a certain number of affiliated faculty. To count as an affiliated faculty, however, one must pass certain minimal thresholds of research productivity, the same that are necessary to be promoted to Associate Professor, as quantified according to Italyâs well intentioned but questionably run initiative to conduct research evaluations using quantifiable parameters.</p>



<p>(For context, every Italian professor maintains a list of publications in a site run by the ministry. Although the site is linked to various bibliographic databases, one has to input each publication manually into a local site at oneâs own university, then the ministry site fetches the data from the local site. The data in the ministry site is used for these research evaluations. At one point, a secretary and I spent long hours entering my publications from the past ten years, to apply for an Italian grant.)</p>



<p>Be that as it may, the compliance office noted that I did not qualify to be an affiliated faculty (or, for that matter, an Associate Professor) based on my 2016-2020 publication record. That would be seven papers in SoDA and two in FOCS: surely Italian Associate Professors are held to high standards! It turns out, however, that one of the criteria counts only journal publications.</p>



<p> Well, how about the paper in J. ACM and the two papers in SIAM J. on Computing published between 2016 and 2020? That would (barely) be enough, but one SICOMP paper has the same title of a SoDA paper (being, in fact, the same paper) and so the ministry site had rejected it. Luckily, the Bocconi administration was able to remove the SoDA paper from the ministry site, I added again the SICOMP version, and now I finally, if barely, qualify to be an Associate Professor and a PhD program affiliated faculty.</p>



<p>This sounds like the beginning of a long and unproductive relationship between me and the Italian system of research evaluation.</p>



<p>P.S. some colleagues at other Italian universities to whom I told this story argued that the Bocconi administration did not correctly apply the government rules, and that one should count conference proceedings indexed by Scopus; other colleagues said that indeed<a href="https://abilitazione.miur.it/public/documenti/2018/DM_Valori_Soglia_589_08082018.pdf"> the government decree n. 589 of August 8, 2018,</a> in article 2, comma 1, part a, only refers to journals. This of course only reinforces my impression that the whole set of evaluation criteria is a dumpster fire that is way too far gone.</p></div>
    </content>
    <updated>2021-04-02T14:09:09Z</updated>
    <published>2021-04-02T14:09:09Z</published>
    <category term="Italy"/>
    <category term="lies damn lies and ..."/>
    <category term="politics"/>
    <category term="things that are terrible"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2021-04-14T02:20:12Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2021-04-02-benign-hotstuff/</id>
    <link href="https://decentralizedthoughts.github.io/2021-04-02-benign-hotstuff/" rel="alternate" type="text/html"/>
    <title>Benign Hotstuff</title>
    <summary>In this post we describe a simple variant of Paxos (or Raft or any Lock-Commit) that is inspired by looking through the lens of HotStuff and Blockchain protocols. The most noticeable difference is that while Paxos and Raft aim to maintain a stable Primary/Leader (and change views infrequently), in Benign...</summary>
    <updated>2021-04-02T09:54:00Z</updated>
    <published>2021-04-02T09:54:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Decentralized Thoughts</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2021-04-13T22:53:54Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8044</id>
    <link href="https://windowsontheory.org/2021/04/01/robustness-in-train-and-test-time/" rel="alternate" type="text/html"/>
    <title>Robustness in train and test time</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Scribe notes by Praneeth Vepakomma Previous post: Unsupervised learning and generative models Next post: Inference and statistical physics. See also all seminar posts and course webpage. lecture slides (pdf) â lecture slides (Powerpoint with animation and annotation) â video In this blog post, we will focus on the topic of robustness â how well (or â¦ <a class="more-link" href="https://windowsontheory.org/2021/04/01/robustness-in-train-and-test-time/">Continue reading <span class="screen-reader-text">Robustness in train and testÂ time</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>Scribe notes by <a href="https://www.media.mit.edu/people/vepakom/overview/">Praneeth Vepakomma</a></em></p>



<p><strong>Previous post:</strong> <a href="https://windowsontheory.org/2021/02/24/unsupervised-learning-and-generative-models/">Unsupervised learning and generative models</a> <strong>Next post:</strong> <a href="https://windowsontheory.org/2021/04/02/inference-and-statistical-physics/">Inference and statistical physics</a>. See also <a href="https://windowsontheory.org/category/ml-theory-seminar/">all seminar posts</a> and <a href="https://boazbk.github.io/mltheoryseminar/cs229br.html#plan">course webpage</a>.</p>



<p><a href="http://files.boazbarak.org/misc/mltheory/ML_seminar_lecture4.pdf">lecture slides (pdf)</a> â <a href="http://files.boazbarak.org/misc/mltheory/ML_seminar_lecture4.pptx">lecture slides (Powerpoint with animation and annotation)</a> â <a href="https://harvard.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=1186415c-f0f1-445c-886f-acd701757cb7">video</a></p>



<p>In this blog post, we will focus on the topic of robustness â how well (or not so well) do machine learning algorithms perform when either their training or testing/deployment data differs from our expectations.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/jmDvjav.png"/></figure>



<p>We will cover the following areas:</p>



<ol><li>Math/stat refresher</li><li>Multiplicative weights algorithm</li><li>Robustness<ul><li>train-time<ul><li>robust statistics</li><li>robust mean estimation</li><li>data poisoning</li></ul></li><li>test-time<ul><li>distribution shifts</li><li>adversarial perturbations</li></ul></li></ul></li></ol>



<h2><img alt="&#x1F4DD;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f4dd.png" style="height: 1em;"/>Math/stat refresher</h2>



<h3>KL refresher</h3>



<p>The KL-divergence between the probability distributions is the expectation of the log of probability ratios:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/aWTh2Lc.png"/></figure>



<p>KL-divergence is always non-negative and can be decomposed as the difference between negative entropy and cross-entropy. The non-negativity property thereby implies that <img alt="\forall" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cforall&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> distributions <img alt="p,q" class="latex" src="https://s0.wp.com/latex.php?latex=p%2Cq&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, <img alt="H(p,q) \geq H(p)." class="latex" src="https://s0.wp.com/latex.php?latex=H%28p%2Cq%29+%5Cgeq+H%28p%29.&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<h3>Introduction to concentration</h3>



<p>Consider a Gaussian random variable <img alt="X \sim N(\mu, \sigma^2)" class="latex" src="https://s0.wp.com/latex.php?latex=X+%5Csim+N%28%5Cmu%2C+%5Csigma%5E2%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Concentration is the phenomenon that if you have i.i.d random variables <img alt="X_1,X_2, \ldots,X_n" class="latex" src="https://s0.wp.com/latex.php?latex=X_1%2CX_2%2C+%5Cldots%2CX_n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> with expectation <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, then the empirical average <img alt="\frac{\sum_i X_i}{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Csum_i+X_i%7D%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is distributed approximately like <img alt="N(\mu, \frac{1}{n})=\frac{1}{\sqrt{n}}N(\mu,1)" class="latex" src="https://s0.wp.com/latex.php?latex=N%28%5Cmu%2C+%5Cfrac%7B1%7D%7Bn%7D%29%3D%5Cfrac%7B1%7D%7B%5Csqrt%7Bn%7D%7DN%28%5Cmu%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/><br/>Therefore the standard deviation of the empricial average is smaller than the standard deviation of each of the <img alt="Y_i" class="latex" src="https://s0.wp.com/latex.php?latex=Y_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>âs. The central limit theorem ensure this asymptotically w.r.t <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, while non-asymptotic versions of this phenomenon can be seen via popular inequalities such as the Chernoff/Hoeffding/Bernstein styled inequalities. These roughly have the form <img alt="P[|\sum_i X_i - \mu.n| \geq \epsilon n] \approx \exp(-\epsilon^2 n) " class="latex" src="https://s0.wp.com/latex.php?latex=P%5B%7C%5Csum_i+X_i+-+%5Cmu.n%7C+%5Cgeq+%5Cepsilon+n%5D+%5Capprox+%5Cexp%28-%5Cepsilon%5E2+n%29+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<h3>Matrix notations</h3>



<h4>Norms</h4>



<p>We denote the spectral norm of a matrix <img alt="\mathbf{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> by <img alt="\left | \mathbf{A} \right| = \underset{| v| = 1}{\max}\left| \mathbf{A}v \right| = \lambda_{\max}(\mathbf{A})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft+%7C+%5Cmathbf%7BA%7D+%5Cright%7C+%3D+%5Cunderset%7B%7C+v%7C+%3D+1%7D%7B%5Cmax%7D%5Cleft%7C+%5Cmathbf%7BA%7Dv+%5Cright%7C+%3D+%5Clambda_%7B%5Cmax%7D%28%5Cmathbf%7BA%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and the Frobenius norm by <img alt="\left | \mathbf{A} \right|_F = \sqrt{\sum\mathbf{A}_{ij}^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft+%7C+%5Cmathbf%7BA%7D+%5Cright%7C_F+%3D+%5Csqrt%7B%5Csum%5Cmathbf%7BA%7D_%7Bij%7D%5E2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<h4>PSD Ordering</h4>



<p>We use the matrix ordering <img alt="\mathbf{A} \preceq \mathbf{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D+%5Cpreceq+%5Cmathbf%7BB%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> if <img alt="v^T\mathbf{A} v \leq v^T\mathbf{B} v" class="latex" src="https://s0.wp.com/latex.php?latex=v%5ET%5Cmathbf%7BA%7D+v+%5Cleq+v%5ET%5Cmathbf%7BB%7D+v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for all vectors <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. We similarly refer to <img alt="\mathbf{A} \in [a,b] \mathbf{I}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D+%5Cin+%5Ba%2Cb%5D+%5Cmathbf%7BI%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> if <img alt="a\mathbf{I} \preceq \mathbf{A} \preceq b\mathbf{I}" class="latex" src="https://s0.wp.com/latex.php?latex=a%5Cmathbf%7BI%7D+%5Cpreceq+%5Cmathbf%7BA%7D+%5Cpreceq+b%5Cmathbf%7BI%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<h4>Matrix/vector valued random variables</h4>



<p><strong>Vector valued normals:</strong> If <img alt="\mu \in \mathbb{R}^{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu+%5Cin+%5Cmathbb%7BR%7D%5E%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a vector, and <img alt="\mathbf{V} \in \mathbb{R}^{d \times d}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BV%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bd+%5Ctimes+d%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a psd covariance matrix, with <img alt="x \sim N(\mu, \mathbf{V})" class="latex" src="https://s0.wp.com/latex.php?latex=x+%5Csim+N%28%5Cmu%2C+%5Cmathbf%7BV%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> normal over <img alt="\mathbb{R}^{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> with</p>



<p><img alt="\mathbb{E}\left[x_{i}\right]=\mu_{i}, \;\; \mathbb{E}\left[\left(x_{i}-\mu_{i}\right)\left(x_{j}-\mu_{j}\right)\right]=\mathbf{V}_{i, j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Cleft%5Bx_%7Bi%7D%5Cright%5D%3D%5Cmu_%7Bi%7D%2C+%5C%3B%5C%3B+%5Cmathbb%7BE%7D%5Cleft%5B%5Cleft%28x_%7Bi%7D-%5Cmu_%7Bi%7D%5Cright%29%5Cleft%28x_%7Bj%7D-%5Cmu_%7Bj%7D%5Cright%29%5Cright%5D%3D%5Cmathbf%7BV%7D_%7Bi%2C+j%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>For every psd matrix V, there exists a Normal distribution where<br/><img alt="\mathbb{E}\left[\left(x_{i}-\mu_{i}\right)\left(x_{j}-\mu_{j}\right)\right]=\mathbf{V}_{i, j}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Cleft%5B%5Cleft%28x_%7Bi%7D-%5Cmu_%7Bi%7D%5Cright%29%5Cleft%28x_%7Bj%7D-%5Cmu_%7Bj%7D%5Cright%29%5Cright%5D%3D%5Cmathbf%7BV%7D_%7Bi%2C+j%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p><strong>Standard vector-valued normal:</strong> This refers to the case when <img alt="\mathrm{x} \sim N\left(0^{d}, I_{d}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7Bx%7D+%5Csim+N%5Cleft%280%5E%7Bd%7D%2C+I_%7Bd%7D%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (or <img alt="\left.\mathrm{x} \sim N(0, I)\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft.%5Cmathrm%7Bx%7D+%5Csim+N%280%2C+I%29%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and</p>



<p><img alt="\mathbb{E} x=\overrightarrow{0},\;\; \mathbb{E}\left[x x^{\top}\right]=I \quad\left(\begin{array}{l} \mathbb{E} x_{i}^{2}=1 \ \mathbb{E} x_{i} x_{j}=0 \text { for } i \neq j \end{array}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D+x%3D%5Coverrightarrow%7B0%7D%2C%5C%3B%5C%3B+%5Cmathbb%7BE%7D%5Cleft%5Bx+x%5E%7B%5Ctop%7D%5Cright%5D%3DI+%5Cquad%5Cleft%28%5Cbegin%7Barray%7D%7Bl%7D+%5Cmathbb%7BE%7D+x_%7Bi%7D%5E%7B2%7D%3D1+%5C+%5Cmathbb%7BE%7D+x_%7Bi%7D+x_%7Bj%7D%3D0+%5Ctext+%7B+for+%7D+i+%5Cneq+j+%5Cend%7Barray%7D%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<h3>Matrix concentration</h3>



<p>For scalar random variables, we saw that if <img alt="Y_{1}, \ldots, Y_{n}" class="latex" src="https://s0.wp.com/latex.php?latex=Y_%7B1%7D%2C+%5Cldots%2C+Y_%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are i.i.d over <img alt="\mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> bounded with expectation <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> then</p>



<p><img alt="\mathrm{Pr}\left[\left|\sum Y_{i}-\mu \cdot n\right| \geq \epsilon n\right] \approx \exp \left(-\epsilon^{2} n\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BPr%7D%5Cleft%5B%5Cleft%7C%5Csum+Y_%7Bi%7D-%5Cmu+%5Ccdot+n%5Cright%7C+%5Cgeq+%5Cepsilon+n%5Cright%5D+%5Capprox+%5Cexp+%5Cleft%28-%5Cepsilon%5E%7B2%7D+n%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>If the random variables were vectors instead, we can easily generalize this to <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> samples of <img alt="Y_i \in \mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=Y_i+%5Cin+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Generalizing to matrices is far more interesting, especially when the norm under consideration is the spectral norm instead of the Frobenius norm.</p>



<h4>Matrix Bernstein inequality:</h4>



<p>If <img alt="Y_{1}, \ldots, Y_{n}" class="latex" src="https://s0.wp.com/latex.php?latex=Y_%7B1%7D%2C+%5Cldots%2C+Y_%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> i.i.d symmetric matrices in <img alt="\mathbb{R}^{d \times d}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5E%7Bd+%5Ctimes+d%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> with <img alt="\mathbb{E} Y_{i}=\mu,\left|Y_{i}\right| \leq O(1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D+Y_%7Bi%7D%3D%5Cmu%2C%5Cleft%7CY_%7Bi%7D%5Cright%7C+%5Cleq+O%281%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (i.e bounded in spectral norm), then</p>



<p><img alt="\mathrm{Pr}\left[\left|\sum Y_{i}-\mu \cdot n\right| \geq \epsilon n\right] \approx d \cdot \exp \left(-\epsilon^{2} n\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BPr%7D%5Cleft%5B%5Cleft%7C%5Csum+Y_%7Bi%7D-%5Cmu+%5Ccdot+n%5Cright%7C+%5Cgeq+%5Cepsilon+n%5Cright%5D+%5Capprox+d+%5Ccdot+%5Cexp+%5Cleft%28-%5Cepsilon%5E%7B2%7D+n%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Note that <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a matrix in this case, instead. The norm under consideration is the spectral norm. Note that the difference in the inequality w.r.t the scalar case is the additional multiplicative factor of d or an additive factor of <img alt="\log(d)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog%28d%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> in the exponent as in the equations above. Please refer to Tropp, 2015 Chapter 6 for formally precise statements.</p>



<p>Another property that follow is the the expected norm of the difference follows<br/><img alt="\mathbb{E}\left|\sum Y_{i}-\mu\right| \leq O(\sqrt{n \log d})" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BE%7D%5Cleft%7C%5Csum+Y_%7Bi%7D-%5Cmu%5Cright%7C+%5Cleq+O%28%5Csqrt%7Bn+%5Clog+d%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>There are some long-standing conjectures and results on cases when one can get rid of this additional factor of <img alt="\log(d)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog%28d%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p><strong>Trivia on log factors:</strong> For example as a tangent (as well as some trivia!) on some results of this flavor, where a <img alt="\log" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> factor was replaced by a constant, includes Spencerâs paper from 1985 titled <a href="https://www.ams.org/journals/tran/1985-289-02/S0002-9947-1985-0784009-0/home.html">âSix standard deviations sufficeâ</a>, which showed that a constant of 6 would suffice in certain bounds where a naive result would instead give a <img alt="\log(n)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clog%28n%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> dependency. The <a href="https://en.wikipedia.org/wiki/Kadison%E2%80%93Singer_problem">Kadison-singer problem</a> (by Spielman-Srivastava) and the <a href="https://arxiv.org/abs/1809.04726">Paulsen problem</a> are two other examples of works in this flavor.</p>



<h4>Random matrices</h4>



<p>For a random <img alt="d \times d" class="latex" src="https://s0.wp.com/latex.php?latex=d+%5Ctimes+d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> symmetric matrix matrix <img alt="\mathbf{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> with <img alt="\mathbf{A}_{i,j} \sim N(0,1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BA%7D_%7Bi%2Cj%7D+%5Csim+N%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the spectrum of eigenvalues is distributed according to <strong>Wigner semi-circle law</strong> on a support between <img alt="[-2\sqrt{d},+2\sqrt{d}]" class="latex" src="https://s0.wp.com/latex.php?latex=%5B-2%5Csqrt%7Bd%7D%2C%2B2%5Csqrt%7Bd%7D%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as shown in the figure below. Note that most mass is observed close to <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/mRmGRUG.png"/></figure>



<h4>Marchenko-Pastur distribution (for random empirical covariance matrices)</h4>



<p>For <img alt="\mathbf{X = \frac{1}{n} AA^T, A} \in \mathbb{R}^{d\times n}, \mathbf{A}_{ij} \sim N(0,1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX+%3D+%5Cfrac%7B1%7D%7Bn%7D+AA%5ET%2C+A%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bd%5Ctimes+n%7D%2C+%5Cmathbf%7BA%7D_%7Bij%7D+%5Csim+N%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/><br/>we have <img alt="\mathbf{X}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf%7BX%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the empirical estimate for covariance of <img alt="x_1,\ldots,x_n \sim  N(0,\mathbf{I}_d) " class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2Cx_n+%5Csim++N%280%2C%5Cmathbf%7BI%7D_d%29+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/><br/>the eigenvalues are distributed according to the Marchenko-Pastur distribution. In the case when, <img alt="d &lt; n" class="latex" src="https://s0.wp.com/latex.php?latex=d+%3C+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> , the eigenvalues will be bounded away from zero. When <img alt="d \approx n" class="latex" src="https://s0.wp.com/latex.php?latex=d+%5Capprox+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> , then it has a lot more mass close to <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (way more than the semi-circle law). When <img alt="d &gt; n " class="latex" src="https://s0.wp.com/latex.php?latex=d+%3E+n+&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> , then there is a spike of <img alt="d-n" class="latex" src="https://s0.wp.com/latex.php?latex=d-n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> eigenvalues at <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and the rest are bounded away from <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p><strong>Connections to the stability of linear regression</strong><br/>In the regime, when <img alt="d \approx n" class="latex" src="https://s0.wp.com/latex.php?latex=d+%5Capprox+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, linear regression is most unstable, which is the case when most of the eigenvalues of the empirical covariance are close to <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. When <img alt="d&gt;n" class="latex" src="https://s0.wp.com/latex.php?latex=d%3En&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, although linear regression is over-parametrized, the solution is not exact, but the approximate solution is still pretty stable as in the case of <img alt="d &lt; n" class="latex" src="https://s0.wp.com/latex.php?latex=d+%3C+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. When <img alt="d\gg n" class="latex" src="https://s0.wp.com/latex.php?latex=d%5Cgg+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, the condition number is infinite, and therefore we use a pseudo-inverse as opposed to the usual inverse in computing the solution for linear regression. This ignores the subspace on which the matrix has <img alt="0" class="latex" src="https://s0.wp.com/latex.php?latex=0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> eigenvalues while the inverse is performed. The condition number of the subspace of the non-zero eigenvectors is finite.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/nXdo5vB.png"/></figure>



<h2>Digression: Multiplicative Weights Algorithm</h2>



<p><strong>Note:</strong> Its variants/connections include techniques/topics of, Follow The Regularized Leader / Regret Minimization / Mirror Descent. Elad Hazanâs <a href="https://arxiv.org/abs/1909.03550">lecture notes</a> on online convex optimization and <a href="https://theoryofcomputing.org/articles/v008a006/">Arora, Hazan, Kaleâs article</a> on multiplicative and matrix multiplicative weights are a great reading source for these topics.</p>



<p>The following is a general setup in online optimization and online learning.</p>



<p><strong>Setup:</strong> <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> possible actions <img alt="a_{1}, \ldots, a_{n}" class="latex" src="https://s0.wp.com/latex.php?latex=a_%7B1%7D%2C+%5Cldots%2C+a_%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>At time <img alt="t=1,2, \ldots, T" class="latex" src="https://s0.wp.com/latex.php?latex=t%3D1%2C2%2C+%5Cldots%2C+T&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we incur loss <img alt="L_{i, t}" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7Bi%2C+t%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for action <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> at time <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. After this action, we also learn the loss for actions we did not take. (This is referred to as the <em>experts</em> model in online learning, in contrast to the <em>bandit</em> model where we only learn the loss for the action taken; the experts model is an easier setup than the bandit model.)</p>



<p>The following is the overall approach for learning to optimize in this online setup.</p>



<p><strong>Overall Approach:</strong></p>



<ul><li>Initialize <img alt="p_{0}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7B0%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> distribution over action space <img alt="[n]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Bn%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. We then take a step based on this distribution and observe the incurred loss.</li><li>Then the distribution is updated as <img alt="p_{t+1}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bt%2B1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> by letting <img alt="p_{t+1}(i) \propto p_{t}(i) \exp \left(-\eta L_{i, t}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bt%2B1%7D%28i%29+%5Cpropto+p_%7Bt%7D%28i%29+%5Cexp+%5Cleft%28-%5Ceta+L_%7Bi%2C+t%7D%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. i.e, if an action gave a bad outcome in terms of loss, we downweight itâs probability for the next draw of action to be taken. <img alt="\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the penalty terms that governs the aggresiveness of the downweighting.</li></ul>



<p>The <em>hope</em> is that this approach converges to a âgood aggregation or strategyâ. We measure the quality of the strategy using <strong>regret</strong>.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/wkWExIB.png"/></figure>



<p>The difference between the cost we incur and the optimal action (or probability distribution over actions) in hindsight is known as the (average) <em>regret</em>.</p>



<p>Note that we compare the loss we incur with the best loss over a <em>fixed</em> (i.e., nonadaptive) probability distribution over the set of possible actions. It can be shown that the best such loss can always be achieved by a distribution that puts all the weight on a single action.</p>



<p>The following theorem bounds the regret.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/waOIq4F.png"/></figure>



<p>The âprior ignoranceâ term captures how good the initialization is. The âsensitivity per stepâ term captures how aggressive the exploration is (i.e this term governs the potential difference between loss incurred at <img alt="p_t" class="latex" src="https://s0.wp.com/latex.php?latex=p_t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> versus <img alt="p_{t-1}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bt-1%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>). As <img alt="\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> also occurs in the denominator of the prior ignorance term, it needs to be carefully chosen to balance the two terms properly.</p>



<p>The first inequality of this theorem is always true for any <img alt="p\ast, p_0" class="latex" src="https://s0.wp.com/latex.php?latex=p%5Cast%2C+p_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> whether <img alt="p\ast" class="latex" src="https://s0.wp.com/latex.php?latex=p%5Cast&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is optimal strategy or not. The second (right-most) inequality is true when optimal <img alt="p\ast" class="latex" src="https://s0.wp.com/latex.php?latex=p%5Cast&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is the delta function (which it will be in optimal strategy), <img alt="p_0" class="latex" src="https://s0.wp.com/latex.php?latex=p_0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is initialized to uniform distribution, and <img alt="\eta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ceta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is set to be <img alt="\sqrt{\frac{\log(n)}{t}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B%5Cfrac%7B%5Clog%28n%29%7D%7Bt%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Note that in this case, the divergence <img alt="\Delta_{KL}(p\ast | p_0) = \log n" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BKL%7D%28p%5Cast+%7C+p_0%29+%3D+%5Clog+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>To state the âoverall approachâ, more precisely, there needs to be a normalization <img alt="Z_{t}" class="latex" src="https://s0.wp.com/latex.php?latex=Z_%7Bt%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> at every step as below<br/><img alt="p_{t+1}(i)=p_{t}(i) \exp \left(-\eta L_{i, t}\right) / Z_{t}" class="latex" src="https://s0.wp.com/latex.php?latex=p_%7Bt%2B1%7D%28i%29%3Dp_%7Bt%7D%28i%29+%5Cexp+%5Cleft%28-%5Ceta+L_%7Bi%2C+t%7D%5Cright%29+%2F+Z_%7Bt%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/><br/>The above can be rearranged as follows upon taking log<br/><img alt="L_{t, i}=\frac{1}{\eta} \log \frac{p_{t}(i)}{p_{t+1}(i) \cdot Z_{i}}" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7Bt%2C+i%7D%3D%5Cfrac%7B1%7D%7B%5Ceta%7D+%5Clog+%5Cfrac%7Bp_%7Bt%7D%28i%29%7D%7Bp_%7Bt%2B1%7D%28i%29+%5Ccdot+Z_%7Bi%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>By substituting this in below</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/jtzs3cB.png"/></figure>



<p>we get the following expanded version of what the regret amounts to be:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/sGwn0Xq.png"/></figure>



<p>The last equation above is due to the telescoping sum where adjacent terms cancel out except the first and last terms.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/kU2Mmfr.png"/></figure>



<p>The last inequality here is because the cross-entropy is always at least as large as the entropy.</p>



<p>So now we have this so far</p>



<p>PF: Regret <img alt="\leq \frac{\Delta_{K L}\left(p^{\ast} | p_{0}\right)}{\eta \cdot T}+\frac{1}{\eta \cdot T} \sum_{t} \Delta_{K L}\left(p_{t}, p_{t+1}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleq+%5Cfrac%7B%5CDelta_%7BK+L%7D%5Cleft%28p%5E%7B%5Cast%7D+%7C+p_%7B0%7D%5Cright%29%7D%7B%5Ceta+%5Ccdot+T%7D%2B%5Cfrac%7B1%7D%7B%5Ceta+%5Ccdot+T%7D+%5Csum_%7Bt%7D+%5CDelta_%7BK+L%7D%5Cleft%28p_%7Bt%7D%2C+p_%7Bt%2B1%7D%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Now there is this property that if <img alt="p, q" class="latex" src="https://s0.wp.com/latex.php?latex=p%2C+q&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> s.t. <img alt="p(i) \propto q(i) \rho_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=p%28i%29+%5Cpropto+q%28i%29+%5Crho_%7Bi%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for <img alt="\rho_{i} \in[1-\eta, 1+\eta]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Crho_%7Bi%7D+%5Cin%5B1-%5Ceta%2C+1%2B%5Ceta%5D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> then <img alt="\Delta_{K L}(p | q) \leq O\left(\eta^{2}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta_%7BK+L%7D%28p+%7C+q%29+%5Cleq+O%5Cleft%28%5Ceta%5E%7B2%7D%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/><br/>Therefore upon substituting this, we prove the upper bound stated over the regret.</p>



<p>This subclaim that was used can be proved as follows</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/pgcdG5S.png"/></figure>



<h3>Generalization of multiplicative weights: Follow The Regularized Leader (FTRL)</h3>



<p>When the set K of actions is convex (set of probability distributions on discrete actions is convex),</p>



<p>At time <img alt="t+1" class="latex" src="https://s0.wp.com/latex.php?latex=t%2B1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, it makes a choice <img alt="x_{t+1} \in K" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7Bt%2B1%7D+%5Cin+K&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and learns cost function <img alt="L_{t+1}: K \rightarrow \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=L_%7Bt%2B1%7D%3A+K+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> (so the setup is again like the experts model unlike the bandit model).</p>



<p>Now the new action in FTRL is based on the following optimization which is a regularized (with <img alt="R(x)" class="latex" src="https://s0.wp.com/latex.php?latex=R%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>) loss:<br/>FTRL: <img alt="x_{t+1}=\arg \min_{x \in R}\left(R(x)+\sum_{i=1}^{t} L_{i}(x)\right)" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7Bt%2B1%7D%3D%5Carg+%5Cmin_%7Bx+%5Cin+R%7D%5Cleft%28R%28x%29%2B%5Csum_%7Bi%3D1%7D%5E%7Bt%7D+L_%7Bi%7D%28x%29%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>In this case, the regret bound is given by the following theorem.<br/><img alt="" src="https://i.imgur.com/lglCvZh.png"/></p>



<p><img alt="x^\ast" class="latex" src="https://s0.wp.com/latex.php?latex=x%5E%5Cast&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> refers to the optimal choice. This indeed has a similar flavor to the theorem we saw for multiplicative weights. To be precise when<br/><img alt="K={\text { set of all distributions on action space }[n]}, \text {and} \; R(x)=-\frac{1}{\eta} H(x)" class="latex" src="https://s0.wp.com/latex.php?latex=K%3D%7B%5Ctext+%7B+set+of+all+distributions+on+action+space+%7D%5Bn%5D%7D%2C+%5Ctext+%7Band%7D+%5C%3B+R%28x%29%3D-%5Cfrac%7B1%7D%7B%5Ceta%7D+H%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> we have that the multiplicate weights becomes FTRL. Similarly there is a view that connects multiplicative weights to mirror descent. (Nisheeth Vishnoi has <a href="https://nisheethvishnoi.wordpress.com/convex-optimization/">notes</a> on this.)</p>



<h2>Train-Time Robustness</h2>



<p>We now look at some robustness issues specific to the <em>training</em> phase in machine learning.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/7DlRj3k.png"/></figure>



<p>For example, during training, there could be adversarially poisoned examples in the training dataset that damage the trained modelâs performance.</p>



<p><strong>Setup:</strong> Suppose <img alt="1-\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=1-%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> samples are generated from a genuine data distribution and <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> samples are maliciously chosen from an arbitrary distribution. i.e, in formal notation: <img alt="x_{1}, \ldots, x_{(1-\epsilon) n} \sim X \subseteq \mathbb{R}^{d}, \;\; \text {and } x_{(1-\epsilon) n+1}, \ldots, x_{n}" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7B1%7D%2C+%5Cldots%2C+x_%7B%281-%5Cepsilon%29+n%7D+%5Csim+X+%5Csubseteq+%5Cmathbb%7BR%7D%5E%7Bd%7D%2C+%5C%3B%5C%3B+%5Ctext+%7Band+%7D+x_%7B%281-%5Cepsilon%29+n%2B1%7D%2C+%5Cldots%2C+x_%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are arbitrary. (While for convenience we assume here that the last <img alt="\epsilon n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> items are maliciously chosen, the learning algorithm does not get the data in order.)</p>



<p>Assume <img alt="\left|x_{i}\right|^{2} \approx 1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%7Cx_%7Bi%7D%5Cright%7C%5E%7B2%7D+%5Capprox+1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for <img alt="i&lt;(1-\epsilon) n" class="latex" src="https://s0.wp.com/latex.php?latex=i%3C%281-%5Cepsilon%29+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Let us start with the task of estimating the mean under this setup.</p>



<p><strong>Mean estimation:</strong> Estimate <img alt="\mu = \mathbb{E}X" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu+%3D+%5Cmathbb%7BE%7DX&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p><strong>Noiseless case:</strong> In the noiseless case, the best estimate for the mean under many measures is the <em>empirical mean</em> <img alt="\hat{\mu}=\frac{1}{n}\sum_i X_i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7B%5Cmu%7D%3D%5Cfrac%7B1%7D%7Bn%7D%5Csum_i+X_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Standard concentration results show that for <img alt="d=1" class="latex" src="https://s0.wp.com/latex.php?latex=d%3D1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> <img alt="|\hat{\mu}-\mu| \leq O(1 / \sqrt{n})" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Chat%7B%5Cmu%7D-%5Cmu%7C+%5Cleq+O%281+%2F+%5Csqrt%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and for a general <img alt="d" class="latex" src="https://s0.wp.com/latex.php?latex=d&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> that <img alt="|\hat{\mu}-\mu| \leq O(\sqrt{d / n})" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Chat%7B%5Cmu%7D-%5Cmu%7C+%5Cleq+O%28%5Csqrt%7Bd+%2F+n%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p><strong>Adversarial case:</strong> If there is even a single malicious sample (that can be arbitrarily large or small) then it is well known that the empirical mean can give an arbitrarily bad approximation of the true population mean. For <img alt="d=1" class="latex" src="https://s0.wp.com/latex.php?latex=d%3D1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> we can use the <em>empirical median</em> <img alt="\mu^{\ast}=\mathrm{sort}\left(x_{1}, \ldots, x_{n}\right)_{n / 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%7B%5Cast%7D%3D%5Cmathrm%7Bsort%7D%5Cleft%28x_%7B1%7D%2C+%5Cldots%2C+x_%7Bn%7D%5Cright%29_%7Bn+%2F+2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. This is guaranteed to lie in the <img alt="\left(\frac{1}{2}-\epsilon, \frac{1}{2}+\epsilon\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%28%5Cfrac%7B1%7D%7B2%7D-%5Cepsilon%2C+%5Cfrac%7B1%7D%7B2%7D%2B%5Cepsilon%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> quantile of real data. This is most optimal because of the property that <img alt="X \sim N(\mu, 1),\left|\mu-\mu^{\ast}\right| \leq O(\epsilon+1 / \sqrt{n})" class="latex" src="https://s0.wp.com/latex.php?latex=X+%5Csim+N%28%5Cmu%2C+1%29%2C%5Cleft%7C%5Cmu-%5Cmu%5E%7B%5Cast%7D%5Cright%7C+%5Cleq+O%28%5Cepsilon%2B1+%2F+%5Csqrt%7Bn%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>In a higher-dimension, this story instead goes as follows:</p>



<p><strong>Setup:</strong> <img alt="x_{1}, \ldots, x_{(1-\epsilon) n} \sim X \subseteq \mathbb{R}^{d}, \quad x_{(1-\epsilon) n+1}, \ldots, x_{n}" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7B1%7D%2C+%5Cldots%2C+x_%7B%281-%5Cepsilon%29+n%7D+%5Csim+X+%5Csubseteq+%5Cmathbb%7BR%7D%5E%7Bd%7D%2C+%5Cquad+x_%7B%281-%5Cepsilon%29+n%2B1%7D%2C+%5Cldots%2C+x_%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> arbitrary.</p>



<p><strong>Median of coordinates (a starter solution):</strong> When <img alt="(d \geq 1)" class="latex" src="https://s0.wp.com/latex.php?latex=%28d+%5Cgeq+1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we have <img alt="\mu_{i}^{\ast}=\mathrm{sort}\left(x_{1, i}, \ldots, x_{n, i}\right)_{n / 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu_%7Bi%7D%5E%7B%5Cast%7D%3D%5Cmathrm%7Bsort%7D%5Cleft%28x_%7B1%2C+i%7D%2C+%5Cldots%2C+x_%7Bn%2C+i%7D%5Cright%29_%7Bn+%2F+2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> per coordinate as an initial naive solution. (i.e a median of coordinates). In this case, say if we have <img alt="X \sim N(\mu, I)," class="latex" src="https://s0.wp.com/latex.php?latex=X+%5Csim+N%28%5Cmu%2C+I%29%2C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> then an adversary can perurb the data to make <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> fraction of the points be <img alt="(\mu_1+M,\mu_2+M,\ldots,\mu_d+M)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Cmu_1%2BM%2C%5Cmu_2%2BM%2C%5Cldots%2C%5Cmu_d%2BM%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> where <img alt="M" class="latex" src="https://s0.wp.com/latex.php?latex=M&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is some large number. This will shift in each coordinate <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> the distribution to have median <img alt="\mu_i + \Omega(\epsilon)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu_i+%2B+%5COmega%28%5Cepsilon%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> instead of median <img alt="\mu_i" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, and hence make <img alt="\mu^{\ast} \approx \mu + c\cdot (\epsilon, \ldots, \epsilon)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%7B%5Cast%7D+%5Capprox+%5Cmu+%2B+c%5Ccdot+%28%5Cepsilon%2C+%5Cldots%2C+%5Cepsilon%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> which implies that <img alt="\left|\mu^{\ast}-\mu\right| \approx c\cdot \epsilon \sqrt{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Cmu%5E%7B%5Cast%7D-%5Cmu%5Cright%7C+%5Capprox+c%5Ccdot+%5Cepsilon+%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for some constant <img alt="c&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=c%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p><strong>The obvious next question is: Can we do better?</strong> i.e, can we avoid paying this dimension-dependent <img alt="\sqrt{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> price?</p>



<p>Yes we can! We can use the âTukey Medianâ!.<br/><img alt="" src="https://i.imgur.com/pbKZ8U6.png"/></p>



<p><strong>What is a Tukey median?</strong></p>



<p>Informally via the picture below, if you have a Tukey median (red), no matter which direction you take from it and look at the half-space, it exactly partitions the data into about half the # of points.<br/><img alt="" src="https://i.imgur.com/NaGRWbB.png"/></p>



<p><strong>Formal definition of Tukey median</strong> is given as follows (fixing some parameters):</p>



<p>A Tukey median* of <img alt="x_{1}, \ldots, x_{n} \in \mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7B1%7D%2C+%5Cldots%2C+x_%7Bn%7D+%5Cin+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a vector <img alt="\mu^{\ast} \in \mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%7B%5Cast%7D+%5Cin+%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> s.t. for every nonzero <img alt="v \in \mathbb{R}^{d}" class="latex" src="https://s0.wp.com/latex.php?latex=v+%5Cin+%5Cmathbb%7BR%7D%5E%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/bBy6GZE.png"/></figure>



<p>A Tukey median need not always exist for a given data. However, we will show that if the data is generated at random according to a nice distribution, and then at most <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> fraction of it is perturbed, a Tukey median will exist with high probability. In fact, the true population mean will be such a median.</p>



<p><strong>Existence of Tukey median</strong></p>



<p><strong>THM:</strong> If <img alt="x_{1}, \ldots, x_{(1-\epsilon) n} \sim N(\mu, I)" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7B1%7D%2C+%5Cldots%2C+x_%7B%281-%5Cepsilon%29+n%7D+%5Csim+N%28%5Cmu%2C+I%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="\sqrt{d / n} \ll \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csqrt%7Bd+%2F+n%7D+%5Cll+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> then</p>



<ol><li>Tukey median <img alt="\mu^{\ast}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%7B%5Cast%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> exists and</li><li>For <em>every</em> Tukey median <img alt="\mu^{\ast}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%7B%5Cast%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> , <img alt="\left|\mu-\mu^{\ast}\right| \leq O(\epsilon)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Cmu-%5Cmu%5E%7B%5Cast%7D%5Cright%7C+%5Cleq+O%28%5Cepsilon%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</li></ol>



<p>Together 1. and 2. mean that if we search over all vectors and output the first Tukey median that we find, then (a) this process will terminate and (b) its output will be a good approximation for the population mean. In particular, we do not have to pay the extra <img alt="\sqrt{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> cost needed in the median of coordinates!</p>



<p>** Proof of 1 (existence):**<br/>The mean itself is the Tukey median in this case because, <img alt="\forall v \neq 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cforall+v+%5Cneq+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, if we define <img alt="Y_{1}=\mathrm{sign}\left(\left\langle x_{1}-\mu, v\right\rangle\right), \ldots, Y_{k}=\mathrm{sign}\left(\left\langle x_{k}-\mu, v\right\rangle\right)" class="latex" src="https://s0.wp.com/latex.php?latex=Y_%7B1%7D%3D%5Cmathrm%7Bsign%7D%5Cleft%28%5Cleft%5Clangle+x_%7B1%7D-%5Cmu%2C+v%5Cright%5Crangle%5Cright%29%2C+%5Cldots%2C+Y_%7Bk%7D%3D%5Cmathrm%7Bsign%7D%5Cleft%28%5Cleft%5Clangle+x_%7Bk%7D-%5Cmu%2C+v%5Cright%5Crangle%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> then these are i.i.d Â±1 vars of mean zero, and thus:</p>



<p><img alt="\mathrm{Pr}\left[\sum Y_{i}&gt;\epsilon k\right]&lt;\exp \left(-\epsilon^{2} n\right) \ll \exp (-d)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BPr%7D%5Cleft%5B%5Csum+Y_%7Bi%7D%3E%5Cepsilon+k%5Cright%5D%3C%5Cexp+%5Cleft%28-%5Cepsilon%5E%7B2%7D+n%5Cright%29+%5Cll+%5Cexp+%28-d%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for <img alt="\sqrt{d / n} \ll \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csqrt%7Bd+%2F+n%7D+%5Cll+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="k=(1-\epsilon)n" class="latex" src="https://s0.wp.com/latex.php?latex=k%3D%281-%5Cepsilon%29n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Through discretization, we can pretend (losing some constants) that there are only <img alt="2^{O(d)}" class="latex" src="https://s0.wp.com/latex.php?latex=2%5E%7BO%28d%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> unit vectors in <img alt="\mathbb{R}^d" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb%7BR%7D%5Ed&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Hence we can use the union bound to conclude that for <em>every</em> unit <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, if we restrict attention to the âgoodâ (i.e., unperturbed) vectors <img alt="x_1,\ldots, x_k" class="latex" src="https://s0.wp.com/latex.php?latex=x_1%2C%5Cldots%2C+x_k&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, then the fraction of them satisfying <img alt="\langle x_i -\mu , v \rangle &gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+x_i+-%5Cmu+%2C+v+%5Crangle+%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> will be in <img alt="1/2 \pm \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F2+%5Cpm+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Since the adversary can perturb at most <img alt="\epsilon n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon+n&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> vectors, the overall frection of <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>âs such that <img alt="\langle x_i -\mu , v \rangle &gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+x_i+-%5Cmu+%2C+v+%5Crangle+%3E0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> will be in <img alt="1/2 \pm 2\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F2+%5Cpm+2%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. QED(1)</p>



<p>** Proof of 2:**<br/>Let <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> be the population mean (i.e., the âgoodâ <img alt="x_i" class="latex" src="https://s0.wp.com/latex.php?latex=x_i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>âs are distributed according to <img alt="N(\mu,I)" class="latex" src="https://s0.wp.com/latex.php?latex=N%28%5Cmu%2CI%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>). Suppose for simplicity, and toward a contradiction, that <img alt="\left|\mu-\mu^{\ast}\right| = 10 \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%7C%5Cmu-%5Cmu%5E%7B%5Cast%7D%5Cright%7C+%3D+10+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>Let <img alt="v=\mu-\mu^{\ast}" class="latex" src="https://s0.wp.com/latex.php?latex=v%3D%5Cmu-%5Cmu%5E%7B%5Cast%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Then,<br/><img alt="\left\langle x_{i}-\mu^{\ast}, v /|v|\right\rangle=\left\langle x_{i}-\mu+v, v /|v|\right\rangle=\left\langle x_{i}-\mu, v /|v|\right\rangle +|v| = \left\langle x_{i}-\mu, v /|v|\right\rangle +10 \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%5Clangle+x_%7Bi%7D-%5Cmu%5E%7B%5Cast%7D%2C+v+%2F%7Cv%7C%5Cright%5Crangle%3D%5Cleft%5Clangle+x_%7Bi%7D-%5Cmu%2Bv%2C+v+%2F%7Cv%7C%5Cright%5Crangle%3D%5Cleft%5Clangle+x_%7Bi%7D-%5Cmu%2C+v+%2F%7Cv%7C%5Cright%5Crangle+%2B%7Cv%7C+%3D+%5Cleft%5Clangle+x_%7Bi%7D-%5Cmu%2C+v+%2F%7Cv%7C%5Cright%5Crangle+%2B10+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>Note that <img alt="\left\langle x_{i}-\mu, v /|v|\right\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%5Clangle+x_%7Bi%7D-%5Cmu%2C+v+%2F%7Cv%7C%5Cright%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is distributed as <img alt="N(0,1)" class="latex" src="https://s0.wp.com/latex.php?latex=N%280%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, and so we get that <img alt="\langle x_i - \mu^* , v/|v|\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+x_i+-+%5Cmu%5E%2A+%2C+v%2F%7Cv%7C%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is distributed as <img alt="N(10\epsilon,1)" class="latex" src="https://s0.wp.com/latex.php?latex=N%2810%5Cepsilon%2C1%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.<br/>Hence, if <img alt="Y_{i}=\mathrm{sign}\left(\left\langle x_{i}-\mu^{\ast}, v /|v|\right\rangle\right)" class="latex" src="https://s0.wp.com/latex.php?latex=Y_%7Bi%7D%3D%5Cmathrm%7Bsign%7D%5Cleft%28%5Cleft%5Clangle+x_%7Bi%7D-%5Cmu%5E%7B%5Cast%7D%2C+v+%2F%7Cv%7C%5Cright%5Crangle%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> then <img alt="\mathrm{Pr}\left[Y_{i}=-1\right]=\mathrm{Pr}[N(10\epsilon ,1) &lt; 0] \leq 1 / 2-5 \epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BPr%7D%5Cleft%5BY_%7Bi%7D%3D-1%5Cright%5D%3D%5Cmathrm%7BPr%7D%5BN%2810%5Cepsilon+%2C1%29+%3C+0%5D+%5Cleq+1+%2F+2-5+%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.<br/>This implies that via a similar concentration argument as above, for every <img alt="v" class="latex" src="https://s0.wp.com/latex.php?latex=v&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, there will be with high probability at most <img alt="1/2 - 4\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=1%2F2+-+4%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> fraction of <img alt="i" class="latex" src="https://s0.wp.com/latex.php?latex=i&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>âs such that <img alt="Y_{i}=-1" class="latex" src="https://s0.wp.com/latex.php?latex=Y_%7Bi%7D%3D-1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, contradicting our assumption that <img alt="\mu^\ast" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu%5E%5Cast&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> was a Tukey median. QED(2)</p>



<p>Exactly computing the Tukey median is NP-hard, but efficient algorithms for robust mean estimation of normals and other distributions exist as referred to in <a href="https://jerryzli.github.io/robust-ml-fall19.html">Jerry Liâs lecture notes</a>. In particular, we can use the following approach using <em>spectral signatures</em> and <em>filtering</em>.</p>



<p><strong>Spectral Signatures</strong> can efficiently <em>certify</em> that a given vector is in fact a robust mean estimator.<br/>Let <img alt="x_{1}, \ldots, x_{(1-\epsilon) n} \sim N(\mu, I)" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7B1%7D%2C+%5Cldots%2C+x_%7B%281-%5Cepsilon%29+n%7D+%5Csim+N%28%5Cmu%2C+I%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="x_{(1-\epsilon) n+1}, \ldots, x_{n}" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7B%281-%5Cepsilon%29+n%2B1%7D%2C+%5Cldots%2C+x_%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> arbitrary<br/>Let <img alt="\hat{\mu}=\frac{1}{n} \sum_{i=1}^{n} x_{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7B%5Cmu%7D%3D%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+x_%7Bi%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> be the empirical mean and <img alt="\widehat{\Sigma}=\frac{1}{n} \sum_{i-1}^{n}\left(x_{i}-\hat{\mu}\right)\left(x_{i}-\hat{\mu}\right)^{\top}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cwidehat%7B%5CSigma%7D%3D%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi-1%7D%5E%7Bn%7D%5Cleft%28x_%7Bi%7D-%5Chat%7B%5Cmu%7D%5Cright%29%5Cleft%28x_%7Bi%7D-%5Chat%7B%5Cmu%7D%5Cright%29%5E%7B%5Ctop%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> be the empirical co-variance. Then we can bound the error of <img alt="\hat{\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as follows:</p>



<p><strong>Claim:</strong> <img alt="|\hat{\mu}-\mu| \leq O\left(\sqrt{\frac{d}{n}}+\sqrt{\epsilon}|\hat{\Sigma}|\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Chat%7B%5Cmu%7D-%5Cmu%7C+%5Cleq+O%5Cleft%28%5Csqrt%7B%5Cfrac%7Bd%7D%7Bn%7D%7D%2B%5Csqrt%7B%5Cepsilon%7D%7C%5Chat%7B%5CSigma%7D%7C%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>In other words, if the spectral norm of the empirical covariance matrix is small, then the empirical mean is a good estimator for the population mean.</p>



<p><strong>Note<img alt="{ }^{\ast}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B+%7D%5E%7B%5Cast%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>:</strong> If all points are from <img alt="N(\mu, I)" class="latex" src="https://s0.wp.com/latex.php?latex=N%28%5Cmu%2C+I%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> then <img alt="|\hat{\Sigma}|=O(\sqrt{d / n})" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5Chat%7B%5CSigma%7D%7C%3DO%28%5Csqrt%7Bd+%2F+n%7D%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>The Proof for this claim is given below<br/><img alt="" src="https://i.imgur.com/1tl31kK.png"/></p>



<p><strong>Explanation:</strong> Here, we assume the <img alt="\mu=0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu%3D0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for simplicity without loss of generality. The norm can be split into additive terms on good points (green text above) and malicious points (red text above). The first term of this inequality follows from standard concentration. For the second (red) term, we can modify it by adding and subtracting <img alt="\hat{\mu}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Chat%7B%5Cmu%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. We can then apply the Cauchy-Schwartz (cs) inequality to prove it. Upon rearranging the terms and dividing by the norm of <img alt="\mu" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmu&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, we get the desired result.</p>



<p><strong>Filtering</strong> is an approach to turn the certificate into an algorithm to actually find the estimator. The idea is that the same claim holds for non-uniform reweighting of data points to estimate the empirical mean and covariance. Hence we can use a violation of the spectral norm condition (the existence of a large eigenvalue) to assign âblameâ to some data points and down-weigh their contributions until we reach a probability distribution over points that on the one hand is spread on roughly <img alt="1-O(\epsilon)" class="latex" src="https://s0.wp.com/latex.php?latex=1-O%28%5Cepsilon%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> fraction of the points and on the other hand leads to a weighted empirical covariance matrix with small spectral norm. The above motivates the following robust mean estimation algorithm in the online case as below:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/XtpiBrm.png"/></figure>



<p><strong>Explanation:</strong> We first compute the mean and covariance based on uniform weighting based, and the certificate is checked via the spectral norm of the covariance. If the quality isnât good enough, then the blame is given to the largest eigenvector of the covariance that contributes to the most error. The weighting is now improved from the uniform initialization via the multiplicative weights styled update as given in step 3. <a href="https://jerryzli.github.io/robust-ml-fall19.html">Jerry Li</a> and <a href="https://www.stat.berkeley.edu/~jsteinhardt/stat260/notes.pdf">Jacob Steinhardt</a> have wonderful lecture notes on these topics.<br/>The algorithm above is computationally efficient while the bounds are not that tight.</p>



<p><strong>SoS algorithms:</strong> Another approach is via sum of squares algorithms where the guarantees for statistical robustness are much tighter, but they are computationally not very efficient although they are polynomial time. A hybrid approach might as well give a balance of these based on the problem at hand to bridge this gap between computational efficiency vs. statistical efficiency.</p>



<p>A list of relevant references is given below.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/G2dFbrm.png"/></figure>



<h2>Test-time robustness</h2>



<p>We now cover robustness issues with distribution shift and adversarial data poisoning during the testing phase in machine learning.</p>



<h3>Warm-up with pictures</h3>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/nym9PHw.png"/></figure>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/WjF0kUi.png"/></figure>



<p>As shown in <a href="https://arxiv.org/abs/1706.03691">Steinhardt, Koh, Liang, 2017</a> the images below illustrate how poisoning samples can drastically alter the performance of a classifier from good to bad.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/8fOuCw8.png"/></figure>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/EjNcU3X.png"/></figure>



<p><a href="https://arxiv.org/abs/1804.00792">Shafahi , Huang, Najibi, Suciu, Studer, Dumitras, Goldstein, 2018</a> showed how poisoning images look perfectly fine to the human perception, while they flip the modelâs performance where a fish is considered to be a dog and vice versa as shown below.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/kVrVMSA.png"/></figure>



<p>Another problem with regards to test-time robustness is the issue of domain shift where the distribution of test data is different from the distribution of training data.</p>



<p><img alt="\underset{x,y \sim D}{\mathbb{E}} L(f(x),y)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderset%7Bx%2Cy+%5Csim+D%7D%7B%5Cmathbb%7BE%7D%7D+L%28f%28x%29%2Cy%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> vs. <img alt="\underset{x,y \sim D'}{\mathbb{E}} L(f(x),y)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cunderset%7Bx%2Cy+%5Csim+D%27%7D%7B%5Cmathbb%7BE%7D%7D+L%28f%28x%29%2Cy%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> . If L is bounded, then if Lipschitz constant is known then distances like earth moverâs distance, T.V distance can be used to bound this. But one needs to be quite careful or these bounds are too large or close to vacuous.</p>



<p>For example, images of dogs taken say in a forest vs. images of dogs taken on roads have huge distance in measures like the ones mentioned above, as many pixels (although not important pixels) across the images are very different and there could be a classifier that performs terribly across the test while it does good on train. But magically, there appears to be a linear relationship between the accuracy on <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> vs accuracy on <img alt="D'" class="latex" src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Typically one would expect that the line would be below the x=y (45 degree) line.</p>



<p>i.e say if it was trained on cifar 10 (<img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>) and tested on cifar10.2 (<img alt="D'" class="latex" src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>) then finding this line to be a bit lower than the <img alt="x=y" class="latex" src="https://s0.wp.com/latex.php?latex=x%3Dy&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> line is not surprising but this linear correlation is quite surprising. It is even more surprising if the datasets are different-say in the case when performance on photos vs illustrations or cartoons of these photos was considered.</p>



<p>What are the potential reasons (intuitively)?</p>



<p>i) Overfitting on cifar test,</p>



<p>ii) Fewer human annotators per image <a href="https://arxiv.org/abs/2005.09619">introduces skew</a> towards hardness of dataset,</p>



<p>iii) Running out of images by human annotators as they might end up choosing images that are easier to annotate.</p>



<p>If we achieve better accuracy on <img alt="D'" class="latex" src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> than we achieved on <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, it is a strong indicator of a drop in hardness. If the errors are in the other direction, then this reasoning isnât as clear.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/Ns4uH9i.png"/></figure>



<h3>A toy theoretical model</h3>



<p>Here is a toy model can demonstrate this surprising linear relationship between accuracies under domain shift.<br/>Let us do a thought experiment where things are linear. Let us assume there was a true vector cat direction in terms of the representation(feature) as shown in the cartoon below. Let there be some correlated idiosyncratic correlations. An example, idiosyncrasy is say due to cats tending to be photographed more in indoors than in outdoors.</p>



<p>Consider <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to be a point that needs to be labeled.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/TUKbBTy.png"/></figure>



<p>In some dataset <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> consider the probability that <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is labeled as a cat is proportional to the following exponential as:<br/><img alt="\mathrm{Pr}[x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BPr%7D%5Bx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> labeled cat <img alt="] \propto \exp (\beta\langle x, C A T+\alpha I\rangle)" class="latex" src="https://s0.wp.com/latex.php?latex=%5D+%5Cpropto+%5Cexp+%28%5Cbeta%5Clangle+x%2C+C+A+T%2B%5Calpha+I%5Crangle%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>where <img alt="\alpha" class="latex" src="https://s0.wp.com/latex.php?latex=%5Calpha&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to denote the theidiosyncratic correlation factor. This is the exponent of the dot product of the image to be labeled <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> with the CAT direction and the idiosyncratic direction. The same can be done for a dataset <img alt="D'" class="latex" src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as</p>



<p>Dataset <img alt="D^{\prime}: \mathrm{Pr}[x" class="latex" src="https://s0.wp.com/latex.php?latex=D%5E%7B%5Cprime%7D%3A+%5Cmathrm%7BPr%7D%5Bx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> labeled cat <img alt="] \propto \exp \left(\beta^{\prime}\left\langle x, C A T+\alpha^{\prime}I^{\prime}\right\rangle\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5D+%5Cpropto+%5Cexp+%5Cleft%28%5Cbeta%5E%7B%5Cprime%7D%5Cleft%5Clangle+x%2C+C+A+T%2B%5Calpha%5E%7B%5Cprime%7DI%5E%7B%5Cprime%7D%5Cright%5Crangle%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Intuitively <img alt="\beta'" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is like the signal to noise ratio. That is, if <img alt="\beta' &lt; \beta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta%27+%3C+%5Cbeta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> then <img alt="D'" class="latex" src="https://s0.wp.com/latex.php?latex=D%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a harder dataset to classify than <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>So in this toy model, the best accuracy that can be reached for any linear classifier is given by the following, where the softmax of the RHS is the classification probability.</p>



<p><img alt="A c c_{D}(C)=\beta\langle C, C A T\rangle+\beta \alpha\langle C, I\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=A+c+c_%7BD%7D%28C%29%3D%5Cbeta%5Clangle+C%2C+C+A+T%5Crangle%2B%5Cbeta+%5Calpha%5Clangle+C%2C+I%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/><br/><img alt="A c c_{D \prime}(C)=\beta^{\prime}\langle C, C A T\rangle+\beta^{\prime} \alpha^{\prime}\left\langle C, I^{\prime}\right\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=A+c+c_%7BD+%5Cprime%7D%28C%29%3D%5Cbeta%5E%7B%5Cprime%7D%5Clangle+C%2C+C+A+T%5Crangle%2B%5Cbeta%5E%7B%5Cprime%7D+%5Calpha%5E%7B%5Cprime%7D%5Cleft%5Clangle+C%2C+I%5E%7B%5Cprime%7D%5Cright%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/><br/>The first term of this accuracy is the universal and transferable part and the second term is the idiosyncratic part.</p>



<p>The following is an For the <img alt="A c c_{D \prime}(C)" class="latex" src="https://s0.wp.com/latex.php?latex=A+c+c_%7BD+%5Cprime%7D%28C%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> we assume that if <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is trained on <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> then we assume <img alt="\approx 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Capprox+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. This is because if <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is trained on <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, then idiosyncratic directions of <img alt="D,D'" class="latex" src="https://s0.wp.com/latex.php?latex=D%2CD%27&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> are orthogonal to each other. So the <img alt="\approx 0" class="latex" src="https://s0.wp.com/latex.php?latex=%5Capprox+0&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and the accuracy will be just this term of <img alt="\beta^{\prime}\langle C, C A T\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta%5E%7B%5Cprime%7D%5Clangle+C%2C+C+A+T%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>If the model is learnt by gradient descent then the gradient direction will always be proportional to <img alt="CAT + \alpha I + Noise" class="latex" src="https://s0.wp.com/latex.php?latex=CAT+%2B+%5Calpha+I+%2B+Noise&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as the gradient is of the form<br/><img alt="\nabla(\beta\langle C, C A T\rangle+\alpha \beta\langle C, I\rangle)=\beta \cdot C A T+\beta \alpha \cdot I" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla%28%5Cbeta%5Clangle+C%2C+C+A+T%5Crangle%2B%5Calpha+%5Cbeta%5Clangle+C%2C+I%5Crangle%29%3D%5Cbeta+%5Ccdot+C+A+T%2B%5Cbeta+%5Calpha+%5Ccdot+I&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/><br/>So, if <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is trained on <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, then <img alt="C \propto C A T+\alpha \cdot I+" class="latex" src="https://s0.wp.com/latex.php?latex=C+%5Cpropto+C+A+T%2B%5Calpha+%5Ccdot+I%2B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> Noise <img alt="=\frac{\gamma}{|C A T+\alpha \cdot I|^{2}}(C A T+\alpha \cdot I)+" class="latex" src="https://s0.wp.com/latex.php?latex=%3D%5Cfrac%7B%5Cgamma%7D%7B%7CC+A+T%2B%5Calpha+%5Ccdot+I%7C%5E%7B2%7D%7D%28C+A+T%2B%5Calpha+%5Ccdot+I%29%2B&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> Noise. Here, <img alt="\beta" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is given by <img alt="\frac{\gamma}{|C A T+\alpha \cdot I|^{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cgamma%7D%7B%7CC+A+T%2B%5Calpha+%5Ccdot+I%7C%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.<br/>Therefore the accuracies will be as follows</p>



<p><img alt="A c c_{D}(C)=\beta\langle C, C A T+\alpha \cdot I\rangle=\beta \gamma" class="latex" src="https://s0.wp.com/latex.php?latex=A+c+c_%7BD%7D%28C%29%3D%5Cbeta%5Clangle+C%2C+C+A+T%2B%5Calpha+%5Ccdot+I%5Crangle%3D%5Cbeta+%5Cgamma&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/><br/><img alt="A c c_{D^{\prime}}(C)=\beta^{\prime}\left\langle C, C A T+\alpha \cdot I^{\prime}\right\rangle=\beta^{\prime} \gamma \cdot \frac{|C A T|^{2}}{|C A T|^{2}+\alpha^{2}|I|^{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=A+c+c_%7BD%5E%7B%5Cprime%7D%7D%28C%29%3D%5Cbeta%5E%7B%5Cprime%7D%5Cleft%5Clangle+C%2C+C+A+T%2B%5Calpha+%5Ccdot+I%5E%7B%5Cprime%7D%5Cright%5Crangle%3D%5Cbeta%5E%7B%5Cprime%7D+%5Cgamma+%5Ccdot+%5Cfrac%7B%7CC+A+T%7C%5E%7B2%7D%7D%7B%7CC+A+T%7C%5E%7B2%7D%2B%5Calpha%5E%7B2%7D%7CI%7C%5E%7B2%7D%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Therefore, we see this form of a linear relationship:<br/><img alt="A c c_{D^{\prime}}(C)=\frac{\beta^{\prime}}{\beta\left(1+\theta^{2}\right)} \cdot \mathrm{Acc}_{D}(C)" class="latex" src="https://s0.wp.com/latex.php?latex=A+c+c_%7BD%5E%7B%5Cprime%7D%7D%28C%29%3D%5Cfrac%7B%5Cbeta%5E%7B%5Cprime%7D%7D%7B%5Cbeta%5Cleft%281%2B%5Ctheta%5E%7B2%7D%5Cright%29%7D+%5Ccdot+%5Cmathrm%7BAcc%7D_%7BD%7D%28C%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></p>



<p>Note that:</p>



<ul><li><img alt="\beta^{\prime} / \beta&lt;1" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbeta%5E%7B%5Cprime%7D+%2F+%5Cbeta%3C1&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> iff <img alt="D^{\prime}" class="latex" src="https://s0.wp.com/latex.php?latex=D%5E%7B%5Cprime%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> harder than <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></li><li><img alt="\theta^{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Ctheta%5E%7B2%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> grows with idiosyncratic component of <img alt="D" class="latex" src="https://s0.wp.com/latex.php?latex=D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/></li></ul>



<p>Although this is a toy theoretical model, it explains a linear relationship. However, finding a model that explains this linear relationship in real-life will be an interesting project to think of.</p>



<h3>Adversarial perturbations</h3>



<p>We now move to the last (yet another active) topic of our blog: adversarial perturbations. As an introductory example (taken from <a href="https://adversarial-ml-tutorial.org/">this tutorial</a>), the hog image <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> was originally classified as a hog with <img alt="\approx 99.6 \%" class="latex" src="https://s0.wp.com/latex.php?latex=%5Capprox+99.6+%5C%25&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> probability. A small amount of noise <img alt="\Delta" class="latex" src="https://s0.wp.com/latex.php?latex=%5CDelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> was added to get the <img alt="x+\Delta" class="latex" src="https://s0.wp.com/latex.php?latex=x%2B%5CDelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> image which to the human eye perceptually looks indistinguishable. That said the model now ends up misclassifying the noised hog to something that is not a hog.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/4jjdwwL.png"/></figure>



<p>Should we be surprised with the efficacy of such adversarial perturbations? Originally-certainly yes, but not as much now in hindsight!</p>



<p>In this example it turns out that the magnitude of each element satisfies <img alt="\left|\Delta_{i}\right| \approx \frac{1}{64}\left|x_{i}\right|" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%7C%5CDelta_%7Bi%7D%5Cright%7C+%5Capprox+%5Cfrac%7B1%7D%7B64%7D%5Cleft%7Cx_%7Bi%7D%5Cright%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and the 2-norm of this vector is roughly as <img alt="|\Delta| \approx \frac{1}{64}|x|" class="latex" src="https://s0.wp.com/latex.php?latex=%7C%5CDelta%7C+%5Capprox+%5Cfrac%7B1%7D%7B64%7D%7Cx%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. Note that the Resnet50 model outputs <img alt="r(x)" class="latex" src="https://s0.wp.com/latex.php?latex=r%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> at the penultimate layer has a dimension <img alt="d=2048" class="latex" src="https://s0.wp.com/latex.php?latex=d%3D2048&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. We scale <img alt="|x|" class="latex" src="https://s0.wp.com/latex.php?latex=%7Cx%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> such that <img alt="|r(x)| \approx \sqrt{d}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Cr%28x%29%7C+%5Capprox+%5Csqrt%7Bd%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. There is a Lipschitz constant <img alt="L" class="latex" src="https://s0.wp.com/latex.php?latex=L&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> w.r.t the preservation of the following norms: <img alt="|r(x+\Delta)-r(x)| \approx L|\Delta|" class="latex" src="https://s0.wp.com/latex.php?latex=%7Cr%28x%2B%5CDelta%29-r%28x%29%7C+%5Capprox+L%7C%5CDelta%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. The final classification decision is made by looking at <img alt="C \cdot H O G" class="latex" src="https://s0.wp.com/latex.php?latex=C+%5Ccdot+H+O+G&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> where <img alt="HOG" class="latex" src="https://s0.wp.com/latex.php?latex=HOG&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is a unit vector such that <img alt="H O G:" class="latex" src="https://s0.wp.com/latex.php?latex=H+O+G%3A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> unit vector s.t. <img alt="\mathrm{Pr}[x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BPr%7D%5Bx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is <img alt="h o g] \propto\langle H O G, r(x)\rangle" class="latex" src="https://s0.wp.com/latex.php?latex=h+o+g%5D+%5Cpropto%5Clangle+H+O+G%2C+r%28x%29%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>. We assume some randomness in the decision as being <img alt="\sqrt{1-c^{2} / d} \cdot N(0, I)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Csqrt%7B1-c%5E%7B2%7D+%2F+d%7D+%5Ccdot+N%280%2C+I%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for simplicity. So we have<br/><img alt="r(x)=C \cdot H O G+\sqrt{1-c^{2} / d} \cdot N(0, I)" class="latex" src="https://s0.wp.com/latex.php?latex=r%28x%29%3DC+%5Ccdot+H+O+G%2B%5Csqrt%7B1-c%5E%7B2%7D+%2F+d%7D+%5Ccdot+N%280%2C+I%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> where <img alt="C=\langle x, HOG \rangle" class="latex" src="https://s0.wp.com/latex.php?latex=C%3D%5Clangle+x%2C+HOG+%5Crangle&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.We now have the probability that <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is not a hog as</p>



<p><img alt="\mathrm{Pr}[x" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathrm%7BPr%7D%5Bx&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is not hog <img alt="]=\frac{\exp (-C)}{\exp (C)+\exp (-C)}" class="latex" src="https://s0.wp.com/latex.php?latex=%5D%3D%5Cfrac%7B%5Cexp+%28-C%29%7D%7B%5Cexp+%28C%29%2B%5Cexp+%28-C%29%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>As we know that the observed probability of <img alt="x" class="latex" src="https://s0.wp.com/latex.php?latex=x&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> being not a hog is 0.0996, we can calculate that <img alt="C \approx 3" class="latex" src="https://s0.wp.com/latex.php?latex=C+%5Capprox+3&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.<br/>Upon normalizing <img alt="|r(x)|^2" class="latex" src="https://s0.wp.com/latex.php?latex=%7Cr%28x%29%7C%5E2&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to be <img alt="d=2048" class="latex" src="https://s0.wp.com/latex.php?latex=d%3D2048&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, We can expect the following square of this dot product w.r.t the representation <img alt="r(x)" class="latex" src="https://s0.wp.com/latex.php?latex=r%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as:<img alt="\langle r(x), H O G\rangle^{2} \approx 3 \approx |r(x)|^2 / 700" class="latex" src="https://s0.wp.com/latex.php?latex=%5Clangle+r%28x%29%2C+H+O+G%5Crangle%5E%7B2%7D+%5Capprox+3+%5Capprox+%7Cr%28x%29%7C%5E2+%2F+700&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/><br/>Therefore the norm of the projection of <img alt="r(x)" class="latex" src="https://s0.wp.com/latex.php?latex=r%28x%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> to the HOG direction is given by <img alt="\left|r(x)_{H O G}\right| \approx \frac{1}{25}|r(x)|" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cleft%7Cr%28x%29_%7BH+O+G%7D%5Cright%7C+%5Capprox+%5Cfrac%7B1%7D%7B25%7D%7Cr%28x%29%7C&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>.</p>



<p>So say if the 2048 vector had one larger element which accounts for the HOG direction, although it still accounts for a small proportion of its total norm. Therefore it wouldnât need too much noise to flip one class to its wrong class label as shown in the cartoons below.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/TzWJX5X.png"/></figure>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/5j65j1P.png"/></figure>



<p>So if the Lipschitz constant is greater than around 2.5 or 3, then a fraction of 1/25 is enough to zero out the hog direction (as <img alt="L \gg \frac{64}{25} \approx 2.5" class="latex" src="https://s0.wp.com/latex.php?latex=L+%5Cgg+%5Cfrac%7B64%7D%7B25%7D+%5Capprox+2.5&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>).</p>



<h3>Robust loss</h3>



<p>What are some strategies for training neural networks that are robust to perturbations?</p>



<ul><li>A set of transformation that do not change the true nature of the data such as for e.g:<br/><img alt="t_{\Delta}(x)=x+\Delta" class="latex" src="https://s0.wp.com/latex.php?latex=t_%7B%5CDelta%7D%28x%29%3Dx%2B%5CDelta&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> where the set is</li></ul>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/kbq4bol.png"/></figure>



<p>i.e, they only perturb the image or data sample by atmost <img alt="\epsilon" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cepsilon&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> upon applying that transformation.<br/>Now, given a loss function <img alt="\mathcal{L}: Y \times Y \rightarrow \mathbb{R}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BL%7D%3A+Y+%5Ctimes+Y+%5Crightarrow+%5Cmathbb%7BR%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and a classifier <img alt="f: X \rightarrow Y" class="latex" src="https://s0.wp.com/latex.php?latex=f%3A+X+%5Crightarrow+Y&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, a <strong>robust loss</strong> function of <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> at point <img alt="(x, y)" class="latex" src="https://s0.wp.com/latex.php?latex=%28x%2C+y%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is defined as</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/9cghGEA.png"/></figure>



<h3>Robust training:</h3>



<p>Given <img alt="x_{1}, y_{1}, \ldots, x_{n}, y_{n}" class="latex" src="https://s0.wp.com/latex.php?latex=x_%7B1%7D%2C+y_%7B1%7D%2C+%5Cldots%2C+x_%7Bn%7D%2C+y_%7Bn%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and arobust loss function, a robust training would involve the minimization of this loss which gives us:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/JKMY5NL.png"/></figure>



<p>Now for the subgoal of finding <img alt="\nabla_{f} \max_{t \in \mathcal{T}} \mathcal{L}\left(f\left(t(x{i})\right), y_{i}\right)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cnabla_%7Bf%7D+%5Cmax_%7Bt+%5Cin+%5Cmathcal%7BT%7D%7D+%5Cmathcal%7BL%7D%5Cleft%28f%5Cleft%28t%28x%7Bi%7D%29%5Cright%29%2C+y_%7Bi%7D%5Cright%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for the sake of optimization, invoking <a href="https://en.wikipedia.org/wiki/Danskin%27s_theorem">Danskinâs Theorem</a> will greatly help. The theorem basically says that if <img alt="g(f, t)" class="latex" src="https://s0.wp.com/latex.php?latex=g%28f%2C+t%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is nice (diff, continuous) and if <img alt="\mathcal{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BT%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> is compact then we have that:</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/XXRYA2k.png"/></figure>



<p>i.e for any function <img alt="g(f,t)" class="latex" src="https://s0.wp.com/latex.php?latex=g%28f%2Ct%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, that depends on <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> and <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> as long as the function space <img alt="\mathcal{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal%7BT%7D&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> from which <img alt="t" class="latex" src="https://s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> needs to be chosen is nice (diff, continuous) and compact, then to find the gradient of the maximum of the function <img alt="g(f,t)" class="latex" src="https://s0.wp.com/latex.php?latex=g%28f%2Ct%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> then by the theorem one can find maximizer <img alt="t^*" class="latex" src="https://s0.wp.com/latex.php?latex=t%5E%2A&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> for any particular <img alt="f" class="latex" src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/>, then that can give the required gradient (after some fine print that is given in note below).<br/>Note: The paper below extends to the case when <img alt="t^{\ast}(f)" class="latex" src="https://s0.wp.com/latex.php?latex=t%5E%7B%5Cast%7D%28f%29&amp;bg=ffffff&amp;fg=666666&amp;s=0&amp;c=20201002"/> non unique though there is other fine print. See Appendix A of <a href="https://arxiv.org/abs/1706.06083">(Madry Makelov Schmidt Tsipras Vladu 2017)</a>.</p>



<p>On the empirical side, there seems to be a trade-off. If one wants to achieve adversarial robustness, it can be achieved by letting go of some model accuracy. See discussion <a href="https://distill.pub/2019/advex-bugs-discussion/">here</a> and the papers cited below.</p>



<figure class="wp-block-image"><img alt="" src="https://i.imgur.com/xFo1uDK.png"/></figure></div>
    </content>
    <updated>2021-04-01T23:14:59Z</updated>
    <published>2021-04-01T23:14:59Z</published>
    <category term="ML Theory seminar"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-04-14T02:20:59Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-2369988409049011731</id>
    <link href="https://blog.computationalcomplexity.org/feeds/2369988409049011731/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/04/want-to-buy-theorem.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/2369988409049011731" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/2369988409049011731" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/04/want-to-buy-theorem.html" rel="alternate" type="text/html"/>
    <title>Want to Buy a Theorem?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This is embarrassing to admit but after a few badly timed trades on GameStop options I find myself a bit tight on money. To raise some cash, I reluctantly decided to sell one of my prized possessions, one of my theorems.</p><p><b>For Sale</b>: Boolean formula satisfiability cannot be solved in both logarithmic space and quasilinear time. For a more formal and slightly more general statement and a proof, seeÂ <a href="https://doi.org/10.1006/jcss.1999.1671">this paper</a>.</p><p><b>Bidding starts</b> at 12 BTC (about $705,000).Â </p><p><b>The winning bid, upon verified payment, will receive:</b></p><p/><ul style="text-align: left;"><li>The ability to give the theorem the name of your choice such as your own name, your best friend's mother's name or "Teddy McTheoremface".</li><li>A non-fungible token (NFT) attesting ownership of the theorem and the name you have chosen for it.</li><li>Anyone citing this result will be required to note that you own it and use the name you chose above. You cannot, however, limit the use of the theorem or receive compensation for its use.Â </li><li>By virtue of owning this theorem you will a Fortnow number of zero. This immediately gives you anÂ ErdÅs number of 2. If you have previously written a paper with PaulÂ ErdÅs then both of us will now have an ErdÅs number of 1.</li></ul><div><b>Frequently Asked Questions</b></div><div><b><br/></b></div><div><b>Q: </b>Why this theorem?</div><div><b><br/></b></div><div><b>A: </b>The theorem is in one of my few solely authored papers and I can't afford to share the proceeds of the sale.Â </div><div><br/></div><div><b>Q: </b>Doesn't Ryan Williams and others have <a href="http://pages.cs.wisc.edu/~dieter/Papers/sat-lb-survey-fttcs.pdf">stronger theorems</a>?</div><div><br/></div><div><b>A: </b>The results are incomparable. Ryan gives bounds on a single algorithm with low time and space. My theorem allows different machines for the time and space bounds.</div><div><br/></div><div>Also, to the best of my knowledge, Ryan's theorem is not for sale.</div><div><br/></div><div><b>Q: </b>Doesn't the proof of the theorem rely on other people's theorems such as Nepomnjascii? Shouldn't he get some of the value from this sale?</div><div><br/></div><div><b>A: </b>I'm not selling the proof of the theorem, just the theorem itself.</div><div><br/></div><div><b>Q: </b>If I purchase this theorem will I get to write next year's April Fools post?</div><div><br/></div><div><b>A: </b>No.</div><p/></div>
    </content>
    <updated>2021-04-01T13:22:00Z</updated>
    <published>2021-04-01T13:22:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-04-14T02:01:29Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/049</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/049" rel="alternate" type="text/html"/>
    <title>TR21-049 |  Kolmogorov complexity and  nondeterminism versus determinism  for polynomial time computations | 

	Juraj Hromkovic</title>
    <summary>We call any consistent and sufficiently powerful formal theory that enables to algorithmically in polynomial time verify whether a text is a proof  \textbf{efficiently verifiable mathematics} (ev-mathematics). We study the question whether nondeterminism is more powerful than determinism for polynomial time computations in the framework of ev-mathematics. Our main results are as follows. \\
"$\P \subsetneq \NP$ or for any deterministic, polynomial time compression algorithm $A$ there exists a nondeterministic, polynomial time compression machine $M$ that reduces infinitely many binary strings logarithmically stronger than $A$." \\
"$\P \subsetneq \NP$ or f-time resource bounded Kolmogorov complexity of any binary string $x$ can be computed in deterministic polynomial time for each polynomial time constructible function $f$."</summary>
    <updated>2021-04-01T12:52:24Z</updated>
    <published>2021-04-01T12:52:24Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-04-14T02:20:28Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18471</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/04/01/computer-science-gets-noted/" rel="alternate" type="text/html"/>
    <title>Computer Science Gets Noted</title>
    <summary>All non-metallic UK currency to feature Computing and Data Science pioneers Cropped from BBC source Charles Babbage and Ada Lovelace will be reunited. Their work on designing and programming a computing device, a century before any machine was built, is being honored by their appearance on the reverse of the 5-pound note in a new [â¦]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>All non-metallic UK currency to feature Computing and Data Science pioneers</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/01/computer-science-gets-noted/babbagelovelacenote/" rel="attachment wp-att-18474"><img alt="" class="alignright wp-image-18474" height="134" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/BabbageLovelaceNote.jpg?resize=250%2C134&amp;ssl=1" width="250"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from BBC <a href="https://www.bbc.com/news/uk-34710261?ocid=socialflow_facebook&amp;">source</a></font></td>
</tr>
</tbody>
</table>
<p>
Charles Babbage and Ada Lovelace will be reunited. Their work on designing and programming a computing device, a century before any machine was built, is being honored by their appearance on the reverse of the 5-pound note in a new series authorized by the Bank of England. Royal Statistical Society Fellow Florence Nightingale will appear on the 10, EDSAC creator Maurice Wilkes on the 20, and Alan Turing on the 50. </p>
<p>
Today, April 1, we are delighted to convey this news and show previews of the banknotes.<br/>
<span id="more-18471"/></p>
<p>
We have mentioned Babbage and Lovelace several times on this blog, the latter first <a href="https://rjlipton.wpcomstaging.com/2010/03/23/its-ada-lovelace-day/">here</a>. Our 2015 <a href="https://rjlipton.wpcomstaging.com/2015/02/17/ada-the-amplifier/">post</a> on her work on his project sought to engage a scholarly consensus that tends to minimize it. We argue that it should be judged on the plane of a doctoral or masters advisory relationship. Our point is that she greatly <em>amplified</em> the technical content of his work as shown to the scientific community. </p>
<p>
The design shown above makes a leap from their Victorian world to our online age. Being âonlineâ back then probably meant being reached by the new railroad system centered on London. The one indelible connection between our world and theirs is shown by the obverse of the banknote: both worlds have a long-serving British queen.</p>
<p/><h2> Nightingale On The 10 </h2><p/>
<p>
Besides a long life (1820â1910), Nightingale has a long entry as a <a href="https://en.wikipedia.org/wiki/Founders_of_statistics">founder</a> of statistics. Her actions and influence extended for more than five decades after her iconic âLady With the Lampâ ministry to soldiers of the Crimean War depicted on the 10-pound note:</p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/01/computer-science-gets-noted/nightingalespecimen2021/" rel="attachment wp-att-18475"><img alt="" class="aligncenter wp-image-18475" height="252" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/NightingaleSpecimen2021.jpg?resize=450%2C252&amp;ssl=1" width="450"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Nightingale 200 Years <a href="https://www.florence-nightingale.co.uk/10-bank-note-featuring-florence-nightingale-1975-1992/">source</a></font>
</td>
</tr>
</tbody></table>
<p>
The picture at right is based on a <a href="https://www.amdigital.co.uk/about/blog/item/nightingale">photo</a> by William Kilburn, one of few series of photos she allowed to be taken. We have not found a definitive word on what she is holdingânot even from this long <a href="https://theclassicphotomag.com/florence-nightingale-the-mysteries-behind-her-iconic-photographs/">article</a>âbut we would like to believe it is a letter or paper regarding medical practice. Already in 1854, the <em>Oxford Chronicle and Reading Gazette</em> noted her bringing much more than a lamp:</p>
<blockquote><p><b> </b> <em> In a knowledge of the ancient languages and of the higher branches of mathematics, in general art, science, and literature, her attainments are extraordinary. There is scarcely a modern language which she does not understand; and she speaks French, German, and Italian as fluently as her native English. </em>
</p></blockquote>
<p>
Not only did she popularize the pie chart and invent other statistical visualization techniques, she was among the first people, period, to bring statistical inference into policy. This included demonstrating with data the higher exposure of nurses to multiple diseases among those they tended. She was inducted into the Royal Statistical Society in <b>1858</b>. </p>
<p>
Her medical conduct principles have been applied expressly during the pandemic, as <a href="https://nursing.vanderbilt.edu/news/lessons-from-florence-nightingale-are-primary-tools-in-covid-19-nursing/">recognized</a> by the time of the 200th anniversary of her birth last May 12, and field hospitals in the UK for Covid-19 treatment are <a href="https://en.wikipedia.org/wiki/COVID-19_hospitals_in_the_United_Kingdom">named</a> for her. Her recognition on the currency was considered a way to honor first-line caregivers and was a unanimous vote of the Bank of England commission. It has the earliest release <a href="https://en.numista.com/catalogue/note203311.html">date</a> of all the banknotes.</p>
<p/><h2> Wilkes On The 20 </h2><p/>
<p>
Maurice Wilkes had an even longer life, 1913 to 2010. This marks a stark contrast to Turing. Wilkes was the second winner, in 1967, of the Turing Award. The Turing Award <a href="https://amturing.acm.org/award_winners/wilkes_1001395.cfm">citation</a> said he was</p>
<blockquote><p><b> </b> <em> âthe builder and designer of the EDSAC, the first computer with an internally stored program.â </em>
</p></blockquote>
<p>
He was thus the first realizer of the full vision of Babbage and Lovelace, although many claim the stored-program distinction for the Manchester <a href="https://en.wikipedia.org/wiki/Manchester_Baby">Baby</a> one year earlier, in 1948.</p>
<p>
There was controversy about the order of him and Turing. The banknotes were intended to go in chronological order. Turingâs famous paper conceiving the computer dates to 1936â37. Wilkesâs first brush with the project that became the <a href="https://en.wikipedia.org/wiki/EDSAC">EDSAC</a> came in 1945, when he spent an iconic 24 hours with a loaned copy of John von Neumannâs <a href="https://en.wikipedia.org/wiki/First_Draft_of_a_Report_on_the_EDVAC">First EDVAC Report</a> that he was not able to mimeograph. However, Wilkes was involved with analog computing devices before 1937 and his EDSAC reached full operation in May 1949, whereas Turingâs own main project, the <a href="https://en.wikipedia.org/wiki/Automatic_Computing_Engine">ACE</a>, launched its <a href="https://en.wikipedia.org/wiki/Pilot_ACE">pilot</a> version only in 1950. </p>
<p>
The ultimate determiner was that the 20-pound note was already recently revised to feature the artist Joseph Turner in February 2020. The Turner issue will have a five-year term, so Wilkes will appear in 2025. The banknote design has not yet been executed.</p>
<p/><h2> Turing On The 50 </h2><p/>
<p>
Of course, this is the highlight for us, and we are delighted to be able to show the Turing design in full glory. It was formally <a href="https://www.bankofengland.co.uk/news/2021/march/the-new-50-note-unveiled">unveiled</a> last week:</p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/01/computer-science-gets-noted/turingbanknote/" rel="attachment wp-att-18476"><img alt="" class="aligncenter wp-image-18476" height="295" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/TuringBanknote.jpg?resize=550%2C295&amp;ssl=1" width="550"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Belfast Telegraph <a href="https://www.belfasttelegraph.co.uk/business/uk-world/birth-date-in-binary-code-among-features-of-new-alan-turing-banknote-40238025.html">source</a></font>
</td>
</tr>
</tbody></table>
<p>
The Pilot ACE machine is shown overlaid with Turing machine code from his 1936 paper. Turing tapes with binary code appear in three places. The big one represents his June 23, 1912 birth dateâexactly how, we leave as a puzzle.  Weâll just hint that all our friends across the Pond write dates differently from how we do. The new banknotes are being released on Turingâs birthday this coming June.</p>
<p>
The <a href="https://en.wikipedia.org/wiki/GCHQ">GCHQ</a>, which grew out of Turingâs employers at Bletchley Park, has just released its own puzzle <a href="https://www.gchq.gov.uk/information/turing-challenge">challenge</a>, which references Easter-egg features of the Turing banknote. The binary code puzzle is 11th of 12, but to our surprise, when you click it, it gives the answer to the birth date part right away.  Then it proceeds to something far more crypticâwell, itâs from GCHQ after all. </p>
<p/><h2> Other Notes </h2><p/>
<p>
The Turing release will also complete the conversion from paper to polymer of all Bank of England notes, which are also the only series issued for Wales. Scotland and Northern Ireland issue their own pound-denominated banknotes with different designs. They have yet to convert to polymer, and that will delay any consideration of adopting the new series.</p>
<p>
Turingâs family on his fatherâs side came from Scotland, so there is some sentiment for the Royal Bank of Scotland adopting the design. There is also discussion of recognizing Robin Milner and/or Donald Michie, though both were born in England. </p>
<p>
Over in Ireland, both sides of the Northern Ireland border are considering the statistician William Gosset, who as Head Brewer of Guinness over a century ago conceived the <a href="https://en.wikipedia.org/wiki/Student's_t-test">Student t-test</a> among many innovations. However, Gosset was also born in England. The Bushmills distillery in Northern Ireland has already <a href="https://web.archive.org/web/20150717073525/http://boini.bankofireland.com/about-boi-group/bank-notes/note/16">appeared</a> on their 5-pound note.</p>
<p>
In fact, Gosset and Milner and Michie were all originally <a href="https://www.bankofengland.co.uk/-/media/boe/files/banknotes/50-character-selection-names.pdf">nominated</a> alongside Turing for the 50-pound note. We spot Bertrand Russell and William Tutte and Karen SpÃ¤rck-Jones on the list. In physics, we notice Paul Dirac and John Bell and Stephen Hawking, and also the astronomer Vera Rubinâwho was only American, no? Bell was from Northern Ireland. </p>
<p>
Sir Michael Atiyah is not on the list because he was living at the timeâno living person other than the monarch may appear on a banknote. This law does not apply to e-currency however, and this leads to what for us is the most exciting aspect of the Bank of England initiative.</p>
<p/><h2> Semi-Fungible Tokens </h2><p/>
<p>
The following is a GLL exclusive. It was conveyed to us by Dr. Lofa Polir, who since our <a href="https://rjlipton.wpcomstaging.com/2020/04/01/research-at-home/">story</a> a year ago has resided in England. It comes from a confluence of several facts:</p>
<ul>
<li>National banks regularly issue limited editions of currency apart from the main series.
</li><li>Government banks have largely refrained from direct involvement with digital currencies, but <a href="https://www.federalreserve.gov/newsevents/speech/brainard20200813a.htm">observe</a> them closely.
</li><li>Large sections of the public have come to ascribe intrinsic value to <em>non-fungible tokens</em> (<a href="https://en.wikipedia.org/wiki/Non-fungible_token">NFTs</a>), which come with a blockchain-verifiable certificate of unique ownership.
</li><li>A non-fungible token by-definition cannot be used as money but only bought and sold as a (unique) commodity.
</li><li>Playing cards in games such as <a href="https://en.wikipedia.org/wiki/Magic:_The_Gathering">Magic: the Gathering</a> are regularly traded online to the extent of <a href="https://en.wikipedia.org/wiki/Mt._Gox">melding</a> with cryptocurrencies or verging on being currency themselves.
</li></ul>
<p>
The Bank of England recently voted to create an active response to this situation: the <em>semi-fungible token</em> (SFT). Like Bitcoin and other cryptocurrencies, SFTs limit their supply, but unlike them, SFTs are maintained by a central body and backed by its assets. An SFT is fungible by dint of having a fixed redemption rate to standard currency, but its own price may go higher.  As with NFTs, each SFT is associated to a piece of digital artwork. </p>
<p>
The new issues are based on the Royal Mailâs 2015 <a href="https://www.collectgbstamps.co.uk/explore/issues/?issue=22720">Inventive Britain</a> collection, whose honorees include Sir Tim Berners-Lee for the World Wide Web. The main stamps were abstract, but associated issues are not subject to the law against likenesses of living persons:</p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/04/01/computer-science-gets-noted/bernersleewww/" rel="attachment wp-att-18477"><img alt="" class="aligncenter wp-image-18477" height="212" src="https://i1.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/04/BernersLeeWWW.png?resize=534%2C212&amp;ssl=1" width="534"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Composite of <a href="https://gb-site.netlify.app/tim-berners-lee-biography/">src1</a>, <a href="https://www.bfdc.co.uk/2007/world_of_invention/sir_tim_berners_lee.html">src2</a></font>
</td>
</tr>
</tbody></table>
<p>
Berners-Lee will be the first honoree in the SFT series. We are not authorized to reveal the art design before its release. But Lofa sent us a list of nominees that includes Geoffrey Hinton, Tony Hoare, and Leslie Valiant on the CS side, plus David Spiegelhalter and Demis Hassabis in data science. Unlike with the banknotes, the Bank of England can âcatch them all.â Imagine, then, you will soon be able to pay for groceries with cards of some of your favorite computer and data scientistsâat least if they are British and you buy the groceries in Britain. </p>
<p/><h2> Open Problems </h2><p/>
<p>
If you see a market for NFTs, do you see one for SFTsâor are they both April Fools?</p>
<p/><p><br/>
[added a sentence to the description of SFTs.]</p></font></font></div>
    </content>
    <updated>2021-04-01T08:36:16Z</updated>
    <published>2021-04-01T08:36:16Z</published>
    <category term="History"/>
    <category term="News"/>
    <category term="People"/>
    <category term="Ada Lovelace"/>
    <category term="April Fool"/>
    <category term="Charles Babbage"/>
    <category term="Florence Nightingale"/>
    <category term="Lofa Polir"/>
    <category term="Maurice Wilkes"/>
    <category term="non-fungible tokens"/>
    <category term="Tim Berners-Lee"/>
    <category term="Turing"/>
    <author>
      <name>KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>GÃ¶del's Lost Letter and P=NP</title>
      <updated>2021-04-14T02:20:38Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/03/31/linkage</id>
    <link href="https://11011110.github.io/blog/2021/03/31/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>As is often the case, not all of these are links; theyâre copies of posts that I made over on my Mastodon account because they were too short to make full posts here.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>As is often the case, not all of these are links; theyâre copies of posts that I made over on my Mastodon account because they were too short to make full posts here.</p>

<ul>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Borromean_rings">Borromean rings</a> (<a href="https://mathstodon.xyz/@11011110/105898319690639869">\(\mathbb{M}\)</a>), now a Good Article on Wikipedia. You canât make the Borromean rings with geometric circles: as Tverberg observed, an inversion would make one of them a line. But then each of the other two circles would span an angle less than \(\pi\) as viewed from the line, leaving an unspanned direction along which the line could escape to infinity, contradicting the inseparability of the rings.</p>

    <p>The same proof shows that <a href="https://en.wikipedia.org/wiki/Brunnian_link">Brunnian links</a> cannot be made from four geometric circles. But the original proof that Borromean rings are not circular, by <a href="https://doi.org/10.4310/jdg/1214440725">Freedman and Skora</a> using 4d hyperbolic geometry, applies more generally to any Brunnian link no matter how many components it has.</p>
  </li>
  <li>
    <p><a href="https://twitter.com/mathpuzzle/status/1366756737882218500">Squares can be divided into \(45^\circ\)â\(60^\circ\)â\(75^\circ\) triangles</a> (<a href="https://mathstodon.xyz/@11011110/105909672693909508">\(\mathbb{M}\)</a>), as Ed Pegg posts on twitter. This is an interesting variant on something I looked at a long time ago, <a href="https://www.ics.uci.edu/~eppstein/junkyard/acute-square/">triangulating the square to minimize the maximum angle</a>. But Peggâs subdivision is not edge-to-edge. And the triangles on my page are not all similar. Do squares have edge-to-edge triangulations by similar acute triangles?</p>
  </li>
  <li>
    <p><a href="https://blogs.sciencemag.org/editors-blog/2021/02/18/a-new-name-change-policy/"><em>Science</em> joins other publishers in making it easy for authors to change their names on past publications</a> (<a href="https://mathstodon.xyz/@11011110/105915141700667095">\(\mathbb{M}\)</a>, <a href="https://www.metafilter.com/190813/Making-it-easier-for-published-scientists-to-change-their-names">via</a>). Transgender scientists made this change necessary but I think it benefits all of us. Iâve seen plenty of name changes in the literature for other reasons (marriage, divorce, escaping prejudice, â¦) and making it easier to find your old papers is generally a good thing.</p>
  </li>
  <li>
    <p>Cairo in Denmark? (<a href="https://mathstodon.xyz/@11011110/105920191343711124">\(\mathbb{M}\)</a>).  This is a screenshot from Google Maps of Havnepromenade in Copenhagen, where it meets the end of Cort Adelers Gade. Iâm sad to have missed this spot when I was in Copenhagen a couple years ago; itâs a nice example of a <a href="https://en.wikipedia.org/wiki/Cairo_pentagonal_tiling">Cairo pentagonal tiling</a>.</p>

    <p style="text-align: center;"><img alt="Pentagonal street pavers on Havnepromenade at Cort Adelers Gade in Copenhagen, taken as a screenshot from Google Maps" src="https://11011110.github.io/blog/assets/2021/Cairo-in-Copenhagen.jpg" style="border-style: solid; border-color: black;" width="80%"/></p>
  </li>
  <li>
    <p>My latest illustration trick: make complex non-Boolean combinations of shapes by overlaying simple shapes, exploding the overlay into simple regions, and unioning the regions into the shapes I want (<a href="https://mathstodon.xyz/@11011110/105929236692195138">\(\mathbb{M}\)</a>). Exploding and unioning are single clicks in Illustratorâs pathfinder tool. I used two levels of this technique to make this diagram of a <a href="https://en.wikipedia.org/wiki/Brunnian_link">Brunnian link</a>: once to make the shape of a single component, and again to make the over-under relation between components.</p>

    <p style="text-align: center;"><img alt="Six-rubberband link" src="https://11011110.github.io/blog/assets/2021/six-rubberband-link.svg"/></p>
  </li>
  <li>
    <p><a href="https://mattwthomas.com/blog/induction-on-reals/">Real induction</a> (<a href="https://mathstodon.xyz/@mwt/105862096547025004">\(\mathbb{M}\)</a>). An induction principle for proving statements about all real numbers in intervals.</p>
  </li>
  <li>
    <p><a href="https://www.patreon.com/posts/new-print-49071226">RecamÃ¡n Sequence</a> (<a href="https://mastodon.social/@joshmillard/105935104495594723">\(\mathbb{M}\)</a>). Linocut art by Josh Millard in a style drawn from a <em>Numberphile</em> video.</p>
  </li>
  <li>
    <p><a href="https://www.quantamagazine.org/mathematicians-inch-closer-to-matrix-multiplication-goal-20210323/"><em>Quanta</em> surveys recent developments in fast matrix multiplication</a> (<a href="https://mathstodon.xyz/@11011110/105943717367375547">\(\mathbb{M}\)</a>), including <a href="https://arxiv.org/abs/2010.05846">a paper from last SODA by Josh Alman and Virginia Williams</a> improving the exponent from 2.3728639 to 2.3728596. Known barriers to the widely-expressed hope that the exponent can be reduced to 2 (see the SODA paperâs introduction) are, I think too specific to be convincing. On the other hand at this rate thereâs still a long way from here to 2â¦</p>
  </li>
  <li>
    <p><a href="https://fredrikj.net/blog/2021/03/printing-algebraic-numbers/">Printing algebraic numbers</a> (<a href="https://mathstodon.xyz/@11011110/105946537048589616">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=26566090">via</a>). So your code can handle symbolic representations of exact algebraic numbers rather than just approximating everything as complex/float. How do you get those representations out to human users in a readable way? From âªFredrik Johanssonâ¬, long-time developer of open symbolic algebra and exact real arithmetic software including SymPy and Calcium.</p>
  </li>
  <li>
    <p><a href="https://en.wikipedia.org/wiki/Fermat%27s_right_triangle_theorem">Fermatâs right triangle theorem</a> (<a href="https://mathstodon.xyz/@11011110/105951398938627194">\(\mathbb{M}\)</a>), now a Good Article on Wikipedia. As Fibonacci observed and Fermat proved (as his only surviving proof), integer right triangles cannot have an area that is a perfect square. One corollary of this is the exponent-4 case of Fermatâs Last Theorem.</p>
  </li>
  <li>
    <p><a href="https://mastodon.social/@tomharris/105956972687307734">Tom Harris asks: Whatâs your favorite OEIS sequence?</a> Mine for this week at least is the <a href="https://en.wikipedia.org/wiki/Moser%E2%80%93De_Bruijn_sequence">MoserâDe Bruijn sequence</a>, OEIS <a href="https://oeis.org/A000695">A000695</a>.</p>
  </li>
  <li>
    <p>The most recent image of me on my site was from 2009 and getting pretty out-of-date. So I took a screenshot of myself as I look these days on Zoom (<a href="https://mathstodon.xyz/@11011110/105964230654539136">\(\mathbb{M}\)</a>). This is far from the first time Iâve let my hair go at least this long, but Iâm still looking forward to a time when I will feel more comfortable getting a haircut.
It occurs to me that there has been a reversal: now, instead of <a href="https://en.wikipedia.org/wiki/Almost_Cut_My_Hair">showing off rebelliousness</a>, I am visibly conforming to safety rules.</p>

    <p style="text-align: center;"><img alt="David Eppstein, self-portrait, Zoom screenshot" src="https://11011110.github.io/blog/assets/2021/zoom.jpg" style="border-style: solid; border-color: black;" width="80%"/></p>
  </li>
  <li>
    <p><a href="https://www.eff.org/deeplinks/2021/03/free-climbing-rock-climbers-open-data-project-threatened-bogus-copyright-claims">Fight over open data in rock climbing</a> (<a href="https://mathstodon.xyz/@11011110/105971892719302236">\(\mathbb{M}\)</a>, <a href="https://news.ycombinator.com/item?id=26609945">via</a>).</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/q/387543/440">Weird probability distributions on dyadic rationals from a simple averaging process</a> (<a href="https://mathstodon.xyz/@11011110/105977625108931414">\(\mathbb{M}\)</a>). Start with the two-element multiset \(\{0,1\}\), repeatedly draw two-element samples (without replacement) from the multiset and include the average into the multiset. The result tends to cluster around some value, not the same value every time but itself drawn from a bimodal distribution, with the clustering sort of looking Cauchy but not quite. Interesting new MathOverflow question.</p>
  </li>
  <li>
    <p><a href="https://blog.computationalcomplexity.org/2021/03/slicing-hypercube.html">How many hyperplanes does it take to slice all the edges of an -dimensional hypercube</a> (<a href="https://mathstodon.xyz/@11011110/105982764812531388">\(\mathbb{M}\)</a>)? Two if you can skim the ends of the edges or include entire edges in hyperplanes but more to really slice them. The obvious answer of axis-parallel cuts can be improved to \(5n/6\), and Lance Fortnow reports that the newest lower bound is \(\Omega(n^{0.57})\), by Gal Yehuda and Amir Yehudayoff, improving <a href="https://arxiv.org/abs/2102.05536">their own new preprint</a>.</p>
  </li>
  <li>
    <p><a href="https://rjlipton.wpcomstaging.com/2021/03/30/all-the-news-that-fits-we-print/">A list of current blogs on computer theory and related math</a> (<a href="https://mathstodon.xyz/@11011110/105986862218758838">\(\mathbb{M}\)</a>), from <em>GÃ¶delâs Lost Letter and P=NP</em>.</p>
  </li>
</ul></div>
    </content>
    <updated>2021-03-31T17:54:00Z</updated>
    <published>2021-03-31T17:54:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-04-02T23:41:50Z</updated>
    </source>
  </entry>
</feed>
