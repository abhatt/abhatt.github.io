<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2021-05-24T11:39:22Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/071</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/071" rel="alternate" type="text/html"/>
    <title>TR21-071 |  On the Algorithmic Content of Quantum Measurements | 

	Samuel Epstein</title>
    <summary>We show that given a quantum measurement, for an overwhelming majority of pure states, no meaningful information is produced. This is independent of the number of outcomes of the quantum measurement. Due to conservation inequalities, such random noise cannot be processed into coherent data.</summary>
    <updated>2021-05-23T14:21:48Z</updated>
    <published>2021-05-23T14:21:48Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-05-24T11:37:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.09922</id>
    <link href="http://arxiv.org/abs/2105.09922" rel="alternate" type="text/html"/>
    <title>Computing the Fr\'echet Distance Between Uncertain Curves in One Dimension</title>
    <feedworld_mtime>1621728000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Buchin:Kevin.html">Kevin Buchin</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/L=ouml=ffler:Maarten.html">Maarten Löffler</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Ophelders:Tim.html">Tim Ophelders</a>, Aleksandr Popov, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Urhausen:J=eacute=r=ocirc=me.html">Jérôme Urhausen</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/v/Verbeek:Kevin.html">Kevin Verbeek</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.09922">PDF</a><br/><b>Abstract: </b>We consider the problem of computing the Fr\'echet distance between two
curves for which the exact locations of the vertices are unknown. Each vertex
may be placed in a given uncertainty region for that vertex, and the objective
is to place vertices so as to minimise the Fr\'echet distance. This problem was
recently shown to be NP-hard in 2D, and it is unclear how to compute an optimal
vertex placement at all.
</p>
<p>We present the first general algorithmic framework for this problem. We prove
that it results in a polynomial-time algorithm for curves in 1D with intervals
as uncertainty regions. In contrast, we show that the problem is NP-hard in 1D
in the case that vertices are placed to maximise the Fr\'echet distance.
</p>
<p>We also study the weak Fr\'echet distance between uncertain curves. While
finding the optimal placement of vertices seems more difficult than the regular
Fr\'echet distance -- and indeed we can easily prove that the problem is
NP-hard in 2D -- the optimal placement of vertices in 1D can be computed in
polynomial time. Finally, we investigate the discrete weak Fr\'echet distance,
for which, somewhat surprisingly, the problem is NP-hard already in 1D.
</p></div>
    </summary>
    <updated>2021-05-23T22:54:58Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-05-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.09838</id>
    <link href="http://arxiv.org/abs/2105.09838" rel="alternate" type="text/html"/>
    <title>Online Risk-Averse Submodular Maximization</title>
    <feedworld_mtime>1621728000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Soma:Tasuku.html">Tasuku Soma</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/y/Yoshida:Yuichi.html">Yuichi Yoshida</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.09838">PDF</a><br/><b>Abstract: </b>We present a polynomial-time online algorithm for maximizing the conditional
value at risk (CVaR) of a monotone stochastic submodular function. Given $T$
i.i.d. samples from an underlying distribution arriving online, our algorithm
produces a sequence of solutions that converges to a ($1-1/e$)-approximate
solution with a convergence rate of $O(T^{-1/4})$ for monotone continuous
DR-submodular functions. Compared with previous offline algorithms, which
require $\Omega(T)$ space, our online algorithm only requires $O(\sqrt{T})$
space. We extend our online algorithm to portfolio optimization for monotone
submodular set functions under a matroid constraint. Experiments conducted on
real-world datasets demonstrate that our algorithm can rapidly achieve CVaRs
that are comparable to those obtained by existing offline algorithms.
</p></div>
    </summary>
    <updated>2021-05-23T22:38:57Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.09772</id>
    <link href="http://arxiv.org/abs/2105.09772" rel="alternate" type="text/html"/>
    <title>Indirect predicates for geometric constructions</title>
    <feedworld_mtime>1621728000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Attene:Marco.html">Marco Attene</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.09772">PDF</a><br/><b>Abstract: </b>Geometric predicates are a basic ingredient to implement a vast range of
algorithms in computational geometry. Modern implementations employ floating
point filtering techniques to combine efficiency and robustness, and
state-of-the-art predicates are guaranteed to be always exact while being only
slightly slower than corresponding (inexact) floating point implementations.
Unfortunately, if the input to these predicates is an intermediate construction
of an algorithm, its floating point representation may be affected by an
approximation error, and correctness is no longer guaranteed. This paper
introduces the concept of indirect geometric predicate: instead of taking the
intermediate construction as an explicit input, an indirect predicate considers
the primitive geometric elements which are combined to produce such a
construction. This makes it possible to keep track of the floating point
approximation, and thus to exploit efficient filters and expansion arithmetic
to exactly resolve the predicate with minimal overhead with respect to a naive
floating point implementation. As a representative example, we show how to
extend standard predicates to the case of points of intersection of linear
elements (i.e. lines and planes) and show that, on classical problems, this
approach outperforms state-of-the-art solutions based on lazy exact
intermediate representations.
</p></div>
    </summary>
    <updated>2021-05-23T22:58:26Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-05-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.09675</id>
    <link href="http://arxiv.org/abs/2105.09675" rel="alternate" type="text/html"/>
    <title>On the Parameterized Complexity of Polytree Learning</title>
    <feedworld_mtime>1621728000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Gr=uuml=ttemeier:Niels.html">Niels Grüttemeier</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Komusiewicz:Christian.html">Christian Komusiewicz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Morawietz:Nils.html">Nils Morawietz</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.09675">PDF</a><br/><b>Abstract: </b>A Bayesian network is a directed acyclic graph that represents statistical
dependencies between variables of a joint probability distribution. A
fundamental task in data science is to learn a Bayesian network from observed
data. \textsc{Polytree Learning} is the problem of learning an optimal Bayesian
network that fulfills the additional property that its underlying undirected
graph is a forest. In this work, we revisit the complexity of \textsc{Polytree
Learning}. We show that \textsc{Polytree Learning} can be solved in $3^n \cdot
|I|^{\mathcal{O}(1)}$ time where $n$ is the number of variables and $|I|$ is
the total instance size. Moreover, we consider the influence of the number of
variables $d$ that might receive a nonempty parent set in the final DAG on the
complexity of \textsc{Polytree Learning}. We show that \textsc{Polytree
Learning} has no $f(d)\cdot |I|^{\mathcal{O}(1)}$-time algorithm, unlike
Bayesian network learning which can be solved in $2^d \cdot
|I|^{\mathcal{O}(1)}$ time. We show that, in contrast, if $d$ and the maximum
parent set size are bounded, then we can obtain efficient algorithms.
</p></div>
    </summary>
    <updated>2021-05-23T22:37:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.09667</id>
    <link href="http://arxiv.org/abs/2105.09667" rel="alternate" type="text/html"/>
    <title>Unreliable Sensors for Reliable Efficient Robots</title>
    <feedworld_mtime>1621728000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Heriban:Adam.html">Adam Heriban</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tixeuil:S=eacute=bastien.html">Sébastien Tixeuil</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.09667">PDF</a><br/><b>Abstract: </b>The vast majority of existing Distributed Computing literature about mobile
robotic swarms considers computability issues: characterizing the set of system
hypotheses that enables problem solvability. By contrast, the focus of this
work is to investigate complexity issues: obtaining quantitative results about
a given problem that admits solutions. Our quantitative measurements rely on a
newly developed simulation framework to benchmark pen and paper designs. First,
we consider the maximum traveled distance when gathering robots at a given
location, not known beforehand (both in the two robots and in the n robots
settings) in the classical OBLOT model, for the FSYNC, SSYNC, and ASYNC
schedulers. This particular metric appears relevant as it correlates closely to
what would be real world fuel consumption. Then, we introduce the possibility
of errors in the vision of robots, and assess the behavior of known rendezvous
(aka two robots gathering) and leader election protocols when sensors are
unreliable. We also introduce two new algorithms, one for fuel efficient
convergence, and one for leader election, that operate reliably despite
unreliable sensors.
</p></div>
    </summary>
    <updated>2021-05-23T22:51:46Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-05-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.09602</id>
    <link href="http://arxiv.org/abs/2105.09602" rel="alternate" type="text/html"/>
    <title>Characterization of Super-stable Matchings</title>
    <feedworld_mtime>1621728000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hu:Changyong.html">Changyong Hu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Garg:Vijay_K=.html">Vijay K. Garg</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.09602">PDF</a><br/><b>Abstract: </b>An instance of the super-stable matching problem with incomplete lists and
ties is an undirected bipartite graph $G = (A \cup B, E)$, with an adjacency
list being a linearly ordered list of ties. Ties are subsets of vertices
equally good for a given vertex. An edge $(x,y) \in E \backslash M$ is a
blocking edge for a matching $M$ if by getting matched to each other neither of
the vertices $x$ and $y$ would become worse off. Thus, there is no disadvantage
if the two vertices would like to match up. A matching $M$ is super-stable if
there is no blocking edge with respect to $M$. It has previously been shown
that super-stable matchings form a distributive lattice and the number of
super-stable matchings can be exponential in the number of vertices. We give
two compact representations of size $O(m)$ that can be used to construct all
super-stable matchings, where $m$ denotes the number of edges in the graph. The
construction of the second representation takes $O(mn)$ time, where $n$ denotes
the number of vertices in the graph, and gives an explicit rotation poset
similar to the rotation poset in the classical stable marriage problem. We also
give a polyhedral characterisation of the set of all super-stable matchings and
prove that the super-stable matching polytope is integral, thus solving an open
problem stated in the book by Gusfield and Irving .
</p></div>
    </summary>
    <updated>2021-05-23T22:49:58Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.09566</id>
    <link href="http://arxiv.org/abs/2105.09566" rel="alternate" type="text/html"/>
    <title>(Sub)linear kernels for edge modification problems towards structured graph classes</title>
    <feedworld_mtime>1621728000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bathie:Gabriel.html">Gabriel Bathie</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bousquet:Nicolas.html">Nicolas Bousquet</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pierron:Th=eacute=o.html">Théo Pierron</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.09566">PDF</a><br/><b>Abstract: </b>In a (parameterized) graph edge modification problem, we are given a graph
$G$, an integer $k$ and a (usually well-structured) class of graphs
$\mathcal{G}$, and ask whether it is possible to transform $G$ into a graph $G'
\in \mathcal{G}$ by adding and/or removing at most $k$ edges. Parameterized
graph edge modification problems received considerable attention in the last
decades.
</p>
<p>In this paper we focus on finding small kernels for edge modification
problems. One of the most studied problems is the \textsc{Cluster Editing}
problem, in which the goal is to partition the vertex set into a disjoint union
of cliques. Even if a $2k$ kernel exists for \textsc{Cluster Editing}, this
kernel does not reduce the size of the instance in most cases. Therefore, we
explore the question of whether linear kernels are a theoretical limit in edge
modification problems, in particular when the target graphs are very structured
(such as a partition into cliques for instance). We prove, as far as we know,
the first sublinear kernel for an edge modification problem. Namely, we show
that \textsc{Clique + Independent Set Deletion}, which is a restriction of
\textsc{Cluster Deletion}, admits a kernel of size $O(k/\log k)$.
</p>
<p>We also obtain small kernels for several other edge modification problems. We
prove that \textsc{Split Addition} (and the equivalent \textsc{Split Deletion})
admits a linear kernel, improving the existing quadratic kernel of Ghosh et al.
\cite{ghosh2015faster}. We also prove that \textsc{Trivially Perfect Addition}
admits a quadratic kernel (improving the cubic kernel of Guo
\cite{guo2007problem}), and finally prove that its triangle-free version
(\textsc{Starforest Deletion}) admits a linear kernel, which is optimal under
ETH.
</p></div>
    </summary>
    <updated>2021-05-23T22:51:08Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.09522</id>
    <link href="http://arxiv.org/abs/2105.09522" rel="alternate" type="text/html"/>
    <title>Matchings with Group Fairness Constraints: Online and Offline Algorithms</title>
    <feedworld_mtime>1621728000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Govind S. Sankar, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Louis:Anand.html">Anand Louis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nasre:Meghana.html">Meghana Nasre</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nimbhorkar:Prajakta.html">Prajakta Nimbhorkar</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.09522">PDF</a><br/><b>Abstract: </b>We consider the problem of assigning items to platforms in the presence of
group fairness constraints. In the input, each item belongs to certain
categories, called classes in this paper. Each platform specifies the group
fairness constraints through an upper bound on the number of items it can serve
from each class. Additionally, each platform also has an upper bound on the
total number of items it can serve. The goal is to assign items to platforms so
as to maximize the number of items assigned while satisfying the upper bounds
of each class. In some cases, there is a revenue associated with matching an
item to a platform, then the goal is to maximize the revenue generated.
</p>
<p>This problem models several important real-world problems like ad-auctions,
scheduling, resource allocations, school choice etc.We also show an interesting
connection to computing a generalized maximum independent set on hypergraphs
and ranking items under group fairness constraints.
</p>
<p>We show that if the classes are arbitrary, then the problem is NP-hard and
has a strong inapproximability. We consider the problem in both online and
offline settings under natural restrictions on the classes. Under these
restrictions, the problem continues to remain NP-hard but admits approximation
algorithms with small approximation factors. We also implement some of the
algorithms. Our experiments show that the algorithms work well in practice both
in terms of efficiency and the number of items that get assigned to some
platform.
</p></div>
    </summary>
    <updated>2021-05-23T22:49:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.09508</id>
    <link href="http://arxiv.org/abs/2105.09508" rel="alternate" type="text/html"/>
    <title>Fast Nonblocking Persistence for Concurrent Data Structures</title>
    <feedworld_mtime>1621728000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Wentao Cai, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wen:Haosen.html">Haosen Wen</a>, Vladimir Maksimovski, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Du:Mingzhe.html">Mingzhe Du</a>, Rafaello Sanna, Shreif Abdallah, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Scott:Michael_L=.html">Michael L. Scott</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.09508">PDF</a><br/><b>Abstract: </b>We present a fully lock-free variant of the recent Montage system for
persistent data structures. Our variant, nbMontage, adds persistence to almost
any nonblocking concurrent structure without introducing significant overhead
or blocking of any kind. Like its predecessor, nbMontage is buffered durably
linearizable: it guarantees that the state recovered in the wake of a crash
will represent a consistent prefix of pre-crash execution. Unlike its
predecessor, nbMontage ensures wait-free progress of the persistence frontier,
thereby bounding the number of recent updates that may be lost on a crash, and
allowing a thread to force an update of the frontier (i.e., to perform a sync
operation) without the risk of blocking. As an extra benefit, the helping
mechanism employed by our wait-free sync significantly reduces its latency.
</p>
<p>Performance results for nonblocking queues, skip lists, trees, and hash
tables rival custom data structures in the literature -- dramatically faster
than achieved with prior general-purpose systems, and generally within 50% of
equivalent non-persistent structures placed in DRAM.
</p></div>
    </summary>
    <updated>2021-05-23T22:40:17Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.09439</id>
    <link href="http://arxiv.org/abs/2105.09439" rel="alternate" type="text/html"/>
    <title>The Simultaneous Assignment Problem</title>
    <feedworld_mtime>1621728000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Madarasi:P=eacute=ter.html">Péter Madarasi</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.09439">PDF</a><br/><b>Abstract: </b>This paper introduces the \emph{Simultaneous Assignment Problem}. Here, we
are given an assignment problem on some of the subgraphs of a given graph, and
we are looking for a heaviest assignment which is feasible when restricted to
any of the assignment problems. More precisely, we are given a graph with a
weight- and a capacity function on its edges and a set of its subgraphs
$H_1,\dots,H_k$ along with a degree upper bound function for each of them. In
addition, we are also given a laminar system on the node set with an upper
bound on the degree-sum of the nodes in each set in the system. We want to
assign each edge a non-negative integer below its capacity such that the total
weight is maximized, the degrees in each subgraph are below the degree upper
bound associated with the subgraph, and the degree-sum bound is respected in
each set of the laminar system.
</p>
<p>The problem is shown to be APX-hard in the unweighted case even if the graph
is a forest and $k=2$. This also implies that the Distance matching problem is
APX-hard in the weighted case and that the Cyclic distance matching problem is
APX-hard in the unweighted case. We identify multiple special cases when the
problem can be solved in strongly polynomial time. One of these cases, the
so-called locally laminar case, is a common generalization of the Hierarchical
b-matching problem and the Laminar matchoid problem, and it implies that both
of these problems can be solved efficiently in the weighted, capacitated case
-- improving upon the most general polynomial-time algorithms for these
problems. The problem can be constant approximated when $k$ is a constant, and
we show that the approximation factor matches the integrality gap of a
strengthened LP-relaxation for small $k$. We give improved approximation
algorithms for special cases, for example, when the degree bounds are uniform
or the graph is sparse.
</p></div>
    </summary>
    <updated>2021-05-23T22:40:11Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.09438</id>
    <link href="http://arxiv.org/abs/2105.09438" rel="alternate" type="text/html"/>
    <title>Heesch Numbers of Unmarked Polyforms</title>
    <feedworld_mtime>1621728000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kaplan:Craig_S=.html">Craig S. Kaplan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.09438">PDF</a><br/><b>Abstract: </b>A shape's Heesch number is the number of layers of copies of the shape that
can be placed around it without gaps or overlaps. Experimentation and
exhaustive searching have turned up examples of shapes with finite Heesch
numbers up to six, but nothing higher. The computational problem of classifying
simple families of shapes by Heesch number can provide more experimental data
to fuel our understanding of this topic. I present a technique for computing
Heesch numbers of non-tiling polyforms using a SAT solver, and the results of
exhaustive computation of Heesch numbers up to 19-ominoes, 17-hexes, and
24-iamonds.
</p></div>
    </summary>
    <updated>2021-05-23T22:58:41Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-05-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.09413</id>
    <link href="http://arxiv.org/abs/2105.09413" rel="alternate" type="text/html"/>
    <title>Diversity in Kemeny Rank Aggregation: A Parameterized Approach</title>
    <feedworld_mtime>1621728000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Arrighi:Emmanuel.html">Emmanuel Arrighi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fernau:Henning.html">Henning Fernau</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lokshtanov:Daniel.html">Daniel Lokshtanov</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Oliveira:Mateus_de_Oliveira.html">Mateus de Oliveira Oliveira</a>, Petra Wolf <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.09413">PDF</a><br/><b>Abstract: </b>In its most traditional setting, the main concern of optimization theory is
the search for optimal solutions for instances of a given computational
problem. A recent trend of research in artificial intelligence, called solution
diversity, has focused on the development of notions of optimality that may be
more appropriate in settings where subjectivity is essential. The idea is that
instead of aiming at the development of algorithms that output a single optimal
solution, the goal is to investigate algorithms that output a small set of
sufficiently good solutions that are sufficiently diverse from one another. In
this way, the user has the opportunity to choose the solution that is most
appropriate to the context at hand. It also displays the richness of the
solution space.
</p>
<p>When combined with techniques from parameterized complexity theory, the
paradigm of diversity of solutions offers a powerful algorithmic framework to
address problems of practical relevance. In this work, we investigate the
impact of this combination in the field of Kemeny Rank Aggregation, a
well-studied class of problems lying in the intersection of order theory and
social choice theory and also in the field of order theory itself. In
particular, we show that the Kemeny Rank Aggregation problem is fixed-parameter
tractable with respect to natural parameters providing natural formalizations
of the notions of diversity and of the notion of a sufficiently good solution.
Our main results work both when considering the traditional setting of
aggregation over linearly ordered votes, and in the more general setting where
votes are partially ordered.
</p></div>
    </summary>
    <updated>2021-05-23T22:41:24Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2021-05-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/2105.09313</id>
    <link href="http://arxiv.org/abs/2105.09313" rel="alternate" type="text/html"/>
    <title>Approximation Algorithms For The Dispersion Problems in a Metric Space</title>
    <feedworld_mtime>1621728000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mishra:Pawan_K=.html">Pawan K. Mishra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Das:Gautam_K=.html">Gautam K. Das</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/2105.09313">PDF</a><br/><b>Abstract: </b>In this article, we consider the $c$-dispersion problem in a metric space
$(X,d)$. Let $P=\{p_{1}, p_{2}, \ldots, p_{n}\}$ be a set of $n$ points in a
metric space $(X,d)$. For each point $p \in P$ and $S \subseteq P$, we define
$cost_{c}(p,S)$ as the sum of distances from $p$ to the nearest $c $ points in
$S \setminus \{p\}$, where $c\geq 1$ is a fixed integer. We define
$cost_{c}(S)=\min_{p \in S}\{cost_{c}(p,S)\}$ for $S \subseteq P$. In the
$c$-dispersion problem, a set $P$ of $n$ points in a metric space $(X,d)$ and a
positive integer $k \in [c+1,n]$ are given. The objective is to find a subset
$S\subseteq P$ of size $k$ such that $cost_{c}(S)$ is maximized. We propose a
simple polynomial time greedy algorithm that produces a $2c$-factor
approximation result for the $c$-dispersion problem in a metric space. The best
known result for the $c$-dispersion problem in the Euclidean metric space
$(X,d)$ is $2c^2$, where $P \subseteq \mathbb{R}^2$ and the distance function
is Euclidean distance [ Amano, K. and Nakano, S. I., Away from Rivals, CCCG,
pp.68-71, 2018 ]. We also prove that the $c$-dispersion problem in a metric
space is $W[1]$-hard.
</p></div>
    </summary>
    <updated>2021-05-23T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2021-05-21T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5510</id>
    <link href="https://www.scottaaronson.com/blog/?p=5510" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5510#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5510" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">On turning 40 today</title>
    <summary xml:lang="en-US">Holy crap. In case you’re wondering how I spent such a milestone of a day: well, I spent hours of it at an important virtual grant review meeting with the Department of Defense. Alas, when it came time for my own big presentation at that meeting—about what my students and I had done over the […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>Holy crap.</p>



<p>In case you’re wondering how I spent such a milestone of a day: well, I spent hours of it at an important virtual grant review meeting with the Department of Defense.  Alas, when it came time for my own big presentation at that meeting—about what my students and I had done over the past five years to lay the theoretical foundations for the recent achievement of quantum computational supremacy—I’d uploaded the completely wrong PowerPoint file (it was something.pptx rather than something.ppt, where they <em>weren’t</em> two versions of the same presentation).  Sorting this out took about 10 minutes, destroyed my momentum, and wasted everyone’s time.  I partly blame the Microsoft Teams platform, whose limitations as conferencing software compared to Zoom necessitated emailing my presentation in the first place.  But of course, part of the blame rests with me.</p>



<p>I had to explain apologetically to the US Department of Defense that I’m no good with tech stuff—being a mere computer science PhD.  And unlike many of my colleagues (who I envy), back in my youth—for at age 40 I’m no longer young—I never had enough time to become <em>both</em> the kind of person who might earn a big grant to do quantum computing theory, <em>and</em> the kind of person who’d be minimally competent at the logistics of a review meeting for such a grant.</p>



<p/><hr/><p/>



<p>Forty years.  Seven-eighths of those years, aware of the finiteness of the speed of light and of its value.  Four-fifths of them, aware of the grislier details of the Holocaust.  Three-quarters of them, aware of what it means to write code.  Two-thirds of them, aware of polynomial versus exponential time.  More than half of them trying to understand the capabilities and limitations of quantum computers as my day job.  And then, rounding the corner, more than a third of the years writing this blog, a third of them being a professor, a quarter of them married, a fifth of them raising kids, a thirtieth of them in the midst of a global pandemic.</p>



<p>I didn’t even come <em>close </em>to achieving everything I hoped I would in my thirties.  At least a half-dozen major papers, ones I expected would’ve been finished years ago (on the mixing of coffee and cream, on complexity and firewalls and AdS/CFT, on certified random numbers from sampling-based quantum supremacy experiments, on the implications of the Raz-Tal oracle separation, …), still need to be revised or even written.  Other projects (e.g., the graphic novel about teaching math to Lily) were excitedly announced and then barely even started.  I never wrote most of my promised blog post about the continuum hypothesis, <em>or</em> the one about Stephen Wolfram’s recrudescent claims of a unified theory of physics.  And covid, which determined the world’s working conditions while we were running out the clock, turned out <em>not</em> to be a hyper-productive time for me.  That’s how you know I’m not Newton (well, it’s the not the only way you know).</p>



<p>Anyway, during the runup to it, one’s 40th birthday feels like a temporal singularity, where you have to compress more and more of what you’d hoped to achieve before age 40 as you get closer and closer to it, because what the hell is there on the other side? <em> The</em>y<em>‘re</em> over-40 and hence “old”; <em>you’re</em> under-40 and hence still “young.”</p>



<p>OK, but here I am on the other side right now, <em>the “old” side</em>, and I’m still here, still thinking and writing and feeling fairly continuous with my pre-singularity embodiment!  And so far, in 16 hours on this side, the most senile thing I’ve done has been to email the wrong file attachment and thereby ruin an important funding presenta… you know what, let’s not even go there.</p>



<p>If you feel compelled to give me a 40<sup>th</sup> birthday present, then just make it a comment on this post, as short or long as you like, about what anything I said or did meant for you.  I’m a total softie for that stuff.</p></div>
    </content>
    <updated>2021-05-21T20:44:41Z</updated>
    <published>2021-05-21T20:44:41Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Adventures in Meatspace"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Nerd Interest"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Self-Referential"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-05-21T20:52:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2021/05/21/international-school-conference-in-algorithms-combinatorics-and-complexity/</id>
    <link href="https://cstheory-events.org/2021/05/21/international-school-conference-in-algorithms-combinatorics-and-complexity/" rel="alternate" type="text/html"/>
    <title>International School-conference in Algorithms, Combinatorics, and Complexity</title>
    <summary>May 24-28, 2021 Online https://indico.eimi.ru/event/199/ An advanced school for young researchers featuring three minicourses in vibrant areas of mathematics and computer science. The target audience includes graduate, master, and senior bachelor students of any mathematical speciality.</summary>
    <updated>2021-05-21T09:14:41Z</updated>
    <published>2021-05-21T09:14:41Z</published>
    <category term="school"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2021-05-24T11:38:39Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=562</id>
    <link href="https://tcsplus.wordpress.com/2021/05/20/tcs-talk-wednesday-may-26-kira-goldner-columbia-university/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, May 26 — Kira Goldner, Columbia University</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, May 26th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). Kira Goldner from Columbia University will speak about “An Overview of Using Mechanism Design for Social Good” (abstract below). You can reserve a spot as an individual or […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, May 26th at 1:00 PM Eastern Time (10:00 AM Pacific Time, 19:00 Central European Time, 17:00 UTC). <a href="https://www.kiragoldner.com/"><strong>Kira Goldner</strong></a> from Columbia University will speak about “<em>An Overview of Using Mechanism Design for Social Good</em>” (abstract below).</p>
<p>You can reserve a spot as an individual or a group to join us live by signing up on <a href="https://sites.google.com/view/tcsplus/welcome/next-tcs-talk">the online form</a>. Due to security concerns, <em>registration is required</em> to attend the interactive talk. (The recorded talk will also be posted <a href="https://sites.google.com/view/tcsplus/welcome/past-talks">on our website</a> afterwards, so people who did not sign up will still be able to watch the talk) As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/view/tcsplus/welcome/suggest-a-talk">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/view/tcsplus/">the website</a>.</p>
<blockquote class="wp-block-quote"><p>Abstract: In order to accurately predict an algorithm’s outcome and quality when it interacts with participants who have a stake in the outcome, we must design it to be robust to strategic manipulation. This is the subject of algorithmic mechanism design, which borrows ideas from game theory and economics to design robust algorithms. In this talk, I will show how results from the theoretical foundations of algorithmic mechanism design can be used to solve problems of societal concern.</p>
<p>I will overview recent work in this area in many different applications — housing, labor markets, carbon license allocations, health insurance markets, and more — as well as discuss open problems and directions ripe for tools from both mechanism design and general TCS.</p></blockquote></div>
    </content>
    <updated>2021-05-20T21:53:27Z</updated>
    <published>2021-05-20T21:53:27Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2021-05-24T11:38:35Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-5273315380945865876</id>
    <link href="https://blog.computationalcomplexity.org/feeds/5273315380945865876/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/emerging-from-pandemic.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5273315380945865876" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/5273315380945865876" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/emerging-from-pandemic.html" rel="alternate" type="text/html"/>
    <title>Emerging from the Pandemic</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The City of Chicago yesterday agreed with the latest CDC guidelines that those of us fully vaccinated no longer have to wear masks in most settings. Lollapalooza, the big Chicago music festival, will be held in full capacity this summer. There are still some restrictions but barring any surprise setbacks should be mostly gone by the Fourth of July.</p><p>It's not appropriate to call the pandemic over. Too many countries are suffering and lack good access to vaccines or strong health care. But in most of the US vaccinations are readily available and it really feels like we are putting the pandemic in the past.</p><p>Variants that beat the vaccines could emerge. Vaccines could become ineffective over time. Too many still need to be vaccinated and too many don't want to do so. Typically problems start small and quickly get big by exponential growth but likely with enough warning if we need boosters or extra precautions. And all those who cut in line to get shots early are now my canaries for the effects wearing off.</p><p>What will this post-pandemic world look like? Continued virtual meetings from people who don't want to spend the 10-15 minutes to get across campus or to come to campus at all. A bigger move to casual dress, even in the corporate world. No more snow days. Changes in ways we won't expect. </p><p>We all have our different attitudes but I'm ready to move on. Time to start the "after times".</p></div>
    </content>
    <updated>2021-05-20T14:07:00Z</updated>
    <published>2021-05-20T14:07:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-05-24T11:11:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=21532</id>
    <link href="https://gilkalai.wordpress.com/2021/05/20/to-cheer-you-up-in-difficult-times-25-some-mathematical-news-part-2/" rel="alternate" type="text/html"/>
    <title>To cheer you up in difficult times 25: some mathematical news! (Part 2)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Topology Quasi-polynomial algorithms for telling if a knot is trivial Marc Lackenby announced a quasi-polynomial time algorithm to decide whether a given knot is the unknot! This is a big breakthrough. This question is known to be both in NP … <a href="https://gilkalai.wordpress.com/2021/05/20/to-cheer-you-up-in-difficult-times-25-some-mathematical-news-part-2/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><h2>Topology</h2>



<p/>


<h3>Quasi-polynomial algorithms for telling if a knot is trivial</h3>


<p>Marc Lackenby announced a quasi-polynomial time algorithm to decide whether a given knot is the unknot! This is a big breakthrough. This question is known to be both in NP and in coNP. See <a href="https://gilkalai.wordpress.com/2012/04/10/greg-kuperberg-it-is-in-np-to-tell-if-a-knot-is-knotted-under-grh/">this post</a>, and updates there in the <a href="https://gilkalai.wordpress.com/2012/04/10/greg-kuperberg-it-is-in-np-to-tell-if-a-knot-is-knotted-under-grh/#comment-43442">comment section</a>.</p>



<h4>Topology seminar, UC Davis, February 2021 and Oxford, March 2021 and <strong>today, May 20!</strong> <strong>16:00 CET</strong>, in the Copenhagen-Jerusalem combinatorics seminar (see details at the end of the post).</h4>



<p><a href="http://people.maths.ox.ac.uk/lackenby/quasipolynomial-talk-oxford.pdf">Unknot recognition in quasi-polynomial time</a> (The link is to the slides)</p>


<h3>NP hardness for related questions regarding knots</h3>


<p>This is a talk by Martin Tancer given at the  Copenhagen-Jerusalem  combinatorics seminar.</p>



<p>Title:  <a href="https://arxiv.org/abs/1810.03502">The unbearable hardness of unknotting</a>  (link to the 2018 arXive paper)</p>



<p>Abstract: </p>



<p>During the talk, I will sketch a proof that deciding if a diagram of the unknot can be untangled using at most k Riedemeister moves (where k is part of the input) is NP-hard. (This is not the same as the unknot recognition but it reveals some difficulties.) Similar ideas can be also used for proving that several other similar invariants are NP-hard to recognize on links.</p>



<p>Joint work with A. de Mesmay, Y. Rieck and E. Sedgwick.</p>



<h3>An approach toward exotic differentiable 4-dim spheres</h3>



<figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p dir="ltr" lang="en">Show one of these 21 links is smoothly slice and collect your Fields medal. <a href="https://twitter.com/hashtag/RBG?src=hash&amp;ref_src=twsrc%5Etfw">#RBG</a> <a href="https://t.co/Uem6CcXp7h">https://t.co/Uem6CcXp7h</a> <a href="https://t.co/eIPYVRktR0">pic.twitter.com/eIPYVRktR0</a></p>— Ian Agol (@agolian) <a href="https://twitter.com/agolian/status/1360269132822835201?ref_src=twsrc%5Etfw">February 12, 2021</a></blockquote></div>
</div></figure>



<p/>



<h3>Monumental work on Arnold’s conjecture</h3>



<p><a href="https://arxiv.org/abs/2103.01507">Arnold Conjecture and Morava K-theory</a> by Mohammed Abouzaid, Andrew J. Blumberg</p>



<figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"><div class="wp-block-embed__wrapper">
<div class="embed-twitter"><blockquote class="twitter-tweet"><p dir="ltr" lang="en">This is a big deal:<a href="https://t.co/CDGhu0GRZ3">https://t.co/CDGhu0GRZ3</a></p>— Justin Curry (@currying) <a href="https://twitter.com/currying/status/1367090561849696261?ref_src=twsrc%5Etfw">March 3, 2021</a></blockquote></div>
</div></figure>



<p>(I heard it from a retweet by Alex Kontorovich.)</p>



<p>Lower bound for the number of fixed points  of a map is a central theme in topology (and other areas) with many great results. The Lefshetz fixed point theorem tells you that a certain signed sum of the fixed points is bounded from below . For symplectic manifold Arnold’s conjecture asserts that the number of fixed point is bounded below from the sun of Betti numbers. This have led and is connected to tremendous mathematics.</p>



<blockquote class="wp-block-quote"><p><em>Abstract (part): We prove that the rank of the cohomology of a closed symplectic manifold with coefficients in a field of characteristic p is smaller than the number of periodic orbits of any non-degenerate Hamiltonian flow.</em></p></blockquote>



<h2>Graph theory</h2>



<h3><a href="https://arxiv.org/abs/2009.05495">Large spanning graphs with odd degrees</a></h3>



<p>Asaf Ferber and Michael Krivelevich solved an old conjecture in Graph theory. (In a 1994 paper by Yair Caro the problem is already referred to as part of the folklore of graph theory.)</p>



<p><strong>Abstract</strong>: We prove that every graph G on n vertices with no isolated vertices contains an induced subgraph of size at least n/10000 with all degrees odd. This solves an old and well-known conjecture in graph theory.</p>



<p>Here is <a href="https://www.quantamagazine.org/mathematicians-answer-old-question-about-odd-graphs-20210519/">an article about it in Quanta Magazine</a>. And here is a <a href="https://www.quantamagazine.org/new-proof-reveals-that-graphs-with-no-pentagons-are-fundamentally-different-20210426/">Quanta Magazine article</a> on the solution of the Erdos-Hajnal conjecture for pentagon-free graphs (that I mentioned in <a href="https://gilkalai.wordpress.com/2021/01/29/possible-future-polymath-projects-2009-2021/">this post</a>).</p>



<p/>



<h3><a href="https://arxiv.org/abs/2011.10939">The Betti numbers of the independence complex of a ternary graph</a></h3>



<p>A ternary graph (aka Trinity graph)  has no induced cycles of length divisible by three. Chudnovsky, Scott, Seymour and Spirkl <a href="https://arxiv.org/abs/1810.00065">proved</a> that for any ternary graph <em>G</em>, the number of independent sets with even cardinality and the independent sets with odd cardinality differ by at most 1. <a href="https://arxiv.org/search/?searchtype=author&amp;query=Wu%2C+H">Hehui Wu</a> and <a href="https://arxiv.org/search/?searchtype=author&amp;query=Zhang%2C+W">Wentao Zhang</a> <a href="https://arxiv.org/abs/2101.07131">proved the stronger conjecture</a> that for ternary graphs, the sum of Betti numbers of the independent complex is at most one. (Both conjectures were proposed by Roy Meshulam and me.) Congratulations to Hehui and Wentao and all other people mentioned in this post. </p>



<p>For related advances see <a href="https://arxiv.org/search/?searchtype=author&amp;query=Engstrom%2C+A">Alexander Engstrom</a>, <a href="https://arxiv.org/abs/2009.11077">On the topological Kalai-Meshulam conjecture</a>;  <a href="https://arxiv.org/search/?searchtype=author&amp;query=Kim%2C+J">Jinha Kim,</a> <a href="https://arxiv.org/abs/2101.07131">The homotopy type of the independence complex of graphs with no induced cycle of length divisible by three </a> <br/><br/>Here is <a href="https://web.math.princeton.edu/~pds/papers/chibounded/paper.pdf">a very nice survey by Paul Seymour and Alex Scott on chi-boundedness</a>. Here is an <a href="https://gilkalai.wordpress.com/2014/12/19/when-a-few-colors-suffice/">earlier post on Trinity graphs and chi-boundedness.</a></p>



<p/>



<p> </p>



<h2>A breakthrough in additive combinatorics<br/><br/></h2>



<figure class="wp-block-image is-resized"><img alt="THP" class="wp-image-21752" height="140" src="https://gilkalai.files.wordpress.com/2021/05/thp.jpg" width="96"/><strong>Huy Tuan Pham</strong></figure>



<p><a href="http://www.its.caltech.edu/~dconlon/">David Conlon</a>, <a href="https://stanford.edu/~jacobfox/">Jacob Fox</a> and <a href="https://web.stanford.edu/~huypham/">Huy Tuan Pham</a> used a new unified approach to solve a variety of open problems around subset sums including the open problems of Burr and Erdős on Ramsey complete sequences (Erdős offered $350 for their solutions) and a problem of Alon and Erdős on estimating the minimum number <img alt="f(n)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002"/> of colors needed to color the positive integers less than <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002"/> so that <img alt="n" class="latex" src="https://s0.wp.com/latex.php?latex=n&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002"/> cannot be written as a sum of elements of the same color. Conlon, Fox and Pham  proved that <img alt="f(n)" class="latex" src="https://s0.wp.com/latex.php?latex=f%28n%29&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002"/> is within a constant factor of <em>n<sup>4/3</sup>φ(n)<sup>-1</sup>(log n)<sup>-1/3</sup>(loglog n)<sup>-2/3</sup></em>, where <em>φ(n)</em> is the Euler totient function. The paper is: </p>



<h3>The paper is: <a href="https://arxiv.org/abs/2104.14766">Subset sums, completeness and colorings</a>,</h3>



<h2><br/>P-adic local Langlands</h2>



<h3><a href="https://www.galoisrepresentations.com/2021/03/09/test-your-intuition-p-adic-local-langlands-edition/">Test Your Intuition: p-adic local Langlands edition</a>!</h3>



<p>This post over Persiflage will test your intuition about 2-dimensional crystalline deformation rings. Since I couldn’t understand even the first sentence, I decided to ask my FB and HUJI friend to explain it to me, so I may try to write about it later. Actually, a better idea is that  Persiflage will try to give a vastly non-technical  explanation from scratch on p-adic local Langlands conjecture, and where it stands and why it matters. (To be clear, I am sure it is a great staff! And perhaps it is not possible to explain it to a wide audience. But it worth a try.)<br/><br/>Meanwhile it reminded me of the following story to which I apparently was a witness.</p>



<p>At the time of my birth, the medical approach was that mother and child needed to stay at the hospital for at least four nights and even for an entire whole week. After two nights at the hospital, my mother decided  that she was ready to go home and got into a lengthy discussion with her doctor about it. At some point the exhausted doctor told her “Madam, I studied seven years in medical school, and I know what is good for you.” My mother was not impressed and replied: “Had I studied seven years in medical school, I would  have been a medical doctor as well. And now, let me go home.” At the end, my mother (and myself) left the hospital on the third day. Years later this became the medical standard. (My own approach is <em>to follow </em>medical doctor’s instructions.)</p>



<h2>Mathematics over the media</h2>



<h3><a href="http://www.ramanujanmachine.com/">Ramanujan machine</a>: machine learning for producing mathematical conjectures</h3>



<p>This is a cool and very interesting direction by very very young Israeli mathematicians  that was reported all over the media. (See, <a href="https://www.nature.com/articles/s41586-021-03229-4">here for a Nature article</a>.)</p>



<p>(There are various groups all over the world attempting to use machine learning to produce mathematical conjectures.)</p>



<h3>A new approach for solving quadratic equations</h3>



<p>I heard it (I think it was a year ago) from a Facebook post written by the prime minister of Singapore! And then I saw it in other places all over the media (<a href="https://www.nytimes.com/2020/02/05/science/quadratic-equations-algebra.html">here on the NYT</a>). I like it, and, in fact, I like the many things <a href="https://www.math.cmu.edu/~ploh/cmu.shtml">Po-Shen Loh</a> (who is my grand academic nephew) does in the service of mathematics.</p>



<p/>



<p>***************</p>



<p><strong>Copenhagen-Jerusalem Combinatorics Seminar</strong><br/><br/>It is my pleasure to invite you to the following talk:<br/>**************************************************************************<br/><a>Thursday, May 20</a> Time: <a>16:00-18:00 CET, Jerusalem +1<br/></a> Location: Zoom: <a href="https://ucph-ku.zoom.us/j/69204766431" rel="noreferrer noopener" target="_blank"/><a href="https://ucph-ku.zoom.us/j/69204766431" rel="noreferrer noopener" target="_blank">https://ucph-ku.zoom.us/j/69204766431</a></p>



<p>************************************************************************** </p>



<p>Marc Lackenby:  Unknot recognition in quasi-polynomial time<br/>**************************************************************************<br/><br/>Abstract: </p>



<p>I will outline a new algorithm for unknot recognition that runs in quasi-polynomial time. The input is a diagram of a knot with n crossings, and the running time is 2^{O((log n)^3)}. The algorithm uses a wide variety of tools from 3-manifold theory, including normal surfaces, hierarchies and Heegaard splittings. In my talk, I will explain this background theory, as well as explain how it fits into the algorithm.</p>



<pre class="wp-block-preformatted"/>



<p/></div>
    </content>
    <updated>2021-05-20T11:56:34Z</updated>
    <published>2021-05-20T11:56:34Z</published>
    <category term="Algebra"/>
    <category term="Combinatorics"/>
    <category term="Geometry"/>
    <category term="Number theory"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2021-05-24T11:37:31Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://theorydish.blog/?p=1883</id>
    <link href="https://theorydish.blog/2021/05/19/entropy-estimation-via-two-chains-streamlining-the-proof-of-the-sunflower-lemma/" rel="alternate" type="text/html"/>
    <title>Entropy Estimation via Two Chains: Streamlining the Proof of the Sunflower Lemma</title>
    <summary>The sunflower lemma describes an interesting combinatorial property of set families: any large family of small sets must contain a large sunflower—a sub-family consisting of sets with a shared core and disjoint petals  (See the figure below for an example and a non-example of sunflowers.) The application of the lemma in theoretical computer science dates back to the influential paper by Razborov in 1985 that established the first super-polynomial monotone circuit lower bound for a function in NP, and since then the lemma has been applied broadly to other problems in theoretical computer science (see STOC version of Alweiss-Lovett-Wu-Zhang for a discussion of applications of the lemma in computer science). Left: 3 sets forming a sunflower. Right: 3 sets NOT forming a sunflower. The lemma was first proved by Erdős-Rado in 1960, who gave the quantitative bound that among distinct sets each of size at most one can find sets forming a sunflower. Erdos-Rado conjectured that the in the bound could be significantly improved to For nearly 60 years since the Erdős-Rado upper bound, all known upper bounds had had the dependence on even for despite much research effort. In 2019, a breakthrough work by Alweiss-Lovett-Wu-Zhang improved the bound significantly to [...]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The <a href="https://en.wikipedia.org/wiki/Sunflower_(mathematics)">sunflower lemma</a> describes an interesting combinatorial property of set families: any large family of small sets must contain a large <em>sunflower</em>—a sub-family consisting of sets <img alt="A_1,\ldots,A_r" class="latex" src="https://s0.wp.com/latex.php?latex=A_1%2C%5Cldots%2CA_r&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> with a shared <em>core</em> <img alt="S:=A_1\cap\cdots\cap A_r" class="latex" src="https://s0.wp.com/latex.php?latex=S%3A%3DA_1%5Ccap%5Ccdots%5Ccap+A_r&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and disjoint <em>petals</em> <img alt="A_1\backslash S,\ldots,A_r\backslash S." class="latex" src="https://s0.wp.com/latex.php?latex=A_1%5Cbackslash+S%2C%5Cldots%2CA_r%5Cbackslash+S.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (See the <a href="https://theorydish.blog/feed/#fig-sunflower">figure</a> below for an example and a non-example of sunflowers.) The application of the lemma in theoretical computer science dates back to the influential <a href="http://people.cs.uchicago.edu/~razborov/files/clique.pdf">paper</a> by Razborov in 1985 that established the first super-polynomial monotone circuit lower bound for a function in NP, and since then the lemma has been applied broadly to other problems in theoretical computer science (see <a href="https://dl.acm.org/doi/abs/10.1145/3357713.3384234">STOC version</a> of Alweiss-Lovett-Wu-Zhang for a discussion of applications of the lemma in computer science).</p>



<div class="wp-block-image" id="fig-sunflower"><figure class="aligncenter size-full is-resized"><img alt="" class="wp-image-2195" height="207" src="https://theorydish.files.wordpress.com/2021/04/sunflower_example-1.png" width="474"/>Left: 3 sets forming a sunflower. Right: 3 sets NOT forming a sunflower.</figure></div>



<p>The lemma was first proved by <a href="https://mathscinet.ams.org/mathscinet-getitem?mr=111692">Erdős-Rado</a> in 1960, who gave the quantitative bound that among <img alt="(r-1)^kk!+1" class="latex" src="https://s0.wp.com/latex.php?latex=%28r-1%29%5Ekk%21%2B1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> distinct sets each of size at most <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> one can find <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> sets forming a sunflower. Erdos-Rado conjectured that the <img alt="k!=k^{k(1 - o(1))} = k^{\Omega(k)}" class="latex" src="https://s0.wp.com/latex.php?latex=k%21%3Dk%5E%7Bk%281+-+o%281%29%29%7D+%3D+k%5E%7B%5COmega%28k%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in the bound could be significantly improved to <img alt="O(1)^k." class="latex" src="https://s0.wp.com/latex.php?latex=O%281%29%5Ek.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> For nearly 60 years since the Erdős-Rado upper bound, all known upper bounds had had the <img alt="k^{\Omega(k)}" class="latex" src="https://s0.wp.com/latex.php?latex=k%5E%7B%5COmega%28k%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> dependence on <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> even for <img alt="r = 3" class="latex" src="https://s0.wp.com/latex.php?latex=r+%3D+3&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> despite much research effort. In 2019, a breakthrough work by <a href="https://arxiv.org/abs/1908.08483">Alweiss-Lovett-Wu-Zhang</a> improved the bound significantly to <img alt="(Cr^3\log k\log\log_2 k)^k" class="latex" src="https://s0.wp.com/latex.php?latex=%28Cr%5E3%5Clog+k%5Clog%5Clog_2+k%29%5Ek&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> being at least <img alt="3" class="latex" src="https://s0.wp.com/latex.php?latex=3&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="C" class="latex" src="https://s0.wp.com/latex.php?latex=C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> being an absolute constant. The breakthrough led to many new results including improved monotone circuit lower bounds by <a href="https://arxiv.org/abs/2012.03883">Cavalar-Kumar-Rossman</a>.</p>



<p>Since the breakthrough of Alweiss-Lovett-Wu-Zhang, researchers have been refining the bound and simplifying the proof. The current best bound is <img alt="(Cr\log k)^k" class="latex" src="https://s0.wp.com/latex.php?latex=%28Cr%5Clog+k%29%5Ek&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for <img alt="k \ge 2" class="latex" src="https://s0.wp.com/latex.php?latex=k+%5Cge+2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> proved by Bell-Chueluecha-Warnke via a minor but powerful twist in the proof of an earlier <img alt="(Cr\log (rk))^k" class="latex" src="https://s0.wp.com/latex.php?latex=%28Cr%5Clog+%28rk%29%29%5Ek&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> <a href="https://arxiv.org/abs/1909.04774">bound</a> by Anup Rao:</p>



<p id="thm-1"><strong>Theorem 1</strong> (Sunflower Lemma <a href="https://arxiv.org/abs/2009.09327">[BCW’20]</a>).<em> There exists a constant <img alt="C&gt;0" class="latex" src="https://s0.wp.com/latex.php?latex=C%3E0&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> such that the following holds for all positive integers <img alt="k\ge 2" class="latex" src="https://s0.wp.com/latex.php?latex=k%5Cge+2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="r." class="latex" src="https://s0.wp.com/latex.php?latex=r.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Let <img alt="\mathcal F" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be a family of at least <img alt="(Cr\log k)^k" class="latex" src="https://s0.wp.com/latex.php?latex=%28Cr%5Clog+k%29%5Ek&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> distinct sets each of size at most <img alt="k." class="latex" src="https://s0.wp.com/latex.php?latex=k.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Then <img alt="\mathcal F" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> contains <img alt="r" class="latex" src="https://s0.wp.com/latex.php?latex=r&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> sets that form a sunflower.</em></p>



<p>The aim of this blog post is to present a streamlined proof of Theorem <a href="https://theorydish.blog/feed/#thm-1">1</a>. The proof is largely based on a <a href="https://terrytao.wordpress.com/2020/07/20/the-sunflower-lemma-via-shannon-entropy/">blog post</a> by Terence Tao where he presented an elegant proof of Rao’s result using <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">Shannon entropy</a>. However, Tao’s proof included a trick of <em>passing to a conditional copy twice</em>, which Tao described as <a href="https://terrytao.wordpress.com/2020/07/20/the-sunflower-lemma-via-shannon-entropy/\#comment-579138">“somewhat magical”</a>. <strong>We show here that the trick is not necessary for the proof, and avoiding the trick gives a simpler proof with a slightly better constant in the bound.</strong></p>



<p>We start by defining <img alt="R" class="latex" src="https://s0.wp.com/latex.php?latex=R&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-spread families, a notion key to all recent proofs of the sunflower lemma. We use <img alt="[N]" class="latex" src="https://s0.wp.com/latex.php?latex=%5BN%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> as a shorthand for <img alt="\{1,\ldots,N\}." class="latex" src="https://s0.wp.com/latex.php?latex=%5C%7B1%2C%5Cldots%2CN%5C%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> We use boldface symbols to denote random variables, and non-boldface ones to denote deterministic quantities.</p>



<p><strong>Definition 1</strong> (Spread family). <em>Let <img alt="\mathcal F" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be a family of finite sets <img alt="A_1,\ldots,A_N" class="latex" src="https://s0.wp.com/latex.php?latex=A_1%2C%5Cldots%2CA_N&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that are not necessarily distinct. For <img alt="R &gt; 1" class="latex" src="https://s0.wp.com/latex.php?latex=R+%3E+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, we say <img alt="\mathcal F" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is <img alt="R" class="latex" src="https://s0.wp.com/latex.php?latex=R&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-spread if for all <img alt="S\subseteq \cup_{n=1}^NA_n," class="latex" src="https://s0.wp.com/latex.php?latex=S%5Csubseteq+%5Ccup_%7Bn%3D1%7D%5ENA_n%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></em></p>



<p class="has-text-align-center"><em><img alt="\begin{aligned} \Pr[S\subseteq A_{\mathbf n}] \le R^{-|S|},\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5CPr%5BS%5Csubseteq+A_%7B%5Cmathbf+n%7D%5D+%5Cle+R%5E%7B-%7CS%7C%7D%2C%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></em></p>



<p><em>where <img alt="\mathbf n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is chosen uniformly at random from <img alt="[N]." class="latex" src="https://s0.wp.com/latex.php?latex=%5BN%5D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></em></p>



<p>The main technical part of recent improvements in the sunflower lemma happens in the proof of the following refinement lemma. We use the base-<img alt="2" class="latex" src="https://s0.wp.com/latex.php?latex=2&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> logarithm throughout.</p>



<p id="lm-2"><strong>Lemma 2</strong> (Refinement). <em>Let <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be a finite set. <em>For <img alt="R &gt; 1" class="latex" src="https://s0.wp.com/latex.php?latex=R+%3E+1&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></em>, let <img alt="\mathcal F" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathcal+F&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be an <img alt="R" class="latex" src="https://s0.wp.com/latex.php?latex=R&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-spread family of sets <img alt="A_1,\ldots,A_N\subseteq X." class="latex" src="https://s0.wp.com/latex.php?latex=A_1%2C%5Cldots%2CA_N%5Csubseteq+X.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></em> <em>Let <img alt="\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be a size-<img alt="\delta|X|" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%7CX%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> subset of <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> chosen uniformly at random for <img alt="\delta\in(1/R,1]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%5Cin%281%2FR%2C1%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> assuming <img alt="\delta|X|" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%7CX%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is an integer. Let <img alt="\mathbf n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be a uniform random number in <img alt="[N]" class="latex" src="https://s0.wp.com/latex.php?latex=%5BN%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> independent of <img alt="\mathbf W." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> There is a random number <img alt="\mathbf n'" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in <img alt="[N]" class="latex" src="https://s0.wp.com/latex.php?latex=%5BN%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (being not independent from <img alt="(\mathbf n,\mathbf W)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf+n%2C%5Cmathbf+W%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in general) such that <img alt="A_{\mathbf n'}\subseteq A_{\mathbf n}\cup\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=A_%7B%5Cmathbf+n%27%7D%5Csubseteq+A_%7B%5Cmathbf+n%7D%5Ccup%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> almost surely and </em></p>



<p class="has-text-align-center"><em><img alt="\begin{aligned}\mathbb E|A_{\mathbf n'}\backslash \mathbf W|\le\frac{4}{\log(R\delta)}\mathbb E|A_{\mathbf n}|.\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%5Cmathbb+E%7CA_%7B%5Cmathbf+n%27%7D%5Cbackslash+%5Cmathbf+W%7C%5Cle%5Cfrac%7B4%7D%7B%5Clog%28R%5Cdelta%29%7D%5Cmathbb+E%7CA_%7B%5Cmathbf+n%7D%7C.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></em></p>



<p>The proof of Theorem <a href="https://theorydish.blog/feed/#thm-1">1</a> using Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> can be found in Rao’s <a href="https://arxiv.org/abs/1909.04774">paper</a> and Tao’s <a href="https://terrytao.wordpress.com/2020/07/20/the-sunflower-lemma-via-shannon-entropy/">blog post</a>, so we omit it here and focus on proving Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a>. Rao and Tao both proved a slightly weaker version of Theorem <a href="https://theorydish.blog/feed/#thm-1">1</a> but this weakness can be overcome using a minor twist observed by <a href="https://arxiv.org/abs/2009.09327">Bell-Chueluecha-Warnke</a>. They also used slightly different forms of Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> but the differences are non-essential.</p>



<p>It is easy to see that in Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> one can convert <img alt="\mathbf n'" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> to a deterministic function of <img alt="\mathbf n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> without violating any property of the original <img alt="\mathbf n'" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> guaranteed by the lemma, because after conditioning on <img alt="\mathbf n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> one can fix any additional randomness in <img alt="\mathbf n'" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> so that <img alt="|A_{\mathbf n'}\backslash \mathbf W|" class="latex" src="https://s0.wp.com/latex.php?latex=%7CA_%7B%5Cmathbf+n%27%7D%5Cbackslash+%5Cmathbf+W%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is minimized. This more deterministic version of <img alt="\mathbf n'" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> can be somewhat more convenient for proving Theorem <a href="https://theorydish.blog/feed/#thm-1">1</a> and it was used in Rao’s proof, but allowing additional randomness in <img alt="\mathbf n'" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> enabled Tao to obtain a simpler proof of Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> using a construction of <img alt="\mathbf n'" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that is easier to analyze.</p>



<p>We follow Tao’s idea of proving Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> using Shannon entropy, but we present the proof in a more streamlined fashion with a slightly sharper constant in the bound (Tao proved a version of Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> where the constant 4 was replaced by 5). Specifically, we present the proof in a way resembling a basic technique in combinatorics called <em><a href="https://en.wikipedia.org/wiki/Double_counting_(proof_technique)">counting in two ways</a></em>: one can show that two quantities are equal by showing that they both count the number of elements in the same set. Here, we estimate the entropy of the same collection of random variables in two different ways, and prove Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a> by comparing the two estimates. The way we obtain the two estimates relies crucially on the chain rule of conditional entropy:</p>



<p class="has-text-align-center" id="eq-1"><img alt="\begin{aligned} \mathbb H(\mathbf a_1,\ldots,\mathbf a_m) = \sum_{i=1}^m\mathbb H(\mathbf a_i|\mathbf a_1,\ldots,\mathbf a_{i-1}). &amp;&amp;  (1) \end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmathbb+H%28%5Cmathbf+a_1%2C%5Cldots%2C%5Cmathbf+a_m%29+%3D+%5Csum_%7Bi%3D1%7D%5Em%5Cmathbb+H%28%5Cmathbf+a_i%7C%5Cmathbf+a_1%2C%5Cldots%2C%5Cmathbf+a_%7Bi-1%7D%29.+%26%26++%281%29+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>Equation <a href="https://theorydish.blog/feed/#eq-1">(1)</a> holds for arbitrary random variables <img alt="\mathbf a_1,\ldots,\mathbf a_m" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+a_1%2C%5Cldots%2C%5Cmathbf+a_m&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> taking values in a discret set. We say that equation <a href="https://theorydish.blog/feed/#eq-1">(1)</a> computes the entropy of <img alt="(\mathbf a_1,\ldots,\mathbf a_m)" class="latex" src="https://s0.wp.com/latex.php?latex=%28%5Cmathbf+a_1%2C%5Cldots%2C%5Cmathbf+a_m%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> via the chain </p>



<p class="has-text-align-center"><img alt="\begin{aligned}\mathbf a_1\rightarrow \cdots \rightarrow \mathbf a_m.\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%5Cmathbf+a_1%5Crightarrow+%5Ccdots+%5Crightarrow+%5Cmathbf+a_m.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>To prove Lemma <a href="https://theorydish.blog/feed/#lm-2">2</a>, we obtain two entropy estimates for the same collection of random variables by applying <a href="https://theorydish.blog/feed/#eq-1">(1)</a> to two different chains.</p>



<p>We need the following useful lemmas about Shannon entropy. We omit their proofs here as they can be found in Tao’s <a href="https://terrytao.wordpress.com/2020/07/20/the-sunflower-lemma-via-shannon-entropy/">blog post</a>, where many basic properties of Shannon entropy are also discussed. (We also highly recommend Tao’s other blog posts about <a href="https://terrytao.wordpress.com/2017/03/01/special-cases-of-shannon-entropy/">Shannon entropy</a> and the <a href="https://terrytao.wordpress.com/2009/08/05/mosers-entropy-compression-argument/">entropy compression argument</a>.)</p>



<p id="lm-3"><strong>Lemma 3</strong> (Subsets of small sets have small conditional entropy). <em>Let <img alt="\mathbf A,\mathbf B" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A%2C%5Cmathbf+B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be finite random sets. Assume <img alt="\mathbf A\subseteq \mathbf B" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A%5Csubseteq+%5Cmathbf+B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> almost surely. Then <img alt="\mathbb H(\mathbf A|\mathbf B)\le \mathbb E|\mathbf B|." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+H%28%5Cmathbf+A%7C%5Cmathbf+B%29%5Cle+%5Cmathbb+E%7C%5Cmathbf+B%7C.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></em></p>



<p id="lm-4"><strong>Lemma 4</strong> (Information-theoretic interpretation of spread). <em>Let <img alt="A_1,\ldots,A_N" class="latex" src="https://s0.wp.com/latex.php?latex=A_1%2C%5Cldots%2CA_N&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be an <img alt="R" class="latex" src="https://s0.wp.com/latex.php?latex=R&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>-spread family of finite sets. Let <img alt="\mathbf n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be chosen uniformly at random from <img alt="[N]." class="latex" src="https://s0.wp.com/latex.php?latex=%5BN%5D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> Let <img alt="\mathbf S" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+S&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be a random set satisfying <img alt="\mathbf S\subseteq A_{\mathbf n}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+S%5Csubseteq+A_%7B%5Cmathbf+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> almost surely. Then</em></p>



<p class="has-text-align-center"><em><img alt="\begin{aligned} \mathbb H(\mathbf n|\mathbf S) \le \mathbb H(\mathbf n) - (\log R) \mathbb E|\mathbf S|.\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%5Cmathbb+H%28%5Cmathbf+n%7C%5Cmathbf+S%29+%5Cle+%5Cmathbb+H%28%5Cmathbf+n%29+-+%28%5Clog+R%29+%5Cmathbb+E%7C%5Cmathbf+S%7C.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></em></p>



<p id="lm-5"><strong>Lemma 5</strong> (Information-theoretic properties of uniformly random subsets of fixed size). <em>Let <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be a finite set. Let <img alt="\mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be a size-<img alt="\delta |X|" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%7CX%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> subset of <img alt="X" class="latex" src="https://s0.wp.com/latex.php?latex=X&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> chosen uniformly at random for <img alt="\delta\in (0,1]" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta%5Cin+%280%2C1%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> assuming <img alt="\delta |X|" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdelta+%7CX%7C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is an integer. Let <img alt="\mathbf A" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> be a random subset of <img alt="X." class="latex" src="https://s0.wp.com/latex.php?latex=X.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> </em>The following inequalities hold:</p>



<ol><li><em>(absorption) <img alt="\mathbb H(\mathbf A\cup \mathbf W) \le \mathbb H(\mathbf W) + 1 + (1 + \log(1/\delta))\mathbb E|\mathbf A|;" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+H%28%5Cmathbf+A%5Ccup+%5Cmathbf+W%29+%5Cle+%5Cmathbb+H%28%5Cmathbf+W%29+%2B+1+%2B+%281+%2B+%5Clog%281%2F%5Cdelta%29%29%5Cmathbb+E%7C%5Cmathbf+A%7C%3B&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></em></li><li><em>(spread) if <img alt="\mathbf A\subseteq \mathbf W" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+A%5Csubseteq+%5Cmathbf+W&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> almost surely, then <img alt="\mathbb H(\mathbf W|\mathbf A)\le \mathbb H(\mathbf W) - \log(1/\delta)\mathbb E|\mathbf A|." class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbb+H%28%5Cmathbf+W%7C%5Cmathbf+A%29%5Cle+%5Cmathbb+H%28%5Cmathbf+W%29+-+%5Clog%281%2F%5Cdelta%29%5Cmathbb+E%7C%5Cmathbf+A%7C.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></em></li></ol>



<p><em>Proof of Lemma </em><a href="https://theorydish.blog/feed/#lm-2">2</a>. If there exists <img alt="n\in[N]" class="latex" src="https://s0.wp.com/latex.php?latex=n%5Cin%5BN%5D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> such that <img alt="A_n" class="latex" src="https://s0.wp.com/latex.php?latex=A_n&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is empty, we can simply choose <img alt="\mathbf n'=n" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cmathbf+n%27%3Dn&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> deterministically. We assume henceforth that <img alt="A_n\neq \emptyset" class="latex" src="https://s0.wp.com/latex.php?latex=A_n%5Cneq+%5Cemptyset&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for all <img alt="n\in[N]." class="latex" src="https://s0.wp.com/latex.php?latex=n%5Cin%5BN%5D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>Following Tao’s proof, we construct <img alt="{\mathbf n}'" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf+n%7D%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> by creating a conditionally independent copy <img alt="({\mathbf n}',{\mathbf W}')" class="latex" src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of <img alt="({\mathbf n},{\mathbf W})" class="latex" src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> given <img alt="{A_{{\mathbf n}'}}\cup {\mathbf W}' = {A_{{\mathbf n}}}\cup{\mathbf W}." class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Ccup+%7B%5Cmathbf+W%7D%27+%3D+%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> In other words, <img alt="({\mathbf n}',{\mathbf W}')" class="latex" src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="({\mathbf n},{\mathbf W})" class="latex" src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> have the same conditional distribution given <img alt="{A_{{\mathbf n}}}\cup{\mathbf W}," class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and they are also conditionally independent given <img alt="{A_{{\mathbf n}}}\cup{\mathbf W}." class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> This construction guarantees that <img alt="{A_{{\mathbf n}'}}\subseteq {A_{{\mathbf n}}}\cup{\mathbf W}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Csubseteq+%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> almost surely, which implies that <img alt="A_{\mathbf n'}\backslash \mathbf W\subseteq A_{\mathbf n}\cap A_{\mathbf n'}" class="latex" src="https://s0.wp.com/latex.php?latex=A_%7B%5Cmathbf+n%27%7D%5Cbackslash+%5Cmathbf+W%5Csubseteq+A_%7B%5Cmathbf+n%7D%5Ccap+A_%7B%5Cmathbf+n%27%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> almost surely.</p>



<p>It remains to prove that the <img alt="{\mathbf n}'" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf+n%7D%27&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> constructed as above satisfies <img alt="{\mathbb E}|A_{\mathbf n}\cap A_{\mathbf n'}|\le \frac 4{\log(R\delta)}{\mathbb E}|{A_{{\mathbf n}}}|." class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb+E%7D%7CA_%7B%5Cmathbf+n%7D%5Ccap+A_%7B%5Cmathbf+n%27%7D%7C%5Cle+%5Cfrac+4%7B%5Clog%28R%5Cdelta%29%7D%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C.&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> We achieve this by estimating <img alt="{\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}')" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> using the chain rule <a href="https://theorydish.blog/feed/#eq-1">(1)</a> via two different chains, one for a lower bound and the other for an upper bound. The lower bound is obtained via the following chain:</p>



<p class="has-text-align-center"><img alt="\begin{aligned}{\mathbf n},{\mathbf W}\rightarrow {\mathbf n}',{\mathbf W}'. \end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%5Crightarrow+%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27.+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>Namely, we apply <a href="https://theorydish.blog/feed/#eq-1">(1)</a> in the following way:</p>



<p class="has-text-align-center" id="eq-2"><img alt="\begin{aligned}{\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}') = {\mathbb H}({\mathbf n},{\mathbf W}) + {\mathbb H}({\mathbf n}',{\mathbf W}'|{\mathbf n},{\mathbf W}). &amp;&amp; (2)\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29+%3D+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29.+%26%26+%282%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>By the independence of <img alt="{\mathbf n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{\mathbf W}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf+W%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>,</p>



<p class="has-text-align-center" id="eq-3"><img alt="\begin{aligned}{\mathbb H}({\mathbf n},{\mathbf W}) = {\mathbb H}({\mathbf n}) + {\mathbb H}({\mathbf W}). &amp;&amp; (3)\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29+%3D+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%29.+%26%26+%283%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>By the conditional independence of <img alt="({\mathbf n}',{\mathbf W}')" class="latex" src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="({\mathbf n},{\mathbf W})" class="latex" src="https://s0.wp.com/latex.php?latex=%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> given <img alt="{A_{{\mathbf n}}}\cup{\mathbf W}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and their identical conditional distribution,</p>



<p class="has-text-align-center" id="eq-4"><img alt="\begin{aligned} &amp; \mathbb H({\mathbf n}',{\mathbf W}'|{\mathbf n},{\mathbf W}) \\ = {} &amp; {\mathbb H}({\mathbf n}',{\mathbf W}'|{\mathbf n},{\mathbf W},{A_{{\mathbf n}}}\cup {\mathbf W})\\ = {} &amp; {\mathbb H}({\mathbf n}',{\mathbf W}'|{A_{{\mathbf n}}}\cup {\mathbf W})\\ = {} &amp; {\mathbb H}({\mathbf n},{\mathbf W}|{A_{{\mathbf n}}}\cup {\mathbf W})\\ = {} &amp; {\mathbb H}({\mathbf n},{\mathbf W}) - {\mathbb H}({A_{{\mathbf n}}}\cup{\mathbf W})\\ = {} &amp; {\mathbb H}({\mathbf n}) + {\mathbb H}({\mathbf W}) - {\mathbb H}({A_{{\mathbf n}}}\cup{\mathbf W})\\ \ge {} &amp; {\mathbb H}({\mathbf n}) - (2 + \log(1/\delta)){\mathbb E}|{A_{{\mathbf n}}}|,&amp; (4) \end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%5Cmathbb+H%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29+%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup+%7B%5Cmathbf+W%7D%29%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup+%7B%5Cmathbf+W%7D%29%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup+%7B%5Cmathbf+W%7D%29%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%29+-+%7B%5Cmathbb+H%7D%28%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D%29%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%29+-+%7B%5Cmathbb+H%7D%28%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D%29%5C%5C+%5Cge+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29+-+%282+%2B+%5Clog%281%2F%5Cdelta%29%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C%2C%26+%284%29+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>where the last inequality is by Lemma <a href="https://theorydish.blog/feed/#lm-5">5</a> Item 1. Plugging <a href="https://theorydish.blog/feed/#eq-3">(3)</a>, <a href="https://theorydish.blog/feed/#eq-4">(4)</a> into <a href="https://theorydish.blog/feed/#eq-2">(2)</a>, we get the following lower bound:</p>



<p class="has-text-align-center" id="eq-5"><img alt="\begin{aligned}&amp;{\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}') \\ \ge {} &amp; 2{\mathbb H}({\mathbf n}) + {\mathbb H}({\mathbf W}) - (2 + \log(1/\delta)){\mathbb E}|{A_{{\mathbf n}}}|. &amp; (5)\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%26%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29+%5C%5C+%5Cge+%7B%7D+%26+2%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%29+-+%282+%2B+%5Clog%281%2F%5Cdelta%29%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C.+%26+%285%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>Now we establish an upper bound for <img alt="{\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}')" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> via a different chain:</p>



<p class="has-text-align-center"><img alt="\begin{aligned}{\mathbf n}\rightarrow {A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}\rightarrow {\mathbf n}'\rightarrow {\mathbf W}\rightarrow {\mathbf W}'.\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7B%5Cmathbf+n%7D%5Crightarrow+%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Crightarrow+%7B%5Cmathbf+n%7D%27%5Crightarrow+%7B%5Cmathbf+W%7D%5Crightarrow+%7B%5Cmathbf+W%7D%27.%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>Namely, we apply <a href="https://theorydish.blog/feed/#eq-1">(1)</a> in the following manner:</p>



<p class="has-text-align-center" id="eq-6"><img alt="\begin{aligned} &amp; {\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}')\\ = {} &amp; {\mathbb H}({\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}',{\mathbf W},{\mathbf W}')\\= {} &amp; {\mathbb H}({\mathbf n})\\&amp; + {\mathbb H}({A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}|{\mathbf n})\\&amp; + {\mathbb H}({\mathbf n}'|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}})\\&amp; + {\mathbb H}({\mathbf W}|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}')\\&amp; + {\mathbb H}({\mathbf W}'|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}',{\mathbf W}).&amp;(6)\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29%5C%5C+%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+W%7D%27%29%5C%5C%3D+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29%5C%5C%26+%2B+%7B%5Cmathbb+H%7D%28%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C%7B%5Cmathbf+n%7D%29%5C%5C%26+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%29%5C%5C%26+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%29%5C%5C%26+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%29.%26%286%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>By Lemma <a href="https://theorydish.blog/feed/#lm-3">3</a> and <img alt="{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}\subseteq {A_{{\mathbf n}}}," class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Csubseteq+%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p class="has-text-align-center" id="eq-7"><img alt="\begin{aligned}{\mathbb H}({A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}|{\mathbf n})\le{\mathbb E}|{A_{{\mathbf n}}}|. &amp;&amp; (7) \end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%7B%5Cmathbb+H%7D%28%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C%7B%5Cmathbf+n%7D%29%5Cle%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C.+%26%26+%287%29+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>By Lemma <a href="https://theorydish.blog/feed/#lm-4">4</a> and <img alt="{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}\subseteq {A_{{\mathbf n}'}}," class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Csubseteq+%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p class="has-text-align-center" id="eq-8"><img alt="\begin{aligned} &amp; {\mathbb H}({\mathbf n}'|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}})\\ \le {} &amp; {\mathbb H}({\mathbf n}'|{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}})\\ \le {} &amp; {\mathbb H}({\mathbf n}') - (\log R){\mathbb E}|{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}|. &amp; (8) \end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%29%5C%5C+%5Cle+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%29%5C%5C+%5Cle+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%29+-+%28%5Clog+R%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C.+%26+%288%29+%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>By Lemma <a href="https://theorydish.blog/feed/#lm-5">5</a> Item 2 and <img alt="{A_{{\mathbf n}'}}\backslash{A_{{\mathbf n}}}\subseteq {\mathbf W}," class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Csubseteq+%7B%5Cmathbf+W%7D%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p class="has-text-align-center" id="eq-9"><img alt="\begin{aligned}&amp;{\mathbb H}({\mathbf W}|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}') \\  \le {} &amp; {\mathbb H}({\mathbf W}|{A_{{\mathbf n}'}}\backslash{A_{{\mathbf n}}}) \\ \le {} &amp; {\mathbb H}({\mathbf W}) - (\log (1/\delta)){\mathbb E}|{A_{{\mathbf n}'}}\backslash{A_{{\mathbf n}}}|. &amp; (9)\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%26%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%29+%5C%5C++%5Cle+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%29+%5C%5C+%5Cle+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%29+-+%28%5Clog+%281%2F%5Cdelta%29%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C.+%26+%289%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>Since <img alt="{\mathbf W}' = ({A_{{\mathbf n}}}\cup{\mathbf W})\backslash({A_{{\mathbf n}'}}\backslash{\mathbf W}')" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbf+W%7D%27+%3D+%28%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccup%7B%5Cmathbf+W%7D%29%5Cbackslash%28%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7B%5Cmathbf+W%7D%27%29&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{A_{{\mathbf n}'}}\backslash{\mathbf W}'\subseteq {A_{{\mathbf n}'}}," class="latex" src="https://s0.wp.com/latex.php?latex=%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7B%5Cmathbf+W%7D%27%5Csubseteq+%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> by Lemma <a href="https://theorydish.blog/feed/#lm-3">3</a>,</p>



<p class="has-text-align-center" id="eq-10"><img alt="\begin{aligned}&amp; {\mathbb H}({\mathbf W}'|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}',{\mathbf W}) \\ \le {} &amp; {\mathbb H}({A_{{\mathbf n}'}}\backslash{\mathbf W}'|{\mathbf n},{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}},{\mathbf n}',{\mathbf W})\\ \le {} &amp; {\mathbb E}|{A_{{\mathbf n}'}}|. &amp;(10)\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%26+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%29+%5C%5C+%5Cle+%7B%7D+%26+%7B%5Cmathbb+H%7D%28%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7B%5Cmathbf+W%7D%27%7C%7B%5Cmathbf+n%7D%2C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%29%5C%5C+%5Cle+%7B%7D+%26+%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C.+%26%2810%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>Plugging <a href="https://theorydish.blog/feed/#eq-7">(7)</a>, <a href="https://theorydish.blog/feed/#eq-8">(8)</a>, <a href="https://theorydish.blog/feed/#eq-9">(9)</a>, <a href="https://theorydish.blog/feed/#eq-10">(10)</a> into <a href="https://theorydish.blog/feed/#eq-6">(6)</a> and simplifying using</p>



<p class="has-text-align-center"><img alt="\begin{aligned}&amp;{\mathbb H}({\mathbf n}') = {\mathbb H}({\mathbf n}),\quad {\mathbb E}|{A_{{\mathbf n}'}}| = {\mathbb E}|{A_{{\mathbf n}}}|,\\&amp;{\mathbb E}|{A_{{\mathbf n}'}}\backslash{A_{{\mathbf n}}}| = {\mathbb E}|{A_{{\mathbf n}'}}| - {\mathbb E}|{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}|,\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%26%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%27%29+%3D+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29%2C%5Cquad+%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C+%3D+%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C%2C%5C%5C%26%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%5Cbackslash%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C+%3D+%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C+-+%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C%2C%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>we get the following upper bound:</p>



<p class="has-text-align-center" id="eq-11"><img alt="\begin{aligned}&amp;{\mathbb H}({\mathbf n},{\mathbf W},{\mathbf n}',{\mathbf W}') \\ \le {} &amp; 2{\mathbb H}({\mathbf n}) + {\mathbb H}({\mathbf W}) + (2 - \log(1/\delta)){\mathbb E}|{A_{{\mathbf n}}}| \\ &amp; - (\log (R\delta)){\mathbb E}|{A_{{\mathbf n}}}\cap{A_{{\mathbf n}'}}|. &amp; (11)\end{aligned}" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D%26%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%2C%7B%5Cmathbf+W%7D%2C%7B%5Cmathbf+n%7D%27%2C%7B%5Cmathbf+W%7D%27%29+%5C%5C+%5Cle+%7B%7D+%26+2%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+n%7D%29+%2B+%7B%5Cmathbb+H%7D%28%7B%5Cmathbf+W%7D%29+%2B+%282+-+%5Clog%281%2F%5Cdelta%29%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C+%5C%5C+%26+-+%28%5Clog+%28R%5Cdelta%29%29%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%5Ccap%7BA_%7B%7B%5Cmathbf+n%7D%27%7D%7D%7C.+%26+%2811%29%5Cend%7Baligned%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p>Comparing <a href="https://theorydish.blog/feed/#eq-5">(5)</a> and <a href="https://theorydish.blog/feed/#eq-11">(11)</a> proves the desired inequality <img alt="{\mathbb E}|A_{\mathbf n}\cap A_{\mathbf n'}|\le \frac 4{\log(R\delta)}{\mathbb E}|{A_{{\mathbf n}}}|.\square" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmathbb+E%7D%7CA_%7B%5Cmathbf+n%7D%5Ccap+A_%7B%5Cmathbf+n%27%7D%7C%5Cle+%5Cfrac+4%7B%5Clog%28R%5Cdelta%29%7D%7B%5Cmathbb+E%7D%7C%7BA_%7B%7B%5Cmathbf+n%7D%7D%7D%7C.%5Csquare&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>



<p><strong>Acknowledgments. </strong>I would like to thank my quals committee, Moses Charikar, Omer Reingold, and Li-Yang Tan for valuable feedback and inspiring discussions.</p></div>
    </content>
    <updated>2021-05-19T23:04:03Z</updated>
    <published>2021-05-19T23:04:03Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Lunjia Hu</name>
    </author>
    <source>
      <id>https://theorydish.blog</id>
      <logo>https://theorydish.files.wordpress.com/2017/03/cropped-nightdish1.jpg?w=32</logo>
      <link href="https://theorydish.blog/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://theorydish.blog" rel="alternate" type="text/html"/>
      <link href="https://theorydish.blog/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://theorydish.blog/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Stanford's CS Theory Research Blog</subtitle>
      <title>Theory Dish</title>
      <updated>2021-05-24T11:38:41Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/05/19/tenure-track-assistant-professorship-in-algorithms-and-complexity-theory-at-lund-university-apply-by-june-8-2021/</id>
    <link href="https://cstheory-jobs.org/2021/05/19/tenure-track-assistant-professorship-in-algorithms-and-complexity-theory-at-lund-university-apply-by-june-8-2021/" rel="alternate" type="text/html"/>
    <title>Tenure-track assistant professorship in algorithms and complexity theory at Lund University (apply by June 8, 2021)</title>
    <summary>The CS department at Lund University invites applications for a tenure-track assistant professorship in algorithms and complexity theory. The application deadline is June 8, 2021. See https://lu.varbi.com/en/what:job/jobID:388991/ for the full announcement with more information and instructions for how to apply. Informal enquiries are welcome and may be sent to jakob.nordstrom@cs.lth.se. Website: https://lu.varbi.com/en/what:job/jobID:388991/ Email: jakob.nordstrom@cs.lth.se</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The CS department at Lund University invites applications for a tenure-track assistant professorship in algorithms and complexity theory. The application deadline is June 8, 2021. See <a href="https://lu.varbi.com/en/what:job/jobID:388991/">https://lu.varbi.com/en/what:job/jobID:388991/</a> for the full announcement with more information and instructions for how to apply. Informal enquiries are welcome and may be sent to jakob.nordstrom@cs.lth.se.</p>
<p>Website: <a href="https://lu.varbi.com/en/what:job/jobID:388991/">https://lu.varbi.com/en/what:job/jobID:388991/</a><br/>
Email: jakob.nordstrom@cs.lth.se</p></div>
    </content>
    <updated>2021-05-19T22:03:56Z</updated>
    <published>2021-05-19T22:03:56Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-05-24T11:37:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8124</id>
    <link href="https://windowsontheory.org/2021/05/19/workshop-on-local-algorithms-guest-post-by-ronitt-rubinfeld/" rel="alternate" type="text/html"/>
    <title>Workshop on Local Algorithms (Guest post by Ronitt Rubinfeld)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The fifth WOLA (Workshop on Local Algorithms) will be virtual, and take place June 14-15. Registration is free, but required: please fill this form by June 10th to attend. Local algorithms — that is, algorithms that compute and make decisions on parts of the output considering only a portion of the input — have been studied in a number of … <a class="more-link" href="https://windowsontheory.org/2021/05/19/workshop-on-local-algorithms-guest-post-by-ronitt-rubinfeld/">Continue reading <span class="screen-reader-text">Workshop on Local Algorithms (Guest post by Ronitt Rubinfeld)</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The fifth <a href="http://www.local-algorithms.com/" rel="noreferrer noopener" target="_blank">WOLA</a> (Workshop on Local Algorithms) will be virtual, and take place June 14-15. <strong>Registration is free</strong>, but required: please fill <a href="https://docs.google.com/forms/d/e/1FAIpQLSdot9yskpr-9DPJ2UK6iuYdzlX25w6tPy6H5UKHEVIR-yTLHg/viewform" rel="noreferrer noopener" target="_blank">this form</a> by June 10th to attend.</p>



<p><em>Local algorithms — that is, algorithms that compute and make decisions on parts of the output considering only a portion of the input — have been studied in a number of areas in theoretical computer science and mathematics. Some of the related areas include sublinear-time algorithms, distributed algorithms, streaming algorithms, (massively) parallel algorithms, inference in large networks, and graphical models. These communities have similar goals but a variety of approaches, techniques, and methods. This workshop is aimed at fostering dialogue and cross-pollination of ideas between the various communities.</em></p>



<p>This year, the workshop will include:</p>



<ul><li><strong>A poster session</strong>: Please submit <a href="http://www.local-algorithms.com/?page=call-for-posters" rel="noreferrer noopener" target="_blank">your poster proposal</a> (title and abstract) at by<strong> May 26th</strong>. Everyone is invited to contribute. This session will take place on gather.town.</li><li><strong>Invited long talks</strong>: the tentative schedule is <a href="http://www.local-algorithms.com/?page=schedule" rel="noreferrer noopener" target="_blank">available</a>, and features talks by James Aspnes, Jelani Nelson, Elaine Shi, Christian Sohler, Uri Stemmer, and Mary Wootters.</li><li><strong>Junior-Senior social meetings</strong></li><li><strong>An AMA (Ask Me Anything) session</strong>, moderated by Merav Parter</li><li><strong>A Slack channel</strong></li><li><strong>An Open Problems session</strong></li></ul>



<p>The Program Committee of WOLA 2021 is comprised of:</p>



<ul><li>Venkatesan Guruswami (CMU)</li><li>Elchanan Mossel (MIT)</li><li>Merav Parter (Weizmann Institute of Science)</li><li>Sofya Raskhodnikova <strong>(chair) </strong>(Boston University)</li><li>Gregory Valiant (Stanford)</li></ul>



<p>and the organizing committee:</p>



<ul><li>Sebastian Brandt (ETH)</li><li>Yannic Maus (Technion)</li><li>Slobodan Mitrović (MIT)</li></ul>



<p>For more detail, see <a href="http://www.local-algorithms.com/?" rel="noreferrer noopener" target="_blank">the website</a>;</p></div>
    </content>
    <updated>2021-05-19T17:56:50Z</updated>
    <published>2021-05-19T17:56:50Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-05-24T11:38:03Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://nisheethvishnoi.wordpress.com/?p=107</id>
    <link href="https://nisheethvishnoi.wordpress.com/2021/05/19/submit-your-papers-to-the-62nd-focs/" rel="alternate" type="text/html"/>
    <title>Submit your papers to the 62nd FOCS!</title>
    <summary>The submission server for the 62nd FOCS is open! Please read the Call for Papers carefully before you submit. Below are some important points: The title/abstract registration deadline is 5 pm EDT on May 31st, 2021. Since this information will be used in paper assignments, significant changes to the title/abstract will not be allowed after […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The <a href="https://easychair.org/conferences/?conf=focs2021">submission server</a> for the 62nd FOCS is open! </p>



<p>Please read the <a href="https://focs2021.cs.colorado.edu/cfp/">Call for Papers</a> carefully before you submit. </p>



<p>Below are some important points:</p>



<ul><li>The title/abstract registration deadline is <strong>5 pm EDT</strong> on <strong>May 31st, 2021</strong>. Since this information will be used in paper assignments, significant changes to the title/abstract will not be allowed after this deadline.</li><li>The full paper submission deadline is <strong>5 pm EDT</strong> on <strong>June 3, 2021</strong>.</li><li>The conference is expected to take place <strong>physically</strong> in Boulder, Colorado <strong>Feb 7-10, 2022</strong>.</li><li>Papers that broaden the reach of the theory of computing, make foundational connections to other areas, or raise important problems and demonstrate that they can benefit from theoretical investigation and analysis are especially encouraged.</li><li>Reviewers will be asked to evaluate submissions <strong>both on conceptual and technical merits</strong>. So, authors are encouraged to emphasize both conceptual and technical novelty in the first few pages of the paper. </li><li>FOCS is an IEEE conference and as such, we will review papers in alignment with the IEEE ethics <a href="https://www.ieee.org/about/corporate/governance/p7-8.html">guidelines</a>. Authors are encouraged to reflect on these guidelines in shaping their work, dissemination, and submission.</li></ul>



<p/>



<p>The PC is eagerly looking forward to your submissions!</p>



<p> </p></div>
    </content>
    <updated>2021-05-19T14:23:56Z</updated>
    <published>2021-05-19T14:23:56Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>nisheethvishnoi</name>
    </author>
    <source>
      <id>https://nisheethvishnoi.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://nisheethvishnoi.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://nisheethvishnoi.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://nisheethvishnoi.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://nisheethvishnoi.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Algorithms, Nature, and Society</title>
      <updated>2021-05-24T11:38:55Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18714</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/05/19/hilbert-tenth-on-rationals/" rel="alternate" type="text/html"/>
    <title>Hilbert Tenth On Rationals</title>
    <summary>We must know. We will know—David Hilbert Mihai Prunescu is at the Institute of Mathematics of the Romanian Academy. He works in logic and complexity theory. We recently discussed his work on Hilbert’s Tenth. Today we thought we would do a follow up discussion of a recent paper of his on the famous Hilbert’s Tenth […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>We must know. We will know—David Hilbert</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/05/19/hilbert-tenth-on-rationals/mp/" rel="attachment wp-att-18716"><img alt="" class="alignright wp-image-18716" height="150" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/05/mp-150x150.jpeg?resize=150%2C150&amp;ssl=1" width="150"/></a></p>
<p>
Mihai Prunescu is at the Institute of Mathematics of the Romanian Academy. He works in logic and complexity theory. We recently discussed his <a href="https://rjlipton.wpcomstaging.com/2021/03/13/hilberts-tenth-again/">work</a> on Hilbert’s Tenth.</p>
<p>
Today we thought we would do a follow up discussion of a recent paper of his on the famous Hilbert’s Tenth problem. </p>
<p>
“Hilbert’s Tenth” is actually a suite of problems: is it decidable whether a finite set of equations of a certain kind have a common solution over a certain domain?  When the domain is the integers and the equations are polynomials set to zero we have the original form, which was shown to be undecidable by Yuriy Matiyasevich, completing work of Julia Robinson, Martin Davis, and Hilary Putnam.  But over the reals, the problem for polynomial equations is decidable, and Hilbert himself had shown this over the complex numbers.  The rational numbers are the key unsolved case, which I posted about <a href="https://rjlipton.wpcomstaging.com/2010/08/07/hilberts-tenth-over-the-rationals/">eleven</a> and <a href="https://rjlipton.wpcomstaging.com/2011/07/19/hilberts-10-5th-problem/">ten</a> years ago and <a href="https://rjlipton.wpcomstaging.com/2019/06/19/diophantine-equations/">again</a> more <a href="https://rjlipton.wpcomstaging.com/2021/03/13/hilberts-tenth-again/">recently</a>.</p>
<p>
The last post, two months ago, was about attempts to get leverage on the rationals by extending the equations to allow exponentiation.  Mihai added several helpful comments to that post, and with his blessing, this is trying to restart the discussion as busy pandemic-impacted university terms for Ken and some others we know draw to a close.</p>
<p>
</p><h2> Subtleties With Exponents </h2><p/>
<p>
He has helped us understand a few subtleties in correspondence since then.  They concern both that rational numbers and exponentials are involved.  The focal point confounds our expectation that the square <img alt="{r^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> of any nonzero rational number <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> should be positive.  Here are two examples of issues:</p>
<ul>
<li> If <img alt="{r = x^{1/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+x%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and we put <img alt="{r^2 = x^{1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%5E2+%3D+x%5E%7B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> then <img alt="{x = -1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%3D+-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> gives a negative value on the right-hand side.
</li><li>If <img alt="{r = x^{1/4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%3D+x%5E%7B1%2F4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and we put <img alt="{r^2 = x^{1/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%5E2+%3D+x%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> then the negative square root of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is a possible value.
</li></ul>
<p>
If we have two equations <img alt="{E_1(\vec{x})=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_1%28%5Cvec%7Bx%7D%29%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{E_2(\vec{x})=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_2%28%5Cvec%7Bx%7D%29%3D0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and we want to say that <img alt="{\vec{x}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cvec%7Bx%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> solves at least one of them, we can introduce the single equation</p>
<p align="center"><img alt="\displaystyle  E_1(\vec{x})\cdot E_2(\vec{x}) = 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E_1%28%5Cvec%7Bx%7D%29%5Ccdot+E_2%28%5Cvec%7Bx%7D%29+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
This is not affected by the subtleties.  But now suppose we want to do AND instead of OR.  The natural idea is to make the equation</p>
<p align="center"><img alt="\displaystyle  E_1(\vec{x})^2 + E_2(\vec{x})^2 = 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E_1%28%5Cvec%7Bx%7D%29%5E2+%2B+E_2%28%5Cvec%7Bx%7D%29%5E2+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
However, suppose we have the equation <img alt="{x^{1/4} = y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7B1%2F4%7D+%3D+y%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.  This is the same as <img alt="{x^{1/4} - y = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7B1%2F4%7D+-+y+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.  If we square it, we get </p>
<p align="center"><img alt="\displaystyle  x^{1/2} + y^2 - 2x^{1/4}y = 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5E%7B1%2F2%7D+%2B+y%5E2+-+2x%5E%7B1%2F4%7Dy+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
The abstract worry is that this could allow solutions where <img alt="{x^{1/2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7B1%2F2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is chosen <i>negative</i>, though not intended as the square of <img alt="{x^{1/4}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7B1%2F4%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> (whether positive or negative).  Mihai gave us a simple example of two exponential equations where unintended solutions occur after squaring and adding them.  Consider</p>
<p align="center"><img alt="\displaystyle  u^v = 0 \wedge w^x = 0. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++u%5Ev+%3D+0+%5Cwedge+w%5Ex+%3D+0.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
versus</p>
<p align="center"><img alt="\displaystyle  u^{2v} + w^{2x} = 0. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++u%5E%7B2v%7D+%2B+w%5E%7B2x%7D+%3D+0.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
The former set is solved only by <img alt="{u = w = 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu+%3D+w+%3D+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> with the exponents nonzero.  The latter, however, allows solutions illustrating both issues above:</p>
<ul>
<li> <img alt="{u = v = 1,\; w = -1,\; x = 1/2;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu+%3D+v+%3D+1%2C%5C%3B+w+%3D+-1%2C%5C%3B+x+%3D+1%2F2%3B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>
</li><li> <img alt="{u = v = w = 1,\; x = 1/4;}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu+%3D+v+%3D+w+%3D+1%2C%5C%3B+x+%3D+1%2F4%3B%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>
</li></ul>
<p>
where in the latter, the negative square root of <img alt="{1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is taken.  What this means is that with rationals and/or exponentials we must watch our manipulations more carefully.  </p>
<p>
</p><h2> The Theorem </h2><p/>
<p>
The following <a href="https://www.cambridge.org/core/journals/journal-of-symbolic-logic/article/abs/exponential-diophantine-problem-for-mathbb-q/48F3FBED93B3138F34D39A52DB560BC0">theorem</a> is essentially due to Mihai: </p>
<blockquote><p><b>Theorem 1</b> <em> Let <img alt="{P(x_1,\dots,x_n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%28x_1%2C%5Cdots%2Cx_n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> be an integer polynomial. Then it is undecidable to determine whether there are <img alt="{a_1,\dots,a_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba_1%2C%5Cdots%2Ca_n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> in <img alt="{{\mathbb Q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+Q%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{b_1,\dots,b_n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb_1%2C%5Cdots%2Cb_n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> also in <img alt="{{\mathbb Q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+Q%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> so that </em></p><em>
<ol>
<li>
<img alt="{P(a_1,\dots,a_n)=0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP%28a_1%2C%5Cdots%2Ca_n%29%3D0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <p/>
</li><li>
For each <img alt="{k=1,\dots,n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%3D1%2C%5Cdots%2Cn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> 	<p/>
<p align="center"><img alt="\displaystyle  b_k = 2^{a_k}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++b_k+%3D+2%5E%7Ba_k%7D.+&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
</li></ol>
</em><p><em/>
</p></blockquote>
<p>Here <img alt="{{\mathbb Q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+Q%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> as usual is the rationals.  The full question whether the above theorem can be proved without any exponential functions <img alt="{2^x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Ex%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> remains open, after much attention by many researchers. </p>
<p>
</p><p/><h2> Some History </h2><p/>
<p/><p>
The reason we say “essentially” due to Prunescu must be explained. He recently published an article showing that <i>The Exponential Diophantine Problem For <img alt="{{\mathbb Q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Cmathbb+Q%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></i> is undecidable: It is in the <i>Journal of Symbolic Logic</i> (JSL), Volume 85, Issue 2. </p>
<p>
His clever proof showed that the solutions <img alt="{(x,y)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28x%2Cy%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> over the rationals of 	</p>
<p align="center"><img alt="\displaystyle  x^y = y^x " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x%5Ey+%3D+y%5Ex+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>lie in a one-dimensional space. This space is parameterized by the integers and so it can be used to define the integers. This immediately proves the undecidability by reduction to the classic Hilbert’s Tenth over the integers.</p>
<p>
Ken and I then wrote a <a href="https://rjlipton.wpcomstaging.com/2021/03/13/hilberts-tenth-again/">post</a> on our blog GLL explaining his JSL result. Mihai kindly added a <a href="https://rjlipton.wpcomstaging.com/2021/03/13/hilberts-tenth-again/#comment-116365">comment</a> on the post that basically stated the above theorem with <img alt="{2^y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Ey%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. </p>
<p>
I was initially puzzled since the above theorem seems to be stronger than his JSL theorem. His published result uses <img alt="{x^y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5Ey%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> a binary exponential function and the new theorem needs only the unary function <img alt="{2^y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Ey%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Now it must be said that the cases could be incomparable—because the extra freedom of allowing any rational base <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> in <img alt="{x^y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5Ey%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> could promote a different kind of analysis that makes the existence of such solutions decidable.  The equivalence logic coming back from undecidable cases over the integers might not apply in the above reduction.  But the undecidability when the base is fixed to be <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> still strikes me as capturing the general import.</p>
<p>
What’s more, the theorem fixing <img alt="{2^x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Ex%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> as the base has a much simpler proof.  It requires only a lemma that extends the famous Euclid Theorem: The value <img alt="{\sqrt{2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csqrt%7B2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is not rational.</p>
<p/><h2> Proof of the Lemma </h2><p/>
<p>
The proof rests on the famous result of Matiyasevich on Hilbert’s Tenth and the following lemma: </p>
<blockquote><p><b>Lemma 2</b> <em> Suppose that <img alt="{x \ge 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cge+0%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/>, <img alt="{x \in {\mathbb Q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%5Cin+%7B%5Cmathbb+Q%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/>, and <img alt="{2^x \in {\mathbb Q}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Ex+%5Cin+%7B%5Cmathbb+Q%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Then <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0&amp;c=20201002"/> is an integer. </em>
</p></blockquote>
<p/><p>
<em>Proof:</em>  Clearly we can assume that <img alt="{x&gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. We will prove the lemma in two steps. </p>
<ol>
<li>
The value <img alt="{2^x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Ex%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is an integer. <p/>
</li><li>
The value <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is an integer.
</li></ol>
<p>
Let <img alt="{x = r/s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%3D+r%2Fs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for <img alt="{r, s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%2C+s%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> integers that are co-prime and <img alt="{r \ge 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{s \ge 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. </p>
<p>
<i>Step (1):</i> Now let 	</p>
<p align="center"><img alt="\displaystyle  2^x = \frac{u}{v} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%5Ex+%3D+%5Cfrac%7Bu%7D%7Bv%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>where <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are co-prime integers. We can assume that <img alt="{|v| \ge 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Cv%7C+%5Cge+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>; otherwise, step (1) is true. Now 	</p>
<p align="center"><img alt="\displaystyle  2^r = \frac{u^s}{v^s}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%5Er+%3D+%5Cfrac%7Bu%5Es%7D%7Bv%5Es%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>But <img alt="{u^s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%5Es%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{v^s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%5Es%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are co-prime since <img alt="{u}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bu%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> are co-prime. But this implies that <img alt="{2^r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Er%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is not an integer, since <img alt="{|v^s| \ge 2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Cv%5Es%7C+%5Cge+2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Note this uses that <img alt="{r \ge 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{s \ge 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs+%5Cge+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. This contradiction proves step (1).</p>
<p>
<i>Step (2):</i> By step (1) we thus have that 	</p>
<p align="center"><img alt="\displaystyle  2^{r/s} = y " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%5E%7Br%2Fs%7D+%3D+y+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>for some integer <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Then 	</p>
<p align="center"><img alt="\displaystyle  2^r = y^s. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%5Er+%3D+y%5Es.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>Thus <img alt="{y^s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%5Es%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is an integer and so by unique factorization it follows that <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> must be an integer power of <img alt="{2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Let <img alt="{y = 2^k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By+%3D+2%5Ek%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Then 	</p>
<p align="center"><img alt="\displaystyle  2^{r} = 2^{sk}. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%5E%7Br%7D+%3D+2%5E%7Bsk%7D.+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>This implies that <img alt="{r=sk}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%3Dsk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and so that <img alt="{x = r/s=k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx+%3D+r%2Fs%3Dk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is an integer. This proves step (2). <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p/><h2> Open Problems </h2><p/>
<p>
We have shared the topic also with Joël Ouaknine and thank him for his comments as well. Addressing our readers, do you have any further comments? What do you think about the rationals? </p>
<p>
Ken related an idea while corresponding with Mihai: Suppose we introduce a special squaring function <img alt="{Sq}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BSq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> with the properties that</p>
<ul>
<li>
<img alt="{Sq(r)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BSq%28r%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is always positive for nonzero <img alt="{r}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Br%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.<p/>
</li><li>
In particular, <img alt="{Sq(x^{1/4})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BSq%28x%5E%7B1%2F4%7D%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> gives only the positive square root of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>.<p/>
</li><li>
Terms with <img alt="{Sq}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BSq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> may not appear in exponents, nor be bases of them.  They may only be added and multiplied.
</li></ul>
<p>
The motivation is that now</p>
<p align="center"><img alt="\displaystyle  Sq(E_1(\vec{x}) + Sq(E_2(\vec{x})) = 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Sq%28E_1%28%5Cvec%7Bx%7D%29+%2B+Sq%28E_2%28%5Cvec%7Bx%7D%29%29+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>
does give the AND of the two equations.  What then becomes of the above undecidability proofs, and does the AND property have other applications?</p></font></font></div>
    </content>
    <updated>2021-05-19T12:37:11Z</updated>
    <published>2021-05-19T12:37:11Z</published>
    <category term="History"/>
    <category term="Open Problems"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="Results"/>
    <category term="Diophantine"/>
    <category term="H10"/>
    <category term="Hilbert Tenth"/>
    <category term="Mihai Prunescu"/>
    <category term="rationals"/>
    <category term="undecidability"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-05-24T11:37:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://ptreview.sublinear.info/?p=1517</id>
    <link href="https://ptreview.sublinear.info/?p=1517" rel="alternate" type="text/html"/>
    <title>Announcing WOLA’21 (Workshop on Local Algorithms)</title>
    <summary>The fifth WOLA (Workshop on Local Algorithms) will be virtual, and take place June 14-15. Registration is free, but required: please fill this form by June 10th to attend. Local algorithms — that is, algorithms that compute and make decisions on parts of the output considering only a portion of the input — have been […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The fifth <a href="http://www.local-algorithms.com">WOLA</a> (Workshop on Local Algorithms) will be virtual, and take place June 14-15. <strong>Registration is free</strong>, but required: please fill <a href="https://docs.google.com/forms/d/e/1FAIpQLSdot9yskpr-9DPJ2UK6iuYdzlX25w6tPy6H5UKHEVIR-yTLHg/viewform">this form</a> by June 10th to attend.</p>



<p><em>Local algorithms — that is, algorithms that compute and make decisions on parts of the output considering only a portion of the input — have been studied in a number of areas in theoretical computer science and mathematics. Some of the related areas include sublinear-time algorithms, distributed algorithms, streaming algorithms, (massively) parallel algorithms, inference in large networks, and graphical models. These communities have similar goals but a variety of approaches, techniques, and methods. This workshop is aimed at fostering dialogue and cross-pollination of ideas between the various communities.</em></p>



<p>This year, the workshop will include:</p>



<ul><li><strong>A poster session</strong>: Please submit <a href="http://www.local-algorithms.com/?page=call-for-posters">your poster proposal</a> (title and abstract) at by<strong> May 26th</strong>. Everyone is invited to contribute. This session will take place on gather.town.</li><li><strong>Invited long talks</strong>: the tentative schedule is <a href="http://www.local-algorithms.com/?page=schedule">available</a>, and features talks by James Aspnes, Jelani Nelson, Elaine Shi, Christian Sohler, Uri Stemmer, and Mary Wootters.</li><li><strong>Junior-Senior social meetings</strong></li><li><strong>An AMA (Ask Me Anything) session</strong>, moderated by Merav Parter </li><li><strong>A Slack channel</strong></li><li><strong>An Open Problems session</strong></li></ul>



<p>The Program Committee of WOLA 2021 is comprised of:</p>



<ul><li>Venkatesan Guruswami (CMU)</li><li>Elchanan Mossel (MIT)</li><li>Merav Parter (Weizmann Institute of Science)</li><li>Sofya Raskhodnikova <strong>(chair) </strong>(Boston University)</li><li>Gregory Valiant (Stanford)</li></ul>



<p>and the organizing committee:</p>



<ul><li>Sebastian Brandt (ETH)</li><li>Yannic Maus (Technion)</li><li>Slobodan Mitrović (MIT)</li></ul>



<p>For more detail, see <a href="http://www.local-algorithms.com/?">the website</a>; to stay up to date with the latest announcements concerning WOLA, <a href="https://lists.csail.mit.edu/mailman/listinfo/wola">join our mailing list</a>!</p></div>
    </content>
    <updated>2021-05-19T00:29:53Z</updated>
    <published>2021-05-19T00:29:53Z</published>
    <category term="Announcement"/>
    <author>
      <name>Clement Canonne</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2021-05-23T23:00:46Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-6555947.post-8966314939306062304</id>
    <link href="http://blog.geomblog.org/feeds/8966314939306062304/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="http://blog.geomblog.org/2021/05/transitions.html#comment-form" rel="replies" type="text/html"/>
    <link href="http://www.blogger.com/feeds/6555947/posts/default/8966314939306062304" rel="edit" type="application/atom+xml"/>
    <link href="http://www.blogger.com/feeds/6555947/posts/default/8966314939306062304" rel="self" type="application/atom+xml"/>
    <link href="http://feedproxy.google.com/~r/TheGeomblog/~3/aZaRzVm3CmE/transitions.html" rel="alternate" type="text/html"/>
    <title>Transitions</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> I've been at the U of Utah and Salt Lake City for 14 years (14.5 really). It was my first academic job and the longest time I've spent anywhere (throughout my whole life). So it's a little hard to accept that I'm moving to my next adventure. </p><p>It's a two-part adventure, because why make one move when you can make two. </p><p>Firstly, as of today, I'm going to working with Alondra Nelson at the White House Office of Science and Technology Policy, advising on matters relating to fairness and bias in tech systems. This is a scary and exciting new position, and I hope to help to nudge things along just a bit further in the direction of tech that can help more than it harms, especially for those who've been left behind in our rush to an algorithmically controlled future. </p><p>Secondly, I'm moving to Brown University to join the CS department there as well as their Data Science Initiative. Together with Seny Kamara and others, I'm going to start a new center on Computing for the People, to help think through what it means to do computer science that truly responds to the needs of people, instead of hiding behind a neutrality that merely gives more power to those already in power. </p><p>Lots of changes, and because of the pandemic, all this will happen in slow machine, but it's a whirlwind of emotions (and new clothes - apparently tech conference T-shirts don't work in formal settings - WHO KNEW!!!). </p><p><br/></p><img alt="" height="1" src="http://feeds.feedburner.com/~r/TheGeomblog/~4/aZaRzVm3CmE" width="1"/></div>
    </content>
    <updated>2021-05-17T14:33:00Z</updated>
    <published>2021-05-17T14:33:00Z</published><feedburner:origlink xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0">http://blog.geomblog.org/2021/05/transitions.html</feedburner:origlink>
    <author>
      <name>Suresh Venkatasubramanian</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/15898357513326041822</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-6555947</id>
      <category term="research"/>
      <category term="community"/>
      <category term="miscellaneous"/>
      <category term="soda"/>
      <category term="conferences"/>
      <category term="data-mining"/>
      <category term="socg"/>
      <category term="blogosphere"/>
      <category term="publishing"/>
      <category term="clustering"/>
      <category term="teaching"/>
      <category term="jobs"/>
      <category term="funding"/>
      <category term="humor"/>
      <category term="awards"/>
      <category term="outreach"/>
      <category term="stoc"/>
      <category term="cs.CG"/>
      <category term="focs"/>
      <category term="nsf"/>
      <category term="reviewing"/>
      <category term="socg-2010"/>
      <category term="fairness"/>
      <category term="academy"/>
      <category term="latex"/>
      <category term="stoc2017"/>
      <category term="theoryfest"/>
      <category term="workshops"/>
      <category term="acm"/>
      <category term="conf-blogs"/>
      <category term="writing"/>
      <category term="cs.DS"/>
      <category term="cs.LG"/>
      <category term="geometry"/>
      <category term="p-vs-nc"/>
      <category term="advising"/>
      <category term="sabbatical"/>
      <category term="simons foundation"/>
      <category term="announcement"/>
      <category term="big-data"/>
      <category term="deadline"/>
      <category term="jeff phillips"/>
      <category term="streaming"/>
      <category term="books"/>
      <category term="large-data"/>
      <category term="p-vs-np"/>
      <category term="cra"/>
      <category term="cstheory"/>
      <category term="focs2010"/>
      <category term="icdm"/>
      <category term="math.PR"/>
      <category term="memorial"/>
      <category term="personal"/>
      <category term="posters"/>
      <category term="potd"/>
      <category term="rajeev motwani"/>
      <category term="shonan"/>
      <category term="socg2012"/>
      <category term="software"/>
      <category term="stoc2012"/>
      <category term="GIA"/>
      <category term="SDM"/>
      <category term="alenex"/>
      <category term="alenex2011"/>
      <category term="arxiv"/>
      <category term="career"/>
      <category term="complexity"/>
      <category term="cs.CC"/>
      <category term="deolalikar"/>
      <category term="distributions"/>
      <category term="madalgo"/>
      <category term="nips"/>
      <category term="sdm2011"/>
      <category term="shape"/>
      <category term="talks"/>
      <category term="technology"/>
      <category term="theory.SE"/>
      <category term="travel"/>
      <category term="video"/>
      <category term="8f-cg"/>
      <category term="DBR"/>
      <category term="ICS"/>
      <category term="LISPI"/>
      <category term="acceptances"/>
      <category term="bibtex"/>
      <category term="bregman"/>
      <category term="cfp"/>
      <category term="clustering-book"/>
      <category term="column"/>
      <category term="combinatorial geometry"/>
      <category term="current-distance"/>
      <category term="ecml-pkdd"/>
      <category term="empirical"/>
      <category term="esa"/>
      <category term="fat*"/>
      <category term="focs2012"/>
      <category term="focs2014"/>
      <category term="fwcg"/>
      <category term="game theory"/>
      <category term="godel"/>
      <category term="graphs"/>
      <category term="implementation"/>
      <category term="journals"/>
      <category term="kernels"/>
      <category term="misc"/>
      <category term="models"/>
      <category term="obituary"/>
      <category term="productivity"/>
      <category term="programming"/>
      <category term="society"/>
      <category term="soda2011"/>
      <category term="topology"/>
      <category term="turing"/>
      <category term="tv"/>
      <category term="women-in-theory"/>
      <category term=".02"/>
      <category term="IMA"/>
      <category term="MOOC"/>
      <category term="PPAD"/>
      <category term="accountability"/>
      <category term="active-learning"/>
      <category term="aggregator"/>
      <category term="algorithms"/>
      <category term="ams"/>
      <category term="analco"/>
      <category term="barriers"/>
      <category term="beamer"/>
      <category term="blogging"/>
      <category term="candes"/>
      <category term="civil rights"/>
      <category term="classification"/>
      <category term="coding-theory"/>
      <category term="coffee"/>
      <category term="conjecture"/>
      <category term="cosmos"/>
      <category term="counting"/>
      <category term="cricket"/>
      <category term="cs.DC"/>
      <category term="dagstuhl"/>
      <category term="databuse"/>
      <category term="dimacs"/>
      <category term="dimensionality-reduction"/>
      <category term="distributed-learning"/>
      <category term="double-blind review"/>
      <category term="duality"/>
      <category term="eda"/>
      <category term="embarrassing"/>
      <category term="ethics"/>
      <category term="expanders"/>
      <category term="experiments"/>
      <category term="fake-news"/>
      <category term="fatml"/>
      <category term="fellowships"/>
      <category term="focs2013"/>
      <category term="fonts"/>
      <category term="gct"/>
      <category term="ggplot"/>
      <category term="gpu"/>
      <category term="graph minors"/>
      <category term="gt.game-theory"/>
      <category term="guest-post"/>
      <category term="guitar"/>
      <category term="hangouts"/>
      <category term="hirsch"/>
      <category term="history"/>
      <category term="ipe"/>
      <category term="ita"/>
      <category term="jmm"/>
      <category term="k-12"/>
      <category term="knuth"/>
      <category term="machine-learning"/>
      <category term="massive"/>
      <category term="math.ST"/>
      <category term="media"/>
      <category term="memes"/>
      <category term="metoo"/>
      <category term="metrics"/>
      <category term="morris"/>
      <category term="movies"/>
      <category term="multicore"/>
      <category term="music"/>
      <category term="narrative"/>
      <category term="networks"/>
      <category term="nih"/>
      <category term="parallelism"/>
      <category term="partha niyogi"/>
      <category term="polymath"/>
      <category term="polymath research"/>
      <category term="polytopes"/>
      <category term="postdocs"/>
      <category term="privacy"/>
      <category term="quant-ph"/>
      <category term="quantum"/>
      <category term="randomness"/>
      <category term="review"/>
      <category term="sampling"/>
      <category term="seminars"/>
      <category term="social-networking"/>
      <category term="soda2014"/>
      <category term="students"/>
      <category term="sublinear"/>
      <category term="submissions"/>
      <category term="summer-school"/>
      <category term="superbowl"/>
      <category term="surveys"/>
      <category term="svn"/>
      <category term="television"/>
      <category term="traffic"/>
      <category term="twitter"/>
      <category term="utah"/>
      <category term="wads"/>
      <category term="white elephant"/>
      <category term="xkcd"/>
      <author>
        <name>Suresh Venkatasubramanian</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/15898357513326041822</uri>
      </author>
      <link href="http://blog.geomblog.org/" rel="alternate" type="text/html"/>
      <link href="http://www.blogger.com/feeds/6555947/posts/default?alt=atom&amp;start-index=26&amp;max-results=25&amp;redirect=false" rel="next" type="application/atom+xml"/>
      <link href="http://feeds.feedburner.com/TheGeomblog" rel="self" type="application/atom+xml"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <subtitle>Ruminations on computational geometry, algorithms, theoretical computer science and life</subtitle>
      <title>The Geomblog</title>
      <updated>2021-05-24T08:07:35Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-7172198197646487541</id>
    <link href="https://blog.computationalcomplexity.org/feeds/7172198197646487541/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/why-do-countries-and-companies-invest.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7172198197646487541" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/7172198197646487541" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/why-do-countries-and-companies-invest.html" rel="alternate" type="text/html"/>
    <title>Why do countries and companies invest their own money (or is it?) in Quantum Computing (Non-Rhetorical)</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> There have been some recent blog posts by Scott (see <a href="https://www.scottaaronson.com/blog/?p=5387">here</a>) and Lance (see <a href="https://blog.computationalcomplexity.org/2021/04/quantum-stories.html">here</a>)  about the hype for SHORT TERM APPLICATIONS of Quantum Computing, which they both object to. </p><p>I have a question that has been touched on but I want to get it more out there.</p><p>PREAMBLE TO QUESTION:  The following scenarios, while distasteful, do make sense: </p><p>a) A researcher on their grants exaggerates or even out-right lies about the applications of their work. </p><p>b) A journalist in their articles exaggerates or even out-right lies about the applications of the science they are covering.</p><p>c) A company exaggerates or even out-right lies about the applications of their project to a venture capitalist or other kind of investor. </p><p>QUESTION: </p><p>Why does a company or country invest THEIR OWN MONEY into Quantum Computing which is unlikely to have a short term profit or benefit? Presumably they hire honest scientists to tell them the limits of the applications in the short term. </p><p>ANSWERS I HAVE GOTTEN FROM ASKING THIS</p><p>1) QC might be able to do something cool and profitable, like factoring, or simulating physics quantum experiments or something else, in the short term. Quantum Crypto is already happening, and that's a close cousin of Quantum Computing. </p><p>2) QC might be able to do something cool and profitable (like in (1)) in the long term, and both companies and countries think they will be around for a long time. (For a list of America's 10 oldest companies see <a href="https://www.businessnewsdaily.com/8122-oldest-companies-in-america.html">here</a>.) </p><p>3) The company or country is in this for the long term, not for a practical project, but because they realize that doing GOOD SCIENCE is of a general benefit (this might make more sense for a country than a company). And funding Quantum Computing is great for science. </p><p>4) Bait and Switch: The company claims they are doing Quantum to attract very smart people to work with them, and then have those smart people do something else.</p><p>5) (this is a variant of 1) While Quantum Computing may not have any short term applications, there will be classical applications INSPIRED by it (this has already happened, see <a href="https://www.scottaaronson.com/blog/?p=3880">here</a>).</p><p>6) Some of these companies make money by applying for GRANTS to do QC, so its NOT their money. In fact, they are using QC to  GET money.</p><p>7) For a country its not the leaders money, its that Taxpayer's money- though that still leaves the question of why spend Taxpayer money on this and not on something else.</p><p>8) For a company its not their money- its Venture Capitalists  and others (though for a big company like Google I would think it IS their money). </p><p>9) The scientists advising the company or country are giving them bad (or at least self-serving) advice so that those scientists can profit- and do good science. So this is a variant of (3) but without the company or country knowing it. </p><p>10) In some countries and companies group-think sets in, so if the leader (who perhaps is not a scientist) thinks intuitively that QC is good, the scientists who work for them, who know better, choose to not speak up, or else they would lose their jobs...or worse. </p><p>11) For countries this could be like going to the moon: Country A wants to beat Country B to the moon for bragging rights. Scientists get to do good research even if they don't care about bragging rights. </p><p>12) (similar to 11 but for a company) If a company does great work on QC then it is good publicity for that company. </p><p>13) Some variant of the <a href="https://en.wikipedia.org/wiki/Greater_fool_theory">greater fool theory</a>. If so, will there be a bubble? A bail-out? </p><p><br/></p></div>
    </content>
    <updated>2021-05-16T17:14:00Z</updated>
    <published>2021-05-16T17:14:00Z</published>
    <author>
      <name>gasarch</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03004932739846901628</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-05-24T11:11:21Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5490</id>
    <link href="https://www.scottaaronson.com/blog/?p=5490" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5490#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5490" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">What I told my kids</title>
    <summary xml:lang="en-US">You’ll hear that it’s not as simple as the Israelis are good guys and Palestinians are bad guys, or vice versa. And that’s true. But it’s also not so complicated that there are no clearly identifiable good guys or bad guys. It’s just that they cut across the sides. The good guys are anyone, on […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>You’ll hear that it’s not as simple as the Israelis are good guys and Palestinians are bad guys, or vice versa.  And that’s true.</p>



<p>But it’s also not so complicated that there are no clearly identifiable good guys or bad guys.  It’s just that they cut across the sides.</p>



<p>The good guys are anyone, on either side, whose ideal end state is two countries, Israel and Palestine, living side by side in peace.</p>



<p>The bad guys are anyone, on either side, whose ideal end state is the other side being, if not outright exterminated, then expelled from its current main population centers (ones where it’s been for several generations or more) and forcibly resettled someplace far away.</p>



<p>(And those whose ideal end state is everyone living together with no border — possibly as part of the general abolition of nation-states?   They’re not bad guys; they can plead insanity.  [<strong>Update:</strong> <a href="https://www.scottaaronson.com/blog/?p=5490#comment-1889918">See here</a> for clarifications!])</p>



<p>Hamas are bad guys.  They fire rockets indiscriminately at population centers, hoping to kill as many civilians as they can.  (Unfortunately for them and fortunately for Israel, they’re not great at that, and also they’re aiming at a target that’s world-historically good at defending itself.)</p>



<p>The IDF, whatever else you say about it, sends evacuation warnings to civilians before it strikes the missile centers that are embedded where they live.  Even if Hamas could aim its missiles, the idea of it extending the same courtesy to Israeli civilians is black comedy.</p>



<p>Netanyahu is not as bad as Hamas, because he has the power to kill millions of Palestinians and yet kills only hundreds … whereas if Hamas had the power to kill all Jews, it told the world in its charter that it would immediately do so, and it’s acted consistently with its word.</p>



<p>(An aside: I’m convinced that Hamas has the most top-heavy management structure of any organization in the world.  Every day, Israel takes out another dozen of its <em>most senior, highest-level</em> commanders, apparently leaving hundreds more.  How many senior commanders do they have?  Do they have even a single junior commander?)</p>



<p>Anyway, not being as bad as Hamas is an extremely low bar, and Netanyahu is a thoroughly bad guy.  He’s corrupt and power-mad.  Like Trump, he winks at his side’s monstrous extremists without taking moral responsibility for them.  And if it were ever possible to believe that he wanted two countries as the ideal end state, it hasn’t been possible to believe that for at least a decade.</p>



<p>Netanyahu and Hamas are allies, not enemies.  Both now blatantly, obviously rely on the other to stay in power, to demonstrate their worldview and thereby beat their internal adversaries.</p>



<p>Whenever you see anyone opine about this conflict, on Facebook or Twitter or in an op-ed or anywhere else, keep your focus relentlessly on the question of what that person <em>wants</em>, of what they’d do if they had unlimited power.  If they’re a Zionist who talks about how “there’s no such place as Palestine,” how it’s a newly invented political construct: OK then, does that mean they’d relocate the 5 million self-described Palestinians to Jordan?  Or where?  If, on the other side, someone keeps talking about the “Zionist occupation,” always leaving it strategically unspecified whether they mean just the West Bank and parts of East Jerusalem or also Tel Aviv and Haifa, if they talk about the Nakba (catastrophe) of Israel’s creation in 1947 … OK then, what’s to be done with the 7 million Jews now living there?  Should they go back to the European countries that murdered their families, or the Arab countries that expelled them?  Should the US take them all?  Out with it!</p>



<p>Don’t let them dodge the question.  Don’t let them change the subject to something they’d much rather talk about, like the details of the other side’s latest outrage.  Those details always seem so important, and yet everyone’s stance on every specific outrage is like 80% predictable if you know their desired end state.  So just keep asking directly about their desired end state.</p>



<p>If, like me, you favor two countries living in peace, then you need never fear anyone asking you the same thing.  You can then shout your desired end state from the rooftops, leaving unsettled only the admittedly-difficult “engineering problem” of how to get there.  Crucially, whatever their disagreements or rivalries, everyone trying to solve the same engineering problem is in a certain sense part of the same team.  At least, there’s rarely any reason to kill someone trying to solve the same problem that you are.</p>



<p>“What is this person’s ideal end state?”  Just keep asking that and there’s a limit to how wrong you can ever be about this.  You can still make factual mistakes, but it’s then almost impossible to make a moral mistake.</p></div>
    </content>
    <updated>2021-05-16T02:26:59Z</updated>
    <published>2021-05-16T02:26:59Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="The Fate of Humanity"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-05-21T20:52:21Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/05/15/linkage</id>
    <link href="https://11011110.github.io/blog/2021/05/15/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Statement of concern from the American Statistical Association over the Greek government’s persecution of former chief statistician Andreas Georgiou (\(\mathbb{M}\)) for (according to the ASA) producing accurate and truthful statistical reports on the Greek economy that cast disrepute on the unverifiable claims of earlier governments.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://www.amstat.org/asa/News/Greek-Statistician-Found-Liable-for-Slander-Continues-to-Face-Persecution.aspx">Statement of concern from the American Statistical Association over the Greek government’s persecution of former chief statistician Andreas Georgiou</a> (<a href="https://mathstodon.xyz/@11011110/106164322094769198">\(\mathbb{M}\)</a>) for (according to the ASA) producing accurate and truthful statistical reports on the Greek economy that cast disrepute on the unverifiable claims of earlier governments.</p>
  </li>
  <li>
    <p><a href="https://twitter.com/arxivabs">That other microblogging site has a bot specifically devoted to replacing links to pdf versions of arxiv preprints by links to the abstracts of the same preprint</a> (<a href="https://mathstodon.xyz/@11011110/106169351037107488">\(\mathbb{M}\)</a>). Is there something like that for mastodon? If not there should be.</p>
  </li>
  <li>
    <p>For a centrally symmetric star-shaped set in the plane, each line through the center cuts its perimeter into two equal-length curves. But these are not the only shapes with this property: 18th-century Jesuit polymath <a href="https://en.wikipedia.org/wiki/Roger_Joseph_Boscovich">Roger Boscovich</a> observed that a heart-like shape formed by three semicircles has the same property (<a href="https://mathstodon.xyz/@11011110/106175759357772585">\(\mathbb{M}\)</a>, <a href="https://doi.org/10.2307/2974900">via</a>).</p>

    <p style="text-align: center;"><img alt="Boscovich's cardioid, with its perimeter bisected by a line through its cusp" src="https://11011110.github.io/blog/assets/2021/boscovich.svg"/></p>
  </li>
  <li>
    <p><a href="https://picturethismaths.wordpress.com/2020/10/31/three-correlations-and-a-samosa/">Three correlations and a samosa</a> (<a href="https://mathstodon.xyz/@11011110/106180193506681489">\(\mathbb{M}\)</a>, <a href="https://picturethismaths.wordpress.com/2021/02/03/three-correlations-and-other-statistical-models/">see also</a>, <a href="https://picturethismaths.wordpress.com/2021/03/18/three-correlations-and-an-algebraic-classification/">see also</a>). \(3\times 3\) symmetric matrices with unit diagonals form a three-dimensional linear space, in which the samosa is a curvy 3d convex set representing the positive definite matrices. Taking sections of it allows you to infer the possible correlations between two variables, given each of their correlations with a third.</p>
  </li>
  <li>
    <p>I couldn’t resist picking up a copy of <em>The Architecture of Trees</em> (<a href="https://mathstodon.xyz/@11011110/106184861337043968">\(\mathbb{M}\)</a>), a coffee-table book centered on pen-and-ink illustrations of the summer and winter forms of over 200 types of tree, on a recent visit to the Mendocino Coast Botanical Gardens (beautiful this time of year with many flowers in bloom). Some reviews: <a href="https://www.startribune.com/new-book-is-tree-tome-like-few-others-part-science-part-art-marvel/507788702/">1</a>, <a href="https://www.thedailybeast.com/the-architecture-of-trees-travel-with-the-book-that-captures-the-beauty-of-trees">2</a>, <a href="http://spacing.ca/national/2020/08/04/book-review-the-architecture-of-trees/ https://dirt.asla.org/2019/07/10/the-architecture-of-trees-2/">3</a>.</p>
  </li>
  <li>
    <p><a href="http://www.xl-muse.com/html/en/index.php?ac=article&amp;at=read&amp;did=239">Dujiangyan Zhongshuge</a> (<a href="https://mathstodon.xyz/@11011110/106192492039422969">\(\mathbb{M}\)</a>, <a href="https://www.thisiscolossal.com/2021/05/x-living-dujiangyan-zhongshuge/">via</a>), bookstore in Chengdu with mirrored floors and ceilings creating the feeling of an infinite Escher palace of books.</p>
  </li>
  <li>
    <p><a href="http://keenan.is/illustrating/">Illustrating geometry</a> (<a href="https://mathstodon.xyz/@11011110/106204129125568328">\(\mathbb{M}\)</a>). An apparently-defunct blog from 2016–2017 with several interesting posts about technical illustrations in mathematics.</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/q/391885/440">Which \(n\times n\times n\) grids have Hamiltonian cycles that turn at every step?</a> (<a href="https://mathstodon.xyz/@11011110/106218352172585271">\(\mathbb{M}\)</a>) After I linked to this, a later answer pointed to the recent book <em>Bicycle or Unicycle？A Collection of Intriguing Mathematical Puzzles</em>, by Stan Wagon and Daniel Velleman, which has solutions for all even \(n\) on pp. 89–96. A simple parity argument shows that it’s impossible on odd grids, but the same book conjectures that these have Hamiltonian paths except in the case \(n=3\).</p>
  </li>
  <li>
    <p><a href="https://wg2021.mimuw.edu.pl/accepted-papers/">Accepted papers to the International Workshop
on Graph-Theoretic Concepts in Computer Science</a> (<a href="https://mathstodon.xyz/@11011110/106220947095567586">\(\mathbb{M}\)</a>). My paper “<a href="https://11011110.github.io/blog/2020/10/19/graphs-stably-matchable.html">The Graphs of Stably Matchable Pairs</a>” is one of them. The conference will be online June 23–25. Unlike many conferences, WG prepares the proceedings after the conference, to allow authors to incorporate feedback from presentations. Details of how to participate don’t seem to be online but I’m sure they’ll be made available soon.</p>
  </li>
  <li>
    <p><a href="https://igorpak.wordpress.com/2021/05/13/why-you-shouldnt-be-too-pessimistic/">Why you shouldn’t be too pessimistic</a> (<a href="https://mathstodon.xyz/@11011110/106230884405427138">\(\mathbb{M}\)</a>). Igor Pak on the nature of mathematical conjectures.</p>
  </li>
  <li>
    <p><a href="https://www.sciencemag.org/careers/2021/05/two-surnames-no-hyphen-claiming-my-identity-latin-american-scientist">Two surnames, no hyphen: Claiming my identity as a Latin American scientist</a> (<a href="https://mathstodon.xyz/@11011110/106241178695539953">\(\mathbb{M}\)</a>, <a href="https://retractionwatch.com/2021/05/15">via</a>). Johana Goyes Vallejos in <em>Science</em>, on respect for diversity in naming styles in academia.</p>
  </li>
</ul></div>
    </content>
    <updated>2021-05-15T17:31:00Z</updated>
    <published>2021-05-15T17:31:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-05-16T00:57:09Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/05/14/congratulations-dr-havvaei</id>
    <link href="https://11011110.github.io/blog/2021/05/14/congratulations-dr-havvaei.html" rel="alternate" type="text/html"/>
    <title>Congratulations, Dr. Havvaei!</title>
    <summary>It’s getting towards the end of the academic year, that time when students think about finishing up their studies, and today we had another successful Ph.D. defense of one of those students. This time it was one of mine, Elham Havvaei (pronounced like the state of Hawai’i, glottal stop at the apostrophe and all, but with a v instead of w, and better known by her nickname Haleh).</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>It’s getting towards the end of the academic year, that time when students think about finishing up their studies, and today we had another successful Ph.D. defense of one of those students. This time it was one of mine, <a href="https://www.ics.uci.edu/~ehavvaei/">Elham Havvaei</a> (pronounced like the state of Hawai’i, glottal stop at the apostrophe and all, but with a v instead of w, and better known by her nickname Haleh).</p>

<p>Haleh is Iranian, but came to UC Irvine via Florida in 2016, after working for a master’s degree with Narsingh Deo at the University of Central Florida. I’ve written here already about the research she’s done with me: <a href="https://11011110.github.io/blog/2018/10/07/recognizing-sparse-leaf.html">reconstructing unknown trees from graphs connecting close-together leaves</a> (from IPEC 2018 and Algorithmica 2020), <a href="https://11011110.github.io/blog/2019/01/29/simplifying-task-milestone.html">simplifying graphs whose vertices and edges represent the milestones, tasks, and ordering constraints of a project</a> (SWAT 2020), and <a href="https://11011110.github.io/blog/2021/01/27/which-induced-subgraph.html">classifying problems of finding large subgraphs with one property in graphs with another property</a> (not yet published).</p>

<p>Her next step will be taking a position at Twitter in San Francisco, as a data scientist.</p>

<p>Congratulations, Haleh!</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106236739159044402">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-05-14T18:48:00Z</updated>
    <published>2021-05-14T18:48:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-05-16T00:57:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/070</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/070" rel="alternate" type="text/html"/>
    <title>TR21-070 |  SOS lower bound for exact planted clique | 

	Shuo Pang</title>
    <summary>We prove a SOS degree lower bound for the planted clique problem on Erd{\"o}s-R\'enyi random graphs $G(n,1/2)$. The bound we get is degree $d=\Omega(\epsilon^2\log n/\log\log n)$ for clique size $\omega=n^{1/2-\epsilon}$, which is almost tight. This improves the result of \cite{barak2019nearly} on the ``soft'' version of the problem, where the family of equality-axioms generated by $x_1+...+x_n=\omega$ was relaxed to one inequality $x_1+...+x_n\geq\omega$.

As a technical by-product, we also ``naturalize'' previous techniques developed for the soft problem. This includes a new way of defining the pseudo-expectation and a more robust method to solve the coarse diagonalization of the moment matrix.</summary>
    <updated>2021-05-13T17:19:11Z</updated>
    <published>2021-05-13T17:19:11Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-05-24T11:37:26Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://rjlipton.wpcomstaging.com/?p=18720</id>
    <link href="https://rjlipton.wpcomstaging.com/2021/05/13/matrix-the-meeting/" rel="alternate" type="text/html"/>
    <title>Matrix—The Meeting</title>
    <summary>That’s how it is with people. Nobody cares how it works as long as it works—Councillor Hamann Santosh Vempala and Nikhil Srivastava announced the first in hopefully a series of online meetings about matrix algorithms. Not about the Matrix—the—movie. Santosh and Nikhil said: we expect to have an attendance of people. Wrong. It was over […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>That’s how it is with people. Nobody cares how it works as long as it works—Councillor Hamann</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<p><a href="https://rjlipton.wpcomstaging.com/2021/05/13/matrix-the-meeting/vempalasrivastava/" rel="attachment wp-att-18739"><img alt="" class="alignright wp-image-18739" height="128" src="https://i2.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/05/VempalaSrivastava.png?resize=192%2C128&amp;ssl=1" width="192"/></a></p>
<p>
Santosh Vempala and Nikhil Srivastava announced the first in hopefully a series of online meetings about matrix algorithms. Not about the <i>Matrix</i>—the—<a href="https://rjlipton.wpcomstaging.com/feed/">movie</a>. Santosh and Nikhil said: we expect to have an attendance of <img alt="{20-60}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B20-60%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> people. Wrong. It was over 200 today.</p>
<p>
Today I thought we would talk about the Zoom meeting and future ones being planned. </p>
<p>
Zoom feels closer to the world of the <i>Matrix</i> movies. If you haven’t seen them, all you need to know is the premise of humanity being diverted in a virtual reality.  How do we know the little figures in those boxes are real people?  More concretely, it seems obvious to Ken and me that simulated human online agents will arrive much earlier than person-like robots.  </p>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.wpcomstaging.com/2021/05/13/matrix-the-meeting/the-matrix-architects-room/" rel="attachment wp-att-18742"><img alt="" class="alignright wp-image-18742" height="216" src="https://i0.wp.com/rjlipton.wpcomstaging.com/wp-content/uploads/2021/05/the-matrix-architects-room.jpg?resize=384%2C216&amp;ssl=1" width="384"/></a>
</td>
</tr>
<tr>
<td class="caption alignright">
<font size="-2"><i>Matrix Reloaded</i> virtual <a href="https://virtualbackgrounds.site/background/the-matrix-architects-room">background</a><br/>
</font>
</td>
</tr>
</tbody></table>
<p>
In particular, how much does it take to automate giving a lecture online?  Ken has spent much time this term upgrading his lecture notes in two courses to broadcast quality.  Delivering them remotely trades against the spontaneity of drawing pictures on a whiteboard or document camera and developing proofs and algorithms step-by-step.  It should be easier to develop an AI capable of reacting to questions put in Zoom chat than with in-class situations, where “reading the room” is also important for modulating the speed and manner of presentation.  </p>
<p>
</p><h2> The Meetings </h2><p/>
<p>
Daniel Kressner, Mike Mahoney, Cleve Moler, Alex Townsend, and Joel Tropp were also organizers of this smeeting on matrix computation. This Wednesday was the first in a series of online meetings. The speakers for today were Peter Bürgisser, Nick Higham, and Cameron Musco, and the panelists were Jim Demmel, Ilse Ipsen, and Richard Peng.</p>
<p>
The blurb for the meetings is:</p>
<blockquote><p><b> </b> <em> We are organizing an online seminar series on “Complexity of Matrix Computations”, whose goal is to bridge the gap between how numerical linear algebra and theoretical computer science researchers view and study the fundamental computational problems of linear algebra. This gap includes foundational issues such as: what is the computational model? What does it mean to solve a problem? On which criteria do we compare algorithms? We also hope to discuss which techniques in theoretical computer science might be useful in numerical linear algebra and vice versa. </em>
</p></blockquote>
<p>
I love seeing the words “fundamental” and “foundational”, and one question resonated even more.</p>
<p>
</p><p/><h2> The Question </h2><p/>
<p/><p>
What does it mean to solve a problem? In this case what does it mean to solve a linear equation? This is the question that was discussed the most—especially at the end of the meeting. </p>
<p>
I have always thought there is an answer to this. The answer is based on asking what the client wants. Imagine Alice is asked by Bob to <tt>solve</tt> a linear system 	</p>
<p align="center"><img alt="\displaystyle  Ax = b " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Ax+%3D+b+&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/></p>
<p>Alice could go off and return the <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> that solves the system. Or she could say there is no such <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. Or she could say there are many such <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>‘s. Which is the correct answer? </p>
<p>
I believe the right answer is: Alice should ask Bob:</p>
<blockquote><p><b> </b> <em> Bob, what will you do with the answer to this? </em>
</p></blockquote>
<p>
Bob could say, for example: </p>
<ol>
<li>
I plan to compute the inner product of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> and <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> for some <img alt="{v}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bv%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> I have. <p/>
</li><li>
I plan to see what the norm of <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is. <p/>
</li><li>
Or, I plan to see what <img alt="{x_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/> is. <p/>
</li><li>
Or, I could be just happy to know that there is some <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>. <p/>
</li><li>
Or, and so on.
</li></ol>
<p>
Thus, I believe, the answer only makes sense if Alice knows what will be done next with the “solution”. What do you think?</p>
<p>
</p><p/><h2> One View </h2><p/>
<p/><p>
What does it mean to solve the equation <img alt="{Ax=b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BAx%3Db%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>, for an invertible matrix <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0&amp;c=20201002"/>? What do precision, accuracy, conditioning, and complexity mean in this context?</p>
<p>
Jim Demmel’s view is captured in his notes that he was kind enough to download to the site <a href="https://app.slack.com/client/T021927P7ST/C021PQXNPEE">SLACK</a>. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
What do you think about this series of meetings? Did you attend them initially? Will you look in next time so we can break 200 attendees?</p>
<p>Santosh says: To join the seminar, please send an email<br/>
<a href="mailto: cmc-l-request@cornell.edu">Join Zoom</a><br/>
after adding the subject “join”. Information about how to connect to the Zoom conference call will be circulated via email to all registered attendees prior to each seminar.</p></font></font></div>
    </content>
    <updated>2021-05-13T12:17:45Z</updated>
    <published>2021-05-13T12:17:45Z</published>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="matrix"/>
    <category term="online"/>
    <category term="open problems"/>
    <category term="practice"/>
    <category term="Theory"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wpcomstaging.com</id>
      <logo>https://s0.wp.com/i/webclip.png</logo>
      <link href="https://rjlipton.wpcomstaging.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wpcomstaging.com" rel="alternate" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel's Lost Letter and P=NP</title>
      <updated>2021-05-24T11:37:38Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-1929264998273205739</id>
    <link href="https://blog.computationalcomplexity.org/feeds/1929264998273205739/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/cryptocurrency-blockchains-and-nfts.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1929264998273205739" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/1929264998273205739" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2021/05/cryptocurrency-blockchains-and-nfts.html" rel="alternate" type="text/html"/>
    <title>Cryptocurrency, Blockchains and NFTs</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p> I first wrote about bitcoin in this blog <a href="https://blog.computationalcomplexity.org/2011/11/making-money-computationally-hard-way.html">ten years ago</a> after I gave a lecture in a cryptography class I taught at Northwestern. Two years later I had a <a href="https://blog.computationalcomplexity.org/2013/12/bitcoins-revisited.html">follow-up post</a>, noting the price moved from $3 to $1000 with a market cap of about $11 Billion. My brother who thought they were a scam back then has since become a cryptocurrency convert. The bitcoin market cap is now over a trillion dollars and other cryptocurrencies are not far behind. No longer can we view cryptocurrencies as simply a neat exercise in applied cryptography now that it has serious value.</p><p>The main uses of cryptocurrencies are for speculation or illegal activities, such as drug sales, ransoms, money laundering and tax evasion. Sure you can buy a Tesla with bitcoins but that's more of a gimmick. Cryptocurrency spending is simply too slow, expensive and volatile right now to replace other methods of electronic payment. </p><p>Non-fungible tokens (NFTs) truly puzzle me. They are just a digital certificate of authentication. What could you do with them you couldn't do with docusign? Collectibles of publicly available digital goods is a fad already fading.</p><p>I'm not a fan of a fiat currency governed by strict rules not under governmental control. Bad things could happen. However thinking of cryptocurrencies and the blockchain technology that underlies them have brought up real needs for our digital world.</p><p/><ul style="text-align: left;"><li>An easy way to pay online without significant fees, expenses or energy consumption.</li><li>An easy and cheap way to transfer money between different countries.</li><li>A distributed database to allow tracking of supply chains, credentials and financial transactions for example. I see less a need to make these databases decentralized.</li><li>A need, for some, to have a digital replacement for the anonymity of cash.</li><li>People need something to believe in once they have given up believing in religion and a functioning democracy. </li></ul><div>Don't change your investing habits based on anything I write in this post. Speculation and illegal activities are powerful forces. Or it could all collapse. Make your bets, or don't.</div><div><br/></div><div><b>Note</b>: Since I wrote this post yesterday, Elon Musk <a href="https://twitter.com/elonmusk/status/1392602041025843203">tweeted</a> that Tesla will no longer accept bitcoins, and the bitcoin market cap has dropped below a trillion.</div><p/></div>
    </content>
    <updated>2021-05-13T11:55:00Z</updated>
    <published>2021-05-13T11:55:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2021-05-24T11:11:21Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2021/05/12/constructive-solid-geometry</id>
    <link href="https://11011110.github.io/blog/2021/05/12/constructive-solid-geometry.html" rel="alternate" type="text/html"/>
    <title>The constructive solid geometry of piecewise-linear functions</title>
    <summary>My latest preprint, “A stronger lower bound on parametric minimum spanning trees” (arXiv:2105.05371, to appear at WADS) gives examples of graphs, with edge weights that are linear functions of a parameter \(\lambda\), such that different choices of \(\lambda\) lead to \(\Omega(m\log n)\) different minimum spanning trees, improving a bound of \(\Omega\bigl(m\alpha(n)\bigr)\) from one of my earlier papers. But it was almost about a different problem in discrete geometry rather than graph theory, and it almost didn’t happen at all. I thought I had a bound for another related problem until the proof fell apart, irreparably. I was in the process of throwing away my mostly-written draft when I found a different proof, allowing me to rescue the paper.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>My latest preprint, “A stronger lower bound on parametric minimum spanning trees” (<a href="https://arxiv.org/abs/2105.05371">arXiv:2105.05371</a>, to appear at WADS) gives examples of graphs, with edge weights that are linear functions of a parameter \(\lambda\), such that different choices of \(\lambda\) lead to \(\Omega(m\log n)\) different minimum spanning trees, improving a bound of \(\Omega\bigl(m\alpha(n)\bigr)\) from <a href="https://doi.org/10.1007/PL00009396">one of my earlier papers</a>. But it was almost about a different problem in discrete geometry rather than graph theory, and it almost didn’t happen at all. I thought I had a bound for another related problem until the proof fell apart, irreparably. I was in the process of throwing away my mostly-written draft when I found a different proof, allowing me to rescue the paper.</p>

<p>Here’s the problem I thought I was solving when I started writing the paper: Suppose you want to <a href="https://en.wikipedia.org/wiki/Constructive_solid_geometry">construct a complicated shape using unions and intersections of simpler shapes</a>. For the version of the problem I was considering, the shapes belong to the Euclidean plane, and the simple shapes that you start with are the half-planes above a line. When you take unions or intersections of these shapes, the more complicated shapes that you get are sets of points above a piecewise linear \(x\)-monotone curve. Another way to understand the same setup is that you’re starting with linear functions and building more-complicated piecewise linear functions by taking pointwise maxima or minima. And what I wanted to know was: If you have a formula expressing a shape using unions and intersections of \(n\) upper halfplanes, or equivalently expressing a piecewise-linear function using maxima and minima of \(n\) linear functions, how complicated can the result be? I thought I had a proof that one could construct shapes with \(\Omega(n\log n)\) vertices, or piecewise-linear functions with \(\Omega(n\log n)\) breakpoints, and when it broke I thought I didn’t have a paper any more.</p>

<p style="text-align: center;"><img alt="Recursive construction of a piecewise linear function by maxima and minima of simpler functions" src="https://11011110.github.io/blog/assets/2021/minmax.svg"/></p>

<p>The figure above illustrates what I thought was the recursive construction. The base case, in the upper left, is a linear function (a piecewise-linear function with one piece, generated by a max-min formula with one term). In middle left we have a second-level function, the pointwise maximum of two of these linear functions, with two pieces. On the bottom left we have a third-level function, the pointwise minimum of two second-level functions, with six pieces. And the large image on the right shows a fourth-level function, the pointwise maximum of two third-level functions, with 16 pieces. At each level of recursion, you replace each line by two perturbed copies, getting a breakpoint where they cross. When you take a maximum, each breakpoint that looked like a local maximum expands to three breakpoints, while each breakpoint that looked like a local minimum stays as just a single breakpoint; the case of taking a minimum is symmetric. Setting up and solving a recurrence for the numbers of breakpoints of each type gives \(\Omega(n\log n)\).</p>

<p>The problem was that I couldn’t control the resulting piecewise-linear functions well enough to ensure that I could expand all of the local maxima into triple breakpoints and produce new breakpoints for each pair of crossing lines. These two issues are related, because you get a tripled breakpoint only for pairs of pairs of lines that have a certain above-below relation, and the breakpoint of a pair of crossing lines changes their above-below relation. It works for each step in the figure, but that’s because these cases are still too small for the problems to show up. So the analysis above breaks down.</p>

<p>As well as unions and intersections of shapes, or minima and maxima of functions, there’s another graph-theoretical interpretation of the same problem, and that’s where the rewritten paper comes in. The piecewise linear functions that you get from recursive unions and intersections correspond to parametric solutions to the <em>bottleneck shortest path problem</em>: find a path that connects two fixed vertices of a graph, whose heaviest edge is as light as possible, and let \(\beta\) (the bottleneck) be the weight of this edge. How does \(\beta\) vary as a function of \(\lambda\)? For <a href="https://en.wikipedia.org/wiki/Series%E2%80%93parallel_graph">series-parallel graphs</a>, the two vertices should be the two terminals, series composition of graphs gives you the maximum of their bottleneck functions, and parallel composition of graphs gives you the minimum of their bottleneck functions. So for these graphs, the parametric bottleneck shortest path problem is the same one that I didn’t solve.</p>

<p>However, the bottleneck shortest path problem is solved by the minimum spanning tree, in the sense that the path between two vertices in a minimum spanning tree is always a bottleneck shortest path (although there may be other equally good paths). Some of the breakpoints of the bottleneck function, the ones that look locally like a minimum of two linear functions, come from combinatorial changes in the parametric minimum spanning tree, and (by negating everything and swapping min for max if necessary) we can ensure that at least half of the changes in the worst-case bottleneck function come from spanning tree changes in this way. Therefore, lower bounds on the bottleneck problem extend to minimum spanning trees, and upper bounds on minimum spanning trees extend to the bottleneck problem. In fact, my previous \(\Omega\bigl(m\alpha(n)\bigr)\) bound on the spanning tree problem came from a \(\Omega\bigl(n\alpha(n)\bigr)\) bound on two-level piecewise linear functions (minima of maxima of linear functions), and a previous \(O(mn^{1/3})\) <a href="https://doi.org/10.1007/PL00009354">upper bound of Tamal Dey on the spanning tree problem</a> implies an \(O(n^{4/3})\) upper bound on the bottleneck problem.</p>

<p>So when my lower bound for the bottleneck problem fell apart, I instead started thinking about trying to find a similar recursive lower bound for spanning trees instead of bottleneck paths, and was more successful. It works more easily because I don’t have to control the piecewise linear functions so carefully in order to keep their crossings and breakpoints intact; instead, I can just take three copies of the lower level of the construction, flatten them by linear transformations so they are each close to a line, with their breakpoints in disjoint intervals of the \(\lambda\)-axis, and combine them as if they were linear. It wouldn’t work for the bottleneck problem because you would only get a constant number of new breakpoints where one of the recursive copies crosses over to the other, but for the spanning tree problem you’re combining trees rather than functions so you get more breakpoints in these regions.</p>

<p>The figure below gives an example of the construction, a series-parallel graph with six vertices (upper right) and linear edge weight functions (upper left) that produces 12 parametric minimum spanning trees (bottom). The red, blue, and green parts show the three copies of the recursive construction that are combined to form this example. For a more detailed explanation see the preprint.
The preprint also includes a packing argument that transforms the resulting \(\Omega(n\log n)\) bound for \(n\)-vertex series-parallel graphs into an \(\Omega(m\log n)\) bound for graphs whose number \(m\) of edges can be significantly larger than \(n\), but I think that’s more just a technicality.</p>

<p style="text-align: center;"><img alt="Six-vertex series-parallel graph with 12 parametric minimum spanning trees" src="https://11011110.github.io/blog/assets/2021/parametric-mst.svg"/></p>

<p>It would be interesting either to find a different construction proving that the halfspace / piecewise linear function / bottleneck path problem can have complexity \(\Omega(n\log n)\), matching this new result, or to prove an upper bound separating this problem from the lower bound on the parametric minimum spanning tree problem, but that will have to wait for another day.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/106225329805495195">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2021-05-12T17:21:00Z</updated>
    <published>2021-05-12T17:21:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2021-05-16T00:57:09Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2021/069</id>
    <link href="https://eccc.weizmann.ac.il/report/2021/069" rel="alternate" type="text/html"/>
    <title>TR21-069 |  PPSZ is better than you think | 

	Dominik Scheder</title>
    <summary>PPSZ, for long time the fastest known algorithm for k-SAT, works by going through the variables of the input formula in random order; each variable is then set randomly to 0 or 1, unless the correct value can be inferred by an efficiently implementable rule (like small-width resolution; or being implied by a small set of clauses).
We show that PPSZ performs exponentially better than previously known, for all k &gt;= 3. For Unique-3-SAT we bound its running time by O(1.306973n), which is somewhat better than the algorithm of Hansen, Kaplan, Zamir, and Zwick.
All improvements are achieved without changing the original PPSZ. The core idea is to pretend that PPSZ does not process the variables in uniformly random order, but according to a carefully designed distribution. We write "pretend" since this can be done without any actual change to the algorithm.</summary>
    <updated>2021-05-12T08:47:32Z</updated>
    <published>2021-05-12T08:47:32Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2021-05-24T11:37:26Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://gradientscience.org/glm_saga/</id>
    <link href="https://gradientscience.org/glm_saga/" rel="alternate" type="text/html"/>
    <title>Debuggable Deep Networks: Sparse Linear Models (Part 1)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><!-- <script src="//code.jquery.com/jquery-1.10.2.js"></script> -->


<!-- <script src="https://code.jquery.com/jquery-1.12.4.js"></script> -->
<!-- <script src=https://code.jquery.com/ui/1.12.1/jquery-ui.min.js></script> -->










<p><a class="bbutton" href="https://arxiv.org/abs/2105.04857" style="float: left; width: 45%;">
<i class="fas fa-file-pdf"/>
    Paper
</a>
<a class="bbutton" href="https://github.com/MadryLab/DebuggableDeepNetworks" style="float: left; width: 45%;">
<i class="fab fa-github"/>
   Code
</a>
<!-- <a class="bbutton" href="/breeds_class_hierarchy">
<i class="fa fa-tree"></i>
&nbsp;&nbsp; Hierarchies
</a> -->
<br/></p>

<p><i>This two-part series overviews our <a href="https://arxiv.org/abs/2105.04857">recent work</a> on constructing deep networks that perform well while, at the same time, being easier to debug. Part 1 (below) describes our toolkit for building such networks and how it can be leveraged in the context of typical language and vision tasks. This toolkit applies the classical primitive of sparse linear classification on top of feature representations derived from deep networks, and includes a custom solver for fitting such sparse linear models at scale. <a href="https://gradientscience.org/debugging">Part 2</a> outlines a suite of human-in-the-loop experiments that we designed to evaluate the debuggability of such networks. These evaluations demonstrate, in particular, that simply inspecting the sparse final decision layer of these networks can facilitate detection of unintended model behaviours—e.g., spurious correlations and input patterns that cause misclassifications. </i></p>

<p>As ML models are being increasingly deployed in the real world, a question that jumps to the forefront is: how do we know these models are doing “the right thing”? In particular, how can we be sure that models aren’t relying on brittle or undesirable correlations extracted from the data, which undermines their robustness and reliability?</p>

<p>It turns out that, as things stand today, we often can’t. In fact, numerous recent studies have pointed out that seemingly accurate ML models base their predictions on data patterns that are unintuitive or unexpected, leading to a variety of  downstream failures. For instance, in a <a href="https://gradientscience.org/adv/">previous post</a> we discussed how adversarial examples arise because models make decisions based on imperceptible features in the data. There are many other examples of this—e.g., image pathology detection models relying on <a href="https://cerre.eu/wp-content/uploads/2020/07/ai_explainability_whitepaper_google.pdf">pen marks made by radiologists</a>; and toxic comment classification systems being disproportionately sensitive to <a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification">identity-group related keywords</a>.</p>

<p>These examples highlight a growing need for model debugging tools: techniques which can facilitate the <i>semi-automatic</i> discovery of such failure modes. In fact, a closely related problem of interpretability—i.e., the task of precisely characterizing how and why models make their decisions, is already a major focus of the ML community.</p>

<h2 id="how-to-debug-your-deep-network">How to debug your deep network?</h2>

<p>A natural approach to model debugging is to inspect the model directly. While this may be feasible in certain settings (e.g., for small linear classifiers or decision trees), it quickly becomes  infeasible as we move towards large, complex models such as deep networks. To work around such scale issues, current approaches (spearheaded in the context of interpretability) attempt to understand  model behavior in a somewhat localized or decomposed manner. In particular, there exist two prominent families of deep network interpretability methods—one that attempts to explain what individual neurons do [<a href="https://arxiv.org/abs/1506.06579">Yosinski et al. 2015</a>, <a href="https://arxiv.org/abs/1704.05796">Bau et al. 2018</a>] and the other one aiming to discern how the model makes decisions for specific inputs [<a href="https://arxiv.org/abs/1312.6034">Simonyan et al. 2013</a>, <a href="https://arxiv.org/abs/1602.04938">Ribeiro et al. 2016</a>]. The challenge however is that, as shown in recent studies [<a href="https://arxiv.org/abs/1810.03292">Adebayo et al., 2018</a>, <a href="https://arxiv.org/abs/2011.05429">Adebayo et al., 2020</a>, <a href="https://arxiv.org/abs/2010.12016">Leavitt &amp; Morcos, 2020</a>], such localized interpretations can be hard to aggregate, are easily fooled, and overall, may not give a clear picture of the model’s reasoning process.</p>

<p>Our work thus takes an alternative approach. First, instead of trying to directly obtain a complete characterization of how and why a deep network makes its decision (which is the goal in  interpretability research), we focus on the more actionable problem of debugging unintended model behaviors. Second, instead of attempting to grapple with the challenge of analyzing these networks in a purely “post hoc” manner, we <i>train</i> them to make them inherently more debuggable.</p>

<p>The specific way we accomplish this goal is motivated by a natural view of a deep network as a composition of a <i>feature extractor</i> and a <i>linear decision layer</i> (see the figure below). From this viewpoint, we can break down the problem of inspecting and understanding a deep network into two subproblems: (1) interpreting the deep features (also known in the literature as neurons—that we will refer to as features henceforth) and (2) understanding how these features are aggregated in the (final) linear decision layer to make predictions.</p>

<p><img alt="Overview" src="https://gradientscience.org/assets/glm_saga/figures/intro.png"/></p>
<div class="footnote">
    <b> Overview of our approach to construct deep networks that are more debuggable:</b> We train a sparse decision layer on (pre-trained) deep feature embeddings and then view the network’s decision process as a linear combination of these features.
</div>

<p>Let us now discuss both of these subproblems in more detail.</p>

<h3 id="task-1-interpreting-deep-features">Task 1: Interpreting (deep) features</h3>

<p>Given the architectural complexity of deep networks, precisely characterizing the role of even a single neuron (in any layer) is challenging. However, research in ML interpretability has brought us a number of heuristics geared towards identifying the input patterns that cause specific neurons (or features) to activate. Thus, for the first task, we leverage some of these existing feature interpretation techniques—specifically, feature visualization, in case of vision models <a href="https://arxiv.org/abs/1904.08939">[Nguyen et al. 2019]</a> and LIME, in case of vision/language models <a href="https://arxiv.org/abs/1602.04938">[Ribeiro et al. 2016]</a>. While these methods have certain limitations, they turn out to be surprisingly effective for model debugging within our framework. Also, note that our approach is fairly modular, and we can substitute these methods with any other/better variants.</p>

<div class="footnote">
    Although LIME was originally used to interpret the predicted outputs of a network, in our work we adapt it to interpret individual neurons instead (see our <a href="https://arxiv.org/abs/2105.04857">paper</a> for more details). 
</div>

<p><img alt="Examples of feature visualization" src="https://gradientscience.org/assets/glm_saga/figures/fv_examples_both.png"/></p>
<div class="footnote">
    <b>Examples of feature visualizations for ImageNet classifiers:</b> Feature visualizations for standard vision models (<i>top</i>) are often hard to parse despite significant research on this front. This may be a side effect of these models relying on human-unintelligible features to make their predictions (discussed in a <a href="https://gradientscience.org/adv/">previous post</a>). On the other hand, robust vision models (<i>bottom</i>) tend to have more human-aligned features <a href="https://arxiv.org/abs/1906.00945">[Engstrom et al. 2019]</a>.
</div>

<p><img alt="Examples of word cloud visualization" src="https://gradientscience.org/assets/glm_saga/figures/wordclouds/wordcloud_top5_sst_6.png"/></p>
<div class="footnote">
    <b>Feature interpretation for language models</b>: Examples of a word cloud visualization for the positive and negative activation of a single neuron for a text sentiment classifier. We generate these by aggregating LIME explanations for features, with the whole process described in our <a href="https://arxiv.org/abs/2105.04857">paper</a>. 
</div>

<h3 id="task-2-examining-the-decision-layer">Task 2: Examining the decision layer</h3>

<p>At first glance, the task of making sense of the decision layer of a deep network appears trivial. Indeed, this layer is linear and interpreting a linear model is a routine task in statistical analysis.  However, this intuition is deceptive—the decision layers of modern deep networks often contain upwards of thousands of (deep) features and millions of parameters—making human inspection intractable.</p>

<p><img alt="Feature visualization dump" src="https://gradientscience.org/assets/glm_saga/figures/small_fv_dump.png"/></p>
<div class="footnote">
    <b>Scale of typical decision layers</b>: Feature visualizations for one quarter (512 out of 2048) of all the features of a robust ImageNet classifier. A typical dense decision layer will rely on a weighted sum of <i>all</i> of these features to produce a single prediction.
</div>

<p>So what can we do about this?</p>

<p>Recall that the major roadblock here is the size of the decision layer. What if we just constrained ourselves only to the “important” weights/features within this layer though? Would that allow us to understand the model?</p>

<p>To test this, we focus our attention on the features that are assigned large weights (in terms of magnitude) by the decision layer.  (Note that all the features are standardized to have zero mean and unit variance to make such a weight comparison more meaningful.)</p>

<p>In the figure below, we evaluate the performance of the decision layer when it is restricted to using: (a) only the “important features” or (b) all features <i>but</i> the important ones. The expectation here is that if the important features are to suffice for model debugging, they should at the very least be enough to let the model match its original performance.</p>

<div>
    <div class="ablation_dense">
        <canvas height="200" id="ablation_dense_chart" width="400"/>
    </div>
</div>
<div class="footnote">
     <b>Feature importance in dense decision layers:</b> Performance of the decision layer when it is restricted to using the "important" features vs the rest of the features. 
</div>

<p>As we can see, this is not the case for typical deep networks. Indeed, for all but one task, the top-k features (k is 10 for vision and 5 for language task) are far from sufficient to recover model performance. Further, there seems to be a great deal of redundancy in the standard decision layer—the model can perform quite well even without using any of the seemingly important features. Clearly, inspecting only the highest-weighted features does not seem to be sufficient from a debugging standpoint.</p>

<h4 id="our-solution-retraining-with-sparsity">Our solution: retraining with sparsity</h4>

<p>To make inspecting the decision layer more tractable for humans and also deal with feature redundancy, we replace that layer entirely. Specifically, rather than finding better heuristics for identifying salient features within the standard (dense) decision layer, we <i>retrain</i> it (on top of the existing feature representations) to be sparse.</p>

<p>To this end, we leverage a classic primitive from statistics: <i>sparse linear classifiers</i>. Concretely, we use the <a href="https://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf">elastic net</a> approach to train regularized linear decision layers on top of the fixed (pre-trained) feature representation.</p>

<p>The elastic net is a popular approach for fitting linear models in statistics, that combines the benefits of both L1 and L2 regularization.  Elastic net solvers yield not one but a series of sparse linear models—each with different sparsity/accuracy—based on the strength of regularization. We can then let our application-specific accuracy vs sparsity needs guide our choice of a specific sparse decision layer from this series.</p>

<p>However, when employing this approach to modern deep networks, we hit an obstacle—existing solvers for training regularized linear models simply cannot scale to the number of datapoints and input features that we would typically have in deep learning. To overcome this problem, we develop a custom, efficient solver for fitting regularized generalized linear models at scale. This solver leverages recent advances in <a href="https://arxiv.org/abs/1902.00071">variance reduced gradient methods</a> and combines them with <a href="https://web.stanford.edu/~hastie/Papers/glmnet.pdf">path-algorithms</a> from statistics to get fast and stable convergence at ImageNet scales. We won’t go into much detail here, but we point the curious reader to our <a href="https://arxiv.org/abs/2105.04857">paper</a> and our <a href="https://github.com/madrylab/glm_saga">standalone PyTorch package</a> (which might be of independent interest) for more information.</p>

<p>To summarize—the elastic net gives us a sparse decision layer that, in turn, enables us  to debug the resulting network by applying the existing feature interpretation methods to a now-significantly-reduced number of features (i.e., only the ones used by the sparse decision layer).</p>

<h2 id="what-do-we-gain-from-sparity">What do we gain from sparity?</h2>

<p>Now that we have our methodology in place, we can apply it to standard ML tasks and measure the impact of enforcing sparsity of the final decision layer. Specifically, we discuss the results of applying it to ResNet-50 classifiers trained on ImageNet and Places-10 (a 10-class subset of Places365), as well as BERT models trained on the Stanford Sentiment Treebank and Wikipedia toxic comment classification tasks.</p>

<h3 id="sparsity-at-the-last-layer-is-almost-free">Sparsity at the last layer is (almost) free</h3>

<p>Needless to say, the usefulness of our method hinges on the degree of sparsity in the decision layer that we can achieve without losing much accuracy. So how far can we turn the sparsity dial? The answer turns out to be: <i>a lot</i>! For instance, the final decision layer of an ImageNet classifier with 2048 features can be reduced by two orders of
magnitude, i.e., to use only 20 features per class, at the cost of only 2% test 
accuracy loss.</p>

<p>In the following demonstration, one can move the slider to the right to increase the density of the final decision layer of a standard ImageNet classifier. And, indeed, with only 2% of weights being non-zero, the model can already essentially match the performance (74%) of a fully dense layer.</p>

<div>
    <div id="reg_acc">
        <img id="reg" src="https://gradientscience.org/feed.xml"/>
        <div id="reg_slider"/>
        <div class="quarterblock"> </div>
        <div class="quarterblock" style="text-align: center;">Accuracy: <span id="reg_accuracy"/>%</div>
        <div class="quarterblock" style="text-align: center;">Non-zero: <span id="reg_sparsity"/>%</div>
        <div class="quarterblock"> </div>
    </div>
</div>
<div class="footnote">
    <b>Sparsity-accuracy trade-off:</b> A visualization of the sparsity of an ImageNet decision layer and its corresponding accuracy as a function of the regularization strength. Move the slider all the way to the right to get the fully dense layer (no regularization, 74% accuracy), or all the way to the left to get the fully sparse layer (maximum regularization, 5% accuracy). 
</div>

<h3 id="a-closer-look-at-sparse-decision-layers">A closer look at sparse decision layers</h3>

<p>Our key motivation for constructing sparse decision layers was that it enables us to manually examine the (reduced set of) features that a network uses. As we saw above, our modified decision layers rely on substantially fewer features per class—which already significantly aids their inspection by a human. But what if we go one step further and look only at the “important” features of our sparse decision layer, as we tried to do with the dense decision layer earlier?</p>

<div>
    <div class="">
        <div class="halfblock">
            <div class="rbutton block clicked sc ablation_button" id="ablation_dense">Dense</div>
        </div>
        <div class="halfblock">
            <div class="rbutton block sc ablation_button" id="ablation_sparse">Sparse</div>
        </div>
    </div>
    <div class="ablation">
        <canvas height="200" id="ablation_chart" width="400"/>
    </div>
</div>
<div class="footnote">
    <b>Feature importance in sparse and dense decision layers:</b> Performance of the decision layer when it is restricted to using the "important" features vs the rest of the features. Try toggling between the two to see the effects of sparsity. 
</div>

<p>As we can see below, for models with a sparse decision layer, the top 5-10 important features are necessary and almost sufficient for capturing the model’s performance. That is, (i) accuracy drops to near chance levels (1/number of classes) if the model does not leverage these features and (ii) using these features alone, the model can nearly recover its original performance. This indicates that the sparsity constraint not only reduces the number of features used by the model, but also makes it easier to rank features based on their importance.</p>

<h3 id="sparse-decision-layers-an-interactive-demonstration">Sparse decision layers: an interactive demonstration</h3>

<p>In the following interactive demonstration, you can explore a subset of the decision layer of a (robust) ResNet-50 on ImageNet with either a sparse or dense decision layer:</p>

<div>
    <div class="">
        <div class="halfblock">
            <div class="rbutton block clicked sc glm_button" id="dense">Dense</div>
        </div>
        <div class="halfblock">
            <div class="rbutton block sc glm_button" id="sparse">Sparse</div>
        </div>
    </div>
    <div class="">
        <div class="quarterblock">
            <div class="rbutton block clicked sc class_button" id="576">Gondola</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc class_button" id="415">Bakery</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc class_button" id="292">Tiger</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc class_button" id="537">Dogsled</div>
        </div>
    </div>
    <div class="" id="linear">
        <div class="block sc" id="glm_class_name">Tiger</div>
        
            
            
            
            
            
            
            
            
            
            
        
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
        <div class="tenthblock">
            <div class="rbutton block feature_button">
                <span class="glm_weight" style="text-align: center;"/>
                <img class="smallimg" src="https://gradientscience.org/feed.xml"/>
            </div>
        </div>
    </div>
    <div id="zoom">
        <img id="feature_big" src="https://gradientscience.org/feed.xml"/>
        
    </div>
</div>
<div class="footnote">
    <b>An interactive demo of the sparse decision layer:</b> Select a dense or sparse model and a corresponding ImageNet class to visualize the features and weights for the corresponding decision layer. The opacity of each features corresponds to the magnitude of its weight in the decision layer, and you can click on a feature to see a larger version of it. 
</div>

<p>Finally, one should note that the features used by sparse decision layers seem somewhat more human-aligned than the ones used by the standard (dense) decision layers. This observation coupled with our previous ablations studies indicate that sparse decision layers could offer a path towards more debuggable deep networks. But, is this really the case? In our <a href="https://gradientscience.org/debugging">next post</a>, we will evaluate whether models obtained via our methodology are indeed easier for humans to understand, and whether they truly aid the diagnosis of unexpected model behaviors.</p></div>
    </summary>
    <updated>2021-05-12T00:00:00Z</updated>
    <published>2021-05-12T00:00:00Z</published>
    <source>
      <id>https://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="https://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="https://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2021-05-23T22:59:32Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://gradientscience.org/debugging/</id>
    <link href="https://gradientscience.org/debugging/" rel="alternate" type="text/html"/>
    <title>Debuggable Deep Networks: Usage and Evaluation (Part 2)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><!-- <script src="//code.jquery.com/jquery-1.10.2.js"></script> -->


<!-- <script src="https://code.jquery.com/jquery-1.12.4.js"></script> -->
<!-- <script src=https://code.jquery.com/ui/1.12.1/jquery-ui.min.js></script> -->










<!-- fancybox -->




<p><a class="bbutton" href="https://arxiv.org/abs/2105.04857" style="float: left; width: 45%;">
<i class="fas fa-file-pdf"/>
    Paper
</a>
<a class="bbutton" href="https://github.com/MadryLab/DebuggableDeepNetworks" style="float: left; width: 45%;">
<i class="fab fa-github"/>
   Code
</a>
<!-- <a class="bbutton" href="/breeds_class_hierarchy">
<i class="fa fa-tree"></i>
&nbsp;&nbsp; Hierarchies
</a> -->
<br/></p>

<p><i>This is the second part of the overview of our <a href="https://arxiv.org/abs/2105.04857">recent work</a> on training more debuggable deep networks. In our <a href="https://gradientscience.org/glm_saga">previous post</a>, we outlined our toolkit for constructing such networks, which involved training (very) sparse linear classifiers on (pre-trained) deep feature embeddings and viewing the network’s decision process as a linear combination of these features. In this post, we will delve deeper into evaluating to what extent these networks are amenable to debugging. Specifically, we want to get a sense of whether humans are able to intuit their behavior and pinpoint their failure modes.</i></p>

<h2 id="do-our-sparse-decision-layers-truly-aid-human-understanding">Do our sparse decision layers truly aid human understanding?</h2>

<p>Although our toolkit enables us to greatly simplify the network’s decision layer (by reducing the number of its weights and thus the features it relies on), it is not immediately obvious whether this will make debugging such models significantly easier.  To properly examine  this, we need to factor humans into the equation. One way to do that is to leverage the notion of <a href="https://arxiv.org/abs/1606.03490">simulatibility</a> used in the context of ML interpretability. According to this notion, an interpretability method is “good” if it can enable a human to reproduce the model’s decision. In our setup, this translates into evaluating how sparsity of the final decision layer influences humans’ ability to predict the model’s classification decision (irrespective of whether this decision is “correct” or not).</p>

<h4 id="the-simulatibility-study">The “simulatibility” study</h4>

<p>One approach to assess simulatibility  would be to ask annotators to guess what the model will label an input (e.g., an image) as, given an interpretation corresponding to that input. However, for non-expert annotators, this might be challenging due to the large number of (often fine-grained) classes that a typical dataset contains. Additionally, human cognitive biases may also muddle the evaluation—e.g., it may be hard for annotators to decouple “what they think the model should label the input as” from “what the interpretation suggests the model actually does” (and we are interested in the latter).</p>

<p>To alleviate these difficulties, we resort instead to the following task setup (conducted using an ImageNet-trained ResNet):</p>

<ol>
  <li>We pick a target class at random, and show annotators visualizations of five randomly-selected features used by the sparse decision layer to detect objects of this class, along with their relative weights.</li>
  <li>We present the annotators with three images from the validation set with varying (but still non-trivial) probabilities of being classified by the model as the target class. (Note that each of these images can potentially belong to different, non-target classes.)</li>
  <li>Finally, we ask annotators to pick which one among these three images they believe to best match the target class.</li>
</ol>

<div class="footnote">
    As mentioned in <a href="https://gradientscience.org/glm_saga">part 1</a>, feature visualizations for standard vision models are often hard to parse, so we use <a href="https://arxiv.org/abs/1906.00945">adversarially-trained models</a> for this study. 
</div>

<p>Here is a sample task (click to enlarge):</p>

<p><a href="https://gradientscience.org/assets/glm_saga/figures/hit_example_sim.png"><img height="350" src="https://gradientscience.org/assets/glm_saga/figures/hit_example_sim.png"/></a></p>

<p>Overall, our intention is to gauge whether humans can intuit which image (out of three) is most prototypical for the target class <i>according to the model</i>. Note that we do not show annotators any information about the target class—such as its name or description—other than illustrations of some of the features that the model uses to identify it.  As discussed previously, this is intentional: we want annotators to select the image that <i>visually</i> matches the features used by the model, instead of using their prior knowledge to associate images with the target label itself.  For instance, if the annotators know that the target label was “car”, they might end up choosing the image that most closely resembles their idea of a car—independent of (or even in contradiction to) how the model actually detects cars. In fact, the “most activating image” in our setup may not even belong to the target class.</p>

<p>Now, how well do humans do on this task?</p>

<p>We find that (MTurk) annotators are pretty good at simulating the behavior of our modified networks—they correctly guess the top activating image (out of three) 63% of the time! In contrast, they essentially fail, with only a 35% success rate (i.e., near chance), when this task is performed using models with standard, i.e., dense, decision layers. This suggests that even with a very simple setup—showing non-experts some of the features the sparse decision layer uses to recognize a target class—humans are actually able to emulate the behavior of our modified networks.</p>

<h2 id="debuggability-via-sparsity">Debuggability via Sparsity</h2>

<p>So far, we identified a number of advantages of employing sparse decision layers, such as having fewer components to analyze, selected features being more influential, and better human simulatibility. But what unintended model behaviors can we (semi-automatically) identify by just probing such decision layers?</p>

<h3 id="uncovering-spurious-correlations-and-biases">Uncovering (spurious) correlations and biases</h3>

<p>Let’s start with trying to uncover model biases. After all, it is by now evident that deep networks rely on undesired correlations extracted from the training data (e.g. <a href="https://gradientscience.org/background">backgrounds</a>, <a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification">identity-related keywords</a>). But can we pinpoint this behavior without resorting to a targeted examination?</p>

<h4 id="bias-in-toxic-comment-classification">Bias in toxic comment classification</h4>

<p>In 2019, Jigsaw hosted a <a href="https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification">competition on Kaggle</a> around creating  toxic comment detection systems. This effort was prompted by that fact that the systems available at the time were found to have incorrectly learned to associate the names of frequently attacked identities (e.g., nationality, religion or sexual identity) with toxicity, and so the goal of the competition was to construct a
“debiased” system. Can we understand to what extent this effort succeeded?</p>

<p>To answer this question we leverage our methodology and fit a sparse decision layer to the debiased model released by the contest organizers, and then inspect the utilized deep features. An example result is shown below:</p>

<p><img alt="Wordcloud visualization of feature used in unbiased BERT" src="https://gradientscience.org/assets/glm_saga/figures/wordclouds/wordcloud_top5_jigsaw-alt-toxic_6_redacted.png"/></p>
<div class="footnote">
    <b>Interpreting the deep features of a debiased sentiment classifier:</b>
    A word cloud visualization (with some of the words redacted) for a deep feature of the debiased model (with a sparse decision layer). The negative activation of this feature turns out to be influenced by Christianity-related words. 
</div>

<p>Looking at this visualization, we can observe that the debiased model no longer positively associates identity terms with toxicity (refer our <a href="https://arxiv.org/abs/2105.04857">paper</a> for a similar visualization corresponding to the original biased model). This seems to be a success—after all, the goal of the competition was to correct the over-sensitivity of prior models to identity-group keywords. However, upon closer inspection, one will note that the model has actually learned a strong, <i>negative</i> association between these keywords and comment toxicity. For example, one can take a word such as “christianity” and append it to toxic sentences to trick the model into thinking that these are non-toxic 74% of the time. One can try it by selecting words to add to the sentence below:</p>

<div>
    <div class="">
        <div class="quarterblock">
            <div class="rbutton block clicked sc toxic_button" id="toxic_" value="">None</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc toxic_button" id="toxic_christianity" value="christianity">+christianity</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc toxic_button" id="toxic_African" value="African">+African</div>
        </div>
        <div class="quarterblock">
            <div class="rbutton block sc toxic_button" id="toxic_Catholic" value="Catholic">+Catholic</div>
        </div>
    </div>
    <div id="toxic_confidence">
        <b>Sentence:</b> Jeez Ed, you seem like a ******* ****** ********* <span id="toxic_add"/>
        <canvas height="200" id="toxic" width="400"/>
    </div>
</div>
<div class="footnote">
    <b>Bias detection in language models: </b> using sparse decision layers we find that the debiased model is still oversensitive to keywords corresponding to frequently attacked identity group, although in the opposite sense from the previous model.
</div>

<p>So, what we see is that rather than being debiased, newer toxic comment detection systems remain disproportionately sensitive to identity terms—it is just the nature of the sensitivity that changed.</p>

<h4 id="spurious-correlations-in-imagenet">Spurious correlations in ImageNet</h4>

<p>In the NLP setting, we can directly measure correlations between the model’s predictions and input data patterns by toggling specific words or phrases in the input corpus. However, it is not obvious how to replicate such analysis in the vision setting. After all, we don’t have automated tools to decompose images into a set of human understandable patterns akin to words or phrases (e.g., “dog ears” or “wheels”).</p>

<p>We thus leverage instead a human-in-the-loop approach that uses (sparse) decision layer inspection as a primitive. Specifically, we enlist annotators on MTurk to identify and describe data patterns that activate individual features that the sparse decision layer uses (for a given class). This in turn allows us to pinpoint the correlations the model has learned between the input data and that class.</p>

<p>Concretely, to identify the data patterns that are positively correlated with a particular (deep) feature, we present to MTurk annotators a set of images that strongly activate it. The expectation here is that if a set of images activate a given  feature, these images should share a common input pattern and the annotators will be able to identify it.</p>

<div class="footnote">
Note that we show annotators images from multiple (two) classes that strongly activate a single feature. This is because images from any single class may have many input patterns in common—only some of which actually activate a specific feature. 
</div>

<p>We then ask annotators: (a) whether they see a common pattern in the images, and, if so, (b) to provide a free text description of that pattern. If the annotators identify a common input pattern, we also ask them if the identified pattern belongs to the class object (“spurious”) or its surroundings (“non-spurious”) for each of the two classes.</p>

<div class="footnote">
In general, we recognize that precisely defining spurious correlations might be challenging and context-dependent. Our definition of spurious correlations was chosen to be objective and easy for annotators to assess.
</div>

<p>Here is an example of the annotation task (click to expand):</p>

<p><a href="https://gradientscience.org/assets/glm_saga/figures/hit_example_spurious.png"><img height="350" src="https://gradientscience.org/assets/glm_saga/figures/hit_example_spurious.png"/></a></p>

<p>Here are a few examples of (spurious) correlations identified by annotators:</p>

<div class="widget">
<span class="widgetheading" id="spurious">Select a class pair:</span>
<div class="choices_one_diff" id="sp"/>
<div class="choices_one_half" id="spuriousimages"> </div>
<div class="choices_one_quarter" id="wcimage"> </div>
<!-- <div class="choices_info">
    <div class="choices_info_text" id="spuriousinfo"> </div>
  </div> -->
</div>
<div style="clear: both;"/>
<div class="footnote">
<b>Detecting input-class correlations in vision models: </b> Select a class pair on the top to see the annotator-provided description for the deep feature that is activated by images of these classes (<i>left</i>). The free-text description provided by the annotators is visualized as a wordcloud (<i>right</i>), along with their selections for whether this input pattern is part of the class object ("non-spurious") or its surroundings ("spurious").
</div>

<p>Note that, one can, in principle, use the same human-in-the-loop methodology to identify input correlations extracted by standard deep networks (with dense decision layers). However, since these models rely on a large number of (deep) features to detect objects of every class, this process can quickly become intractable (see our <a href="https://arxiv.org/abs/2105.04857">paper</a> for details).</p>

<p>The above studies demonstrate that for typical vision and NLP tasks, sparsity in the decision layer makes it easier to look deeper into the model and understand what patterns it has extracted from its training corpus.</p>

<h3 id="creating-effective-counterfactuals">Creating effective counterfactuals</h3>

<p>Our second approach for characterizing model failure modes uses the lens of counterfactuals. We specifically focus on counterfactuals that are (minor) variations of given inputs that prompt the model to make a different prediction. Counterfactuals can be very helpful from a debugging standpoint—they can confirm that specific input patterns are not just correlated with the model prediction but actually causally influence them. Additionally, such counterfactuals can be used to provide recourse to users—e.g., to let them realize what attributes (e.g., credit rating) they should change to get the desired outcome (e.g., granting a loan). We will now discuss how to leverage the correlations identified in the previous section to construct counterfactuals for models with sparse decision layers.</p>

<h4 id="language-counterfactuals-in-sentiment-classification">Language counterfactuals in sentiment classification</h4>

<p>In sentiment classification, the task is to label a given sentence as having either positive or negative sentiment. Here, we consider counterfactuals via word substitution, effectively asking “what word could I have used instead to change the sentiment predicted by the model for a given sentence?”</p>

<p>To this end, we consider the words that are positively and negatively correlated with features used by the sparse decision layer as candidates for word substitution. For example, the word “astounding” activates a feature that a BERT model uses to detect positive sentiment, whereas the word “condescending” is negatively correlated with the activation of this feature. By substituting such a positively-correlated word with its negatively-correlated counterpart, we can effectively “flip” the corresponding feature. A demonstration of this process is shown below:</p>

<div>
    <div class="halfblock">
        <table class="reg_table">
            <tbody><tr>
                <th class="reg_header" colspan="3">Positive activation</th>
            </tr>
            <tr class="reg_cell">
                <td class="positive_cell">impressed</td><td class="positive_cell">brings</td><td class="positive_cell">marvel</td>
            </tr>
            <tr class="reg_cell">
                <td class="positive_cell">exhilirated</td><td class="positive_cell main_cell rbutton cf_button">astounding</td><td class="positive_cell">completes</td>
            </tr>
            <tr class="reg_cell">
                <td class="positive_cell">hilariously</td><td class="positive_cell">successfully</td><td class="positive_cell">yes</td>
            </tr>
        </tbody></table>
    </div>
    <div class="halfblock">
        <table class="reg_table">
            <tbody><tr>
                <th class="reg_header" colspan="3">Negative activation</th>
            </tr>
            <tr class="reg_cell">
                <td class="negative_cell rbutton cf_button">idiots</td><td class="negative_cell rbutton cf_button">inconsistent</td><td class="negative_cell rbutton cf_button">maddening</td>
            </tr>
            <tr class="reg_cell">
                <td class="negative_cell rbutton cf_button">cheat</td><td class="negative_cell rbutton cf_button">condescending</td><td class="negative_cell rbutton cf_button">failure</td>
            </tr>
            <tr class="reg_cell">
                <td class="negative_cell rbutton cf_button">dahmer</td><td class="negative_cell rbutton cf_button">pointless</td><td class="negative_cell rbutton cf_button">unseemly</td>
            </tr>
        </tbody></table>
    </div>
    <div id="sst_counterfactual">
        <b>Sentence:</b> The acting, costumes, music, cinematography and sound are all <i>[<span id="word_counterfactual">astounding</span>]</i> given the proudction's austere locales.
        <canvas height="200" id="sst_canvas" width="400"/>
    </div>
</div>
<div class="footnote">
<b>Language counterfactuals:</b> A wordcloud visualization for a deep feature (used by the sparse decision layer) that positively activates for the  sentence shown above. By replacing the specific word that activated this feature (in this case "astounding"), with any word that  deactivates it (<i>select on the right</i>), we can effectively flip the sentiment predicted by the model. In this way, we can construct counterfactuals for our modified deep networks via one-word substitutions.
</div>

<p>It turns out that these one-word modifications are indeed already quite successful (i.e., they cause a change in the model’s prediction 73% of the time). The obtained sentence pairs—which can be viewed as counterfactuals for one another—allow us to gain insight into data patterns that cause the model to predict a certain outcome. Finally, we find that for standard models finding effective counterfactuals that flip the model’s prediction is harder—the one-word modifications described above can  only change the model’s decision in 52% of cases.</p>

<h4 id="imagenet-counterfactuals">ImageNet counterfactuals</h4>
<p>For ImageNet-trained models, we can directly use the patterns <a href="https://gradientscience.org/feed.xml#spurious-correlations-in-imagenet">previously</a> identified by the annotators to generate counterfactual images that change its prediction. To this end, we manually modify images to add or subtract these patterns and observe the effect of this operation on the model’s decision.</p>

<p>For example, annotators identify a background feature “chainlink fence” to be spuriously
correlated with “ballplayers”. Using this information, we can then take images
of people playing basketball or tennis (correctly labeled as “basketball” or
“racket” by the model) and manually insert a “chainlink fence” into the
background, which successfully changes the model’s prediction to “ballplayer”.</p>

<p><img alt="ImageNet counterfactuals" src="https://gradientscience.org/assets/glm_saga/figures/counterfactuals.png"/></p>
<div class="footnote">
<b>Counterfactuals for ImageNet classifiers:</b> By adding specific spurious patterns to correctly-classified images (<i>top</i>), we can fool the model into predicting the desired class (<i>bottom</i>). 
</div>

<p>Thus, the counterfactuals that our methodology produced indeed allow us to identify data patterns that are causally linked to the model’s decision making process.</p>

<h3 id="identifying-reasons-for-misclassification">Identifying reasons for misclassification</h3>
<p>Finally, we turn our attention to debugging model errors. After all, when our models are wrong, it would be helpful to know why this was the case.</p>

<p>In the ImageNet setting, we find that many (over 30%) of the misclassifications of the 
sparse-decision-layer models can be attributed to a single “problematic”
feature. That is, manually removing this feature results in a correct prediction. One can thus view the feature interpretation for this problematic feature as a justification for the model’s error.</p>

<p><img alt="Problematic features" src="https://gradientscience.org/assets/glm_saga/figures/problematic.png"/></p>
<div class="footnote">
<b>A closer look at ImageNet misclassifications:</b> Examples of erroneously classified ImageNet images (<i>top</i>), along with the feature visualization for the "problematic feature" from the incorrect class (<i>bottom</i>). We find that manually setting the activation of this problematic feature to zero is sufficient to fix the model's mistake in each of these cases.
</div>

<p>Ideally, given such a justification, we would like humans to be able to identify the part of the image corresponding to the problematic feature that caused the model to make a mistake. How can we evaluate whether this is the case?
Namely, can we obtain an unbiased assessment of whether the data patterns that activate the problematic feature are noticeably present in the misclassified image?</p>

<p>To answer this question, we conduct a study on MTurk wherein we present annotators with an image, along with feature visualizations for: (i) the most activated feature from the true class and (ii) the problematic feature that is activated for the erroneous class. We do not explicitly tell the annotators what classes these features correspond to. We then ask annotators to select the patterns (feature visualizations) that match the image, and to determine which pattern is a better match if they select both.</p>

<p>Here is an example of a task we present to the annotators (click to expand):</p>

<p><a href="https://gradientscience.org/assets/glm_saga/figures/hit_example_mis.png"><img height="350" src="https://gradientscience.org/assets/glm_saga/figures/hit_example_mis.png"/></a></p>

<div class="footnote">
As a control, we also rerun this experiment while replacing the problematic feature with a randomly-chosen feature. This serves as a baseline to compare annotator selection for the features from the true/incorrect classes. 
</div>

<p>It turns out that not only do annotators frequently (70% of the time) identify the top feature from the wrongly-predicted class as present in the image, but also that this feature is actually a better match than the top feature for the ground truth class (60% of the time). In contrast, annotators select the control (randomly-chosen) deep feature to be a match for the image only 16% of the time. One can explore some examples here:</p>

<div class="widget">
<span class="widgetheading" id="misclass">Inspect misclassified images:</span>
<div class="choices_one_full" id="mis"/>
  <div class="blocktxt" id="mislabels" style="float: none;"> </div>
  <div id="misimages" style="clear: both;"> </div>
</div>
<div style="clear: both;"/>
<div class="footnote">
<b>Misclassifications validated by MTurk annotators: </b> Select an image on the top to see its true and predicted labels, along with the most highly activated deep feature (of those used by the sparse decision layer) for both these classes. In all cases, annotators select the top feature from the (incorrect) predicted class to be present in the image, and to be a better match than the top feature from the true class.
</div>

<p>This experiment validates (devoid of confirmation biases from the class label) that humans can identify the data patterns that trigger the error-inducing problematic deep features. Note that once these patterns have been identified, one can examine them to better understand the root cause (e.g., issues with the training data) for model errors.</p>

<h2 id="conclusions">Conclusions</h2>

<p>Over the course of this two-part series, we have shown that a natural approach of fitting sparse linear models over deep feature representations can already be surprisingly effective in creating more debuggable deep networks. In particular, we saw that models constructed using this methodology are more concise and amenable to human understanding—making it easier to detect and analyze unintended behaviors such as biases and misclassification. Going forward, this methodology of modifying the network architecture to make it inherently easier to probe can offer an attractive alternative to the existing paradigm of purely post-hoc debugging. Additionally, our analysis introduces a suite of human-in-the-loop techniques for model debugging at scale and thus can help guide further work in this field.</p>







<span class="choices_info_text"/><br/><span class="choices_info_text" style="color: red;"><b/></span><br/><span class="choices_info_text" style="color: green;"><b/></span><br/><hr/><h3 style="text-align: center;"><h3><div class="sp_txt" style="text-align: center; font-weight: 300; margin: 0.75em auto;"/><div class="wc_img blockimg"><img src="https://gradientscience.org/&quot; + base +                     &quot;wc_&quot; + pair + &quot;.png"/><hr style="margin: 0.75em auto;"/><div class="sp_txt" style="text-align: center; font-weight: 200;"><span/></div><hr style="margin: 0.3em auto;"/><h3 style="text-align: center;"><h3><div class="sp_txt" style="text-align: center;"><span style="font-weight: 200;"/></div><br/><span/></h3></h3></div><div class="sp_img blockimg"><img src="https://gradientscience.org/&quot; + base +                     &quot;sample_&quot; + pair + &quot;_&quot; + i + &quot;.png"/></div><div class="mis_txt blocktxt thirdblock"><span class="widgetheading"/><span class="choices_info_text"/><br/><span class="choices_info_text"/><div class="mis_txt blocktxt thirdblock"><br/><span class="widgetheading"/></div><div class="mis_img blockimg thirdblock"><img src="https://gradientscience.org/&quot; + origSrc + &quot;"/></div><div class="mis_img blockimg thirdblock"><img src="https://gradientscience.org/&quot; + base +                     &quot;dst_&quot; + pair + &quot;_&quot; + i + &quot;.png"/></div></div></h3></h3></div>
    </summary>
    <updated>2021-05-12T00:00:00Z</updated>
    <published>2021-05-12T00:00:00Z</published>
    <source>
      <id>https://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="https://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="https://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2021-05-23T22:59:33Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=8117</id>
    <link href="https://windowsontheory.org/2021/05/11/stoc-test-of-time-award/" rel="alternate" type="text/html"/>
    <title>STOC Test of time award</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">A reminder: the deadline to submit nominations for the STOC Test of Time Award is May 24. You can nominate papers for the 10 year award – STOC 2007-2011 20 year award – STOC 1997-2001 30 year award – STOC 1987-1991 The award website ( https://sigact.org/prizes/stoc_tot.html ) helpfully contains links to the papers published in … <a class="more-link" href="https://windowsontheory.org/2021/05/11/stoc-test-of-time-award/">Continue reading <span class="screen-reader-text">STOC Test of time award</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A reminder: the deadline to submit nominations for the <a href="https://sigact.org/prizes/stoc_tot.html">STOC Test of Time Award</a> is <strong>May 24</strong>.  You can nominate papers for the </p>



<ul><li>10 year award – STOC 2007-2011</li><li>20 year award – STOC 1997-2001</li><li>30 year award – STOC 1987-1991<br/><br/>The award website ( <a href="https://sigact.org/prizes/stoc_tot.html">https://sigact.org/prizes/stoc_tot.html </a>) helpfully contains links to the papers published in all these conferences. <br/><br/>Please nominate the papers you think have most influenced our field!</li></ul>



<p/></div>
    </content>
    <updated>2021-05-11T18:28:47Z</updated>
    <published>2021-05-11T18:28:47Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>Boaz Barak</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2021-05-24T11:38:01Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2021/05/10/two-phd-postdoc-positions-in-algorithms-and-complexity-theory-at-goethe-university-of-frankfurt-apply-by-june-15-2021/</id>
    <link href="https://cstheory-jobs.org/2021/05/10/two-phd-postdoc-positions-in-algorithms-and-complexity-theory-at-goethe-university-of-frankfurt-apply-by-june-15-2021/" rel="alternate" type="text/html"/>
    <title>Two PhD/Postdoc Positions in Algorithms and Complexity Theory at Goethe-University of Frankfurt (apply by June 15, 2021)</title>
    <summary>The research group conducts research on fundamental questions of computation and information, is driven by curiosity, and provides a friendly, open-minded, and positive social environment. Potential research topics include algebraic graph algorithms, the theory of machine learning on graphs, circuit complexity, pseudorandomness, fine-grained and parameterized complexity. Includes some teaching. Website: https://tcs.uni-frankfurt.de/positions/ Email: tcs-applications@dlist.uni-frankfurt.de</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The research group conducts research on fundamental questions of computation and information, is driven by curiosity, and provides a friendly, open-minded, and positive social environment. Potential research topics include algebraic graph algorithms, the theory of machine learning on graphs, circuit complexity, pseudorandomness, fine-grained and parameterized complexity. Includes some teaching.</p>
<p>Website: <a href="https://tcs.uni-frankfurt.de/positions/">https://tcs.uni-frankfurt.de/positions/</a><br/>
Email: tcs-applications@dlist.uni-frankfurt.de</p></div>
    </content>
    <updated>2021-05-10T15:29:28Z</updated>
    <published>2021-05-10T15:29:28Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2021-05-24T11:37:42Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=5486</id>
    <link href="https://www.scottaaronson.com/blog/?p=5486" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=5486#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=5486" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">Three updates</title>
    <summary xml:lang="en-US">For those who read my reply to Richard Borcherds on “teapot supremacy”: seeking better data, I ordered a dozen terra cotta flowerpots, and smashed eight of them on my driveway with my 4-year-old son, dropping each one from approximately 2 meters. For each flowerpot, we counted how many pieces it broke into, seeking insight about […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><ol><li>For those who read my <a href="https://www.scottaaronson.com/blog/?p=5460">reply to Richard Borcherds on “teapot supremacy”</a>: seeking better data, I ordered a dozen terra cotta flowerpots, and smashed eight of them on my driveway with my 4-year-old son, dropping each one from approximately 2 meters.  For each flowerpot, we counted how many pieces it broke into, seeking insight about the distribution over that number.  Unfortunately, it <em>still</em> proved nearly impossible to get good data, for a reason commenters had already warned me about: namely, there were typically 5-10 largeish shards, followed by “long tail” of smaller and smaller shards (eventually, just terra cotta specks), with no obvious place to draw the line and stop counting.  Nevertheless, when I attempted to count only the shards that were “fingernail-length or larger,” here’s what I got: 1 pot with 9 shards, 1 with 11 shards, 2 with 13 shards, 2 with 15 shards, 1 with 17 shards, 1 with 19 shards.  This is a beautiful (too beautiful?) symmetric distribution centered around a mean of 14 shards, although it’s anyone’s guess whether it approximates a Gaussian or something else.  I have <em>no idea</em> why every pot broke into an odd number of shards, unless of course it was a 1-in-256-level fluke, or some cognitive bias that made me preferentially stop counting the shards at odd numbers.<br/></li><li>Thanks so much to everyone who congratulated me for the <a href="https://www.scottaaronson.com/blog/?p=5448">ACM Prize</a>, and especially those who (per my request) suggested charities to which to give bits of the proceeds!  Tonight, after going through the complete list of suggestions, I made my first, but far from last, round of donations: $1000 each to the <a href="https://www.evidenceaction.org/dewormtheworld/">Deworm the World Initiative</a>, <a href="https://www.givedirectly.org/?gclid=CjwKCAjwkN6EBhBNEiwADVfya1RLgM2x4aobbEZ9yTMwTgLbgCdW77zHuI1h5avh0ysXmUHvLYw_vxoCWtcQAvD_BwE">GiveDirectly</a>, the <a href="https://support.worldwildlife.org/site/Donation2?df_id=14650&amp;14650.donation=form1&amp;s_src=AWE2010OQ18507A04091RX&amp;gclid=CjwKCAjwkN6EBhBNEiwADVfya2ZHOOTObCbQVxvbv-R-KF6XGSu8klv7OL_F8WJwFaFyCIgaCBIXexoCaeUQAvD_BwE">World Wildlife Fund</a>, the <a href="https://www.nature.org/en-us/">Nature Conservancy</a>, and <a href="https://www.mathcamp.org/">Canada/USA Mathcamp</a> (which had a huge impact on me when I attended it as a 15-year-old).  One constraint, which might never arise in a decade of moral philosophy seminars, ended up being especially important in practice: if the donation form was confusing or buggy, or if it wouldn’t accept my donation without some onerous confirmation step involving a no-longer-in-use cellphone, then I simply moved on to the next charity.<br/></li><li>Bobby Kleinberg asked me to advertise the <a href="https://sigact.org/prizes/stoc_tot.html">call for nominations</a> for the brand-new STOC Test of Time Award.  The nomination deadline is coming up soon: May 24. </li></ol>



<p/></div>
    </content>
    <updated>2021-05-10T05:47:52Z</updated>
    <published>2021-05-10T05:47:52Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2021-05-21T20:52:21Z</updated>
    </source>
  </entry>
</feed>
