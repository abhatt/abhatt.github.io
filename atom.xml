<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-05-05T14:23:06Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00791</id>
    <link href="http://arxiv.org/abs/1905.00791" rel="alternate" type="text/html"/>
    <title>Flip Distance to some Plane Configurations</title>
    <feedworld_mtime>1557014400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Biniaz:Ahmad.html">Ahmad Biniaz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maheshwari:Anil.html">Anil Maheshwari</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Smid:Michiel.html">Michiel Smid</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00791">PDF</a><br/><b>Abstract: </b>We study an old geometric optimization problem in the plane. Given a perfect
matching $M$ on a set of $n$ points in the plane, we can transform it to a
non-crossing perfect matching by a finite sequence of flip operations. The flip
operation removes two crossing edges from $M$ and adds two non-crossing edges.
Let $f(M)$ and $F(M)$ denote the minimum and maximum lengths of a flip sequence
on $M$, respectively. It has been proved by Bonnet and Miltzow (2016) that
$f(M)=O(n^2)$ and by van Leeuwen and Schoone (1980) that $F(M)=O(n^3)$. We
prove that $f(M)=O(n\Delta)$ where $\Delta$ is the spread of the point set,
which is defined as the ratio between the longest and the shortest pairwise
distances. This improves the previous bound if the point set has sublinear
spread. For a matching $M$ on $n$ points in convex position we prove that
$f(M)=n/2-1$ and $F(M)={{n/2} \choose 2}$; these bounds are tight.
</p>
<p>Any bound on $F(\cdot)$ carries over to the bichromatic setting, while this
is not necessarily true for $f(\cdot)$. Let $M'$ be a bichromatic matching. The
best known upper bound for $f(M')$ is the same as for $F(M')$, which is
essentially $O(n^3)$. We prove that $f(M')\le n-2$ for points in convex
position, and $f(M')= O(n^2)$ for semi-collinear points.
</p>
<p>The flip operation can also be defined on spanning trees. For a spanning tree
$T$ on a convex point set we show that $f(T)=O(n\log n)$.
</p></div>
    </summary>
    <updated>2019-05-05T00:00:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00727</id>
    <link href="http://arxiv.org/abs/1905.00727" rel="alternate" type="text/html"/>
    <title>Pseudo-Triangle Visibility Graph: Characterization and Reconstruction</title>
    <feedworld_mtime>1557014400</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mehrpour:Sahar.html">Sahar Mehrpour</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zarei:Alireza.html">Alireza Zarei</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00727">PDF</a><br/><b>Abstract: </b>The visibility graph of a simple polygon represents visibility relations
between its vertices. Knowing the correct order of the vertices around the
boundary of a polygon and its visibility graph, it is an open problem to locate
the vertices in a plane in such a way that it will be consistent with this
visibility graph. This problem has been solved for special cases when we know
that the target polygon is a {\it tower} or a {\it spiral}. Knowing that a
given visibility graph belongs to a simple polygon with at most three concave
chains on its boundary, a {\it pseuodo-triangle}, we propose a linear time
algorithm for reconstructing one of its corresponding polygons. Moreover, we
introduce a set of necessary and sufficient properties for characterizing
visibility graphs of pseudo-triangles and propose polynomial algorithms for
checking these properties.
</p></div>
    </summary>
    <updated>2019-05-05T00:03:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://ptreview.sublinear.info/?p=1116</id>
    <link href="https://ptreview.sublinear.info/?p=1116" rel="alternate" type="text/html"/>
    <title>News for April 2019</title>
    <summary>After a quite slow month of March, things sped up quite significantly in April: six different papers, ranging from graph testing to function testing to quantum distribution testing! Update (05/04): We missed one. Seven! Junta correlation is testable, by Anindya De, Elchanan Mossel, and Joe Neeman (arXiv). Junta testing really has seen a lot of […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>After a quite slow month of March, things sped up quite significantly in April: six different papers, ranging from graph testing to function testing to quantum distribution testing!</p>



<p><strong>Update (05/04): </strong>We missed one. Seven!</p>



<p><strong>Junta correlation is testable</strong>, by Anindya De, Elchanan Mossel, and Joe Neeman (<a href="https://arxiv.org/abs/1904.04216">arXiv</a>).  Junta testing really has seen <a href="https://ptreview.sublinear.info/?s=junta">a lot of attention and progress</a> over the pass couple years! In this new work, the focus is on the <em>tolerant</em> version of (Boolean) junta testing, where the goal is to decide whether a given function \(f\colon \{-1,1\}^n\to \{-1,1\}\) is close to some \(k\)-junta, or far from any such simple function. The current paper improves on previous work of Blais, Canonne, Eden, Levi, and Ron, which showed how to distinguish between \(\varepsilon\)-close to \(k\)-junta and \(16\varepsilon\)-far from \(k\)-junta in \(O_{k,\varepsilon}(1)\) queries, in two ways: (i) the gap-factor of 16 can now be made arbitrarily small, yielding the first <em>fully</em> tolerant non-trivial junta tester; and (ii) the new algorithm also identifies the underlying “core junta” (up to permutation of the coordinates). Besides the other results contained in the paper (such as results for a relaxation of the tolerant question akin to that of Blais et al.), a key aspect of this paper is to go beyond the use of (set) influence as a proxy for distance to junta-ness which underlied all previous work, thus potentially opening a new avenue towards get fully tolerant, \(\mathrm{poly}(k,1/\varepsilon)\)-query junta testing algorithms.</p>



<p><strong>Testing Tensor Products</strong>, by Irit Dinur and Konstantin Golubev (<a href="https://arxiv.org/abs/1904.12747">arXiv</a>). In this paper, the authors consider the following testing question: given query access to a Boolean function \(f\colon [n]^d \to \mathbb{F}_2\), test whether \(f\) is of the form \(f(x) = f_1(x_1)+\dots f_d(x_d)\) (equivalently, this is the task of testing whether a \(d\)-dimensional tensor with \(\pm 1\)  is of rank \(1\)). They provide two different proximity-oblivious testers (POT) for this task: a \(4\)-query one, reminiscent and building upon the BLR linearity test; as well as a \((d+1)\)-query POT <em>(which they dub “Shapka Test,” one can assume for their love of warm and comfortable hats)</em>, whose analysis is significantly simpler.</p>



<p>Changing direction, the next paper we saw is concerned with quantum testing of (regular, good ol’ classical) probability distributions:</p>



<p><strong>Quantum Algorithms for Classical Probability Distributions</strong>, by Aleksandrs Belovs (<a href="https://arxiv.org/abs/1904.02192">arXiv</a>). This work on quantum distribution testing considers 4 of the existing models of access to samples from an unknown probability distribution, analyzes their relationship, and argues their merits. Further, the autho establishes that for the simple hypothesis testing question of distinguishing between two (known, fixed) probability distributions \(p,q\), in all four models the optimal sample complexity is proportional to the inverse of the Hellinger distance between  \(p\) and \(q\)—in contrast to the classical setting, where it is known to be proportional to the <em>squared</em> inverse of this distance.</p>



<p>Turning to graphs, the next work considers clustering of bounded-degree graphs, a topic we <a href="https://ptreview.sublinear.info/?p=1075">recently discussed as well</a>:</p>



<p><strong>Robust Clustering Oracle and Local Reconstructor of Cluster Structure of Graphs</strong>, by Pan Peng (<a href="https://arxiv.org/abs/1904.09710">arXiv</a>). Consider the following noisy model: an unknown bounded-degree graph (of maximum degree \(d\)) \(G\) which is \((k, \phi_{\rm in},\phi_{\rm out})\)-clusterable (i.e., clusterable in at most \(k\) clusters of inner and outer conductances bounded by \(\phi_{\rm in},\phi_{\rm out}\)) is adversarially modified in at most \($\varepsilon d n\) edges between clusters. This noise model, introduced in this work, leads to the natural questions of “recovering the underlying graph \(G\)”: this is what the author tackles, by designing sublinear-time <em>local clustering oracles</em> and <em>local reconstruction algorithms</em>  (local filters) in this setting. Further, in view of the noise model reminiscent of property testing in the bounded-degree graph setting, connections to testing clusterability are discussed; the implications of the results for testing clusterability are discussed in Section 1.4.</p>



<p><strong>A Faster Local Algorithm for Detecting Bounded-Size Cuts with Applications to Higher-Connectivity Problems</strong>, by Sebastian Forster and Liu Yang (<a href="https://arxiv.org/abs/1904.08382">arXiv</a>). The authors focus on the problem of finding an edge (or vertex) cut in a given graph from a <em>local</em> viewpoint, i.e., in the setting of local computation algorithms, and provide a slew of results in detecting bounded-size cuts. This in turn has direct implications for testing \(k\)-edge and \(k\)-vertex connectivity, directed and undirected, in both the bounded-degree and general (unbounded degree) models, improving on or subsuming the current state-of-the-art across the board  <em>(see Section 1.2.4 of the paper, and the very helpful Tables 3 and 4, for a summary of the improvements)</em>.</p>



<p><strong>Random walks and forbidden minors II: A \(\mathrm{poly}(d\varepsilon^−1)\)-query tester for minor-closed properties of bounded degree graphs</strong>, by Akash Kumar, C. Seshadhri, and Andrew Stolman (<a href="https://eccc.weizmann.ac.il/report/2019/046/">ECCC</a>). Following their <a href="https://ptreview.sublinear.info/?p=999">breakthrough from last May</a>, the authors are at it again! Leveraging a random-walk approach, they resolve the following open question in testing bounded-degree graph: <em>is there a \(\mathrm{poly}(1/\varepsilon)\)-query tester for planarity (and, more generality, for minor-closed graph properties)? </em>The previous state-of-the-art, due to Levi and Ron, had a quasipolynomial dependence on \(1/\varepsilon\).<br/>Without spoiling too much: the answer to the open question is <em>yes—</em> and further the authors settle it by showing an even more general result on testing \(H\)-freeness (for any constant-size graph \(H\)).</p>



<p><strong>Update (05/04): </strong></p>



<p><strong>Testing Unateness Nearly Optimally</strong>, by Xi Chen and Erik Waingarten (<a href="https://arxiv.org/abs/1904.05309">arXiv</a>). Recall that a function \(f\colon \{0,1\}^n\to \{0,1\}\) is said to be unate if there exists some \(s\in\{0,1\}^n\) such that \(f(\cdot \oplus s)\) is monotone; i.e., if \(f\) is either non-increasing or non-decreasing in each coordinate. Testing unateness has seen a surge of interest over the past year or so; this work essentially settles the question, up to polylogarithmic factors in \(n\) (and the exact dependence on \(\varepsilon\)). Namely, the authors present and analyze an \(\tilde{O}(n^{2/3}/\varepsilon^2)\)-query adaptive tester for unateness, which nearly matches the \(\tilde{\Omega}(n^{2/3})\)-query lower bound for adaptive testers previously established by Chen, Waingarten, and Xie.  </p></div>
    </content>
    <updated>2019-05-04T06:22:27Z</updated>
    <published>2019-05-04T06:22:27Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Clement Canonne</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2019-05-05T05:22:02Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00850</id>
    <link href="http://arxiv.org/abs/1905.00850" rel="alternate" type="text/html"/>
    <title>Log Diameter Rounds Algorithms for $2$-Vertex and $2$-Edge Connectivity</title>
    <feedworld_mtime>1556928000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Andoni:Alexandr.html">Alexandr Andoni</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Stein:Clifford.html">Clifford Stein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhong:Peilin.html">Peilin Zhong</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00850">PDF</a><br/><b>Abstract: </b>Many modern parallel systems, such as MapReduce, Hadoop and Spark, can be
modeled well by the MPC model. The MPC model captures well coarse-grained
computation on large data --- data is distributed to processors, each of which
has a sublinear (in the input data) amount of memory and we alternate between
rounds of computation and rounds of communication, where each machine can
communicate an amount of data as large as the size of its memory. This model is
stronger than the classical PRAM model, and it is an intriguing question to
design algorithms whose running time is smaller than in the PRAM model.
</p>
<p>In this paper, we study two fundamental problems, $2$-edge connectivity and
$2$-vertex connectivity (biconnectivity). PRAM algorithms which run in $O(\log
n)$ time have been known for many years. We give algorithms using roughly log
diameter rounds in the MPC model. Our main results are, for an $n$-vertex,
$m$-edge graph of diameter $D$ and bi-diameter $D'$, 1) a $O(\log
D\log\log_{m/n} n)$ parallel time $2$-edge connectivity algorithm, 2) a $O(\log
D\log^2\log_{m/n}n+\log D'\log\log_{m/n}n)$ parallel time biconnectivity
algorithm, where the bi-diameter $D'$ is the largest cycle length over all the
vertex pairs in the same biconnected component. Our results are fully scalable,
meaning that the memory per processor can be $O(n^{\delta})$ for arbitrary
constant $\delta&gt;0$, and the total memory used is linear in the problem size.
Our $2$-edge connectivity algorithm achieves the same parallel time as the
connectivity algorithm of Andoni et al. (FOCS 2018). We also show an
$\Omega(\log D')$ conditional lower bound for the biconnectivity problem.
</p></div>
    </summary>
    <updated>2019-05-04T23:46:06Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00848</id>
    <link href="http://arxiv.org/abs/1905.00848" rel="alternate" type="text/html"/>
    <title>Budget-Feasible Mechanism Design for Non-Monotone Submodular Objectives: Offline and Online</title>
    <feedworld_mtime>1556928000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Amanatidis:Georgios.html">Georgios Amanatidis</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kleer:Pieter.html">Pieter Kleer</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sch=auml=fer:Guido.html">Guido Schäfer</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00848">PDF</a><br/><b>Abstract: </b>The framework of budget-feasible mechanism design studies procurement
auctions where the auctioneer (buyer) aims to maximize his valuation function
subject to a hard budget constraint. We study the problem of designing truthful
mechanisms that have good approximation guarantees and never pay the
participating agents (sellers) more than the budget. We focus on the case of
general (non-monotone) submodular valuation functions and derive the first
truthful, budget-feasible and $O(1)$-approximate mechanisms that run in
polynomial time in the value query model, for both offline and online auctions.
Prior to our work, the only $O(1)$-approximation mechanism known for
non-monotone submodular objectives required an exponential number of value
queries.
</p>
<p>At the heart of our approach lies a novel greedy algorithm for non-monotone
submodular maximization under a knapsack constraint. Our algorithm builds two
candidate solutions simultaneously (to achieve a good approximation), yet
ensures that agents cannot jump from one solution to the other (to implicitly
enforce truthfulness). Ours is the first mechanism for the problem
where---crucially---the agents are not ordered with respect to their marginal
value per cost. This allows us to appropriately adapt these ideas to the online
setting as well.
</p>
<p>To further illustrate the applicability of our approach, we also consider the
case where additional feasibility constraints are present. We obtain
$O(p)$-approximation mechanisms for both monotone and non-monotone submodular
objectives, when the feasible solutions are independent sets of a $p$-system.
With the exception of additive valuation functions, no mechanisms were known
for this setting prior to our work. Finally, we provide lower bounds suggesting
that, when one cares about non-trivial approximation guarantees in polynomial
time, our results are asymptotically best possible.
</p></div>
    </summary>
    <updated>2019-05-04T23:49:01Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00812</id>
    <link href="http://arxiv.org/abs/1905.00812" rel="alternate" type="text/html"/>
    <title>Near Optimal Jointly Private Packing Algorithms via Dual Multiplicative Weight Update</title>
    <feedworld_mtime>1556928000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Zhiyi.html">Zhiyi Huang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhu:Xue.html">Xue Zhu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00812">PDF</a><br/><b>Abstract: </b>We present an improved $(\epsilon, \delta)$-jointly differentially private
algorithm for packing problems. Our algorithm gives a feasible output that is
approximately optimal up to an $\alpha n$ additive factor as long as the supply
of each resource is at least $\tilde{O}(\sqrt{m} / \alpha \epsilon)$, where $m$
is the number of resources. This improves the previous result by Hsu et
al.~(SODA '16), which requires the total supply to be at least $\tilde{O}(m^2 /
\alpha \epsilon)$, and only guarantees approximate feasibility in terms of
total violation. Further, we complement our algorithm with an almost matching
hardness result, showing that $\Omega(\sqrt{m \ln(1/\delta)} / \alpha
\epsilon)$ supply is necessary for any $(\epsilon, \delta)$-jointly
differentially private algorithm to compute an approximately optimal packing
solution. Finally, we introduce an alternative approach that runs in linear
time, is exactly truthful, can be implemented online, and can be
$\epsilon$-jointly differentially private, but requires a larger supply of each
resource.
</p></div>
    </summary>
    <updated>2019-05-04T23:55:18Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00790</id>
    <link href="http://arxiv.org/abs/1905.00790" rel="alternate" type="text/html"/>
    <title>Minimum Ply Covering of Points with Disks and Squares</title>
    <feedworld_mtime>1556928000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Therese Biedl, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Biniaz:Ahmad.html">Ahmad Biniaz</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lubiw:Anna.html">Anna Lubiw</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00790">PDF</a><br/><b>Abstract: </b>Following the seminal work of Erlebach and van Leeuwen in SODA 2008, we
introduce the minimum ply covering problem. Given a set $P$ of points and a set
$S$ of geometric objects, both in the plane, our goal is to find a subset $S'$
of $S$ that covers all points of $P$ while minimizing the maximum number of
objects covering any point in the plane (not only points of $P$). For objects
that are unit squares and unit disks, this problem is NP-hard and cannot be
approximated by a ratio smaller than 2. We present 2-approximation algorithms
for this problem with respect to unit squares and unit disks. Our algorithms
run in polynomial time when the optimum objective value is bounded by a
constant.
</p>
<p>Motivated by channel-assignment in wireless networks, we consider a variant
of the problem where the selected unit disks must be 3-colorable, i.e., colored
by three colors such that all disks of the same color are pairwise disjoint. We
present a polynomial-time algorithm that achieves a 2-approximate solution,
i.e., a solution that is 6-colorable.
</p>
<p>We also study the weighted version of the problem in dimension one, where $P$
and $S$ are points and weighted intervals on a line, respectively. We present
an algorithm that solves this problem in $O(n + m + M )$-time where $n$ is the
number of points, $m$ is the number of intervals, and $M$ is the number of
pairs of overlapping intervals. This repairs a solution claimed by Nandy,
Pandit, and Roy in CCCG 2017.
</p></div>
    </summary>
    <updated>2019-05-04T23:59:13Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00767</id>
    <link href="http://arxiv.org/abs/1905.00767" rel="alternate" type="text/html"/>
    <title>Scalable and Jointly Differentially Private Packing</title>
    <feedworld_mtime>1556928000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Zhiyi.html">Zhiyi Huang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhu:Xue.html">Xue Zhu</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00767">PDF</a><br/><b>Abstract: </b>We introduce an $(\epsilon, \delta)$-jointly differentially private algorithm
for packing problems. Our algorithm not only achieves the optimal trade-off
between the privacy parameter $\epsilon$ and the minimum supply requirement (up
to logarithmic factors), but is also scalable in the sense that the running
time is linear in the number of agents $n$. Previous algorithms either run in
cubic time in $n$, or require a minimum supply per resource that is $\sqrt{n}$
times larger than the best possible.
</p></div>
    </summary>
    <updated>2019-05-04T23:55:04Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00731</id>
    <link href="http://arxiv.org/abs/1905.00731" rel="alternate" type="text/html"/>
    <title>Static Pricing: Universal Guarantees for Reusable Resources</title>
    <feedworld_mtime>1556928000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Besbes:Omar.html">Omar Besbes</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Elmachtoub:Adam_N=.html">Adam N. Elmachtoub</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sun:Yunjie.html">Yunjie Sun</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00731">PDF</a><br/><b>Abstract: </b>We consider a fundamental pricing model in which a fixed number of units of a
reusable resource are used to serve customers. Customers arrive to the system
according to a stochastic process and upon arrival decide to purchase or not
the service depending on their willingness to pay and the current price. The
service time during which the resource is used by the customer is stochastic
and the firm may incur a service cost. This model represents various markets
for reusable resources such as cloud computing, shared vehicles, rotable parts,
and hotel rooms. In the present paper, we analyze this pricing problem when the
firm attempts to maximize a weighted combination of three central metrics:
profit, market share, and service level. Under Poisson arrivals, exponential
service times, and standard assumptions on the willingness to pay distribution,
we establish series of results that characterize the performance of static
pricing in such environments. In particular, while an optimal policy is fully
dynamic in such a context, we prove that a static pricing policy simultaneously
guarantees 78.9% of the profit, market share, and service level from the
optimal policy. Notably this result holds for any service rate and number of
units the firm operates. In the special case where there are two units and the
induced demand demand is linear, we also prove that the static policy
guarantees 95.5% of the profit from the optimal policy. Our numerical findings
on a large testbed of instances suggest that the latter result is quite
indicative of the profit obtained by the static pricing policy across all
parameters.
</p></div>
    </summary>
    <updated>2019-05-04T23:50:45Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00656</id>
    <link href="http://arxiv.org/abs/1905.00656" rel="alternate" type="text/html"/>
    <title>Efficient approximation schemes for uniform-cost clustering problems in planar graphs</title>
    <feedworld_mtime>1556928000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Cohen=Addad:Vincent.html">Vincent Cohen-Addad</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Pilipczuk:Marcin.html">Marcin Pilipczuk</a>, Michał Pilipczuk <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00656">PDF</a><br/><b>Abstract: </b>We consider the $k$-Median problem on planar graphs: given an edge-weighted
planar graph $G$, a set of clients $C \subseteq V(G)$, a set of facilities $F
\subseteq V(G)$, and an integer parameter $k$, the task is to find a set of at
most $k$ facilities whose opening minimizes the total connection cost of
clients, where each client contributes to the cost with the distance to the
closest open facility. We give two new approximation schemes for this problem:
-- FPT Approximation Scheme: for any $\epsilon&gt;0$, in time
$2^{O(k\epsilon^{-3}\log (k\epsilon^{-1}))}\cdot n^{O(1)}$ we can compute a
solution that
</p>
<p>(1) has connection cost at most $(1+\epsilon)$ times the optimum, with high
probability. -- Efficient Bicriteria Approximation Scheme: for any
$\epsilon&gt;0$, in time $2^{O(\epsilon^{-5}\log (\epsilon^{-1}))}\cdot n^{O(1)}$
we can compute a set of at most $(1+\epsilon)k$ facilities
</p>
<p>(2) whose opening yields connection cost at most $(1+\epsilon)$ times the
optimum connection cost for opening at most $k$ facilities, with high
probability.
</p>
<p>As a direct corollary of the second result we obtain an EPTAS for the Uniform
Facility Location on planar graphs, with same running time.
</p>
<p>Our main technical tool is a new construction of a "coreset for facilities"
for $k$-Median in planar graphs: we show that in polynomial time one can
compute a subset of facilities $F_0\subseteq F$ of size $k\cdot (\log
n/\epsilon)^{O(\epsilon^{-3})}$ with a guarantee that there is a
$(1+\epsilon)$-approximate solution contained in $F_0$.
</p></div>
    </summary>
    <updated>2019-05-04T23:47:50Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00640</id>
    <link href="http://arxiv.org/abs/1905.00640" rel="alternate" type="text/html"/>
    <title>Tight Approximation Bounds for Maximum Multi-Coverage</title>
    <feedworld_mtime>1556928000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Barman:Siddharth.html">Siddharth Barman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fawzi:Omar.html">Omar Fawzi</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghoshal:Suprovat.html">Suprovat Ghoshal</a>, Emirhan Gürpınar <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00640">PDF</a><br/><b>Abstract: </b>In the classic maximum coverage problem, we are given subsets $T_1, \dots,
T_m$ of a universe $[n]$ along with an integer $k$ and the objective is to find
a subset $S \subseteq [m]$ of size $k$ that maximizes $C(S) := |\cup_{i \in S}
T_i|$. It is well-known that the greedy algorithm for this problem achieves an
approximation ratio of $(1-e^{-1})$ and there is a matching inapproximability
result. We note that in the maximum coverage problem if an element $e \in [n]$
is covered by several sets, it is still counted only once. By contrast, if we
change the problem and count each element $e$ as many times as it is covered,
then we obtain a linear objective function, $C^{(\infty)}(S) = \sum_{i \in S}
|T_i|$, which can be easily maximized under a cardinality constraint.
</p>
<p>We study the maximum $\ell$-multi-coverage problem which naturally
interpolates between these two extremes. In this problem, an element can be
counted up to $\ell$ times but no more; hence, we consider maximizing the
function $C^{(\ell)}(S) = \sum_{e \in [n]} \min\{\ell, |\{i \in S : e \in
T_i\}| \}$, subject to the constraint $|S| \leq k$. Note that the case of $\ell
= 1$ corresponds to the standard maximum coverage setting and $\ell = \infty$
gives us a linear objective.
</p>
<p>We develop an efficient approximation algorithm that achieves an
approximation ratio of $1 - \frac{\ell^{\ell}e^{-\ell}}{\ell!}$ for the
$\ell$-multi-coverage problem. In particular, when $\ell = 2$, this factor is
$1-2e^{-2} \approx 0.73$ and as $\ell$ grows the approximation ratio behaves as
$1 - \frac{1}{\sqrt{2\pi \ell}}$. We also prove that this approximation ratio
is tight, i.e., establish a matching hardness-of-approximation result, under
the Unique Games Conjecture.
</p></div>
    </summary>
    <updated>2019-05-04T23:47:17Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00612</id>
    <link href="http://arxiv.org/abs/1905.00612" rel="alternate" type="text/html"/>
    <title>Online Circle Packing</title>
    <feedworld_mtime>1556928000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fekete:S=aacute=ndor_P=.html">Sándor P. Fekete</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/H=ouml=veling:Sven_von.html">Sven von Höveling</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Scheffer:Christian.html">Christian Scheffer</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00612">PDF</a><br/><b>Abstract: </b>We consider the online problem of packing circles into a square container. A
sequence of circles has to be packed one at a time, without knowledge of the
following incoming circles and without moving previously packed circles. We
present an algorithm that packs any online sequence of circles with a combined
area not larger than 0.350389 0.350389 of the square's area, improving the
previous best value of {\pi}/10 \approx 0.31416; even in an offline setting,
there is an upper bound of {\pi}/(3 + 2 \sqrt{2}) \approx 0.5390. If only
circles with radii of at least 0.026622 are considered, our algorithm achieves
the higher value 0.375898. As a byproduct, we give an online algorithm for
packing circles into a 1\times b rectangle with b \geq 1. This algorithm is
worst case-optimal for b \geq 2.36.
</p></div>
    </summary>
    <updated>2019-05-04T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00580</id>
    <link href="http://arxiv.org/abs/1905.00580" rel="alternate" type="text/html"/>
    <title>Deterministic Leader Election in Programmable Matter</title>
    <feedworld_mtime>1556928000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Emek:Yuval.html">Yuval Emek</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kutten:Shay.html">Shay Kutten</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lavi:Ron.html">Ron Lavi</a>, William K. Moses Jr <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00580">PDF</a><br/><b>Abstract: </b>Addressing a fundamental problem in programmable matter, we present the first
deterministic algorithm to elect a unique leader in a system of connected
amoebots assuming only that amoebots are initially contracted. Previous
algorithms either used randomization, made various assumptions (shapes with no
holes, or known shared chirality), or elected several co-leaders in some cases.
</p>
<p>Some of the building blocks we introduce in constructing the algorithm are of
interest by themselves, especially the procedure we present for reaching common
chirality among the amoebots. Given the leader election and the chirality
agreement building block, it is known that various tasks in programmable matter
can be performed or improved.
</p>
<p>The main idea of the new algorithm is the usage of the ability of the
amoebots to move, which previous leader election algorithms have not used.
</p></div>
    </summary>
    <updated>2019-05-04T23:49:27Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00566</id>
    <link href="http://arxiv.org/abs/1905.00566" rel="alternate" type="text/html"/>
    <title>Graph Coloring via Degeneracy in Streaming and Other Space-Conscious Models</title>
    <feedworld_mtime>1556928000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bera:Suman_K=.html">Suman K. Bera</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chakrabarti:Amit.html">Amit Chakrabarti</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghosh:Prantar.html">Prantar Ghosh</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00566">PDF</a><br/><b>Abstract: </b>We study the problem of coloring a given graph using a small number of colors
in several well-established models of computation for big data. These include
the data streaming model, the general graph query model, the massively parallel
computation (MPC) model, and the CONGESTED-CLIQUE and the LOCAL models of
distributed computation. On the one hand, we give algorithms with sublinear
complexity, for the appropriate notion of complexity in each of these models.
Our algorithms color a graph $G$ using about $\kappa(G)$ colors, where
$\kappa(G)$ is the degeneracy of $G$: this parameter is closely related to the
arboricity $\alpha(G)$. As a function of $\kappa(G)$ alone, our results are
close to best possible, since the optimal number of colors is $\kappa(G)+1$.
</p>
<p>On the other hand, we establish certain lower bounds indicating that
sublinear algorithms probably cannot go much further. In particular, we prove
that any randomized coloring algorithm that uses $\kappa(G)+1$ many colors,
would require $\Omega(n^2)$ storage in the one pass streaming model, and
$\Omega(n^2)$ many queries in the general graph query model, where $n$ is the
number of vertices in the graph. These lower bounds hold even when the value of
$\kappa(G)$ is known in advance; at the same time, our upper bounds do not
require $\kappa(G)$ to be given in advance.
</p></div>
    </summary>
    <updated>2019-05-04T23:51:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00518</id>
    <link href="http://arxiv.org/abs/1905.00518" rel="alternate" type="text/html"/>
    <title>Reconfiguring Undirected Paths</title>
    <feedworld_mtime>1556928000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Demaine:Erik_D=.html">Erik D. Demaine</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Eppstein:David.html">David Eppstein</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hesterberg:Adam.html">Adam Hesterberg</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jain:Kshitij.html">Kshitij Jain</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lubiw:Anna.html">Anna Lubiw</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Uehara:Ryuhei.html">Ryuhei Uehara</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/u/Uno:Yushi.html">Yushi Uno</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00518">PDF</a><br/><b>Abstract: </b>We consider problems in which a simple path of fixed length, in an undirected
graph, is to be shifted from a start position to a goal position by moves that
add an edge to either end of the path and remove an edge from the other end. We
show that this problem may be solved in linear time in trees, and is
fixed-parameter tractable when parameterized either by the cyclomatic number of
the input graph or by the length of the path. However, it is PSPACE-complete
for paths of unbounded length in graphs of bounded bandwidth.
</p></div>
    </summary>
    <updated>2019-05-04T23:21:58Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1905.00444</id>
    <link href="http://arxiv.org/abs/1905.00444" rel="alternate" type="text/html"/>
    <title>Establishing the Quantum Supremacy Frontier with a 281 Pflop/s Simulation</title>
    <feedworld_mtime>1556928000</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Benjamin Villalonga, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lyakh:Dmitry.html">Dmitry Lyakh</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Boixo:Sergio.html">Sergio Boixo</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Neven:Hartmut.html">Hartmut Neven</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Humble:Travis_S=.html">Travis S. Humble</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Biswas:Rupak.html">Rupak Biswas</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/r/Rieffel:Eleanor_G=.html">Eleanor G. Rieffel</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Ho:Alan.html">Alan Ho</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Mandr=agrave=:Salvatore.html">Salvatore Mandrà</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1905.00444">PDF</a><br/><b>Abstract: </b>Noisy Intermediate-Scale Quantum (NISQ) computers aim to perform
computational tasks beyond the capabilities of the most powerful classical
computers, thereby achieving "Quantum Supremacy", a major milestone in quantum
computing. NISQ Supremacy requires comparison with a state-of-the-art classical
simulator. We report HPC simulations of hard random quantum circuits (RQC),
sustaining an average performance of 281 Pflop/s (true single precision) on
Summit, currently the fastest supercomputer in the world. In addition, we
propose a standard benchmark for NISQ computers based on qFlex, a
tensor-network-based classical high-performance simulator of RQC, which are
considered the leading proposal for Quantum Supremacy.
</p></div>
    </summary>
    <updated>2019-05-04T23:20:36Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-05-03T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4184</id>
    <link href="https://www.scottaaronson.com/blog/?p=4184" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4184#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4184" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">On the scientific accuracy of “Avengers: Endgame”</title>
    <summary xml:lang="en-US">[BY REQUEST: SPOILERS FOLLOW] Today Ben Lindbergh, a writer for The Wringer, put out an article about the scientific plausibility (!) of the time-travel sequences in the new “Avengers” movie. The article relied on two interviewees: (1) David Deutsch, who confirmed that he has no idea what the “Deutsch proposition” mentioned by Tony Stark refers […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><p>[BY REQUEST: SPOILERS FOLLOW]</p>



<p>Today Ben Lindbergh, a writer for <em>The Wringer</em>, put out <a href="https://www.theringer.com/movies/2019/5/3/18527776/marvel-avengers-endgame-time-travel-david-deutsch-proposition-scott-aaronson">an article</a> about the scientific plausibility (!) of the time-travel sequences in the new “Avengers” movie.  The article relied on two interviewees:</p>



<p>(1) David Deutsch, who confirmed that he has no idea what the “Deutsch proposition” mentioned by Tony Stark refers to but declined to comment further, and</p>



<p>(2) some quantum computing dude from UT Austin who had no similar scruples about spouting off on the movie.</p>



<p>To be clear, the UT Austin dude hadn’t even <em>seen</em> the movie, or any of the previous “Avengers” movies for that matter!  He just watched the clips dealing with time travel.  Yet Lindbergh still saw fit to introduce him as “a real-life [Tony] Stark without the vast fortune and fancy suit.”  Hey, I’ll take it.</p>



<p>Anyway, if you’ve seen the movie, and/or you know Deutsch’s causal consistency proposal for quantum closed timelike curves, and you can do better than I did at trying to reconcile the two, feel free to take a stab in the comments.</p></div>
    </content>
    <updated>2019-05-03T19:01:35Z</updated>
    <published>2019-05-03T19:01:35Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Announcements"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Embarrassing Myself"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Nerd Interest"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Procrastination"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2019-05-04T01:07:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>https://www.scottaaronson.com/blog/?p=4182</id>
    <link href="https://www.scottaaronson.com/blog/?p=4182" rel="alternate" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?p=4182#comments" rel="replies" type="text/html"/>
    <link href="https://www.scottaaronson.com/blog/?feed=atom&amp;p=4182" rel="replies" type="application/atom+xml"/>
    <title xml:lang="en-US">A small post</title>
    <summary xml:lang="en-US">I really liked this article by Chris Monroe, of the University of Maryland and IonQ, entitled “Quantum computing is a marathon not a sprint.” The crazier expectations get in this field—and right now they’re really crazy, believe me—the more it needs to be said. In a piece for Communications of the ACM, Moshe Vardi came […]</summary>
    <content type="xhtml" xml:lang="en-US"><div xmlns="http://www.w3.org/1999/xhtml"><ol><li>I really liked <a href="https://venturebeat.com/2019/04/21/quantum-computing-is-a-marathon-not-a-sprint/?fbclid=IwAR2WHdke1ppFQFE4ZfYswa06lXe-sUM2PZ2neLmLOhFTrJYHY3iXNesGHb8">this article by Chris Monroe</a>, of the University of Maryland and IonQ, entitled “Quantum computing is a marathon not a sprint.”  The crazier expectations get in this field—and right now they’re <em>really</em> crazy, believe me—the more it needs to be said.</li><li>In a <a href="https://cacm.acm.org/magazines/2019/5/236426-quantum-hype-and-quantum-skepticism/fulltext?mobile=false">piece for <em>Communications of the ACM</em></a>, Moshe Vardi came out as a “quantum computing skeptic.”  But it turns out what he means by that is not that he knows a reason why QC is impossible in principle, but simply that it’s often overhyped and that it will be hard to establish a viable quantum computing industry.  By that standard, I’m a “QC skeptic” as well!  But then what does that make Gil Kalai or Michel Dyakonov?</li><li>Friend-of-the-blog Bram Cohen asked me to link to <a href="https://www.chia.net/2019/04/04/chia-network-announces-second-vdf-competition-with-in-total-prize-money.en.html">this second-round competition</a> for Verifiable Delay Functions, sponsored by his company Chia.  Apparently the first link I provided actually mattered in sending serious entrants their way.</li><li>Blogging, it turns out, is really, really hard when (a) your life has become a pile of real-world obligations stretching out to infinity, <em>and also</em> (b) the Internet has become a war zone, with anything you say quote-mined by people looking to embarrass you.  But don’t worry, I’ll have more to say soon.  In the meantime, doesn’t anyone have more questions about the research papers discussed in the previous post?  Y’know, NEEXP in MIP*?  SBP versus QMA?  Gentle measurement of quantum states and differential privacy turning out to be almost the same subject?</li></ol></div>
    </content>
    <updated>2019-05-03T07:51:41Z</updated>
    <published>2019-05-03T07:51:41Z</published>
    <category scheme="https://www.scottaaronson.com/blog" term="Procrastination"/>
    <category scheme="https://www.scottaaronson.com/blog" term="Quantum"/>
    <author>
      <name>Scott</name>
      <uri>http://www.scottaaronson.com</uri>
    </author>
    <source>
      <id>https://www.scottaaronson.com/blog/?feed=atom</id>
      <link href="https://www.scottaaronson.com/blog" rel="alternate" type="text/html"/>
      <link href="https://www.scottaaronson.com/blog/?feed=atom" rel="self" type="application/atom+xml"/>
      <subtitle xml:lang="en-US">The Blog of Scott Aaronson</subtitle>
      <title xml:lang="en-US">Shtetl-Optimized</title>
      <updated>2019-05-04T01:07:06Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15827</id>
    <link href="https://rjlipton.wordpress.com/2019/05/02/sedgewick-wins-an-award/" rel="alternate" type="text/html"/>
    <title>Sedgewick Wins An Award</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">An award for educational writing [ ACM ] Robert Sedgewick is the 2018 recipient of the ACM Outstanding Educator Award. Today we congratulate Bob on this wonderful honor. The award is named after Karl Karlstrom. Years ago, he was an editor at the publishing house Prentice-Hall. To convey why the award was named for him, … … <a href="https://rjlipton.wordpress.com/2019/05/02/sedgewick-wins-an-award/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>An award for educational writing</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/05/02/sedgewick-wins-an-award/sedgewick_1183631/" rel="attachment wp-att-15828"><img alt="" class="alignright  wp-image-15828" src="https://rjlipton.files.wordpress.com/2019/05/sedgewick_1183631.jpeg?w=200" width="200"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[ ACM ]</font></td>
</tr>
</tbody>
</table>
<p>
Robert Sedgewick is the 2018 recipient of the ACM Outstanding Educator Award. </p>
<p>
Today we congratulate Bob on this wonderful honor.</p>
<p>
The award is named after Karl Karlstrom. Years ago, he was an editor at the publishing house Prentice-Hall. To convey why the award was named for him, it may suffice to quote one nugget. This is “Fortune <a href="http://motd.ambians.com/quotes.php/name/linux_computers/toc_id/1-1-4/s/340">341</a>” from the old <tt>motd</tt> (message of the day) program which gave some humor or wisdom when you logged into UNIX/Linux:</p>
<blockquote><p><b> </b> <em> “I have travelled the length and breadth of this country, and have talked with the best people in business administration. I can assure you on the highest authority that data processing is a fad and won’t last out the year.”</em></p><em>
</em><p><em>
— Editor in charge of business books at Prentice-Hall publishers, responding to Karl V. Karlstrom (a junior editor who had recommended a manuscript on the new science of data processing), c. 1957 </em>
</p></blockquote>
<p>Karl K. had a knack for being right, you see.</p>
<p>
</p><p/><h2> Karl Stories </h2><p/>
<p/><p>
I recall Karl fondly. We mostly interacted when we were both attending some theory conference. I often found myself talking to him over a drink while we sat in a hotel bar. This was back, ages ago, when I did drink a beer or two. Karl was one we could count on to amuse and also—most importantly—pick up the bar tab. He had an expense account. The IEEE “Computer Pioneers” <a href="https://history.computer.org/pioneers/karlstrom.html">site</a> says this about him:</p>
<blockquote><p><b> </b> <em> Early computer science textbook editor who put Prentice-Hall in the forefront, but who lost heart when he learned that the best textbook criteria are short words, big type, wide margins, and colored illustrations. ACM named its education award after him. </em>
</p></blockquote>
<p>
</p><p/><h2> Bob Stories </h2><p/>
<p/><p>
The ACM award may be named for Karlstrom, but I suspect that many of the awardees, including Bob, never had the pleasure of meeting Karlstrom. Too bad. </p>
<p>
I believe we all know why Bob was selected to get this award. He has done some wonderful work in many aspects of education. He is best known for his series of Algorithms textbooks. I thought it might be fun to recall a couple of Sedgewick stories that have nothing to do with his main work.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>A big result</i>. One day Bob grabbed me and told me that he had a wonderful result. This is when I was still at Princeton. I asked what was the breakthrough? He explained:</p>
<blockquote><p><b> </b> <em> I now can do arrows really well. Really. </em>
</p></blockquote>
<p/><p>
What? He explained that TeX and LaTeX did not do arrows well. This refers, of course, to arrows as in directed graphs or flow diagrams. Bob uses lots of diagrams, with lots of arrows, in his textbooks. He had worked hard to get a postscript hack that made arrows look great. Thus he could typeset an arrow so it looked perfect even when it touched another object. I listened and was unsure what to make of his claim. Was he losing it? He then showed me a print-out of some of his arrows. I have to say they really did look quite good.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>A secret result</i>. Bob and I worked for a while on a front-end to TeX we called <i>notech</i>. The concept was to have the absolute minimum of commands, and have the notech system figure out what you mean. For example, in an earlier system for typesetting from Bell Labs, called <a href="https://en.wikipedia.org/wiki/Troff">Troff</a>, a new paragraph was marked by the command <img alt="{.PP}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B.PP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{.PP}"/>. Thus</p>
<p>
This is part of a paragraph. .PP And this is the start of the next paragraph. </p>
<p>
This is ugly and TeX’s idea is much better. As you probably know the start of a new paragraph is marked by a blank line. No ugly command like <img alt="{.PP}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B.PP%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{.PP}"/>.</p>
<p>
What Bob and I did was to try and take this idea as far as possible. The system notech tried to guess line breaks, math displays, tables, verbatim for C code, text displays, and much more. It did this with out using commands for as much as possible. I used the system for years for all my papers and memos and notes. Eventually, I gave it up and switched to LaTeX like every one else.</p>
<p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>A public result</i>. Bob also worked with me and my team in the 1980’s on systems for designing VLSI chips. One such paper was joint with Jacobo Valdes, Gopalakrishnan Vijayan, and Stephen North: <a href="https://dl.acm.org/citation.cfm?id=809246">VLSI Layout as Programming</a>. The trouble with this and related work is that it never took off; it never had as much impact as we thought it would. Oh well.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
We wish Bob the best. May he be awarded many other prizes.</p>
<p/></font></font></div>
    </content>
    <updated>2019-05-03T04:32:02Z</updated>
    <published>2019-05-03T04:32:02Z</published>
    <category term="History"/>
    <category term="News"/>
    <category term="People"/>
    <category term="ACM"/>
    <category term="arrows"/>
    <category term="award"/>
    <category term="Sedgewick"/>
    <category term="TeX"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-05-05T14:21:03Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/066</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/066" rel="alternate" type="text/html"/>
    <title>TR19-066 |  A Lower Bound for Sampling Disjoint Sets | 

	Thomas Watson</title>
    <summary>Suppose Alice and Bob each start with private randomness and no other input, and they wish to engage in a protocol in which Alice ends up with a set $x\subseteq[n]$ and Bob ends up with a set $y\subseteq[n]$, such that $(x,y)$ is uniformly distributed over all pairs of disjoint sets. We prove that for some constant $\varepsilon&gt;0$, this requires $\Omega(n)$ communication even to get within statistical distance $\varepsilon$ of the target distribution. Previously, Ambainis, Schulman, Ta-Shma, Vazirani, and Wigderson (FOCS 1998) proved that $\Omega(\sqrt{n})$ communication is required to get within $\varepsilon$ of the uniform distribution over all pairs of disjoint sets of size $\sqrt{n}$.</summary>
    <updated>2019-05-03T00:14:32Z</updated>
    <published>2019-05-03T00:14:32Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-05-05T14:20:50Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/05/02/playing-model-trains</id>
    <link href="https://11011110.github.io/blog/2019/05/02/playing-model-trains.html" rel="alternate" type="text/html"/>
    <title>Playing with model trains and calling it graph theory</title>
    <summary>You’ve probably played with model trains, for instance with something like the Brio set shown below.1 And if you’ve built a layout with a model train set, you may well have wondered: is it possible for my train to use all the parts of my track? Searching on tineye finds that this image was on Amazon in 2008. Presumably it was supplied to them by Brio? ↩</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>You’ve probably played with model trains, for instance with something like the <a href="https://en.wikipedia.org/wiki/Brio_(company)">Brio</a> set shown below.<sup id="fnref:fn"><a class="footnote" href="https://11011110.github.io/blog/2019/05/02/playing-model-trains.html#fn:fn">1</a></sup> And if you’ve built a layout with a model train set, you may well have wondered: is it possible for my train to use all the parts of my track?</p>

<p style="text-align: center;"><img alt="Brio train set" src="https://11011110.github.io/blog/assets/2019/brio-33133.jpg"/></p>

<p>For instance, in the layout shown in this image, if your train starts on the far right, moving downward, it will be stuck in a loop that it can never escape. There are no choice points where the train can switch to another track until it returns to the Y at the right, moving in the same direction. On the other hand, if you allow yourself to reverse the train, it can reverse back through the other entrance to the Y and reach the rest of the track. It’s also possible for a long-enough train to block itself, preventing it from escaping certain parts of the track that a short train could negotiate more easily.</p>

<p>My newest preprint, “Reconfiguring Undirected Paths” (with Demaine, Hesterberg, Jain, Lubiw, Uehara, and Uno, <a href="https://arxiv.org/abs/1905.00518">arXiv:1905.00518</a>), considers an abstract model for such problems, in which the train track is modeled as an undirected graph and the train is a simple path in the graph. You can slide the train by adding an edge to one end of the path and removing an edge from the other end; we don’t distinguish which end of the train is which, so it can slide in both directions. The vertices of the graph model points where you can choose which of several directions to slide the train. Because it’s an undirected graph, these are like the three-way and four-way junctions in the middle of the image (allowing the train to enter and exit along any pair of track segments) rather than the Y junctions at the far right (where a train that enters at one of the two top edges of the Y has to exit the bottom).</p>

<p>For instance, in a  grid graph, the different positions of a length- path and the ways that one position can shift into another can be visualized as the state space shown below.</p>

<p style="text-align: center;"><img alt="PSPACE-hardness reduction for path reconfiguration" src="https://11011110.github.io/blog/assets/2019/path-reconfig-states.svg"/></p>

<p>Testing whether a long train can slide from one position to another turns out to be PSPACE-complete, even on graphs of bounded bandwidth, by a reduction from <a href="https://en.wikipedia.org/wiki/Nondeterministic_constraint_logic">nondeterministic constraint logic</a>. Here’s an example of an NCL problem transformed by our reduction into a path-sliding problem:</p>

<p style="text-align: center;"><img alt="PSPACE-hardness reduction for path reconfiguration" src="https://11011110.github.io/blog/assets/2019/path-reconfig-redux.svg" width="80%"/></p>

<p>Our main results are a <a href="https://11011110.github.io/blog/2019/05/02/Parameterized complexity">fixed-parameter tractable algorithm</a> parameterized by train length (so it’s fast for short trains) and a linear time algorithm when the graph is a tree. Both cases are based on the same intuition, that the problem becomes easier if we can maneuver the train onto a long enough path. For the parameterized version, if the graph has a path twice as long as the train that can be reached from the starting position of the train, and another long path that can reach the ending position, then we can maneuver the train onto the first long path, send it on an express route directly from the first long path to the second one, and then maneuver it from there into its final position. On the other hand, until we find these long paths, we can restrict our attention to a subgraph with no long paths; this implies that it has bounded <a href="https://11011110.github.io/blog/2019/05/02/Tree-depth">tree-depth</a> and makes searching within the subgraph easy. The linear time tree algorithm similarly involves a lot of back-and-forth maneuvering of the train to free up longer and longer segments of it until the whole train is freed to move from the start to the goal.</p>

<p>A shorter version of our paper will appear at <a href="http://wads.org/">WADS</a> this summer.
While it was in submission to WADS, a related preprint appeared on arXiv: “The Parameterized Complexity of Motion Planning for Snake-Like Robots”, by Gupta, Sa’ar, and Zehavi (<a href="https://arxiv.org/abs/1903.02445">arXiv:1903.02445</a>). They show that for a graph-theoretic model of the <a href="https://en.wikipedia.org/wiki/Snake_(video_game_genre)">Snake video game</a>, getting the snake from one position to another is fixed-parameter tractable in the length of the snake. For this problem, snakes are again paths in graphs, but they can move only in one direction, and the techniques they use to prove fixed-parameter tractability involve sparsifying the state space instead of maneuvering into long paths. <a href="https://11011110.github.io/blog/2018/08/06/congratulations-dr-gupta.html">Sid Gupta was my student</a> at UCI before taking his current postdoc in Israel, but I haven’t talked to him about this, so I think their work must be independent and its appearance at about the same time a coincidence.</p>

<div class="footnotes">
  <ol>
    <li id="fn:fn">
      <p>Searching on tineye finds that this image was on Amazon in 2008. Presumably it was supplied to them by Brio? <a class="reversefootnote" href="https://11011110.github.io/blog/2019/05/02/playing-model-trains.html#fnref:fn">↩</a></p>
    </li>
  </ol>
</div>

<p>(<a href="https://mathstodon.xyz/@11011110/102029697142872437">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2019-05-02T19:03:00Z</updated>
    <published>2019-05-02T19:03:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-05-04T16:39:59Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-3204953698007493895</id>
    <link href="https://blog.computationalcomplexity.org/feeds/3204953698007493895/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/the-next-chapter.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3204953698007493895" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/3204953698007493895" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/05/the-next-chapter.html" rel="alternate" type="text/html"/>
    <title>The Next Chapter</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><div class="separator" style="clear: both; text-align: center;">
<a href="https://4.bp.blogspot.com/-R3cQu_-p5K8/XMhzxr-ZDrI/AAAAAAABnYQ/4tXefV5yICMHnD_U4g8QmFFftzHzeOWTQCLcBGAs/s1600/COS_stacked_blk_red.jpg" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="95" src="https://4.bp.blogspot.com/-R3cQu_-p5K8/XMhzxr-ZDrI/AAAAAAABnYQ/4tXefV5yICMHnD_U4g8QmFFftzHzeOWTQCLcBGAs/s320/COS_stacked_blk_red.jpg" width="320"/></a></div>
<div>
<br/></div>
I've <a href="https://news.iit.edu/stories/2019/05/lance-fortnow-designated-new-college-science-dean">accepted a position</a> as Dean of the <a href="https://science.iit.edu/">College of Science</a> at the Illinois Institute of Technology in Chicago starting in August. It's an exciting opportunity to really build up the sciences and computing in the city that I have spent the bulk of my academic career and grew to love.<br/>
<div>
<br/></div>
<div>
I had a fantastic time at Georgia Tech over the last seven years working with an incredible faculty, staff and students in the School of Computer Science. This is a special place and I enjoyed watching the school, the institute and the City of Atlanta grow and evolve.<br/>
<br/>
After I <a href="https://twitter.com/fortnow/status/1123644799825907712">tweeted</a> the news yesterday, Bill Cook reminded me that<br/>
<blockquote class="tr_bq">
Illinois Tech was the long-time home of Karl Menger, the first person to pose the problem of determining the complexity of the TSP. Now you can settle it!</blockquote>
I wouldn't bet on my settling the complexity of traveling salesman even if I didn't have a college to run. But it goes to remind us that wherever you go in life, P and NP will be right there waiting for you. </div></div>
    </content>
    <updated>2019-05-02T12:20:00Z</updated>
    <published>2019-05-02T12:20:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-05-05T12:03:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=12881</id>
    <link href="https://gilkalai.wordpress.com/2019/05/01/the-last-paper-of-catherine-renyi-and-alfred-renyi-counting-k-trees/" rel="alternate" type="text/html"/>
    <title>The last paper of Catherine Rényi and Alfréd Rényi: Counting k-Trees</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">A k-tree is a graph obtained as follows: A clique with k vertices is a k-tree. A k-tree with n+1 vertices is obtained from a k-tree with n-vertices by adding a new vertex and connecting it to all vertices of a … <a href="https://gilkalai.wordpress.com/2019/05/01/the-last-paper-of-catherine-renyi-and-alfred-renyi-counting-k-trees/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>A <em>k</em>-tree is a graph obtained as follows: A clique with <em>k</em> vertices is a <em>k</em>-tree. A <em>k</em>-tree with <em>n+1</em> vertices is obtained from a <em>k</em>-tree with n-vertices by adding a new vertex and connecting it to all vertices of a <em> k</em>-clique. There is a <a href="https://www.sciencedirect.com/science/article/pii/S0021980069801201">beautiful formula</a> by Beineke and Pippert (1969) for the number of <em>k</em>-trees with <em>n</em> labelled vertices. Their number is</p>
<p style="text-align: center;"><img alt="{{n} \choose {k}}(k(n-k)+1)^{n-k-2}." class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Bn%7D+%5Cchoose+%7Bk%7D%7D%28k%28n-k%29%2B1%29%5E%7Bn-k-2%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="{{n} \choose {k}}(k(n-k)+1)^{n-k-2}."/></p>
<p>If we count <strong>rooted</strong> <em>k</em>-trees where the root is a <em>k</em>-clique the formula becomes somewhat simpler.</p>
<p style="text-align: center;"><img alt="(k(n-k)+1)^{n-k-1}." class="latex" src="https://s0.wp.com/latex.php?latex=%28k%28n-k%29%2B1%29%5E%7Bn-k-1%7D.&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="(k(n-k)+1)^{n-k-1}."/></p>
<p>In 1972, when I was a teenage undergraduate student I was very interested in various extensions of Cayley’s formula for counting labeled trees. I thought about the question of finding a <a href="https://en.wikipedia.org/wiki/Pr%C3%BCfer_sequence">Prüfer code</a> for<em> k</em>-trees and  how to extend the results by Beineke and  Pippert when  for every clique of size <img alt="k-1" class="latex" src="https://s0.wp.com/latex.php?latex=k-1&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k-1"/> in the <em>k</em>-tree we specify its “degree”, namely, the number of <em>k</em>-cliques containing it. (I will come back to the mathematics at the end of the post.) I thank Miki Simonovits for the photos and description and very helpful comments.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/03/lakeloui.jpg"><img alt="" class="alignnone size-full wp-image-17134" height="434" src="https://gilkalai.files.wordpress.com/2019/03/lakeloui.jpg?w=640&amp;h=434" width="640"/></a></p>
<p><strong><span style="color: #ff0000;">Above, Kató Renyi, Paul Turan, Vera Sós, and Paul Erdős ; below Kató, Vera, and Lea Schönheim. Pictures: Jochanan (Janos) Schönheim.</span></strong></p>
<p><a href="https://gilkalai.files.wordpress.com/2019/03/lakelouise69.jpg"><img alt="" class="alignnone size-full wp-image-17137" height="447" src="https://gilkalai.files.wordpress.com/2019/03/lakelouise69.jpg?w=640&amp;h=447" width="640"/></a></p>
<p> </p>
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2015/04/renyi-turan-erdos.jpg"><img alt="renyi-turan-erdos" class="alignnone size-full wp-image-12883" src="https://gilkalai.files.wordpress.com/2015/04/renyi-turan-erdos.jpg?w=640"/></a></p>
<p><strong><span style="color: #ff0000;">From right, Rényi, Tur<b>á</b>n and Erdős and Grätzer. </span></strong></p>
<p> </p>
<p>While I was working on enumeration of <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-trees I came across  a paper by Catherine Rényi and Alfréd Rényi that did everything I intended to do and quite a bit more.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/rr5.png"><img alt="" class="alignnone size-large wp-image-17340" height="260" src="https://gilkalai.files.wordpress.com/2019/04/rr5.png?w=640&amp;h=260" width="640"/></a></p>
<p>What caught my eye was a heartbreaking footnote: when the paper was completed Catherine Rényi was no longer alive.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/rr2r.png"><img alt="" class="alignnone size-large wp-image-17341" height="172" src="https://gilkalai.files.wordpress.com/2019/04/rr2r.png?w=640&amp;h=172" width="640"/></a></p>
<p>The proceedings where the paper appeared were of a conference in combinatorics in Hungary in 1969. This was the first international conference in combinatorics that took place in Hungary.  The list of speakers consists of the best combinatorialists in the world and many young people including Laci Lovasz, Laci Babai, Endre Szemeredi, and many more who since then have become world-class  scientists.</p>
<p>Years later Vera Sós told me the story of Alfréd Rényi’s lecture at this conference, the first international conference in combinatorics that took place in Hungary:  “Kató died on August 23, on the day of arrival of the conference on “Combinatorial Theory and its Applications” (Balatonfured, August 24-29). Alfréd Renyi gave his talk (with the same title as the paper) on August 27 and his talk was longer than initially scheduled.  They proved the results in the paper just the week before the conference. The paper appeared in the proceedings  of the conference.”</p>
<p>Alfréd Renyi was one of the organizers of the conference and also served as one of the editors of the proceedings of the conference, which appeared in 1970. A few months after the conference, on February 1, 1970 Alfréd Rényi  died of a violent illness. The proceedings are dedicated to the memory of Catherine Rényi and Alfréd Rényi.</p>
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2019/03/scan-19.jpg"><img alt="" class="alignnone size-full wp-image-17135" height="444" src="https://gilkalai.files.wordpress.com/2019/03/scan-19.jpg?w=640&amp;h=444" width="640"/></a></p>
<p><a href="https://gilkalai.files.wordpress.com/2019/03/scan-4.jpg"><img alt="" class="alignnone size-full wp-image-17136" height="445" src="https://gilkalai.files.wordpress.com/2019/03/scan-4.jpg?w=640&amp;h=445" width="640"/></a></p>
<p><a href="https://gilkalai.files.wordpress.com/2019/03/erdos-renyi.png"><img alt="" class="alignnone size-full wp-image-17138" height="426" src="https://gilkalai.files.wordpress.com/2019/03/erdos-renyi.png?w=640&amp;h=426" width="640"/></a></p>
<p>Two pictures showing Alfréd and Catherine Rényi and a picture of Alfred Rényi and Paul Erdős.</p>
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/srrhe.png"><img alt="" class="alignnone size-full wp-image-17315" height="498" src="https://gilkalai.files.wordpress.com/2019/04/srrhe.png?w=640&amp;h=498" width="640"/></a></p>
<p>Repeating a picture from last-week <a href="https://gilkalai.wordpress.com/2019/04/21/the-random-matrix-and-more/">post</a>. From left: Sándor Szalai,  Catherine Rényi, Alfréd Rényi, András Hajnal and Paul Erdős (Matrahaza)</p>
<p>Going back to my story. I was 17 at the time and naturally I wondered if counting trees and similar things is what I want to do in my life. Shortly afterwards I went to the army. Without belittling the excitement of the army I quickly reached the conclusion that I prefer to count trees and to do similar things. My first result as a PhD student was another high dimensional extension of Cayley’s formula (mentioned in <a href="https://gilkalai.wordpress.com/2008/06/10/hellys-theorem-hypertrees-and-strange-enumeration-i/">this post</a> and a few subsequent posts).  The question of how to generalize both formulas for <img alt="k" class="latex" src="https://s0.wp.com/latex.php?latex=k&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="k"/>-trees and for my hypertrees is still an open problem. We know the objects we want to count,  we know what the outcome should be, and we know that we can cheat and use weighted counting, but still I don’t know how to make it work.</p>
<p>Some more comments on k-trees:</p>
<ol>
<li>Regarding the degree sequences for k-trees. You cannot specify the actual (k-2)-faces because those (in fact just the graph) determines the k-tree completely. So you need to count rooted k-trees and to specify the (k-2)-faces in terms of how they “grew” from the root.</li>
<li>The case that all degrees are 1 and 2 that correspond to paths for ordinary trees and to triangulating polygons with diagonals for 2-trees are precisely the stacked (k-1)-dimensional polytopes. This is a special case of the Renyi &amp; Renyi formula that was also <a href="https://link.springer.com/article/10.1007%2FBF02330563?LI=true"> found, with a different proof,</a> by Beineke and Pippert.</li>
<li>It is  unlikely that there would be a matrix-tree formula for k-trees since telling  if a graph contains a 2-tree  is known to be NP complete. See <a href="https://mathoverflow.net/questions/281848/spanning-k-trees">this MO question</a>. Maybe some matrix-tree formulas are available when we start with special classes of graphs.</li>
<li>Regarding the general objects – those are simplicial complexes that are Cohen-Macaulay and their dual (blocker) is also Cohen-Macaulay.</li>
</ol>
<p>This post is just about a single paper of Catherine Rényi and Alfréd Rényi mainly through my eyes from 45 years ago. Catherine Rényi’s  main interest originally was  Number theory, she was a student of Turàn, and soon she became  interested in the theory of Complex Analytic Functions. Alfréd Rényi was a student of Frigyes Riesz and he is known for many contributions in number theory, graph theory and combinatorics and primarily in probability theory.  Alfréd Rényi wrote several papers about enumeration of trees, and this joint paper was Catherine Rényi ‘s first paper on this topic.</p></div>
    </content>
    <updated>2019-05-01T14:19:17Z</updated>
    <published>2019-05-01T14:19:17Z</published>
    <category term="Combinatorics"/>
    <category term="People"/>
    <category term="Alfr&#xE9;d R&#xE9;nyi"/>
    <category term="Catherine R&#xE9;nyi"/>
    <category term="k-trees"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-05-05T14:20:58Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/065</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/065" rel="alternate" type="text/html"/>
    <title>TR19-065 |  Derandomization from Algebraic Hardness: Treading the Borders | 

	Mrinal Kumar, 

	Ramprasad Saptharishi, 

	Noam Solomon</title>
    <summary>A hitting-set generator (HSG) is a polynomial map $Gen:\mathbb{F}^k \to \mathbb{F}^n$ such that for all $n$-variate polynomials $Q$ of small enough circuit size and degree, if $Q$ is non-zero, then $Q\circ Gen$ is non-zero. In this paper, we give a new construction of  such a HSG assuming that we have an explicit polynomial of sufficient hardness in the sense of approximative or border complexity.  Formally, we prove the following result over any characteristic zero field $\mathbb{F}$:

Suppose $P(z_1,\ldots, z_k)$ is an explicit $k$-variate degree $d$ polynomial that is not in the border of circuits of size $s$. Then, there is an explicit hitting-set generator $Gen_P:\mathbb{F}^{2k} \rightarrow \mathbb{F}^n$ such that  every non-zero $n$-variate degree $D$ polynomial $Q(x_1, x_2, \ldots, x_n)$ in the border of size $s'$ circuits satisfies $Q \neq 0 \Rightarrow Q \circ Gen_P \neq 0$, provided $n^{10k}d D s' \leq s$. 

This is the first HSG in the algebraic setting that yields a complete derandomization of polynomial identity testing (PIT) for general circuits from a suitable algebraic hardness assumption.

As a direct consequence, we show that even a slightly non-trivial explicit construction of hitting sets for polynomials in the border of constant-variate circuits implies a deterministic polynomial time algorithm for PIT. More precisely, we prove the following theorem:

Let $\delta &gt; 0$ be any constant and $k$ be a large enough constant. Suppose, for every $s \geq k$, there is an explicit hitting set of size $s^{k-\delta}$ for all degree $s$ polynomials in the border of $k$-variate size $s$ algebraic circuits. Then, there is an explicit hitting set of size $poly(s)$ for the border $s$-variate algebraic circuits of size $s$ and degree $s$. 

Unlike the prior constructions of such maps [NW94, KI04, AGS18, KST19], our construction is purely algebraic and does not rely on the notion of combinatorial designs.</summary>
    <updated>2019-05-01T05:54:23Z</updated>
    <published>2019-05-01T05:54:23Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-05-05T14:20:50Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/04/30/linkage</id>
    <link href="https://11011110.github.io/blog/2019/04/30/linkage.html" rel="alternate" type="text/html"/>
    <title>Linkage</title>
    <summary>Good article, terrible headline (). Bill Gasarch rants about several recent instances of clickbaity, inaccurate, and overhyped media coverage of theoretical computer science topics. I suspect the answer to his question “is it just our field?” is no.</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><ul>
  <li>
    <p><a href="https://blog.computationalcomplexity.org/2019/04/good-article-terrible-headline.html">Good article, terrible headline</a> (<a href="https://mathstodon.xyz/@11011110/101938798669973189"/>). Bill Gasarch rants about several recent instances of clickbaity, inaccurate, and overhyped media coverage of theoretical computer science topics. I suspect the answer to his question “is it just our field?” is no.</p>
  </li>
  <li>
    <p><a href="https://www.vox.com/science-and-health/2019/4/16/18311194/black-hole-katie-bouman-trolls">Vox on the sexist backlash against astronomer Katie Bouman</a> (<a href="https://mathstodon.xyz/@11011110/101942756338391262"/>, <a href="https://www.cnn.com/2019/04/12/us/andrew-chael-katie-bouman-black-hole-image-trnd/index.html">see also</a>), of black hole image fame, after she was cast by the media in the “lone genius” role typically reserved for men and untypical of how science actually happens.</p>
  </li>
  <li>
    <p><a href="https://aperiodical.com/2019/04/mathematical-sign-language-interview-with-dr-jess-boland/">Mathematical sign language</a> (<a href="https://mathstodon.xyz/@11011110/101950143529837988"/>). Hearing-impaired eletrical engineering researcher Jess Boland discovered that weren’t enough technical terms in British Sign Language to cover the mathematics she uses in her work, so she’s been creating new ones as well as promoting the ones BSL already had. Katie Steckles interviews her for <em>The Aperiodical</em>.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1804.05452">Regular polygon surfaces</a> (<a href="https://mathstodon.xyz/@11011110/101955536664219652"/>). Ian Alevy answers <a href="http://cs.smith.edu/~jorourke/TOPP/P72.html#Problem.72">Problem 72 of The Open Problems Project</a>: every topological sphere made of regular pentagons can be constructed by gluing regular dodecahedra together. You can also <a href="https://momath.org/mathmonday/the-paragons-system/">glue dodecahedra to get higher-genus surfaces</a>, but Alevy’s theorem doesn’t apply, so we don’t know whether all higher-genus regular-pentagon surfaces are formed that way.</p>
  </li>
  <li>
    <p><a href="https://www.insidehighered.com/news/2019/04/12/czech-president-blocks-professorships-academic-critics">Czech president Miloš Zeman “has repeatedly used presidential powers to block the professorships of political opponents”</a> (<a href="https://mathstodon.xyz/@11011110/101965701030220573"/>). Charles University is now suing to allow their promotions to go through.</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/1904.08845">Planar point sets determine many pairwise crossing segments</a> (<a href="https://mathstodon.xyz/@11011110/101968467896290245"/>). János Pach, Natan Rubin, and Gábor Tardos make significant progress on  whether every<br/>
 points in the plane have a large matching where all edges cross each other. A 1994 paper by Paul Erdős and half a dozen others only managed to prove this for “large” meaning . The new paper proves a much stronger bound,  (Ryan Williams’ favorite function).</p>
  </li>
  <li>
    <p><a href="https://randomascii.wordpress.com/2019/04/21/on2-in-createprocess/">Why asymptotics matters</a> (<a href="https://mathstodon.xyz/@11011110/101970781407484011"/>, <a href="https://news.ycombinator.com/item?id=19716673">via</a>): because if you don’t pay attention to it you get problems like this slow quadratic-time process creation bug in Windows 10.</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@jsiehler/101982200745112808">Snap cube puzzle</a>. The cubes have one peg and five holes; how many ways can you snap them into a connected  block with no pegs showing? See link in discussion thread for spoilers.</p>
  </li>
  <li>
    <p>There’s lots of reasons to be unenthusiastic about newly-official-presidential-candidate Biden involving multiple instances of poor treatment of African-Americans and women, but here’s another more techy reason: <a href="https://www.pastemagazine.com/articles/2019/04/biden-to-attend-fundraiser-hosted-by-comcast-blue.html">his first major fundraiser as a candidate closely involves anti-net-neutrality lobbyists from Comcast</a> (<a href="https://mathstodon.xyz/@11011110/101987719804605064"/>).</p>
  </li>
  <li>
    <p><a href="https://adsabs.github.io/blog/transition-reminder">SAO/NASA Astrophysics Data System updates its user interface</a> (<a href="https://mathstodon.xyz/@11011110/101996739440443058"/>). <a href="http://adsabs.harvard.edu/">The ADS</a> is a useful database of papers in astronomy and related fields. From comments on their post, the new UI is very slow. It is <a href="http://adsabs.github.io/help/faq/">unusable without JavaScript</a>. And it <a href="https://en.wikipedia.org/wiki/Special:Diff/892128592">“sends the users’ personal identifying information to at least 5 third-party companies”</a>. This is progress?</p>
  </li>
  <li>
    <p><a href="https://mathoverflow.net/q/329910/440">I ask for a reference for an easy fact about divisibility representations of partial orders</a> (<a href="https://mathstodon.xyz/@11011110/102002516978139958"/>). The MathOverflow community isn’t very helpful, preferring instead to simultaneously complain that it’s too trivial and explain why it’s true to me as if I didn’t already say in my question that I thought it was trivial.`</p>
  </li>
  <li>
    <p><a href="https://mathstodon.xyz/@henryseg/101975738950740643">Cannon-Thurston maps for veering triangulations</a>, whatever those are. Henry Segerman posts some pretty pictures from his joint work with David Bachman and Saul Schleimer.</p>
  </li>
  <li>
    <p><a href="https://www.mathunion.org/fileadmin/CWM/Initiatives/CWMNewsletter1.pdf">Newsletter of the IMU Committee for Women</a> (<a href="https://aperiodical.com/2019/04/imu-committee-for-women-in-mathematics-now-has-a-newsletter/">via</a>). Includes an interview with Marie-Francoise Roy and the announcement of the book <em>World Women in Mathematics 2018</em>.</p>
  </li>
  <li>
    <p><a href="https://rjlipton.wordpress.com/2019/04/30/network-coding-yields-lower-bounds/">Network coding yields lower bounds</a> (<a href="https://mathstodon.xyz/@11011110/102018096543192991"/>). Lipton and Regan report on <a href="https://arxiv.org/abs/1902.10935">a new paper by Afshani, Freksen, Kamma, and Larsen</a> on lower bounds for multiplication. If algorithmically opening and recombining network messages never improves fractional flow, then  circuit size for multiplication is optimal. But the same lower bound holds for simpler bit-shifting operations, so it’s not clear how it could extend from circuits to bignum algorithms.</p>
  </li>
</ul></div>
    </content>
    <updated>2019-04-30T23:15:00Z</updated>
    <published>2019-04-30T23:15:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-05-04T16:39:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/064</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/064" rel="alternate" type="text/html"/>
    <title>TR19-064 |  Randomness and Intractability in Kolmogorov Complexity | 

	Igor Carboni Oliveira</title>
    <summary>We introduce randomized time-bounded Kolmogorov complexity (rKt), a natural extension of Levin's notion of Kolmogorov complexity from 1984. A string w of low rKt complexity can be decompressed from a short representation via a time-bounded algorithm that outputs w with high probability. 

This complexity measure gives rise to a decision problem over strings: MrKtP (The Minimum rKt Problem). We explore ideas from pseudorandomness to prove that MrKtP and its variants cannot be solved in randomized quasi-polynomial time. This exhibits a natural string compression problem that is provably intractable, even for randomized computations. Our techniques also imply that there is no n^{1-eps}-approximate algorithm for MrKtP running in randomized quasi-polynomial time. 

Complementing this lower bound, we observe connections between rKt, the power of randomness in computing, and circuit complexity. In particular, we present the first hardness magnification theorem for a natural problem that is unconditionally hard against a strong model of computation.</summary>
    <updated>2019-04-30T18:41:08Z</updated>
    <published>2019-04-30T18:41:08Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-05-05T14:20:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15814</id>
    <link href="https://rjlipton.wordpress.com/2019/04/30/network-coding-yields-lower-bounds/" rel="alternate" type="text/html"/>
    <title>Network Coding Yields Lower Bounds</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Practice leads theory Peyman Afshani, Casper Freksen, Lior Kamma, and Kasper Larsen have a beautiful new paper titled “Lower Bounds for Multiplication via Network Coding”. Today we will talk about how practical computing played a role in this theory research. The authors (AFKL) state this: In this work, we prove that if a central conjecture … … <a href="https://rjlipton.wordpress.com/2019/04/30/network-coding-yields-lower-bounds/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Practice leads theory</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<a href="https://rjlipton.files.wordpress.com/2019/04/akkl.jpg"><img alt="" class="alignright wp-image-15816" height="162" src="https://rjlipton.files.wordpress.com/2019/04/akkl.jpg?w=123&amp;h=162" width="123"/></a><p/><p>
Peyman Afshani, Casper Freksen, Lior Kamma, and Kasper Larsen have a beautiful new <a href="https://arxiv.org/abs/1902.10935">paper</a> titled “Lower Bounds for Multiplication via Network Coding”. </p><p>
Today we will talk about how practical computing played a role in this theory research.</p><p>
The authors (AFKL) state this:</p><blockquote><p><b> </b> <em> In this work, we prove that if a central conjecture in the area of network coding is true, then any constant degree boolean circuit for multiplication must have size <img alt="{\Omega(n \log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n+%5Clog+n%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\Omega(n \log n)}"/>, thus <s>almost</s> completely settling the complexity of multiplication circuits. </em>
</p></blockquote><p><span id="more-15814"/></p><p/><p>
We added the strikeout because of the <img alt="{O(n \log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n+%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n \log n)}"/> upper bound that we discussed recently <a href="https://rjlipton.wordpress.com/2019/03/29/integer-multiplication-in-nlogn-time/">here</a>.</p><p>
AFKL have conditionally solved a long standing open problem: “How hard is it to multiply two <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-bit numbers?” Their proof shows that a conjecture from practice implies a circuit lower bound. This is rare: using a conjecture from practice, to solve a complexity open problem. We have used conjectures from many parts of mathematics, and from some parts of physics, to make progress, but drawing on experience with practical networking is strikingly fresh. </p><p>
</p><p/><h2> Integer Multiplication </h2><p/><p/><p>
The authors AFKL explain the history of the multiplication problem. We knew some of the story, but not all the delicious details.</p><blockquote><p><b> </b> <em> In 1960, Andrey Kolmogorov conjectured that the thousands of years old <img alt="{O(n^{2})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%7B2%7D%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{O(n^{2})}"/>-time algorithm is optimal and he arranged a seminar at Moscow State University with the goal of proving this conjecture. However only a week into the seminar, the student Anatoly Karatsuba came up with an <img alt="{O(n^{\log_{2}3}) \approx O(n^{1.585})}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E%7B%5Clog_%7B2%7D3%7D%29+%5Capprox+O%28n%5E%7B1.585%7D%29%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{O(n^{\log_{2}3}) \approx O(n^{1.585})}"/> time algorithm. The algorithm was presented at the next seminar meeting and the seminar was terminated. </em>
</p></blockquote><p/><p>
Ken and I wish we could have Kolmogorov’s luck, in one of our seminars. Partly because it would advance knowledge; partly because it would let us out of teaching. Sweet.</p><p>
The main result of AKKL is:</p><blockquote><p><b>Theorem 1</b> <em><a name="NC2mult"/> Assuming the Network Conjecture, every general boolean circuit that computes the product of two <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n}"/>-bit integers has size order at least <img alt="{n\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Clog+n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n\log n}"/>. </em>
</p></blockquote><p/><p>
This says that the boolean complexity of multiplication is super-linear. No restriction of a bounded depth, no restriction on the operations allowed, no restrictions at all. Given our non-existent lower bounds this is remarkable. If it was unconditional, it would be a terrific result. But it still is a strong one. </p><p>
We will next explain what the Network Coding Conjecture (NCC) is. </p><p>
</p><p/><h2> Network Coding </h2><p/><p/><p>
One of the basic papers was authored by Rudolf Ahlswede, Ning Cai, Shuo-Yen Li, and Raymond Yeung <a href="http://www.inf.fu-berlin.de/lehre/WS11/Wireless/papers/CodAhlswede00.pdf">here</a>. The paper has close to ten thousand citations, which would be amazing for a theory paper.</p><p>
In basic networks each node can receive and send messages to and from other nodes. They can only move messages around—they are not allowed to peer into a message. The concept of <i>network coding</i> is to allow nodes also to decode and encode messages. Nodes can peer into messages and create new ones. The goal, of course, is to decrease the time required to transmit information through the network.</p><p>
The following example combines figures from a 2004 <a href="http://www.eecg.utoronto.ca/~bli/papers/allerton04.pdf">paper</a> by Zongpeng Li and Baochun Li which formulated the NCC. At left is a situation where two senders, <img alt="{S_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S_1}"/> with an <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-bit message <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a}"/> and <img alt="{S_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S_2}"/> with an <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/>-bit message <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/>, wish to transmit to respective receivers <img alt="{T_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_1}"/> and <img alt="{T_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T_2}"/>. The network’s links are one-way as shown, with two intermediate nodes <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> and <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/>, and each link can carry <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> bits at any one time.</p><p/><p><br/>
<a href="https://rjlipton.files.wordpress.com/2019/04/flowfigures.png"><img alt="" class="aligncenter wp-image-15815" height="115" src="https://rjlipton.files.wordpress.com/2019/04/flowfigures.png?w=500&amp;h=115" width="500"/></a></p><p/><p><br/>
If <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a}"/> and <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/> are black-boxes that must be kept entire, there is no way to solve this in three time steps. But if the nodes can read messages and do lightweight computations, then the middle diagram gives a viable solution. Node <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> reads <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a}"/> and <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/> and on-the-fly transmits their bitwise exclusive-or to node <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/>. Node <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> relays this to each receiver, who has also received the other party’s message directly. The receivers can each do a final exclusive-or to recover the messages intended for them. </p><p>
The ability to look inside messages seems powerful, and there are networks where it helps even more dramatically. Incidentally, as <a href="https://en.wikipedia.org/wiki/Linear_network_coding">noted</a> by Wikipedia, the exclusive-or trick was anticipated in a 1978 <a href="https://ieeexplore.ieee.org/document/1455117">paper</a> showing how the two senders can exchange their messages <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a}"/> and <img alt="{b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bb%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{b}"/> by relaying them to a satellite which transmits <img alt="{a \oplus b}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba+%5Coplus+b%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a \oplus b}"/> back to each.</p><p>
</p><p/><h2> The Conjecture </h2><p/><p/><p>
However, there is another solution if the links are bi-directional and messages can be broken in half. Sender <img alt="{S_1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S_1}"/> simply routes half of <img alt="{a}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Ba%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{a}"/> one way around the network and the other half the other way. Sender <img alt="{S_2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS_2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S_2}"/> does similarly. This is shown at far right. Each link never has more than <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> bits of total load and the three-step elapsed time is the same. Moreover, the link from <img alt="{A}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BA%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{A}"/> to <img alt="{B}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B}"/> is not needed. This is just an undirected network commodity flow with fractional units.</p><p>
In fact, <i>no</i> example is known in an undirected network where encoding beats fractional routing. That is, the known network encoding rate is just the flow rate of the network. The network coding (NCC) conjecture is informally:</p><blockquote><p><b> </b> <em> <i>The coding rate is never better than the flow rate in undirected graphs</i>. </em>
</p></blockquote><p/><p>
The paper by Li and Li gave formal details and several equivalent statements. Quoting them:</p><blockquote><p><b> </b> <em> For undirected networks with integral routing, there still exist configurations that are feasible with network coding but infeasible with routing only. For undirected networks with fractional routing, we show that the potential of network coding to help increase throughput in a capacitied network is equivalent to the potential of network coding to increase bandwidth efficiency in an uncapacitied network. We conjecture that these benefits are non-existent. </em>
</p></blockquote><p>
</p><p/><h2> Good and Bad News </h2><p/><p/><p>
What has become of the NCC in the fifteen years since? Here’s how Ken and I see it:</p><p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>The Good News</i>: The NCC helps solve long-standing open problems. Since this conjecture is widely believed this is impressive. Besides integer multiplication, NCC has been used to prove other lower bounds. For example, Larsen working with Alireza Farhadi, Mohammad Hajiaghayi, and Elaine Shi used it to <a href="https://arxiv.org/abs/1811.01313">prove</a> lower bounds on sorting with external memory. </p><p>
<img alt="{\bullet }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cbullet+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\bullet }"/> <i>The Bad News</i>: The NCC helps solve long-standing open problems. This suggests that this conjecture could be deep and hard to resolve. The boolean complexity of integer multiplication is a long standing open question. Since the NCC leads to a non-linear lower bound, perhaps proving this conjecture could be hopeless.</p><p>
I have mixed feelings about these lower bound results. They are impressive and shed light on hard open problems. But I wonder if the NCC could be wrong. There is a long <a href="https://rjlipton.wordpress.com/2010/06/19/guessing-the-truth/">history</a> in complexity theory where guesses of the form: </p><blockquote><p><b> </b> <em> <i>The obvious algorithm is optimal</i> </em>
</p></blockquote><p>have failed. The situation strikes us as resembling that of the (Strong) Exponential Time Hypothesis, in ways we <a href="https://rjlipton.wordpress.com/2015/06/01/puzzling-evidence/">discussed</a> four years ago. </p><p>
</p><p/><h2> How the New Paper Works </h2><p/><p/><p>
The authors AKKL did not know that an <img alt="{O(n\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n\log n)}"/> upper bound had been proved for integer multiplication when they posted their paper. They did, however, prove a stronger version of Theorem (<a href="https://rjlipton.wordpress.com/feed/#NC2mult">1</a>) for a problem with a known <img alt="{n\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Clog+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n\log n}"/> upper bound. This is to create circuits with <img alt="{n + \log_2 n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn+%2B+%5Clog_2+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n + \log_2 n}"/> input gates and <img alt="{2n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2n}"/> output gates that given <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> and a binary number <img alt="{\ell \leq n = |x|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell+%5Cleq+n+%3D+%7Cx%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell \leq n = |x|}"/> output the string <img alt="{y}" class="latex" src="https://s0.wp.com/latex.php?latex=%7By%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{y}"/> whose bits <img alt="{n-\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn-%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n-\ell}"/> through <img alt="{2n-\ell-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2n-%5Cell-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2n-\ell-1}"/> equal <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/>, with other bits <img alt="{0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0}"/>. A conditional lower bound on this <i>shift</i> task implies the same for multiplication, since the shift is the same as multiplication by <img alt="{2^\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5E%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2^\ell}"/>. </p><blockquote><p><b>Theorem 2</b> <em><a name="NC2shift"/> Assuming the NCC, circuits for the shift task need size order <img alt="{n\log n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5Clog+n%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{n\log n}"/>. </em>
</p></blockquote><p/><p>
The proof is disarmingly elementary: The input <img alt="{x}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x}"/> gives <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> “senders” and each value of <img alt="{\ell}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell}"/> creates a different set of <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> “receivers.” With a circuit <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> fixed, they show one can fix a shift <img alt="{\ell_0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_0}"/> so that the average distance from sender to receiver in an undirected multi-commodity flow is <img alt="{\Omega(\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Omega(\log n)}"/>, giving <img alt="{\Omega(n\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5COmega%28n%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Omega(n\log n)}"/> total flow. If <img alt="{C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C}"/> achieves smaller size, then it represents a counterexample to the NCC. </p><p>
Pretty neat—this is half a page in the paper. The paper proves more intricate results relating to conjectures by Les Valiant about Boolean circuits of bounded fan-in and <img alt="{O(\log n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Clog+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\log n)}"/> depth that compute permutations and their reduction to depth-3 circuits of unbounded fan-in. This also may extend to the sorting/shifting problem Ken wrote about long ago in a guest <a href="https://blog.computationalcomplexity.org/2007/07/concrete-open-problem.html">post</a> for Lance Fortnow and Bill Gasarch’s blog.</p><p>
</p><p/><h2> Open Problems </h2><p/><p/><p>
Is the NCC true? Can it be proved for some interesting classes of graphs? I believe it is known for tiny size graphs of at most six nodes. What about, for example, planar graphs?</p><p>
[inserted “conditionally” before “solved” in intro]</p><table class="image alignright">











































</table></font></font></div>
    </content>
    <updated>2019-04-30T13:27:11Z</updated>
    <published>2019-04-30T13:27:11Z</published>
    <category term="algorithms"/>
    <category term="All Posts"/>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="News"/>
    <category term="Open Problems"/>
    <category term="Proofs"/>
    <category term="Results"/>
    <category term="trick"/>
    <category term="Casper Freksen"/>
    <category term="circuits"/>
    <category term="communication complexity"/>
    <category term="conjecture"/>
    <category term="integer multiplication"/>
    <category term="Kasper Larsen"/>
    <category term="Lior Kamma"/>
    <category term="lower bounds"/>
    <category term="network coding"/>
    <category term="Peyman Afshani"/>
    <category term="reduction"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-05-05T14:21:02Z</updated>
    </source>
  </entry>

  <entry>
    <id>http://gradientscience.org/policy_gradients_pt3</id>
    <link href="http://gradientscience.org/policy_gradients_pt3" rel="alternate" type="text/html"/>
    <title>A Closer Look at Deep Policy Gradients (Part 3&amp;#58; Landscapes and Trust Regions)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>This post is the last of a three part series about our recent paper: “<a href="https://arxiv.org/abs/1811.02553">Are Deep
Policy Gradient Algorithms Truly Policy Gradient
Algorithms?</a>” Today, we will analyze agents’
reward landscapes as well as try to understand to what extent, and by what mechanisms,
our agents enforce so-called <i>trust regions</i>.</p>

<p>First, a quick recap (it’s been a while!):</p>
<ul>
  <li>
    <p>In our <a href="https://gradientscience.org/policy_gradients_pt1">first post</a>, we outlined the
RL framework and introduced policy gradient algorithms. We saw that
auxiliary optimizations hidden in the implementation details of RL algorithms
drastically impact performance. These findings highlighted the need for a more
fine-grained analysis of how algorithms really operate.</p>
  </li>
  <li>
    <p>In our <a href="https://gradientscience.org/policy_gradients_pt2">second post</a>, we zoomed in on
three algorithms: trust region policy optimization (TRPO), proximal policy
optimization (PPO), and an algorithm we called “PPO-M.” PPO-M is the core PPO
algorithm exactly as described in the <a href="https://arxiv.org/abs/1707.06347">original
paper</a>, without any of the auxiliary
optimizations. Using these methods as a test-bed, we studied two core
primitives of the policy gradient framework: gradient estimation and value
prediction.</p>
  </li>
</ul>

<p>Our discussion today begins where we left off in our second post. Recall that
last time we studied the variance of the gradient estimates our algorithms use
to maximize rewards. We found (among other things) that, despite high variance, algorithm steps were
still (very slightly) correlated with the actual, “true” gradient of the reward.
However, how good was this true gradient to begin with? After all, a fundamental
assumption of the whole policy gradient framework is that our gradient steps actually point in a direction (in policy
parameter space) that increases the reward. Is this indeed so in practice?</p>

<h2 id="optimization-landscapes">Optimization Landscapes</h2>
<p>Recall from our <a href="https://gradientscience.org/policy_gradients_pt1">first post</a> that
policy gradient methods treat reward maximization as a zeroth-order optimization
problem. That is, they maximize the objective by applying first order methods
with finite sample gradient estimates of the form:</p>



<p>Here,  represents the cumulative reward of a trajectory , where
 is sampled from the distribution of trajectories induced by the current
policy . We let  represent an easily computable
function of  that is an unbiased estimator of the gradient of the reward
(seen on the left hand side)—for more details see <a href="https://gradientscience.org/policy_gradients_pt1/#rl-with-policy-gradients">our previous post</a>.
Finally, we denote by  the number of trajectories used to estimate the
gradient.</p>

<p>An important point is, however, that instead of following the gradients of the cumulative reward (as
suggested by the above equation), the algorithms we analyze actually use a
<i>surrogate reward</i> at each step. The surrogate reward is a function of the
collected trajectories that is meant to locally approximate the true reward,
while providing other benefits such as easier computation and a smoother
optimization landscape. Consequently, at each step these algorithms maximize the
surrogate rewards  instead of following the gradient of the
true reward .</p>

<p>A natural question to ask is: <i>do steps maximizing the surrogate reward
consistently increase policy returns?</i> To answer this, we will use
<i>landscape plots</i> as a tool for visualizing the landscape of returns
around a given policy :</p>

<p><img alt="Labeled diagram of optimization landscape plot" src="https://gradientscience.org/images/rl/labeled_landscape.jpg"/></p>

<p>Here, for each point  in the plot,  and  specify a policy
 parameterized by</p>



<p>where  is the step computed by the studied algorithm. (Note that
we include the random Gaussian  direction to visualize how
“important” the step direction is compared to a random baseline). The  axis
corresponds to the return attained by the policy , which we
denote by .</p>

<p>Now, when we make this plot for a random  (corresponding to a randomly
initialized policy network), everything looks as expected:</p>

<p><img alt="Landscape for randomly initialized network" src="https://gradientscience.org/images/rl/step_0_landscape.jpg"/></p>

<p>That is, the return increases significantly more quickly along the step direction than in
the random direction. However, repeating these landscape experiments at later
iterations, we find a more surprising picture: going in the step direction
actually <i>decreases</i> the average cumulative reward obtained by the resulting
agent!</p>

<p><img alt="Landscape for trained networks" src="https://gradientscience.org/images/rl/landscape_step_150_300_450.jpg"/></p>

<p>So what exactly is going on here? The steps computed by these algorithms are
estimates of the gradient of the reward, so it is unexpected that the reward
plateaus (or in some cases decreases!) along this direction.</p>

<p>We find that the
answer lies in a <i>misalignment</i> of the true reward and the surrogate. Specifically, while
the steps taken do correspond to an improvement in the surrogate problem, they
do <i>not</i> always correspond to a similar improvement in the true return.
Here are the same graphs as above shown again, this time with the corresponding
optimization landscapes of the <i>surrogate loss</i>
<sup><a href="https://gradientscience.org/feed.xml#footnote1">1</a></sup>:</p>
<div class="footnote">
<sup><a id="footnote1">1</a></sup>All of the landscapes we plot in this post are for PPO; we have (similar)
results for TRPO in <a href="https://arxiv.org/abs/1811.02553">our paper</a>.
</div>

<p><img alt="Surrogate vs real landscape" src="https://gradientscience.org/images/rl/surrogate_vs_real_landscape.jpg"/></p>

<!-- The surrogate reward landscape misalignment is further complicated by the fact -->
<!-- that all of the rewards plotted in the landscapes we’ve seen so far are computed with -->
<!-- many, many more samples than an algorithm would ever collect at a single step in -->
<!-- practice.  -->

<p>To make matters worse, we find that in the low sample regime that policy
gradient methods actually operate in, it is hard to even <i>discern</i> directions of
improvement in the true reward landscape. (In all the plots above,
we use orders of magnitude more samples than an agent would ever see in practice
at a single step.) In the plot below, we visualize reward landscapes while
varying the number of samples used to estimate the expected return of a policy
:</p>

<p><img alt="Surrogate vs real landscape" src="https://gradientscience.org/images/rl/landscape_conc.jpg"/></p>

<p>In contrast to the smooth landscape we see on the right and in the plots above,
the reward landscape actually accessible to the model is jagged and poorly behaved.
This landscape makes it thus near-impossible for an agent to distinguish between good
and bad points in its relevant sample regime, even when the true underlying
landscape is fairly well-behaved!</p>

<p>Overall, our investigation into the optimization landscape of policy gradient algorithms
reveals that (a) the surrogate reward function is often misaligned with the
underlying true return of the policy, and (b) in the relevant sample regime, it
is hard to distinguish between “good” steps and “bad” steps, even when looking at the true reward
landscape. As always, however, <i>none of this stops the agents from training
and continually improving reward in the average sense</i>. This raises some
key questions about the landscape of policy optimization:</p>

<ul>
  <li>Given that the function we actually optimize is so often misaligned with the
underlying rewards, how is it that agents continually improve?</li>
  <li>Can we explain or link the local behaviour we observe in the landscape with a
more global view of policy optimization?</li>
  <li>How do we ensure that the reward landscape is navigable? And, more generally,
what is the best way to navigate it?</li>
</ul>

<h2 id="trust-regions">Trust Regions</h2>
<p>Let us now turn our attention to another important notion in the popular policy gradient algorithms: that of the <i>trust region</i>. 
Recall that a convenient way to think about our training process is to view it as a series of policy parameter iterates:</p>



<p>An important aspect of this process is ensuring that the steps we take don’t
lead us outside of the region (of parameter space) where the samples we
collected are informative. Intuitively, if we collect samples at a given set of
policy parameters , there is no reason to expect that these samples
should tell us about the performance of a new set of parameters that is far away
from .</p>

<p>Thus, in order to ensure that gradient steps are predictive, classical
algorithms like the 
<a href="http://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf">conservative policy update</a> 
employ update schemes that constrain the probability distributions induced by 
successive policy parameters.  The <a href="https://arxiv.org/abs/1502.05477">TRPO paper</a> 
in particular showed 
that one can 
guarantee monotonic policy improvement with each step by solving a surrogate problem of the following form:</p>



<p>The second, “penalty” term in the above objective, referred to as the <i>trust region</i> penalty, is a
critical component of modern policy gradient algorithms. TRPO, one of the
algorithms we study, proposes a relaxation of \eqref{eq:klpen} that
instead imposes a hard constraint on the
<i>mean</i> KL divergence<sup><a href="https://gradientscience.org/feed.xml#footnote2">2</a></sup>
 (estimated using the empirical samples we obtain):</p>



<div class="footnote">
<sup><a id="footnote2">2</a></sup>It's worth noting that 
<a href="https://arxiv.org/abs/1705.10528">a
recent paper</a> showed that under some conditions, mean KL is actually
sufficient.
</div>

<p>In other words, we try to ensure that the <i>average</i> distance between 
conditional probability distributions is small. 
Finally, PPO approximates the mean KL bound of TRPO by attempting to
constrain a <i>ratio</i> between successive conditional probability
distributions, instead of the KL divergence. The exact mechanism for
enforcing this is shown in the 
box below. Intuitively, however, what PPO does is just throw away
(i.e. get no gradient signal from) the
rewards incurred from any state-action pair such that:</p>



<p>where  is a user-chosen hyperparameter.</p>

<section class="container">
<div>
<div class="checkboxdiv">
<input id="ac-1" name="accordion-1" type="checkbox"/>
<label for="ac-1"><span class="fas fa-chevron-right" id="titlespan"/> <strong>The PPO update step</strong> (Click to expand)</label>
<article class="small">
<i>Note that the following is only for interested readers and is unessential 
for reading the rest of the blog post.</i> <br/> <br/>

The exact update used by PPO is as follows, where $\widehat{A}_\pi$ is
the <a href="https://arxiv.org/abs/1506.02438">generalized advantage 
estimate</a>:
$$
\begin{array}{c}{\max _{\theta} \mathbb{E}_{\left(s_{t}, a_{t}\right) \sim \pi}\left[\min \left(\operatorname{clip}\left(\rho_{t}, 1-\varepsilon, 1+\varepsilon\right) \widehat{A}_{\pi}\left(s_{t}, a_{t}\right), \rho_{t} \widehat{A}_{\pi}\left(s_{t}, a_{t}\right)\right)\right]} \\ {\text{where }\ \ \rho_{t}=\frac{\pi_{\theta}\left(a_{t} | s_{t}\right)}{\pi\left(a_{t} | s_{t}\right)}}\end{array}
$$
As described in the main text, this intuitively corresponds to throwing
away (i.e. getting no gradient signal) from state-action pairs where the
ratio of conditional probabilities between successive policies is too high.
</article>
</div>
</div>
</section>
<p><br/></p>

<p>To recap, there is a theoretically motivated algorithm \eqref{eq:klpen} which
constrains maximum KL. This motivates TRPO’s bound on
mean KL in \eqref{eq:trpotrust}, which in turn motivates the 
ratio-based bound of PPO (shown in the box). This chain of approximations might 
lead us to ask: <i>how well do these algorithms actually maintain trust regions</i>?</p>

<p>We first plot the mean KL divergence between successive policies for each
algorithm:
<img src="https://gradientscience.org/images/rl/meankl_trust.jpg" style="width: 50%;"/>
<br/></p>

<p>TRPO seems to constrain this very well
<a href="https://gradientscience.org/feed.xml#footnote3"><sup>3</sup></a>
! On the other hand, our two
varieties of the PPO algorithm paint a drastically different picture. Recall that we decided to
separately study two versions of PPO: PPO (based on a state-of-the-art implementation), 
and PPO-M, which we defined to be the core PPO algorithm without auxiliary optimizations. 
PPO <i>with</i> optimizations does quite well at maintaining a KL trust region, 
but PPO-M does not. This is unexpected: PPO’s main mechanism for maintaining the
trust region (the ratio clipping) is present in both methods—the
differences are only in auxiliary optimizations such as Adam learning rate annealing or
orthogonal initialization. As such, it is unclear exactly which mechanisms 
in PPO are responsible for maintaining the mean KL constraint.</p>

<div class="footnote">
<a id="footnote3"><sup>3</sup></a>
Note that this is somewhat unsurprising, since TRPO constrains this
directly in its optimization.
</div>

<p>In fact, we find that PPO’s inability to maintain a KL-based trust region 
is not entirely due to the looseness of its relaxation; it turns out that 
PPO does not even successfully enforce its <i>own</i> ratio-based trust region.
Below, we plot the maximum ratio \eqref{eq:ratio} between successive
policies for the three algorithms in question:</p>

<p><img src="https://gradientscience.org/images/rl/maxratio_trust.jpg" style="width: 50%;"/>
<br/></p>

<p>In the above, the dotted line represents , corresponding to the 
bound in \eqref{eq:ratio}—it looks like the max ratio is not kept at all! And once again, 
simply adding the auxiliary, code-level optimizations to PPO-M
yields <i>better</i> trust region
enforcement, despite the main clipping mechanism staying the same.
Indeed, it turns out that the
way that PPO enforces the ratio trust region does not actually keep the ratios
from becoming too large or too small. In fact, in our paper we show that there
are <i>infinite</i> optima of the optimization problem PPO solves to find each
step and only <i>one</i> of them enforces the intended trust region bound.</p>

<h2 id="wrapping-up">Wrapping Up</h2>

<p>Deep reinforcement learning algorithms are rooted in a well-grounded framework
of classical RL, and have shown great promise in practice. However, as we’ve
found in our three-part investigation, this framework often falls a little short
of explaining the behavior of these algorithms in practice.</p>

<p>Beyond just being disconcerting, this disconnect impedes our understanding of
why these algorithms succeed (or fail). It also poses a major barrier to
addressing key challenges facing deep RL, such as widespread brittleness and
poor reproducibility (as has been observed by our 
<a href="https://gradientscience.org/policy_gradients_pt1">study in part one</a> and
many others, e.g., [<a href="https://arxiv.org/abs/1709.06560">1</a>,
<a href="https://www.alexirpan.com/2018/02/14/rl-hard.html">2</a>,
<a href="http://amid.fish/reproducing-deep-rl">3</a>]).</p>

<p>To close this gap, we need to either develop methods that adhere more closely to
theory, or build theory that can capture what makes existing policy gradient
methods successful. In both cases, the first step is to precisely pinpoint where
theory and practice diverge. Even more broadly, our findings suggest that
developing a deep RL toolkit that is truly robust and reliable will require
moving beyond the current benchmark-driven evaluation model, to a more
fine-grained understanding of deep RL algorithms.</p></div>
    </summary>
    <updated>2019-04-30T00:00:00Z</updated>
    <published>2019-04-30T00:00:00Z</published>
    <source>
      <id>http://gradientscience.org/</id>
      <author>
        <name>Gradient Science</name>
      </author>
      <link href="http://gradientscience.org/" rel="alternate" type="text/html"/>
      <link href="http://gradientscience.org/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Research highlights and perspectives on machine learning and optimization from MadryLab.</subtitle>
      <title>gradient science</title>
      <updated>2019-05-05T00:03:49Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-6025884855508893027</id>
    <link href="https://blog.computationalcomplexity.org/feeds/6025884855508893027/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/04/x-3-y-3-z-3-33-has-solution-in-z-and.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6025884855508893027" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/6025884855508893027" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/04/x-3-y-3-z-3-33-has-solution-in-z-and.html" rel="alternate" type="text/html"/>
    <title>x3   + y3 + z3 = 33  has a solution in Z. And its big!</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Consider the following problem:<br/>
<br/>
Given k, a natural number, determine if there exists x,y,z INTEGERS such that x<sup>3</sup>+y<sup>3</sup>+z<sup>3</sup>=k.<br/>
<br/>
It is not obvious that this problem is decidable (I think it is but have not been able to find an exact statement to that affect; however, if it was not solvable, I would know that, hence it is solvable. If you know a ref give it in the comments.)<br/>
<br/>
<br/>
If k≡ 4,5 mod 9  then mod arguments easily show there is no solution. <a href="https://arxiv.org/pdf/1604.07746.pdf">Huisman</a> showed that if k≤ 1000, k≡1,2,3,6,7,8 mod 9 and max(|x|,|y|,|z|) ≤ 10<sup>15</sup> and k is NOT one of<br/>
<br/>
33, 42, 114, 165, 390, 579, 627, 633, 732, 795, 906, 921, 975<br/>
<br/>
then there was a solution. For those on the list it was unknown.<br/>
<br/>
Recently <a href="https://people.maths.bris.ac.uk/~maarb/papers/cubesv1.pdf">Booker</a> (not Cory Booker, the candidate for prez, but Andrew Booker who I assume is a math-computer science person and is not running for prez) showed that<br/>
<br/>
x<sup>3</sup> + y<sup>3</sup> + z<sup>3</sup> =33<br/>
<br/>
DOES have a solution in INTEGERS. It is<br/>
<br/>
x= 8,866,128,975,287,528<br/>
<br/>
y=-8,778,405,442,862,239<br/>
<br/>
z=-2,736,111,468,807,040<br/>
<br/>
<br/>
does that make us more likely or less likely to think that<br/>
<br/>
x<sup>3</sup> + y<sup>3</sup> + z<sup>3</sup> =42<br/>
<br/>
has a solution? How about =114, etc, the others on the list?<br/>
<br/>
Rather than say what I think is true (I have no idea) here is what I HOPE is true: that the resolution of these problems leads to some mathematics of interest.<br/>
<br/>
<br/>
<br/>
<br/>
<br/></div>
    </content>
    <updated>2019-04-29T02:26:00Z</updated>
    <published>2019-04-29T02:26:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-05-05T12:03:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/063</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/063" rel="alternate" type="text/html"/>
    <title>TR19-063 |  Efficient Black-Box Identity Testing for Free Group Algebra | 

	Abhranil Chatterjee, 

	Vikraman Arvind, 

	Partha Mukhopadhyay, 

	Rajit Datta</title>
    <summary>Hrubeš and Wigderson [HW14] initiated the study of
  noncommutative arithmetic circuits with division computing a
  noncommutative rational function in the free skew field, and
  raised the question of rational identity testing. It is now known
  that the problem can be solved in deterministic polynomial time in
  the white-box model for noncommutative formulas with
  inverses, and in randomized polynomial time in the black-box
  model [GGOW16, IQS18, DM18], where the running time is
  polynomial in the size of the formula. 

  The complexity of identity testing of noncommutative rational
  functions remains open in general (when the formula size
  is not polynomially bounded). We solve the problem for a natural
  special case. We consider polynomial expressions in the free group
  algebra $\mathbb{F}\langle X, X^{-1}\rangle$ where $X=\{x_1, x_2, \ldots, x_n\}$, a
  subclass of rational expressions of inversion height one. Our main
  results are the following.

1. Given a degree $d$ expression $f$ in $\mathbb{F}\langle X, X^{-1}\rangle$ as a black-box, we obtain a randomized $\text{poly}(n,d)$ algorithm to check
  whether $f$ is an identically zero expression or not. We obtain this
  by generalizing the Amitsur-Levitzki theorem [AL50] to
  $\mathbb{F}\langle X, X^{-1}\rangle$. This also yields a deterministic identity testing algorithm (and even
  an expression reconstruction algorithm) that is polynomial time in
  the sparsity of the input expression.

2. Given an expression $f$ in $\mathbb{F}\langle X, X^{-1}\rangle$ of degree at most
  $D$, and sparsity $s$, as black-box, we can check whether $f$ is
  identically zero or not in randomized $\text{poly}(n,\log s, \log D)$
  time.</summary>
    <updated>2019-04-28T15:46:25Z</updated>
    <published>2019-04-28T15:46:25Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-05-05T14:20:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-events.org/2019/04/26/new-york-area-theory-day-spring-2019/</id>
    <link href="https://cstheory-events.org/2019/04/26/new-york-area-theory-day-spring-2019/" rel="alternate" type="text/html"/>
    <title>New York Area Theory Day (Spring 2019)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">May 10, 2019 Columbia University http://www.cs.columbia.edu/theory/s19-tday.html The New York Area Theory Day, co-organized by Columbia, IBM, and NYU, is a semi-annual conference aiming to bring together researchers in the New York Metropolitan area. It usually features a few hour long talks on recent advances in theoretical computer science. The speakers this time are Sepehr Assadi, … <a class="more-link" href="https://cstheory-events.org/2019/04/26/new-york-area-theory-day-spring-2019/">Continue reading <span class="screen-reader-text">New York Area Theory Day (Spring 2019)</span></a></div>
    </summary>
    <updated>2019-04-26T22:05:01Z</updated>
    <published>2019-04-26T22:05:01Z</published>
    <category term="other"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-events.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-events.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-events.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-events.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-events.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Aggregator for CS theory workshops, schools, and so on</subtitle>
      <title>CS Theory Events</title>
      <updated>2019-05-05T14:22:35Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4238</id>
    <link href="https://lucatrevisan.wordpress.com/2019/04/25/online-optimization-post-2-constructing-pseudorandom-sets/" rel="alternate" type="text/html"/>
    <title>Online Optimization Post 2: Constructing Pseudorandom Sets</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Today we will see how to use the analysis of the multiplicative weights algorithm in order to construct pseudorandom sets. The method will yield constructions that are optimal in terms of the size of the pseudorandom set, but not very … <a href="https://lucatrevisan.wordpress.com/2019/04/25/online-optimization-post-2-constructing-pseudorandom-sets/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
 Today we will see how to use the analysis of the multiplicative weights algorithm in order to construct pseudorandom sets. </p>
<p>
The method will yield constructions that are optimal in terms of the size of the pseudorandom set, but not very efficient, although there is at least one case (getting an “almost pairwise independent” pseudorandom generator) in which the method does something that I am not sure how to replicate with other techniques. </p>
<p>
Mostly, the point of this post is to illustrate a concept that will reoccur in more interesting contexts: that we can use an online optimization algorithm in order to construct a combinatorial object satisfying certain desired properties. The idea is to run a game between a “builder” against an “inspector,” in which the inspector runs the online optimization algorithm with the goal of finding a violated property in what the builder is building, and the builder plays the role of the adversary selecting the cost functions, with the advantage that it gets to build a piece of the construction after seeing what property the “inspector” is looking for. By the regret analysis of the online optimization problem, if the builder did well at each round against the inspector, then it will do well also against the “offline optimum” that looks for a violated property after seeing the whole construction. For example, the construction of graph sparsifiers by Allen-Zhu, Liao and Orecchia can be cast in this framework.</p>
<p>
(In some other applications, it will be the “builder” that runs the algorithm and the “inspector” who plays the role of the adversary. This will be the case of the Frieze-Kannan weak regularity lemma and of the Impagliazzo hard-core lemma. In those cases we capitalize on the fact that we know that there is a very good offline optimum, and we keep going for as long as the adversary is able to find violated properties in what the builder is constructing. After a sufficiently large number of rounds, the regret experienced by the algorithm would exceed the general regret bound, so the process must terminate in a small number of rounds. I have been told that this is just the “dual view” of what I described in the previous paragraph.)</p>
<p>
But, back the pseudorandom sets: if <img alt="{{\cal C} = \{ C_1,\ldots,C_N \}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7B%5Ccal+C%7D+%3D+%5C%7B+C_1%2C%5Cldots%2CC_N+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{\cal C} = \{ C_1,\ldots,C_N \}}"/> is a collection of boolean functions <img alt="{C_i : \{ 0,1 \}^n \rightarrow \{ 0,1 \}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_i+%3A+%5C%7B+0%2C1+%5C%7D%5En+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_i : \{ 0,1 \}^n \rightarrow \{ 0,1 \}}"/>, for example the functions computed by circuits of a certain type and a certain size, then a multiset <img alt="{S\subseteq \{ 0,1 \}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%5Csubseteq+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S\subseteq \{ 0,1 \}^n}"/> is <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>-pseudorandom for <img alt="{\cal C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\cal C}"/> if, for every <img alt="{C_i \in \cal C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_i+%5Cin+%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_i \in \cal C}"/>, we have </p>
<p align="center"><img alt="\displaystyle  | \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) =1] - \mathop{\mathbb P}_{s \sim S} [C_i(s) = 1 ] | \leq \epsilon " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7C+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D1%5D+-+%5Cmathop%7B%5Cmathbb+P%7D_%7Bs+%5Csim+S%7D+%5BC_i%28s%29+%3D+1+%5D+%7C+%5Cleq+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  | \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) =1] - \mathop{\mathbb P}_{s \sim S} [C_i(s) = 1 ] | \leq \epsilon "/></p>
<p> That is, sampling uniformly from <img alt="{S}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BS%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{S}"/>, which we can do with <img alt="{\log_2 |S|}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog_2+%7CS%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log_2 |S|}"/> random bits, is as good as sampling uniformly from <img alt="{\{ 0,1 \}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ 0,1 \}^n}"/>, which requires <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> bits, as far as the functions in <img alt="{\cal C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\cal C}"/> are concerned.</p>
<p>
It is easy to use Chernoff bounds and union bounds to argue that there is such a set of size <img alt="{O(N/\epsilon^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28N%2F%5Cepsilon%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(N/\epsilon^2)}"/>, so that we can sample from it using only <img alt="{\log N + 2\log \frac 1 \epsilon + O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog+N+%2B+2%5Clog+%5Cfrac+1+%5Cepsilon+%2B+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log N + 2\log \frac 1 \epsilon + O(1)}"/> random bits.</p>
<p>
We will prove this result (while also providing an “algorithm” for the construction) using multiplicative weights.</p>
<p>
<span id="more-4238"/></p>
<p>
First of all, possibly by changing <img alt="{N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BN%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{N}"/> to <img alt="{2N}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2N%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2N}"/>, we may assume that for every function <img alt="{C \in {\cal C}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC+%5Cin+%7B%5Ccal+C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C \in {\cal C}}"/> the function <img alt="{1-C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1-C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1-C}"/> is also in <img alt="{\cal C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\cal C}"/>. This simplifies things a bit because then the pseudorandom condition is equivalent to just</p>
<p/><p align="center"><img alt="\displaystyle  \forall C\in {\cal C} \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C (u) =1] - \mathop{\mathbb P}_{s \sim S} [C(s) = 1 ] \geq - \epsilon " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+C%5Cin+%7B%5Ccal+C%7D+%5C+%5C+%5C+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C+%28u%29+%3D1%5D+-+%5Cmathop%7B%5Cmathbb+P%7D_%7Bs+%5Csim+S%7D+%5BC%28s%29+%3D+1+%5D+%5Cgeq+-+%5Cepsilon+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall C\in {\cal C} \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C (u) =1] - \mathop{\mathbb P}_{s \sim S} [C(s) = 1 ] \geq - \epsilon "/></p>
<p>
We will make up an “experts” setup in which there is an expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> for each function <img alt="{C_i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BC_i%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{C_i}"/>. Thus, the algorithm, at each step, comes up with a probability distribution <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> over the functions, which we can think of as a “probabilistic function.” At time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>, the adversary chooses a string <img alt="{s_t \in \{ 0,1 \}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_t+%5Cin+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_t \in \{ 0,1 \}^n}"/> and defines the cost function </p>
<p align="center"><img alt="\displaystyle  f_t (x) := \sum_{i=1}^N x(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++f_t+%28x%29+%3A%3D+%5Csum_%7Bi%3D1%7D%5EN+x%28i%29+%5Ccdot+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s_t%29+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  f_t (x) := \sum_{i=1}^N x(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) "/></p>
<p> where the adversary chooses an <img alt="{s_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_t}"/> such that <img alt="{f_t(x_t) \geq 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x_t%29+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t(x_t) \geq 0}"/>. At this point, the reader should try, without reading ahead, to establish: </p>
<ol>
<li> That such a choice of <img alt="{s_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_t}"/> is always possible;
</li><li> That the cost function is of the form <img alt="{f_t(x) = \langle \ell_t , x\rangle}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t%28x%29+%3D+%5Clangle+%5Cell_t+%2C+x%5Crangle%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t(x) = \langle \ell_t , x\rangle}"/>, where the loss vector <img alt="{\ell_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_t}"/> satisfies <img alt="{|\ell_t (i) | \leq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C%5Cell_t+%28i%29+%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|\ell_t (i) | \leq 1}"/>, so that the regret after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps is <img alt="{\leq 2 \sqrt{T \ln N}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleq+2+%5Csqrt%7BT+%5Cln+N%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\leq 2 \sqrt{T \ln N}}"/>;
</li><li> That the sequence <img alt="{s_1,\ldots,s_T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_1%2C%5Cldots%2Cs_T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_1,\ldots,s_T}"/> of choices by the adversary determines a <img alt="{2\sqrt {\frac {\ln N}{T}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B2%5Csqrt+%7B%5Cfrac+%7B%5Cln+N%7D%7BT%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{2\sqrt {\frac {\ln N}{T}}}"/>-pseudorandom multiset for <img alt="{\cal C}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Ccal+C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\cal C}"/>, and, in particular, we get an <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>-pseudorandom multiset of cardinality <img alt="{4 \frac {\ln N}{\epsilon^2}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B4+%5Cfrac+%7B%5Cln+N%7D%7B%5Cepsilon%5E2%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{4 \frac {\ln N}{\epsilon^2}}"/>
</li></ol>
<p> For the first point, note that for a random <img alt="{s \sim \{ 0,1 \}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s \sim \{ 0,1 \}^n}"/> we have </p>
<p align="center"><img alt="\displaystyle  \mathop{\mathbb E}_{s\sim \{ 0,1 \}^n} \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s) \right) = 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmathop%7B%5Cmathbb+E%7D_%7Bs%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5Csum_%7Bi%3D1%7D%5EN+x_t%28i%29+%5Ccdot+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s%29+%5Cright%29+%3D+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \mathop{\mathbb E}_{s\sim \{ 0,1 \}^n} \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s) \right) = 0 "/></p>
<p> so there is an <img alt="{s_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_t}"/> such that </p>
<p align="center"><img alt="\displaystyle  \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bi%3D1%7D%5EN+x_t%28i%29+%5Ccdot+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s_t%29+%5Cright%29+%5Cgeq+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{i=1}^N x_t(i) \cdot \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq 0 "/></p>
<p> For the second point we just have to inspect the definition, and for the last point we have, by construction </p>
<p align="center"><img alt="\displaystyle  \sum_{t=1}^T f_t(x_t) \geq 0 " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+f_t%28x_t%29+%5Cgeq+0+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{t=1}^T f_t(x_t) \geq 0 "/></p>
<p> so the regret bound is </p>
<p align="center"><img alt="\displaystyle  \min_{x} \sum_{t=1}^T f_t(x) \geq - {\rm Regret}_T \geq - 2 \sqrt{T\ln n} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin_%7Bx%7D+%5Csum_%7Bt%3D1%7D%5ET+f_t%28x%29+%5Cgeq+-+%7B%5Crm+Regret%7D_T+%5Cgeq+-+2+%5Csqrt%7BT%5Cln+n%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \min_{x} \sum_{t=1}^T f_t(x) \geq - {\rm Regret}_T \geq - 2 \sqrt{T\ln n} "/></p>
<p> which, after dividing by <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/>, is </p>
<p align="center"><img alt="\displaystyle  \forall i : \ \ \ \frac 1T \sum_{t=1}^T \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq - 2 \sqrt{\frac {\ln n}{T}} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+i+%3A+%5C+%5C+%5C+%5Cfrac+1T+%5Csum_%7Bt%3D1%7D%5ET+%5Cleft%28+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+C_i+%28s_t%29+%5Cright%29+%5Cgeq+-+2+%5Csqrt%7B%5Cfrac+%7B%5Cln+n%7D%7BT%7D%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall i : \ \ \ \frac 1T \sum_{t=1}^T \left( \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - C_i (s_t) \right) \geq - 2 \sqrt{\frac {\ln n}{T}} "/></p>
<p align="center"><img alt="\displaystyle  \forall i: \ \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - \Pr_{s\in \{ s_1,\ldots,s_T \} } [C_i (s) = 1 ] \geq - 2 \sqrt{\frac {\ln n}{T}} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cforall+i%3A+%5C+%5C+%5C+%5C+%5Cmathop%7B%5Cmathbb+P%7D_%7Bu+%5Csim+%5C%7B+0%2C1+%5C%7D%5En%7D+%5B+C_i+%28u%29+%3D+1+%5D+-+%5CPr_%7Bs%5Cin+%5C%7B+s_1%2C%5Cldots%2Cs_T+%5C%7D+%7D+%5BC_i+%28s%29+%3D+1+%5D+%5Cgeq+-+2+%5Csqrt%7B%5Cfrac+%7B%5Cln+n%7D%7BT%7D%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \forall i: \ \ \ \ \mathop{\mathbb P}_{u \sim \{ 0,1 \}^n} [ C_i (u) = 1 ] - \Pr_{s\in \{ s_1,\ldots,s_T \} } [C_i (s) = 1 ] \geq - 2 \sqrt{\frac {\ln n}{T}} "/></p>
<p> Consider now the application of constructing a small-support distribution over <img alt="{\{ 0,1 \}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ 0,1 \}^n}"/> that is <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>-almost-pairwise-independent, meaning that if <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> is a random string sampled according to this distribution, then, for every <img alt="{i,j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%2Cj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i,j}"/>, the marginal <img alt="{(s_i,s_j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28s_i%2Cs_j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(s_i,s_j)}"/> is <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>-close to the uniform distribution over <img alt="{\{ 0,1 \}^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ 0,1 \}^2}"/> in total variation distance. This is the same thing as asking for a small-support distribution that is <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>-pseudorandom for all functions <img alt="{\{ 0,1 \}^n \rightarrow \{ 0,1 \}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+0%2C1+%5C%7D%5En+%5Crightarrow+%5C%7B+0%2C1+%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ 0,1 \}^n \rightarrow \{ 0,1 \}}"/> that depend on only two input variables. There are only <img alt="{O(n^2)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28n%5E2%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(n^2)}"/> such functions, so the above construction gives us a pseudorandom distribution that is uniform over a set of size <img alt="{O(\epsilon^{-2} \ln n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BO%28%5Cepsilon%5E%7B-2%7D+%5Cln+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{O(\epsilon^{-2} \ln n)}"/>, meaning that the distribution can be sampled using <img alt="{\log\log n + 2 \log \frac 1 \epsilon + O(1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Clog%5Clog+n+%2B+2+%5Clog+%5Cfrac+1+%5Cepsilon+%2B+O%281%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\log\log n + 2 \log \frac 1 \epsilon + O(1)}"/> random bits. Furthermore the algorithm can be implemented to run in time <img alt="{n^{O(1)} / \epsilon^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%5E%7BO%281%29%7D+%2F+%5Cepsilon%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n^{O(1)} / \epsilon^2}"/>. The only tricky step is how to find the string <img alt="{s_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_t}"/> at each step. For a string <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/>, the loss <img alt="{f (x_t)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf+%28x_t%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f (x_t)}"/> obtained by choosing <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/> as the “reference string” is a polynomial of degree 2 in the bits of <img alt="{s}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s}"/>, so we can find a no-worse-than-average <img alt="{s_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bs_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{s_t}"/> using the method of conditional expectations. I am not sure if there is a more standard way of doing this construction, perhaps one in which the bit <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/> of the <img alt="{k}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bk%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{k}"/>-th string in the sample space can be generated in time <img alt="{(\log n)^{O(1)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%28%5Clog+n%29%5E%7BO%281%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{(\log n)^{O(1)}}"/>. The standard approach is to combine a small-bias generator with a linear family of pairwise independent hash functions, but even using Ta-Shma’s construction of small-bias generators we would not get the correct dependency on <img alt="{\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon}"/>. This framework can “derandomize Chernoff bounds” in other settings as well, such as randomized rounding of packing and covering integer linear programs, and it is basically the same thing as the method of “pessimistic estimators” described in the Motwani-Raghavan book on randomized algorithms. </p>
<p/></div>
    </content>
    <updated>2019-04-26T01:00:06Z</updated>
    <published>2019-04-26T01:00:06Z</published>
    <category term="theory"/>
    <category term="multiplicative weights"/>
    <category term="online optimization"/>
    <category term="Pseudorandomness"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2019-05-05T14:20:08Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://tcsplus.wordpress.com/?p=351</id>
    <link href="https://tcsplus.wordpress.com/2019/04/25/tcs-talk-wednesday-may-1st-chris-peikert-university-of-michigan/" rel="alternate" type="text/html"/>
    <title>TCS+ talk: Wednesday, May 1st — Chris Peikert, University of Michigan</title>
    <summary>The next TCS+ talk will take place this coming Wednesday, May 1st at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). Chris Peikert from University of Michigan will speak about “Noninteractive Zero Knowledge for NP from Learning With Errors” (abstract below). Please make sure you reserve a spot for […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The next TCS+ talk will take place this coming Wednesday, May 1st at 1:00 PM Eastern Time (10:00 AM Pacific Time, 18:00 Central European Time, 17:00 UTC). <strong><a href="http://web.eecs.umich.edu/~cpeikert/">Chris Peikert</a></strong> from University of Michigan will speak about “<em>Noninteractive Zero Knowledge for NP from Learning With Errors</em>” (abstract below).</p>
<p>Please make sure you reserve a spot for your group to join us live by signing up on <a href="https://sites.google.com/site/plustcs/livetalk/live-seat-reservation">the online form</a>. As usual, for more information about the TCS+ online seminar series and the upcoming talks, or to <a href="https://sites.google.com/site/plustcs/suggest">suggest</a> a possible topic or speaker, please see <a href="https://sites.google.com/site/plustcs/">the website</a>.</p>
<blockquote><p>Abstract: We finally close the long-standing problem of constructing a noninteractive zero-knowledge (NIZK) proof system for any NP language with security based on the Learning With Errors (LWE) problem, and thereby on worst-case lattice problems. Our proof system instantiates a framework developed in a series of recent works for soundly applying the Fiat—Shamir transform using a hash function family that is <em>correlation intractable</em> for a suitable class of relations. Previously, such hash families were based either on “exotic” assumptions (e.g., indistinguishability obfuscation or optimal hardness of ad-hoc LWE variants) or, more recently, on the existence of circularly secure fully homomorphic encryption. However, none of these assumptions are known to be implied by LWE or worst-case hardness.</p>
<p>Our main technical contribution is a hash family that is correlation intractable for arbitrary size-<img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" title="S"/> circuits, for any polynomially bounded <img alt="S" class="latex" src="https://s0.wp.com/latex.php?latex=S&amp;bg=fff&amp;fg=444444&amp;s=0" title="S"/>, based on LWE (with small polynomial approximation factors). Our construction can be instantiated in two possible “modes,” yielding a NIZK that is either computationally sound and statistically zero knowledge in the common random string model, or vice-versa in the common reference string model.</p>
<p>(This is joint work with Sina Shiehian. Paper: <a href="https://eprint.iacr.org/2019/158" rel="noopener" target="_blank">https://eprint.iacr.org/2019/158</a>)</p></blockquote></div>
    </content>
    <updated>2019-04-25T20:01:09Z</updated>
    <published>2019-04-25T20:01:09Z</published>
    <category term="Announcements"/>
    <author>
      <name>plustcs</name>
    </author>
    <source>
      <id>https://tcsplus.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://tcsplus.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://tcsplus.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://tcsplus.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://tcsplus.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A carbon-free dissemination of ideas across the globe.</subtitle>
      <title>TCS+</title>
      <updated>2019-05-05T14:22:16Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://corner.mimuw.edu.pl/?p=1084</id>
    <link href="http://corner.mimuw.edu.pl/?p=1084" rel="alternate" type="text/html"/>
    <title>Call for Participation: HALG 2019 (Highlights of Algorithms)</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">------------------------------------------------------------------- 4rd Highlights of Algorithms conference (HALG 2019) Copenhagen, June 14-16, 2019 http://highlightsofalgorithms.org/ The Highlights of Algorithms conference is a forum for presenting the highlights of recent developments in algorithms and for discussing potential further advances in this area. The … <a href="http://corner.mimuw.edu.pl/?p=1084">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>-------------------------------------------------------------------<br/>
4rd Highlights of Algorithms conference (HALG 2019)<br/>
Copenhagen, June 14-16, 2019<br/>
<a href="http://highlightsofalgorithms.org/" rel="noopener noreferrer" target="_blank">http://highlightsofalgorithms.org/</a></p>
<p>The Highlights of Algorithms conference is a forum for presenting the<br/>
highlights of recent developments in algorithms and for discussing<br/>
potential further advances in this area. The conference will provide a<br/>
broad picture of the latest research in algorithms through a series of<br/>
invited talks, as well as the possibility for all researchers and<br/>
students to present their recent results through a series of short<br/>
talks and poster presentations. Attending the Highlights of Algorithms<br/>
conference will also be an opportunity for networking and meeting<br/>
leading researchers in algorithms.<br/>
-------------------------------------------------------------------</p>
<p>PROGRAM</p>
<p>The conference will begin on Friday, June 14, at 9:00 and end on<br/>
Sunday, June 16, at 18:00. A detailed schedule and a list of all<br/>
accepted short contributions can be found at:<br/>
<a href="http://2018.highlightsofalgorithms.org/programme" rel="noopener noreferrer" target="_blank">2018.highlightsofalgorithms.org/programme</a>.</p>
<p>-------------------------------------------------------------------</p>
<p>REGISTRATION</p>
<p>Please register on our webpage<br/>
     <a href="http://highlightsofalgorithms.org/registration" rel="noopener noreferrer" target="_blank">http://highlightsofalgorithms.org/registration</a><br/>
We have done our best to keep registration fees at a minimum:</p>
<p>Early registration (by April 29, 2019)<br/>
- academic rate (incl. postdocs): 160€<br/>
- student rate: 115€</p>
<p>Regular registration will be 50€ more expensive.</p>
<p>The organizers strongly recommend that you book your hotel as soon as possible.</p>
<p>-------------------------------------------------------------------</p>
<p>CONFERENCE VENUE</p>
<p>The conference will take place at the H.C. Ørsted Institute of the<br/>
University of Copenhagen.<br/>
The address is: Universitetsparken 5, DK-2100 Copenhagen.</p>
<p>-------------------------------------------------------------------</p>
<p>INVITED SPEAKERS</p>
<p>Survey speakers:<br/>
Monika Henzinger (University of Vienna)<br/>
Thomas Vidick (California Institute of Technology)<br/>
Laszlo Vegh (London School of Economics)<br/>
James Lee (University of Washington)<br/>
Timothy Chan (University of Illinois at Urbana-Champaign)<br/>
Sergei Vassilvitskii (Google, New York)</p>
<p>Invited talks:<br/>
Martin Grohe (RWTH Aachen University)<br/>
Josh Alman (MIT)<br/>
Nima Anari (Stanford University)<br/>
Michal Koucký (Charles University)<br/>
Naveen Garg (IIT Delhi)<br/>
Vera Traub (University of Bonn)<br/>
Rico Zenklusen (ETH Zurich)<br/>
Shayan Oveis Gharan (University of Washington)<br/>
Greg Bodwin (MIT)<br/>
Cliff Stein (Columbia University)<br/>
Sungjin Im (University of California at Merced)<br/>
C. Seshadhriy (University of California, Santa Cruz)<br/>
Shay Moran (Technion)<br/>
Bundit Laekhanukit (Shanghai University of Finance and Economics)<br/>
Sebastien Bubeck (Microsoft Research, Redmond)<br/>
Sushant Sachdeva (University of Toronto)<br/>
Kunal Talwar (Google Brain)<br/>
Moses Charikar (Stanford University)<br/>
Shuichi Hirahara (University of Tokyo)</p>
<p>------------------------------------------------------------------</p></div>
    </content>
    <updated>2019-04-25T12:36:21Z</updated>
    <published>2019-04-25T12:36:21Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>sank</name>
    </author>
    <source>
      <id>http://corner.mimuw.edu.pl</id>
      <link href="http://corner.mimuw.edu.pl/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="http://corner.mimuw.edu.pl" rel="alternate" type="text/html"/>
      <subtitle>University of Warsaw</subtitle>
      <title>Banach's Algorithmic Corner</title>
      <updated>2019-05-05T00:04:03Z</updated>
    </source>
  </entry>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-8078353386966881451</id>
    <link href="https://blog.computationalcomplexity.org/feeds/8078353386966881451/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/04/geo-centric-complexity.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8078353386966881451" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/8078353386966881451" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/04/geo-centric-complexity.html" rel="alternate" type="text/html"/>
    <title>Geo-Centric Complexity</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">An interesting discussion during Dagstuhl last month about the US-centric view of theory. Bad enough that all talks and papers in an international venue are in English but we also have<br/>
<ul>
<li><a href="https://en.wiktionary.org/wiki/Manhattan_distance">Manhattan Distance</a>. How are foreigners supposed to know about the structure of streets in New York? What's wrong with grid distance?</li>
<li><a href="https://en.wikipedia.org/wiki/Las_Vegas_algorithm">Las Vegas Algorithms</a>. I found this one a little unfair, after all Monte Carlo algorithms came first. Still today might not Macau algorithms make sense?</li>
<li><a href="https://en.wikipedia.org/wiki/Arthur%E2%80%93Merlin_protocol">Arthur-Merlin Games</a>. A British reference by a Hungarian living in the US (László Babai who also coined Las Vegas algorithms). Still the Chinese might not know the fables. Glad the Europeans don't remember the <a href="https://blog.computationalcomplexity.org/2017/04/alice-and-bob-and-pat-and-vanna.html">Pat and Vanna</a> terminology I used in my first STOC talk. </li>
<li>Alice and Bob. The famous pair of cryptographers but how generically American can you get. Why not Amare and Bhati?</li>
</ul>
<div>
I have two minds here. We shouldn't alienate or confuse those who didn't grow up in an Anglo-American culture. On the other hand, I hate to have to try and make all terminology culturally neutral, you'd just end up with technical and ugly names, like P and NP.</div></div>
    </content>
    <updated>2019-04-25T12:12:00Z</updated>
    <published>2019-04-25T12:12:00Z</published>
    <author>
      <name>Lance Fortnow</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/06752030912874378610</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-05-05T12:03:24Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=17389</id>
    <link href="https://gilkalai.wordpress.com/2019/04/25/are-natural-mathematical-problems-bad-problems/" rel="alternate" type="text/html"/>
    <title>Are Natural Mathematical Problems Bad Problems?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">One unique aspect of the conference “Visions in Mathematics Towards 2000” (see the previous post) was that there were several discussion sessions where speakers and other participants presented some thoughts about mathematics (or some specific areas), discussed and argued.  In … <a href="https://gilkalai.wordpress.com/2019/04/25/are-natural-mathematical-problems-bad-problems/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>One unique aspect of the conference “Visions in Mathematics Towards 2000” (see the <a href="https://gilkalai.wordpress.com/2019/04/23/an-invitation-to-a-conference-visions-in-mathematics-towards-2000/">previous post</a>) was that there were several discussion sessions where speakers and other participants presented some thoughts about mathematics (or some specific areas), discussed and argued.  In the lectures themselves you could also see a large amount of audience participation and discussions which was very nice.</p>
<p>Let me draw your attention to  one question raised and discussed in one of the discussion sessions.</p>
<h3><a href="https://youtu.be/Fme_r-nE4CI?t=1400">3.4 Discussion on Geometry with introduction by M. Gromov</a></h3>
<p/>
<p>Now, lets skip a lot of interesting staff and move <a href="https://youtu.be/Fme_r-nE4CI?t=1400">to minute 23:20</a> where Noga Alon asked Misha Gromov to elaborate a statement from his <a href="https://youtu.be/gd6EB2Zk6OE">opening lecture of the conference</a> that  the densest packing problem in <img alt="R^3" class="latex" src="https://s0.wp.com/latex.php?latex=R%5E3&amp;bg=ffffff&amp;fg=333333&amp;s=0" title="R^3"/> is not interesting.  In what follows Misha Gromov passionately argued that natural problems are bad problems (or are even stupid questions), and a lovely discussion emerged (in 25:00 Yuval Neeman commented about cosmology in response to Connes’s earlier remarks but then around 27:00 Vitali asked Misha to name some bad problems in geometry and the discussion resumed.) Misha made several lovely provocative further comments: he rejected the claim that this is a matter of taste, and argued that people make conjectures when they absolutely have no right to do so.</p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/misha-natural-bad.png"><img alt="" class="alignnone size-full wp-image-17390" height="361" src="https://gilkalai.files.wordpress.com/2019/04/misha-natural-bad.png?w=640&amp;h=361" width="640"/></a></p>
<p><strong><span style="color: #ff0000;"> Misha argues passionately that natural problems are stupid problems</span></strong></p>
<p>Actually one problem that Misha mentioned in his lecture as interesting (see also Gromov’s proceedings paper <a href="https://www.ihes.fr/~gromov/wp-content/uploads/2018/08/SpacesandQuestions.pdf">Spaces and questions),</a> and that was raised both by him and by me is to prove an exponential upper bound for the number of simplicial 3-spheres with n facets. I remember that we talked about it in the conference and Misha was certain that the problem could be solved for shellable spheres while I was confident that the case of shellable spheres would be as hard as the general case.  He was right! This goes back to works of physicists Durhuus and Jonsson see this paper <a href="https://arxiv.org/abs/0902.0436">On locally constructible spheres and balls</a> by Bruno Benedetti and  Günter M. Ziegler.</p>
<h5>(Disclaimer: I asked quite a few questions that were both unnatural and stupid and made several conjectures when I had no right to do so.)</h5>
<p><span id="more-17389"/></p>
<p>encore</p>
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/v1.png"><img alt="" class="alignnone size-medium wp-image-17401" height="188" src="https://gilkalai.files.wordpress.com/2019/04/v1.png?w=300&amp;h=188" width="300"/></a>  <a href="https://gilkalai.files.wordpress.com/2019/04/v2.png"><img alt="" class="alignnone size-medium wp-image-17402" height="210" src="https://gilkalai.files.wordpress.com/2019/04/v2.png?w=300&amp;h=210" width="300"/>  </a></p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/n1.png"><img alt="" class="alignnone size-medium wp-image-17403" height="227" src="https://gilkalai.files.wordpress.com/2019/04/n1.png?w=300&amp;h=227" width="300"/></a> <a href="https://gilkalai.files.wordpress.com/2019/04/n2.png"><img alt="" class="alignnone size-medium wp-image-17404" height="300" src="https://gilkalai.files.wordpress.com/2019/04/n2.png?w=214&amp;h=300" width="214"/></a></p>
<p><span style="color: #ff0000;">Vitali Milman attacked the solution of the 4CT as “bad”and Segei Novikov disagreed and referred to the proof as “great”.  </span></p>
<p> </p>
<p> </p></div>
    </content>
    <updated>2019-04-25T09:42:26Z</updated>
    <published>2019-04-25T09:42:26Z</published>
    <category term="Combinatorics"/>
    <category term="Conferences"/>
    <category term="Open discussion"/>
    <category term="What is Mathematics"/>
    <category term="Misha Gromov"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-05-05T14:20:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4236</id>
    <link href="https://lucatrevisan.wordpress.com/2019/04/24/online-optimization-post-1-multiplicative-weights/" rel="alternate" type="text/html"/>
    <title>Online Optimization Post 1: Multiplicative Weights</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">The multiplicative weights or hedge algorithm is the most well known and most frequently rediscovered algorithm in online optimization. The problem it solves is usually described in the following language: we want to design an algorithm that makes the best … <a href="https://lucatrevisan.wordpress.com/2019/04/24/online-optimization-post-1-multiplicative-weights/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
 The <em>multiplicative weights</em> or <em>hedge</em> algorithm is the most well known and most frequently rediscovered algorithm in online optimization. </p>
<p>
The problem it solves is usually described in the following language: we want to design an algorithm that makes the best possible use of the advice coming from <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> self-described experts. At each time step <img alt="{t=1,2,\ldots}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%2C2%2C%5Cldots%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t=1,2,\ldots}"/>, the algorithm has to decide with what probability to follow the advice of each of the experts, that is, the algorithm has to come up with a probability distribution <img alt="{x_t = (x_t(1),\ldots,x_t(n))}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t+%3D+%28x_t%281%29%2C%5Cldots%2Cx_t%28n%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t = (x_t(1),\ldots,x_t(n))}"/> where <img alt="{x_t (i) \geq 0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t+%28i%29+%5Cgeq+0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t (i) \geq 0}"/> and <img alt="{\sum_{i=1}^n x_t(i)=1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29%3D1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_{i=1}^n x_t(i)=1}"/>. After the algorithm makes this choice, it is revealed that following the advice of expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> leads to loss <img alt="{\ell_t (i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t+%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_t (i)}"/>, so that the expected loss of the algorithm at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> is <img alt="{\sum_{i=1}^n x_t(i) \ell_t (i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Cell_t+%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\sum_{i=1}^n x_t(i) \ell_t (i)}"/>. A loss can be negative, in which case its absolute value can be interpreted as a profit.</p>
<p>
After <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps, the algorithm “regrets” that it did not just always follow the advice of the expert that, with hindsight, was the best one, so that the regret of the algorithm after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps is </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T = \left( \sum_{t=1}^T\sum_{i=1}^n x_t(i) \ell_t(i) \right) - \left( \min_{i=1,\ldots,n} \ \ \sum_{t=1}^T \ell_t(i) \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%3D+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Cell_t%28i%29+%5Cright%29+-+%5Cleft%28+%5Cmin_%7Bi%3D1%2C%5Cldots%2Cn%7D+%5C+%5C+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28i%29+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T = \left( \sum_{t=1}^T\sum_{i=1}^n x_t(i) \ell_t(i) \right) - \left( \min_{i=1,\ldots,n} \ \ \sum_{t=1}^T \ell_t(i) \right) "/></p>
<p>
This corresponds to the instantiation of the framework we described in the previous post to the special case in which the set of feasible solutions <img alt="{K}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BK%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{K}"/> is the set <img alt="{\Delta \subseteq {\mathbb R}^n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CDelta+%5Csubseteq+%7B%5Cmathbb+R%7D%5En%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Delta \subseteq {\mathbb R}^n}"/> of probability distributions over the sample space <img alt="{\{ 1,\ldots,n\}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5C%7B+1%2C%5Cldots%2Cn%5C%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\{ 1,\ldots,n\}}"/> and in which the loss functions <img alt="{f_t (x)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t+%28x%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t (x)}"/> are linear functions of the form <img alt="{f_t (x) = \sum_i x(i) \ell_t (i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bf_t+%28x%29+%3D+%5Csum_i+x%28i%29+%5Cell_t+%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{f_t (x) = \sum_i x(i) \ell_t (i)}"/>. In order to bound the regret, we also have to bound the “magnitude” of the loss functions, so in the following we will assume that for all <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> and all <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> we have <img alt="{| \ell_t (i) | \leq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Cell_t+%28i%29+%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{| \ell_t (i) | \leq 1}"/>, and otherwise we can scale everything by a known upper bound on <img alt="{\max_{t,i} |\ell_t |}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cmax_%7Bt%2Ci%7D+%7C%5Cell_t+%7C%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\max_{t,i} |\ell_t |}"/>.</p>
<p>
We now describe the algorithm.</p>
<p>
The algorithm maintains at each step <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> a vector of <em>weights</em> <img alt="{w_t = (w_t(1),\ldots,w_t(n))}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_t+%3D+%28w_t%281%29%2C%5Cldots%2Cw_t%28n%29%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w_t = (w_t(1),\ldots,w_t(n))}"/> which is initialized as <img alt="{w_1 := (1,\ldots,1)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_1+%3A%3D+%281%2C%5Cldots%2C1%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w_1 := (1,\ldots,1)}"/>. The algorithm performs the following operations at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>: </p>
<ul>
<li> <img alt="{w_t (i) := w_{t-1} (i) \cdot e^{-\epsilon \ell_{t-1} (i) }}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bw_t+%28i%29+%3A%3D+w_%7Bt-1%7D+%28i%29+%5Ccdot+e%5E%7B-%5Cepsilon+%5Cell_%7Bt-1%7D+%28i%29+%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{w_t (i) := w_{t-1} (i) \cdot e^{-\epsilon \ell_{t-1} (i) }}"/>
</li><li> <img alt="{x_t (i) := \displaystyle \frac {w_t (i) }{\sum_{j=1}^n w_t(j) }}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t+%28i%29+%3A%3D+%5Cdisplaystyle+%5Cfrac+%7Bw_t+%28i%29+%7D%7B%5Csum_%7Bj%3D1%7D%5En+w_t%28j%29+%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t (i) := \displaystyle \frac {w_t (i) }{\sum_{j=1}^n w_t(j) }}"/>
</li></ul>
<p>
That is, the weight of expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> is <img alt="{e^{-\epsilon \sum_{k=1}^{t-1} \ell_k (i)}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%5E%7B-%5Cepsilon+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k+%28i%29%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e^{-\epsilon \sum_{k=1}^{t-1} \ell_k (i)}}"/>, and the probability <img alt="{x_t(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t(i)}"/> of following the advice of expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> is proportional to the weight. The parameter <img alt="{\epsilon&gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon&gt;0}"/> is hardwired into the algorithm and we will optimize it later. Note that the algorithm gives higher weight to experts that produced small losses (or negative losses of large absolute value) in the past, and thus puts higher probability on such experts.</p>
<p>
We will prove the following bound.</p>
<blockquote><p><b>Theorem 1</b> <em> Assuming that for all <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> and <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> we have <img alt="{| \ell_t(i) | \leq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7C+%5Cell_t%28i%29+%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{| \ell_t(i) | \leq 1}"/>, for every <img alt="{0 &lt; \epsilon &lt; 1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B0+%3C+%5Cepsilon+%3C+1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{0 &lt; \epsilon &lt; 1/2}"/>, after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps the multiplicative weight algorithm experiences a regret that is always bounded as </em></p><em>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n x_t(i) \ell^2 _t (i) + \frac {\ln n}{\epsilon} \leq \epsilon T + \frac {\ln n}{\epsilon} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Cell%5E2+_t+%28i%29+%2B+%5Cfrac+%7B%5Cln+n%7D%7B%5Cepsilon%7D+%5Cleq+%5Cepsilon+T+%2B+%5Cfrac+%7B%5Cln+n%7D%7B%5Cepsilon%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq \epsilon \sum_{t=1}^T \sum_{i=1}^n x_t(i) \ell^2 _t (i) + \frac {\ln n}{\epsilon} \leq \epsilon T + \frac {\ln n}{\epsilon} "/></p>
<p> In particular, if <img alt="{T &gt; 4 \ln n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT+%3E+4+%5Cln+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T &gt; 4 \ln n}"/>, by setting <img alt="{\epsilon = \sqrt{\frac{\ln n}{T}}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3D+%5Csqrt%7B%5Cfrac%7B%5Cln+n%7D%7BT%7D%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon = \sqrt{\frac{\ln n}{T}}}"/> we achieve a regret bound </p>
<p align="center"><img alt="\displaystyle  {\rm Regret}_T \leq 2 \sqrt{T \ln n} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%7B%5Crm+Regret%7D_T+%5Cleq+2+%5Csqrt%7BT+%5Cln+n%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  {\rm Regret}_T \leq 2 \sqrt{T \ln n} "/></p>
</em><p><em> </em></p></blockquote>
<p/><p>
<span id="more-4236"/></p>
<p>
We will start by giving a short proof of the above theorem. </p>
<p>
For each time step <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>, define the quantity</p>
<p/><p align="center"><img alt="\displaystyle  W_t := \sum_{i=1}^n w_t(i) \ . " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_t+%3A%3D+%5Csum_%7Bi%3D1%7D%5En+w_t%28i%29+%5C+.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  W_t := \sum_{i=1}^n w_t(i) \ . "/></p>
<p> We want to prove that, roughly speaking, the only way for an adversary to make the algorithm incur a large loss is to produce a sequence of loss functions such that <em>even the best expert incurs a large loss</em>. The proof will work by showing that if the algorithm incurs a large loss after <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> steps, then <img alt="{W_{T+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{W_{T+1}}"/> is small, and that if <img alt="{W_{T+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{W_{T+1}}"/> is small, then even the best expert incurs a large loss.</p>
<p>
Let us define </p>
<p align="center"><img alt="\displaystyle  L^* = \min_{i = 1,\ldots, n} \sum_{t=1}^T \ell_t (i) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++L%5E%2A+%3D+%5Cmin_%7Bi+%3D+1%2C%5Cldots%2C+n%7D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t+%28i%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  L^* = \min_{i = 1,\ldots, n} \sum_{t=1}^T \ell_t (i) "/></p>
<p> to be the loss of the best expert. Then we have</p>
<blockquote><p><b>Lemma 2 (If <img alt="{W_{T+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{W_{T+1}}"/> is small, then <img alt="{L^*}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%5E%2A%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L^*}"/> is large)</b> <em> </em></p><em>
<p align="center"><img alt="\displaystyle  W_{T+1} \geq e^{-\epsilon L^*} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7BT%2B1%7D+%5Cgeq+e%5E%7B-%5Cepsilon+L%5E%2A%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  W_{T+1} \geq e^{-\epsilon L^*} "/></p>
</em><p><em> </em></p></blockquote>
<p/><p>
<em>Proof:</em>  Let <img alt="{j}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bj%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{j}"/> be an index such that <img alt="{L^* = \sum_{t=1}^T \ell_t (j)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BL%5E%2A+%3D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t+%28j%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{L^* = \sum_{t=1}^T \ell_t (j)}"/>. Then we have </p>
<p align="center"><img alt="\displaystyle  W_{T+1} = \sum_{i=1}^n e^{-\epsilon \sum_{t=1}^T \ell_t(i) } \geq e^{-\epsilon \sum_{t=1}^T \ell_t(j)} = e^{-\epsilon L^*} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7BT%2B1%7D+%3D+%5Csum_%7Bi%3D1%7D%5En+e%5E%7B-%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28i%29+%7D+%5Cgeq+e%5E%7B-%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28j%29%7D+%3D+e%5E%7B-%5Cepsilon+L%5E%2A%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  W_{T+1} = \sum_{i=1}^n e^{-\epsilon \sum_{t=1}^T \ell_t(i) } \geq e^{-\epsilon \sum_{t=1}^T \ell_t(j)} = e^{-\epsilon L^*} "/></p>
<p> <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<blockquote><p><b>Lemma 3 (If the loss of the algorithm is large then <img alt="{W_{T+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BW_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{W_{T+1}}"/> is small)</b> <em> </em></p><em>
<p align="center"><img alt="\displaystyle  W_{T+1} \leq n \prod_{t=1}^n (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t , \ell^2_t \rangle) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7BT%2B1%7D+%5Cleq+n+%5Cprod_%7Bt%3D1%7D%5En+%281+-+%5Cepsilon+%5Clangle+x_t+%2C+%5Cell_t+%5Crangle+%2B+%5Cepsilon%5E2+%5Clangle+x_t+%2C+%5Cell%5E2_t+%5Crangle%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  W_{T+1} \leq n \prod_{t=1}^n (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t , \ell^2_t \rangle) "/></p>
</em><p><em> where <img alt="{\ell_t^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cell_t%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\ell_t^2}"/> is the vector whose <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/>-th coordinate is <img alt="{\left( \ell_t (i)\right)^2 }" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cleft%28+%5Cell_t+%28i%29%5Cright%29%5E2+%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\left( \ell_t (i)\right)^2 }"/> </em></p></blockquote>
<p/><p>
<em>Proof:</em>  Since we know that <img alt="{W_1 = n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BW_1+%3D+n%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{W_1 = n}"/>, it is enough to prove that, for every <img alt="{t=1,\ldots, T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%3D1%2C%5Cldots%2C+T%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t=1,\ldots, T}"/>, we have <a name="eq.lemma.two"/></p><a name="eq.lemma.two">
<p align="center"><img alt="\displaystyle  W_{t+1} \leq (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t, \ell_t^2 \rangle ) \cdot W_t  \ \ \ \ \ (1)" class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++W_%7Bt%2B1%7D+%5Cleq+%281+-+%5Cepsilon+%5Clangle+x_t+%2C+%5Cell_t+%5Crangle+%2B+%5Cepsilon%5E2+%5Clangle+x_t%2C+%5Cell_t%5E2+%5Crangle+%29+%5Ccdot+W_t++%5C+%5C+%5C+%5C+%5C+%281%29&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  W_{t+1} \leq (1 - \epsilon \langle x_t , \ell_t \rangle + \epsilon^2 \langle x_t, \ell_t^2 \rangle ) \cdot W_t  \ \ \ \ \ (1)"/></p>
</a><p><a name="eq.lemma.two"/> And we see that </p>
<p align="center"><img alt="\displaystyle  \frac{W_{t+1}}{W_t} = \sum_{i=1}^n \frac {w_{t+1}(i)}{W_t} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cfrac%7BW_%7Bt%2B1%7D%7D%7BW_t%7D+%3D+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac+%7Bw_%7Bt%2B1%7D%28i%29%7D%7BW_t%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \frac{W_{t+1}}{W_t} = \sum_{i=1}^n \frac {w_{t+1}(i)}{W_t} "/></p>
<p align="center"><img alt="\displaystyle  = \sum_{i=1}^n \frac {w_t(i) \cdot e^{-\epsilon \ell_t (i) } }{W_t} " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_%7Bi%3D1%7D%5En+%5Cfrac+%7Bw_t%28i%29+%5Ccdot+e%5E%7B-%5Cepsilon+%5Cell_t+%28i%29+%7D+%7D%7BW_t%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \sum_{i=1}^n \frac {w_t(i) \cdot e^{-\epsilon \ell_t (i) } }{W_t} "/></p>
<p align="center"><img alt="\displaystyle  = \sum_{i=1}^n x_t(i) \cdot e^{-\epsilon \ell_t(i) } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Ccdot+e%5E%7B-%5Cepsilon+%5Cell_t%28i%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = \sum_{i=1}^n x_t(i) \cdot e^{-\epsilon \ell_t(i) } "/></p>
<p align="center"><img alt="\displaystyle  \leq \sum_{i=1}^n x_t(i) \cdot ( 1 - \epsilon \ell_t (i) + \epsilon^2 \ell_t^2(i) ) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleq+%5Csum_%7Bi%3D1%7D%5En+x_t%28i%29+%5Ccdot+%28+1+-+%5Cepsilon+%5Cell_t+%28i%29+%2B+%5Cepsilon%5E2+%5Cell_t%5E2%28i%29+%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \leq \sum_{i=1}^n x_t(i) \cdot ( 1 - \epsilon \ell_t (i) + \epsilon^2 \ell_t^2(i) ) "/></p>
<p align="center"><img alt="\displaystyle  = 1 - \epsilon \langle x_t, \ell_t \rangle + \epsilon^2 \langle \ell_t^2 , x_t \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%3D+1+-+%5Cepsilon+%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%2B+%5Cepsilon%5E2+%5Clangle+%5Cell_t%5E2+%2C+x_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  = 1 - \epsilon \langle x_t, \ell_t \rangle + \epsilon^2 \langle \ell_t^2 , x_t \rangle "/></p>
<p> where we used the definitions of our quantities and the fact that <img alt="{e^{-z} \leq 1-z+z^2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Be%5E%7B-z%7D+%5Cleq+1-z%2Bz%5E2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{e^{-z} \leq 1-z+z^2}"/> for <img alt="{|z| \leq 1/2}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Cz%7C+%5Cleq+1%2F2%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|z| \leq 1/2}"/>. <img alt="\Box" class="latex" src="https://s0.wp.com/latex.php?latex=%5CBox&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\Box"/></p>
<p>
Using the fact that <img alt="{1-z \leq e^{-z}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1-z+%5Cleq+e%5E%7B-z%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1-z \leq e^{-z}}"/> for all <img alt="{|z| \leq 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Cz%7C+%5Cleq+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{|z| \leq 1}"/>, the above lemmas can be restated as </p>
<p align="center"><img alt="\displaystyle  \ln W_{T+1} \leq \ln n - \left(\sum_{t=1}^T \epsilon \langle \ell_t , x_t \rangle \right) + \left( \sum_{t=1}^T\epsilon^2 \langle \ell_t^2 x_t\rangle \right) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cln+W_%7BT%2B1%7D+%5Cleq+%5Cln+n+-+%5Cleft%28%5Csum_%7Bt%3D1%7D%5ET+%5Cepsilon+%5Clangle+%5Cell_t+%2C+x_t+%5Crangle+%5Cright%29+%2B+%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET%5Cepsilon%5E2+%5Clangle+%5Cell_t%5E2+x_t%5Crangle+%5Cright%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \ln W_{T+1} \leq \ln n - \left(\sum_{t=1}^T \epsilon \langle \ell_t , x_t \rangle \right) + \left( \sum_{t=1}^T\epsilon^2 \langle \ell_t^2 x_t\rangle \right) "/></p>
<p> and </p>
<p align="center"><img alt="\displaystyle  \ln W_{T+1} \geq - \epsilon L^* " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cln+W_%7BT%2B1%7D+%5Cgeq+-+%5Cepsilon+L%5E%2A+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \ln W_{T+1} \geq - \epsilon L^* "/></p>
<p> which together imply </p>
<p align="center"><img alt="\displaystyle  \left( \sum_{t=1}^T \langle \ell_t , x_t \rangle \right) - L^* \leq \frac{\ln n}{\epsilon} + \epsilon \sum_{t=1}^T \langle \ell^2_t , x_t \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+%5Cell_t+%2C+x_t+%5Crangle+%5Cright%29+-+L%5E%2A+%5Cleq+%5Cfrac%7B%5Cln+n%7D%7B%5Cepsilon%7D+%2B+%5Cepsilon+%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+%5Cell%5E2_t+%2C+x_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \left( \sum_{t=1}^T \langle \ell_t , x_t \rangle \right) - L^* \leq \frac{\ln n}{\epsilon} + \epsilon \sum_{t=1}^T \langle \ell^2_t , x_t \rangle "/></p>
<p> as desired.</p>
<p>
Personally, I find all of the above very unsatisfactory, because both the algorithm and the analysis, but especially the analysis, seem to come out of nowhere. In fact, I never felt that I actually understood this analysis until I saw it presented as a special case of the <em>Follow The Regularized Leader</em> framework that we will discuss in a future post. (We will actually prove a slightly weaker bound, but with a much more satisfying proof.)</p>
<p>
Here is, however, a story of how a statistical physicist might have invented the algorithm and might have come up with the analysis. Let’s call the loss caused by expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> after <img alt="{t-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t-1}"/> steps the <em>energy</em> of expert <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>: </p>
<p align="center"><img alt="\displaystyle  E_t(i) = \sum_{k=1}^{t-1} \ell_k(i) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++E_t%28i%29+%3D+%5Csum_%7Bk%3D1%7D%5E%7Bt-1%7D+%5Cell_k%28i%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  E_t(i) = \sum_{k=1}^{t-1} \ell_k(i) "/></p>
<p> Note that we have defined it in such a way that the algorithm knows <img alt="{E_t(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_t(i)}"/> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>. Our offline optimum is the energy of the lowest energy expert at time <img alt="{T+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T+1}"/>, that, is, the energy of the <em>ground state</em> at time <img alt="{T+1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%2B1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T+1}"/>. When we have a collection of numbers <img alt="{E_t(1),\ldots, E_t(n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_t%281%29%2C%5Cldots%2C+E_t%28n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_t(1),\ldots, E_t(n)}"/>, a nice lower bound to their minimum is </p>
<p align="center"><img alt="\displaystyle  \min_i E_t(i) \geq - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cmin_i+E_t%28i%29+%5Cgeq+-+%5Cfrac+1+%5Cepsilon+%5Cln+%5Csum_%7Bi%3D1%7D%5En+e%5E%7B-%5Cepsilon+E_t%28i%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \min_i E_t(i) \geq - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } "/></p>
<p> which is true for every <img alt="{\epsilon &gt;0}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cepsilon+%3E0%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\epsilon &gt;0}"/>. The right-hand side above is the <em>free energy</em> at temperature <img alt="{\frac 1 \epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cfrac+1+%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\frac 1 \epsilon}"/> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>. This seems like the kind of expression that we could use to bound the offline optimum, so let’s give it a name </p>
<p align="center"><img alt="\displaystyle  \Phi_t := - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPhi_t+%3A%3D+-+%5Cfrac+1+%5Cepsilon+%5Cln+%5Csum_%7Bi%3D1%7D%5En+e%5E%7B-%5Cepsilon+E_t%28i%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \Phi_t := - \frac 1 \epsilon \ln \sum_{i=1}^n e^{-\epsilon E_t(i) } "/></p>
<p> In terms of coming up with an algorithm, all that we have got to work with at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> are the losses of the experts at times <img alt="{1,\ldots,t-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2C%5Cldots%2Ct-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1,\ldots,t-1}"/>. If the adversary chooses to make one of the experts consistently much better than the others, it is clear that, in order to get any reasonable regret bound, the algorithm will have to put much of the probability mass in most of the steps on that expert. This suggests that the <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> should put higher probability on experts that have done well in the first <img alt="{t-1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt-1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t-1}"/> steps, that is <img alt="{x_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x_t}"/> should put higher probability on “lower-energy” experts. When we have a system in which, at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/>, state <img alt="{i}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bi%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{i}"/> has energy <img alt="{E_t(i)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BE_t%28i%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{E_t(i)}"/>, a standard distribution that puts higher probability on lower energy states is the <em>Gibbs distribution</em> at temperature <img alt="{1/\epsilon}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B1%2F%5Cepsilon%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{1/\epsilon}"/>, defined as </p>
<p align="center"><img alt="\displaystyle  x_t(i) = \frac { e^{-\epsilon E_t (i)} }{\sum_j e^{-\epsilon E_t(j) } } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++x_t%28i%29+%3D+%5Cfrac+%7B+e%5E%7B-%5Cepsilon+E_t+%28i%29%7D+%7D%7B%5Csum_j+e%5E%7B-%5Cepsilon+E_t%28j%29+%7D+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  x_t(i) = \frac { e^{-\epsilon E_t (i)} }{\sum_j e^{-\epsilon E_t(j) } } "/></p>
<p> where the denominator above is also called the <em>partition function</em> at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> </p>
<p align="center"><img alt="\displaystyle  Z_t := \sum_{j=1}^n e^{-\epsilon E_t(j) } " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++Z_t+%3A%3D+%5Csum_%7Bj%3D1%7D%5En+e%5E%7B-%5Cepsilon+E_t%28j%29+%7D+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  Z_t := \sum_{j=1}^n e^{-\epsilon E_t(j) } "/></p>
<p> So far we have “rediscovered” our multiplicative weights algorithm, and the quantity <img alt="{W_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BW_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{W_t}"/> that we had in our analysis gets interpreted as the partition function <img alt="{Z_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BZ_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{Z_t}"/>. The fact that <img alt="{\Phi_{T+1}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CPhi_%7BT%2B1%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Phi_{T+1}}"/> bounds the offline optimum suggests that we should use <img alt="{\Phi_t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CPhi_t%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\Phi_t}"/> as a potential function, and aim for an analysis involving a telescoping sum. Indeed some manipulations (the same as in the short proof above, but which are now more mechanical) give that the loss of the algorithm at time <img alt="{t}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bt%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{t}"/> is </p>
<p align="center"><img alt="\displaystyle  \langle x_t, \ell_t \rangle \leq \Phi_{t+1} - \Phi_{t} + \langle x_t , \ell^2 _t \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%5Cleq+%5CPhi_%7Bt%2B1%7D+-+%5CPhi_%7Bt%7D+%2B+%5Clangle+x_t+%2C+%5Cell%5E2+_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \langle x_t, \ell_t \rangle \leq \Phi_{t+1} - \Phi_{t} + \langle x_t , \ell^2 _t \rangle "/></p>
<p> which telescopes to give </p>
<p align="center"><img alt="\displaystyle  \sum_{t=1}^T \langle x_t, \ell_t \rangle \leq \Phi_{T+1} - \Phi_1 + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%5Cleq+%5CPhi_%7BT%2B1%7D+-+%5CPhi_1+%2B+%5Csum_%7Bt%3D1%7D%5ET%5Clangle+x_t+%2C+%5Cell%5E2+_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \sum_{t=1}^T \langle x_t, \ell_t \rangle \leq \Phi_{T+1} - \Phi_1 + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle "/></p>
<p> Recalling that </p>
<p align="center"><img alt="\displaystyle  \Phi_1 = - \frac 1 {\epsilon} \ln n " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPhi_1+%3D+-+%5Cfrac+1+%7B%5Cepsilon%7D+%5Cln+n+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \Phi_1 = - \frac 1 {\epsilon} \ln n "/></p>
<p> and </p>
<p align="center"><img alt="\displaystyle  \Phi_{T+1} \leq \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5CPhi_%7BT%2B1%7D+%5Cleq+%5Cmin_%7Bj%3D1%2C%5Cldots%2C+n%7D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28j%29+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \Phi_{T+1} \leq \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) "/></p>
<p> we have again </p>
<p align="center"><img alt="\displaystyle  \left( \sum_{t=1}^T \langle x_t, \ell_t \rangle \right) - \left( \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) \right) \leq \frac{\ln n}{\epsilon} + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++%5Cleft%28+%5Csum_%7Bt%3D1%7D%5ET+%5Clangle+x_t%2C+%5Cell_t+%5Crangle+%5Cright%29+-+%5Cleft%28+%5Cmin_%7Bj%3D1%2C%5Cldots%2C+n%7D+%5Csum_%7Bt%3D1%7D%5ET+%5Cell_t%28j%29+%5Cright%29+%5Cleq+%5Cfrac%7B%5Cln+n%7D%7B%5Cepsilon%7D+%2B+%5Csum_%7Bt%3D1%7D%5ET%5Clangle+x_t+%2C+%5Cell%5E2+_t+%5Crangle+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  \left( \sum_{t=1}^T \langle x_t, \ell_t \rangle \right) - \left( \min_{j=1,\ldots, n} \sum_{t=1}^T \ell_t(j) \right) \leq \frac{\ln n}{\epsilon} + \sum_{t=1}^T\langle x_t , \ell^2 _t \rangle "/></p>
<p> As mentioned above, we will give a better story when we get to the <em>Follow The Regularized Leader</em> framework. In the next post, we will discuss complexity-theory consequences of the result we just proved. </p></div>
    </content>
    <updated>2019-04-25T06:44:54Z</updated>
    <published>2019-04-25T06:44:54Z</published>
    <category term="theory"/>
    <category term="multiplicative weights"/>
    <category term="online optimization"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2019-05-05T14:20:08Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=15798</id>
    <link href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/" rel="alternate" type="text/html"/>
    <title>Why Check A Proof?</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Why check another’s proof? [Russell and Whitehead ] Bertrand Russell and Alfred Whitehead were not primarily trying to mechanize mathematics in writing their famous book. They wanted to assure precision and certainty in proofs while minimizing the axioms and rules they rest on. They cared more about checking proofs than generating theorems. By the way: … … <a href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>
<font color="#0044cc"><br/>
<em>Why check another’s proof?</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/russellwhitehead1900s/" rel="attachment wp-att-15809"><img alt="" class="alignright size-medium wp-image-15809" height="203" src="https://rjlipton.files.wordpress.com/2019/04/russellwhitehead1900s.png?w=300&amp;h=203" width="300"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">[Russell and Whitehead ]</font></td>
</tr>
</tbody>
</table>
<p>
Bertrand Russell and Alfred Whitehead were not primarily trying to mechanize mathematics in writing their famous book. They wanted to assure precision and certainty in proofs while minimizing the axioms and rules they rest on. They cared more about checking proofs than generating theorems. By the way: They are listed in the order Whitehead and Russell on the book. See <a href="https://thonyc.wordpress.com/2016/05/19/bertrand-russell-did-not-write-principia-mathematica/">this</a> for a discussion about the importance of the order.</p>
<p><a href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/unknown-120/" rel="attachment wp-att-15804"><img alt="" class="aligncenter size-full wp-image-15804" src="https://rjlipton.files.wordpress.com/2019/04/unknown-1.jpeg?w=584"/></a></p>
<p>
Today Ken and I thought we would add a few more thoughts on why proofs get checked.<br/>
<span id="more-15798"/></p>
<p>
We discussed those who <em>claim</em> proofs in our previous <a href="https://rjlipton.wordpress.com/2019/04/21/pnp-proofs/">post</a>. Once a proof is claimed, it needs people to check it. This is not as fraught as the <a href="https://en.wikipedia.org/wiki/Replication_crisis">replication crisis</a> in other sciences where “proof” is a statement of statistical significance whose most intensive check needs repeating the experiment. </p>
<p>
If you do a Google search on “why check proofs” you get lots of hits on using automated proof checkers. Coming on eleven decades after the publication of Russell and Whitehead’s three-volume <a href="https://en.wikipedia.org/wiki/Principia_Mathematica">opus</a> <em>Principia Mathematica</em>, these are still in their formative years. We <a href="https://rjlipton.wordpress.com/2013/07/14/surely-you-are-joking/">covered</a> a major system of this kind some years ago. </p>
<p>
We are personally more interested in what motivates us <em>humans</em> to check proofs. We believe that there are various factors that make it less or more likely to find a good human checker. So today we will try to list some of them. </p>
<p>
</p><p/><h2> Why Check A Proof? </h2><p/>
<p/><p>
One of the questions that was raised by some commenters to our recent post is: <i>Why should I check your proof?</i></p>
<p>
This is a critical question. If their is no reason to check your proof, then your result will not get checked. It is almost a tautology. We like this question and thought we could suggest several ways to increase the likelihood that one will check another person’s proof. </p>
<p>
So lets assume that Alice is claiming some new theorem <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> and we ponder whether Bob will spend time checking it.</p>
<p>
</p><p/><h3> Bob has to </h3><p/>
<p>This happens when Bob is required to check her proof. This can happen if Bob is a referee of her paper. It could also be when Bob is hired to do this task. It usually is a weak reason for making someone do the checking. In real life we think that it is unlikely to be a strong motivator.</p>
<p>
</p><p/><h3> Bob wants to </h3><p/>
<p>This happens when Bob feels that he will benefit from checking. The main type of situation here is: Alice’s theorem <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> uses some new method or trick. If Bob believes that this method can be used in his work, in his research, in his future papers, then he is strongly motivated.</p>
<p>
We are all very self-centered in our research. If we think we could in the future use your method we are likely to spent time and energy on your proof. Thus if Bob is convinced that Alice has a some new ideas, he is much more likely to spent the time checking her theorem. This means that Alice should—if possible–explain that her proof of <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> uses something new. Proofs that are “just technical inductions” are very unlikely to get Bob to read them. In many areas some authors have stated things like: <i>The proof is a careful induction…</i> This is not a good idea. </p>
<p>
</p><p/><h3> Bob needs to </h3><p/>
<p>This happens when Bob has some “skin” in the game. A classic situation is when Bob has an earlier result that is affected by Alice’s new theorem. If <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> is stronger than Bob’s previous result, then he is motivated to check her theorem. Or if <img alt="{T}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BT%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{T}"/> shows that his earlier theorem is false, this is a very strong motivation. Or perhaps Alice has proven a lemma that enables Bob to push something through.</p>
<p>
</p><p/><h2> Skin in the Game </h2><p/>
<p/><p>
Often we have situations where you do have skin in the game. An old <a href="https://rjlipton.wordpress.com/2009/09/27/surprises-in-mathematics-and-theory/">example</a> that comes to mind is from group theory. The problem is a natural question about a class of groups: Let <img alt="{B(m,n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%28m%2Cn%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B(m,n)}"/> be the class of groups that are generated by <img alt="{m}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bm%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{m}"/> elements and all elements in the group satisfy, <img alt="{x^{n} = 1}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bx%5E%7Bn%7D+%3D+1%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{x^{n} = 1}"/>. Sergei Adian and Pyotr Novikov proved that <img alt="{B(m, n)}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BB%28m%2C+n%29%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{B(m, n)}"/> is infinite for <img alt="{n}" class="latex" src="https://s0.wp.com/latex.php?latex=%7Bn%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{n}"/> odd, <img alt="{{n \ge 4381}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%7Bn+%5Cge+4381%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{{n \ge 4381}}"/> by a long complex combinatorial proof in 1968. This is a famous result. </p>
<p>
Shortly after another group theorist, John Britton, claimed an alternative proof in 1970. Unfortunately, Adian later discovered that Britton’s proof was wrong. I do not have first-hand information, but I was told that Adian was motivated by wanting to have <i>the</i> proof. He worked hard until he discovered an unrepairable bug in Britton’s 300-page monograph. The proof was unsalvageable.</p>
<p>
<a href="https://rjlipton.wordpress.com/2019/04/24/why-check-a-proof/yau/" rel="attachment wp-att-15802"><img alt="" class="aligncenter size-medium wp-image-15802" height="300" src="https://rjlipton.files.wordpress.com/2019/04/yau.jpg?w=199&amp;h=300" width="199"/></a></p>
<p>
A much newer example is from a recent book by Shing-Tung Yau, <a href="https://yalebooks.yale.edu/book/9780300235906/shape-life">The Shape of a Life</a>. He is a famous geometry expert and has made many important contributions to many areas of mathematics. We will probably discuss his book in detail in the future, but for today it has a neat example of “skin in the game”. He writes about an enumeration problem of counting how many curves lie on a certain manifold—a century old problem. One group used a clever trick to get the number 	</p>
<p align="center"><img alt="\displaystyle  317,206,375. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++317%2C206%2C375.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  317,206,375. "/></p>
<p>However another group discovered via a different method that the count was 	</p>
<p align="center"><img alt="\displaystyle  2,682,549,425. " class="latex" src="https://s0.wp.com/latex.php?latex=%5Cdisplaystyle++2%2C682%2C549%2C425.+&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="\displaystyle  2,682,549,425. "/></p>
<p>Somewhat a different count—not even close. Clearly, both sets of authors were heavily motivated to check their work. And within a month the larger count was found to be wrong and the first was correct.</p>
<p>
</p><p/><h2> A<del datetime="2019-04-24T13:55:30-04:00">n</del> <del datetime="2019-04-24T13:55:11-04:00">Un</del>resolved Claim </h2><p/>
<p/><p>
This is from the wonderful P vs NP <a href="https://www.win.tue.nl/~gwoegi/P-versus-NP.htm">pages</a> of Gerhard Woeginger. It was pointed out to us by the commenter <i>gentzen</i>. Quoting Woeginger’s page, including its use of “showed”:</p>
<blockquote><p><b> </b> <em> In February 2016, Mathias Hauptmann showed that P is not equal to NP. Hauptmann starts from the assumption that P equals <img alt="{\Sigma_{2}^{p}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CSigma_%7B2%7D%5E%7Bp%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\Sigma_{2}^{p}}"/>, proves a new variant of the Union Theorem of McCreight and Meyer for <img alt="{\Sigma_{2}^{p}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5CSigma_%7B2%7D%5E%7Bp%7D%7D&amp;bg=e8e8e8&amp;fg=000000&amp;s=0" title="{\Sigma_{2}^{p}}"/>, and eventually derives a contradiction. This implies P not equal to NP. </em>
</p></blockquote>
<p/><p>
Woeginger gives a link to Hauptmann’s <a href="http://arxiv.org/abs/1602.04781">paper</a>, “On Alternation and the Union Theorem,” and thanks two people who communicated this to him. </p>
<p>
The union <a href="https://people.csail.mit.edu/meyer/meyer-mccreight.pdf">theorem</a> of Albert Meyer and Edward McCreight is the classic theorem that shows how to encode many complexity classes into one. Hauptmann’s idea is not unreasonable. He makes an assumption that P=NP and tries to use it to improve the union theorem. This is a nice idea: Make a strong assumption and then try to improve a deep result. The hope is that this will lead to a contradiction. His abstract ends by saying, “Hence the assumption <img alt="{P = \Sigma_{2}^{p}}" class="latex" src="https://s0.wp.com/latex.php?latex=%7BP+%3D+%5CSigma_%7B2%7D%5E%7Bp%7D%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{P = \Sigma_{2}^{p}}"/> cannot hold.” We do not know if this paper has received a thorough reading. <b>Update:</b> We have learned that a pair of experts reviewed the argument and found that part of it implied a contradiction to the deterministic time hierarchy theorem, while another part relativizes in a way that would yield a false statement under certain oracles.</p>
<p>
</p><p/><h2> A Resolved Claim </h2><p/>
<p/><p>
Hauptmann is a colleague of Norbert Blum at the University of Bonn. Two years ago, Blum claimed to prove P <img alt="{\neq}" class="latex" src="https://s0.wp.com/latex.php?latex=%7B%5Cneq%7D&amp;bg=ffffff&amp;fg=000000&amp;s=0" title="{\neq}"/> NP by making technical improvements on a well-known circuit-based attack from the 1980s and 1990s. He has had a long track record of expertise and reliability in this area and his <a href="http://arxiv.org/trackback/1708.03486">paper</a> was read right away. </p>
<p>
The reading was helped by his paper being well-organized, straightforward, and relatively short—the crucial segment was under ten pages. The news broke while we were preparing a post on the August 2017 total solar eclipse in the US. In the 24–48 hours it took us to modify our <a href="https://rjlipton.wordpress.com/2017/08/17/on-the-edge-of-eclipses-and-pnp/">post</a>, we were already able to draw on several accounts by first-responder readers and check those accounts ourselves against the paper. </p>
<p>
The error was triangulated in an interesting way. It was first observed that if Blum’s attack could succeed by the means and premises stated, then it would extend to prove something else that is known not to be true. Once this was ascertained, a closer reading was able to zero in on the exact technical point of error. Blum soon acknowledged this and that the breach was unfixable. The attempt still combines circuit theory and graph theory in ways a student can benefit from learning about, and this furnished its own incentive to read it.</p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
We appreciate the comments on the previous post and hope this adds some additional insights.</p>
<p>
[added update about Hauptmann’s paper]</p></font></font></div>
    </content>
    <updated>2019-04-24T14:32:25Z</updated>
    <published>2019-04-24T14:32:25Z</published>
    <category term="History"/>
    <category term="Ideas"/>
    <category term="Oldies"/>
    <category term="Open Problems"/>
    <category term="P=NP"/>
    <category term="People"/>
    <category term="Proofs"/>
    <category term="check proofs"/>
    <category term="conjecture"/>
    <category term="motivation"/>
    <category term="P&#x2260;NP"/>
    <category term="skin in game"/>
    <author>
      <name>rjlipton</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-05-05T14:21:02Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://11011110.github.io/blog/2019/04/23/euler-characteristics-nonmanifold</id>
    <link href="https://11011110.github.io/blog/2019/04/23/euler-characteristics-nonmanifold.html" rel="alternate" type="text/html"/>
    <title>Euler characteristics of non-manifold polycubes</title>
    <summary>From a block of cubes, remove two non-adjacent and non-opposite cubes. The resulting polycube has a boundary that is not a manifold: between the two removed cubes, there is an edge shared by four squares, but a two-dimensional manifold can only have two faces per edge. Nevertheless, we can compute its Euler characteristic as the number of vertices () minus the number of edges () plus the number of square faces (). , the same number we would expect for the Euler characteristic of a topological sphere! What does it mean?</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>From a  block of cubes, remove two non-adjacent and non-opposite cubes. The resulting polycube has a boundary that is not a <a href="https://en.wikipedia.org/wiki/Manifold">manifold</a>: between the two removed cubes, there is an edge shared by four squares, but a two-dimensional manifold can only have two faces per edge. Nevertheless, we can compute its Euler characteristic as the number of vertices () minus the number of edges () plus the number of square faces (). , the same number we would expect for the Euler characteristic of a topological sphere! What does it mean?</p>

<p style="text-align: center;"><img alt="Removing two non-adjacent and non-opposite cubes from a 2x2 block of cubes" src="https://11011110.github.io/blog/assets/2019/nonmanifold-polycube.svg"/></p>

<p>Any finite union of cubes of the integer lattice (not even necessarily connected) has as its boundary a set of vertices, edges, and squares, with each edge incident to an even number of squares. We can define the Euler characteristic to be the number of vertices minus edges plus squares, in the usual way. But we can also compute it in a different, more intrinsic and topological, way. For any  in the range , define the “shrunken interior” of the polycube to be the set of points of the interior farther than  from the boundary, and define the “shrunken exterior” in the same way. Then the shrunken interior and shrunken exterior both have (possibly disconnected) 2-manifolds as boundaries. We can define their Euler characteristics in the standard way from any cell decomposition of these boundaries (it doesn’t matter which cell decomposition we choose). Then the Euler characteristic of the polycube is the average of the Euler characteristics of the shrunken interior and shrunken exterior!</p>

<p>In the case of the mutilated  block, the shrunken interior and shrunken exterior are both topological balls (ignoring the puncture at infinity as it doesn’t have a boundary), so the average of their Euler characteristics is the Euler characteristic of a sphere, as we calculated.</p>

<p>There’s probably a simpler and more conceptual way of doing it, but here’s an explanation for why the Euler characteristic of the polycube boundary is the average of the Euler characteristics of the interior and exterior. Form a cell complex on the boundary of the interior and exterior, together, in the following way: expand each square of the polycube boundary to a cuboid with thickness 0.1, expand each edge into a cylinder with diameter 0.2 (big enough to enclose all the intersections of two expanded squares), and expand each vertex into a sphere with diameter 0.3 (big enough to enclose all the intersections of two cylinders but small enough that no two of these spheres touch). Remove the union of these expanded shapes from the space, and consider what’s left. It has the same topology as the union of the shrunken interior and exterior, and its boundary is now naturally divided up into cells: offset squares patches on the sides of each expanded square face, cylindrical patches on each expanded edge, and spherical patches on each expanded vertex, with curves where two patches meet.</p>

<p>Let’s calculate the Euler characteristic of this cell complex. Each square of the polycube leads to two offset square patches, so the  squares contribute   to the Euler characteristic. Each edge  of the polycube might be adjacent to two or four squares; call this number . Then the cylinder around  includes  surface patches between pairs of squares and  curves connecting them to the square patches. The patches count  and the curves count  for a total contribution to the Euler characteristic of .</p>

<p>Finally, each vertex of the polycube becomes a sphere, subdivided by the patches and curves of the complex. These spheres also contain all the vertices of the complex. The Euler characteristic of a subdivided sphere would be , but the vertex spheres have some parts of their subdivision removed. Where each edge cylinder or expanded square comes into the sphere, a patch of surfaces is removed, and the curves between these removed patches are also removed. An edge  with degree  contributes to the removal of  curves and  patches (one for itself and  for each adjacent square). So if there are  vertices in the polycube, the  contribution from the Euler characteristics of the subdivided spheres is modified by subtracting  for each incident edge. The total modification at both endpoints of each edge is . The  that we calculated here is cancelled by the  on the cylinder for , and we are left with a total modification of  where  is the number of polycube edges.</p>

<p>Putting all the pieces of this calculation together, the complex we have constructed on the union of the shrunken interior and exterior has Euler characteristic . Therefore, the Euler characteristic of the polycube boundary itself, , equals the average of the characteristics of the interior and exterior. The same reasoning shows more generally that whenever you have a finite cell complex embedded into , dividing space up into chambers, the Euler characteristic of the complex equals half the sum of Euler characteristics of the manifolds bounding shrunken chambers.</p>

<p>Although Euler characteristics of 2-manifolds embedded without boundary in  are always even, this averaging method can produce non-manifold surfaces with odd Euler characteristic. For instance, consider mutilating the  block in a different way, by removing two opposite cubes. The interior and exterior of the resulting polycube are both connected, but the interior is a solid torus and the exterior is a ball. So the Euler characteristic of the polycube should be the average of the torus and sphere, . And if we actually calculate it we get .</p>

<p>As this example shows, it’s possible for a polycube to have different topologies of surface on the interior and exterior, and it’s also possible to have different numbers of surfaces: for instance, two cubes attached vertex-to-vertex produce two interior surfaces but only one exterior. For cell complexes in , there appears to be no restriction on which combinations of surfaces are possible. But for cell complexes in other spaces (other 3-manifolds than Euclidean space) it may be possible to embed 2-manifolds with odd Euler characteristic. When this happens, the number of odd chambers of a cell complex must always be even. For, the parity of the sum of the Euler characteristics of the chambers must be even, in order to be able to divide by two and get an integer as the Euler characteristic of the cell complex.</p>

<p>(<a href="https://mathstodon.xyz/@11011110/101978143052398446">Discuss on Mastodon</a>)</p></div>
    </content>
    <updated>2019-04-23T16:37:00Z</updated>
    <published>2019-04-23T16:37:00Z</published>
    <author>
      <name>David Eppstein</name>
    </author>
    <source>
      <id>https://11011110.github.io/blog/feed.xml</id>
      <author>
        <name>David Eppstein</name>
      </author>
      <link href="https://11011110.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/>
      <link href="https://11011110.github.io/blog/" rel="alternate" type="text/html"/>
      <subtitle>Geometry, graphs, algorithms, and more</subtitle>
      <title>11011110</title>
      <updated>2019-05-04T16:39:59Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://gilkalai.wordpress.com/?p=17325</id>
    <link href="https://gilkalai.wordpress.com/2019/04/23/an-invitation-to-a-conference-visions-in-mathematics-towards-2000/" rel="alternate" type="text/html"/>
    <title>An Invitation to a Conference: Visions in Mathematics towards 2000</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">Let me invite you to a conference. The conference took place in 1999 but only recently the 57 videos of the lectures and the discussion sessions are publicly available. (I thank Vitali Milman for telling me about it.) One novel … <a href="https://gilkalai.wordpress.com/2019/04/23/an-invitation-to-a-conference-visions-in-mathematics-towards-2000/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Let me invite you to a conference. The conference took place in 1999 but only recently <a href="https://www.youtube.com/playlist?list=PLP0YToNcfAwLBd8yibTtjv3aHfcbT4GBA">the 57 videos of the lectures and the discussion sessions are publicly available.</a> (I thank Vitali Milman for telling me about it.) One novel idea of Vitali Milman was to hold discussion sessions and they were quite interesting. (But, I am biased, I like discussions.) I will invite you to one of the heated discussions in the next post. There were very many nice talks and very many nice visions. And it is fun to watch the videos and judge the ideas in the perspective of time.</p>
<p>The proceedings appeared as GAFA special volumes, but alas the articles are not electronically available even to GAFA’s subscribers. Let me encourage both Birkhauser and the contributors to make them more available. My talk was: <a href="https://youtu.be/Wjg1_QwjUos">An invitation to Tverberg’s theorem</a>, and my own contribution to the Proceedings <a href="http://www.ma.huji.ac.il/~kalai/VIS.pdf">Combinatorics with a Geometric Flavor </a>is probably the widest scope survey article I ever wrote.  At the end of each section I added a brief philosophical thought about mathematics and those are collected in the post “<a href="https://gilkalai.wordpress.com/2008/10/12/about-mathematics/">about mathematics</a>“.</p>
<p>Two more things: The conference (and few others organized by Vitali) was  in Tel Aviv with a few days at the dead see and this worked very nicely.  Vitali also organized in the mid 90s another very successful geometry conference unofficially celebrating Gromov’s 50th birthday with, among others, a very nice lecture by Gregory Perelman. If videos will become available I will be delighted to invite you to that conference as well. Update from Vitali: It was also the week of Jeff Cheeger’s 50th birthday which was also celebrated. Grisha Perelman gave an absolutely excellent talk  talk on works of Cheeger.  Lectures were not videotaped.</p>
<p/>
<p><span style="color: #ff0000;">Avi Wigderson’s lecture</span></p>
<p><span style="color: #ff0000;">Are so called “natural questions” good for mathematics. Specifically is Kepler’s questions about the densest packing of unit balls in 3-space interesting? Watch a discussion of Misha Gromov, Noga Alon, Laci Lovasz and others. (next post)</span></p>
<h2/>
<h2><a href="https://gilkalai.files.wordpress.com/2019/04/vis99-p1.png"><img alt="" class="alignnone size-full wp-image-17362" height="363" src="https://gilkalai.files.wordpress.com/2019/04/vis99-p1.png?w=640&amp;h=363" width="640"/></a></h2>
<p><span style="color: #ff0000;">We were all so much younger!  (And in that old millennium,  we were also all men <img alt="&#x1F626;" class="wp-smiley" src="https://s0.wp.com/wp-content/mu-plugins/wpcom-smileys/twemoji/2/72x72/1f626.png" style="height: 1em;"/> )</span></p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/misha5.png"><img alt="" class="alignnone size-full wp-image-17366" height="614" src="https://gilkalai.files.wordpress.com/2019/04/misha5.png?w=640&amp;h=614" width="640"/></a></p>
<p><span style="color: #ff0000;">Misha Gromov argues passionately that natural problems are bad problems (see next post)</span></p>
<p>Pictures of most participants ad two slides are below</p>
<p><span id="more-17325"/><br/>
<a href="https://gilkalai.files.wordpress.com/2019/04/vim1.png"><img alt="VIM1.png" class="alignnone size-full wp-image-17372" src="https://gilkalai.files.wordpress.com/2019/04/vim1.png?w=640" style="font-size: 12px;"/></a><br/>
<a href="https://gilkalai.files.wordpress.com/2019/04/vim2.png"><img alt="VIM2.png" class="alignnone size-full wp-image-17373" src="https://gilkalai.files.wordpress.com/2019/04/vim2.png?w=640"/></a><a href="https://gilkalai.files.wordpress.com/2019/04/vim3.png"><img alt="VIM3.png" class="alignnone size-full wp-image-17374" src="https://gilkalai.files.wordpress.com/2019/04/vim3.png?w=640"/></a></p>
<p> </p>
<p><a href="https://gilkalai.files.wordpress.com/2019/04/vis.png"><img alt="" class="alignnone size-full wp-image-17387" height="373" src="https://gilkalai.files.wordpress.com/2019/04/vis.png?w=640&amp;h=373" width="640"/></a><a href="https://gilkalai.files.wordpress.com/2019/04/vim5.png"><img alt="VIM5.png" class="alignnone size-full wp-image-17376" src="https://gilkalai.files.wordpress.com/2019/04/vim5.png?w=640"/></a><a href="https://gilkalai.files.wordpress.com/2019/04/vim6.png"><img alt="VIM6.png" class="alignnone size-full wp-image-17377" src="https://gilkalai.files.wordpress.com/2019/04/vim6.png?w=640"/></a><a href="https://gilkalai.files.wordpress.com/2019/04/vim7.png"><img alt="VIM7.png" class="alignnone size-full wp-image-17378" src="https://gilkalai.files.wordpress.com/2019/04/vim7.png?w=640"/></a><a href="https://gilkalai.files.wordpress.com/2019/04/vim4.png"><img alt="VIM4.png" class="alignnone size-full wp-image-17375" src="https://gilkalai.files.wordpress.com/2019/04/vim4.png?w=640"/></a></p></div>
    </content>
    <updated>2019-04-23T15:40:47Z</updated>
    <published>2019-04-23T15:40:47Z</published>
    <category term="Combinatorics"/>
    <category term="Conferences"/>
    <category term="What is Mathematics"/>
    <category term="Vitali Milman"/>
    <author>
      <name>Gil Kalai</name>
    </author>
    <source>
      <id>https://gilkalai.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://gilkalai.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://gilkalai.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://gilkalai.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://gilkalai.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>Gil Kalai's blog</subtitle>
      <title>Combinatorics and more</title>
      <updated>2019-05-05T14:20:58Z</updated>
    </source>
  </entry>
</feed>
