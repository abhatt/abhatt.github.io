<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:planet="http://planet.intertwingly.net/" xmlns:indexing="urn:atom-extension:indexing" indexing:index="no"><access:restriction xmlns:access="http://www.bloglines.com/about/specs/fac-1.0" relationship="deny"/>
  <title>Theory of Computing Blog Aggregator</title>
  <updated>2019-12-09T13:21:30Z</updated>
  <generator uri="http://intertwingly.net/code/venus/">Venus</generator>
  <author>
    <name>Arnab Bhattacharyya, Suresh Venkatasubramanian</name>
    <email>arbhat+cstheoryfeed@gmail.com</email>
  </author>
  <id>http://www.cstheory-feed.org/atom.xml</id>
  <link href="http://www.cstheory-feed.org/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="http://www.cstheory-feed.org/" rel="alternate"/>

  <entry>
    <id>tag:blogger.com,1999:blog-3722233.post-693865784966232415</id>
    <link href="https://blog.computationalcomplexity.org/feeds/693865784966232415/comments/default" rel="replies" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/12/what-do-you-call-your-ugrad-non.html#comment-form" rel="replies" type="text/html"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/693865784966232415" rel="edit" type="application/atom+xml"/>
    <link href="https://www.blogger.com/feeds/3722233/posts/default/693865784966232415" rel="self" type="application/atom+xml"/>
    <link href="https://blog.computationalcomplexity.org/2019/12/what-do-you-call-your-ugrad-non.html" rel="alternate" type="text/html"/>
    <title>What do you call your ugrad non-algorithms theory course?</title>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">I am in the process of reviewing<br/>
<br/>
<i>                     What can be computed: A Practical Guide to the Theory of Computation</i><br/>
<i>                     by John MacCormick</i><br/>
<br/>
<br/>
and I need YOUR help for the first SENTENCE.  I began by saying<br/>
<br/>
<br/>
                    This is a text book for a course on <i>Formal Language Theory</i><br/>
<i><br/></i>
but then I realized that this is not what we call the course at UMCP. Then I got to thinking: what do other schools call it? I have the following so far:<br/>
<br/>
UMCP: Elementary Theory of Computation<br/>
<br/>
Harvard: Introduction to Theory of Computation<br/>
<br/>
MIT: Automata, Computability, and, Complexity<br/>
<br/>
Clark: Automata Theory<br/>
<br/>
(My spellcheck does not think Automata is a word. Also Computability. Usually I listen to my spellcheckers, but I checked and YES, I spelled them right.)<br/>
<br/>
For some other schools I either hit a place I needed an account, or I just got titles without a description so I could not be sure.<br/>
<br/>This is where YOU come in!<br/>
<br/>
Please leave comments with your school and the title of the course at your school that covers a reasonable overlap with: Regular Sets, Context Free Sets, Decidable and Undecidble and r.e. sets, P, NP, perhaps other complexity classes, and NP-completeness. Its FINE if your answer is one of the above ones, or one of the other comments--- I plan to later set this up as a pigeonhole principle problem.<br/>
<br/>
I suspect that courses in algorithms are called <i>Algorithms </i>or <i>Introduction to Algorithms.</i><br/>
<i><br/></i>
I suspect that courses in cryptography are called <i>Cryptography </i><i> </i>or <i>Intro to Cryptography.</i><br/>
<br/>
<br/>
Why does the non-algorithm, non-crypto theory course have more names?<br/>
<br/>
<br/>
<br/>
<br/>
<i><br/></i>
<br/></div>
    </content>
    <updated>2019-12-09T04:00:00Z</updated>
    <published>2019-12-09T04:00:00Z</published>
    <author>
      <name>GASARCH</name>
      <email>noreply@blogger.com</email>
      <uri>http://www.blogger.com/profile/03615736448441925334</uri>
    </author>
    <source>
      <id>tag:blogger.com,1999:blog-3722233</id>
      <category term="typecast"/>
      <category term="focs metacomments"/>
      <author>
        <name>Lance Fortnow</name>
        <email>noreply@blogger.com</email>
        <uri>http://www.blogger.com/profile/06752030912874378610</uri>
      </author>
      <link href="https://blog.computationalcomplexity.org/feeds/posts/default" rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default" rel="self" type="application/atom+xml"/>
      <link href="https://blog.computationalcomplexity.org/" rel="alternate" type="text/html"/>
      <link href="http://pubsubhubbub.appspot.com/" rel="hub" type="text/html"/>
      <link href="https://www.blogger.com/feeds/3722233/posts/default?start-index=26&amp;max-results=25" rel="next" type="application/atom+xml"/>
      <subtitle>Computational Complexity and other fun stuff in math and computer science from Lance Fortnow and Bill Gasarch</subtitle>
      <title>Computational Complexity</title>
      <updated>2019-12-09T11:53:53Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1912.03264</id>
    <link href="http://arxiv.org/abs/1912.03264" rel="alternate" type="text/html"/>
    <title>PU-GCN: Point Cloud Upsampling using Graph Convolutional Networks</title>
    <feedworld_mtime>1575849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/q/Qian:Guocheng.html">Guocheng Qian</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/a/Abualshour:Abdulellah.html">Abdulellah Abualshour</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Guohao.html">Guohao Li</a>, Ali Thabet, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Ghanem:Bernard.html">Bernard Ghanem</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1912.03264">PDF</a><br/><b>Abstract: </b>Upsampling sparse, noisy, and non-uniform point clouds is a challenging task.
In this paper, we propose 3 novel point upsampling modules: Multi-branch GCN,
Clone GCN, and NodeShuffle. Our modules use Graph Convolutional Networks (GCNs)
to better encode local point information. Our upsampling modules are versatile
and can be incorporated into any point cloud upsampling pipeline. We show how
our 3 modules consistently improve state-of-the-art methods in all point
upsampling metrics. We also propose a new multi-scale point feature extractor,
called Inception DenseGCN. We modify current Inception GCN algorithms by
introducing DenseGCN blocks. By aggregating data at multiple scales, our new
feature extractor is more resilient to density changes along point cloud
surfaces. We combine Inception DenseGCN with one of our upsampling modules
(NodeShuffle) into a new point upsampling pipeline: PU-GCN. We show both
qualitatively and quantitatively the advantages of PU-GCN against the
state-of-the-art in terms of fine-grained upsampling quality and point cloud
uniformity. The website and source code of this work is available at
https://sites.google.com/kaust.edu.sa/pugcn and
https://github.com/guochengqian/PU-GCN respectively.
</p></div>
    </summary>
    <updated>2019-12-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-12-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1912.03185</id>
    <link href="http://arxiv.org/abs/1912.03185" rel="alternate" type="text/html"/>
    <title>Parameterized Complexity of Partial Scheduling</title>
    <feedworld_mtime>1575849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/n/Nederlof:Jesper.html">Jesper Nederlof</a>, Céline Swennenhuis <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1912.03185">PDF</a><br/><b>Abstract: </b>We study a natural variant of scheduling that we call \emph{partial
scheduling}: In this variant an instance of a scheduling problem along with an
integer $k$ is given and one seeks an optimal schedule where not all, but only
$k$ jobs have to be processed.
</p>
<p>We study the Fixed Parameter Tractability of partial scheduling problems
parameterized by $k$ for all variants of scheduling problems that minimize the
makespan and involve unit/arbitrary processing times, identical/unrelated
parallel machines, release/due dates, and precedence constraints. That is, we
investigate whether algorithms with runtimes of the type $\mathcal{O}^*(f(k))$
exist, where the $\mathcal{O}^*(\cdot)$ notation omits factors polynomial in
the input size. We obtain a \emph{trichotomy} by categorizing each variant to
be either in $P$, $NP$-complete and Fixed Parameter Tractable by $k$, or
W[1]-hard by $k$.
</p>
<p>As one of our main technical contributions, we give an $\mathcal{O}^*(8^k)$
time algorithm to solve instances of $k$-scheduling problems minimizing the
makespan with unit job lengths, precedence constraints and release dates.
</p></div>
    </summary>
    <updated>2019-12-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-12-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1912.03134</id>
    <link href="http://arxiv.org/abs/1912.03134" rel="alternate" type="text/html"/>
    <title>Topological and Geometric Reconstruction of Metric Graphs in $\mathbb{R}^n$</title>
    <feedworld_mtime>1575849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fasy:Brittany_Terese.html">Brittany Terese Fasy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Komendarczyk:Rafal.html">Rafal Komendarczyk</a>, Sushovan Majhi, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wenk:Carola.html">Carola Wenk</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1912.03134">PDF</a><br/><b>Abstract: </b>We propose an algorithm to estimate the topology of an embedded metric graph
from a well-sampled finite subset of the underlying graph.
</p></div>
    </summary>
    <updated>2019-12-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-12-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1912.03127</id>
    <link href="http://arxiv.org/abs/1912.03127" rel="alternate" type="text/html"/>
    <title>Dominating sets reconfiguration under token sliding</title>
    <feedworld_mtime>1575849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bonamy:Marthe.html">Marthe Bonamy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Dorbec:Paul.html">Paul Dorbec</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Ouvrard:Paul.html">Paul Ouvrard</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1912.03127">PDF</a><br/><b>Abstract: </b>Let $G$ be a graph and $D_s$ and $D_{\textsf{t}}$ be two dominating sets of
$G$ of size $k$. Does there exist a sequence $\langle D_0 = D_s, D_1, \ldots,
D_{\ell-1}, D_\ell = D_{\textsf{t}} \rangle$ of dominating sets of $G$ such
that $D_{i+1}$ can be obtained from $D_i$ by replacing one vertex with one of
its neighbors? In this paper, we investigate the complexity of this decision
problem. We first prove that this problem is PSPACE-complete, even when
restricted to split, bipartite or bounded tree-width graphs. On the other hand,
we prove that it can be solved in polynomial time on dually chordal graphs (a
superclass of both trees and interval graphs) or cographs.
</p></div>
    </summary>
    <updated>2019-12-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-12-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1912.03088</id>
    <link href="http://arxiv.org/abs/1912.03088" rel="alternate" type="text/html"/>
    <title>Scheduling on Hybrid Platforms: Improved Approximability Window</title>
    <feedworld_mtime>1575849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Vincent Fagnon, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kacem:Imed.html">Imed Kacem</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lucarelli:Giorgio.html">Giorgio Lucarelli</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Simon:Bertrand.html">Bertrand Simon</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1912.03088">PDF</a><br/><b>Abstract: </b>Modern platforms are using accelerators in conjunction with standard
processing units in order to reduce the running time of specific operations,
such as matrix operations, and improve their performance. Scheduling on such
hybrid platforms is a challenging problem since the algorithms used for the
case of homogeneous resources do not adapt well. In this paper we consider the
problem of scheduling a set of tasks subject to precedence constraints on
hybrid platforms, composed of two types of processing units. We propose a
$(3+2\sqrt{2})$-approximation algorithm and a conditional lower bound of 3 on
the approximation ratio. These results improve upon the 6-approximation
algorithm proposed by Kedad-Sidhoum et al. as well as the lower bound of 2 due
to Svensson for identical machines. Our algorithm is inspired by the former one
and distinguishes the allocation and the scheduling phases. However, we propose
a different allocation procedure which, although is less efficient for the
allocation sub-problem, leads to an improved approximation ratio for the whole
scheduling problem. This approximation ratio actually decreases when the number
of processing units of each type is close and matches the conditional lower
bound when they are equal.
</p></div>
    </summary>
    <updated>2019-12-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-12-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1912.03042</id>
    <link href="http://arxiv.org/abs/1912.03042" rel="alternate" type="text/html"/>
    <title>Constructive derandomization of query algorithms</title>
    <feedworld_mtime>1575849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Blanc:Guy.html">Guy Blanc</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lange:Jane.html">Jane Lange</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tan:Li=Yang.html">Li-Yang Tan</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1912.03042">PDF</a><br/><b>Abstract: </b>We give efficient deterministic algorithms for converting randomized query
algorithms into deterministic ones. We first give an algorithm that takes as
input a randomized $q$-query algorithm $R$ with description length $N$ and a
parameter $\varepsilon$, runs in time $\mathrm{poly}(N) \cdot
2^{O(q/\varepsilon)}$, and returns a deterministic $O(q/\varepsilon)$-query
algorithm $D$ that $\varepsilon$-approximates the acceptance probabilities of
$R$. These parameters are near-optimal: runtime $N + 2^{\Omega(q/\varepsilon)}$
and query complexity $\Omega(q/\varepsilon)$ are necessary.
</p>
<p>Next, we give algorithms for instance-optimal and online versions of the
problem:
</p>
<p>$\circ$ Instance optimal: Construct a deterministic $q^\star_R$-query
algorithm $D$, where $q^\star_R$ is minimum query complexity of any
deterministic algorithm that $\varepsilon$-approximates $R$.
</p>
<p>$\circ$ Online: Deterministically approximate the acceptance probability of
$R$ for a specific input $\underline{x}$ in time
$\mathrm{poly}(N,q,1/\varepsilon)$, without constructing $D$ in its entirety.
</p>
<p>Applying the techniques we develop for these extensions, we constructivize
classic results that relate the deterministic, randomized, and quantum query
complexities of boolean functions (Nisan, STOC 1989; Beals et al., FOCS 1998).
This has direct implications for the Turing machine model of computation:
sublinear-time algorithms for total decision problems can be efficiently
derandomized and dequantized with a subexponential-time preprocessing step.
</p></div>
    </summary>
    <updated>2019-12-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-12-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1912.03033</id>
    <link href="http://arxiv.org/abs/1912.03033" rel="alternate" type="text/html"/>
    <title>Recovering the homology of immerged manifolds</title>
    <feedworld_mtime>1575849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/t/Tinarrage:Rapha=euml=l.html">Raphaël Tinarrage</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1912.03033">PDF</a><br/><b>Abstract: </b>Given a sample of an abstract manifold immerged in some Euclidean space, we
describe a way to recover the singular homology of the original manifold. It
consists in estimating its tangent bundle-seen as subset of another Euclidean
space-in a measure theoretic point of view, and in applying measure-based
filtrations for persistent homology. The construction we propose is consistent
and stable, and does not involve the knowledge of the dimension of the
manifold.
</p></div>
    </summary>
    <updated>2019-12-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-12-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1912.03032</id>
    <link href="http://arxiv.org/abs/1912.03032" rel="alternate" type="text/html"/>
    <title>Topology-Preserving Terrain Simplification</title>
    <feedworld_mtime>1575849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fugacci:Ulderico.html">Ulderico Fugacci</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kerber:Michael.html">Michael Kerber</a>, Hugo Manet <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1912.03032">PDF</a><br/><b>Abstract: </b>We give necessary and sufficient criteria for elementary operations in a
two-dimensional terrain to preserve the persistent homology induced by the
height function. These operations are edge flips and removals of interior
vertices, re-triangulating the link of the removed vertex. This problem is
motivated by topological terrain simplification, which means removing as many
critical vertices of a terrain as possible while maintaining geometric
closeness to the original surface. Existing methods manage to reduce the
maximal possible number of critical vertices, but increase thereby the number
of regular vertices. Our method can be used to post-process a simplified
terrain, drastically reducing its size and preserving its favorable properties.
</p></div>
    </summary>
    <updated>2019-12-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-12-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1912.03013</id>
    <link href="http://arxiv.org/abs/1912.03013" rel="alternate" type="text/html"/>
    <title>The canonical pairs of bounded depth Frege systems</title>
    <feedworld_mtime>1575849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Pavel Pudlak <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1912.03013">PDF</a><br/><b>Abstract: </b>The canonical pair of a proof system $P$ is the pair of disjoint NP sets
where one set is the set of all satisfiable CNF formulas and the other is the
set of CNF formulas that have $P$-proofs bounded by some polynomial. We give a
combinatorial characterization of the canonical pairs of depth~$d$ Frege
systems. Our characterization is based on certain games, introduced in this
article, that are parametrized by a number~$k$, also called the depth. We show
that the canonical pair of a depth~$d$ Frege system is polynomially equivalent
to the pair $(A_{d+2},B_{d+2})$ where $A_{d+2}$ (respectively, $B_{d+1}$) are
depth {$d+1$} games in which Player~I (Player II) has a positional winning
strategy. Although this characterization is stated in terms of games, we will
show that these combinatorial structures can be viewed as generalizations of
monotone Boolean circuits. In particular, depth~1 games are essentially
monotone Boolean circuits. Thus we get a generalization of the monotone
feasible interpolation for Resolution, which is a property that enables one to
reduce the task of proving lower bounds on the size of refutations to lower
bounds on the size of monotone Boolean circuits. However, we do not have a
method yet for proving lower bounds on the size of depth~$d$ games for $d&gt;1$.
</p></div>
    </summary>
    <updated>2019-12-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Complexity"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CC" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Complexity (cs.CC) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CC updates on arXiv.org</title>
      <updated>2019-12-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1912.02953</id>
    <link href="http://arxiv.org/abs/1912.02953" rel="alternate" type="text/html"/>
    <title>On the Complexity of the Stability Problem of Binary Freezing Totalistic Cellular Automata</title>
    <feedworld_mtime>1575849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/g/Goles:Eric.html">Eric Goles</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maldonado:Diego.html">Diego Maldonado</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Montealegre:Pedro.html">Pedro Montealegre</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/o/Ollinger:Nicolas.html">Nicolas Ollinger</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1912.02953">PDF</a><br/><b>Abstract: </b>In this paper we study the family of two-state Totalistic Freezing Cellular
Automata (TFCA) defined over the triangular and square grids with von Neumann
neighborhoods. We say that a Cellular Automaton is Freezing and Totalistic if
the active cells remain unchanged, and the new value of an inactive cell
depends only on the sum of its active neighbors. We classify all the Cellular
Automata in the class of TFCA, grouping them in five different classes: the
Trivial rules, Turing Universal rules,Algebraic rules, Topological rules and
Fractal Growing rules. At the same time, we study in this family the Stability
problem, consisting in deciding whether an inactive cell becomes active, given
an initial configuration.We exploit the properties of the automata in each
group to show that:
</p>
<p>- For Algebraic and Topological Rules the Stability problem is in
$\text{NC}$.
</p>
<p>- For Turing Universal rules the Stability problem is $\text{P}$-Complete.
</p></div>
    </summary>
    <updated>2019-12-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-12-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1912.02938</id>
    <link href="http://arxiv.org/abs/1912.02938" rel="alternate" type="text/html"/>
    <title>Lower Bounds for Compressed Sensing with Generative Models</title>
    <feedworld_mtime>1575849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kamath:Akshay.html">Akshay Kamath</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Karmalkar:Sushrut.html">Sushrut Karmalkar</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/p/Price:Eric.html">Eric Price</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1912.02938">PDF</a><br/><b>Abstract: </b>The goal of compressed sensing is to learn a structured signal $x$ from a
limited number of noisy linear measurements $y \approx Ax$. In traditional
compressed sensing, "structure" is represented by sparsity in some known basis.
Inspired by the success of deep learning in modeling images, recent work
starting with~\cite{BJPD17} has instead considered structure to come from a
generative model $G: \mathbb{R}^k \to \mathbb{R}^n$. We present two results
establishing the difficulty of this latter task, showing that existing bounds
are tight. First, we provide a lower bound matching the~\cite{BJPD17} upper
bound for compressed sensing from $L$-Lipschitz generative models $G$. In
particular, there exists such a function that requires roughly $\Omega(k \log
L)$ linear measurements for sparse recovery to be possible. This holds even for
the more relaxed goal of \emph{nonuniform} recovery. Second, we show that
generative models generalize sparsity as a representation of structure. In
particular, we construct a ReLU-based neural network $G: \mathbb{R}^{2k} \to
\mathbb{R}^n$ with $O(1)$ layers and $O(kn)$ activations per layer, such that
the range of $G$ contains all $k$-sparse vectors.
</p></div>
    </summary>
    <updated>2019-12-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-12-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1912.02900</id>
    <link href="http://arxiv.org/abs/1912.02900" rel="alternate" type="text/html"/>
    <title>Pinning Down the Strong Wilber 1 Bound for Binary Search Trees</title>
    <feedworld_mtime>1575849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chalermsook:Parinya.html">Parinya Chalermsook</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chuzhoy:Julia.html">Julia Chuzhoy</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Saranurak:Thatchaphol.html">Thatchaphol Saranurak</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1912.02900">PDF</a><br/><b>Abstract: </b>The famous dynamic optimality conjecture of Sleator and Tarjan from 1985
conjectures the existence of an $O(1)$-competitive algorithm for binary search
trees (BST's). Even the simpler problem of (offline) approximation of the
optimal cost of a BST for a given input, that we denote by $OPT$, is still
widely open, with the best current algorithm achieving an $O(\log\log
n)$-approximation. A major challenge in designing such algorithms is to obtain
a tight lower bound on $OPT$ that is algorithm friendly. Although several
candidate lower bounds were suggested in the past, such as WB-1 and WB-2 bounds
by Wilber, and Independent Rectangles bound by Demaine et al., the only
currently known non-trivial approximation algorithm achieves an $O(\log\log n)$
approximation factor by comparing $OPT$ with a weak variant of WB-1, that uses
a \emph{fixed} partitioning of the keys. This bound, however, is known to have
a gap of $\Omega(\log\log n)$, and therefore it cannot yield better
approximation algorithms. To overcome this obstacle, it is natural to consider
a stronger variant of WB-1, that maximizes the bound over \emph{all}
partitionings of the keys. An interesting question, mentioned by Iacono and by
Kozma, is whether the $O(\log\log n)$-approximation can be improved by using
this stronger bound.
</p>
<p>In this paper, we show that the gap between the stronger WB-1 bound and $OPT$
may be as large as $\Omega(\log\log n/\log\log\log n)$. This rules out the hope
of obtaining better approximation algorithms via the only known algorithmic
approach, combined with the stronger WB-1 bound. We also provide algorithmic
results: for any parameter $D$, we present a simple $O(D)$-approximation
algorithm with running time $\exp\left(O\left (n^{1/2^D}\log n\right )\right
)$. This implies an $O(1)$-approximation algorithm in sub-exponential time.
</p></div>
    </summary>
    <updated>2019-12-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-12-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1912.02858</id>
    <link href="http://arxiv.org/abs/1912.02858" rel="alternate" type="text/html"/>
    <title>Settling the relationship between Wilber's bounds for dynamic optimality</title>
    <feedworld_mtime>1575849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Lecomte:Victor.html">Victor Lecomte</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Weinstein:Omri.html">Omri Weinstein</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1912.02858">PDF</a><br/><b>Abstract: </b>In FOCS 1986, Wilber proposed two combinatorial lower bounds on the
operational cost of any binary search tree (BST) for a given access sequence $X
\in [n]^m$. Both bounds play a central role in the ongoing pursuit of the
dynamic optimality conjecture (Tarjan and Sleator, 1985), but their
relationship remained unknown for more than three decades. We show that
Wilber's Funnel bound dominates his Alternation bound for all $X$, and give a
tight $\Theta(\lg\lg n)$ separation for some $X$, answering Wilber's conjecture
and an open problem of Iacono, Demaine et. al. The main ingredient of the proof
is a new *symmetric* characterization of Wilber's Funnel bound, which proves
that it is invariant under rotations of $X$. We use this characterization to
provide initial indication that the Funnel bound matches the Independent
Rectangle bound (Demaine et al., 2009), by proving that when the Funnel bound
is constant, $\mathsf{IRB}_{\diagup\hspace{-.6em}\square}$ is linear. To the
best of our knowledge, our results provide the first progress on Wilber's
conjecture that the Funnel bound is dynamically optimal (1986).
</p></div>
    </summary>
    <updated>2019-12-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-12-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1912.02820</id>
    <link href="http://arxiv.org/abs/1912.02820" rel="alternate" type="text/html"/>
    <title>Complexity of a Root Clustering Algorithm</title>
    <feedworld_mtime>1575849600</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Batra:Prashant.html">Prashant Batra</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Sharma:Vikram.html">Vikram Sharma</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1912.02820">PDF</a><br/><b>Abstract: </b>Approximating the roots of a holomorphic function in an input box is a
fundamental problem in many domains. Most algorithms in the literature for
solving this problem are conditional, i.e., they make some simplifying
assumptions, such as, all the roots are simple or there are no roots on the
boundary of the input box, or the underlying machine model is Real RAM. Root
clustering is a generalization of the root approximation problem that allows
for errors in the computation and makes no assumption on the multiplicity of
the roots. An unconditional algorithm for computing a root clustering of a
holomorphic function was given by Yap, Sagraloff and Sharma in 2013. They
proposed a subdivision based algorithm using effective predicates based on
Pellet's test while avoiding any comparison with zeros (using soft zero
comparisons instead). In this paper, we analyze the running time of their
algorithm. We use the continuous amortization framework to derive an upper
bound on the size of the subdivision tree. We specialize this bound to the case
of polynomials and some simple transcendental functions such as exponential and
trigonometric sine. We show that the algorithm takes exponential time even for
these simple functions, unlike the case of polynomials. We also derive a bound
on the bit-precision used by the algorithm. To the best of our knowledge, this
is the first such result for holomorphic functions. We introduce new geometric
parameters, such as the relative growth of the function on the input box, for
analyzing the algorithm. Thus, our estimates naturally generalize the known
results, i.e., for the case of polynomials.
</p></div>
    </summary>
    <updated>2019-12-09T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-12-09T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://rjlipton.wordpress.com/?p=16454</id>
    <link href="https://rjlipton.wordpress.com/2019/12/08/hctor-garcia-molina-1953-2019/" rel="alternate" type="text/html"/>
    <title>Héctor Garcia-Molina, 1953–2019</title>
    <summary>We all lost a great person Cropped from Mexican NotiCyTI obit Héctor Garcia-Molina died just before Thanksgiving. He was a computer scientist and Professor in both the Departments of Computer Science and Electrical Engineering at Stanford University. Today Ken and I write in sadness about his passing and to express some personal appreciations. We at […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><font color="#0044cc"><br/>
<em>We all lost a great person</em><br/>
<font color="#000000"/></font></p><font color="#0044cc"><font color="#000000">
<table class="image alignright">
<tbody>
<tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2019/12/29-11-19-molina.jpg"><img alt="" class="alignright wp-image-16456" height="180" src="https://rjlipton.files.wordpress.com/2019/12/29-11-19-molina.jpg?w=165&amp;h=180" width="165"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Cropped from Mexican NotiCyTI <a href="https://noticyti.com/politica-cyt-i/742-muere-h%C3%A9ctor-garc%C3%ADa-molina,-el-mexicano-que-contribuy%C3%B3-al-estrellato-de-google-y-cisco.html">obit</a></font></td>
</tr>
</tbody>
</table>
<p>
Héctor Garcia-Molina <a href="https://news.stanford.edu/2019/12/06/hector-garcia-molina-influential-computer-scientist-database-expert-dies-65/">died</a> just before Thanksgiving. He was a computer scientist and Professor in both the Departments of Computer Science and Electrical Engineering at Stanford University.</p>
<p>
Today Ken and I write in sadness about his passing and to express some personal appreciations. </p>
<p>
We at GLL have talked about him before. See <a href="https://rjlipton.wordpress.com/2015/03/08/lint-for-math/">this</a> for his story about the fun of using an IBM mainframe for teaching. Or see <a href="https://rjlipton.wordpress.com/2011/11/03/whos-afraid-of-arrows-paradox/">this</a> for a story about Héctor and meetings.</p>
<p>
I, Dick, had the pleasure to have worked with him, while we were both faculty at Princeton in the 1980’s and beyond.</p>
<p>
</p><p/><h2> His Clock </h2><p/>
<p/><p>
Héctor was the chair of the Stanford Computer Science Department from January 2001 to December 2004. Stanford then rotated the chair so all took their turn. I know that he “hated” being an administrator in general. But of course being a team player he took his turn. </p>
<p>
One way to see his real feelings about being a chair was to look at the clock his students constructed for him. The clock was a digital timer that counted down the seconds that remained in his term as the chair. It started at roughly 126227808. I am sure he did fine, but the clock was a statement. </p>
<p>
</p><p/><h2> His Worst Paper </h2><p/>
<p/><p>
While Héctor was at Princeton we worked together on a project—the <a href="https://rjlipton.wordpress.com/2010/05/20/sorting-out-chess-endgames/">MMM</a> project. It led to one of his least cited papers—a 1984 <a href="https://ieeexplore.ieee.org/document/1676454">paper</a> with me and Jacobo Valdes. OK, it has 48 citations according to IEEE. The idea of the project was to use memory rather than processors to speed up computations. He was a joy to work with: he was careful, and thoughtful, and just fun to work with on any project. </p>
<p>
We did write a second <a href="https://books.google.com/books?id=AMrcBwAAQBAJ&amp;pg=PA565&amp;lpg=PA565&amp;dq=The+Massive+Memory+Machine+Project&amp;source=bl&amp;ots=_QFT07G56P&amp;sig=ACfU3U0-KV060yO3PHRllCnj3Y5o10BX-w&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwj7o67o-abmAhXHwFkKHRfzDs4Q6AEwDXoECAkQAQ#v=onepage&amp;q=The Massive Memory Machine Project&amp;f=false">paper</a> with Richard Cullingford instead of Valdes, which Héctor presented at a meeting on knowledge-based management systems. The above Google Books link goes to the end with a <a href="https://books.google.com/books?id=AMrcBwAAQBAJ&amp;pg=PA565&amp;lpg=PA565&amp;dq=The+Massive+Memory+Machine+Project&amp;source=bl&amp;ots=_QFT07G56P&amp;sig=ACfU3U0-KV060yO3PHRllCnj3Y5o10BX-w&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwj7o67o-abmAhXHwFkKHRfzDs4Q6AEwDXoECAkQAQ#v=onepage&amp;q=The Massive Memory Machine Project&amp;f=false">discussion</a> in which a simple issue was raised—we paraphrase:</p>
<blockquote><p><b> </b> <em> Won’t it take forever just to write all zeros to the memory? </em>
</p></blockquote>
<p/><p>
Héctor had a scientist’s answer: the project was still not at the prototype stage so he didn’t know. He said the project should be viewed as a scientific experiment to find out. Maybe he also had an inkling that what was coming was a decade of breakthroughs in CPU design and parallel/pipeline processing after all. </p>
<p>
</p><p/><h2> His Predictions </h2><p/>
<p/><p>
Héctor was unparalleled at seeing the future. I always thought that one of his abilities, one that set him apart, was his ability to predict directions of research. This allowed him, and his students, to write early papers in research areas before they became hot. This is one of the talents that made him so amazing.</p>
<p>
I recall way before the world wide web was created he had students working on adding links to documents. I recall a talk by one of his students at Princeton that discussed what we now call URLs. One question that was raised during the talk was: How were the links going to be created? There was a lively discussion about this. Could they be created automatically? If not why would people take the time to create the links? Indeed.</p>
<p>
Héctor saw that links would be created. That people would take the effort to create them. I must admit that he was right, and he saw the future better than most. I wish I had a fraction of his ability to see directions like he did.</p>
<p>
</p><p/><h2> Guiding Students </h2><p/>
<p/><p>
Héctor told me that when he first got to Stanford the fund managers and investors roamed the halls. They would ask anyone they could if they had an idea for a company or a startup. It was a constant issue that Héctor had to deal with. They were continually trying to steal away students. </p>
<p>
He told me he felt like he was the head of an abbey and was always having to protect his charges within the walls.</p>
<p>
When the impetus came from within it was different. Of course, Héctor was the advisor of Sergey Brin at the time he and Larry Page conceived Google. Brin and Page found that their search engine prototypes were so good the dataflow was constantly straining Stanford’s machines. They needed to scrounge for more disks and processors to mount their servers. Héctor already oversaw the Stanford Digital Libraries Project and he <a href="https://www.cnbc.com/2018/09/04/8-surprising-facts-you-might-not-know-about-googles-early-days.html">arranged</a> for funds to purchase spare parts for the data servers. </p>
<p>
It is interesting that in this 2001 <a href="https://sigmod.org/publications/interviews/pdf/hector-final1.pdf">interview</a> in the <em>SIGMOD Record</em>, Héctor did not have a high opinion of the industrial side of his area:</p>
<blockquote><p><b> </b> <em> Again, I don’t think industry really does very much research. They come up with an idea and they try to sell it. If it was a good idea, maybe they will make money. Even if it was a bad idea, if they have good marketing people, they might still make money and we never know … I don’t think they have an advantage over [academics] in testing the ideas and evaluating them and performing measurements and really understanding what are the right techniques. </em>
</p></blockquote>
<p/><p>
In the same interview, he had sage advice for students after completing their PhDs and during the tenure process, mainly on the side of not trying to play the system but focus on doing what you love.</p>
<p>
</p><p/><h2> Stanford Home Team </h2><p/>
<p/><p>
Héctor had been a graduate student at Stanford. So his return there as a professor was a kind of homecoming. He was a home-team player in many senses. One of them is shown by this photo:</p>
<p/><p/>
<table style="margin: auto;">
<tbody><tr>
<td>
<a href="https://rjlipton.files.wordpress.com/2019/12/hector-garcia-molina-sports.jpg"><img alt="" class="aligncenter size-medium wp-image-16457" height="200" src="https://rjlipton.files.wordpress.com/2019/12/hector-garcia-molina-sports.jpg?w=300&amp;h=200" width="300"/></a>
</td>
</tr>
<tr>
<td class="caption alignright"><font size="-2">Stanford University obituary <a href="https://news.stanford.edu/2019/12/06/hector-garcia-molina-influential-computer-scientist-database-expert-dies-65/">source</a></font>
</td>
</tr>
</tbody></table>
<p>
He was a registered Stanford sports photographer. He also taught a course at Stanford on photography. We don’t know if he had special insights on detecting “deepfake” photos and videos. </p>
<p>
</p><p/><h2> Open Problems </h2><p/>
<p/><p>
Our condolences go to all his family. Héctor you will be missed. You are missed. </p>
<p/></font></font></div>
    </content>
    <updated>2019-12-08T23:27:02Z</updated>
    <published>2019-12-08T23:27:02Z</published>
    <category term="All Posts"/>
    <category term="News"/>
    <category term="People"/>
    <category term="databases"/>
    <category term="Hector Garcia-Molina"/>
    <category term="memorial"/>
    <category term="memory machines"/>
    <author>
      <name>RJLipton+KWRegan</name>
    </author>
    <source>
      <id>https://rjlipton.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://rjlipton.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://rjlipton.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://rjlipton.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://rjlipton.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>a personal view of the theory of computation</subtitle>
      <title>Gödel’s Lost Letter and P=NP</title>
      <updated>2019-12-09T13:20:36Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/12/08/assistant-professor-position-tenure-track-at-hse-university-moscow-apply-by-january-12-2020/</id>
    <link href="https://cstheory-jobs.org/2019/12/08/assistant-professor-position-tenure-track-at-hse-university-moscow-apply-by-january-12-2020/" rel="alternate" type="text/html"/>
    <title>Assistant Professor Position (Tenure Track) at HSE University, Moscow (apply by January 12, 2020)</title>
    <summary>HSE University, Faculty of Computer Science invites applications for full-time, tenure-track positions of Assistant Professor in all areas of computer science including but not limited to programming language theory, software engineering, system programming, algorithms, computation complexity, bioinformatics, artificial intelligence, and machine learning, etc. Website: https://iri.hse.ru/computer_science2020 Email: s.karapetyan@hse.ru</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>HSE University, Faculty of Computer Science invites applications for full-time, tenure-track positions of Assistant Professor in all areas of computer science including but not limited to programming language theory, software engineering, system programming, algorithms, computation complexity, bioinformatics, artificial intelligence, and machine learning, etc.</p>
<p>Website: <a href="https://iri.hse.ru/computer_science2020">https://iri.hse.ru/computer_science2020</a><br/>
Email: s.karapetyan@hse.ru</p></div>
    </content>
    <updated>2019-12-08T19:52:48Z</updated>
    <published>2019-12-08T19:52:48Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-12-09T13:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/180</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/180" rel="alternate" type="text/html"/>
    <title>TR19-180 |  Covering Codes for Insertions and Deletions | 

	Andreas Lenz, 

	Cyrus Rashtchian, 

	Paul Siegel, 

	Eitan Yaakobi</title>
    <summary>A covering code is a set of codewords with the property that the union of balls, suitably defined, around these codewords covers an entire space. Generally, the goal is to find the covering code with the minimum size codebook. While most prior work on covering codes has focused on the Hamming metric, we consider the problem of designing covering codes defined in terms of  insertions and  deletions. First, we provide new sphere-covering lower bounds on the minimum possible size of such codes. Then, we provide new existential upper bounds on the size of optimal covering codes for a single insertion or a single deletion that are tight up to a constant factor. Finally, we derive improved upper bounds for covering codes using $R\geq 2$ insertions or deletions. We prove that codes exist with density that is only a factor $O(R \log R)$ larger than the lower bounds for all fixed $R$. In particular, our upper bounds have an optimal dependence on the word length, and we achieve asymptotic density matching the best known bounds for Hamming distance covering codes.</summary>
    <updated>2019-12-08T09:03:55Z</updated>
    <published>2019-12-08T09:03:55Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-12-09T13:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1912.02814</id>
    <link href="http://arxiv.org/abs/1912.02814" rel="alternate" type="text/html"/>
    <title>Efficient Deterministic Distributed Coloring with Small Bandwidth</title>
    <feedworld_mtime>1575763200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bamberger:Philipp.html">Philipp Bamberger</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/k/Kuhn:Fabian.html">Fabian Kuhn</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Maus:Yannic.html">Yannic Maus</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1912.02814">PDF</a><br/><b>Abstract: </b>We show that the $(degree+1)$-list coloring problem can be solved
deterministically in $O(D \cdot \log n \cdot\log^3 \Delta)$ in the CONGEST
model, where $D$ is the diameter of the graph, $n$ the number of nodes, and
$\Delta$ is the maximum degree. Using the network decomposition algorithm from
Rozhon and Ghaffari this implies the first efficient deterministic, that is,
$\text{poly}\log n$-time, CONGEST algorithm for the $\Delta+1$-coloring and the
$(degree+1)$-list coloring problem. Previously the best known algorithm
required $2^{O(\sqrt{\log n})}$ rounds and was not based on network
decompositions.
</p>
<p>Our results also imply deterministic $O(\log^3 \Delta)$-round algorithms in
MPC and the CONGESTED CLIQUE.
</p></div>
    </summary>
    <updated>2019-12-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-12-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1912.02728</id>
    <link href="http://arxiv.org/abs/1912.02728" rel="alternate" type="text/html"/>
    <title>Algorithm for Finding the Maximum Clique Based on Continuous Time Quantum Walk</title>
    <feedworld_mtime>1575763200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Xi.html">Xi Li</a>, Mingyou Wu, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Chen:Hanwu.html">Hanwu Chen</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1912.02728">PDF</a><br/><b>Abstract: </b>In this work, we consider the application of continuous time quantum
walking(CTQW) to the Maximum Clique(MC) Problem. Performing CTQW on graphs will
generate distinct periodic probability amplitude for different vertices. We
will show that the intensity of the probability amplitude at frequency indeed
implies the clique structure of some special kinds of graph. And recursive
algorithms with time complexity $O(N^5)$ in classical computers for finding the
maximum clique are proposed. We have experimented on random graphs where each
edge exists with probabilities 0.3, 0.5 and 0.7. Although counter examples are
not found for random graphs, whether these algorithms are universal is not
known to us.
</p></div>
    </summary>
    <updated>2019-12-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-12-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1912.02563</id>
    <link href="http://arxiv.org/abs/1912.02563" rel="alternate" type="text/html"/>
    <title>Universality of persistence diagrams and the bottleneck and Wasserstein distances</title>
    <feedworld_mtime>1575763200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/b/Bubenik:Peter.html">Peter Bubenik</a>, Alex Elchesen <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1912.02563">PDF</a><br/><b>Abstract: </b>We undertake a formal study of persistence diagrams and their metrics. We
show that barcodes and persistence diagrams together with the bottleneck
distance and the Wasserstein distances are obtained via universal constructions
and thus have corresponding universal properties. In addition, the
1-Wasserstein distance satisfies Kantorovich-Rubinstein duality. Our
constructions and results apply to any metric space with a distinguished
basepoint. For example, they can also be applied to multiparameter persistence
modules.
</p></div>
    </summary>
    <updated>2019-12-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-12-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1912.02430</id>
    <link href="http://arxiv.org/abs/1912.02430" rel="alternate" type="text/html"/>
    <title>Power of Pre-Processing: Production Scheduling with Variable Energy Pricing and Power-Saving States</title>
    <feedworld_mtime>1575763200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>Ondřej Benedikt, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/M=oacute=dos:Istv=aacute=n.html">István Módos</a>, Zdeněk Hanzálek <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1912.02430">PDF</a><br/><b>Abstract: </b>This paper addresses a single machine scheduling problem with non-preemptive
jobs to minimize the total electricity cost. Two latest trends in the area of
the energy-aware scheduling are considered, namely the variable energy pricing
and the power-saving states of a machine. Scheduling of the jobs and the
machine states are considered jointly to achieve the highest possible savings.
Although this problem has been previously addressed in the literature, the
reported results of the state-of-the-art method show that the optimal solutions
can be found only for instances with up to 35 jobs and 209 intervals within 3
hours of computation. We propose an elegant pre-processing technique called
SPACES for computing the optimal switching of the machine states with respect
to the energy costs. The optimal switchings are associated with the shortest
paths in an interval-state graph that describes all possible transitions
between the machine states in time. This idea allows us to implement efficient
integer linear programming and constraint programming models of the problem
while preserving the optimality. The efficiency of the models lies in the
simplification of the optimal switching representation. The results of the
experiments show that our approach outperforms the existing state-of-the-art
exact method. On a set of benchmark instances with varying sizes and different
state transition graphs, the proposed approach finds the optimal solutions even
for the large instances with up to 190 jobs and 1277 intervals within an hour
of computation.
</p></div>
    </summary>
    <updated>2019-12-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-12-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1912.02297</id>
    <link href="http://arxiv.org/abs/1912.02297" rel="alternate" type="text/html"/>
    <title>A Densest ternary circle packing in the plane</title>
    <feedworld_mtime>1575763200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/f/Fernique:Thomas.html">Thomas Fernique</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1912.02297">PDF</a><br/><b>Abstract: </b>We consider circle packings in the plane with circles of sizes $1$, $r\simeq
0.834$ and $s\simeq 0.651$. These sizes are algebraic numbers which allow a
compact packing, that is, a packing in which each hole is formed by three
mutually tangent circles. Compact packings are believed to maximize the density
when there are possible. We prove that it is indeed the case for these sizes.
The proof should be generalizable to other sizes which allow compact packings
and is a first step towards a general result.
</p></div>
    </summary>
    <updated>2019-12-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-12-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1912.02283</id>
    <link href="http://arxiv.org/abs/1912.02283" rel="alternate" type="text/html"/>
    <title>Sub-linear RACE Sketches for Approximate Kernel Density Estimation on Streaming Data</title>
    <feedworld_mtime>1575763200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/c/Coleman:Benjamin.html">Benjamin Coleman</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Shrivastava:Anshumali.html">Anshumali Shrivastava</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1912.02283">PDF</a><br/><b>Abstract: </b>Kernel density estimation is a simple and effective method that lies at the
heart of many important machine learning applications. Unfortunately, kernel
methods scale poorly for large, high dimensional datasets. Approximate kernel
density estimation has a prohibitively high memory and computation cost,
especially in the streaming setting. Recent sampling algorithms for high
dimensional densities can reduce the computation cost but cannot operate
online, while streaming algorithms cannot handle high dimensional datasets due
to the curse of dimensionality. We propose RACE, an efficient sketching
algorithm for kernel density estimation on high-dimensional streaming data.
RACE compresses a set of N high dimensional vectors into a small array of
integer counters. This array is sufficient to estimate the kernel density for a
large class of kernels. Our sketch is practical to implement and comes with
strong theoretical guarantees. We evaluate our method on real-world
high-dimensional datasets and show that our sketch achieves 10x better
compression compared to competing methods.
</p></div>
    </summary>
    <updated>2019-12-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-12-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1912.02278</id>
    <link href="http://arxiv.org/abs/1912.02278" rel="alternate" type="text/html"/>
    <title>A Framework for Robust Realistic Geometric Computations</title>
    <feedworld_mtime>1575763200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/e/Erickson:Jeff.html">Jeff Erickson</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hoog:Ivor_van_der.html">Ivor van der Hoog</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/m/Miltzow:Tillmann.html">Tillmann Miltzow</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1912.02278">PDF</a><br/><b>Abstract: </b>We propose a new paradigm for robust geometric computations that complements
the classical fixed precision paradigm and the exact geometric computation
paradigm. We provide a framework where we study algorithmic problems under
smoothed analysis of the input, the relaxation of the problem requirements, or
the witness of a recognition problem. Our framework specifies a widely
applicable set of prerequisites that make real RAM algorithms suitable for
smoothed analysis. We prove that suitable algorithms can (under smoothed
analysis) be robustly executed with expected logarithmic bit-precision. This
shows in a formal way that inputs which need high bit-precision are contrived
and that these algorithms are likely robust for realistic input. Interestingly
our techniques generalize to problems with a natural notion of resource
augmentation (geometric packing, the art gallery problem) and recognition
problems (recognition of realizable order types or disk intersection graphs).
</p>
<p>Our results also have theoretical implications for some ER-hard problems:
These problems have input instances where their real verification algorithm
requires at least exponential bit-precision which makes it difficult to place
these ER-hard problems in NP. Our results imply for a host of ER-complete
problems that this exponential bit-precision phenomenon comes from nearly
degenerate instances.
</p>
<p>It is not evident that problems that have a real verification algorithm
belong to ER. Therefore, we conclude with a real RAM analogue to the Cook-Levin
Theorem. This gives an easy proof of ER-membership, as real verification
algorithms are much more versatile than ETR-formulas.
</p></div>
    </summary>
    <updated>2019-12-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Computational Geometry"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.CG" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Computational Geometry (cs.CG) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.CG updates on arXiv.org</title>
      <updated>2019-12-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1912.02217</id>
    <link href="http://arxiv.org/abs/1912.02217" rel="alternate" type="text/html"/>
    <title>Assessing the best edit in perturbation-based iterative refinement algorithms to compute the median string</title>
    <feedworld_mtime>1575763200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b>P. Mirabal, J. Abreu, D. Seco <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1912.02217">PDF</a><br/><b>Abstract: </b>Strings are a natural representation of biological data such as DNA, RNA and
protein sequences. The problem of finding a string that summarizes a set of
sequences has direct application in relative compression algorithms for genome
and proteome analysis, where reference sequences need to be chosen. Median
strings have been used as representatives of a set of strings in different
domains. However, several formulations of those problems are NP-Complete.
Alternatively, heuristic approaches that iteratively refine an initial coarse
solution by applying edit operations have been proposed. Recently, we
investigated the selection of the optimal edit operations to speed up
convergence without spoiling the quality of the approximated median string. We
propose a novel algorithm that outperforms state of the art heuristic
approximations to the median string in terms of convergence speed by estimating
the effect of a perturbation in the minimization of the expressions that define
the median strings. We present corpus of comparative experiments to validate
these results.
</p></div>
    </summary>
    <updated>2019-12-08T00:00:00Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-12-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>http://arxiv.org/abs/1908.11515</id>
    <link href="http://arxiv.org/abs/1908.11515" rel="alternate" type="text/html"/>
    <title>MURS: Practical and Robust Privacy Amplification with Multi-Party Differential Privacy</title>
    <feedworld_mtime>1575763200</feedworld_mtime>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><b>Authors: </b><a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/w/Wang:Tianhao.html">Tianhao Wang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/x/Xu:Min.html">Min Xu</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/d/Ding:Bolin.html">Bolin Ding</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/z/Zhou:Jingren.html">Jingren Zhou</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Hong:Cheng.html">Cheng Hong</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/h/Huang:Zhicong.html">Zhicong Huang</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/l/Li:Ninghui.html">Ninghui Li</a>, <a href="http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/j/Jha:Somesh.html">Somesh Jha</a> <br/><b>Download:</b> <a href="http://arxiv.org/pdf/1908.11515">PDF</a><br/><b>Abstract: </b>When collecting information, local differential privacy (LDP) alleviates
privacy concerns of users because their private information is randomized
before being sent to the central aggregator. However, LDP results in loss of
utility due to the amount of noise that is added to each individual data item.
To address this issue, recent work introduced an intermediate server with the
assumption that this intermediate server did not collude with the aggregator.
Using this trust model, one can add less noise to achieve the same privacy
guarantee; thus improving the utility.
</p>
<p>In this paper, we investigate this multiple-party setting of LDP. We first
analyze the threat model and identify potential adversaries. We then make
observations about existing approaches and propose new techniques that achieve
a better privacy-utility tradeoff than existing ones. Finally, we perform
experiments to compare different methods and demonstrate the benefits of using
our proposed method.
</p></div>
    </summary>
    <updated>2019-12-08T23:27:37Z</updated>
    <author>
      <name/>
    </author>
    <source>
      <id>http://arxiv.org/</id>
      <category term="Computer Science -- Data Structures and Algorithms"/>
      <link href="http://arxiv.org/" rel="alternate" type="text/html"/>
      <link href="http://export.arxiv.org/rss/cs.DS" rel="self" type="application/rdf+xml"/>
      <subtitle>Computer Science -- Data Structures and Algorithms (cs.DS) updates on the arXiv.org e-print archive</subtitle>
      <title>cs.DS updates on arXiv.org</title>
      <updated>2019-12-06T01:30:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/12/07/postdoctoral-fellowship-at-bocconi-university-apply-by-december-15-2019/</id>
    <link href="https://cstheory-jobs.org/2019/12/07/postdoctoral-fellowship-at-bocconi-university-apply-by-december-15-2019/" rel="alternate" type="text/html"/>
    <title>Postdoctoral Fellowship at Bocconi University (apply by December 15, 2019)</title>
    <summary>Two postdoctoral positions, each for one year renewable to a second, are available to work in Luca Trevisan’s group at Bocconi University on topics related to average-case analysis of algorithms, approximation algorithms, and combinatorial constructions. The positions have a very competitive salary and relocation benefits. Funding for travel is available. Website: https://www.unibocconi.eu/wps/wcm/connect/3ecc85da-ac66-46ac-9db7-09ac0ef9b715/Call-2ADR-01B1-Bidsa-Erc.pdf?MOD=AJPERES&amp;CVID=mUjaHAv Email: L.Trevisan@unibocconi.it</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Two postdoctoral positions, each for one year renewable to a second, are available to work in Luca Trevisan’s group at Bocconi University on topics related to average-case analysis of algorithms, approximation algorithms, and combinatorial constructions.</p>
<p>The positions have a very competitive salary and relocation benefits. Funding for travel is available.</p>
<p>Website: <a href="https://www.unibocconi.eu/wps/wcm/connect/3ecc85da-ac66-46ac-9db7-09ac0ef9b715/Call-2ADR-01B1-Bidsa-Erc.pdf?MOD=AJPERES&amp;CVID=mUjaHAv">https://www.unibocconi.eu/wps/wcm/connect/3ecc85da-ac66-46ac-9db7-09ac0ef9b715/Call-2ADR-01B1-Bidsa-Erc.pdf?MOD=AJPERES&amp;CVID=mUjaHAv</a><br/>
Email: L.Trevisan@unibocconi.it</p></div>
    </content>
    <updated>2019-12-07T23:11:27Z</updated>
    <published>2019-12-07T23:11:27Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-12-09T13:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://lucatrevisan.wordpress.com/?p=4279</id>
    <link href="https://lucatrevisan.wordpress.com/2019/12/07/postdoc-position-at-bocconi/" rel="alternate" type="text/html"/>
    <title>Postdoc Position at Bocconi</title>
    <summary type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml">I am recruiting for two postdoctoral positions, each for one year renewable to a second, to work with me at Bocconi University on topics related to average-case analysis of algorithms, approximation algorithms, and combinatorial constructions. The positions have a very … <a href="https://lucatrevisan.wordpress.com/2019/12/07/postdoc-position-at-bocconi/">Continue reading <span class="meta-nav">→</span></a></div>
    </summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>I am recruiting for two postdoctoral positions, each for one year renewable to a second, to work with me at Bocconi University on topics related to average-case analysis of algorithms, approximation algorithms, and combinatorial constructions. </p>
<p>The positions have a very competitive salary and relocation benefits. Funding for travel is available.</p>
<p>Application information is at <a href="https://www.unibocconi.eu/wps/wcm/connect/3ecc85da-ac66-46ac-9db7-09ac0ef9b715/Call-2ADR-01B1-Bidsa-Erc.pdf?MOD=AJPERES&amp;CVID=mUjaHAv">this link</a>. The deadline  is <b>December 15</b>. If you apply, please also send me an email (L.Trevisan at unibocconi.it) to let me know.</p></div>
    </content>
    <updated>2019-12-07T23:07:19Z</updated>
    <published>2019-12-07T23:07:19Z</published>
    <category term="theory"/>
    <category term="jobs"/>
    <author>
      <name>luca</name>
    </author>
    <source>
      <id>https://lucatrevisan.wordpress.com</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://lucatrevisan.wordpress.com/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://lucatrevisan.wordpress.com" rel="alternate" type="text/html"/>
      <link href="https://lucatrevisan.wordpress.com/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://lucatrevisan.wordpress.com/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>"Marge, I agree with you - in theory. In theory, communism works. In theory." -- Homer Simpson</subtitle>
      <title>in   theory</title>
      <updated>2019-12-09T13:20:11Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/179</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/179" rel="alternate" type="text/html"/>
    <title>TR19-179 |  Towards Optimal Separations between Quantum and Randomized Query Complexities | 

	Avishay Tal</title>
    <summary>The query model offers a concrete setting where quantum algorithms are provably superior to randomized algorithms. Beautiful results by Bernstein-Vazirani, Simon,  Aaronson, and others presented partial Boolean functions that can be computed by quantum algorithms making much fewer queries compared to their randomized analogs. To date, separations of $O(1)$ vs. $\sqrt{N}$ between quantum and randomized query complexities remain the state-of-the-art (where $N$ is the input length), leaving open the question of whether $O(1)$ vs. $N^{1/2+\Omega(1)}$ separations are possible?

We answer this question in the affirmative. Our separating problem is a variant of the Aaronson-Ambainis $k$-fold Forrelation problem. We show that our variant:
(1) Can be solved by a quantum algorithm making $2^{O(k)}$ queries to the inputs.
(2) Requires at least $\tilde{\Omega}(N^{2(k-1)/(3k-1)})$ queries for any randomized algorithm.

For any constant $\varepsilon&gt;0$, this gives a  $O(1)$  vs. $N^{2/3-\varepsilon}$ separation between the quantum and randomized query complexities of partial Boolean functions. 

Our proof is Fourier analytical and uses new bounds on the Fourier spectrum of classical decision trees, which could be of independent interest. 

Looking forward, we conjecture that the Fourier bounds could be further improved in a precise manner, and show that such conjectured bounds imply optimal $O(1)$ vs. $N^{1-\varepsilon}$ separations between the quantum and randomized query complexities of partial Boolean functions.</summary>
    <updated>2019-12-07T14:25:04Z</updated>
    <published>2019-12-07T14:25:04Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-12-09T13:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/12/07/tenure-track-faculty-at-university-of-michigan-apply-by-january-1-2020/</id>
    <link href="https://cstheory-jobs.org/2019/12/07/tenure-track-faculty-at-university-of-michigan-apply-by-january-1-2020/" rel="alternate" type="text/html"/>
    <title>Tenure-track faculty at University of Michigan (apply by January 1, 2020)</title>
    <summary>Computer Science and Engineering at the University of Michigan currently invites applications for multiple tenure-track and teaching faculty (lecturer) positions. We seek exceptional candidates at all levels in all areas across computer science and computer engineering. We also have a targeted search for an endowed professorship in theoretical computer science (the Fischer Chair). Website: https://cse.engin.umich.edu/about/faculty-hiring/ […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Computer Science and Engineering at the University of Michigan currently invites applications for multiple tenure-track and teaching faculty (lecturer) positions. We seek exceptional candidates at all levels in all areas across computer science and computer engineering. We also have a targeted search for an endowed professorship in theoretical computer science (the Fischer Chair).</p>
<p>Website: <a href="https://cse.engin.umich.edu/about/faculty-hiring/">https://cse.engin.umich.edu/about/faculty-hiring/</a><br/>
Email: kuipers@umich.edu</p></div>
    </content>
    <updated>2019-12-07T03:53:14Z</updated>
    <published>2019-12-07T03:53:14Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-12-09T13:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/178</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/178" rel="alternate" type="text/html"/>
    <title>TR19-178 |  Almost Tight Lower Bounds on Regular Resolution Refutations of Tseitin Formulas for All Constant-Degree Graphs | 

	Dmitry Itsykson, 

	Artur Riazanov, 

	Petr Smirnov, 

	Danil Sagunov</title>
    <summary>We show that the size of any regular resolution refutation of Tseitin formula $T(G,c)$ based on a graph $G$ is at least $2^{\Omega(tw(G)/\log n)}$, where $n$ is the number of vertices in $G$ and $tw(G)$ is the treewidth of $G$. For constant degree graphs there is known upper bound $2^{O(tw(G))}$ [AR11, GTT18], so our lower bound is tight up to a logarithmic factor in the exponent.

In order to prove this result we show that any regular resolution proof of Tseitin formula $T(G,c)$ of size $S$ can be converted to a read-once branching program computing satisfiable Tseitin formula $T(G,c')$ of size $S^{O(\log n)}$. Then we show that any read-once branching program computing satisfiable Tseitin formula $T(G,c')$ has size at least $2^{\Omega(tw(G))}$; the latter improves the recent result of Glinskih and Itsykson [GI19].</summary>
    <updated>2019-12-07T01:51:51Z</updated>
    <published>2019-12-07T01:51:51Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-12-09T13:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/12/06/faculty-at-george-washington-university-apply-by-january-9-2020/</id>
    <link href="https://cstheory-jobs.org/2019/12/06/faculty-at-george-washington-university-apply-by-january-9-2020/" rel="alternate" type="text/html"/>
    <title>Faculty at George Washington University (apply by January 9, 2020)</title>
    <summary>The Department of Computer Science at The George Washington University invites applications for two tenure track positions at the Assistant, Associate or Full Professor level, beginning as early as Fall 2020. One position focuses on Machine Learning and related areas; the other position welcomes all areas of theoretical and applied computer science. Website: https://www.gwu.jobs/postings/72053 Email: […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Department of Computer Science at The George Washington University invites applications for two tenure track positions at the Assistant, Associate or Full Professor level, beginning as early as Fall 2020. One position focuses on Machine Learning and related areas; the other position welcomes all areas of theoretical and applied computer science.</p>
<p>Website: <a href="https://www.gwu.jobs/postings/72053">https://www.gwu.jobs/postings/72053</a><br/>
Email: cssearch@gwu.edu</p></div>
    </content>
    <updated>2019-12-06T21:33:40Z</updated>
    <published>2019-12-06T21:33:40Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-12-09T13:20:38Z</updated>
    </source>
  </entry>

  <entry>
    <id>https://decentralizedthoughts.github.io/2019-12-06-dce-the-three-scalability-bottlenecks-of-state-machine-replication/</id>
    <link href="https://decentralizedthoughts.github.io/2019-12-06-dce-the-three-scalability-bottlenecks-of-state-machine-replication/" rel="alternate" type="text/html"/>
    <title>Data, Consensus, Execution: Three Scalability Bottlenecks for State Machine Replication</title>
    <summary>If anyone asks you: how can I scale my State Machine Replication (Blockchain) system? You should answer back with a question: what is your bottleneck? Is it Data, Consensus or Execution? Data: Shipping the commands to all the replicas. For example, if a block contains 1MB of commands, then you...</summary>
    <updated>2019-12-06T17:05:00Z</updated>
    <published>2019-12-06T17:05:00Z</published>
    <source>
      <id>https://decentralizedthoughts.github.io</id>
      <author>
        <name>Ittai Abraham</name>
      </author>
      <link href="https://decentralizedthoughts.github.io" rel="alternate" type="text/html"/>
      <link href="https://decentralizedthoughts.github.io/feed.xml" rel="self" type="application/atom+xml"/>
      <subtitle>Decentralized thoughts about decentralization</subtitle>
      <title>Decentralized Thoughts</title>
      <updated>2019-12-08T23:37:00Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-us">
    <id>https://eccc.weizmann.ac.il/report/2019/177</id>
    <link href="https://eccc.weizmann.ac.il/report/2019/177" rel="alternate" type="text/html"/>
    <title>TR19-177 |  Pseudo-deterministic Streaming | 

	Shafi Goldwasser, 

	Ofer Grossman, 

	Sidhanth Mohanty, 

	David Woodruff</title>
    <summary>A pseudo-deterministic algorithm is a (randomized) algorithm which, when run multiple times on the same input, with high probability outputs the same result on all executions. Classic streaming algorithms, such as those for finding heavy hitters, approximate counting, $\ell_2$ approximation, finding a nonzero entry in a vector (for turnstile algorithms) are not pseudo-deterministic. For example, in the instance of finding a nonzero entry in a vector, for any known low-space algorithm $A$, there exists a stream $x$ so that running $A$ twice on $x$ (using different randomness) would with high probability result in two different entries as the output.

In this work, we study whether it is inherent that these algorithms output different values on different executions. That is, we ask whether these problems have low-memory pseudo-deterministic algorithms. For instance, we show that there is no low-memory pseudo-deterministic algorithm for finding a nonzero entry in a vector (given in a turnstile fashion), and also that there is no low-dimensional pseudo-deterministic sketching algorithm for $\ell_2$ norm estimation.  We also exhibit problems which do have low memory pseudo-deterministic algorithms but no low memory deterministic algorithm, such as outputting a nonzero row of a matrix, or outputting a basis for the row-span of a matrix.

We also investigate multi-pseudo-deterministic algorithms: algorithms which with high probability output one of a few options. We show the first lower bounds for such algorithms. This implies that there are streaming problems such that every low space algorithm for the problem must have inputs where there are many valid outputs, all with a significant probability of being outputted.</summary>
    <updated>2019-12-06T14:44:41Z</updated>
    <published>2019-12-06T14:44:41Z</published>
    <source>
      <id>https://eccc.weizmann.ac.il/</id>
      <author>
        <name>ECCC papers</name>
      </author>
      <link href="https://eccc.weizmann.ac.il/" rel="alternate" type="text/html"/>
      <link href="https://example.com/feeds/reports/" rel="self" type="application/atom+xml"/>
      <subtitle>Latest Reports published at https://eccc.weizmann.ac.il</subtitle>
      <title>ECCC - Reports</title>
      <updated>2019-12-09T13:20:25Z</updated>
    </source>
  </entry>

  <entry xml:lang="en-US">
    <id>http://ptreview.sublinear.info/?p=1227</id>
    <link href="https://ptreview.sublinear.info/?p=1227" rel="alternate" type="text/html"/>
    <title>News for November 2019</title>
    <summary>We hit the mother-lode of property testing papers this month. Stick with us, as we cover 10 (!) papers that appeared online in November. EDIT: We actually have 11 papers, check out Optimal Adaptive Detection of Monotone Patterns at the bottom. Testing noisy linear functions for sparsity, by Xue Chen, Anindya De, and Rocco A. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>We hit the mother-lode of property testing papers this month. Stick with us, as we cover 10 (!) papers that appeared online in November.</p>



<p>EDIT: We actually have 11 papers, check out <em>Optimal Adaptive Detection of Monotone Patterns</em> at the bottom.</p>



<p><strong>Testing noisy linear functions for sparsity</strong>, by Xue Chen, Anindya De, and Rocco A. Servedio (<a href="https://arxiv.org/abs/1911.00911">arXiv</a>). Given samples from a noisy linear model \(y = w\cdot x + \mathrm{noise}\), test whether \(w\) is \(k\)-sparse, or far from being \(k\)-sparse. This is a property testing version of the celebrated sparse recovery problem, whose sample complexity is well-known to be \(O(k\log n)\), where the data lies in \(\mathbb{R}^n\). This paper shows that the testing version of the problem can be solved (tolerantly) with a number of samples independent of \(n\), assuming technical conditions: the distribution of coordinates of \(x\) are i.i.d. and non-Gaussian, and the noise distribution is known to the algorithm. Surprisingly, all these conditions are needed, otherwise the dependence on \(n\) is \(\tilde \Omega(\log n)\), essentially the same as the recovery problem.</p>



<p><strong>Pan-Private Uniformity Testing</strong>, by Kareem Amin, Matthew Joseph, Jieming Mao (<a href="https://arxiv.org/abs/1911.01452">arXiv</a>). Differentially private distribution testing has now seen significant study, in both the local and central models of privacy. This paper studies a distribution testing in the pan-private model, which is intermediate: the algorithm receives samples one by one in the clear, but it must maintain a differentially private internal state at all time steps. The sample complexity turns out to be qualitatively intermediate to the two other models: testing uniformity over \([k]\) requires \(\Theta(\sqrt{k})\) samples in the central model, \(\Theta(k)\) samples in the local model, and this paper shows that \(\Theta(k^{2/3})\) samples are necessary and sufficient in the pan-private model.</p>



<p><strong>Almost Optimal Testers for Concise Representations</strong>, by Nader Bshouty (<a href="https://eccc.weizmann.ac.il/report/2019/156/">ECCC</a>). This work gives a unified approach for testing for a plethora of different classes which possess some sort of <em>sparsity</em>. These classes include \(k\)-juntas, \(k\)-linear functions, \(k\)-terms, various types of DNFs, decision lists, functions with bounded Fourier degree, and much more. </p>



<p><strong>Unified Sample-Optimal Property Estimation in Near-Linear Time</strong>, by Yi Hao and Alon Orlitsky (<a href="https://arxiv.org/abs/1911.03105">arXiv</a>). This paper presents a unified approach for estimating several distribution properties with both near-optimal time and sample complexity, based on piecewise-polynomial approximation. Some applications include estimators for Shannon entropy, power sums, distance to uniformity,  normalized support size, and normalized support coverage. More generally, results hold for all Lipschitz properties, and consequences include high-confidence property estimation (outperforming the “median trick”) and differentially private property estimation.</p>



<p><strong>Testing linear-invariant properties</strong>, by Jonathan Tidor and Yufei Zhao (<a href="https://arxiv.org/abs/1911.06793">arXiv</a>). This paper studies property testing of functions which are in a formal sense, definable by restrictions to subspaces of bounded degree. This class of functions is a broad generalization of testing whether a function is linear, or a degree-\(d\) polynomial (for constant \(d\)). The algorithm is the oblivious one, which simply repeatedly takes random restrictions and tests whether the property is satisfied or not (similar to the classic linearity test of BLR, along with many others). </p>



<p><strong>Approximating the Distance to Monotonicity of Boolean Functions</strong>, by Ramesh Krishnan S. Pallavoor, Sofya Raskhodnikova, Erik Waingarten (<a href="https://eccc.weizmann.ac.il/report/2019/163/">ECCC</a>). This paper studies the following fundamental question in tolerant testing: given a Boolean function on the hypercube, test whether it is \(\varepsilon’\)-close or \(\varepsilon\)-far from monotone. It is shown that there is a non-adaptive polynomial query algorithm which can solve this problem for \(\varepsilon’ = \varepsilon/\tilde \Theta(\sqrt{n})\), implying an algorithm which can approximate distance to monotonicity up to a multiplicative \(\tilde O(\sqrt{n})\) (addressing an <a href="https://ptreview.sublinear.info/?p=250">open problem</a> by Sesh). They also give a lower bound demonstrating that improving this approximating factor significantly would necessitate exponentially-many queries. Interestingly, this is proved for the (easier) erasure-resilient model, and also implies lower bounds for tolerant testing of unateness and juntas.</p>



<p><strong>Testing Properties of Multiple Distributions with Few Samples</strong>, by Maryam Aliakbarpour and Sandeep Silwal (<a href="https://arxiv.org/abs/1911.07324">arXiv</a>). This paper introduces a new model for distribution testing. Generally, we are given \(n\) samples from a distribution which is either (say) uniform or far from uniform, and we wish to test which is the case. The authors here study the problem where we are given a <em>single sample</em> from \(n\) different distributions which are either all uniform or far from uniform, and we wish to test which is the case. By additionally assuming a structural condition in the latter case (it is argued that <em>some</em> structural condition is necessary), they give sample-optimal algorithms for testing uniformity, identity, and closeness.</p>



<p><strong>Random Restrictions of High-Dimensional Distributions and Uniformity Testing with Subcube Conditioning</strong>, by Clément L. Canonne, Xi Chen, Gautam Kamath, Amit Levi, and Erik Waingarten (<a href="https://eccc.weizmann.ac.il/report/2019/165/">ECCC</a>, <a href="https://arxiv.org/abs/1911.07357">arXiv</a>). By now, it is well-known that testing uniformity over the \(n\)-dimensional hypercube requires \(\Omega(2^{n/2})\) samples — the curse of dimensionality quickly makes this problem intractable. One option is to assume that the distribution is product, which causes the complexity to drop to \(O(\sqrt{n})\). This paper instead assumes one has stronger access to the distribution — namely, one can receive samples conditioned on being from some subcube of the domain. With this, the paper shows that the complexity drops to the near-optimal \(\tilde O(\sqrt{n})\) samples. The related problem of testing whether a distribution is either uniform or has large mean is also considered. </p>



<p><strong>Property Testing of LP-Type Problems</strong>, by Rogers Epstein, Sandeep Silwal (<a href="https://arxiv.org/abs/1911.08320">arXiv</a>). An LP-Type problem (also known as a generalized linear program) is an optimization problem sharing some properties with linear programs. More formally, they consist of a set of constraints \(S\) and a function \(\varphi\) which maps subsets of \(S\) to some totally ordered set, such that \(\varphi\) possesses monotonicity and locality properties. This paper considers the problem of testing whether \(\varphi(S) \leq k\), or whether at least an \(\varepsilon\)-fraction of constraints in \(S\) must be removed for \(\varphi(S) \leq k\) to hold. This paper gives an algorithm with query complexity \(O(\delta/\varepsilon)\), where \(\delta\) is a dimension measure of the problem. This is applied to testing problems for linear separability, smallest enclosing ball, smallest intersecting ball, smallest volume annulus. The authors also provide lower bounds for some of these problems as well.</p>



<p><strong>Near-Optimal Algorithm for Distribution-Free Junta Testing</strong>, by Xiaojin Zhang (<a href="https://arxiv.org/abs/1911.10833">arXiv</a>). This paper presents an (adaptive) algorithm for testing juntas, in the distribution-free model with one-sided error. The query complexity is \(\tilde O(k/\varepsilon)\), which is nearly optimal. Algorithms with this sample complexity were previously known under the uniform distribution, or with two-sided error, but this is the first paper to achieve it in the distribution-free model with one-sided error.</p>



<p><strong>Optimal Adaptive Detection of Monotone Patterns</strong>, by Omri Ben-Eliezer, Shoham Letzter, Erik Waingarten (<a href="https://arxiv.org/abs/1911.01169">arXiv</a>). Consider the problem of testing whether a function has no monotone increasing subsequences of length \(k\), versus being \(\varepsilon\)-far from having this property. Note that this is a generalization of testing whether a function is monotone (decreasing), which corresponds to the case \(k = 2\). This work shows that the adaptive sample complexity of this problem is \(O_{k,\varepsilon}(\log n)\), matching the lower bound for monotonicity testing. This is in comparison to the non-adaptive sample complexity, which is \(O_{k,\varepsilon}((\log n)^{\lfloor \log_2 k\rfloor})\). In fact, the main result provides a certificate of being far, in the form of a monotone increasing subsequence of length \(k\).</p></div>
    </content>
    <updated>2019-12-06T04:41:03Z</updated>
    <published>2019-12-06T04:41:03Z</published>
    <category term="Monthly digest"/>
    <author>
      <name>Gautam Kamath</name>
    </author>
    <source>
      <id>https://ptreview.sublinear.info</id>
      <link href="https://ptreview.sublinear.info/?feed=rss2" rel="self" type="application/atom+xml"/>
      <link href="https://ptreview.sublinear.info" rel="alternate" type="text/html"/>
      <subtitle>The latest in property testing and sublinear time algorithms</subtitle>
      <title>Property Testing Review</title>
      <updated>2019-12-08T23:36:50Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://windowsontheory.org/?p=7588</id>
    <link href="https://windowsontheory.org/2019/12/05/deep-double-descent/" rel="alternate" type="text/html"/>
    <title>Deep Double Descent (cross-posted on OpenAI blog)</title>
    <summary>By Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever This is a lightly edited and expanded version of the following post on the OpenAI blog about the following paper. While I usually don’t advertise my own papers on this blog, I thought this might be of interest to theorists, and […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p><em>By Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever</em></p>



<p><em>This is a lightly edited and expanded version of the following post on the <a href="https://openai.com/blog/deep-double-descent/">OpenAI blog</a> about the following <a href="http://mltheory.org/deep.pdf">paper</a>. While I usually don’t advertise my own papers on this blog, I thought this might be of interest to theorists, and a good follow up to <a href="https://windowsontheory.org/2019/11/15/puzzles-of-modern-machine-learning/">my prior post</a>. I promise not to make a habit out of it. –Boaz</em></p>



<p><strong>TL;DR:</strong>   <a href="http://mltheory.org/deep.pdf">Our paper</a> shows that <a href="https://arxiv.org/abs/1812.11118" rel="noreferrer noopener" target="_blank">double descent</a> occurs in conventional modern deep learning settings: visual classification in the presence of label noise (CIFAR 10, CIFAR 100) and machine translation (IWSLT’14 and WMT’14). As we increase the number of parameters in a neural network, initially the test error decreases, then increases, and then, just as the model is able to fit the train set, it undergoes a second descent, again decreasing as the number of parameters increases. This behavior also extends over train epochs, where a single model undergoes double-descent in test error over the course of training. Surprisingly (at least to us!), we show these phenomenon can lead to a regime where “<em><strong>more data hurts</strong></em>”—training a deep network on a larger train set actually performs worse.</p>



<h2>Introduction</h2>



<p>Open a statistics textbook and you are likely to see warnings against the danger of “overfitting”: If you are trying to find a good classifier or regressor for a given set of labeled examples, you would be well-advised to steer clear of having so many parameters in your model that you are able to completely fit the training data, because you risk not generalizing to new data.</p>



<p>The canonical example for this is polynomial regression. Suppose that we get <em>n</em> samples of the form <em>(x, p(x)+noise)</em> where <em>x</em> is a real number and <em>p(x)</em> is a cubic (i.e. degree 3) polynomial. If we try to fit the samples with a degree 1 polynomial—-a linear function, then we would get many points wrong. If we try to fit it with just the right degree, we would get a very good predictor. However, as the degree grows, we get worse till the degree is large enough to fit all the noisy training points, at which point the regressor is terrible, as shown in this figure:</p>



<figure class="wp-block-image"><img alt="" src="https://lh4.googleusercontent.com/H4f4ST5B9RnLX1ski6HEI7RBV5gqvk7WGiFR0qf6Savafmep6i08RYlpF5sgtq9oVqQ6ZkuglvCn0PTMQ_uaK3XStJlSskTSM6I52SyCZ91FeAcphq11MKa56wsfnDAG6GuruTT3"/></figure>



<p>It seems that the higher the degree, the worse things are, but what happens if we go <em>even higher</em>? It seems like a crazy idea—-why would we increase the degree beyond the number of samples? But it corresponds to the practice of having many more  parameters than training samples in modern deep learning. Just like in deep learning, when the degree is larger than the number of samples, there is more than one polynomial that fits the data– but we choose a specific one: the one found running gradient descent.</p>



<p>Here is what happens if we do this for degree 1000, fitting a polynomial using gradient descent (see <a href="https://colab.research.google.com/drive/1oMuUz3_BOENSoaOVOymLoB2mHeYBex8S">this notebook</a>):</p>



<figure class="wp-block-image"><img alt="" src="https://lh6.googleusercontent.com/DjQoPWG5gCueu_sAORKtrhNrrWY35OU3y-RXKJx0AZxIofVzZo-psP-JFsr_a4v20pWhH7EFfFPozckotWzvKypEebkPRJmdSa3vuy39ABdp2PqtqMr7dPX1PTDfMT362n4dGzVG"/></figure>



<p>We still fit all the training points, but now we do so in a more controlled way which actually tracks quite closely the ground truth. We see that despite what we learn in statistics textbooks, sometimes overfitting is not that bad, as long as you go “all in” rather than “barely overfitting” the data. That is, overfitting doesn’t hurt us if we take the number of parameters to be much larger than what is needed to just fit the training set — and in fact, as we see in deep learning, larger models are often better.</p>



<p>The above is not a novel observation. <a href="https://arxiv.org/abs/1812.11118">Belkin et al </a>called this phenomenon <strong><em>“double descent”</em></strong> and this goes back to <a href="http://www.ki.tu-berlin.de/fileadmin/fg135/publikationen/opper/Op01.pdf">even</a> <a href="https://arxiv.org/abs/1710.03667">earlier</a> <a href="https://arxiv.org/abs/1809.09349">works</a><a href="http://www.ki.tu-berlin.de/fileadmin/fg135/publikationen/opper/Op03b.pdf"> </a>. In this <a href="http://mltheory.org/deep.pdf">new paper</a> we (Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever) extend the prior works and report on a variety of experiments showing that “double descent” is widely prevalent across several modern deep neural networks and for several natural tasks such as image recognition (for the CIFAR 10 and CIFAR 100 datasets) and language translation (for IWSLT’14 and WMT’14 datasets).  As we increase the number of parameters in a neural network, initially the test error decreases, then increases, and then, just as the model is able to fit the train set, it undergoes a <em>second descent,</em> again decreasing as the number of parameters increases.  Moreover, double descent also extends beyond number of parameters to other measures of “complexity” such as the number of training epochs of the algorithm. </p>



<p>The take-away from our work (and the prior works it builds on) is that neither the classical statisticians’ conventional wisdom that <strong><em>“too large models are worse”</em></strong> nor the modern ML paradigm that <strong><em>“bigger models are always better”</em></strong><em> </em>always hold. Rather it all depends on whether you are on the first or second descent.  Further more, these insights also allow us to generate natural settings in which even the age-old adage of <strong><em>“more data is always better”</em></strong> is violated!</p>



<p>In the rest of this blog post we present a few sample results from this recent paper.</p>



<h3 id="modelwisedoubledescent">Model-wise Double Descent</h3>



<p>We observed many cases in which, just like in the polynomial interpolation example above, the test error undergoes a “double descent” as we increase the complexity of the model. The figure below demonstrates one such example: we plot the test error as a function of the complexity of the model for ResNet18 networks. The complexity of the model is the width of the layers, and the dataset is CIFAR10 with 15% label noise. Notice that the peak in test error occurs around the “interpolation threshold”: when the models are just barely large enough to fit the train set. In all cases we’ve observed, changes which affect the interpolation threshold (such as changing the optimization algorithm, changing the number of train samples, or varying the amount of label noise) also affect the location of the test error peak correspondingly. </p>



<p>We found the double descent phenomena is most prominent in settings with added label noise— without it, the peak is much smaller and easy to miss. But adding label noise amplifies this general behavior and allows us to investigate it easily.</p>



<figure class="wp-block-image"><img alt="" src="https://lh4.googleusercontent.com/tFWTvNkFwG8Ljx8zp9X3Ul6aUZT9YmkwcNk07g6_EFUklZoBUd9MqApBEojLzOp9N7yndveLwg5A7uj_vxpGVofQ2QCe-bbMAguQB38cK4NhRfXBp-SWMDQUt9x44r6d_fMur7NO"/></figure>



<p/>



<h3 id="samplewisenonmonotonicity">Sample-Wise Nonmonotonicity</h3>



<p>Using the model-wise double descent phenomenon we can obtain examples where training on <strong>more data actually hurts</strong>. To see this, let’s look at the effect of increasing the number of train samples on the test error vs. model size graph. The below plot shows Transformers trained on a language-translation task (with no added label noise):</p>



<figure class="wp-block-image"><img alt="" src="https://lh5.googleusercontent.com/YkuZGjuWGKoCcl1gNbQfaKO-96hX9ShcVZ_Dj0KlKddRZhgpMGtzs427zPuC9QwHOOKgmuV5E0aEKOU3zwhwpGmdiWwDDDBIk7hroJZfRLa7kXSThzwTDZoNSM8I2M9OfWxIW5X_"/></figure>



<p>On the one hand, (as expected) increasing the number of samples generally shifts the curve downwards towards lower test error. On the other hand, it also shifts the curve to the right: since more samples require larger models to fit, the interpolation threshold (and hence, the peak in test error) shifts to the right. For intermediate model sizes, these two effects combine, and we see that <strong>training on 4.5x more samples actually hurts test performance.</strong></p>



<h3 id="epochwisedoubledescent">Epoch-Wise Double Descent</h3>



<p><strong>There is a regime where training longer reverses overfitting.</strong> Let’s look closer at the experiment from the “Model-wise Double Descent” section, and plot Test Error as a function of both model-size and number of optimization steps. In the plot below to the right, each column tracks the Test Error of a given model over the course of training. The top horizontal dotted-line corresponds to the double-descent of the first figure. But we can also see that for a fixed large model, as training proceeds test error goes down, then up and down again—we call this phenomenon “epoch-wise double-descent.”</p>



<figure class="wp-block-image"><img alt="" src="https://lh3.googleusercontent.com/SCP6M-txj9ax5g9cR9Ry27X2nF1sEJbpsfC9FiME5umNm-BxcHHBmhseuuWEm3mHMzo_0o5q-92ETcaU0OEXnhTmv_7MpOREaaeIMs7zbL9P5aFeqkXMYh8O8VN4FdASnAu0HOPI"/></figure>



<p>Moreover, if we plot the Train error of the same models and the corresponding interpolation contour (dotted line) we see that it exactly matches the ridge of high test error (on the right).</p>



<p><strong>In general, the peak of test error appears systematically when models are just barely able to fit the train set.</strong></p>



<p>Our intuition is that for models at the interpolation threshold, there is effectively only one model that fits the train data, and forcing it to fit even slightly-noisy or mis-specified labels will destroy its global structure. That is, there are no “good models”, which both interpolate the train set, and perform well on the test set. However in the over-parameterized regime, there are many models that fit the train set, and there exist “good models” which both interpolate the train set and perform well on the distribution. Moreover, the implicit bias of SGD leads it to such “good” models, for reasons we don’t yet understand.</p>



<p>The above intuition is theoretically justified for linear models, via a series of recent works including [<a href="https://arxiv.org/abs/1903.08560">Hastie et al.</a>] and [<a href="https://arxiv.org/abs/1908.05355">Mei-Montanari</a>]. We leave fully understanding the mechanisms behind double descent in deep neural networks as an important open question.</p>



<p/>



<hr class="wp-block-separator"/>



<h2>Commentary: Experiments for Theory</h2>



<p>The experiments above are especially interesting (in our opinion) because of how they can inform ML theory: any theory of ML must be consistent with “double descent.” In particular, one ambitious hope for what it means to “theoretically explain ML” is to prove a theorem of the form:</p>



<p class="has-text-align-center">“If the distribution satisfies property X and architecture/initialization satisfies property Y, then SGD trained on ‘n’ samples, for T steps, will have small test error with high probability”</p>



<p>For values of X, Y, n, T, “small” and “high” that are used in practice.</p>



<p>However, these experiments show that these properties are likely more subtle than we may have hoped for, and must be non-monotonic in certain natural parameters.</p>



<p>This rules out even certain natural “conditional conjectures” that we may have hoped for, for example the conjecture that</p>



<p class="has-text-align-center">“If SGD on a width W network works for learning from ‘n’ samples from distribution D, then SGD on a width W+1 network will work at least as well”</p>



<p>Or the conjecture</p>



<p class="has-text-align-center">“If SGD on a certain network and distribution works for learning with ‘n’ samples, then it will work at least as well with n+1 samples”</p>



<p>It also appears to conflict with a “2-phase” view of the trajectory of SGD, as an initial “learning phase” and then an “overfitting phase” — in particular, because the overfitting is sometimes reversed (at least, as measured by test error) by further training.</p>



<p>Finally, the fact that these phenomena are not specific to neural networks, but appear to hold fairly universally for natural learning methods (linear/kernel regression, decision trees, random features) gives us hope that there is a deeper phenomenon at work, and we are yet to find the right abstraction.</p>



<p/>



<p/>



<p/>



<p><em>We especially thank Mikhail Belkin and Christopher Olah for helpful discussions throughout this work.</em> <em>The polynomial example is inspired in part by experiments in [<a href="https://arxiv.org/abs/1903.09139">Muthukumar et al.</a>]</em>.</p>



<p/></div>
    </content>
    <updated>2019-12-05T17:00:00Z</updated>
    <published>2019-12-05T17:00:00Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>preetum</name>
    </author>
    <source>
      <id>https://windowsontheory.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://windowsontheory.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://windowsontheory.org" rel="alternate" type="text/html"/>
      <link href="https://windowsontheory.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://windowsontheory.org/?pushpress=hub" rel="hub" type="text/html"/>
      <subtitle>A Research Blog</subtitle>
      <title>Windows On Theory</title>
      <updated>2019-12-09T13:20:49Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/12/05/postdoctoral-fellow-at-the-university-of-texas-at-austin-apply-by-january-15-2020-3/</id>
    <link href="https://cstheory-jobs.org/2019/12/05/postdoctoral-fellow-at-the-university-of-texas-at-austin-apply-by-january-15-2020-3/" rel="alternate" type="text/html"/>
    <title>Postdoctoral Fellow at The University of Texas at Austin (apply by January 15, 2020)</title>
    <summary>The Computer Science Department at UT Austin invites applications for a Postdoctoral Fellow in theoretical computer science for the 2020-21 academic year. The Fellow will work with Dana Moshkovitz and David Zuckerman on pseudorandomness and computational complexity. Review of applicants will begin on January 15, but applications will be accepted until the position is filled. […]</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>The Computer Science Department at UT Austin invites applications for a Postdoctoral Fellow in theoretical computer science for the 2020-21 academic year. The Fellow will work with Dana Moshkovitz and David Zuckerman on pseudorandomness and computational complexity. Review of applicants will begin on January 15, but applications will be accepted until the position is filled.</p>
<p>Website: <a href="https://utaustin.wd1.myworkdayjobs.com/UTstaff/job/UT-MAIN-CAMPUS/Postdoctoral-Fellow_R_00006957">https://utaustin.wd1.myworkdayjobs.com/UTstaff/job/UT-MAIN-CAMPUS/Postdoctoral-Fellow_R_00006957</a><br/>
Email: maguilar@cs.utexas.edu</p></div>
    </content>
    <updated>2019-12-05T14:24:31Z</updated>
    <published>2019-12-05T14:24:31Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-12-09T13:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/12/05/postdoc-at-university-of-edinburgh-apply-by-december-9-2019-2/</id>
    <link href="https://cstheory-jobs.org/2019/12/05/postdoc-at-university-of-edinburgh-apply-by-december-9-2019-2/" rel="alternate" type="text/html"/>
    <title>postdoc at University of Edinburgh (apply by December 9, 2019)</title>
    <summary>Applications are invited for a postdoctoral position in algorithms in the School of Informatics at the University of Edinburgh. The position is for a period of up to 3 years, and can start at any time in 2020. Website: https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.jobspec?p_id=050307 Email: h.sun@ed.ac.uk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Applications are invited for a postdoctoral position in algorithms in the School of Informatics at the University of Edinburgh. The position is for a period of up to 3 years, and can start at any time in 2020.</p>
<p>Website: <a href="https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.jobspec?p_id=050307">https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.jobspec?p_id=050307</a><br/>
Email: h.sun@ed.ac.uk</p></div>
    </content>
    <updated>2019-12-05T10:53:50Z</updated>
    <published>2019-12-05T10:53:50Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-12-09T13:20:38Z</updated>
    </source>
  </entry>

  <entry xml:lang="en">
    <id>http://cstheory-jobs.org/2019/12/05/postdoc-at-university-of-edinburgh-apply-by-december-9-2019/</id>
    <link href="https://cstheory-jobs.org/2019/12/05/postdoc-at-university-of-edinburgh-apply-by-december-9-2019/" rel="alternate" type="text/html"/>
    <title>postdoc at University of Edinburgh (apply by December 9, 2019)</title>
    <summary>Applications are invited for a postdoctoral position in algorithms in the School of Informatics at the University of Edinburgh. The position is for a period of up to 3 years, and can start at any time in 2020. Website: https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.display_form Email: h.sun@ed.ac.uk</summary>
    <content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><p>Applications are invited for a postdoctoral position in algorithms in the School of Informatics at the University of Edinburgh. The position is for a period of up to 3 years, and can start at any time in 2020.</p>
<p>Website: <a href="https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.display_form">https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.display_form</a><br/>
Email: h.sun@ed.ac.uk</p></div>
    </content>
    <updated>2019-12-05T10:49:33Z</updated>
    <published>2019-12-05T10:49:33Z</published>
    <category term="Uncategorized"/>
    <author>
      <name>shacharlovett</name>
    </author>
    <source>
      <id>https://cstheory-jobs.org</id>
      <logo>https://s0.wp.com/i/buttonw-com.png</logo>
      <link href="https://cstheory-jobs.org/feed/" rel="self" type="application/atom+xml"/>
      <link href="https://cstheory-jobs.org" rel="alternate" type="text/html"/>
      <link href="https://cstheory-jobs.org/osd.xml" rel="search" type="application/opensearchdescription+xml"/>
      <link href="https://cstheory-jobs.org/?pushpress=hub" rel="hub" type="text/html"/>
      <title>Theoretical Computer Science Jobs</title>
      <updated>2019-12-09T13:20:38Z</updated>
    </source>
  </entry>
</feed>
